<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,108.36,98.73,386.03,15.51;1,201.96,120.69,198.98,15.51">Fast LSIS Profile Entropy Features for Robot Visual Self-Localization</title>
				<funder ref="#_pH8xeFk">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder ref="#_kS4SACj">
					<orgName type="full">Research Fund for the Doctoral Program of Higher Education of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,176.76,154.07,68.44,9.96"><forename type="first">Herve</forename><surname>Glotin</surname></persName>
							<email>glotin@univ-tln.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab. Sciences de l&apos;Information et des Systmes LSIS</orgName>
								<orgName type="institution">UMR USTV CNRS 6168 La Garde</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Sud Toulon</orgName>
								<address>
									<settlement>Var</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,262.20,154.07,78.30,9.96"><forename type="first">Zhong-Qiu</forename><surname>Zhao</surname></persName>
							<email>zhongqiuzhao@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab. Sciences de l&apos;Information et des Systmes LSIS</orgName>
								<orgName type="institution">UMR USTV CNRS 6168 La Garde</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer &amp; Information</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,357.36,154.07,77.70,9.96"><forename type="first">Emilie</forename><surname>Dumont</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab. Sciences de l&apos;Information et des Systmes LSIS</orgName>
								<orgName type="institution">UMR USTV CNRS 6168 La Garde</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,108.36,98.73,386.03,15.51;1,201.96,120.69,198.98,15.51">Fast LSIS Profile Entropy Features for Robot Visual Self-Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7043B114B310103700FA60A647A729AE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Managment]: Visual Systems-CBIR Efficient visual features</term>
					<term>Fast Content Based Information Retrieval</term>
					<term>Profile Entropy Features</term>
					<term>LS-SVM</term>
					<term>Robot Vision</term>
					<term>Adaptive Localisation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the Robot Vision task, the participants are asked to answer where is the robot using its vision. The robot may be in 5 rooms (BO-One-person office, CR-Corridor, EO-Two-persons office, KT-Kitchen, PA-Printer Area). In order to train our models we structured the views of each room into several sub-classes: BO-inside, exit, enter; CR-enter, exit, leftstairs, nostairs, rightstairs; EO-enter, exit, inside; KT-enter, exit, cooking hearth, table, television; PA-cabinet, enter, exit. Then an SVM was constructed for each of these 19 sub-classes. After that, we combined the results of SVMs by maximizing to get the final decision. We run our classification models on the new Profile Entropy Features (PEF) that combines RGB color and texture, yielding to one hundred of dimension, and we compare them to generic Descriptor of Fourier (DF). We also made a fusion of the models on these 2 different features. So we got 3 runs. In our experiments, for each decision, we used only the current image, but we do not exploit continuity of the sequences. For this case, a total of 7 teams submitted runs. The official evaluation give for the SVM(PEF) run a score of 544, and for SVM(DF) run a score of -32, while their fusion a score of 509.5. Thus our result possesses the 5th rank over the seven. The experiments show that our SVM model works well with little training cost, and PEF feature works much better than DF feature. It could be concluded that PEF is quite efficient: it is very fast to be computed, with around 10 images computed per second on usual pentium, and less of 2 hours of training (compared to 60 hours for the best systems), but still give a competitive results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task addresses the problem of topological localization of a mobile robot using visual information. Specifically, participants will be asked to determine the topological location of a robot based on images acquired with a perspective camera mounted on a robot platform. The details of this task and dataset are shown in <ref type="bibr" coords="2,224.04,73.43,14.67,9.96" target="#b9">[10]</ref>.</p><p>We manually classified the views of each room into several classes: BO-inside, exit, enter; CRenter, exit, leftstairs, nostairs, rightstairs; EO-enter, exit, inside; KT-enter, exit, cooking hearth, table, television; PA-cabinet, enter, exit. Then an SVM was constructed for each small class, to distinguish this small-calss from all the others and recognize if the robot is currently in this small-class or not. So we constructed 19 SVMs in total(one per class). After that, we combined the results of SVMs by maximizing to get the final decision. We run our classification models on PEF(Profile Entropy Feature) and DF(Descriptor of Fourier) features and compared them, and we also made a fusion of the models on these 2 different features. So we submitted 3 runs. In our experiments, for each decision, we used only the current image, but not exploit continuity of the sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Profile Entropy Features</head><p>We propose in <ref type="bibr" coords="2,157.51,247.67,10.43,9.96" target="#b0">[1,</ref><ref type="bibr" coords="2,171.90,247.67,7.79,9.96" target="#b1">2]</ref> a new feature equal to the pixel 'profile' entropy. A pixel profile can be a simple arithmetic mean in horizontal (or vertical) direction. The advantage of such feature is to combine raw shape and texture representations in a low cpu cost feature.</p><p>Let I be an image (or a part of) of L(I) rows, and C(I) columns. The PEF are computed on these normalized RGB channels : l = (R + G + B)/3, r = R/l, and g = G/l. We consider the profiles of the orthogonal projections of the pixels to the horizontal X axis, noted Π op X , and to the vertical Y axis (Π op Y ), where op is a projection operator. This one is either the arithmetic mean of the pixels (noted Π Ari .</p><p>), or their harmonic mean (noted Π Harm .</p><p>), as illustrated in Fig. <ref type="figure" coords="2,461.59,331.43,4.41,9.96" target="#fig_0">1</ref> and Fig. <ref type="figure" coords="2,504.74,331.43,4.14,9.96" target="#fig_2">2</ref>. Thus the length of a given profile is either S = C(I) or S = L(I).</p><p>Then, for each profile, we estimate its probability distribution function ( p df ) on N bins (where N = round( √ S) as proposed in <ref type="bibr" coords="2,231.06,368.39,10.29,9.96" target="#b2">[3]</ref>).</p><p>For each channel, and each operator op , we compute : Φ op X (I) = p df (Π op X (I)). Considering that the sources are ergodic, we set P EF X component to the normalised entropy of this distribution : P EF X (I) = H(Φ op X (I))/log(N ), where N the number of bins of the considered distribution, and H the usual entropy function. We compute similar PEF on Y axis :</p><formula xml:id="formula_0" coords="2,90.00,461.73,138.30,14.01">P EF Y (I) = H(Φ op Y (I))/log(N ).</formula><p>We set a third PEF component to the entropy of the direct distribution of all the pixels in I, p df (I) :</p><formula xml:id="formula_1" coords="2,90.00,510.23,169.01,31.68">P EF B (I) = H( p df(I))/log(N ), where N = round( L(I) * C(I)) bins.</formula><p>The whole PEF features are the concatenation of P EF X , P EF Y and P EF B , with the usual mean and standard deviation of each channel of I.</p><p>The PEF are computed on three horizontal (noted '=') or vertical (' ' ) equal segmented subimages, and on the whole image. For exemple, for a given operator, we have the whole image plus the three ' ' subimages, and for each of the 3 channels we have P EF X , Y , B , plus their mean and variance, thus we have 4 * 3 * (3 + 2) = 60 dimensions. We note '#' the concatenation of '=' and ' ' PEF, without duplication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Fast classification using Least Squares Support Vector Machines</head><p>In order to design fast image retrieval systems, we use the Least Squares Support Vector Machine (LS-SVM). The SVM <ref type="bibr" coords="2,185.83,693.23,10.43,9.96" target="#b4">[5]</ref> first maps the data into a higher dimensional input space by some kernel functions, and then learns a separating hyperspace to maximize the margin. Currently, because of its good generalization capability, this technique has been widely applied in many areas such as face detection, image retrieval, and so on <ref type="bibr" coords="3,281.82,355.79,10.43,9.96" target="#b5">[6,</ref><ref type="bibr" coords="3,295.25,355.79,7.03,9.96" target="#b6">7]</ref>. The SVM is typically based on an ε-insensitive cost function, meaning that approximation errors smaller than ε will not increase the cost function value. This results in a quadratic convex optimization problem. So instead of using an ε-insensitive cost function, a quadratic cost function can be used. The least squares support vector machines (LS-SVM) <ref type="bibr" coords="3,137.97,403.67,10.43,9.96" target="#b7">[8]</ref> are reformulations to the standard SVMs which lead to solving linear KKT systems instead, which is quite computationally attractive. Thus, in all our experiments, we will use the LS-SVMlab1.5 (http://www.esat.kuleuven.ac.be/sista/lssvmlab/).</p><p>In our experiments, the RBF kernel</p><formula xml:id="formula_2" coords="3,225.72,460.03,151.56,18.40">K(x 1 -x 2 ) = exp(-|x 1 -x 2 | 2 /σ 2 )</formula><p>is selected as the kernel function of our LS-SVM. So there is a corresponding parameter, σ , to be tuned. A large value of σ 2 indicates a stronger smoothing. Moreover, there is another parameter, γ, needing tuning to find the tradeoff between to stress minimizing of the complexity of the model and to stress good fitting of the training data points.</p><p>We set these two parameters as respectively. So hundred of SVMs were constructed for each SVM model, and then we selected the best SVM using the validation set.</p><formula xml:id="formula_3" coords="3,217.92,551.59,27.65,11.20">σ 2 = [</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>In this task, the participants are asked to answer 'where are the robots'. The robots may be in 5 rooms (BO-One-person office, CR-Corridor, EO-Two-persons office, KT-Kitchen, PA-Printer Area). We manually classified the views of each room into several classes: BO-inside, exit, en-   <ref type="figure" coords="4,357.86,486.95,3.88,9.96" target="#fig_3">3</ref>). Then an SVM was constructed for each small class, to distinguish this small-calss from all the others and recognize if the robot is currently in this small-class or not. So we constructed 19 SVMs in total. After that, we combined the results of SVMs by maximizing to get the final decision. We run our classification models on PEF(Profile Entropy Feature) and DF <ref type="bibr" coords="4,278.00,534.83,10.55,9.96" target="#b8">[9]</ref> features and compared them, and we also made a fusion of the models on these 2 different features. So we got 3 runs. In our experiments, for each definition, we used only the current image, but not exploit continuity of the sequences. The evaluation was performed by the organizer. The following rules are used when calculating the score for a single test sequence: (1). +1.0 points for each correctly classified image Correct detection of an unknown room is treated the same way as correct classification. (2). -0.5 points for each misclassified image (3). 0.0 points for each image that was not classified (the algorithm refrained from the decision) In case several test sequences are used, the scores are calculated separately for each test sequence and then summarized.</p><p>As the evaluation, we got the SVM+PEF run with the score of 544, and the SVM+DF run with the score of -32, and the fusion run with the score of 509.5 (Table <ref type="table" coords="4,405.10,654.35,3.88,9.96">4</ref>). The best result of us possesses the 9th rank. The experiments show that our SVM model works well with little training cost, and PEF feature works much better than DF feature. It could be concluded that PEF is efficient. Moreover, PEF is fast with around 10 images computed per second on usual pentium. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Our SVM model on PEF works well, with medium performances, needing much less training time than other systems (around 50 times less). It could be concluded that our system with PEF feature is efficient for this task. We also noticed that the best result of those who used the continuity information, which is from SIMD group, attains a much higher score: 916.5. So in the future, we will combine in our system the continuity information through HMM optimisation, which shall results in a significant enhancement.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,90.00,276.23,422.65,9.96;3,90.00,288.23,422.66,9.96;3,90.00,300.23,243.39,9.96;3,174.16,59.25,254.19,202.46"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Horizontal X (Bottom) and, vertical Y (Top Right) profiles using arithmetic (-.-) and harmonic (-) projections of the luminance of an image of a tree. It shows clearly the difference between the two projections for this structured pattern.</figDesc><graphic coords="3,174.16,59.25,254.19,202.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,90.00,277.55,422.79,9.96;4,90.00,289.43,48.92,9.96;4,174.16,59.85,254.94,203.06"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Similar to Fig.1 but for an image of the concept sky : arithmetic and harmonic profiles are similar.</figDesc><graphic coords="4,174.16,59.85,254.94,203.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,215.52,353.99,171.80,9.96;5,103.08,59.04,396.80,280.58"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The framework of our system</figDesc><graphic coords="5,103.08,59.04,396.80,280.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,90.00,318.35,422.74,178.56"><head>Table 1 :</head><label>1</label><figDesc>The submitted runs of ImageCLEF2009 Robot Vision Task</figDesc><table coords="4,101.52,329.99,399.80,121.20"><row><cell>Rank</cell><cell>Run Tag</cell><cell>Method</cell><cell cols="2">Score Training time</cell></row><row><cell>1</cell><cell>MIRG UG</cell><cell>matched points; illumination filter</cell><cell>890.5</cell><cell>unknown</cell></row><row><cell>2</cell><cell>Idiap</cell><cell cols="2">CRFH+SIFT+PACT combined using G-DAS 793.0</cell><cell>unknown</cell></row><row><cell>3</cell><cell>UAIC</cell><cell>Full search using all frames</cell><cell>787.0</cell><cell>60h</cell></row><row><cell>4</cell><cell>CVIUI2R</cell><cell>LAB+histograms+Probabilistic SVM</cell><cell>784.0</cell><cell>unknown</cell></row><row><cell>5</cell><cell>LSIS 1</cell><cell>PEF+SVM</cell><cell>544.0</cell><cell>1h</cell></row><row><cell>6</cell><cell>SIMD</cell><cell>Hough+Canny+SIFT</cell><cell>511.0</cell><cell>60h</cell></row><row><cell>7</cell><cell>LSIS 3</cell><cell>Fusion of (PEF+SVM)(DF+SVM)</cell><cell>509.5</cell><cell>2h</cell></row><row><cell>8</cell><cell>MRIM</cell><cell>5x5 patches, Color SIFT, Language Model</cell><cell>456.5</cell><cell>unknown</cell></row><row><cell>9</cell><cell>LSIS 2</cell><cell>DF+SVM</cell><cell>-32.0</cell><cell>1h</cell></row></table><note coords="4,90.00,475.07,422.74,9.96;4,90.00,486.95,264.02,9.96"><p>ter; CR-enter, exit, leftstairs, nostairs, rightstairs; EO-enter, exit, inside; KT-enter, exit, cooking hearth, table, television; PA-cabinet, enter, exit (Figure</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>This work was supported by <rs type="funder">French National Agency of Research</rs> (<rs type="grantNumber">ANR-06-MDCA-002</rs>) and <rs type="funder">Research Fund for the Doctoral Program of Higher Education of China</rs> (<rs type="grantNumber">200803591024</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_pH8xeFk">
					<idno type="grant-number">ANR-06-MDCA-002</idno>
				</org>
				<org type="funding" xml:id="_kS4SACj">
					<idno type="grant-number">200803591024</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,105.47,588.23,407.18,9.96;5,100.56,600.23,347.08,9.96" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="5,155.72,588.23,322.17,9.96">Information retrieval and robust perception for a scaled multi-structuration</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<pubPlace>Toulon</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University Sud Toulon-Var</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Thesis for habilitation of research direction</note>
</biblStruct>

<biblStruct coords="5,105.47,619.91,407.21,9.96;5,100.56,631.79,412.16,9.96;5,100.56,643.79,122.36,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,264.24,619.91,248.44,9.96;5,100.56,631.79,94.12,9.96">Efficient Image Concept Indexing by Harmonic &amp; Arithmetic Profiles Entropy</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><forename type="middle">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ayache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,224.40,631.79,222.74,9.96">IEEE International Conference on Image Processing</title>
		<meeting><address><addrLine>Cairo, Egypt</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-11-07">2009. November 7-11, 2009 (2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.47,663.47,407.24,9.96;5,100.56,675.35,203.65,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,187.86,663.47,324.85,9.96;5,100.56,675.35,19.88,9.96">On estimation of entropy and mutual information of continuous distributions</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Moddemeijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,127.66,675.35,74.32,9.96">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="233" to="246" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.47,695.03,382.31,9.96" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m" coord="5,159.68,695.03,172.67,9.96">The nature of statistical learning theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.47,61.43,302.11,9.96" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m" coord="6,159.67,61.43,110.93,9.96">Statistical learning theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.47,81.35,407.42,9.96;6,100.56,93.35,288.19,9.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,205.13,81.35,217.61,9.96">Face detection using spectral histograms and SVMs</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">A</forename><surname>Waring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,431.02,81.35,81.88,9.96;6,100.56,93.35,185.25,9.96">IEEE Transactions on Systems, Man, and Cybernetics, Part B</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="467" to="476" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.47,113.27,407.41,9.96;6,100.56,125.27,412.44,9.96;6,100.56,137.15,27.80,9.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,220.59,113.27,253.42,9.96">Support vector machine active learning for image retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chang</forename><surname>Edward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,493.70,113.27,19.18,9.96;6,100.56,125.27,283.22,9.96">Proceedings of the ninth ACM international conference on Multimedia</title>
		<meeting>the ninth ACM international conference on Multimedia<address><addrLine>Ottawa, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="107" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.47,157.07,407.30,9.96;6,100.56,169.07,162.88,9.96" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="6,257.39,157.07,255.38,9.96;6,100.56,169.07,78.41,9.96">Least Squares Support Vector Machine Classifiers Neural Processing Letters</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A K</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="293" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.47,188.99,407.32,9.96;6,100.56,200.99,412.33,9.96;6,100.56,212.87,27.80,9.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,376.13,188.99,136.66,9.96;6,100.56,200.99,280.01,9.96">Generalized Fourier Descriptors with Applications to Objects Recognition in SVM Context, 30</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Smach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lemaitre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Gauthier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Miteran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Atri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,389.41,200.99,93.89,9.96">J. Math Imaging Vis</title>
		<imprint>
			<biblScope unit="page" from="43" to="71" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,110.50,232.79,402.12,9.96;6,100.56,244.79,180.87,9.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,275.74,232.79,202.73,9.96">Overview of the CLEF 2009 robot vision track</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pronobis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Jensfelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,485.88,232.79,26.74,9.96;6,100.56,244.79,81.96,9.96">CLEF working notes 2009</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
