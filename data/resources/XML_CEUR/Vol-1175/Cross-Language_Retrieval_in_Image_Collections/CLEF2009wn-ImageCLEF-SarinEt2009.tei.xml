<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,107.87,148.86,387.27,15.15;1,170.61,170.78,261.78,15.15">Joint Equal Contribution of Global and Local Features for Image Annotation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,187.05,204.67,108.35,8.74"><forename type="first">Supheakmungkol</forename><surname>Sarin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Global Information and Telecommunication Studies</orgName>
								<orgName type="institution">Waseda University</orgName>
								<address>
									<addrLine>1011 Okuboyama, Nishi-Tomida, Honjo-shi</addrLine>
									<postCode>367-0035</postCode>
									<settlement>Saitama-ken</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,318.09,204.67,97.86,8.74"><forename type="first">Wataru</forename><surname>Kameyama</surname></persName>
							<email>wataru@waseda.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Global Information and Telecommunication Studies</orgName>
								<orgName type="institution">Waseda University</orgName>
								<address>
									<addrLine>1011 Okuboyama, Nishi-Tomida, Honjo-shi</addrLine>
									<postCode>367-0035</postCode>
									<settlement>Saitama-ken</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,107.87,148.86,387.27,15.15;1,170.61,170.78,261.78,15.15">Joint Equal Contribution of Global and Local Features for Image Annotation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3E9A1BB47B8FC5FC979B65FDF7414669</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>I.4.10 [Image Processing and Computer Vision]: Image Representation</term>
					<term>I.4.9 [Image Processing and Computer Vision]: Applications Measurement, Performance, Experimentation Automatic Image Annotation, K Nearest Neighbors, Joint Equal Contribution, Saliency, Color, Texture, Gist of scene</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image annotation is a very important task as the number of photographs has gone sky-high. This paper describes our participation in the ImageCLEF Large Scale Visual Concept Detection and Annotation Task 2009. We present the method used for our best run. Our approach is inspired from a recently proposed method where joint equal contribution (JEC) of simple global color and texture features can outperform the state-of-the-art annotation techniques <ref type="bibr" coords="1,285.90,359.22,14.61,8.74" target="#b9">[10]</ref>. Our idea is that if such simple features could do so well, then the combination of higher-level features would do even better. Study has shown that the concurrent use of saliency and gist of the scene is a major trait of human vision system. Therefore, in this preliminary study, we propose to explore the combination of different visual features at global, local and scene levels including global and local color, texture, and gist of the scene. The experiments confirm that higher-level features lead to better performance. Through the experiments, we also found that using 40 nearest neighbors and HSV, HSV (at saliency regions), HAAR, GIST (full scene), GIST (scene at the center) as features produce the best result.We finally identify the weakness in our approach and ways on how the system could be optimized and improved.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The International Data Corporation (IDC) forecasts that there will be 500 billion images captured by 2010 <ref type="bibr" coords="1,128.38,732.15,9.96,8.74" target="#b3">[4]</ref>. Therefore, Automatic Image Annotation (AIA) is a very important problem given this exponential increase of images. AIA has been an ongoing research for more than 10 years and has been very active in the recent years. Researchers have been trying to exploit different kinds of resources from visual, textual, ontology to social labeling over the Internet. For a complete survey, please refer to <ref type="bibr" coords="2,155.62,135.93,10.52,8.74" target="#b1">[2,</ref><ref type="bibr" coords="2,169.80,135.93,7.01,8.74" target="#b6">7]</ref>. The hybrid models mixing visual and textual features usually produce the best results. However, they tend to be complex.</p><p>Recently, Makadia et al. introduce a rather simple method <ref type="bibr" coords="2,362.15,171.80,14.61,8.74" target="#b9">[10]</ref>. They extract global color and texture as features; calculate image similarity as the average distance using these features; and the keywords are obtained from the nearest neighbors with the least distance. Surprisingly, this approach outperforms the state-of-the-art algorithms in image annotations. This has inspired us. We believe that if such low-level features can do so well, then higher-level features would give even better performance.</p><p>In this paper, we describe our participation to the Large Scale Visual Concept Detection and Annotation Task of ImageCLEF 2009 <ref type="bibr" coords="2,258.88,267.44,14.61,8.74" target="#b10">[11]</ref>. We submitted 5 runs to this task. Here, we describe our best run (run id: KameyamaLab 21 2 1245594455534) where we propose to utilize features at the saliency regions of image as well as the holistic scene descriptor feature of the image in addition to the features proposed in <ref type="bibr" coords="2,248.10,303.30,14.61,8.74" target="#b9">[10]</ref>. We found that the fusion of features at global, local and scene levels can augment the performance of the system. Experiments also reveal that 5 features that can jointly produce the best results are HSV, HSV (at saliency regions), HAAR, GIST (full scene) and GIST (scene at the center). This best result is observed at the 40 nearest neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach and Implementation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Concept</head><p>In the work of Makadia et al. <ref type="bibr" coords="2,222.75,414.32,14.61,8.74" target="#b9">[10]</ref>, they extract 3 color histograms namely, RGB, HSV and LAB and 4 textures namely, Gabor, Haar, GaborQ and HaarQ. These are only basic global colors and texture features. We believe that using these features to represent the image is not enough. We need more higher-level features that could represent image globally at the scene level as well as locally at the Region Of Interest (ROI) level.</p><p>Human exhibits the exquisite ability at rapidly identifying the gist of the scene of the image. Usually, a human observer of an image at a fraction of second can summarize the essential information about the image such as indoor/outdoor, street, beach, landscape, etc. <ref type="bibr" coords="2,441.85,509.96,10.52,8.74" target="#b2">[3,</ref><ref type="bibr" coords="2,456.11,509.96,11.62,8.74" target="#b12">13]</ref>. Saliency is also a very important point of interest when human observes image because they tend to focus on some important regions or ROIs. Study has shown that the concurrent use of gist of the scene and saliency is a major trait of human vision system <ref type="bibr" coords="2,322.79,545.82,14.61,8.74" target="#b13">[14]</ref>. These give reasons for our idea. In this paper, we would like to capture these important features in addition to the basic ones proposed in <ref type="bibr" coords="3,144.83,123.98,14.61,8.74" target="#b9">[10]</ref>. The original research on gist of the scene has been reported in <ref type="bibr" coords="3,448.84,123.98,15.50,8.74" target="#b11">[12]</ref> with quite a successful rate. For saliency detection, Itti et al.'s work <ref type="bibr" coords="3,355.65,135.93,10.52,8.74" target="#b7">[8]</ref> has been the most popular one. However, it is rather complex and computationally expensive. A recent approach introduced by Hou et al. in <ref type="bibr" coords="3,152.13,159.84,10.52,8.74" target="#b4">[5]</ref> is simple and gives good performance in real-time computation. Therefore, we choose to implement the later in our work. The outline of our approach is shown in Figure <ref type="figure" coords="3,505.26,171.80,3.87,8.74" target="#fig_0">1</ref>. First the features are extracted at image level as well as ROI level. Then we combine the distance of image equally and use K Nearest Neighbor (KNN) method for label transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gist of the scene</head><p>The gist descriptors describe the spatial layout of an image using global features derived from the spatial envelope of an image. It is shown to be very good in scene categorization <ref type="bibr" coords="3,458.62,272.33,14.61,8.74" target="#b11">[12]</ref>. In this implementation, we calculate the gist descriptors of two variants of the original image. The first variant is the resized version (256 x 256) and the second one is the square size of the center of the image. The reason is that we want both full scene and the focused scene which is usually at the center. We resize the image for smaller computational cost. Figure <ref type="figure" coords="3,383.96,320.15,4.98,8.74" target="#fig_1">2</ref> shows the process. For each variant, a 512-dimensional vector is extracted.  <ref type="bibr" coords="3,156.36,615.56,10.52,8.74" target="#b4">[5]</ref> proposed a bottom up approach where they make use of scale invariance of natural image statistics. They calculate a spectral residual as the difference between original log spectrum and its mean-filtered version. The saliency map is obtained by applying inverse Fourier transform to the spectral residual. We compute the color histogram of the saliency regions for the three color spaces namely, RGB, LAB and HSV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global Color and Texture</head><p>We extract three global color histogram RGB, LAB and HSV. We also extract the two wavelet textures Haar and Gabor. It is noted that HaarQ and GaborQ are not implemented in our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distance Metric</head><p>We follow <ref type="bibr" coords="4,137.51,130.41,15.50,8.74" target="#b9">[10]</ref> by using KL-divergence as distance metric for LAB and LAB (saliency) and L1 for the other features. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Label Transfer</head><p>In <ref type="bibr" coords="4,102.85,383.00,14.61,8.74" target="#b9">[10]</ref>, first the keywords are selected from the nearest neighbor. If more keywords are needed, they are selected from neighbors 2 through N based on co-occurrence and frequency. Each feature contributes equally towards the image distance. Let d(i, j) be combined distance of image Ii and Ij. If dk (i,j) is the scaled distance, then</p><formula xml:id="formula_0" coords="4,255.69,439.62,257.31,30.48">d(i, j) = 1 N N K=1 dk (i,j)<label>(1)</label></formula><p>In our case, we cannot rely on the co-occurrence and frequency of the training data for the test set. Therefore, we directly rank the keywords of the top K nearest neighbors. We set a threshold to keep the number of concepts falls between 6 and 17 (minimum and maximum number of concepts for an image in the training dataset).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>Two different kinds of evaluations were conducted. The first evaluation is for the purpose to test and build our system prior to the release of the official test dataset. The second evaluation is the evaluation of our run submitted to ImageCLEF VCDT track. The MIR Flickr 25000 <ref type="bibr" coords="4,468.99,591.37,10.52,8.74" target="#b5">[6]</ref> is used in this evaluation campaign with the annotation size of 53 concepts. Please refer to <ref type="bibr" coords="4,464.26,603.33,15.50,8.74" target="#b10">[11]</ref> for the detailed procedures and the annotation process of the dataset used for evaluation campaign of ImageCLEF VCDT track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Precision, Recall and Keyword Coverage</head><p>In the first evaluation, we conduct it using the 5000-photo training dataset. We divide this into our training and testing set (4500 + 500). The test set is generated randomly. We calculate the precision, recall and keyword coverage (recalled keywords) of different combinations of features at different numbers of nearest neighborhoods. It is noted that for each experiment we repeat it 20 times and the result is the average. Table <ref type="table" coords="4,290.71,721.33,4.98,8.74">2</ref> gives the names of combinations of features used in the evaluation and their correspondent features. Figure <ref type="figure" coords="4,351.46,733.29,4.98,8.74" target="#fig_3">3</ref> shows the precision and recall rate of each combination methods. We can see that the full combination (Color + Texture + Color  To further analyze, we calculate the F-Measure which is the harmonic mean of precision and recall. We also compute the keyword coverage which is the number of keywords recalled by the system. These results are shown in Figure <ref type="figure" coords="5,279.54,474.61,3.87,8.74" target="#fig_4">4</ref>. The F-Measure rate confirms our assumption that more advanced features lead to better performance and that the selective combination produces the best result. We can also see that at the K = 40, we get the best result. The number of keyword coverage drops with the increase size of neighbors. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation per Concept</head><p>In this evaluation, the training set and test set are the complete 5000-photo training dataset and 13000-photo test dataset of ImageCLEF VCDT 2009 respectively. We use the selective combination at K = 40 to generate the result which is the best run that we submitted to the track. For each concept, the Area Under Curve (AUC) and Equal Error Rate (EER) are calculated. Figure <ref type="figure" coords="6,508.01,123.98,4.98,8.74" target="#fig_5">5</ref> shows the results of each concept. The average AUC is 0.16 while EER is 0.45. The results are not good and some concepts are not detected at all. One of the reasons that contribute to this poor performance is that the evaluation of EER and AUC requires confident score of each annotated concepts while our system does not provide this probabilistic number. We simply give 1 and 0 to concept detected and undetected respectively. Another reason is the difference between the distributions of the concepts in the training set and the testing set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We report our preliminary experiments combining local and global features for image annotation task based on JEC and KNN model. Generally, it is confirmed that more advanced features are needed though we still need to further investigate on the independence of each feature. This is validated by the fact that our selective combination using only 5 features gives better performance than the total combination of features. Additionally, the experiments show that our approach tends to prefer common concepts to the uncommon ones, thus, leaving some concepts totally undetected. This is because we use KNN where the algorithm assigns the most common concepts of the K nearest neighbors to the test image. Therefore, the selection of K is important but more importantly this adhoc JEC <ref type="bibr" coords="6,242.53,533.49,15.50,8.74" target="#b9">[10]</ref> that we follow might not work best. We need to define a probabilistic model where dynamic weighting scheme can be generated on the fly based on the features and concepts of the nearest neighbors. We also would like to define and integrate some other advanced content-based features (e.g. SIFT <ref type="bibr" coords="6,311.90,569.35,10.79,8.74" target="#b8">[9]</ref>) and optical features like aperture, shutter speed, ISO, focal length, etc. that have become increasingly available <ref type="bibr" coords="6,406.92,581.31,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="6,421.63,581.31,7.01,8.74" target="#b5">6]</ref>. These define our future works.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,211.73,731.35,179.55,8.74;2,103.50,567.07,396.00,149.17"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Flow Diagram of the Approach</figDesc><graphic coords="2,103.50,567.07,396.00,149.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,128.30,559.32,346.40,8.74;3,157.50,352.80,288.00,191.40"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The two variants of original image that we extract the gist descriptor</figDesc><graphic coords="3,157.50,352.80,288.00,191.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,96.38,110.82,124.76,8.74;5,290.62,110.82,36.75,8.74;5,96.38,125.57,72.00,8.74;5,290.62,125.57,187.91,8.74;5,96.38,137.92,393.21,8.74;5,290.62,149.88,204.99,8.74;5,290.62,161.83,122.18,8.74;5,96.38,174.19,37.63,8.74;5,290.62,174.19,205.30,8.74;5,290.62,186.14,54.05,8.74;5,156.07,208.00,290.86,8.74;5,90.00,238.87,423.00,8.74;5,90.00,250.83,423.00,8.74;5,90.00,262.78,338.34,8.74"><head></head><label></label><figDesc>RGB + LAB + HSV + HAAR + GABOR Color + Texture + Color Saliency + Gist RGB + LAB + HSV + HAAR + GABOR + RGB saliency + LAB saliency + HSV saliency + GIST 256 + GIST center Selective HSV + HAAR + HSV saliency + GIST 256 + GIST center Table 2: Feature Combination Names and Correspondent Features Saliency + Gist) gives better results in both precision and recall. More importantly, the selective combination of HSV, HAAR, HSV saliency, GITS 256 and GIST center gives the best results. We found this combination by doing random combination among all the features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,149.83,427.27,303.34,8.74;5,103.50,283.00,396.01,129.15"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Precision and Recall Rate by Number of Nearest Neighbors</figDesc><graphic coords="5,103.50,283.00,396.01,129.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="5,120.48,676.14,362.03,8.74;5,103.50,530.70,396.00,130.33"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: F-Measure and Recalled Keyword Rate by Number of Nearest Neighbors</figDesc><graphic coords="5,103.50,530.70,396.00,130.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="6,122.56,370.57,357.88,8.74;6,92.70,216.40,417.59,139.06"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Equal Error Rate (EER) and Area Under Curve (AUC) of each concept</figDesc><graphic coords="6,92.70,216.40,417.59,139.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,90.00,142.36,423.00,189.21"><head>Table 1 :</head><label>1</label><figDesc>Table 1 summarizes our features, their respective categories and distance metrics. Features, Categories and Distance Metrics</figDesc><table coords="4,158.16,175.04,286.68,134.67"><row><cell>Feature Name</cell><cell>Category</cell><cell cols="2">Dimension Distance Metric</cell></row><row><cell>RGB</cell><cell>Global Color</cell><cell>48</cell><cell>L1</cell></row><row><cell>LAB</cell><cell>-</cell><cell>48</cell><cell>KL-divergence</cell></row><row><cell>HSV</cell><cell>-</cell><cell>48</cell><cell>L1</cell></row><row><cell>HAAR</cell><cell>Global Texture</cell><cell>96</cell><cell>L1</cell></row><row><cell>GABOR</cell><cell>-</cell><cell>64</cell><cell>L1</cell></row><row><cell>RGB saliency</cell><cell>Local Color</cell><cell>48</cell><cell>L1</cell></row><row><cell>LAB saliency</cell><cell>-</cell><cell>48</cell><cell>KL-divergence</cell></row><row><cell>HSV saliency</cell><cell>-</cell><cell>48</cell><cell>L1</cell></row><row><cell>GITS 256</cell><cell>Scene Descriptor</cell><cell>512</cell><cell>L1</cell></row><row><cell>GITS center</cell><cell>-</cell><cell>512</cell><cell>L1</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,110.48,648.03,180.45,8.74" xml:id="b0">
	<monogr>
		<ptr target="http://www.exif.org" />
		<title level="m" coord="6,110.48,648.03,81.38,8.74">EXIF Specification</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,110.48,667.96,402.52,8.74;6,110.48,679.91,291.01,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,367.31,667.96,145.69,8.74;6,110.48,679.91,111.75,8.74">Image retrieval: Ideas, influences, and trends of the new age</title>
		<author>
			<persName coords=""><forename type="first">Ritendra</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dhiraj</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,230.93,679.91,88.81,8.74">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="60" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,110.48,699.84,402.52,8.74;6,110.48,711.79,322.89,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,171.32,699.84,341.68,8.74;6,110.48,711.79,29.90,8.74">Framing pictures: The role of knowledge in automatized encoding and memory for gist</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,148.46,711.79,195.65,8.74">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="316" to="355" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,110.48,731.72,402.52,8.74;6,110.48,743.67,402.52,8.74;7,110.48,112.02,402.52,8.74;7,110.48,123.98,53.71,8.74" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="6,413.48,743.67,99.52,8.74;7,110.48,112.02,289.33,8.74">The Expanding Digital Universe: A Forecast of Worldwide Information Growth Through</title>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">F</forename><surname>Gantz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Reinsel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Chute</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wolfgang</forename><surname>Schlichting</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Mcarthur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Minton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Irida</forename><surname>Xheneti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anna</forename><surname>Toncheva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Manfrediz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2010. March 2007</date>
		</imprint>
	</monogr>
	<note type="report_type">IDC White Paper</note>
</biblStruct>

<biblStruct coords="7,110.48,143.90,402.52,8.74;7,110.48,155.86,402.52,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,249.08,143.90,219.02,8.74">Saliency Detection: A Spectral Residual Approach</title>
		<author>
			<persName coords=""><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liqing</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,490.33,143.90,22.67,8.74;7,110.48,155.86,324.48,8.74">Proc. IEEE Conference on Computer Vision and Pattern Recognition CVPR &apos;07</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition CVPR &apos;07</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,175.78,402.52,8.74;7,110.48,187.74,402.52,8.74;7,110.48,199.69,158.08,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,289.45,175.78,156.10,8.74">The MIR flickr retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">J</forename><surname>Huiskes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,471.90,175.78,41.10,8.74;7,110.48,187.74,398.68,8.74">MIR &apos;08: Proceeding of the 1st ACM international conference on Multimedia information retrieval</title>
		<meeting><address><addrLine>NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="39" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,219.62,402.52,8.74;7,110.48,231.57,112.60,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,181.06,219.62,274.25,8.74">Image retrieval: Research and use in the information explosion</title>
		<author>
			<persName coords=""><forename type="first">Masashi</forename><surname>Inoue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,464.57,219.62,48.42,8.74;7,110.48,231.57,48.42,8.74">Progress in Informatics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="3" to="14" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,251.50,402.52,8.74;7,110.48,263.45,402.52,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,319.41,251.50,193.59,8.74;7,110.48,263.45,106.68,8.74">A Model of Saliency-Based Visual Attention for Rapid Scene Analysis</title>
		<author>
			<persName coords=""><forename type="first">Laurent</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christof</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ernst</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,225.54,263.45,176.89,8.74">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,283.38,402.52,8.74;7,110.48,295.33,402.52,8.74;7,110.48,307.29,22.69,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,169.83,283.38,232.26,8.74">Object recognition from local scale-invariant features</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,424.89,283.38,88.11,8.74;7,110.48,295.33,197.33,8.74">Proc. Seventh IEEE International Conference on Computer Vision</title>
		<meeting>Seventh IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1999">September 20-27, 1999</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,327.21,402.52,8.74;7,110.48,339.17,178.93,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,361.16,327.21,151.84,8.74;7,110.48,339.17,15.94,8.74">A New Baseline for Image Annotation</title>
		<author>
			<persName coords=""><forename type="first">Ameesh</forename><surname>Makadia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vladimir</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,147.27,339.17,43.99,8.74">ECCV (3)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="316" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,359.09,402.52,8.74;7,110.48,371.05,342.85,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,266.56,359.09,246.44,8.74;7,110.48,371.05,136.12,8.74">Overview of the CLEF 2009 Large Scale Visual Concept Detection and Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">Stefanie</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Dunker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,268.13,371.05,88.69,8.74">CLEF working notes</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,390.97,402.52,8.74;7,110.48,402.93,402.52,8.74;7,110.48,414.88,22.69,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,268.40,390.97,244.60,8.74;7,110.48,402.93,132.98,8.74">Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope</title>
		<author>
			<persName coords=""><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,255.44,402.93,185.50,8.74">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,434.81,402.52,8.74;7,110.48,446.76,237.38,8.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,166.99,434.81,183.20,8.74">Short-term conceptual memory for pictures</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">C</forename><surname>Potter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,356.95,434.81,156.05,8.74;7,110.48,446.76,131.81,8.74">Journal of Experimental Psychology: Human Learning and Memory</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="509" to="522" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,466.69,402.52,8.74;7,110.48,478.65,135.23,8.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="7,209.26,466.69,214.62,8.74">Biologically inspired mobile-robot self localization</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Siagian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,431.64,466.69,81.36,8.74;7,110.48,478.65,36.64,8.74">The Neuromorphic Engineer</title>
		<imprint>
			<biblScope unit="page" from="1" to="2" />
			<date type="published" when="2007-12">Dec 2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
