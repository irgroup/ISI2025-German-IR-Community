<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,169.13,74.37,257.23,12.54;1,242.09,92.87,111.37,12.54">University of Glasgow at ImageCLEF 2009 Robot Vision Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,194.81,120.29,40.89,10.04"><forename type="first">Yue</forename><surname>Feng</surname></persName>
							<email>yuefeng@dcs.gla.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Multimedia Information Retrieval Group</orgName>
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<postCode>G12 8RZ</postCode>
									<settlement>Glasgow</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,243.41,120.29,64.51,10.04"><forename type="first">Martin</forename><surname>Halvey</surname></persName>
							<email>halvey@dcs.gla.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Multimedia Information Retrieval Group</orgName>
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<postCode>G12 8RZ</postCode>
									<settlement>Glasgow</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,329.23,120.29,71.33,10.04"><forename type="first">Joemon</forename><forename type="middle">M</forename><surname>Jose</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Multimedia Information Retrieval Group</orgName>
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<postCode>G12 8RZ</postCode>
									<settlement>Glasgow</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,169.13,74.37,257.23,12.54;1,242.09,92.87,111.37,12.54">University of Glasgow at ImageCLEF 2009 Robot Vision Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8F04BB046060E55C42C240B95206234A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I.4 [Image Processing and Computer vision]: I.4.8 Scene Analysis</term>
					<term>I.4.9 Applications</term>
					<term>I.5 [Pattern Recognition]: I.5.4 Applications Algorithms, Measurement, Performance, Experimentation Robot Vision, Computer Vision, Illumination Filter, Decision Rules</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The submission from the Multimedia Information Retrieval Group at the University of Glasgow for the ImageCLEF 2009 Robot Vision Task combines point matching methodologies with rule based decision techniques. Instead of the whole image we use a large set of interesting points extracted from the image contents to represent each image. The points of interest are extracted using an edge corner detector. The RANAC method [3] was then applied to estimate the similarity between the test and training images based on the number of matched pairs of points. The location of robot is then annotated based on the training image that contains the highest number of matched point pairs with the test image. A set of decision rules with the respect to the trajectory behaviour of robot's motion are defined to refine the final results. An illumination filter was also applied for two of the runs in order to reduce the illumination effect. Three runs were submitted using the different of combination of the above approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this paper, we describe the system and examine the approaches and results for the three independent runs submitted by the Multimedia Information Retrieval group at the University of Glasgow for the ImageCLEF 209 Robot Vision task.</p><p>The goal of the ImageCLEF 2009 Robot Vision Task is to address the problem of topological localisation of a mobile robot using visual information. Specifically, participants were asked to determine the topological location of a robot based on images acquired with a perspective camera mounted on a robot platform <ref type="bibr" coords="1,456.95,641.68,10.86,9.05" target="#b1">[2]</ref>. Given a test sequence, a system/algorithm must be able to provide information about the location of the robot, where the relevant location information is available in a training sequence. Our strategy is to analyse the visual content of the test sequence and compare it with the training sequence to decide the location. Given a set of training sequences which are annotated with location information and an unannotated video frame captured by the robot, the best solution for identifying the location of this unannotated frame is to find the closest frame in the training set. Two possible techniques could be applied to solve this problem: classification and image matching. The classification approach <ref type="bibr" coords="1,165.89,734.20,10.68,9.05" target="#b5">[6]</ref>, which is famous for classifying images into one of a trained set of groups, according to feature distance to each group, requires an initial training step to obtain the class properties. However, its performance for identifying images belonging to an unknown class is not very good due to a lack of training samples, which are not available at the application stage. The image matching <ref type="bibr" coords="2,392.12,86.36,11.86,9.05" target="#b6">[7]</ref> approach is another potential solution, which is able to automatically and efficiently detect that two images are similar, or find similar images to a test image within a database of images. This approach estimates the visual distance between the unannotated frame and each frame in the training sequence and returns a ranked list, where the location of the unannotated frame could be annotated as the same as training image with the highest score in the rank list. In order to annotate the unknown frames, which are captured in an unknown room, the computed rank list is not reliable since all the training images on the list are all annotated. Thus, a similarity threshold could be introduced to filter out the false matches and annotate the unknown rooms. Thus, our intuition is that an image matching approach is a better approach than a classification approach for the ImageCLEF 2009 Robot Vision Task.</p><p>It is well known that when an object/robot moves through its environment it does not move randomly. Instead, it usually follows specific trajectories or motion patterns corresponding to its intentions <ref type="bibr" coords="2,415.55,228.56,10.66,9.05" target="#b0">[1]</ref>. Knowledge about such patterns can be used to assist any system to robustly keep track of robot in its environment and identify the location of the robot. Motivated by the above work <ref type="bibr" coords="2,200.57,278.27,10.87,9.05" target="#b0">[1,</ref><ref type="bibr" coords="2,215.89,278.27,7.41,9.05" target="#b5">6,</ref><ref type="bibr" coords="2,227.75,278.27,7.22,9.05" target="#b6">7]</ref>, our adopted approach combines image matching with a rule based decision approach. In addition, an illumination filter is integrated into one of the runs in order to minimise lighting effects with the goal of improving the predictive accuracy.</p><p>The rest of the paper is organised as follows. The next section introduces our proposed methodology for the robot vision task. In Section 3, we describe our submitted runs and the results of those runs. Finally, in Section 4 we present a conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methodology</head><p>For our approach image matching is first applied to find the most closely matching frames from the training sequences in order to annotate the test frame. A rule based decision maker is then applied in order to refine the results based on the movement behaviour of the robot. In addition, an illumination filter is applied to pre-process the sequences in order to reduce the lighting effectiveness. Each of these steps is described in more detail in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Points Based Image Matching</head><p>Considering both the training and test sequences are captured using the same camera in the same geographic condition, we assume that frames taken in the same location will contain similar content and geometric information.</p><p>Motivated by the above assumption, the proposed image matching algorithm is designed in the following successive stages: (1) A corner detection method is first used to create an initial group of points of interest; (2) The Ransac algorithm <ref type="bibr" coords="2,163.98,581.65,11.69,9.05" target="#b2">[3]</ref> is then applied to establish point correspondences between two frames and calculate the Fundamental matrix <ref type="bibr" coords="2,171.17,594.85,10.76,9.05" target="#b3">[4]</ref>. This matrix encodes an epipolar constraint which is applied to the general motion and rigid structure; this is used to compute the geometric information for refining the matched point pairs. (3) The number of refined matched points will be regarded as the similarity between two frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Points of Interest</head><p>In order to reduce the computation cost, we use points of interest (POI) rather than all pixels in the frame. Considering the fact that corners are local image features characterised by locations where variations of intensity in both X and Y directions are high, it is easier to detect and compare the POI within and around the corners, such as edges or textures. To this end, for this work we employ Harris corner detector <ref type="bibr" coords="2,402.26,710.08,12.03,9.05" target="#b4">[5]</ref> to initiate the POI, since it has strong invariance to rotation, scale, illumination variation and image noise. The Harris corner detector uses the local autocorrelation function to measure the local changes of the signal to detect the corner positions in each frame. More details can be found in <ref type="bibr" coords="3,229.21,73.14,10.79,9.05" target="#b4">[5]</ref>. Results of the corner detection are shown in Figure <ref type="figure" coords="3,471.35,73.14,3.76,9.05" target="#fig_0">1</ref>, where the detected corners are represented by '*' and '+'.</p><p>One frame from training sequence.</p><p>One frame from test sequence</p><p>The detected corners of the above frames </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">Point Matching</head><p>The next step of our approach is to use a point matching technique to establish point correspondences between two frames. The point matching method generates putative matches between two frames by looking for points that are maximally correlated with each other inside a window surrounding each point. Only points that correlate strongly with each other in both directions are returned. Given the initial P points of interests, a parameter X is used for checking whether this point is fitted or not, is first estimated using N points chosen at random from P. It is then found how many points in P fit the model with values of X within a tolerance value T given by the user. If the number is satisfactory, it is regarded as a fit and the operation terminates with success. Such operations are carried on looping through all the POI. In the present work, T is set at 95%. Setting such a high threshold value reduces the number of points of interest, so only the points with 'good match' are kept.. Examples of matched points can be seen in Figure <ref type="figure" coords="3,185.18,594.37,7.96,9.05" target="#fig_2">2a</ref>, where a pair of matched points is represented by '*' and '+'.</p><p>The initial matching pair may contain many mismatches, thus a post processing step for refining the results is needed. Given the assumption in Section 2.1 that frames taken in the same location will contain similar geometric information, we applied the fundamental matrix <ref type="bibr" coords="3,314.87,643.96,10.91,9.05" target="#b3">[4]</ref>. The fundamental matrix was initially designed for finding corresponding points in stereo images, where it helps to find the matching pairs using the computed geometric information.</p><p>Given the initial matching points, the fundamental matrix F can be estimated given a minimum of seven point's correspondences. Its seven parameters represent the only geometric information about the cameras that can be obtained through points correspondences alone.  Thus, after applying the matrix F on all the paired points to filter out the mis-matched points, the number of matched point pairs remaining will be regarded as the similarity between two images.</p><p>In order to localise the robot, we assume that the position can be retrieved by finding the most similar frame in the training sequence to the test sequence. Thus, the most similar frame should contain the most matched points with the testing frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Decision Rules</head><p>Given the results of point matching, each test frame can be annotated as being from one of the possible rooms and the trajectory of the robot could be generated. The trajectory could be represented using the extracted annotation information frame by frame. An example (Example 1) of the extracted trajectory could be represented as:</p><formula xml:id="formula_0" coords="4,136.82,541.68,321.70,62.70">ppppppppppppppppppppppppp period 1 cccc period 2 pp period 3 cccccccccccccccccccc ccccccccccccccccccccc period 4</formula><p>eeeee eeeee eeeee eeeee eeeee eeeee eeeee eeeee period 5</p><p>kkkkkkkkkkkkk kkkkkkkkkkkkk period 6 cccccccccc ccccccccccccc period 7</p><p>Where p, c, e and k represent 'printing area', 'corridor', 'kitchen', and 'two-person office' respectively. In this example, the robot travels continuously from 'printing area' to 'corridor', then back to 'printing area', 'corridor', 'two-persons office', 'kitchen' and 'corridor'. In order to study the movement behave of robot, each continuous moving shot is considered as one period, this sequence contains seven periods in different rooms.</p><p>By studying the training sequence released as part of the ImageCLEF 2009 Robot Vision training and test sets, we find (i) the robot does not move 'randomly', (ii) the period of time that the robot stays in the one room is always more than 0.5 seconds, which corresponds to more than 12 continuously frames, (iii) the robot always enters one room from the outside of the room, e.g. 'pa' or 'corridor', instead of from another room, then it exits this room to the place where it came from instead of to a different place.</p><p>Based on the above observations, we defined a set of rules to help determine the location of the robot at any given time, Rule 1: Time length. The robot will not stay in one place for a period less than 20 frames. For instance, in the 2 nd and the 3 rd period of the above example, the extracted sequence shown that the robot only stayed continuously in these two locations for just four and two frames, respectively, which is satisfied with this rule. Thus, there must be a false detection in the 2 nd and the 3 rd period.</p><p>Rule 2: Jumping room. If the location of the robot changes from room A to room B without passing through the 'corridor' and 'printing area', there must be a false detection. For instance, in the 5 th and 6 th period of the above example, the extracted sequence shown that the robot moved from 'two-person office' to 'kitchen', which is a typical 'jumping room' case that robot did not enter certain place during changing rooms. Thus, there must be a false detection between the 5 th and the 6 th period.</p><p>Rule 3: Unknown room. Since test sequence contains additional rooms that were not images previously, no correspondence frames in the training set could be used to annotate these rooms. In addition, considering the nature of image matching algorithm that one test frame will be annotated with the most similar frame in the training set even there is only one matched point pair found, a false annotation is not avoidable. In the experiment, we found that the number of detected matched point pair between the unknown frame and the training frames is very limited. Thus, we define one rule that any frame detected less than 15 matched point pairs with the training frames is annotated as an unknown room.</p><p>To refine the false detection, if the initial detection does not obey the rule 1, we do as follows:</p><p>ÔÇ∑ If the location before the false detection period is the same as the location behind it, the location of the false period will be revised and annotated the same as the pervious period. For example, the 2 nd period will be corrected to the 1 st period.</p><p>If the initial detection does not obey the rule 2, we do as follows:</p><p>ÔÇ∑ A window with a size of N frames is applied on the location boundary to recalculate the similarity. ÔÇ∑ The similarity between the test image and the top 10 matched training images is summed as the recalculated similarity. The frames with the highest score will be used to annotate the current frame with the location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Illumination Filter</head><p>The sequences used in the task are taken in different rooms under different illumination settings, e.g. some of the images are from bright offices where as some are taken in dark corridor. Considering the performance of the image matching method that we have described above is highly dominated by two factors, the number of points of interests and point matching techniques, the more points of interest that are found, the easier the point matching step becomes. Thus, the poor visual quality of the sequence could affect the performance of the edge detector and thus could reduce the number of points of interest for point matching. With the goal of improving the results, a method to reduce the effect of illumination effects was used for one of the runs which were submitted. In order to reduce the illumination effect an illumination filter called Retinex <ref type="bibr" coords="5,453.46,706.96,10.68,9.05" target="#b7">[8]</ref>, was applied. Retinex can improve visual rendering of a frame in which lighting conditions are not good. Once the illumination condition is improved, the number of points of interest detected by corner detector can be increased, as the increased image quality should result the more corners being detected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Submitted Runs</head><p>We submitted three runs for ImageCLEF 2009 Robot Vision task using the different combination of the image matching approach, decision rules approach and illusion filter approach. The detailed descriptions of each run are outlined below:</p><p>(i) The first run is regarded as the baseline and it uses every first frame out of every five continuous frames of both the training and testing sequences for image matching, this is followed by the application of the rule based model to refine the results. (ii) The second run uses all of the frames in training and testing set for image matching, followed by the application of the rule based model. The illumination filter is applied for pre-processing the frames. (iii) The third run uses every first frame out of every five continuous frames for both training and test sequence for image matching, followed by the application of the rule based model to refine the results. The illumination filter is applied for pre-processing the frames.</p><p>Considering that every second of video contains 25 frames and the visual difference between two continuous frames is very limited, a large amount of redundancy is containing in the sequence and increases the computing cost. In order to improve the efficiency, we split one sequence into shots, where every shot contains five continuous frames and the first frame of the five is regarded as the key-frame to represent the shot. Instead of training and annotate using every frame, we now only take the key-frames into account. Once a key-frame has been annotated, all the rest frames in this shot are annotated as the same. Since the computation cost of our approach is linear, the key-frame representation can reduce the processing time by 80%. However, this may result in false detection while the robot changes location. For example, in the example one, the frames from 21 st to 25 th are annotated as 'printing area'. However, the robot moves out from the printing area to corridor at 23 rd frame, since the 21 st frame is annotated as 'p' the other four frames are annotated similarly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results and Evaluation</head><p>We have generated multiple runs of annotation results based on the approaches presented before. All three runs are submitted to ImageCLEF for official evaluation. The 'score' is used as the measure of the overall performance of the systems. The following rules are used when calculating the score for a single test sequence: ÔÇ∑ +1.0 points for each correctly annotated frame. ÔÇ∑ Correct detection of an unknown room is treated the same way as correct annotation. ÔÇ∑ -0.5 points for each misannotated frame. ÔÇ∑ 0.0 points for each image that was not annotated (the algorithm refrained from the decision).</p><p>In addition, we also use 'accuracy' as another way of measure of the performance, which shows the percentage of frames that are annotated correctly.</p><p>Table <ref type="table" coords="6,97.93,586.21,4.98,9.05" target="#tab_0">1</ref> shows the results of the three runs. It can be seen clearly that the second run achieved the highest accuracy and score overall. Compared with the first run, the second run improves the accuracy from 59% to 68.5%, which shows that using more frames for training and testing on all test frames could increase the image matching results. In addition, it also shows that the illumination filter does not improve the system performance.</p><p>Table <ref type="table" coords="6,96.25,649.12,4.98,9.05" target="#tab_0">1</ref> lists the performance of the submitted runs. The baseline run offers a reasonable starting performance for the combination of image matching and rule based decision approaches. After using more frames for training and testing and an illusion filter, the second run obtains a 9.5% and 240 point performance gain in terms of accuracy and score, respectively. In the last run, an illusion filter is used for pre-processing and applied to the same number of frames as the baseline for training and testing the accuracy and score is largely reduced by 33.1% and 838.5, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accuracy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions</head><p>The multimedia information retrieval group at the University of Glasgow participated in the ImageCLEF Robot Vision task. In this paper we have presented the methodology, experiments and the preliminary results for the task. The model of combining image matching, decision rules and illumination filter contributes the high effectiveness. The results reflect the magnitude of the difficulty of the problem at robot vision, while we believe we have gained much insight to the practical problems, and future evaluations have the potential to produce much better results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,70.94,426.23,244.53,9.05;3,118.70,264.00,180.70,135.00"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The original frames and results for corner detection</figDesc><graphic coords="3,118.70,264.00,180.70,135.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,70.94,73.13,453.07,9.06;4,70.94,86.36,267.58,9.05;4,70.94,109.48,49.48,12.00;4,70.94,122.95,453.29,9.96;4,70.94,137.11,453.39,9.06;4,70.94,150.32,107.97,9.05"><head></head><label></label><figDesc>Given the computed F, we applied it on all the matching pairs to eliminate the incorrect matching pairs, where the matched point pair should satisfied with the following formula, ùëüùëáùêπùë• = 0 ùëü x‚Ä≤ are the corresponding points in two images, Fx describes a line (an epipolar line), where the other corresponding point x‚Ä≤ on the other image must lie. The results of the matching point pairs after the refining step are illustrated at Figure 2b.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,70.94,315.83,452.33,9.05;4,70.94,327.47,130.69,9.05;4,120.25,171.35,175.65,132.35"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (Left most) the initial matching map and (right most) the matching map after filtering, the points linked with lines are correctly matched.</figDesc><graphic coords="4,120.25,171.35,175.65,132.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,70.94,73.90,361.59,56.79"><head>Table 1 :</head><label>1</label><figDesc>Results of submitted 3 runs, total frame 1689.</figDesc><table coords="7,408.70,73.90,23.83,8.89"><row><cell>Score</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,92.71,285.95,431.43,9.05;7,106.94,299.15,366.12,9.05" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,326.71,285.95,197.43,9.05;7,106.94,299.15,50.13,9.05">Learning motion patterns of people for compliant robot motion</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bennewitz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cielniak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,164.09,299.15,187.08,9.05">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="48" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,92.71,312.47,216.21,9.05;7,326.11,312.47,198.32,9.05;7,106.94,325.67,247.21,9.05" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,326.11,312.47,198.32,9.05;7,106.94,325.67,45.52,9.05">Overview of the CLEF 2009 robot vision track</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pronobis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Jensfelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,158.72,325.67,104.51,9.05">CLEF working notes 2009</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,92.71,338.87,431.69,9.05;7,106.94,352.07,349.82,9.05" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,233.87,338.87,290.53,9.05;7,106.94,352.07,30.76,9.05">Efficient particle filtering using RANSAC with application to 3D face tracking</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,147.92,352.07,116.08,9.05">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="581" to="592" />
			<date type="published" when="2006-06-01">1 June 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,92.71,365.27,431.03,9.05;7,106.94,378.59,267.11,9.05" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,272.78,365.27,198.13,9.05">A new approach to estimating fundamental matrix</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">X</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">P</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,481.59,365.27,42.15,9.05;7,106.94,378.59,71.75,9.05">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="56" to="60" />
			<date type="published" when="2006-01-01">1 January 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,92.71,391.79,431.87,9.05;7,106.94,404.99,20.10,9.05" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,228.48,391.79,157.30,9.05">A combined Corner and Edge Detector</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Stephens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,397.33,391.79,72.99,9.05">Proc. Alvey Conf</title>
		<meeting>Alvey Conf</meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="189" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,92.71,418.19,388.84,9.05" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,180.59,418.19,250.24,9.05">A hybrid approach for classification based multimedia retrieval</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,440.48,418.19,29.33,9.05">SAMT&apos;</title>
		<imprint>
			<biblScope unit="volume">08</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,92.71,431.39,431.62,9.05;7,106.94,444.73,94.29,9.05" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,230.55,431.39,289.71,9.05">A shape-match based algorithm for pseudo-3D conversion of 2D videos</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">S</forename><surname>Ipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,106.94,444.73,18.83,9.05">ICIP</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="808" to="811" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,92.71,457.93,431.60,9.05;7,106.94,471.13,173.96,9.05" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,215.59,457.93,212.80,9.05">Retinex processing for automatic image enhancement</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>Jobson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,440.17,457.93,84.13,9.05;7,106.94,471.13,31.23,9.05">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="100" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
