<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,129.32,148.86,344.36,15.15;1,138.28,170.78,326.44,15.15">The University of Amsterdam&apos;s Concept Detection System at ImageCLEF 2009</title>
				<funder ref="#_FjG5ErU">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,149.65,204.67,44.27,8.74"><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Intelligent Systems Lab Amsterdam (ISLA)</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,202.37,204.67,86.06,8.74"><forename type="first">Theo</forename><surname>Van De Sande</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Intelligent Systems Lab Amsterdam (ISLA)</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,291.75,204.67,29.49,8.74;1,343.93,204.67,55.80,8.74"><forename type="first">Arnold</forename><forename type="middle">W M</forename><surname>Gevers</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Intelligent Systems Lab Amsterdam (ISLA)</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,408.99,204.67,44.36,8.74"><surname>Smeulders</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Intelligent Systems Lab Amsterdam (ISLA)</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,129.32,148.86,344.36,15.15;1,138.28,170.78,326.44,15.15">The University of Amsterdam&apos;s Concept Detection System at ImageCLEF 2009</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">99628E45A098601A3F04D9D8AD37EE84</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.4 Systems and Software</term>
					<term>I.4.7 [Image Processing and Computer Vision]: Feature Measurement Performance, Measurement, Experimentation Color, Invariance, Concept Detection, Object and Scene Recognition, Bag-of-Words, Photo Annotation, Spatial Pyramid</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our group within the University of Amsterdam participated in the large-scale visual concept detection task of ImageCLEF 2009. Our experiments focus on increasing the robustness of the individual concept detectors based on the bag-of-words approach, and less on the hierarchical nature of the concept set used. To increase the robustness of individual concept detectors, our experiments emphasize in particular the role of visual sampling, the value of color invariant features, the influence of codebook construction, and the effectiveness of kernel-based learning parameters. The participation in ImageCLEF 2009 has been successful, resulting in the top ranking for the large-scale visual concept detection task in terms of both EER and AUC. For 40 out of 53 individual concepts, we obtain the best performance of all submissions to this task. For the hierarchical evaluation, which considers the whole hierarchy of concepts instead of single detectors, using the concept likelihoods estimated by our detectors directly works better than scaling these likelihoods based on the class priors.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Robust image retrieval is highly relevant in a world that is adapting swiftly to visual communication. Online services like Flickr show that the sheer number of photos available online is too much for any human to grasp. Many people place their entire photo album on the internet. Most commercial image search engines provide access to photos based on text or other metadata, as this is still the easiest way for a user to describe an information need. The indices of these search engines are based on the filename, associated text or (social) tagging. This results in disappointing retrieval performance when the visual content is not mentioned, or properly reflected in the associated text. In addition, when the photos originate from non-English speaking countries, such as China, or the Netherlands, querying the content becomes much harder.  To cater for robust image retrieval, the promising solutions from literature are in majority concept-based <ref type="bibr" coords="2,154.30,272.61,14.61,8.74" target="#b15">[16]</ref>, where detectors are related to objects, like a telephone, scenes, like a kitchen, and people, like big group. Any one of those brings an understanding of the current content. The elements in such a lexicon offer users a semantic entry by allowing them to query on presence or absence of visual content elements.</p><p>The Large-Scale Visual Concept Detection Task <ref type="bibr" coords="2,313.25,320.43,15.50,8.74" target="#b11">[12]</ref> evaluates 53 visual concept detectors. The concepts used are from the personal photo album domain: beach holidays, snow, plants, indoor, mountains, still-life, small group of people, portrait. For more information on the dataset and concepts used, see the overview paper <ref type="bibr" coords="2,258.20,356.29,14.61,8.74" target="#b11">[12]</ref>.</p><p>Based on our previous work on concept detection <ref type="bibr" coords="2,325.53,368.25,15.50,8.74" target="#b18">[19,</ref><ref type="bibr" coords="2,344.62,368.25,11.62,8.74" target="#b14">15]</ref>, we have focused on improving the robustness of the visual features used in our concept detectors. Systems with the best performance in image retrieval <ref type="bibr" coords="2,171.77,392.16,15.50,8.74" target="#b10">[11,</ref><ref type="bibr" coords="2,191.35,392.16,12.73,8.74" target="#b18">19]</ref> and video retrieval <ref type="bibr" coords="2,295.18,392.16,15.50,8.74" target="#b21">[22,</ref><ref type="bibr" coords="2,314.76,392.16,12.73,8.74" target="#b14">15]</ref> use combinations of multiple features for concept detection. The basis for these combinations is formed by good color features and multiple point sampling strategies.</p><p>This paper is organized as follows. Section 2 defines our concept detection system. Section 3 details our experiments and results. Finally, in section 4, conclusions are drawn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Concept Detection System</head><p>We perceive concept detection as a combined computer vision and machine learning problem. Given an n-dimensional visual feature vector x i , the aim is to obtain a measure, which indicates whether semantic concept ω j is present in photo i. We may choose from various visual feature extraction methods to obtain x i , and from a variety of supervised machine learning approaches to learn the relation between ω j and x i . The supervised machine learning process is composed of two phases: training and testing. In the first phase, the optimal configuration of features is learned from the training data. In the second phase, the classifier assigns a probability p(ω j |x i ) to each input feature vector for each semantic concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sampling Strategy</head><p>The visual appearance of a concept has a strong dependency on the viewpoint under which it is recorded. Salient point methods <ref type="bibr" coords="2,233.35,636.26,15.50,8.74" target="#b16">[17]</ref> introduce robustness against viewpoint changes by selecting points, which can be recovered under different perspectives. Another solution is to simply use many points, which is achieved by dense sampling. We summarize our sampling approach in Figure <ref type="figure" coords="2,121.44,672.13,3.87,8.74" target="#fig_2">2</ref>.</p><p>Harris-Laplace point detector In order to determine salient points, Harris-Laplace relies on a Harris corner detector. By applying it on multiple scales, it is possible to select the characteristic scale of a local corner using the Laplacian operator <ref type="bibr" coords="2,311.70,721.73,14.61,8.74" target="#b16">[17]</ref>. Hence, for each corner the Harris-Laplace detector selects a scale-invariant point if the local image structure under a Laplacian operator has a stable maximum. Dense point detector For concepts with many homogenous areas, like scenes, corners are often rare. Hence, for these concepts relying on a Harris-Laplace detector can be suboptimal. To counter the shortcoming of Harris-Laplace, random and dense sampling strategies have been proposed <ref type="bibr" coords="3,133.08,317.01,10.52,8.74" target="#b3">[4,</ref><ref type="bibr" coords="3,147.57,317.01,7.01,8.74" target="#b5">6]</ref>. We employ dense sampling, which samples an image grid in a uniform fashion using a fixed pixel interval between regions. In our experiments we use an interval distance of 6 pixels and sample at multiple scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatio-temporal sampling</head><p>Spatial pyramid weighting Both Harris-Laplace and dense sampling give an equal weight to all keypoints, irrespective of their spatial location in the image frame. In order to overcome this limitation, Lazebnik et al. <ref type="bibr" coords="3,225.65,390.72,10.52,8.74" target="#b6">[7]</ref> suggest to repeatedly sample fixed subregions of an image, e.g. 1x1, 2x2, 4x4, etc., and to aggregate the different resolutions into a so called spatial pyramid, which allows for region-specific weighting. Since every region is an image in itself, the spatial pyramid can be used in combination with both the Harris-Laplace point detector and dense point sampling <ref type="bibr" coords="3,131.95,438.54,14.61,8.74" target="#b17">[18]</ref>. Reported results using concept detection experiments are not yet conclusive in the ideal spatial pyramid configuration, some claim 2x2 is sufficient <ref type="bibr" coords="3,369.95,450.50,9.96,8.74" target="#b6">[7]</ref>, others suggest to include 1x3 also <ref type="bibr" coords="3,109.98,462.45,14.61,8.74" target="#b10">[11]</ref>. We use a spatial pyramid of 1x1, 2x2, and 1x3 regions in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Visual Feature Extraction</head><p>In the previous section, we addressed the dependency of the visual appearance of semantic concepts on the viewpoint under which they are recorded. However, the lighting conditions during photography also play an important role. We <ref type="bibr" coords="3,290.93,532.64,15.50,8.74" target="#b18">[19]</ref> analyzed the properties of color features under classes of illumination changes within the diagonal model of illumination change, and specifically for data sets consisting of Flickr images. In ImageCLEF, the images used also originate from Flickr. Here we summarize the main findings. We present an overview of the visual features used in Figure <ref type="figure" coords="3,133.07,580.46,3.87,8.74" target="#fig_3">3</ref>.</p><p>The features are computed around salient points obtained from the Harris-Laplace detector and dense sampling.</p><p>SIFT The SIFT feature proposed by Lowe <ref type="bibr" coords="3,287.92,630.27,15.50,8.74" target="#b9">[10]</ref> describes the local shape of a region using edge orientation histograms. The gradient of an image is shift-invariant: taking the derivative cancels out offsets <ref type="bibr" coords="3,137.97,654.18,14.61,8.74" target="#b18">[19]</ref>. Under light intensity changes, i.e. a scaling of the intensity channel, the gradient direction and the relative gradient magnitude remain the same. Because the SIFT feature is normalized, the gradient magnitude changes have no effect on the final feature. To compute SIFT features, we use the version described by Lowe <ref type="bibr" coords="3,296.62,690.05,14.61,8.74" target="#b9">[10]</ref>.</p><p>OpponentSIFT OpponentSIFT describes all the channels in the opponent color space using SIFT features. The information in the O 3 channel is equal to the intensity information, while the other channels describe the color information in the image. The feature normalization, as effective in SIFT, cancels out any local changes in light intensity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Invariant visual descriptors</head><p>C-SIFT The C-SIFT feature uses the C invariant <ref type="bibr" coords="4,329.81,404.03,9.96,8.74" target="#b4">[5]</ref>, which can be intuitively seen as the gradient (or derivative) for the normalized opponent color space O1/I and O2/I. The I intensity channel remains unchanged. C-SIFT is known to be scale-invariant with respect to light intensity. See <ref type="bibr" coords="4,107.71,439.90,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="4,121.55,439.90,12.73,8.74" target="#b18">19]</ref> for detailed evaluation.</p><p>RGB-SIFT For the RGB-SIFT, the SIFT feature is computed for each RGB channel independently. Due to the normalizations performed within SIFT, it is equal to transformed color SIFT <ref type="bibr" coords="4,116.22,489.70,14.61,8.74" target="#b18">[19]</ref>. The feature is scale-invariant, shift-invariant, and invariant to light color changes and shift.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Codebook Transform</head><p>To avoid using all visual features in an image, while incorporating translation invariance and a robustness to noise, we follow the well known codebook approach, see e.g. <ref type="bibr" coords="4,436.69,559.89,10.52,8.74" target="#b7">[8,</ref><ref type="bibr" coords="4,451.48,559.89,7.75,8.74" target="#b5">6,</ref><ref type="bibr" coords="4,463.49,559.89,12.73,8.74" target="#b22">23,</ref><ref type="bibr" coords="4,480.50,559.89,12.73,8.74" target="#b19">20,</ref><ref type="bibr" coords="4,497.50,559.89,11.62,8.74" target="#b18">19]</ref>. First, we assign visual features to discrete codewords predefined in a codebook. Then, we use the frequency distribution of the codewords as a compact feature vector representing an image frame. Two important variables in the codebook representation are codebook construction and codeword assignment. An extensive comparison of codebook representation variables is presented by Van Gemert et al. in <ref type="bibr" coords="4,162.63,619.67,14.61,8.74" target="#b19">[20]</ref>. Here we detail codebook construction and codeword assignment using hard and soft variants, following the scheme in Figure <ref type="figure" coords="4,305.39,631.62,3.87,8.74">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Codebook construction</head><p>We employ k-means clustering. K-means partitions the visual feature space by minimizing the variance between a predefined number of k clusters. The advantage of the k-means algorithm is its simplicity. A disadvantage of k-means is its emphasis on clusters of dense areas in feature space. Hence, k-means does not spread clusters evenly throughout feature space. We fix the visual codebook to a maximum of 4000 codewords.</p><p>Hard-assignment Given a codebook of codewords, obtained from clustering, the traditional codebook approach describes each feature by the single best representative codeword in the code-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Codebook representation</head><p>Softassign</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Codebook</head><p>Hardassign Clustering Codebook library SVM Figure <ref type="figure" coords="5,121.55,265.71,3.87,8.74">4</ref>: General scheme for transforming visual features into a codebook, where we distinguish between codebook construction using clustering and codeword assignment using soft and hard variants. We combine various codeword frequency distributions into a codebook library. This then forms the input to an SVM classifier.</p><p>book, i.e. hard-assignment. Basically, an image is represented by a histogram of codeword frequencies describing the probability density over codewords.</p><p>Soft-assignment In a recent paper <ref type="bibr" coords="5,254.37,371.25,14.61,8.74" target="#b19">[20]</ref>, it is shown that the traditional codebook approach may be improved by using soft-assignment through kernel codebooks. A kernel codebook uses a kernel function to smooth the hard-assignment of image features to codewords. Out of the various forms of kernel-codebooks, we selected codeword uncertainty based on its empirical performance <ref type="bibr" coords="5,484.81,407.11,14.61,8.74" target="#b19">[20]</ref>.</p><p>Codebook library Each of the possible sampling methods from Section 2.1 coupled with each visual feature extraction method from Section 2.2, a clustering method, and an assignment approach results in a separate visual codebook. An example is a codebook based on dense sampling of RGB-SIFT features in combination with hard-assignment. We collect all possible codebook combinations in a visual codebook library. Naturally, the codebooks can be combined using various configurations. For simplicity, we employ equal weights in our experiments when combining codebooks to form a library.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Kernel-based Learning</head><p>Learning robust concept detectors from large-scale visual codebooks is typically achieved by kernelbased learning methods. From all kernel-based learning approaches on offer, the support vector machine is commonly regarded as a solid choice. An overview is given together with the codebook transformations in Figure <ref type="figure" coords="5,204.90,586.88,3.87,8.74">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Support vector machine</head><p>We use the support vector machine framework <ref type="bibr" coords="5,432.23,612.78,15.50,8.74" target="#b20">[21]</ref> for supervised learning of concepts. Here we use the LIBSVM implementation <ref type="bibr" coords="5,362.10,624.74,10.52,8.74" target="#b1">[2]</ref> with probabilistic output <ref type="bibr" coords="5,484.60,624.74,15.50,8.74" target="#b12">[13,</ref><ref type="bibr" coords="5,502.48,624.74,7.01,8.74" target="#b8">9]</ref>. The parameter of the support vector machine we optimize is C. In order to handle imbalance in the number of positive versus negative training examples, we fix the weights of the positive and negative class by estimation from the class priors on training data. It was shown by Zhang et al. <ref type="bibr" coords="5,90.00,672.56,15.50,8.74" target="#b22">[23]</ref> that in a codebook-approach to concept detection the earth movers distance and χ 2 kernel are to be preferred. We employ the χ 2 kernel, as it is less expensive in terms of computation.</p><p>3 Concept Detection Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Submitted Runs</head><p>We have submitted five different runs. All runs use both Harris-Laplace and dense sampling with the SVM classifier. We do not use the EXIF metadata provided for the photos. Our system has been developed based on the PASCAL VOC <ref type="bibr" coords="6,289.34,178.13,10.52,8.74" target="#b2">[3]</ref> and TRECVID Sound and Vision datasets <ref type="bibr" coords="6,494.73,178.13,14.61,8.74" target="#b13">[14]</ref>.</p><p>For ImageCLEF, we have learned new concept models based on the provided annotations. The only parameter specifically optimized for this dataset is the slack parameter C of the SVM. All other parameter settings are the same as in our PASCAL VOC 2008 system <ref type="bibr" coords="6,440.49,214.00,14.61,8.74" target="#b18">[19]</ref>. Extracting features, training models and applying those models on the test set was finished within 72 hours.</p><p>• OpponentSIFT: single color descriptor with hard assignment.</p><p>• 2-SIFT: two color descriptors (OpponentSIFT and SIFT) with hard assignment.</p><p>• 4-SIFT: four color descriptors (OpponentSIFT, C-SIFT, RGB-SIFT and SIFT) with hard assignment.</p><p>• Rescaled 4-SIFT: the same ordering of images as 4-SIFT, but with all concept detector outputs linearly scaled so the number of images with a score &gt; 0.5 is equal to the concept prior probability in the training set.</p><p>• Soft 4-SIFT: four color descriptors (OpponentSIFT, C-SIFT, RGB-SIFT and SIFT) with soft assignment. The soft assignment parameters have been taken from our PASCAL VOC 2008 system <ref type="bibr" coords="6,171.20,383.69,14.61,8.74" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation Per Concept</head><p>In table 1, the overall scores for the evaluation of concept detectors are shown. As for the evaluation of single detectors only the ranking of the images within a single concept matters, the rescaled version of 4-SIFT achieves the exact same performance as 4-SIFT. We note that the 4-SIFT run with hard assignment achieves not only the highest performance amongst our runs, but also over all other runs submitted to the Large-Scale Visual Concept Detection task. In table <ref type="table" coords="6,144.40,489.63,3.87,8.74" target="#tab_1">2</ref>, the Area Under the Curve scores have been split out per concept. We observe that the three aesthetic concepts have the lowest scores. This comes as no surprise, because these concepts are highly subjective: even human annotators only agree around 80% of the time with each other. For virtually all concepts besides the aesthetic ones, either the Soft 4-SIFT or the Hard 4-SIFT is the best run. This confirms our beliefs that these (color) descriptors are not redundant when used in combinations. Therefore, we recommend the use of these 4 descriptors instead of 1 or 2. The difference in overall performance between the Soft 4-SIFT or the Hard 4-SIFT run is quite small. Because the soft codebook assignment smoothing parameter was directly taken from a different dataset, we expect that the soft assignment run could be improved if the soft assignment parameter was selected with cross-validation on the training set.  . This analysis has shown us that our system is falling behind for concepts that correspond to conditions we have included invariance against. Our method is designed to be robust to unsharp images, so for Out-of-focus, Partly-Blurred and No-Blur there are better approaches possible. For the concepts Overexposed, Underexposed, Neutral-Illumination, Night and Sunny, recognizing how the scene is illuminated is very important. Because we are using invariant color descriptors, a lot of the discriminative lighting information is no longer present in the descriptors. Again, there should be better approaches possible for these concepts, such as estimating the color temperature and overall light intensity.</p><p>Our system was developed on other datasets, and only the concept models were specifically learned for the Photo Annotation dataset. Its good performance on this dataset, without changing the parameter settings, shows that it is generic and generalizes to multiple datasets. But, our system only performs well on this dataset because the train and test set come from the same source and have been obtained at the same time. Generalization across the boundary of multiple datasets is still an unsolved problem: for photos downloaded from Flickr in a different season or general web images, the performance will be significantly worse. However, all systems participating in the Photo Annotation task are 'overtrained' in this sense, and the models they learned too specific. An interesting avenue for future editions is to have a second test set with photos from a different source or moment in time, so this problem can be investigated further. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation Per Image</head><p>For the hierarchical evaluation, overall results are shown in table <ref type="table" coords="8,391.38,280.92,3.87,8.74" target="#tab_2">3</ref>. When compared to the evaluation per concept, the Soft 4-SIFT run is now slightly better than the normal 4-SIFT run. Our attempt to improve performance for the hierarchical evaluation measure using a linear rescaling of the concept likelihoods has had the opposite effect: the normal 4-SIFT run is better than the Rescaled 4-SIFT run. Therefore, further investigation into building a cascade of concept classifiers is needed, as simply using the individual concept classifiers with their class priors does not work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>Our focus on invariant visual features for concept detection in ImageCLEF 2009 has been successful. It has resulted in the top ranking for the large-scale visual concept detection task in terms of both EER and AUC. For 40 individual concepts, we obtain the best performance of all submissions to the task. For the hierarchical evaluation, using the concept likelihoods estimated by our detectors directly works better than scaling these likelihoods based on the class priors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,90.00,207.78,422.99,8.74;2,90.00,219.74,423.00,8.74;2,90.00,231.69,7.75,8.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: University of Amsterdam's ImageCLEF 2009 concept detection scheme, using the conventions shown on the right. The scheme serves as the blueprint for the organization of Section 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,90.00,237.36,423.00,8.74;3,90.00,249.32,351.24,8.74"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: General scheme for sampling of image regions, including Harris-Laplace and dense point selection, and a spatial pyramid. Detail of Figure 1, using the same conventions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,90.00,322.40,423.00,8.74;4,90.00,334.36,55.15,8.74"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: General scheme of the visual feature extraction methods used in our ImageCLEF 2009 experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,90.00,597.23,423.00,154.67"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table coords="6,453.29,597.23,59.71,8.74"><row><cell>Together, our</cell></row></table><note coords="6,129.38,719.24,383.62,8.74;6,90.00,731.20,423.00,8.74;6,90.00,743.15,58.98,8.74"><p>Overall results of the University of Amsterdam evaluated over all concepts in the Large-Scale Visual Concept Detection Task using the equal error rate (EER) and the area under the curve (AUC).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,90.00,430.34,423.00,76.43"><head>Table 2 :</head><label>2</label><figDesc>Results per concept for our runs in the Large-Scale Visual Concept Detection Task using the Area Under the Curve. The highest score per concept is highlighted using a grey background. The concepts are ordered by their highest score.runs obtain the highest Area Under the Curve scores for 40 out of 53 concepts in the Photo Annotation task (20 for Soft 4-SIFT, 17 for 4-SIFT and 3 for the other runs)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,90.00,114.01,423.00,126.21"><head>Table 3 :</head><label>3</label><figDesc>Results using the hierarchical evaluation measures for our runs in the Large-Scale Visual Concept Detection Task.</figDesc><table coords="8,325.90,114.01,114.63,8.74"><row><cell>Average Annotation Score</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was supported by the <rs type="programName">EC-FP6 VIDI-Video</rs> project.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_FjG5ErU">
					<orgName type="program" subtype="full">EC-FP6 VIDI-Video</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,110.48,552.31,402.52,8.74;8,110.48,564.27,268.25,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,297.50,552.31,211.39,8.74">Performance evaluation of local color invariants</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J</forename><surname>Burghouts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Geusebroek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,110.48,564.27,188.81,8.74">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="48" to="62" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.48,583.81,402.52,8.74;8,110.48,595.76,265.12,10.95" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="8,234.82,583.81,205.20,8.74">LIBSVM: a library for support vector machines</title>
		<author>
			<persName coords=""><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/~cjlin/libsvm" />
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.48,615.30,402.52,8.74;8,110.48,627.26,255.07,8.74" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="8,452.55,615.30,60.45,8.74;8,110.48,627.26,140.81,8.74">The PASCAL Visual Object Classes Challenge</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<pubPlace>VOC</pubPlace>
		</imprint>
	</monogr>
	<note>Results</note>
</biblStruct>

<biblStruct coords="8,110.48,646.79,402.52,8.74;8,110.48,658.75,402.53,8.74;8,110.48,670.70,99.49,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,222.01,646.79,286.86,8.74">A bayesian hierarchical model for learning natural scene categories</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,122.61,658.75,273.66,8.74">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>San Diego, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="524" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.48,690.24,402.52,8.74;8,110.48,702.20,402.53,8.74;8,110.48,714.15,48.70,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,489.17,690.24,23.83,8.74;8,110.48,702.20,42.29,8.74">Color invariance</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Geusebroek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Van Den Boomgaard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Geerts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,162.35,702.20,287.58,8.74">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1338" to="1350" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.48,733.69,402.52,8.74;8,110.48,745.64,334.51,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,213.63,733.69,215.49,8.74">Creating efficient codebooks for visual recognition</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,449.71,733.69,63.28,8.74;8,110.48,745.64,167.15,8.74">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="604" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,112.02,402.52,8.74;9,110.48,123.98,402.52,8.74;9,110.48,135.93,277.91,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,277.50,112.02,235.50,8.74;9,110.48,123.98,155.11,8.74">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,287.60,123.98,225.40,8.74;9,110.48,135.93,48.17,8.74">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,154.75,402.52,8.74;9,110.48,166.71,402.52,8.74;9,110.48,178.66,22.69,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,231.48,154.75,281.51,8.74;9,110.48,166.71,139.59,8.74">Representing and recognizing the visual appearance of materials using three-dimensional textons</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,263.09,166.71,187.23,8.74">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="44" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,197.48,402.52,8.74;9,110.48,209.44,250.38,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,287.13,197.48,225.88,8.74;9,110.48,209.44,67.60,8.74">A note on Platt&apos;s probabilistic outputs for support vector machines</title>
		<author>
			<persName coords=""><forename type="first">H.-T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">C</forename><surname>Weng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,187.21,209.44,76.69,8.74">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="267" to="276" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,228.26,402.52,8.74;9,110.48,240.21,176.68,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,166.78,228.26,244.33,8.74">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,419.60,228.26,93.40,8.74;9,110.48,240.21,84.79,8.74">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,259.04,402.52,8.74;9,110.48,270.99,402.52,8.74;9,110.48,282.95,402.52,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,392.23,259.04,120.77,8.74;9,110.48,270.99,173.01,8.74">Learning object representations for visual object class recognition</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Marsza Lek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Harzallah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,322.04,270.99,173.33,8.74;9,110.48,282.95,298.43,8.74">conjunction with IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Rio de Janeiro, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>Visual Recognition Challenge workshop</note>
</biblStruct>

<biblStruct coords="9,110.48,301.77,402.52,8.74;9,110.48,313.72,297.19,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,225.89,301.77,287.11,8.74;9,110.48,313.72,67.01,8.74">Overview of the clef 2009 large scale visual concept detection and annotation task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dunker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,198.53,313.72,112.23,8.74">CLEF working notes 2009</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,332.54,402.52,8.74;9,110.48,344.50,402.52,8.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,164.42,332.54,127.60,8.74">Probabilities for SV machines</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,216.84,344.50,158.56,8.74">Advances in Large Margin Classifiers</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="61" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,363.32,402.52,8.74;9,110.48,375.28,402.52,8.74;9,110.48,387.23,49.26,8.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,300.45,363.32,162.56,8.74">Evaluation campaigns and TRECVid</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,489.79,363.32,23.21,8.74;9,110.48,375.28,263.42,8.74">ACM International Workshop on Multimedia Information Retrieval</title>
		<meeting><address><addrLine>Santa Barbara, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="321" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,406.05,402.52,8.74;9,110.48,418.01,402.52,8.74;9,110.48,429.96,370.49,8.74" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="9,241.56,418.01,267.09,8.74">The MediaMill TRECVID 2008 semantic video search engine</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>De Rooij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Huurnink</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,122.93,429.96,190.00,8.74">Proceedings of the 6th TRECVID Workshop</title>
		<meeting>the 6th TRECVID Workshop<address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-11">November 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,448.78,402.52,8.74;9,110.48,460.74,184.58,8.74" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="9,259.28,448.78,127.94,8.74">Concept-based video retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,395.44,448.78,117.56,8.74;9,110.48,460.74,92.98,8.74">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="215" to="322" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,479.56,402.52,8.74;9,110.48,491.52,290.81,8.74" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="9,265.96,479.56,185.28,8.74">Local invariant feature detectors: A survey</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,459.91,479.56,53.09,8.74;9,110.48,491.52,198.92,8.74">Foundations and Trends in Computer Graphics and Vision</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="177" to="280" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,510.34,402.52,8.74;9,110.48,522.29,402.52,8.74;9,110.48,534.25,90.83,8.74" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="9,363.00,510.34,150.00,8.74;9,110.48,522.29,117.27,8.74">A comparison of color features for visual concept classification</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,246.30,522.29,262.55,8.74">ACM International Conference on Image and Video Retrieval</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="141" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,553.07,402.52,8.74;9,110.48,565.02,402.53,8.74;9,110.48,576.98,54.38,8.74" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="9,348.72,553.07,164.27,8.74;9,110.48,565.02,91.53,8.74">Evaluating color descriptors for object and scene recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,210.16,565.02,283.57,8.74">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct coords="9,110.48,595.80,402.52,8.74;9,110.48,607.76,402.52,8.74;9,110.48,619.71,22.69,8.74" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="9,460.95,595.80,52.05,8.74;9,110.48,607.76,40.85,8.74">Visual word ambiguity</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Veenman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Geusebroek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,164.36,607.76,294.37,8.74">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct coords="9,110.48,638.53,402.52,8.74;9,110.48,650.49,78.04,8.74" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="9,175.81,638.53,181.40,8.74">The Nature of Statistical Learning Theory</title>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York, USA</pubPlace>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct coords="9,110.48,669.31,402.52,8.74;9,110.48,681.26,402.52,8.74;9,110.48,693.22,115.85,8.74" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="9,307.80,669.31,205.20,8.74;9,110.48,681.26,32.79,8.74">Video diver: generic video indexing with diverse features</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,162.38,681.26,289.57,8.74">ACM International Workshop on Multimedia Information Retrieval</title>
		<meeting><address><addrLine>Augsburg, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="61" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,712.04,402.52,8.74;9,110.48,724.00,402.52,8.74;9,110.48,735.95,169.95,8.74" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="9,352.75,712.04,160.25,8.74;9,110.48,724.00,285.45,8.74">Local features and kernels for classification of texture and object categories: A comprehensive study</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Marsza Lek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,406.54,724.00,106.45,8.74;9,110.48,735.95,73.08,8.74">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="213" to="238" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
