<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,163.08,143.75,285.36,11.85">UAIC at ImageCLEF 2009 Photo Annotation Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,207.12,180.28,49.16,8.48"><forename type="first">Adrian</forename><surname>Iftene</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,262.20,180.28,66.94,8.48"><forename type="first">Loredana</forename><surname>Vamanu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,336.60,180.28,67.55,8.48"><forename type="first">Cosmina</forename><surname>Croitoru</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,163.08,143.75,285.36,11.85">UAIC at ImageCLEF 2009 Photo Annotation Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">036B7BBCC565D4EA8DB54C53158F5EAD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ImageCLEF</term>
					<term>Visual Concept Detection</term>
					<term>Image Annotation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The present article describes the system used for the our first participation in the imageCLEF 2009 Photo Annotation task. For the image classification we used four components: (1) first uses face recognition, (2) second one use training data, (3) third one uses associated exif file and (4) the fourth uses default values calculated according to the degree of occurrence in the training set data. The UAIC team's debut in the ImageCLEF competition has enriched us with the experience of developing the first system for the Photo Annotation task, at the same time setting the scene for subsequent ImageCLEF participations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Following the tradition from 2008, ImageCLEF 2009 offered again a visual concept detection and annotation task. In 2009, the organizers were focused on the extension of the task concerning the amount of data available and the amount of concepts annotated. In comparison with 2008 when were over 1800 images for training and 1000 images for testing available, in 2009 the training and test set consisted from thousand images from Flickr image database.</p><p>All images had multiple annotations with references to holistic visual concepts and were annotated at an image-based level. Few examples of categories for the visual concepts are: Abstract Categories (Landscape, Family&amp;Friends, Partylife, etc.), Seasons <ref type="bibr" coords="1,176.76,523.36,76.71,8.48">(Sumer, winter, etc.)</ref>, Persons (no, single, big groups), Quality <ref type="bibr" coords="1,419.88,523.36,44.31,8.48;1,142.92,534.16,63.17,8.48">(blurred, underexposed, etc.)</ref>, Representation <ref type="bibr" coords="1,274.92,534.16,137.57,8.48">(portrait, macro image, canvas, etc.)</ref>.</p><p>The visual concepts were organized in a small ontology with 53 concepts. Participants may use the hierarchical order of the concepts and the relations between concepts for solving the annotation task.</p><p>The structure of the rest of the paper is as follows: Section 2 describes our system and it main components; Section 3 describes our results. In the last Section we summarize our participation in the track and give some conclusions about the experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The UAIC system</head><p>The system built by us for this task has four main components. First component try to identify in every image people faces and after that accordingly with the number of these faces make the classification. Second one use for classification clusters built from training data and calculates for every image the minimum of distances between image and clusters. Third one uses for classification details extracted from associated exif file. If none from these components cannot performs the image classification, it is making by fourth module that uses default values calculated according to the degree of occurrence in the training set data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Face Recognition</head><p>Some of categories implied the presence of people, such as Abstract Categories (through concepts: Family Friends, PartyLife, Beach Holidays, CityLife), Activity through the concept Sports, of course Persons and Representation having one of the concepts Portrait. Finding a program that recognized faces in a photo and that could distinguish if it is a portrait or not was maybe the easiest solution that could help, in correlation with other concepts discovered with other means, to annotate the obvious concepts but also the more abstract ones.</p><p>We discovered a JAVA library, Faint 1 (developed by Malte Mathiszig, University of Oldenburg), that did exactly what we need it: recognizes if there are any faces in the photo given. Also, we can receive how many and how much percent of that picture is the face. In this way we were able to decide if there is a big group, a small one, if the photo was a portrait.</p><p>Unfortunately, if the light isn't normal for example if there is a picture taken on a black night or if there is fog or a shaken photo the result will not be accurate. After our estimations, it works well for 80% of these cases, in comparison with day pictures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Clustering using Training Data</head><p>We also used a similarity processing for finding some concepts. For this we have selected the most representing pictures for some concepts from test data and we used JAI<ref type="foot" coords="3,156.48,225.56,2.81,5.11" target="#foot_0">2</ref> (Java Advanced Imaging API) for manipulating images easily and a small program that calculates a rate of similarity between the chosen photos and the photo wanted to be annotated.</p><p>It was hard to find the most representing photos for concepts as every concept can be seen so different in different seasons, different time of day, etc; but the hardest part was to decide the acceptance rate. Using the training data, we ran for some images that we expected to be annotated with one or more of the concepts that were illustrated by the pictures in our small clusters and we notated the rates and we also made the same thing with pictures that shouldn't be annotated with one of the concepts but were very similar with the pictures chosen to compare to and we also notated the rates.</p><p>In the end, we made kind of a compromise average rate and this was our limit rate. Of course that this algorithm could be improved, it can be calculated a rate for every cluster and maybe this way the program would be more accurate. The concepts that we tried to annotate in this way were CityLife, Clouds, Snow, Desert, Sea and Snow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Exif Processing</head><p>We processed the exif information for every picture and according to the parameters of the camera with which the picture was taken we were able to annotate concepts for example related to illumination, but also correlating with other concepts found, some more abstract concepts like City Life or Landscape-Nature could also be found this way. Because the information taken from exif can or can not be accurate, and some of our limits can be subjective the concepts discovered annotated this way that were not clearly were set to 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Default values</head><p>There are five categories that contain disjoint concepts, which implies that only one concept from this kind of category can be found in a photo. Taking this into consideration, if by any other method a concept from a disjoint category was not discovered than a default value will be inserted. The default concept is selected according to the degree of occurrence in the training set data, the statistic with the occurrence values for every concept were delivered at the same time with the test data.</p><p>There are two cases: first one if there was no way in order to find one of the concepts from the category then the neutral (the "no" concept (for example: No_Visual_Time)) will be annotated and the other case is: if one of the concepts could be distinguished but with no certainty then the concept that has appeared the most in the test data will be the one annotated, as it is the most probably the one that appears in the photo.</p><p>For example for the Seasons category, if there was nothing in the picture that indicated a season then the No_Visual_Season concept will be annotated, otherwise it will be annotated with Summer as it is by far the most frequent in the test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>The time necessary to run the application for the test data was 24 hours. It took so long because of the similarities process as this supposed to compare a photo with 7 up to 20 photos for every category that was chosen for this process. The computer on which it ran had 2 GB memory, an Intel E8200 Dual Core processor with 2600 MHz and a Western Digital 320GB HDD with 10 000 rotations/min HDD.</p><p>For the annotation process it was taken into consideration the relation between categories and the hierarchical order.</p><p>We submitted only one run with the following official evaluation <ref type="bibr" coords="4,407.64,369.88,60.90,8.48;4,142.92,380.68,20.36,8.48" target="#b0">(Paramita et al., 2009)</ref>:   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>The system built by us for this task has four main components. First component try to identify in every image people faces and after that accordingly with the number of these faces make the classification. Second one use for classification clusters built from training data and calculates for every image the minimum of distances between image and clusters. Third one uses for classification details extracted from associated exif file. If none from these components cannot performs the image classification, it is making by fourth module that uses default values calculated according to the degree of occurrence in the training set data.</p><p>From our run evaluation we conclude that some of the applied rules are better than others. Thus, the second rule from rules presented above that uses clusters from training data for classification is the worst rule, and the fourth rule that uses the default values calculated according to the degree of occurrence in the training set data is the best rule.</p><p>For the future, we will perform a more detailed analysis of the results in order to identify exactly the confidence of our rules and we will try to apply rules in descending order of these values.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,264.48,410.43,82.53,7.70"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: UAIC System</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,153.60,643.76,119.71,8.48;2,372.84,295.14,45.27,7.66;2,424.08,340.86,27.16,7.66;2,403.56,373.62,32.80,7.66;2,353.16,345.42,23.92,7.66;2,354.12,385.18,28.24,7.53;2,186.48,335.94,15.99,7.66;2,173.64,346.02,41.42,7.66;2,243.72,335.94,14.57,7.66;2,232.92,346.02,36.26,7.66;2,294.96,337.38,25.51,7.66;2,295.92,347.22,23.56,7.66;2,245.76,290.31,33.62,7.60;2,226.20,389.91,47.32,7.60"><head>1</head><label></label><figDesc>Faint: http://faint.sourceforge.net</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,229.92,341.19,151.60,7.70;5,153.60,360.88,302.70,8.48"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: EER -Detailed Results on Classes Similar, our detailed results regarding AUC on classes are presented in Figure 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,228.96,581.79,153.52,7.70"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: AUC -Detailed Results on Classes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,142.92,402.82,325.67,143.78"><head>Table 1 :</head><label>1</label><figDesc>UAIC run in Photo Annotation Task</figDesc><table coords="4,142.92,418.68,325.67,127.92"><row><cell>Run ID</cell><cell cols="2">Average EER Average AUC</cell></row><row><cell>UAIC_34_2_1244812428616_changed</cell><cell>0.4797</cell><cell>0.105589</cell></row><row><cell cols="3">Our detailed results regarding EER on classes are presented in Figure 2. We can</cell></row><row><cell cols="3">see how the results are almost the same on different classes (around 0.5) with an aver-</cell></row><row><cell>age of 0.4797.</cell><cell></cell><cell></cell></row><row><cell cols="3">The better values were obtained for classes where default value rule was applied:</cell></row><row><cell cols="3">17 (No_Visual_Place) with precision 0.529716, 33 (Canvas) with precision 0.522709,</cell></row><row><cell cols="3">8 (Sports) with precision 0.518878, and 19 (Flowers) with precision 0.517818.</cell></row><row><cell cols="3">The lower values were obtained for classes where clusters from training data were</cell></row><row><cell cols="3">used: 13 (Winter) with precision 0.335529, 12 (Autumn) with precision 0.354693, 29</cell></row><row><cell>(Night) with precision 0.394617.</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="3,158.88,644.58,215.11,7.66"><p>JAI: http://java.sun.com/javase/technologies/desktop/media/jai/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The authors would like to thank the following members of the UAIC team: all students from group 1A, second year, for their help and support at different stages of system development.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="6,142.92,486.42,325.37,7.66;6,153.60,496.26,256.75,7.66" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,299.76,486.42,168.53,7.66;6,153.60,496.26,67.94,7.66">Diversity in photo retrieval: overview of the Im-ageCLEFPhoto task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Paramita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,244.92,496.26,88.28,7.66">CLEF working notes 2009</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
