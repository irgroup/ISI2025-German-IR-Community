<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,151.42,148.86,300.16,15.15;1,216.31,170.78,170.39,15.15">Overview of the wikipediaMM task at ImageCLEF 2009</title>
				<funder>
					<orgName type="full">European Union</orgName>
				</funder>
				<funder ref="#_dkEt7Hj">
					<orgName type="full">European Commission</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,218.15,204.67,80.64,8.74"><forename type="first">Theodora</forename><surname>Tsikrika</surname></persName>
							<email>theodora.tsikrika@cwi.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">CWI</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,325.95,204.67,54.44,8.74"><forename type="first">Jana</forename><surname>Kludas</surname></persName>
							<email>jana.kludas@cui.unige.ch</email>
							<affiliation key="aff1">
								<orgName type="laboratory">CUI</orgName>
								<orgName type="institution">University of Geneva</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,151.42,148.86,300.16,15.15;1,216.31,170.78,170.39,15.15">Overview of the wikipediaMM task at ImageCLEF 2009</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CA12E1C4968CD04B7D586EBF54AC595F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ImageCLEF</term>
					<term>Wikipedia image collection</term>
					<term>image retrieval</term>
					<term>evaluation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ImageCLEF's wikipediaMM task provides a testbed for the system-oriented evaluation of multimedia information retrieval from a collection of Wikipedia images. The aim is to investigate retrieval approaches in the context of a large and heterogeneous collection of images (similar to those encountered on the Web) that are searched for by users with diverse information needs. This paper presents an overview of the resources, topics, and assessments of the wikipediaMM task at ImageCLEF 2009, summarises the retrieval approaches employed by the participating groups, and provides a first analysis of the main evaluation results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The wikipediaMM task is an ad-hoc image retrieval task. The evaluation scenario is thereby similar to the classic TREC ad-hoc retrieval task and the ImageCLEF photo retrieval task: simulation of the situation in which a system knows the set of documents to be searched, but cannot anticipate the particular topic that will be investigated (i.e. topics are not known to the system in advance). Given a multimedia query that consists of a title and one or more sample images describing a user's multimedia information need, the aim is to find as many relevant images as possible from the (INEX MM) wikipedia image collection. A multi-modal retrieval approach in that case should be able to combine the relevance of different media types into a single ranking that is presented to the user.</p><p>The wikipediaMM task differs from other benchmarks in multimedia information retrieval, like TRECVID, in the sense that the textual modality in the wikipedia image collection contains less noise than the speech transcripts in TRECVID. Maybe that is one of the reasons why, both in last year's task and in <ref type="bibr" coords="1,187.72,608.71,100.50,8.74">INEX Multimedia 2006</ref><ref type="bibr" coords="1,288.22,608.71,23.98,8.74" target="#b3">-2007</ref> (where this image collection was also used), it has proven challenging to outperform the text-only approaches. This year, the aim is to promote the investigation of multi-modal approaches to the forefront of this task by providing a number of resources to support the participants towards this research direction.</p><p>The paper is organised as follows. First, we introduce the task's resources: the wikipedia image collection and additional resources, the topics, and the assessments (Sections 2-4). Section 5 presents the approaches employed by the participating groups and Section 6 summarises their main results. Section 7 concludes the paper.</p><p>The resources used for the wikipediaMM task are based on Wikipedia data. The collection is the (INEX MM) wikipedia image collection, which consists of approximately 150,000 JPEG and PNG Wikipedia images provided by Wikipedia users. Each image is associated with usergenerated alphanumeric, unstructured metadata in English. These metadata usually contain a brief caption or description of the image, the Wikipedia user who uploaded the image, and the copyright information. These descriptions are highly heterogeneous and of varying length. Further information about the image collection can be found in <ref type="bibr" coords="2,334.11,205.57,9.97,8.74" target="#b3">[4]</ref>. Additional resources were also provided to support the participants in their investigations of multi-modal approaches. These resources are:</p><p>Image similarity matrix: The similarity matrix for the images in the collection has been constructed by the IMEDIA group at INRIA. For each image in the collection, this matrix contains the list of the top K = 1000 most similar images in the collection together with their similarity scores. The same is given for each image in the topics. The similarity scores are based on the distance between images; therefore, the lower the score, the more similar the images. Further details on the features and distance metric used can be found in <ref type="bibr" coords="2,488.20,610.64,9.97,8.74" target="#b0">[1]</ref>.</p><p>Image classification scores: For each image, the classification scores for the 101 MediaMill concepts have been provided by UvA <ref type="bibr" coords="2,273.91,642.53,9.96,8.74" target="#b2">[3]</ref>. The UvA classifier is trained on manually annotated TRECVID video data for concepts selected for the broadcast news domain.</p><p>Image features: For each image, the set of the 120D feature vectors that has been used to derive the above image classification scores <ref type="bibr" coords="2,309.06,686.36,10.52,8.74" target="#b1">[2]</ref> has also been made available. Participants can use these feature vectors to custom-build a content-based image retrieval (CBIR) system, without having to pre-process the image collection.</p><p>The additional resources are beneficial to researchers who wish to exploit visual evidence without performing image analysis. Of course, participants could also extract their own image features.</p><p>The topics are descriptions of multimedia information needs that contain textual and visual hints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Topic Format</head><p>These multimedia queries consist of a textual part, the query title, and a visual part, one or several example images.</p><p>&lt;title&gt; query by keywords &lt;image&gt; query by image content (one or several) &lt;narrative&gt; description of query in which the definitive definition of relevance and irrelevance are given</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">&lt;title&gt;</head><p>The topic &lt;title&gt; simulates a user who does not have (or want to use) example images or other visual constraints. The query expressed in the topic &lt;title&gt; is therefore a text-only query. This profile is likely to fit most users searching digital libraries.</p><p>Upon discovering that a text-only query does not produce many relevant hits, a user might decide to add visual hints and formulate a multimedia query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">&lt;image&gt;</head><p>The visual hints are example images, which can be taken from outside or inside the wikipedia image collection and can be of any common format. Each topic has at least one example image, but it can have several, e.g., to describe the visual diversity of the topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">&lt;narrative&gt;</head><p>A clear and precise description of the information need is required in order to unambiguously determine whether or not a given document fulfils the given information need. In a test collection this description is known as the narrative. It is the only true and accurate interpretation of a user's needs. Precise recording of the narrative is important for scientific repeatability -there must exist, somewhere, a definitive description of what is and is not relevant to the user. To aid this, the &lt;narrative&gt; should explain not only what information is being sought, but also the context and motivation of the information need, i.e., why the information is being sought and what work-task it might help to solve. These different types of information sources (textual terms and visual examples) can be used in any combination. It is up to the systems how to use, combine or ignore this information; the relevance of a result does not directly depend on these constraints, but it is decided by manual assessments based on the &lt;narrative&gt;.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Topic Development</head><p>The topics in the ImageCLEF 2009 wikipediaMM task have been partly developed by the participants and partly by the organisers. This year the participation in the topic development process was not obligatory, so only 2 of the participating groups submitted a total of 11 candidate topics. The rest of the candidate topics were created by the organisers with the help of the log of an image search engine. After a selection process performed by the organisers, a final list of 45 topics was created.</p><p>These final topics range from simple, and thus relatively easy (e.g., "bikes"), to semantic, and hence highly difficult (e.g., "aerial photos of non-artificial landscapes"), with the latter forming the bulk of the topics. Semantic topics typically have a complex set of constraints, need world knowledge, and/or contain ambiguous terms, so they are expected to be challenging for current state-of-the-art retrieval algorithms. We encouraged the participants to use multi-modal approaches since they are more appropriate for dealing with semantic information needs. On average, the 45 topics contain 1.7 images and 2.7 words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Assessments</head><p>The wikipediaMM task is an image retrieval task, where an image with its metadata is either relevant or not (binary relevance). We adopted TREC-style pooling of the retrieved images with a pool depth of 50, resulting in pools of between 299 and 802 images with a mean and median both around 545. The evaluation was performed by the participants of the task within a period of 4 weeks after the submission of runs. The 7 groups that participated in the evaluation process used the web-based interface that was used last year and which has also been previously employed in the INEX Multimedia and TREC Enterprise tracks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Participants</head><p>A total of 8 groups submitted 57 runs: CEA (LIC2M-CEA, Centre CEA de Saclay, France), DCU (Dublin City University, School of Computing, Ireland), DEU (Dokuz Eylul University, Department of Computer Engineering, Turkey), IIIT-Hyderabad (Search and Info Extraction Lab, India), LaHC (Laboratoire Hubert Curien, UMR CNRS, France), SZTAKI (Hungarian Academy of Science, Hungary), SINAI (Intelligent Systems, University of Jaen, Spain) and UALICANTE (Software and Computer Systems, University of Alicante, Spain).  <ref type="table" coords="4,133.17,556.50,4.98,8.74" target="#tab_0">1</ref> gives an overview of the types of the submitted runs. This year more multi-modal (text/visual) than text-only runs were submitted. A short description of the participants' approaches follows.</p><p>CEA (12 runs) They extended the approach they employed last year by refining the textual query expansion procedure and introducing of a k-NN based visual reranking procedure.</p><p>Their main aim was to examine whether combining textual and content-based retrieval improves over purely textual search.</p><p>DCU (5 runs) Their main effort concerned the expansion of the image metadata using the Wikipedia abstracts' collection DBpedia. Since the metadata is short for retrieval by query text, they expand the query and documents using the Rocchio algorithm. For retrieval, they used the LEMUR toolkit. They also submitted one visual run.</p><p>DEU (6 runs) Their research interests focussed on 1) the expansion of native documents and queries, term phrase selection based on WordNet, WSD and WordNet similarity functions and 2) a new reranking approach with Boolean retrieval and C3M based clustering.</p><p>IIT-H (1 run) Their system automatically ranks the most similar images to a given textual query using a combination of the Vector Space Model and the Boolean model. The system preprocesses the data set in order to remove the non-informative terms.</p><p>LaHC (13 runs) In this second participation, they extended their approach (a multimedia document model defined as a vector of textual and visual terms weighted using tf.idf) by using 1) additional information for the textual part (legend and image bounding text extracted from the original documents), 2) different image detectors and descriptors, and 3) a new text/image combination approach.</p><p>SINAI (4 runs) Their approach focussed on query and document expansion techniques based on WordNet. They used the LEMUR toolkit as their retrieval system. SZTAKI (7 runs) They used both textual and visual features and employed image segmentation, SIFT keypoints, Okapi BM25 based text retrieval, and query expansion by an online thesaurus. They preprocessed the annotation text to remove author and copyright information and biased retrieval towards images with filenames containing relevant terms.</p><p>UALICANTE (9 runs) They used IR-n, a retrieval system based on passages and applied two different term selection strategies for query expansion: Probabilistic Relevance Feedback and Local Context Analysis, and their multi-modal versions. They also used the same technique for Camel Case decompounding of image filenames that they used in last year's participation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>Table <ref type="table" coords="5,117.58,401.91,4.98,8.74" target="#tab_2">2</ref> presents the evaluation results for the 15 best performing runs ranked by Mean Average Precision (MAP). DEU's text-only runs performed best. But as already seen last year, approaches that fuse several modalities can compete with the text-only ones. Furthermore, it is notable that all participants that used both mono-media and multi-modal algorithms achieved their best results with their multi-modal runs. The complete list of results can be found at the ImageCLEF website http://www.imageclef.org/2009/wikiMM-results. Next, we analyse the evaluation results. In our analysis, we use only the top 90% of the runs to exclude noisy and buggy results. Furthermore, we excluded 3 runs that we considered to be redundant, i.e., they were produced by the same group and achieved the exact same result, so as to reduce the bias of the analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Performance per modality for all topics</head><p>Table <ref type="table" coords="5,118.54,731.32,4.98,8.74" target="#tab_3">3</ref> shows the average performance and standard deviation with respect to modality. On average, the multi-modal runs manage to outperform the mono-media runs with respect to all examined evaluation metrics (MAP, Precison at 20, and precision after R (= number of relevant) documents are retrieved). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Performance per topic and per modality</head><p>To analyse the average difficulty of the topics, we classify the topics based on the average MAP values per topic as follows:</p><formula xml:id="formula_0" coords="6,90.00,309.22,139.47,66.67">easy: aM AP &gt; 0.3 medium: 0.2 &lt; aM AP &lt;= 0.3 hard: 0.1 &lt; aM AP &lt;= 0.2 very hard: aM AP &lt; 0.1.</formula><p>Table <ref type="table" coords="6,132.35,387.51,4.98,8.74" target="#tab_4">4</ref> presents the top 6 topics per class (i.e., easy, medium, hard, and very hard), together with the total number of topics per class. Most of the topics are considered to be hard. This was actually intended during the topic development process where we opted for highly semantic topics that are challenging for current retrieval approaches. Nonetheless, 10 out of 45 topics were of easy and medium difficulty. Only 7 topics were very hard to solve. Therein, topics #97 "woman in pink dress" and #98 "close up of people doing sport" can be considered as unsolvable, since their aM AP &lt; 0.05. We also analysed the performance of runs that use only text (TXT) versus text and visual resources (TXTIMG). Figure <ref type="figure" coords="6,223.96,603.99,4.98,8.74" target="#fig_1">2</ref> shows the average performance on each topic for all runs, the text-only and text-visual based ones. The text-based runs outperform the text-visual ones in 22 out of the 45. So, slightly more than half of the topics profit from a multi-modal approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Visuality of topics</head><p>The "visuality" of topics can be deduced from the performance of text-only and text-visual approaches that we presented in the last section. We consider that if, for a topic, the text-visual approaches improve significantly the MAP over all runs (e.g., by dif f (M AP ) &gt;= 0.01), then we could consider that to be a visual topic. In the same way, we can define topics as textual, if the text-only approaches improve significantly the MAP over all runs of a topic. Based on this, 15 of the topics can be characterised as textual and 14 as visual. The remaining 16 topics, where no clear improvements are observed, are considered to be neutral.  <ref type="table" coords="7,132.41,299.18,4.98,8.74" target="#tab_5">5</ref> presents the top 10 topics in each group, as well as some statistics on the topic, their relevant documents, and their distribution over the difficulty. As expected, visual topics have more image examples per topic (#images/topic) than textual ones (1.66 vs. 1.85); however, the neutral topics have an even higher average of 2.06 images per topic. The same tendency is observed in the average number of words in the topics (#words/topic). Short titled topics are better solved with text-only approaches, topics with longer titles tend to be visual or neutral. Therefore, it appears that the latter two groups contain the more complex/semantic topics. The distribution of the textual, visual and neutral topics over the classes expressing their difficulty shows that the visual topics are more likely to fall into the easy/medium class than the textual or neutral ones. The neutral topics seem to contain in general very difficult topics, where neither the text-only approaches nor the text-visual ones could achieve good retrieval results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Effect of Query Expansion and Relevance Feedback</head><p>Finally, we analyse the effect of the application of query expansion (QE) and relevance feedback (FB) techniques. Similarly to the analysis in the previous section, we consider the techniques to be useful for a topic, if they improved significantly the MAP over all runs. Table <ref type="table" coords="7,451.64,705.91,4.98,8.74" target="#tab_6">6</ref> presents the top 10 best performing topics for these techniques and some statistics. Query expansion is useful for 17 topics and relevance feedback for 11. The statistics show that these techniques can help improve the retrieval results for topics defined without too much detail, e.g., topics having a short title (#words/topic) and/or a small number of example images (#images/topic). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>This year (similarly to 2008), a text-based approach performed best in the wikipediaMM task, even though highly semantic multimedia topics were developed with the aim to encourage and show the potential of multi-modal approaches. It is worth noting though that all of the participants that submitted both mono-media and multi-modal runs achieved their best results with their multimodal runs. Additionally, we as organisers are really glad to see more than half of the submitted runs being multi-modal.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,118.48,493.98,366.05,8.74;2,96.38,227.22,423.02,250.25"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Wikipedia image+metadata example from the wikipedia image collection.</figDesc><graphic coords="2,96.38,227.22,423.02,250.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,134.56,265.71,333.88,8.74;7,131.42,108.86,340.16,141.73"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Average topic performance over all, text-only and text/visual runs</figDesc><graphic coords="7,131.42,108.86,340.16,141.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,211.60,457.57,179.80,82.26"><head>Table 1 :</head><label>1</label><figDesc>Types of the 57 submitted runs.</figDesc><table coords="4,237.71,470.49,127.58,69.34"><row><cell>run type</cell><cell># runs</cell></row><row><cell>text</cell><cell>26</cell></row><row><cell>visual</cell><cell>2</cell></row><row><cell>text/visual</cell><cell>29</cell></row><row><cell>query expansion</cell><cell>18</cell></row><row><cell>relevance feedback</cell><cell>7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,104.94,556.50,24.07,8.74"><head>Table</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,104.02,494.18,394.97,138.67"><head>Table 2 :</head><label>2</label><figDesc>results for the top 15 runs</figDesc><table coords="5,104.02,506.78,394.97,126.07"><row><cell></cell><cell>Participant</cell><cell>Run</cell><cell>Modality</cell><cell>FB/QE</cell><cell>MAP</cell><cell>P@10</cell><cell>P@20</cell><cell>R-prec.</cell></row><row><cell>1</cell><cell>deuceng</cell><cell>deuwiki2009 205</cell><cell>TXT</cell><cell>QE</cell><cell>0.2397</cell><cell>0.4000</cell><cell>0.3133</cell><cell>0.2683</cell></row><row><cell>2</cell><cell>deuceng</cell><cell>deuwiki2009 204</cell><cell>TXT</cell><cell>QE</cell><cell>0.2375</cell><cell>0.4000</cell><cell>0.3111</cell><cell>0.2692</cell></row><row><cell>3</cell><cell>deuceng</cell><cell>deuwiki2009 202</cell><cell>TXT</cell><cell>QE</cell><cell>0.2358</cell><cell>0.3933</cell><cell>0.3189</cell><cell>0.2708</cell></row><row><cell>4</cell><cell>lach</cell><cell>TXTIMG 100 3 1 5 meanstd</cell><cell>TXTIMG</cell><cell>NOFB</cell><cell>0.2178</cell><cell>0.3378</cell><cell>0.2811</cell><cell>0.2538</cell></row><row><cell>5</cell><cell>lach</cell><cell>TXTIMG 50 3 1 5 meanstd</cell><cell>TXTIMG</cell><cell>NOFB</cell><cell>0.2148</cell><cell>0.3356</cell><cell>0.2867</cell><cell>0.2536</cell></row><row><cell>6</cell><cell>cea</cell><cell>cealateblock</cell><cell>TXTIMG</cell><cell>QE</cell><cell>0.2051</cell><cell>0.3622</cell><cell>0.2744</cell><cell>0.2388</cell></row><row><cell>7</cell><cell>cea</cell><cell>ceaearlyblock</cell><cell>TXTIMG</cell><cell>QE</cell><cell>0.2046</cell><cell>0.3556</cell><cell>0.2833</cell><cell>0.2439</cell></row><row><cell>8</cell><cell>cea</cell><cell>ceabofblock</cell><cell>TXTIMG</cell><cell>QE</cell><cell>0.1975</cell><cell>0.3689</cell><cell>0.2789</cell><cell>0.2342</cell></row><row><cell>9</cell><cell>cea</cell><cell>ceatlepblock</cell><cell>TXTIMG</cell><cell>QE</cell><cell>0.1959</cell><cell>0.3467</cell><cell>0.2733</cell><cell>0.2236</cell></row><row><cell>10</cell><cell>cea</cell><cell>ceabofblockres</cell><cell>TXTIMG</cell><cell>QE</cell><cell>0.1949</cell><cell>0.3689</cell><cell>0.2789</cell><cell>0.2357</cell></row><row><cell>11</cell><cell>cea</cell><cell>ceatlepblockres</cell><cell>TXTIMG</cell><cell>QE</cell><cell>0.1934</cell><cell>0.3467</cell><cell>0.2733</cell><cell>0.2236</cell></row><row><cell>12</cell><cell>lach</cell><cell>TXTIMG Siftdense 0.084</cell><cell>TXTIMG</cell><cell>NOFB</cell><cell>0.1903</cell><cell>0.3111</cell><cell>0.2700</cell><cell>0.2324</cell></row><row><cell>13</cell><cell>lach</cell><cell>TXT 100 3 1 5</cell><cell>TXT</cell><cell>NOFB</cell><cell>0.1890</cell><cell>0.2956</cell><cell>0.2544</cell><cell>0.2179</cell></row><row><cell>14</cell><cell>lach</cell><cell>TXT 50 3 1 5</cell><cell>TXT</cell><cell>NOFB</cell><cell>0.1880</cell><cell>0.3000</cell><cell>0.2489</cell><cell>0.2145</cell></row><row><cell>15</cell><cell>ualicante</cell><cell>Alicante-MMLCA</cell><cell>TXTIMG</cell><cell>FB</cell><cell>0.1878</cell><cell>0.2733</cell><cell>0.2478</cell><cell>0.2138</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,102.01,156.13,398.98,70.71"><head>Table 3 :</head><label>3</label><figDesc>Results per modality over all topics.</figDesc><table coords="6,102.01,169.48,398.98,57.36"><row><cell>Modality</cell><cell>MAP Mean</cell><cell>SD</cell><cell>P@20 Mean</cell><cell>SD</cell><cell>R-prec. Mean SD</cell></row><row><cell>All top 90% runs (46 runs)</cell><cell cols="5">0.1751 0.0302 0.2356 0.0624 0.2076 0.0572</cell></row><row><cell>TXT in top 90% runs (23 runs)</cell><cell cols="5">0.1726 0.0326 0.2278 0.0427 0.2038 0.0328</cell></row><row><cell cols="6">TXTIMG in top 90% runs (23 runs) 0.1775 0.0281 0.2433 0.0364 0.2115 0.0307</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,90.00,489.30,423.01,85.21"><head>Table 4 :</head><label>4</label><figDesc>Topics classified based on their difficulty. The top 6 topics are shown per class together with the total number of topics per class.</figDesc><table coords="6,107.01,511.80,388.98,62.70"><row><cell>easy</cell><cell>medium</cell><cell>hard</cell><cell>very hard</cell></row><row><cell>(112) hot air balloons</cell><cell>(118) coral reef underwater</cell><cell>(120) yellow flower</cell><cell>(105) snowy street</cell></row><row><cell>(88) madonna portrait</cell><cell>(90) satellite image of river</cell><cell>(91) landline telephone</cell><cell>(78) sculpture of an animal</cell></row><row><cell>(80) orthodox icons</cell><cell>(110) desert landscape</cell><cell>(99) flowers on trees</cell><cell>(117) earth from space</cell></row><row><cell>(108) bird nest</cell><cell>(77) real rainbow</cell><cell>(79) stamp human face</cell><cell>(85) aerial ph. of landscapes</cell></row><row><cell>(103) palm trees</cell><cell></cell><cell>(107) red fruit</cell><cell>(89) people laughing</cell></row><row><cell>(93) close up antenna</cell><cell></cell><cell>(94) people with dogs</cell><cell>(97) woman in pink dress</cell></row><row><cell>6</cell><cell>4</cell><cell>28</cell><cell>7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,90.00,450.96,423.00,180.11"><head>Table 5 :</head><label>5</label><figDesc>Top 10 best performing topics for textual and text-visual runs relative to the average over all runs.</figDesc><table coords="7,106.35,471.53,390.30,159.54"><row><cell></cell><cell>textual</cell><cell>visual</cell><cell>neutral</cell></row><row><cell>topics</cell><cell>(83) advertisement for cars</cell><cell>(115) notes on music sheet</cell><cell>(76) shopping on a market</cell></row><row><cell></cell><cell>(102) building site</cell><cell>(90) satellite image of river</cell><cell>(77) real rainbow</cell></row><row><cell></cell><cell>(94) people with dogs</cell><cell>(118) coral reef underwater</cell><cell>(78) sculpture of an animal</cell></row><row><cell></cell><cell>(92) bikes</cell><cell>(110) desert landscape</cell><cell>(79) stamp without human face</cell></row><row><cell></cell><cell>(95) group of dogs</cell><cell>(120) yellow flower</cell><cell>(81) Greek mythological figures</cell></row><row><cell></cell><cell>(99) flowers on trees</cell><cell>(86) situation after katrina</cell><cell>(82) rider on horse</cell></row><row><cell></cell><cell>(111) pol. campaign poster</cell><cell>(87) airplane crash</cell><cell>(84) advertisement on buses</cell></row><row><cell></cell><cell>(103) palm trees</cell><cell>(117) earth from space</cell><cell>(101) fire</cell></row><row><cell></cell><cell>(96) cartoon with a cat</cell><cell>(88) madonna portrait</cell><cell>(104) street musician</cell></row><row><cell></cell><cell>(119) harbor</cell><cell>(93) close up of antenna</cell><cell>(105) snowy street</cell></row><row><cell># out of 45</cell><cell>15</cell><cell>14</cell><cell>16</cell></row><row><cell>#images/topic</cell><cell>1.66</cell><cell>1.85</cell><cell>2.06</cell></row><row><cell>#words/topic</cell><cell>2.53</cell><cell>3.00</cell><cell>3.31</cell></row><row><cell>#reldocs</cell><cell>35.33</cell><cell>36.28</cell><cell>36.50</cell></row><row><cell>#words/reldocs</cell><cell>29.65</cell><cell>44.99</cell><cell>39.24</cell></row><row><cell>easy</cell><cell>2</cell><cell>3</cell><cell>1</cell></row><row><cell>medium</cell><cell>0</cell><cell>3</cell><cell>1</cell></row><row><cell>hard</cell><cell>12</cell><cell>7</cell><cell>9</cell></row><row><cell>very hard</cell><cell>1</cell><cell>1</cell><cell>5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="8,90.00,143.59,423.00,180.11"><head>Table 6 :</head><label>6</label><figDesc>Top 10 best performing topics for textual and text-visual runs relative to the average over all runs.</figDesc><table coords="8,158.45,164.16,286.09,159.54"><row><cell></cell><cell>QE</cell><cell>FB</cell></row><row><cell>topics</cell><cell>(110) desert landscape</cell><cell>(88) madonna portrait</cell></row><row><cell></cell><cell>(118) coral reef underwater</cell><cell>(115) notes on music sheet</cell></row><row><cell></cell><cell>(120) yellow flower</cell><cell>(87) airplane crash</cell></row><row><cell></cell><cell>(109) tennis player on court</cell><cell>(93) close up of antenna</cell></row><row><cell></cell><cell>(92) bikes</cell><cell>(96) cartoon with a cat</cell></row><row><cell></cell><cell>(82) rider on horse</cell><cell>(79) stamp without human face</cell></row><row><cell></cell><cell>(101) fire</cell><cell>(116) illustration of engines</cell></row><row><cell></cell><cell>(115) notes on music sheet</cell><cell>(118) coral reef underwater</cell></row><row><cell></cell><cell>(117) earth from space</cell><cell>(95) group of dogs</cell></row><row><cell></cell><cell>(119) harbor</cell><cell>(104) street musician</cell></row><row><cell># out of 45</cell><cell>17</cell><cell>11</cell></row><row><cell>#images/topic</cell><cell>1.94</cell><cell>1.72</cell></row><row><cell>#words/topic</cell><cell>2.76</cell><cell>3.18</cell></row><row><cell>#reldocs</cell><cell>46.47</cell><cell>40.36</cell></row><row><cell>#words/reldocs</cell><cell>37.98</cell><cell>42.74</cell></row><row><cell>easy</cell><cell>1</cell><cell>2</cell></row><row><cell>medium</cell><cell>2</cell><cell>1</cell></row><row><cell>hard</cell><cell>11</cell><cell>8</cell></row><row><cell>very hard</cell><cell>3</cell><cell>0</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8">Acknowledgements</head><p><rs type="person">Theodora Tsikrika</rs> was supported by the <rs type="funder">European Union</rs> via the <rs type="funder">European Commission</rs> project <rs type="projectName">VITALAS</rs> (contract no. <rs type="grantNumber">045389</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_dkEt7Hj">
					<idno type="grant-number">045389</idno>
					<orgName type="project" subtype="full">VITALAS</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,105.50,561.06,407.50,8.74;8,105.50,573.02,321.01,8.74" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="8,178.07,561.06,334.93,8.74;8,105.50,573.02,73.55,8.74">Image retrieval with active relevance feedback using both visual and keywordbased descriptors</title>
		<author>
			<persName coords=""><forename type="first">Marin</forename><surname>Ferecatu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<pubPlace>France</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Universit de Versailles</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

<biblStruct coords="8,105.50,591.73,407.51,8.74;8,105.50,603.69,407.51,8.74;8,105.50,615.64,407.51,8.74;8,105.50,627.60,282.92,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,193.91,603.69,298.01,8.74">Robust scene categorization by learning image statistics in context</title>
		<author>
			<persName coords=""><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jan-Mark</forename><surname>Geusebroek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cor</forename><forename type="middle">J</forename><surname>Veenman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arnold</forename><forename type="middle">W M</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,105.50,615.64,402.40,8.74">Proceedings of the 2006 Conference on Computer Vision and Pattern Recognition Workshop</title>
		<meeting>the 2006 Conference on Computer Vision and Pattern Recognition Workshop<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page">105</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,646.32,407.51,8.74;8,105.50,658.27,407.51,8.74;8,105.50,670.23,407.51,8.74;8,105.50,682.18,243.61,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,187.09,658.27,325.91,8.74;8,105.50,670.23,47.05,8.74">The challenge problem for automated detection of 101 semantic concepts in multimedia</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marcel</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jan-Mark</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arnold</forename><forename type="middle">W M</forename><surname>Geusebroek</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,174.69,670.23,333.64,8.74">Proceedings of the 14th annual ACM international conference on Multimedia</title>
		<meeting>the 14th annual ACM international conference on Multimedia<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="421" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,700.90,407.50,8.74;8,105.50,712.85,407.50,8.74;8,105.50,724.81,407.51,8.74;8,105.50,736.76,302.42,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,282.17,700.90,146.90,8.74">The INEX 2006 multimedia track</title>
		<author>
			<persName coords=""><forename type="first">Thijs</forename><surname>Westerveld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roelof</forename><surname>Van Zwol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,315.46,712.85,197.54,8.74;8,105.50,724.81,402.82,8.74">Advances in XML Information Retrieval: 5th International Workshop of the Initiative for the Evaluation of XML Retrieval, INEX 2006</title>
		<title level="s" coord="8,105.50,736.76,100.69,8.74">Revised Selected Papers</title>
		<editor>
			<persName><forename type="first">Norbert</forename><surname>Fuhr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mounia</forename><surname>Lalmas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andrew</forename><surname>Trotman</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4518</biblScope>
			<biblScope unit="page" from="331" to="344" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
