<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,220.26,92.93,171.50,11.75">UNT at ImageCLEFmed 2009</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,275.52,134.23,61.11,8.72"><forename type="first">Miguel</forename><forename type="middle">E</forename><surname>Ruiz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Information Department of Library and Information Sciences</orgName>
								<orgName type="institution">University of North Texas</orgName>
								<address>
									<addrLine>1155 Union Circle</addrLine>
									<postCode>311068, 76203-1068</postCode>
									<settlement>Denton</settlement>
									<region>Texas</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,220.26,92.93,171.50,11.75">UNT at ImageCLEFmed 2009</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">54A546C6D45BEEE3271CD0487982650A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For this year our team participated in the medical image retrieval task. Most of our effort was invested in processing the collection using Metamap to assign Unified Medical Language System (UMLS) concepts to each of the images that included some associated text. This process generated metadata that was added to each image and included the UMLS concept number as well as the primary terms associated to each concept. Queries were also processed using Metamap to generate the corresponding UMLS concepts and terms associated with each query request. The SMART system was used to perform retrieval using a generalized vector space model that included the original text, the automatically assigned UMLS concepts, and the UMLS terms. We use a simple weighting scheme (tf-idf) to perform retrieval. Our text based runs included a simple run and a retrieval feedback run. The parameters for retrieval feedback and for the linear combination of the generalized vector space model were tuned using queries for the 2008 CLEF medical image retrieval task (imageCLEFmed). We also worked on using the results from the open source content-based image retrieval (CBIR) system GIFT but ran into some technical problems that prevented us to generate the retrieval results on time for the deadline. However, the University of Geneva (UG) team allowed us to use one of their Image results. The mixed results were generated using the GIFT run provided by UG and used a standard fusion mechanism by combining the text and CBIR results into a single list. To tune the parameters for the combination we used the results from the imageclefmed 2008 queries. Our results indicate that the pseudo relevance feedback mechanism yields only small improvements. The Combination of image features and text gave mixed results. While the combination of standard retrieval and CBIR yields small improvements, the combination of retrieval feedback and CBIR resulted in results significantly below using only text. At this point we still are investigating the reasons for this unexpected result.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Retrieval of medical images from medical related literature is been used more and more frequently by physicians and other health related professions and seems to be a promising application for content based image retrieval systems. Our research for the CLEF medical image retrieval track consisted in evaluating the contribution of automatically generated UMLS terms and its impact in retrieval performance of images with caption or other associated text. We also explored the use of automatically extracted image features such as the ones generated by content-based image retrieval systems. We used the SMART retrieval system <ref type="bibr" coords="1,292.02,666.25,59.03,8.76" target="#b3">(Salton, 1971)</ref> for handling the text retrieval part of this task and the GNU Image Finding Tool (GIFT) for performing content-based image retrieval . We also use Metamap <ref type="bibr" coords="1,152.41,691.15,65.59,8.76" target="#b0">(Aronson, 2001)</ref>, which the an open source tool developed by the National Library of medicine, for automatic identification of medical terms compiled in the Unified Medical Language System (UMLS).</p><p>Section 1 of this paper presents a brief description of the systems used. Section 2 describes the way in which the text, images and queries were processed. Section 3 describes the runs that were submitted and section 4 presents our results and analysis. Finally section 5 presents our conclusions and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">System Description</head><p>The system used in these experiments combines three independent open source tools that have been extensively used in the literature. The SMART system, which was developed by G. Salton and his collaborators at Cornell University, is an information retrieval system that uses the vector space model to represent queries and documents and perform retrieval using a variety of weighting schemes <ref type="bibr" coords="2,92.70,252.31,57.24,8.76" target="#b3">(Salton, 1971)</ref>. The version that we use was modified by Miguel Ruiz and currently supports 13 European languages using the ISO-Latin-I encoding. Smart also allows the use of multiple indexes that can be combined using a linear model into what can be considered a generalized vector space model <ref type="bibr" coords="2,120.93,289.63,99.28,8.76" target="#b2">(Ruiz &amp; Southwick, 2006</ref>). As will be explained in more detail in the next section, documents and queries are represented using three different types of terms: Free text, UMLS concepts, and the terms from the UMLS vocabulary. Since each of these three types of terms have very different origin (one is author generated while the other two are automatically assigned), we decided to keep them in four separate indexes (called ctypes in SMART) and use the following linear model to compute similarity:</p><formula xml:id="formula_0" coords="2,102.18,371.01,408.04,15.96">) , ( ) , ( ) , ( ) , ( ) , ( D Q sim D Q sim D Q sim D Q sim D Q sim terms concepts caption title × + × + × + × = γ η δ λ</formula><p>where λ, δ, η, and γ are coefficients that control the contribution of the similarity of the text from the title, text from the captions, UMLS concepts and UMLS terms respectively. sim is the similarity score which is computed using the standard dot product of the query Q and document D vectors.</p><p>For processing the images and extracting features from the images we used the GNU Image Retrieval Tool (GIFT), which is an open source CBIR system created by the University of Geneva (GNU Image Finding Tool (GIFT), 2004). GIFT works also with a vector space model that uses automatically extracted features such as color, texture and shape. GIFT uses a tf-idf similarity to rank images according to the similarity with a single image. For our mixed runs we combined the image and text results using a linear combination with two parameters that control the contribution of the results from each system.</p><p>Metamap is an open source tool developed by Alan Aronson at the National Library of Medicine (NLM). Metamap takes free text and maps it to UMLS concepts using natural language processing and information extraction techniques <ref type="bibr" coords="2,251.69,570.49,65.64,8.76" target="#b0">(Aronson, 2001)</ref>. This tool is offered to institutions that subscribe to UMLS Knowledge Sources which is provided by the NLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Document Processing</head><p>The data collection is described in more detail in the ImageCLEFmed overview paper <ref type="bibr" coords="2,449.33,643.15,56.99,8.76;2,92.70,655.57,22.20,8.76" target="#b1">(Müller, et al., 2009)</ref>. We processed all 74,901 documents associated to the images by using the UMLS knowledge sources and then adding two new fields (UMLS concepts, and UMLS terms) to each document. For each term we also added the corresponding semantic type generated by Metamap. The following is an example that shows a document with added fields using the standard simple document format of SMART. queries were also processed using Metamap and generating a similar representation as for the documents. The following is an example of one of the official queries after adding the UMLS concepts, and terms:</p><p>The images were processed using GIFT standard add-image routine. Unfortunately we had some technical problems before the deadline for submitting official results which did not allow us to create our own image runs. However, Henning Müller and the University of Geneva team supplied us with a visual run (also generated with GIFT) that we were able to use in our mixed runs. This was a simple run using 8 gray levels which is a type of run that has perform reasonably well in previous years.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Description of Runs and Parameters</head><p>Parameter tuning was done using the 2008 queries that use the same collection of this year. For text based runs we decided to generate two types of runs: a) Simple retrieval runs: using the free text, concepts and UMLS terms. b) Relevance feedback runs: automatic retrieval feedback using Rocchio's formula and assuming that the top n documents are relevant and expanding the queries with the top m terms.</p><p>For the relevance feedback runs we used the smart implementation of the Rocchio algorithm which applies query expansion on each of the indexes. In other words, our relevance feedback runs will generate expanded terms for each of the fields that are used in the retrieval model (title, captions, UMLS concepts and UMLS terms). The Rocchio formula requires several parameters such as the number of documents to be considered relevant in the automatic feedback (n), the number of terms to be added to each filed (m) and the coefficients that control the contribution of the original query terms (α), the terms found in the relevant document (β) and the terms found in the non-relevant documents (δ). All these parameters were set by optimizing the retrieval performance on the 2008 queries. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Photos of erythema</head><p>.T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Photos of erythema</head><p>The official text runs that we are: one with retrieval feedback and one without retrieval feedback. We also submitted two mixed runs that are basically the combination of the previously described text based runs and the results from the CBIR system. As explained previously we used a linear combination of the text based and CBIR results are combined using a linear function that includes two coefficients that control the contribution of each run to the final results. These coefficients were also tuned using the 2008 queries which yield a 3:1 ratio for combining the text and CBIR results (the text based results contributed 3 times more than the CBIR results). This is consistent with previous setting that we have used in <ref type="bibr" coords="5,250.23,155.77,45.49,8.76">CLEF 2005</ref><ref type="bibr" coords="5,303.10,155.77,20.91,8.76">CLEF , 2006</ref><ref type="bibr" coords="5,344.98,155.77,18.83,8.76">CLEF and 2007</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results and Analysis</head><p>Table <ref type="table" coords="5,118.86,216.01,5.24,8.76" target="#tab_0">1</ref> shows the official results of our four runs. Our highest scoring run was the one that used retrieval feedback with text only which improved the textual runs without feedback by 7%. Although noticeable this is not a significant difference. When we combined these textual runs with the CBIR results, which has a much lower score (MAP=0.0153). Our combination with the simple text run yields very small improvement of results. However, the combination with the retrieval feedback run yields a significant drop in performance. We still are not sure of the reason for this drop in performance but will continue exploring to confirm whether it was an error in one of our procedures or if this is in fact a problem that needs to be examined in more detail. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>Our experiments show that retrieval feedback does improve performance on the text based runs, although this improvement is relatively small (7%). Processing the entire collection using Metamap was time consuming but we thing that the results justify the usage of the automatically assigned UMLS concepts and the corresponding terms. Parameter tuning of this generalized vector space model with 4 different spaces proved to be tricky and time consuming. We plan to work more on the query by query analysis so that we can get a better sense of what works in this approach and what needs to be improved or changed. One possibility that we will explore is the use a faceted classification that could be more appropriate for tagging images along the main aspects of interest for medical image retrieval such as image modality, orientation, etc.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,158.22,489.41,331.06,8.25;3,158.22,502.61,325.26,8.25;3,158.22,515.81,333.08,8.25;3,158.22,529.01,4.95,8.25"><head></head><label></label><figDesc>Figure 1b. (Figure continued.) (b) Coronal precontrast T1-weighted (700/20) spin-echo MR image of the right lower leg at the level of the fibula. (c) Coronal postcontrast T1weighted (700/20) spin-echo MR image of the right lower leg at the same position as in b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,147.66,346.75,302.35,107.04"><head>Table 1 Official Runs</head><label>1</label><figDesc></figDesc><table coords="5,147.66,367.45,302.35,86.34"><row><cell>Run name</cell><cell>Type</cell><cell>MAP</cell><cell>Bpref</cell><cell>P@10</cell></row><row><cell>UNTtextb1</cell><cell>Text</cell><cell>0.2416</cell><cell>0.2784</cell><cell>0.404</cell></row><row><cell>UNTtextrf</cell><cell>Text with</cell><cell>0.2585</cell><cell>0.2826</cell><cell>0.436</cell></row><row><cell></cell><cell>Retrieval</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>feedback</cell><cell></cell><cell></cell><cell></cell></row><row><cell>UNTmixed1</cell><cell>Mixed</cell><cell>0.2447</cell><cell>0.2796</cell><cell>0.404</cell></row><row><cell>UNTmixedrf1</cell><cell>Mixed with</cell><cell>0.1924</cell><cell>0.2358</cell><cell>0.42</cell></row><row><cell></cell><cell>retrieval feedback</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,92.70,667.57,392.28,8.76;5,92.70,679.99,341.83,8.76;5,92.70,692.53,312.56,8.76;6,92.70,68.71,307.46,8.76;6,92.70,81.13,131.76,8.76" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,188.86,667.57,296.12,8.76;5,92.70,679.99,75.67,8.76">Effective Mapping of Biomedical Text to the UMLS Metathesaurus: The MetaMap Program</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Aronson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aronson</surname></persName>
		</author>
		<ptr target="http://www.gnu.org/software/gift" />
	</analytic>
	<monogr>
		<title level="m" coord="5,224.96,679.99,209.57,8.76;5,92.70,692.53,312.56,8.76;6,92.70,68.71,132.62,8.76">Effective Mapping of Biomedical Text to the UMLS MetathesaurAmerican Medical Informatics Association Annual Symposium. GNU Image Finding Tool (GIFT)</title>
		<imprint>
			<date type="published" when="2001">2001. 2004. August 22, 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,92.70,102.97,42.89,8.76;6,211.40,102.97,308.11,8.76;6,92.70,115.45,376.58,8.76" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,480.21,102.97,39.30,8.76;6,92.70,115.45,194.95,8.76">Overview of the CLEF 2009 medical image retrieval track</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Radhouani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bakke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,294.15,115.45,84.58,8.76">CLEF working notes</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,92.70,137.29,399.66,8.76;6,92.70,149.71,390.61,8.76;6,92.70,162.13,396.89,8.76" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,244.92,137.29,247.44,8.76;6,92.70,149.71,56.59,8.76">UB at CLEF 2005: Bilingual Portuguese and Medical Image retrieval tasks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Southwick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,166.41,149.71,316.90,8.76;6,92.70,162.13,169.03,8.76">Accessing Multilingual Information Repositories: 6th Workshop of the Cross-Language Evaluation Forum, CLEF 2005</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<publisher>Speringer</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>Revised Selected Pape</note>
</biblStruct>

<biblStruct coords="6,92.70,183.97,411.62,8.76;6,92.70,196.45,143.27,8.76" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="6,169.62,183.97,330.23,8.76">The SMART Retrieval System: Experiments in Automatic Document Processing</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971">1971</date>
			<publisher>Prentice Hall</publisher>
			<pubPlace>Englewood Cliff, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
