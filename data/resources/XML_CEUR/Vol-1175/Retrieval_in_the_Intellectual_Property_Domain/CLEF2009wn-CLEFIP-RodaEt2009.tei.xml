<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,123.16,146.74,356.66,18.57;1,183.41,168.65,236.17,18.57">CLEF-IP 2009: retrieval experiments in the Intellectual Property domain</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,169.65,204.00,67.43,9.96"><forename type="first">Giovanna</forename><surname>Roda</surname></persName>
						</author>
						<author>
							<persName coords="1,247.63,204.00,42.48,9.96"><forename type="first">John</forename><surname>Tait</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">The Information Retrieval Facility (IRF)</orgName>
								<address>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,300.66,204.00,55.94,9.96"><forename type="first">Florina</forename><surname>Piroi</surname></persName>
							<email>f.piroi@ir-facility.org</email>
							<affiliation key="aff1">
								<orgName type="department">The Information Retrieval Facility (IRF)</orgName>
								<address>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,367.16,204.00,61.73,9.96"><forename type="first">Veronika</forename><surname>Zenz</surname></persName>
							<email>v.zenz@matrixware.comj.tait</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Matrixware Information Services GmbH</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,123.16,146.74,356.66,18.57;1,183.41,168.65,236.17,18.57">CLEF-IP 2009: retrieval experiments in the Intellectual Property domain</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7DC37204C0100B0664887846741C70B5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.4 Systems and Software -Performance evaluation Algorithms</term>
					<term>Experimentation</term>
					<term>Measurement Patent retrieval</term>
					<term>Prior art search</term>
					<term>Intellectual Property</term>
					<term>Test collection</term>
					<term>Evaluation track</term>
					<term>Benchmarking</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The ClefIp track ran for the rst time within Clef 2009. The purpose of the track was twofold: to encourage and facilitate research in the area of patent retrieval by providing a large clean data set for experimentation; to create a large test collection of patents in the three main European languages for the evaluation of crosslingual information access. The track focused on the task of prior art search. The 15 European teams who participated in the track deployed a rich range of Information Retrieval techniques adapting them to this new specic domain and task. A large-scale test collection for evaluation purposes was created by exploiting patent citations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Cross Language Evaluation Forum Clef 1 originally arose from a work on Cross Lingual Information Retrieval in the US Federal National Institute of Standards and Technology Text Retrieval Conference Trec 2 but has been run separately since 2000. Each year since then a number of tasks on both crosslingual information retrieval (Clir) and monolingual information retrieval in nonEnglish languages have been run. In 2008 the Information Retrieval Facility (Irf) and Matrixware Information Services GmbH obtained the agreement to run a track which allowed groups to assess their systems on a large collection of patent documents containing a mixture of English, French and German documents derived from European Patent Oce data. This became known as the ClefIp track, which investigates IR techniques in the Intellectual Property domain of patents.</p><p>One main requirement for a patent to be granted is that the invention it describes should be novel: that is there should be no earlier patent or other publication describing the invention. The novelty breaking document can be published anywhere in any language. Hence when a person undertakes a search, for example to determine whether an idea is potentially patentable, or to try to prove a patent should not have been granted (a so-called opposition search), the search is inherently crosslingual, especially if it is exhaustive.</p><p>The patent system allows inventors a monopoly on the use of their invention for a xed period of time in return for public disclosure of the invention. Furthermore, the patent system is a major underpinning of the company value in a number of industries, which makes patent retrieval an important economic activity.</p><p>Although there is important previous academic research work on patent retrieval (see for example the Acm Sigir 2000 Workshop <ref type="bibr" coords="2,278.32,266.78,10.51,9.96" target="#b8">[9]</ref> or more recently the Ntcir workshop series <ref type="bibr" coords="2,499.72,266.78,9.96,9.96" target="#b4">[5]</ref>, there was little work involving nonEnglish European Languages and participation by European groups was low. ClefIp grew out of desire to promote such European research work and also to encourage academic use of a large clean collection of patents being made available to researchers by Matrixware (through the Information Retrieval Facility).</p><p>ClefIp has been a major success. For the rst time a large number of European groups <ref type="bibr" coords="2,90.00,338.51,17.71,9.96" target="#b14">(15)</ref> have been working on a patent corpus of signicant size within an integrated and single IR evaluation collection. Although it would be unreasonable to pretend the work is beyond criticism it does represent a signicant step forward for both IR community and patent searchers. <ref type="bibr" coords="2,90.01,390.99,7.89,15.77" target="#b1">2</ref> The CLEF-IP Patent Test Collection</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Document Collection</head><p>The ClefIp track had at its disposal a collection of patent documents published between 1978 and 2006 at the European Patent Oce (Epo). The whole collection consists of approximately 1.6 million individual patents. As suggested in <ref type="bibr" coords="2,296.34,461.46,9.96,9.96" target="#b5">[6]</ref>, we split the available data into two parts 1. the test collection corpus (or target dataset) -all documents with publication date between 1985 and 2000 <ref type="bibr" coords="2,208.97,493.35,13.28,9.96" target="#b0">(1,</ref><ref type="bibr" coords="2,222.25,493.35,13.28,9.96">958</ref>,955 patent documents pertaining to 1,022,388 patents, 75Gb) 2. the pool for topic selection -all documents with publication date from 2001 to 2006 (712,889 patent documents pertaining to 518,035 patents, 25Gb) Patents published prior to 1985 were excluded from the outset, as before this year many documents were not led in electronic form and the optical character recognition software that was used to digitize the documents produced noisy data. The upper limit, 2006, was induced by our data providera commercial institutionwhich, at the time the track was agreed on, had not made more recent documents available.</p><p>The documents are provided in Xml format and correspond to the Alexandria Xml Dtd<ref type="foot" coords="2,505.76,602.83,3.97,7.94" target="#foot_2">3</ref> . Patent documents are structured documents consisting of four major sections: bibliographic data, abstract, description and claims. Non-linguistic parts of patents like technical drawings, tables of formulas were left out which put the focus of this years track on the (multi)lingual aspect of patent retrieval: Epo patents are written in one of the three ocial languages English, German and French. 69% of the documents in the ClefIp collection have English as their main language, 23% German and 7% French. The claims of a granted patent are available in all 3 languages and also other sections, especially the title are given in several languages. That means the document collection itself is multilingual, with the dierent text sections being labeled with a language code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Patent documents and kind codes</head><p>In general, to one patent are associated several patent documents published at dierent stages of the patent's lifecycle. Each document is marked with a kind code that species the stage it was published in. The kind code is denoted by a letter possibly followed by a onedigit numerical code that gives additional information on the nature of the document. In the case of the Epo, A stands for a patent's application stage and B for a patent's granted stage, B1 denotes a patent specication and B2 a later, amended version of the patent specication <ref type="foot" coords="3,412.41,187.42,3.97,7.94" target="#foot_3">4</ref> .</p><p>Characteristic to our patent document collection is that les corresponding to patent documents published at various stages need not contain the whole data pertinent to a patent. For example, a B1 document of a patent granted by the Epo contains, among other, the title, the description, and the claims in three languages (English, German, French), but it usually does not contain an abstract, while an A2 document contains the original patent application (in one language) but no citation information except the one provided by the applicant. <ref type="foot" coords="3,375.86,259.15,3.97,7.94" target="#foot_4">5</ref>The ClefIp collection was delivered to the participants as is, without joining the documents related to the same patent into one document. Since the objective of a search are patents (identied by patent numbers, without kind code), it is up to the participants to collate multiple retrieved documents for a single patent into one result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Tasks and Topics</head><p>The goal of the ClefIp tasks consisted in nding prior art for a patent. The tasks mimic an important reallife scenario of an IP search professional. Performed at various stages of the patent life-cycle, prior art search is one of the most common search types and a critical activity in the patent domain. Before applying for a patent, inventors perform a such a search to determine whether the invention fullls the requirement of novelty and to formulate the claims as to not conict with existing prior art. During the application procedure, a prior art search is executed by patent examiners at the respective patent oce, in order to determine the patentability of an application by uncovering relevant material published prior to the ling date of the application. Finally parties that try to oppose a granted patent use this kind of search to unveil prior art that invalidates patents claims of originality.</p><p>For detailed information on information sources in patents and patent searching see <ref type="bibr" coords="3,467.55,474.89,10.52,9.96" target="#b2">[3]</ref> and <ref type="bibr" coords="3,499.72,474.89,9.97,9.96" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tasks</head><p>Participants were provided with sets of patents from the topic pool and asked to return all patents in the collection which constituted prior art for the given topic patents. Participants could choose among dierent topic sets of sizes ranging from 500 to 10000. The general goal in ClefIp was to nd prior art for a given topic patent. We proposed one main task and three optional language subtasks. For the language subtasks a dierent topic representation was adopted that allowed to focus on the impact of the language used for query formulation.</p><p>The main task of the track did not restrict the language used for retrieving documents. Participants were allowed to exploit the multilinguality of the patent topics. The three optional subtasks were dedicated to crosslingual search. According to Rule 71(3) of the European Patent Convention <ref type="bibr" coords="3,125.68,638.73,9.97,9.96" target="#b0">[1]</ref>, European granted patents must contain claims in the three ocial languages of the European Patent Oce (English, French, and German). This data is wellsuited for investigating the eect of languages in the retrieval of prior art. In the three parallel multilingual subtasks topics are represented by title and claims, in the respective language, extracted from the same B1 patent document. Participants were presented the same patents as in the main task, but with textual parts (title, claims) only in one language. The usage of bibliographic data, e.g. Ipc classes was allowed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic representation</head><p>In ClefIp a topic is itself a patent. Since patents come in several version corresponding to the dierent stages of the patent's life-cycle, we were faced with the problem of how to best represent a patent topic.</p><p>A patent examiner initiates a prior art search with a full patent application, hence one could think about taking highest version of the patent application's le would be best for simulating a real search task. However such a choice would have led to a large number of topics with missing elds. For instance, for EuroPCTs patents (currently about 70% of EP applications are EuroPCTs) whose PCT predecessor was published in English, French or German, the application les contain only bibliographic data (no abstract and no description or claims).</p><p>In order to overcome these shortcomings of the data, we decided to assemble a virtual patent application le to be used as a topic by starting from the B1 document. If the abstract was missing in the B1 document we added it from the most current document where the abstract was included. Finally we removed citation information from the bibliographical content of the patent document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic selection</head><p>Since relevance assessments were generated by exploiting existing manually created information (see section 3.1) ClefIp had a topic pool of hundreds of thousands of patents at hand. Evaluation platforms usually strive to evaluate against large numbers of topics, as robustness and reliability of the evaluation results increase with the number of topics <ref type="bibr" coords="4,350.79,403.15,33.30,9.96">[15] [16]</ref>. This is especially true when relevance judgments are not complete and the number of relevant documents per topic is very small as is the case in ClefIp where each topic has on average only 6 relevant documents. In order to maximize the number of topics while still allowing also groups with less computational resources to participate, four dierent topic bundles were assembled that diered in the number of topics. For each task participants could chose between the topics set S (500 topics), M (1,000 topics), L (5,000 topics), and XL (10,000 topics) with the smaller sets being subsets of the larger ones. Participants were asked to submit results for the largest of the 4 sets they were able to process.</p><p>From the initial pool of 500, 000 potential topics, candidate topics were selected according to the following criteria:</p><p>1. availability of granted patent 2. full text description available 3. at least three citations</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">at least one highly relevant citation</head><p>The rst criteria restricts the pool of candidate topics to those patents for which a granted patent is available. This restriction was imposed in order to guarantee that each topic would include claims in the three ocial languages of the EPO: German, English and French. In this fashion, we are also able to provide topics that can be used for parallel multi-lingual tasks. Still, not all patent documents corresponding to granted patents contained a full text description. Hence we imposed this additional requirement on a topic. Starting from a topics pool of approximately 500,000 patents, we were left with almost 16,000 patents fullling the above requirements. From these patents, we randomly selected 10,000 topics, which bundled in four subsets constitute the nal topic sets. In the same manner 500 topics were chosen which together with relevance assessments were provided to the participants as training set.</p><p>For an in-depth discussion of topic selection for ClefIp see <ref type="bibr" coords="4,373.43,741.88,14.61,9.96" target="#b12">[13]</ref>.  3 Relevance Assessment Methodology</p><p>This section describes the two types of relevance assessments used in ClefIp2009: (1) assessments automatically extracted from patent citations as well as (2) manual assessments by patent experts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Automatic Relevance Assessment</head><p>A common challenge in IR evaluation is the creation of ground truth data against which to evaluate retrieval systems. The common procedure of pooling and manual assessment is very labor-intensive. Voluntary assessors are dicult to nd, especially when expert knowledge is required as is the case of the patent eld. Researchers in the eld of patents and prior art search however are in the lucky position of already having partial ground truth at hand: patent citations. Citations are extracted from several sources:</p><p>1. applicant's disclosure : some patent oces (e.g. USPTO) require applicants to disclose all known relevant publications when applying for a patent 2. patent oce search report : each patent oce will do a search for prior art to judge the novelty of a patent 3. opposition procedures : often enough, a company will monitor granted patents of its competitors and, if possible, le an opposition procedure (i.e. a claim that a granted patent is not actually novel).</p><p>There are two major advantages of extracting ground truth from citations. First citations are established by members of the patent oces, applicants and patent attorneys, in short by highly qualied people. Second, search reports are publicly available and are made for any patent </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head><p>The general method for generating relevance assessments from patent citations is described in <ref type="bibr" coords="6,499.72,369.71,9.96,9.96" target="#b5">[6]</ref>. This idea had already been exploited at the Ntcir workshop series <ref type="foot" coords="6,385.95,379.56,3.97,7.94" target="#foot_5">6</ref> . Further discussions within the 1st Irf Symposium in 2007 <ref type="foot" coords="6,230.53,391.52,3.97,7.94" target="#foot_6">7</ref> led to a clearer formalization of the method.</p><p>For ClefIp 2009 we used an extended list of citations that includes not only patents cited directly by the patent topic, but also patents cited by patent family members and family members of cited patents. By means of patent families we were able to increase the number of citations by a factor of seven. Figure <ref type="figure" coords="6,200.09,441.44,4.98,9.96" target="#fig_1">1</ref> illustrates the process of gathering direct and extended citations.</p><p>A patent family consists of patents granted by dierent patent authorities but related to the same invention (one also says that all patents in a family share the same priority data). For Clef Ip this close (also called simple) patent family denition was applied, as opposed to the extended patent family denition which also includes patents related via a split of one patent application into two or more patents. Figure <ref type="figure" coords="6,236.64,501.21,4.98,9.96" target="#fig_1">1</ref> (from <ref type="bibr" coords="6,272.35,501.21,15.49,9.96" target="#b9">[10]</ref>) illustrates an example of extended families.</p><p>In the process of gathering citations, patents from ∼ 70 dierent patent oces (including Uspto, Sipo, Jpo, etc.) were considered. Out of the resulting lists of citations all nonEpo patents were discarded as they were not present in the target data set and thus not relevant to our track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Characteristics of patent citations as relevance judgments</head><p>What is to be noted when using citations lists as relevant judgments is that:</p><p>• citations have dierent degrees of relevancy (e.g. sometimes applicants cite not really relevant patents). This can be spotted easily by labeling citations as coming from applicant or from examiner and patent experts advise to chose patents with less than 25 -30 citations coming from the applicant.</p><p>• the lists are incomplete: even though, by considering patent families and opposition procedures, we have quite good lists of judgments, the nature of the search is such that it often stops when it nds one or only a few documents that are very relevant for the patent. The Guidelines for examination in the Epo <ref type="bibr" coords="6,285.86,704.89,10.51,9.96" target="#b1">[2]</ref> prescribe that if the search results in several documents of equal relevance, the search report should normally contain no more than one of them. This means that we have incomplete recall bases which must be taken into account when interpreting the evaluation results presented here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Further automatic methods</head><p>To conclude this section we describe further possibilities of extending the set of relevance judgements. These sources have not been used in the current evaluation procedure as they seem to be less reliable indicators of relevancy. Nevertheless they are interesting avenues to consider in the future, which is why they are mentioned here: A list of citations can be expanded by looking at patents cited in cited patents, if we assume some level of transitivity of this relation. It is however arguable how relevant a patent C is to patent A if we have something like A cites B and B cites C. Moreover, such a judgment cannot be done automatically.</p><p>In addition, a number of other features of patents can be used to identify potentially relevant documents: co-authorship (in this case "co-inventorship"), if we assume that an inventor generally has one area of research, co-ownership if we assume that a company specializes in one eld, or co-classication if two patents are classied in the same class according to one of the dierent classication models at dierent patent oces. Again, these features would require intellectual eort to consider.</p><p>Recently, a new approach for extracting prior art items from citations has been presented in <ref type="bibr" coords="7,90.00,346.92,14.61,9.96" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Manual Relevance Assessment by Patent Experts</head><p>A number of patent experts were contacted for the manual assessment of a small part of the track's experimental results. Communicating the project's goals and procedures was not an easy task, nor was it motivating them to invest some time for this assessment activity. Nevertheless, a total of 7 experts agreed to assess the relevance of retrieved patents for one or more topics. Topics were chosen by the experts out of our collection according to their area of expertise. A limit of around 200 retrieved patents to assess seemed to provide an acceptable amount of work. This limit allowed us to pool experimental data up to depth 20.</p><p>The engagement of patent experts resulted in 12 topics assessed up to rank 20 for all runs. A total of 3140 retrieval results were assessed with an average of 264 results per topic.</p><p>The results were submitted too late to be included in the track's evaluation report. In the section on evaluation activities we are going to report on the results obtained by using this additional small set of data for evaluation even though this collection is too small a sample to draw any general conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Submissions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Submission format</head><p>For all tasks, a submission consisted of a single Ascii text le containing at most 1, 000 lines per topic, in the standard format used for most Trec submissions: white space is used to separate columns, the width of the columns is not important, but it is important to have exactly ve columns per line with at least one space between the columns. • the second column is the query number within that topic. This is currently unused and should always be Q0;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EP1133908</head><p>• the third column is the ocial document number of the retrieved document;</p><p>• the fourth column is the rank of the document retrieved;</p><p>• the fth column shows the score (integer or oating point) that generated the ranking. This score must be in decreasing order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Submitted runs</head><p>A total of 70 experiments from 14 dierent teams and 15 participating institutions (the University of Tampere and Sics joined forces) was submitted to ClefIp 2009. Table <ref type="table" coords="8,419.73,604.98,4.98,9.96" target="#tab_0">1</ref> contains a list of all submitted runs. Experiments ranged over all proposed tasks (one main task and three language tasks) and over three (S, M, XL) of the proposed task sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Submission System</head><p>Clear and detailed guidelines together with automated format checks are critical in managing large-scale experimentations. For the upload and verication of runs a track management system was developed based on the open source document management system Alfresco<ref type="foot" coords="9,339.82,437.06,3.97,7.94" target="#foot_7">8</ref> and the web interface Docasu <ref type="foot" coords="9,481.29,437.06,3.97,7.94" target="#foot_8">9</ref> . The system provides an easy-to-use Web-frontend that allows participants to upload and download runs and any other type of le (e.g. descriptions of the runs). The system oers version control as well as a number of syntactical correctness tests. The validation process that is triggered on submission of a run returns a detailed description of the problematic content. This is added as an annotation to the run and is displayed in the user interface. Most format errors were therefore detected automatically and corrected by the participants themselves. Still one error type passed the validation and made the postprocessing of some runs necessary: patents listed as relevant on several dierent ranks for the same topic patent. Such duplicate entries were ltered out by us before evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group-ID</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Description of Submitted Runs</head><p>A comparison of the retrieval systems used in the ClefIp Task is given in Table <ref type="table" coords="9,457.25,614.94,3.87,9.96" target="#tab_1">2</ref>. The usage of Machine Translation (MT) is displayed in the second column, showing that MT was applied only by two groups, both using Google Translate. Methods used for selecting query terms are listed in the third column. As ClefIp topics are whole patent documents many participants found it necessary to apply some kind of term selection in order to limit the number of terms in the query. Methods for term selection based on term weighting are shown here while preselection based on patent-elds is shown separately in Table <ref type="table" coords="9,354.54,686.67,3.87,9.96">3</ref>. Given that each patent document could contain elds in up to three languages many participants chose to build separate indexes per language, while others just generated one mixed-language index or used text elds only in </p><formula xml:id="formula_0" coords="10,96.38,218.55,408.01,117.56">- x - - x x - - x - clep-run - - x - - - x - - - Tud x - x x x x x - - - Uaic - x x x x x x x x - clep-ug x x x x x x x x x * clep-unige x x x x - x x x x applicant, in- ventor UniNE x x x x x x x x x - uscom - ? ? ? ? x x x x - Utasics x x x x x x x x x -</formula><p>Table <ref type="table" coords="10,208.52,347.96,3.87,9.96">3</ref>: Fields used in indexing and query formulation one languages discarding information given in the other languages. The granularity of the index varied too, as some participants chose to concatenate all text elds into one index elds, while others indexed dierent elds separately. In addition several special indexes like phrase or passage indexes, concept indexes and Ipc indexes were used. A summary on which indexes were built and which ranking models were applied is given in Table <ref type="table" coords="10,321.19,427.40,3.87,9.96" target="#tab_1">2</ref>.</p><p>Table <ref type="table" coords="10,132.43,470.72,4.98,9.96">3</ref> gives an overview over the patent elds used in query formulation and indexing. The text elds title, claims, abstract and description were used most often. Among the bibliographic elds Ipc was the eld exploited most, it was used either as post-processing lter or as part of the query. Only two groups used the citation information that was present in the document set. Other very patent-specic information like priority, applicant, inventor information was only rarely used. Some further remarks on the runs that were submitted to the track:</p><p>• As this was the rst year for ClefIp many participants were absorbed with understanding the data and task and getting the system running. The ClefIp track presented several major challenges A new retrieval domain (patents) and task (prior art). The large size of the collection. The special language used in patents. Participants had not only to deal with German, English and French text but also with the specialities of patent-specic language (Patentese).</p><p>The large size of topics. In most Clef tracks a topic consists of few selected query words while for ClefIp a topic consists of a whole patent. The prior art task might thus also be tackled from the viewpoint of a document similarity or as proposed by Nlel as a plagiarism detection task.</p><p>• Cross-linguality: participants approached the multilingual nature of the ClefIp document collection in dierent ways: Some groups like clep-ug or Uaic did not focus on the multilingual nature of the data. Other participants like Hildesheim and clep-dcu chose to use only data in one specic language while many others used several monolingual retrieval systems to retrieve relevant documents and merged their results. Two groups made use of machine translation: Utasics used Google translate in the Main task to make patent-elds available in all three languages. They report that using the Google translation engine actually deteriorated their results. hcuge used Google translate to generate the elds in the missing languages in the monolingual tasks. humb applied cross-lingual concept tagging.</p><p>• Several teams integrated patent-specic know-how in their retrieval systems by: Using classication information (Ipc, Ecla) was mostly found helpful. Several participants used the Ipc class in their query formulation as a post-ranking lter criterium. While using Ipc classes to lter out generally improves the retrieval results, it also makes it impossible to retrieve relevant patents that don't share an Ipc class with the topic.</p><p>hcuge and humb exploited citation information given in the corpus. Apart from patent classication information and citations further bibliographic data (e.g. inventor, applicant, priority information) was used only by humb.</p><p>Only few groups had patent expertise at the beginning of the track. Aware of this problem some groups started cooperation with patent experts, like for example Utasics who are currently analysing patent experts' query formulation strategies.</p><p>• Even though query and indexing time were not evaluation criteria, participants had to start thinking about performance due to the amount of data.</p><p>• Dierent strategies were applied for indexing/ranking on patent level. Several teams applied the concept of virtual patent documents introduced by the organizers in the presentation of topics for indexing a set of patent documents as a single entity.</p><p>• Some teams combined several dierent strategies in their systems: this was done on a large scale by the humb team. cwi proposes a graphical user interface for combining search strategies.</p><p>• The training set, consisting of 500 patents with relevance assessments, was used by almost all of the participants, mostly for tuning and checking their strategies. humb used the training set also for Machine Learning. For this aim, it showed to be too small and they generated a larger one from the the citations available in the corpus.</p><p>• Having made the evaluation data available allowed many participants (among them Tud, Utasics, Hildesheim, clep-ug) to run additional experiments after the ocial evaluation. They report on new insights obtained (e.g. further tuning and comparisons of approaches) in their working notes papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>We evaluated the experiments by some of the most commonly used metrics for IR eectiveness evaluation. A correlation analysis shows that the rankings of the systems obtained with dierent topic sizes can be considered equivalent. The manual assessments obtained from patent experts allowed us to perform some preliminary analysis on the completeness of the automatically generated set of relevance assessments. The complete collection of measured values for all evaluation bundles is provided in the Clef Ip 2009 Evaluation Summary ( <ref type="bibr" coords="11,229.03,711.26,14.86,9.96" target="#b10">[11]</ref>). Detailed tables for the manually assessed patents will be provided in a separate report <ref type="bibr" coords="11,220.47,723.21,18.58,9.96">([12]</ref>).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Measurements</head><p>After some corrections of data formats, we created experiment bundles based on size and task.</p><p>For each experiment we computed 10 standard IR measures:</p><p>• Precision, Precision@5, Precision@10, Precision@100</p><p>• Recall, Recall@5, Recall@10, Recall@100</p><p>• MAP</p><p>• nDCG (with reduction factor given by a logarithm in base 10)</p><p>All computations were done with Soire 10 , a software for IR evaluation based on a service oriented architecture. Results were doublechecked against trec_eval 11 , the standard program for evaluation used in the Trec evaluation campaign, except for nDCG for which, at the time of the evaluation, we were not aware of a publicly available implementation. MAP, recall@100 and precision@100 of the best run for each participant are listed in Table <ref type="table" coords="13,508.02,111.35,4.98,9.96" target="#tab_3">4</ref> and illustrated in Figure <ref type="figure" coords="13,201.23,123.31,3.87,9.96">3</ref>. The values were calculated on the small topic set. The MAP values range from 0.0031 to 0.27 and are quite low in comparison with other CLEF tracks. The Precision values are generally low, but it must be noted that the average topic had 6 relevant documents, meaning that the upper boundary for precision@100 was at 0.06. Recall@100, a highly important measure in prior art search, ranges from 0.02 to 0.57. It must be noted these low values might be due to the incompleteness of the automatically generated set of relevance assessments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Correlation analysis</head><p>In order to see whether the evaluations obtained with the three dierent bundle sizes (S, M, XL) could be considered equivalent we did a correlation analysis comparing the vectors of MAPs computed for each of the bundles.</p><p>In addition to that, we also evaluated the results obtained by the track's participants for the 12 patents that were manually assessed by patent experts. We evaluated the runs from three bundles extracting only the 12 patents (when present) from each runle. We called these three extra-small evaluation bundles and named them ManS, ManM, ManXL.  The rankings obtained with topic sets S, M, and L are highly correlated, suggesting that the three bundles an be considered equivalent for evaluation purposes. As expected, the correlation between S, M, XL and the respective ManS, ManM, ManXL rankings by MAP drops drastically.</p><p>It must however be noted that the limited number of patents in the manual assessment bundle (12) is not sucient for drawing any conclusion. We hope to be able to collect more data in the future in order to assess the quality of our automatically generated test collection. Patent experts marked in average 8 of the proposed patents as relevant to the seed patent. For a comparison:</p><p>• 5.4 is the average number of citations for the 12 seed patents that were assessed manually</p><p>• for the whole collection, there are in average 6 citations per patent Furthermore, some of the automatically extracted citations (13 out of 34) were marked as not relevant by patent experts. Again, in order to have some meaningful results a larger set of data is needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Lessons Learned and Plans for 2010</head><p>In the 2009 collection only patent documents with data in French, English and German were included. One area in which to extend the track for 2010 is provide additional patent data in more European languages.</p><p>Patents are organized in what are known as patent families. A patent might be originally led in France in French, and then subsequently to ease enforcement of that patent in the United States a related patent might be led in English with the US Patents and Trademarks Oce. Although the full text of the patent will not be a direct translation of the French (for example because of dierent formulaic legal wordings) the two documents may be comparable, in the sense of a Comparable Corpus in Machine Translation). It might be that such comparable data will be useful to participants to mine for technical and other terms. The 2009 collection does not lend itself to this use and we will seek to make the collection more suitable for that purpose.</p><p>For the rst year we measured the overall eectiveness of systems. A more realistic evaluation should be layered in order to measure the contribution of each single component to the overall eectiveness results as proposed in the GRID@CLEF track ( <ref type="bibr" coords="14,356.71,566.90,10.96,9.96" target="#b3">[4]</ref>) and also by <ref type="bibr" coords="14,428.85,566.90,9.97,9.96" target="#b6">[7]</ref>. Analysis of the data should be statistical.</p><p>The 2009 task was also somewhat unrealistic in terms of a model of the work of patent professionals. Real patent searching often involves many cycles of query reformulation and results review, rather than one o queries and results set. In 2010 we would like to move to a more realistic model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Epilogue</head><p>CLEF IP has to be regarded as a major success: looking at previous CLEF tracks we regarded four to six groups as a satisfactory rst year participation rate. Fifteen is a very satisfactory number of participants -a tribute to those who did the work and to the timeliness of the task and data. In terms of retrieval eectiveness the results have proved hard to evaluate: if there is an over all conclusion the eective combination of a wide range of indexing methods is best, rather than a single silver bullet or wooden cross. However some of the results from groups other than Humboldt University indicate that specic techniques may work well: we look forward to more results next year. Also it is unclear how well the 2009 task and methodology maps to what makes a good (or better) system from the point of view of patent searchers -this is an area where we clearly need to improve. Finally we need to be clear that a degree of caution is needed for what is inevitably an initial analysis of a very complex set of results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,180.94,408.05,241.12,9.96"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Patent citation extension used in CLEF-IP09</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,126.97,271.13,349.08,9.96;6,172.11,109.45,258.31,147.11"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example for close and extended patent families. Source: OECD ([10])</figDesc><graphic coords="6,172.11,109.45,258.31,147.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="12,138.02,305.92,326.97,9.96"><head>Figure</head><label></label><figDesc>Figure 3: MAP, Precision@100 and Recall@100 of best run/participant (S)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="13,135.42,459.18,332.17,9.96"><head>SFigure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Correlation of rankings by MAP: comparison of XL, M, S bundles</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,104.94,669.21,253.91,76.08"><head>Table 1 :</head><label>1</label><figDesc>List of active participants and runs submitted</figDesc><table coords="7,142.29,669.21,209.17,34.37"><row><cell></cell><cell>Q0</cell><cell>EP1107664 1</cell><cell>3020</cell></row><row><cell>EP1133908</cell><cell>Q0</cell><cell>EP0826302 2</cell><cell>3019</cell></row><row><cell>EP1133908</cell><cell>Q0</cell><cell>EP0383071 3</cell><cell>2995</cell></row></table><note coords="7,104.94,715.40,28.25,9.96;7,104.94,734.33,253.91,10.96"><p><p>where:</p>• the rst column is the topic number (a patent number);</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,96.38,109.04,409.21,296.29"><head>Table 2 :</head><label>2</label><figDesc>Index, Query formulation</figDesc><table coords="9,96.38,109.04,409.21,274.49"><row><cell></cell><cell cols="3">MT qterm selection indexes</cell><cell>ranking</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>model</cell><cell></cell></row><row><cell>cwi</cell><cell>-</cell><cell>tf-idf</cell><cell>?</cell><cell cols="2">boolean, bm25</cell></row><row><cell>clep-dcu</cell><cell>-</cell><cell>none</cell><cell>one english only</cell><cell></cell><cell></cell></row><row><cell>hcuge</cell><cell>x</cell><cell>none</cell><cell>?</cell><cell>bm25</cell><cell></cell></row><row><cell>Hildesheim</cell><cell>-</cell><cell>none</cell><cell>one german only</cell><cell>?</cell><cell></cell></row><row><cell>humb</cell><cell>-</cell><cell>?</cell><cell>one per language, one additional</cell><cell>kl, bm25</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>phrase index for english, crosslin-</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>gual concept index</cell><cell></cell><cell></cell></row><row><cell>Nlel</cell><cell>-</cell><cell>random walks</cell><cell>mixed language passage index</cell><cell cols="2">passage similar-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ity</cell><cell></cell></row><row><cell>clep-run</cell><cell>-</cell><cell>none</cell><cell>one english only</cell><cell>tf-idf</cell><cell></cell></row><row><cell>Tud</cell><cell>-</cell><cell>none</cell><cell>one per language, one for Ipc</cell><cell>tf-idf</cell><cell></cell></row><row><cell>Uaic</cell><cell>-</cell><cell>none</cell><cell>one mixed language index (split</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>in 4 indexes for performance rea-</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>sons)</cell><cell></cell><cell></cell></row><row><cell>clep-ug</cell><cell>-</cell><cell>tf-idf</cell><cell>one mixed language</cell><cell cols="2">bm25, cosine re-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">trieval model</cell></row><row><cell>clep-unige</cell><cell>-</cell><cell>?</cell><cell>one engilsh only</cell><cell>tf-idf,</cell><cell>bm25,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>fast</cell><cell></cell></row><row><cell>UniNE</cell><cell>-</cell><cell>tf-idf</cell><cell>one mixed language index</cell><cell cols="2">tf-idf, bm25, dfr</cell></row><row><cell>uscom</cell><cell>-</cell><cell>tf-idf</cell><cell>one mixed language index</cell><cell>bm25</cell><cell></cell></row><row><cell>Utasics</cell><cell>x</cell><cell>ratf, tf-idf</cell><cell>1 per language, 1 for Ipc</cell><cell>?</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="12,122.10,305.92,358.80,233.32"><head>Table 4 :</head><label>4</label><figDesc>3: MAP, Precision@100 and Recall@100 of best run/participant (S) MAP, Precision@100, Recall@100 of best run/participant (S)</figDesc><table coords="12,122.10,339.10,358.80,177.73"><row><cell>Group-ID</cell><cell>Run-ID</cell><cell>MAP</cell><cell cols="2">Recall@100 Precision@100</cell></row><row><cell>humb</cell><cell>1</cell><cell cols="2">0.2714 0.57996</cell><cell>0.0317</cell></row><row><cell>hcuge</cell><cell>BiTeM</cell><cell cols="2">0.1145 0.40479</cell><cell>0.0238</cell></row><row><cell>uscom</cell><cell>BM25bt</cell><cell cols="2">0.1133 0.36100</cell><cell>0.0213</cell></row><row><cell>UTASICS</cell><cell>all-ratf-ipcr</cell><cell cols="2">0.1096 0.36626</cell><cell>0.0208</cell></row><row><cell>UniNE</cell><cell>strat3</cell><cell cols="2">0.1024 0.34182</cell><cell>0.0201</cell></row><row><cell>TUD</cell><cell>800noTitle</cell><cell cols="2">0.0975 0.42202</cell><cell>0.0237</cell></row><row><cell>clep-dcu</cell><cell>Filtered2</cell><cell cols="2">0.0913 0.35309</cell><cell>0.0208</cell></row><row><cell cols="2">clep-unige RUN3</cell><cell cols="2">0.0900 0.29790</cell><cell>0.0172</cell></row><row><cell>clep-ug</cell><cell cols="3">infdocfreqCosEnglishTerms 0.0715 0.24470</cell><cell>0.0152</cell></row><row><cell>cwi</cell><cell>categorybm25</cell><cell cols="2">0.0697 0.29386</cell><cell>0.0172</cell></row><row><cell>clep-run</cell><cell>ClaimsBOW</cell><cell cols="2">0.0540 0.22015</cell><cell>0.0129</cell></row><row><cell>NLEL</cell><cell>MethodA</cell><cell cols="2">0.0289 0.11866</cell><cell>0.0076</cell></row><row><cell>UAIC</cell><cell>MethodAnew</cell><cell cols="2">0.0094 0.03420</cell><cell>0.0023</cell></row><row><cell cols="2">Hildesheim MethodAnew</cell><cell cols="2">0.0031 0.02340</cell><cell>0.0011</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="13,90.00,298.10,423.01,46.83"><head>Table 5 lists</head><label>5</label><figDesc>Kendall's τ and Spearman's ρ for all compared rankings. Figures 4 5 illustrates the correlation between pairs of bundles together with the best leastsquares linear t.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="14,90.01,109.16,323.11,143.86"><head>Table 5 :</head><label>5</label><figDesc>Correlations of systems rankings for MAP 5.1.2 Some remarks on the manually assessed patents</figDesc><table coords="14,210.28,109.16,182.45,87.47"><row><cell>Correlation</cell><cell>#runs</cell><cell>τ</cell><cell>ρ</cell></row><row><cell>M vs XL</cell><cell>24</cell><cell cols="2">0.9203 0.9977</cell></row><row><cell>S vs M</cell><cell>29</cell><cell cols="2">0.9160 0.9970</cell></row><row><cell>S vs XL</cell><cell>24</cell><cell cols="2">0.9058 0.9947</cell></row><row><cell>XL vs ManXL</cell><cell>24</cell><cell>0.5</cell><cell>0.7760</cell></row><row><cell>M vs ManM</cell><cell>29</cell><cell cols="2">0.6228 0.8622</cell></row><row><cell>S vs ManS</cell><cell>48</cell><cell cols="2">0.4066 0.7031</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,105.24,722.18,119.05,8.47"><p>http://www.clef-campaign.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,105.24,731.68,84.68,8.47"><p>http://trec.nist.gov</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,105.24,719.24,209.61,8.47"><p>http://www.ir-facility.org/pdf/clef/patent-document.dtd</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="3,105.24,705.22,384.98,8.47"><p>For a complete list of kind codes used by various patent oces see http://tinyurl.com/EPO-kindcodes</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="3,105.24,714.70,407.78,8.47;3,90.00,724.17,423.08,8.47;3,90.00,733.64,346.23,8.47"><p>It is not in the scope of this paper to discuss the origins of the content in the Epo patent documents. We only note that applications to the Epo may originate from patents granted by other patent oces, in which case the Epo may publish patent documents with incomplete content, referring to the original patent.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="6,105.24,735.52,135.49,8.47"><p>http://research.nii.ac.jp/ntcir/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="6,105.24,745.01,315.82,8.47"><p>http://www.ir-facility.org/symposium/irf-symposium-2007/the-working-groups</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="9,105.24,729.25,96.50,8.47"><p>http://www.alfresco.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8" coords="9,105.24,738.75,113.72,8.47"><p>http://docasu.sourceforge.net/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9" coords="12,105.24,734.21,105.22,8.47"><p>http://soire.matrixware.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10" coords="12,105.24,743.70,111.09,8.47"><p>http://trec.nist.gov/trec_eval</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We would like to thank <rs type="person">Judy Hickey</rs>, <rs type="person">Henk Tomas</rs> and all the other patent experts who helped us with manual assessments and who shared their know-how on prior art searches with us. Thanks to <rs type="person">Evangelos Kanoulas</rs> and <rs type="person">Emine Yilmaz</rs> for interesting discussions on creating large test collections.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="15,110.47,295.70,388.06,10.46" xml:id="b0">
	<monogr>
		<ptr target="http://www.epo.org/patents/law/legal-texts" />
		<title level="m" coord="15,110.47,295.91,125.64,10.18">European Patent Convention</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="15,110.47,315.63,402.53,10.46;15,110.47,327.58,191.38,10.46" xml:id="b1">
	<monogr>
		<ptr target="http://www.epo.org/patents/law/legal-texts/guidelines.html" />
		<title level="m" coord="15,110.47,315.83,249.89,10.18">Guidelines for Examination in the European Patent Oce</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,110.47,347.71,299.17,10.18" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="15,196.42,347.71,131.50,10.18">Information sources in patents</title>
		<author>
			<persName coords=""><surname>Stephen R Adams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>K.G. Saur</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,110.47,367.80,402.54,9.96;15,110.47,379.59,402.53,10.18;15,110.47,391.55,370.49,10.18" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="15,230.47,367.80,282.54,9.96;15,110.47,379.76,51.35,9.96">Dealing with multilingual information access: Grid experiments at trebleclef</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,412.76,379.59,100.24,10.18;15,110.47,391.55,274.34,10.18">Post-proceedings of the Fourth Italian Research Conference on Digital Library Systems</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Esposito</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>In Agosti</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Thanos</surname></persName>
		</editor>
		<meeting><address><addrLine>IRCDL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,110.47,411.64,402.53,9.96;15,110.47,423.43,402.53,10.18;15,110.47,435.39,402.53,10.18;15,110.47,447.34,402.53,10.18;15,110.47,459.46,402.53,9.96;15,110.47,471.41,52.10,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="15,336.63,411.64,176.37,9.96;15,110.47,423.60,102.16,9.96">Overview of the Patent Retrieval Task at the NTCIR-6 Workshop</title>
		<author>
			<persName coords=""><forename type="first">Atsushi</forename><surname>Fujii</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Makoto</forename><surname>Iwayama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noriko</forename><surname>Kando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,435.61,423.43,77.38,10.18;15,110.47,435.39,402.53,10.18;15,110.47,447.34,328.83,10.18">Proceedings of the Sixth NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, Question Answering, and Cross-Lingual Information Access</title>
		<editor>
			<persName><forename type="first">Noriko</forename><surname>Kando</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><forename type="middle">Kirk</forename><surname>Evans</surname></persName>
		</editor>
		<meeting>the Sixth NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, Question Answering, and Cross-Lingual Information Access<address><addrLine>Hitotsubashi, Chiyoda-ku, Tokyo; Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-05">May 2007</date>
			<biblScope unit="page" from="101" to="8430" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Informatics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="15,110.47,491.34,402.53,9.96;15,110.47,503.14,402.53,10.18;15,110.47,515.09,93.65,10.18" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="15,233.79,491.34,279.21,9.96;15,110.47,503.30,43.02,9.96">A Methodology for Building a Patent Test Collection for Prior art Search</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Graf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Azzopardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,175.29,503.14,337.71,10.18;15,110.47,515.09,62.47,10.18">Proceedings of the Second International Workshop on Evaluating Information Access (EVIA)</title>
		<meeting>the Second International Workshop on Evaluating Information Access (EVIA)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,110.47,535.02,402.53,10.18;15,110.47,546.97,338.81,10.18" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="15,232.69,535.18,201.48,9.96">Toward automated component-level evaluation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,454.77,535.02,58.24,10.18;15,110.47,546.97,156.68,10.18">SIGIR Workshop on the Future of IR Evaluation</title>
		<meeting><address><addrLine>Boston, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2930. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,110.47,566.89,402.53,10.18;15,110.47,579.01,53.40,9.96" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="15,338.71,566.89,170.01,10.18">Patent searching : tools and techniques</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Long</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><surname>Rodgers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,110.47,598.94,402.54,9.96;15,110.47,610.73,186.93,10.18" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="15,274.89,598.94,132.53,9.96">Workshop on Patent Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Noriko</forename><surname>Kando</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mun-Kew</forename><surname>Leong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,416.17,598.94,96.85,9.96;15,110.47,610.73,99.27,10.18">SIGIR 2000 Workshop Report). SIGIR Forum</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2830</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,110.47,630.65,402.52,10.18;15,110.47,642.61,83.10,10.18" xml:id="b9">
	<analytic>
	</analytic>
	<monogr>
		<title level="m" coord="15,110.47,630.82,253.23,9.96">Organisation for Economic Co-operation and Development</title>
		<imprint>
			<date type="published" when="2009-02">Feb. 2009</date>
		</imprint>
	</monogr>
	<note>OECD</note>
</biblStruct>

<biblStruct coords="15,110.47,662.70,402.53,9.96;15,110.47,674.66,22.69,9.96" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="15,331.06,662.70,154.11,9.96">CLEF-IP 2009 Evaluation Summary</title>
		<author>
			<persName coords=""><forename type="first">Florina</forename><surname>Piroi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Giovanna</forename><surname>Roda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Veronika</forename><surname>Zenz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009-07">July 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,110.47,694.59,402.53,9.96;15,110.47,706.54,159.72,9.96" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Florina</forename><surname>Piroi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Giovanna</forename><surname>Roda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Veronika</forename><surname>Zenz</surname></persName>
		</author>
		<title level="m" coord="15,333.09,694.59,179.91,9.96;15,110.47,706.54,7.47,9.96">CLEF-IP 2009 Evaluation Summary part II</title>
		<imprint>
			<date type="published" when="2009-09">September 2009</date>
		</imprint>
	</monogr>
	<note>in preparation</note>
</biblStruct>

<biblStruct coords="15,110.47,726.47,402.53,9.96;15,110.47,738.25,367.72,10.18" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="15,185.82,738.42,137.31,9.96">So Many Topics, So Little Time</title>
		<author>
			<persName coords=""><forename type="first">Giovanna</forename><surname>Roda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Veronika</forename><surname>Zenz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Lupu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kalervo</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christa</forename><surname>Womser-Hacker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,332.65,738.25,57.88,10.18">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1621</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,110.47,111.35,402.54,9.96;16,110.47,123.14,402.52,10.18;16,110.47,135.27,32.40,9.96" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="16,277.12,111.35,216.77,9.96">Findcite automatically nding prior art patents</title>
		<author>
			<persName coords=""><forename type="first">Shahzad</forename><surname>Tiwana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellis</forename><surname>Horowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,110.47,123.14,355.38,10.18">PaIR &apos;09: Proceeding of the 1st ACM workshop on Patent information retrieval</title>
		<imprint>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="16,110.47,155.02,402.53,10.18;16,110.47,166.98,402.53,10.18;16,110.47,179.10,149.17,9.96" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="16,196.20,155.19,83.53,9.96">Topic set size redux</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,300.18,155.02,212.81,10.18;16,110.47,166.98,332.36,10.18">SIGIR &apos;09: Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page">806807</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,110.47,199.03,402.54,9.96;16,110.47,210.81,402.54,10.18;16,110.47,222.77,402.53,10.18;16,110.47,234.89,52.30,9.96" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="16,287.36,199.03,225.65,9.96;16,110.47,210.98,19.90,9.96">The eect of topic set size on retrieval experiment error</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,153.88,210.81,359.12,10.18;16,110.47,222.77,233.59,10.18">SIGIR &apos;02: Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page">316323</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
