<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
				<funder>
					<orgName type="full">IRF</orgName>
				</funder>
				<funder ref="#_PuJ6YTH">
					<orgName type="full">FEDER and Xunta de Galicia</orgName>
				</funder>
				<funder ref="#_Fxn9y5Z">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,217.56,204.07,87.03,9.96"><forename type="first">José</forename><forename type="middle">Carlos</forename><surname>Toucedo</surname></persName>
							<email>josecarlos.toucedo@usc.es</email>
						</author>
						<author>
							<persName coords="1,312.71,204.07,72.92,9.96"><forename type="first">David</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
							<email>david.losada@usc.es</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Santiago de Compostela at CLEF-IP09</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Grupo de Sistemas Inteligentes Dept. Electrónica</orgName>
								<orgName type="institution">Computación</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Universidad de Santiago de Compostela</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">32ECDE94C7AA6C58D9946D1659CB85E8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.3 [Information storage and retrieval]: Information Search and Retrieval-query formulation; H.3.7 [Information storage and retrieval]: Digital Libraries Performance</term>
					<term>Experimentation</term>
					<term>Measurement Patent Retrieval</term>
					<term>Prior Art Search</term>
					<term>Cross Language</term>
					<term>Query Formulation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we describe our participation in CLEF-IP 2009 (prior-art search task). This was the first year of the task and we focused on how to build effectively a prior art query from a patent. Basically, we implemented simple strategies to extract terms from some textual fields of the patent documents and gave preference to title terms. We ran experiments with standard BM25 configurations and we paid little attention to language-dependent issues.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The main task of the CLEF-IP09 track is to investigate Information Retrieval (IR) techniques for patent retrieval, specifically for prior art search. Prior art consists of any kind of information (publication, product, process, etc.) within the patent and non-patent literature that has been made available to the public, maybe orally, before the filing date of a patent application. Therefore, a prior art search tries to retrieve any prior record with identical or similar contents to a given patent application.</p><p>This track provides the participants with a huge collection of more than one million patents from the European Patent Office (EPO). This is composed of all the patents published between 1986 and 2000. Every patent, identified by an unique number, consists of several XML documents generated at different stages of the patent's life-cycle. Therefore, each patent document is identified by the patent number plus the stage (represented as a kind code plus a version number). For instance, the patent document with the id 0981201-A3 denotes the third version of the application of the patent 0981201. The information within a patent document is structured. Some fields appear in all the stages, such as the title, the bibliographic data, the description and the claims. However, there are some fields like the abstract that only occur in a particular stage. At the EPO, the patents can be written in English, French or German. The title and the claims of every patent are translated into these three languages. The rest of the patent is written in a single language.</p><p>In an information retrieval setting the patent to be evaluated can be regarded as the information need and all the granted patents to date as the document collection. Since a patent is made up of several documents all these documents have to be taken into account in order to produce a query patent. In the prior art search task, the query patent provided is built from the non-common fields of the patent documents and from the common fields of the document with the highest stage. A query patent constructed in this way in the EPO is about 3500 terms long on average. The query patent is therefore a long and verbose document and many of its terms are redundant or unrepresentative.</p><p>Throghout these notes we will explain the approach we have taken to address the prior art search task. This year our main objective has been to formulate a concise query that effectively represents the underlying information need.</p><p>Since this is our first participation in CLEF, we have just focused on query formulation. We recognize that there are many issues such as link analysis, entity extraction, cross-language retrieval, field boosting, etc. that might play a key role in prior art search but these will be considered for next editions of this track.</p><p>The rest of the paper is organized as follows. Section 2 describes the approach we have taken, specifically how the query is built and what experiments we designed; the runs we submitted are explained in section 3 and the conclusions we extracted are exposed in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach taken</head><p>Although every patent is composed of several documents, this track requires that retrieval is performed at patent level. This problem can be addressed following two different approaches: a) building an index of patents or b) building an index of patent documents. The former requires to define an effective strategy to combine several patent documents into a single patent representation. The second approach is simpler because it only requires to post-process the retrieved documents in order to obtain one result per patent. Our choice was to assign a patent the score of its highest ranked document. This follows the intuition that the patent document that is the most similar to the patent query reflects well the connection between the query and the underlying patent.</p><p>Although the documents contain terms from three different languages, no language-oriented distinction was made during the index construction<ref type="foot" coords="2,311.40,500.19,3.97,4.77" target="#foot_0">1</ref> . The index contains all terms in any language for each patent document. Note that some fields are translated into the three languages in the patent (e.g. the title) and these translations appear in the index associated to the same document. Furthermore, stemming was not applied and an English stopword list (with 733 stopwords) was used in order to remove common words. This makes sense because almost 70% of the data was written in English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Query formulation</head><p>The query patent is too long to be processed in a reasonable time and contains noisy terms that might harm performance. We think that a good query preprocessing is a key factor in order to achieve good effectiveness.</p><p>Our experiments focused on extracting the most significant terms from the query patent, i.e. those terms that are discriminative. To this aim, we used inverse document frequency (idf). In our evaluation with the training set, we concentrated on deciding the number of terms that should be included into the query. We ran this process in both a language-independent and languagedependent way (i.e. a single ranking of terms vs. three rankings of terms, one for each language).</p><p>The number of query terms is difficult to set because few query terms make that the query processing is fast but the information need might be misrepresented; on the other hand, if many terms are taken the query will contain many noisy terms and, furthermore, the query processing time might be prohibitive. We have studied two methods to choose a suitable number of terms: (i) establishing a fixed number of terms for all queries and (ii) establishing a fixed percentage of the query patent length (i.e. the query size varies from one query patent to another).</p><p>Once the number of query terms has been selected, we must determine how they are extracted. We explored two strategies: language-independent and language-dependent. Suppose that we select n terms from the original query patent regardless of the language. This means that all query patent terms (english, french and german terms) are ranked together and we simply select the n terms with the highest idf from this list. Because of the nature of the languages, it is likely that the three languages present different idf patterns. Besides, there are fewer German/French documents than English documents and, therefore, this introduces a bias in terms of idf. We therefore felt that we needed to test other alternatives for selecting terms. We tried out an extraction of terms where each language contributes with the same number of terms. In this second strategy we first grouped the terms of a query patent depending on their language (no classification was needed since every field in the XML is tagged with language information). Next, the highest n ′ = ⌊n/3⌋ terms from each group are extracted. The query is finally obtained by compiling the terms from the three groups.</p><p>So far, we simply explained which options we have considered for query formulation. In section 2.3 we will explain how some combinations of these strategies perform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Retrieval model</head><p>We used the well-known BM25 retrieval model <ref type="bibr" coords="3,297.37,372.91,10.45,9.96" target="#b1">[2]</ref> with the usual parameters (b = 0.75, k 1 = 1.2, k 3 = 1000), but we also tried several variations for b and k 1 in the submitted runs.</p><p>The platform under which we executed our experiments was the Lemur Toolkit <ref type="bibr" coords="3,454.57,396.79,10.00,9.96" target="#b0">[1]</ref>. All experiments were executed in the LDC, a system provided by the Information Retrieval Facility (IRF)<ref type="foot" coords="3,154.08,419.91,3.97,4.77" target="#foot_1">2</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training</head><p>With the training data provided by the track, we studied two dimensions: query length and language. Query length refers to the way in which query size is set. As argued above, this can be done in a query-dependent (i.e. a given percentage of the patent query terms are selected) or query-independent way (i.e. a fixed number of terms are selected for all queries). The language dimension reflects the way in which terms are ranked (language-independent, i.e. a single rank for all terms; language-dependent, one rank of terms for every language). Hence, our training consisted of studying how the four combinations of these dimensions perform in terms of two well-known measures, MAP and Bpref.</p><p>The following results were obtained with the large training set (500 queries) of the main task, which contains queries in the three languages. In this case, we only tried the usual parameters for the BM25 retrieval model.</p><p>Figures <ref type="figure" coords="3,140.16,598.51,13.35,9.96" target="#fig_1">1(a</ref>) and 2(a) consider the case where the number of terms is fixed for all queries. We clearly get better performance when the language is not taken into account during the training. However, figures 1(b) and 2(b), where terms are selected using a percentage of the query length, show a different trend. Figure <ref type="figure" coords="3,222.26,634.39,18.28,9.96" target="#fig_1">1(b)</ref> shows that, for values less than 50%<ref type="foot" coords="3,401.28,633.63,3.97,4.77" target="#foot_2">3</ref> , no significant difference can be stablished in terms of MAP. In contrast, figure <ref type="figure" coords="3,335.44,646.27,18.28,9.96" target="#fig_2">2(b)</ref> shows that the language-dependent choice is slightly more consistent than the language-independent one in terms of Bpref.</p><p>We have to choose between two opposite models, the model that consists of combining the query-dependent and language-dependent strategies and, on the other hand, the model that considers the query-independent and language-independent strategies together. If we observe carefully the plots we will note that these two models do not differ in MAP values but, in terms of Bpref, the model that is language and query dependent presents the best performance.   Based on this training, we also concluded that a 40% of query length is a good trade-off between performance and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Submitted runs</head><p>We participated in the Main task of this track with eight runs for the Small set of topics, which contains 500 queries in different languages.</p><p>First, we submitted four runs considering the scenario that best worked for our training experiments. These four runs differ on the retrieval model parameters:</p><formula xml:id="formula_0" coords="4,105.00,658.03,205.02,66.96">• uscom BM25a: b = 0.2, k 1 = 0.1, k 3 = 1000 • uscom BM25b: b = 0.75, k 1 = 1.2, k 3 = 1000 • uscom BM25c: b = 0.75, k 1 = 1.6, k 3 = 1000 • uscom BM25d : b = 1, k 1 = 1.2, k 3 = 1000</formula><p>Furthermore, we submitted four additional runs where the final queries were expanded with the title terms of the query patent. In this way, the query term frequency of these terms is augmented and the presence of the title terms in the final queries is guaranteed. These new runs are labeled as the previous ones plus an extra "t".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and conclusions</head><p>The official evaluation results of our submitted runs are summarized in Table <ref type="table" coords="5,431.05,178.15,3.90,9.96" target="#tab_0">1</ref> The first conclusion we can extract from the evaluation is that our decision to force the presence of title terms worked well. Regardless of the configuration of the BM25 parameters, the run with the title terms always obtains better performance than its counterpart. Therefore, we can state that the title terms represent an important factor in prior art search.</p><p>Furthermore, among the configurations with the title terms the best run is the one labeled as uscom BM25bt. This run corresponds to the usual parameters of the BM25 retrieval model, i.e. b = 0.75, k 1 = 1.2, k 3 = 1000.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,239.52,298.15,124.21,9.96"><head>Figure 1</head><label>1</label><figDesc>Figure 1: MAP performance</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,233.88,507.79,135.37,9.96"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: BPREF performance</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,101.52,178.15,400.01,140.04"><head>Table 1 :</head><label>1</label><figDesc>. Submitted runs for CLEF-IP 09</figDesc><table coords="5,101.52,200.76,400.01,93.65"><row><cell></cell><cell>P</cell><cell>P5</cell><cell>P10</cell><cell>P100</cell><cell>R</cell><cell>R5</cell><cell>R10</cell><cell>R100</cell><cell>MAP</cell><cell>nDCG</cell></row><row><cell>uscom BM25a</cell><cell>.0029</cell><cell>.0948</cell><cell>.0644</cell><cell>.0141</cell><cell>.4247</cell><cell>.0900</cell><cell>.1183</cell><cell>.2473</cell><cell>.0837</cell><cell>.4466</cell></row><row><cell>uscom BM25b</cell><cell>.0041</cell><cell>.1184</cell><cell>.0858</cell><cell>.0205</cell><cell>.5553</cell><cell>.1082</cell><cell>.1569</cell><cell>.3509</cell><cell>.1079</cell><cell>.4410</cell></row><row><cell>uscom BM25c</cell><cell>.0042</cell><cell>.1180</cell><cell>.0858</cell><cell>.0206</cell><cell>.5563</cell><cell>.1104</cell><cell>.1564</cell><cell>.3504</cell><cell>.1071</cell><cell>.4341</cell></row><row><cell>uscom BM25d</cell><cell>.0042</cell><cell>.1188</cell><cell>.0852</cell><cell>.0206</cell><cell>.5630</cell><cell>.1113</cell><cell>.1558</cell><cell>.3500</cell><cell>.1071</cell><cell>.4346</cell></row><row><cell>uscom BM25at</cell><cell>.0031</cell><cell>.1004</cell><cell>.0680</cell><cell>.0151</cell><cell>.4549</cell><cell>.0937</cell><cell>.1223</cell><cell>.2637</cell><cell>.0867</cell><cell>.4331</cell></row><row><cell>uscom BM25bt</cell><cell>.0042</cell><cell cols="3">.1280 .0908 .0213</cell><cell>.5729</cell><cell cols="4">.1176 .1631 .3610 .1133</cell><cell>.4588</cell></row><row><cell>uscom BM25ct</cell><cell>.0042</cell><cell>.1268</cell><cell>.0898</cell><cell>.0212</cell><cell>.5722</cell><cell>.1172</cell><cell>.1611</cell><cell>.3602</cell><cell>.1132</cell><cell>.4544</cell></row><row><cell cols="2">uscom BM25dt .0043</cell><cell>.1252</cell><cell>.0892</cell><cell cols="2">.0213 .5773</cell><cell>.1163</cell><cell>.1606</cell><cell>.3609</cell><cell>.1121</cell><cell>.4455</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,105.24,733.68,407.70,7.97;2,90.00,743.16,84.88,7.97"><p>We deeply thank the support of Erik Graf and Leif Azzopardi, from University of Glasgow, who granted us access to their indexes.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,105.24,737.04,204.26,7.97"><p>We are grateful to the IRF for the support given to us.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,105.24,746.52,328.24,7.97"><p>We are not taking into account greater values because the resulting queries are too long.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5">Acknowledgements</head><p>We are deeply grateful to <rs type="person">Erik Graff</rs> and <rs type="person">Leif Azzopardi</rs>, from <rs type="affiliation">University of Glasgow</rs>, for their help during our experiments.</p><p>We also thank the support of the <rs type="funder">IRF</rs>. This research was co-funded by <rs type="funder">FEDER and Xunta de Galicia</rs> under projects <rs type="grantNumber">07SIN005206PR</rs> and <rs type="grantNumber">2008/068</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_PuJ6YTH">
					<idno type="grant-number">07SIN005206PR</idno>
				</org>
				<org type="funding" xml:id="_Fxn9y5Z">
					<idno type="grant-number">2008/068</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,105.48,571.15,235.92,9.96" xml:id="b0">
	<monogr>
		<ptr target="http://www.lemurproject.org" />
		<title level="m" coord="5,105.48,571.15,82.53,9.96">The Lemur Toolkit</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.48,591.07,407.69,9.96;5,105.48,603.07,91.09,9.96" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
		<title level="m" coord="5,444.28,591.07,64.92,9.96">Okapi at trec-3</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="109" to="126" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
