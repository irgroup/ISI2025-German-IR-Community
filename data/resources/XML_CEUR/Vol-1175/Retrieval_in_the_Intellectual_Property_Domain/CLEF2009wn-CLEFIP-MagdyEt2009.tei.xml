<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.07,74.42,417.18,12.52;1,270.02,90.50,55.25,12.52">DCU @ CLEF-IP 2009: Exploring Standard IR Techniques on Patent Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,193.22,116.90,52.57,8.88"><forename type="first">Walid</forename><surname>Magdy</surname></persName>
							<email>wmagdy@computing.dcu.ie</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="laboratory">Centre for Next Generation Localization</orgName>
								<orgName type="institution" key="instit1">Dublin</orgName>
								<orgName type="institution" key="instit2">City University</orgName>
								<address>
									<settlement>Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,253.47,116.90,72.81,8.88"><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
							<email>jleveling@computing.dcu.ie</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="laboratory">Centre for Next Generation Localization</orgName>
								<orgName type="institution" key="instit1">Dublin</orgName>
								<orgName type="institution" key="instit2">City University</orgName>
								<address>
									<settlement>Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,333.03,116.90,69.02,8.88"><forename type="first">Gareth</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
							<email>gjones@computing.dcu.ie</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="laboratory">Centre for Next Generation Localization</orgName>
								<orgName type="institution" key="instit1">Dublin</orgName>
								<orgName type="institution" key="instit2">City University</orgName>
								<address>
									<settlement>Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.07,74.42,417.18,12.52;1,270.02,90.50,55.25,12.52">DCU @ CLEF-IP 2009: Exploring Standard IR Techniques on Patent Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9EBE106967B929BA8714D839E898D3C7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing -Indexing methods</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>Performance, Experimentation Patent Retrieval</term>
					<term>Query Formulation</term>
					<term>CLEF-IP track</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the experiments and results for our participation in CLEF-IP 2009, which in newly launched this year. Our work applied standard information retrieval (IR) techniques to patent search. Different experiments tested various methods for the patent retrieval, including query formulation, structured index, weighted fields, filtering, and relevance feedback. Some methods did not show expected good retrieval effectiveness such as blind relevance feedback, other experiments showed acceptable performance. Query formulation was the key task for achieving better retrieval effectiveness, and this was performed through giving some higher weights to the text in certain fields. For the best runs, the retrieval effectiveness is still lower than IR applications for other domains illustrating the fact of the difficulty of patent search. The official results have shown that among fifteen participants we achieved the seventh and the fourth ranks from the mean average precision (MAP) and recall point of view, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper presents the participation of Dublin City University (DCU) in the CLEF-IP track 2009. Our participation was in the main task which is retrieving patents prior art. The aim for a given patent (which is considered as the topic) is to retrieve all of its citations automatically <ref type="bibr" coords="1,358.95,519.34,10.68,8.88" target="#b0">[1]</ref>. Only three runs were submitted, but more unofficial experiments were performed by us for this task. Fifteen participants have submitted 48 runs; according to MAP scores, our best run achieved the seventh rank across participants and the 22 nd across all runs. However, according to recall scores, our best run achieved the fourth rank across all participants and the fourth rank across all 48 submitted runs.</p><p>The paper is organized as follows: Section 2 describes the data for the task and an analysis of its nature; Section 3 presents all the experiments for this task; Section 4 shows the results; then Section 5 discusses these results; Finally, Section 6 concludes the paper and provides possible future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data Pre-Processing</head><p>More than 1.9M documents were provided representing different versions of 1M patents between the period 1985 and 2000. For our experiments, all different versions for a single patent were merged in one document with fields were updated from its latest versions. Patent structure is very rich, and some fields are presented in three languages (English "EN", German "DE", and French "FR") namely the title and claims. Some fields are for formal use only, hence, candidate relevant fields are extracted from these final merged versions for indexing. Only the patent title, abstract, description, claims, and classifications fields are extracted from the patents. However, many patents lack some of these fields. The only fields that are present in all patents are the title and the classifications; other fields are omitted in some patents (Figure <ref type="figure" coords="1,358.46,720.69,3.63,8.88" target="#fig_0">1</ref>). The "description" field is related to "claims" field, as if "claims" field is missing, then "description" is missing too, however, the opposite in not true, as some documents contain a "claims" field while the "description" field is missing. The "abstract" field is an optional part that is present in some patents.</p><p>Figure <ref type="figure" coords="2,117.75,96.14,4.96,8.88" target="#fig_0">1</ref> shows that about 23% of the patents do not contain the claims and description fields, while 73% of them are formed of titles only.</p><p>In order to avoid language problems, the English fields only are extracted from these fields. This step will lead to the loss of extra 0.23×(0.23+0.9) = 7.4% of the patents which lack the claims and description fields (these are the German and French patents with claims only in one language). In addition, all non-English patents lack the abstract, description fields. The final outcome resulted in 30% of the collection suffering from missing most of the fields, this portion of the collection is mostly comprises the titles only with a small portion of it containing abstracts too.</p><p>In order to maintain the full structure and overcome the lack of some fields in some patents, the abstract (if it exists) is copied to the description and claims fields; otherwise, the title is used instead. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimentations</head><p>In this Section, different experiments for indexing and searching the data are discussed. After merging different versions of patents and extracting the relevant fields, some pre-processing is performed for the patent text in order to prepare it for indexing. Different methods were used for query formulation to search the collection.</p><p>Many experiments were performed on the training topics provided by the task organizers, however, a small number were submitted on the test data for the official runs. The training set contains 500 patent topics, which was sufficient to compare different methods and select the best for the official submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Text Pre-Processing</head><p>Patent text contains many formulas, numeric references, chemical symbols, and patent-specific words (such as method, system, or device) that can cause a negative effect on the retrieval process. Some filtering of the text is done by removing:</p><p>1. Predefined stop words 1 , 2. Digits, and 3. Field-specific stop words To get the fields stop words, the field frequency for terms is calculated separately for each field. The field frequency for a term "T" in field "X" is the number of fields of type "X" containing the term "T" across all documents. For each field, all terms with field frequency higher than 5% of the term with highest field frequency for this field are considered as stop words. For example, for "title" field, the following words have been identified as stop words {method, device, apparatus, process, etc}; for another field such as "claims", the following words have been identified as stop words {claim, according, wherein, said, etc}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Structured Indexing</head><p>For indexing, Indri <ref type="bibr" coords="2,153.39,697.89,11.70,8.88" target="#b4">[5]</ref> was used to create a structured index for patents. A structured index keeps the fields information in the index. The index was structured as shown in Figure <ref type="figure" coords="2,354.49,709.41,3.76,8.88" target="#fig_2">2</ref>. This structured index allows searching specific fields instead of searching in the full document. It also allows giving different weights for each field while searching. As shown in Figure <ref type="figure" coords="3,410.29,84.74,3.76,8.88" target="#fig_0">1</ref>, "desc1" and "claim1" are sub-fields for the description and claims fields respectively. "desc1" is the first paragraph in the description field; typically it carries useful information about the field of the invention and what the invention is about. "claim1" is the first claim in the claims sections, and it describes the main idea of the invention in the patent. The field "class" carries the IPC classification [6] information of the patent, the three top classification levels are used, the rest levels are discarded (example: B01J, C01G, C22B).</p><p>As mentioned earlier, for patents that lacks of some fields, the empty fields are filled with the abstract if exists or with the title otherwise. Pre-processing includes stemming using the Porter stemmer <ref type="bibr" coords="3,446.43,165.13,10.68,8.88" target="#b3">[4]</ref>.  </p><formula xml:id="formula_0" coords="3,70.95,186.05,20.96,5.37">&lt;DOC&gt;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Query Formulation</head><p>Query formulation can be seen as one major task in patent retrieval <ref type="bibr" coords="3,356.43,354.12,10.68,8.88" target="#b2">[3]</ref>. As a full patent is considered as the topic, extracting the best representative text with the proper weights is the enabling key for good retrieval results. Using the full patent as a query is not practical due to the huge amount of text in one patent. Hence, text from certain fields was extracted and tested to search the structured index with different weights to different fields. Various combinations of fields were tested with different weights with enabling/disabling filtering using third level classification and enabling/disabling blind relevance feedback Error! Reference source not found.. The patent topic text was pre-processed in the same way as in the indexing phase by removing stop words and digits, in addition to removing special characters, symbols and all words of small length (one or two letters).</p><p>Similar to the indexed documents, only English parts are used, which means all non-English patent topics will miss the abstract and description fields to be used in search. However, the amount of text present in claims and titles should be sufficient to create a representative query. In patent topics, there were no missing fields, and all claims and titles are present in the three languages. The following fields were used with different combinations to construct the queries {"title", "abstract", "desc1" (first line in description), "claim_main" (first sentence in first claim), "claim1" (first claim), and "claims"}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Some of the tested experiments seemed to be ineffective with retrieval. Blind relevance feedback and structured search have negative impact on the results. All experiments with blind relevance feedback led to a degradation in the MAP to around 60% of the original runs without feedback, and this can stem from the low quality of the highly ranked results. On the other hand, structured retrieval was tested by searching each field in the patent topic to its corresponding field in the index. Different weights for fields were tested; however, all experiments led to lower MAP and recall than searching in the full index as a whole without directing each field to its correspondent. Since patent documents were treated as a full document neglecting its structure, patent topics which were used for formulating the queries were tested by giving different weights to the text in each field. Giving higher weight to text in "title", "desc1", and "claim_main" has been proven to produce the best results across all runs.</p><p>According to the tested experiments on the training data, three runs were submitted on the official topics with the same setup as the best results in training. The three runs are nearly the same but with minor modifications. The common setup for the three runs was as follows:</p><p>1. The patent document is treated as a full document, neglecting its structure. 2. English text only is indexed with stemming (Porter stemmer).</p><p>3. Stop words are removed, in addition to digits and words of length less than two letters. 4. A query is formulated from the following fields with the following weights: a. 5×Title b. 1×Abstract (English topics only) c. 3×Desc1 (English topics only) d. 2×Claim_main e. 1×Claims 5. Additional bi-grams words with a frequency in the text higher than one were used in query. The text of the fields: "title", "abstract", "desc1", and "claim_main" was used for extracting the bi-grams words. The difference between the three runs is as follows: 1. 1 st Run: No filtering is performed 2. 2 nd Run: Filtering is performed for all results that do not match up to the third level classification of the patent topic (at least one common classification should be present) 3. 3 rd Run: The same as 2 nd run, but with removing all words in the query of length less than three Runs were submitted on the extra large collection that contains 10,000 patent topics. The average time for running this amount of topics was around 30 hours (about ten seconds on average for retrieving results of one topic).</p><p>Table <ref type="table" coords="4,114.40,234.13,4.96,8.88" target="#tab_1">1</ref> shows the results of the three submitted runs on three different collection sizes S: small (500 topics), M: medium (1,000 topics), and XL: extra large (10,000 topics) <ref type="bibr" coords="4,339.37,245.65,10.68,8.88" target="#b1">[2]</ref>. In Table <ref type="table" coords="4,396.62,245.65,3.76,8.88" target="#tab_1">1</ref>, it is shown that the 3 rd Run always gets the best results from the precision and recall perspective. The 1 st run always has the worst results, which shows that applying the filtering over the results based on the patent classification is always useful. For all runs (for official and training ones), the retrieval effectiveness is relatively low when compared to other IR tasks; this can stem from the nature of patent documents itself in addition to the task of finding cited patents which are relative to the patent topic from the conceptual point of view, not from the word matching. This is discussed in the next section. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>In this section some analysis is done trying to identify the reason behind the low retrieval effectiveness for the patent retrieval task. In order to analyze this problem, the overlap between each topic in the training data and its relevant cited patents is computed; in addition, the overlap between the topics and the top five ranked nonrelevant documents is calculated (the reason behind selecting the number "five" is that the average number of relevant documents for all topics is between five and six). The overlap is measured using two measures: 1) cosine measure between each two corresponding fields of the two compared patents. 2) Percentage of zero overlap (no shared words) between two corresponding fields of the two compared patents. The same preprocessing is done for all patents and topics, where stop words are removed including digits, and the comparison is based on the stemmed version of words. From Figure <ref type="figure" coords="4,296.18,624.58,4.96,8.88">3</ref> and<ref type="figure" coords="4,320.78,624.58,3.76,8.88">4</ref>, it seems that relying on common words between topics and relevant documents for patent retrieval is not the best approach. Figure <ref type="figure" coords="4,424.10,636.10,4.96,8.88">3</ref> shows that the cosine measure between the top ranked non-relevant documents to the topic is nearly double that of the relevant documents for all fields. The same is shown in Figure <ref type="figure" coords="4,292.58,659.01,3.76,8.88">4</ref>, where surprisingly, 12% of the relevant documents for topics have not any shared words in any field to the topics. This outcome has proven the importance of introducing different approaches for query formulation instead of relying on word matching in the patent topics only.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rel vs top non-rel docs m atching to topics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper, we described our participation in the CLEF-IP track 2009. Standard IR techniques were tested with some focusing on query formulation. Our experiments illustrated the challenge of patent search task, where some analysis showed that depending on word matching is not the best solution as in other IR applications. Our best result was through treating patents as a full document with some pre-processing by removing standard stop words in addition to patent-specific stop words; and on the query phase, text is extracted from patent topic with giving higher weights to some specific fields. Some additional experiments showed the poor effectiveness of using blind relevance feedback or using patent structure in index.</p><p>For future work, more investigation is required for checking the best use of patent structure in both index and query phases; machine learning can be a useful approach for identifying the best weights for different fields. Furthermore, query expansion through the conceptual meaning of words is a potential approach to be tested. Finally, machine translation can be a good solution to overcome the problem of multi-lingual documents and queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgment</head><p>This research is supported by the Science Foundation Ireland (Grant 07/CE/I1142) as part of the Centre for Next Generation Localisation (CNGL) project</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,182.42,395.28,230.33,8.88"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Percentage of patents with claims in EN\FR\DE</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,70.95,747.90,2.51,4.50;2,75.51,749.34,175.62,7.17;2,283.58,294.30,19.27,7.84;2,297.02,316.74,19.27,7.84;2,207.26,307.98,19.27,7.84;2,148.35,253.70,33.46,6.19;2,148.35,263.78,22.48,6.19;2,148.35,273.98,22.48,6.19;2,347.77,257.84,14.23,5.81;2,417.97,258.20,14.23,5.81;2,410.17,228.43,20.49,4.28;2,410.17,235.51,31.90,4.28;2,288.38,244.32,10.67,4.37;2,267.26,235.20,7.79,4.37;2,250.70,237.00,10.79,4.37;2,420.49,355.85,10.84,4.44;2,394.45,345.05,7.84,4.44;2,369.25,351.53,10.84,4.44"><head></head><label></label><figDesc>1 http://members.unine.ch/jacques.savoy/clef/index.html</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,190.94,315.00,213.46,8.88"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Structured text for a patent in TREC format</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="5,84.63,182.10,426.10,8.03"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Cosine measure between fields of topics and the corresponding ones in relevant and top retrieved documents</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="5,77.79,315.12,439.81,8.88;5,207.98,326.64,179.31,8.88"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Percentage of fields with zero common (shared) words between that of topics and the corresponding ones in relevant and top retrieved documents</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,76.95,337.68,441.36,161.87"><head>Table 1 :</head><label>1</label><figDesc>Three submitted runs results for three different collection sizes (P: Precision, P&lt;n&gt;: Precision at n, R: Recall, and R&lt;n&gt;: Recall at n)</figDesc><table coords="4,76.95,362.28,441.36,137.27"><row><cell cols="2">Size Run #</cell><cell>P</cell><cell>P5</cell><cell>P10</cell><cell>P100</cell><cell>R</cell><cell>R5</cell><cell>R10</cell><cell>R100</cell><cell>MAP nDCG</cell></row><row><cell></cell><cell cols="10">Run 1 0.0032 0.0916 0.0694 0.0184 0.5380 0.0818 0.1222 0.3086 0.0815 0.4142</cell></row><row><cell>S</cell><cell cols="10">Run 2 0.0038 0.1008 0.0774 0.0208 0.6156 0.0908 0.1373 0.3526 0.0896 0.4120</cell></row><row><cell></cell><cell cols="10">Run 3 0.0038 0.1036 0.0768 0.0208 0.6236 0.0952 0.1363 0.3531 0.0913 0.4183</cell></row><row><cell></cell><cell cols="10">Run 1 0.0032 0.0984 0.0709 0.0183 0.5289 0.0916 0.1302 0.3135 0.0891 0.4265</cell></row><row><cell>M</cell><cell cols="10">Run 2 0.0037 0.1074 0.0785 0.0203 0.6071 0.1013 0.1440 0.3504 0.0970 0.4254</cell></row><row><cell></cell><cell cols="10">Run 3 0.0037 0.1092 0.0785 0.0204 0.6128 0.1032 0.1433 0.3516 0.0979 0.4264</cell></row><row><cell></cell><cell cols="10">Run 1 0.0033 0.1081 0.0768 0.0185 0.5438 0.1010 0.1405 0.3188 0.0968 0.4281</cell></row><row><cell>XL</cell><cell cols="10">Run 2 0.0038 0.1169 0.0839 0.0208 0.6240 0.1102 0.1546 0.3608 0.1074 0.4291</cell></row><row><cell></cell><cell cols="10">Run 3 0.0038 0.1173 0.0840 0.0208 0.6267 0.1107 0.1546 0.3612 0.1074 0.4266</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,88.95,596.79,435.49,8.03;5,88.95,607.11,99.04,8.03" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,196.47,596.79,277.40,8.03">A methodology for building a patent test collection for prior art search</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Graf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Azzopardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,482.07,596.79,42.37,8.03;5,88.95,607.11,74.21,8.03">EVIA-2008 Workshop, NTCIR-7</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,88.95,617.55,315.75,8.03" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,203.79,617.55,129.02,8.03">CLEF-IP 2009 Evaluation Summary</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Piroi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Roda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Zenz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,339.74,617.55,40.65,8.03">CLEF 2009</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,88.95,627.71,435.50,8.18;5,94.95,638.03,429.49,8.18;5,88.95,648.35,80.08,8.18" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,240.50,627.86,169.96,8.03">Overview of patent retrieval task at NTCIR-4</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fujii</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Iwayama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,428.52,627.71,95.93,8.18;5,94.95,638.03,394.83,8.18">Proceedings of the fourth TCIR workshop on evaluation of information retrieval, automatic text summarization and question answering</title>
		<meeting>the fourth TCIR workshop on evaluation of information retrieval, automatic text summarization and question answering<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-04">June 2-4. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,88.95,658.67,307.10,8.18" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,135.27,658.82,122.72,8.03">An Algorithm for Suffix Stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,263.89,658.67,32.40,8.18">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,88.95,669.26,435.41,8.03;5,88.95,679.43,284.34,8.18" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,283.34,669.26,237.49,8.03">Indri: A language model-based search engine for complex queries</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,98.79,679.43,248.32,8.18">Proceedings of the International Conference on Intelligence Analysis</title>
		<meeting>the International Conference on Intelligence Analysis</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
