<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,230.04,90.85,135.10,12.19;1,132.24,106.93,330.62,12.19">Are Passages Enough? The MIRACLE Team Participation at QA@CLEF2009</title>
				<funder ref="#_jb9vzBY">
					<orgName type="full">Research Network MAVIR</orgName>
				</funder>
				<funder ref="#_emVQS8g">
					<orgName type="full">Spanish Ministry of Education</orgName>
				</funder>
				<funder>
					<orgName type="full">Regional Government of Madrid</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,155.16,149.85,52.24,8.74"><forename type="first">María</forename><surname>Teresa</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Carlos III de Madrid</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,209.68,149.85,51.01,8.74"><forename type="first">Vicente</forename><surname>-Díez</surname></persName>
							<email>tvicente@inf.uc3m.es</email>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Carlos III de Madrid</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,267.46,149.85,94.64,8.74"><forename type="first">César</forename><surname>De Pablo-Sánchez</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Carlos III de Madrid</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,368.99,149.85,66.98,8.74"><forename type="first">Paloma</forename><surname>Martínez</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Carlos III de Madrid</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,200.64,161.37,98.37,8.74"><forename type="first">Julián</forename><forename type="middle">Moreno</forename><surname>Schneider</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Carlos III de Madrid</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,305.76,161.37,88.88,8.74"><forename type="first">Marta</forename><forename type="middle">Garrote</forename><surname>Salazar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Carlos III de Madrid</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,230.04,90.85,135.10,12.19;1,132.24,106.93,330.62,12.19">Are Passages Enough? The MIRACLE Team Participation at QA@CLEF2009</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C1BB9E8F9B7702AC1D6BD0D7C8EFDEE5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries Question Answering, Spanish, legal domain, temporal indexing, temporal normalization 1</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper summarizes the participation of the MIRACLE team in the Multilingual Question Answering Track at CLEF 2009. In this campaign, we took part in the monolingual Spanish task at ResPubliQA@CLEF 2009 and submitted two runs. We have adapted our QA system which has been evaluated in EFE and Wikipedia to the new JRC-Acquis collection and the legal domain. We tested the use of answer filtering and ranking techniques to a base system using passage retrieval with no success. Our run using question analysis and passage retrieval obtained a global accuracy of 0.33 while the addition of an answer filtering step obtained 0.29. We provide an initial analysis of the results across the different questions types while we research the reason why it is difficult to leverage previous QA techniques. A different focus of our work has been on temporal reasoning applied to question answering and also detailed discussion of this issue in the new collection and analysis of the questions is provided.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>We describe the MIRACLE team participation in the ResPubliQA exercise at the Multilingual Question Answering Track at CLEF 2009. The MIRACLE team is a consortium formed by three universities from Madrid, (Universidad Politécnica de Madrid, Universidad Autónoma de Madrid and Universidad Carlos III de Madrid) and DAEDALUS, a small and medium size enterprise (SME). We submitted two runs for the Spanish monolingual subtask which summarize our attempts to adapt our QA system to the new requirements of the task. This year, the main task departed from previous exercises in an attempt to explore new domains, question types and multilingual experiments. The change in application domain has been triggered by the use of the JRC-Acquis document collection which is formed by European legislation translated in several EU languages. This fact raises the problem of dealing with legal language which includes richer terminology and is considerably more complex than news or academic language used in EFE and Wikipedia collections. Moreover, new kind of information needs are required to be solved which has motivated the inclusion of question asking for objectives, motivations, procedures, etc. in addition to the traditional factual and definitional questions. The new types of questions often required longer answers and therefore the expected response of the system has been fixed again at the paragraph level. Nevertheless it should be possible to take advantage of answer selection techniques developed in previous campaigns. This has been in fact one of the hypothesis we would like to test with our participation. Unfortunately, our experiments in this line have not been successful and we have not found configurations that performed substantially better than our baseline. A different aspect of our work has centered on the use of temporal information in the process of QA and we report results for different indexing configurations. Finally, a global objective was to enlarge the capabilities of the QA system and advance towards an architecture that allows domain adaptation and multilingual processing.</p><p>The rest of the paper is structured as follows, the second section describes the system architecture with special attention paid to the novelties introduced this year, Section 3 introduces the submitted runs and analyzes the results. Finally, conclusions and future work are presented in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Description</head><p>The system architecture is similar to our previous system <ref type="bibr" coords="2,323.73,153.57,11.68,8.74" target="#b1">[2]</ref> and is based on a pipeline which analyzes questions, retrieves documents and performs answer extraction based on linguistic and semantic information. Different strategies can be used depending on the type of the question and the expected type of the answer. The architectural schema is shown in Figure <ref type="figure" coords="2,233.49,188.08,3.76,8.74">1</ref>. A number of modules have been modified, extended or reorganized in order to adjust for the requirements of the task and the legal domain. Other modules have been included to carry new experiments. The main changes performed in the system are outlined:</p><p>Adding parsers for the new collections as well as supporting the indexing of passages.</p><p>The evaluation procedure was modified to work for passages and a fallback strategy for passages was included.</p><p>New rules have been developed for Question Analysis, Question Classification and Answer Filtering for the legal domain using the development set. Query generation has been adapted to the domain and page heuristics for Wikipedia removed. Temporal Management was added to normalize temporal expressions and integrated into language analysis and indexing routines. New functionality for mining acronyms offline and add them to query generation. The ranking module was redesigned for modularity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Indexes</head><p>Indexes are really important for QA as obtaining a good retrieval subsystem can considerably improve the final results of the QA system. Due to the change in the document collection, all IR indexes have been newly created using Lucene as the retrieval engine. To accomplish the task of storing the relevant information as appropriately as needed, we have designed two different document types or indexing units:</p><p>• Document, where all the information related to title, note and the text of a collection file is stored.</p><p>• Paragraph, which store each paragraph, the title and the notes in a different document. Lucene uses a length document normalization term in the retrieval score which was arguably of no help in the case of paragraph scoring because paragraphs are expected to have more uniform lengths. Both types of indexes, with length normalization and without were tested.</p><p>all our experiments previous to the submission the paragraph or passage index worked better than the</p><p>• Simple Index, where the text analyzer used is a simple analyzer adapted for Spanish. It makes grammar</p><p>• s a recognition and normalization of time expressions. These time Finally, other modifications required the query generation process to be changed to use the same analyzer that he idea of a rule engine, was initially considered for classifying question types; later, it has also been used not he change of linguistic domain meant some changes in the new rules. Below, we present an example of a new his rule has been created to detect the topic in definition questions. In most of them, the topic in the answer</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Temporal Management</head><p>the temporal question answering (TQA) as the specialization of the QA task in which In document index. Besides, we also created different index types regarding the analysis, characterized by the linguistic analyzer used in each case:</p><p>based parsing, stems words using a snowball-generated stemmer, removes stop words, replaces accented characters in the ISO Latin 1 character set and converts text into lower case. All the texts are stored in the same field: text. Temporal Index, which add expressions are normalized and included in the index. Texts are also stored also in the field text.</p><p>was used to create the index.</p><p>T only in the Question Classification module, but also in the Answer Filter, Timex Analyzer and Topic Detection ones <ref type="bibr" coords="3,91.72,293.37,10.63,8.74" target="#b1">[2]</ref>. The rules have a left part that expresses a pattern and a right part specifying the actions to be taken each time the pattern is found. The pattern could refer to lexical, syntactic and/or semantic elements.</p><p>T rule, developed to handle the extraction of definitions on this year corpus: EXISTENTIAL QUESTI WORD_I(N, OBTAIN_FOCUS()) AND (WORD_I(N+1, ":") OR WORD_I(N+1, "\"") AND WORD_I(N-1, "\"") OR WORD_I(N+1, "\"") AND WORD_I(N+2, ":") AND WOD_I(N-1, "\"")) THEN ANS END T paragraph was written in quotation marks and/or followed by colon. This rule locates the topic of the question and looks for it in the source documents. Some authors have defined questions have some features that denote temporality <ref type="bibr" coords="3,293.57,624.15,10.64,8.74" target="#b3">[4]</ref>, as well as a means for providing short and focused answers to temporal information needs formulated in natural language <ref type="bibr" coords="3,354.79,635.67,10.65,8.74" target="#b5">[6]</ref>. Previous work has already faced up to this problem for the treatment of other languages, like in <ref type="bibr" coords="3,307.03,647.19,11.72,8.74" target="#b6">[7]</ref> or <ref type="bibr" coords="3,333.52,647.19,10.64,8.74" target="#b7">[8]</ref>, or also in Spanish <ref type="bibr" coords="3,427.85,647.19,10.60,8.74" target="#b2">[3]</ref>. Temporal questions can be classified into 2 main categories according to the role of temporality in their resolution:</p><p>• Temporally Restricted (TR) questions are those containing some time restriction: "¿Qué resolución • (TA) are those whose target is a temporal expression or a date:</p><p>fue adoptada por el Consejo el 10 de octubre de 1994?" ("What resolution was adopted by the Council on 10 October 1994?")</p><p>Questions with a Timex Answer "¿Cuándo empieza la campaña anual de comercio de cereales?" ("When does the marketing year for cereals begin?")</p><p>In this campa n, temporal management preserves the approach taken by the MIRACLE QA system participating everal adjustments were made in the temporal expressions recognition, resolution and normalization integrated een considered potentially more sified, case of TA queries, the module must favour a</p><p>As a nov ieval approach</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acronym mining</head><p>f the collection, a large number of questions were expected to be expansion of acronyms, e implemented a simple offline procedure to mine acronyms by scanning the collection and searching for a nce we have cleaned the file, we index the acronyms and their expansions separately to be able to search by index is used in two different places in the QA system: nd adds searching terms to the query that is sent to • s the text extracted from the paragraph to determine if that paragraph</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer Filter and Passage Fallback Strategy</head><p>rocess the result list from the information retrieval module he possibility of getting no answer from the answer filter led to the development of a module that simply ig in CLEF 2008 <ref type="bibr" coords="4,132.85,84.93,10.64,8.74" target="#b1">[2]</ref>. This decision is based on later complementary work that was made in order to evaluate the QA system performance versus a baseline system without temporal management capabilities <ref type="bibr" coords="4,488.36,96.45,10.64,8.74" target="#b8">[9]</ref>. The experiments showed that additional temporal information management can quantitative and qualitatively benefit the results. This led us to predict that the use of such strategies could enrich future developments. S system to enhance its coverage on the new collections. Similarly to the previous version, the date of creation of each document is adopted as the reference date, needed to resolve the relative expressions that contains. In JRC-Acquis documents this information is provided by the "date.created" attribute. Question analysis, indexes generation and answer selection modules have b influential for achieving better results by means of the application of temporal management. They have been slightly adapted to the requirements of this year's competition, keeping the essence of their functionality.</p><p>During question analysis process, queries, including those with temporal features, are clas distinguishing between TR and TA queries. If a TA query is detected, it determines the granularity of the expected answer (complete date, only year, month, etc.</p><p>).</p><p>The answer selector is involved in two directions: in the temporal answer, whereas if it manages TR queries, it applies extraction rules based on the temporal inference mechanism and demotes the candidates not fulfilling the temporal restrictions.</p><p>elty, this year we have created more sophisticated indexes according to the paragraph retr of the competition. In some configurations, the normalized resolution of temporal expressions is included in the index instead of the expression itself. The main objective is to assess the behaviour of the QA system using different index configurations, mainly focusing on the temporal queries of the collection.</p><p>Due to the nature o especially about organizations. On the other hand, the recall of the information retrieval step could be improved by including the acronym and their expansion in the query. W pattern which introduces a new entity and provides their acronym between parentheses. Then, results are filtered in order to increase their precision. First, only those associations that occur at least twice in the corpus are considered. As parentheses often convey other relations like persons and their country of origin, another filter removed countries (Spain) and their acronyms (ES) from the list. Finally, some few frequent mistakes were manually removed and acronyms with more than one expansion were also checked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>O acronym or by expansion.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The</head><p>• Query Generation, where it analyzes the question a the document collection index.</p><p>Answer Filter, where it analyze contains the acronym (or the expansion) and if so, identifies the paragraph as correct answer.</p><p>This module, previously called Answer Extractor, p and selected chunks to form a possible candidate answer. In previous years, this module was designed to extract answers selected from the document. In this campaign, the answer must be the complete text of a paragraph therefore, this year the module works as a filter which removes passages with no answers. The kind of linguistic rules used last year to perform answer extraction has been adapted and new rules to detect acronyms, definitions as expressed in the new corpora and new rules for temporal questions have been developed.</p><p>T creates answers from the retrieved documents. This module is called Passage Fallback Strategy. It takes the documents returned by the information retrieval module and generates an answer from every document. The way of generating the indexes (concretely the paragraph index) makes possible the functionality of this module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation module</head><p>unt part of the development process of the QA system. In order to develop and test the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and results</head><p>subm ual Spanish task. They correspond to the configurations of the system on passage retrieval using the simple index. Question</p><p>• ge fallback strategy after number of additional configurations were also tested but no improvements over the baseline were found</p><p>Evaluation is a paramo system the English development test provided by CLEF organizers was translated to Spanish and a small goldstandard with answers was developed. Mean Reciprocal Rank (MRR) and Confidence Weighted Score (CWS) were consistently used to compare the outputs of the different configurations with the development gold standard. Periodically, the output and the XML logs of different executions were manually inspected to complete the gold standard and to detect integration problems.</p><p>We itted two runs for the monoling that yielded best results during our development using the translated question set. Paradoxically, both runs match with the simplest configurations that we have tested.</p><p>• Baseline (mira091eses): The system is based analysis is performed to generate queries and the acronym expansion is used. Baseline + Answer Filter (mira092eses): Adds answer filtering and the passa the previous passage retrieval.</p><p>A consistently. In fact, most of the additions seem to produce worse results on our development test. We considered different functions for Answer Ranking and Passage Re-ranking which we have tested for previous participations and some new ones. Different passage length normalization strategies were also applied to the indexes. Finally, a great deal of effort was devoted to the treatment of temporal expressions in question analysis, indexing and extraction and more detailed experiments are presented below. The results on the CLEF09 test set show similar conclusions to those we obtained during our development process, the baseline system using passage retrieval is hard to beat and in fact our second run provide lower accuracy. As in the case of our development experiments there are changes for individual answers of a number of questions but the overall effect is not positive.</p><p>After the evaluation, and using the larger test set of 500 questions we have decided to carry a class based analysis in order to understand the causes behind our unfruitful efforts. We have manually annotated the questions and grouped them in 6 main question types. In contrast with our expectations, the performance of the second submitted run is also worse for the factual and definition questions. As we have considered these questions types in previous evaluations we expected to have better coverage in the Answer Filter and therefore an improvement. Similar behaviour has been observed across answer types for factual questions, being the class of TIMEX questions the only where the more complex configuration really improves.</p><p>Our analysis of the errors show that further work is needed to be able to cope with the complexities of the domain. For example, questions are in general more complex and include a large number of domain specific terminologies that our question analysis rules do not handle correctly. The process of finding the focus of the question which is crucial for question classification is specially error prone. Answer Extraction needs also further adaptation to the domain for factual questions as the typology of NE and generalized NE has not wide coverage. Problems with definitions are rooted more deeply and probably require the use of different specialized retrieval strategies. This year evidence along with previous experiments seems to support that definitions depend deeply on the stylistics of the domain. Finally, new question types would require further study of techniques that help to improve the classification of passages as bearing procedures, objectives, etc. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation of temporal questions</head><p>With the aim of evaluating the temporal management capabilities of the QA system, we decided to extract the temporal questions from the whole corpus. 46 out of 500 queries denote temporal information, that means a 9,20% over the total. 24 of them are TR questions, whereas TA queries are 22 (4,80% and 4,40% out of the total, respectively). This subset has been studied, evaluating the correctness of the returned answers by two different configurations of the QA system. The results are presented in Table <ref type="table" coords="7,342.46,328.30,3.76,8.74" target="#tab_5">3</ref>.  As we can observe, better figures are obtained by the set of TQ in both runs. There is no significant difference between TA and TR queries in the first run, while in the second one they achieve a difference of 22%. In our opinion, the second configuration, with answer filtering and answer creation, enhances precision for TA queries, whereas for TR queries, temporal restrictions introduce noise that the system is not able to solve.</p><p>Non-submitted runs present similar configurations to the submitted ones, but adopting a different index generation and question analysis strategies. The approach consisted on the inclusion of normalized temporal expressions into the index, as well as in the question analysis process, aiming to increase recall. We tested the performance over the total corpus of questions, but worse results were achieved even if the study is restricted to temporal questions. Results are also presented in Table <ref type="table" coords="7,296.80,580.72,3.77,8.74" target="#tab_5">3</ref>, which show no improvement regarding the submitted runs. Performance difference between TA and TR queries remains stable, since the system has a better response to questions without temporal restrictions. The lost of accuracy can be due to the lack of a more sophisticated inference mechanism at the time of retrieval, capable of reasoning with different granularities in normalized dates format <ref type="bibr" coords="7,126.97,626.75,15.37,8.74" target="#b9">[10]</ref>. In addition, we suspect that answer selection module is not filtering candidate answers properly, so current inference mechanism gives more weigh to paragraphs containing dates matching with restrictions in the query, while the rest of terms lose relevancy. Though relative dates present a low frequency in the collections, they are not being correctly solved, as reference date, taken from that of the documents creation, is always set to the same value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Future Work</head><p>From our point of view, the new ResPubliQA exercise is a challenge for QA systems in two main facets of the problem domain adaptation and multilinguality. This year our efforts have focused on the first problem where we have ported the system and the techniques developed for EFE and Wikipedia to the new legal collection JRC-Acquis. However, our experiments, which are exemplified with the submitted runs, show that a system mainly based on passage retrieval performs quite well. Baseline passage retrieval results provided by the organizers <ref type="bibr" coords="8,508.26,96.45,16.33,8.74" target="#b10">[11]</ref> also support these. We are carrying further experiments using the larger test set in order to find how answer selection could help for ResPubliQA questions as well as the differences between passage retrieval alternatives.</p><p>Regarding our focus on temporal reasoning applied to QA we would explore how question temporal constraints can be integrated at other steps in the process. We expect to compare the effectiveness of temporal reasoning as constraints for filtering answers and for the purpose of re-ranking.</p><p>Finally, further work in the general architecture of the QA is expected to help in at least three areas: separation of domain knowledge from general techniques, adding different languages to the system and effective evaluation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,106.32,370.83,192.77,8.74;3,77.40,390.73,108.01,7.20;3,197.40,400.75,156.01,7.20"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example of rule for answer extraction RULE("definition") ON TYPE ("DEFINITION") AND</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,163.68,404.83,274.25,196.43"><head>Table 1 .</head><label>1</label><figDesc>Answer accuracy has been calculated as the ratio of questions correctly answered to the total number of questions. Only the first candidate answer is considered, rejecting the rest of possibilities.</figDesc><table coords="5,163.68,404.83,274.25,196.43"><row><cell></cell><cell>Question Analysis</cell><cell>Information Retrieval</cell></row><row><cell></cell><cell>Linguistic</cell><cell>Query</cell></row><row><cell></cell><cell>Analysis</cell><cell>Generation</cell><cell>Answe</cell></row><row><cell></cell><cell>Question</cell><cell>Information</cell></row><row><cell></cell><cell>Classification</cell><cell>Retrieval</cell></row><row><cell></cell><cell>Offline</cell><cell></cell></row><row><cell></cell><cell>Operations</cell><cell></cell></row><row><cell></cell><cell>Collection</cell><cell></cell></row><row><cell></cell><cell>Indexer</cell><cell></cell></row><row><cell>Document</cell><cell></cell><cell></cell></row><row><cell>Collection</cell><cell>Linguistic</cell><cell>Document Index</cell></row><row><cell></cell><cell>Analysis</cell><cell></cell></row><row><cell cols="3">Figure 3: mira091eses configuration (BL)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,106.32,420.75,140.04,8.74"><head>Table 1 :</head><label>1</label><figDesc>Results for submitted runs</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,91.86,75.09,407.04,167.80"><head>Table 2 :</head><label>2</label><figDesc>An analysis of runs by question type</figDesc><table coords="7,91.86,75.09,407.04,148.78"><row><cell>Question Type</cell><cell>mira091eses</cell><cell cols="2">mira092eses TOTAL</cell><cell>mira091eses Accuracy</cell><cell>mira092eses Accuracy</cell></row><row><cell></cell><cell>BL</cell><cell>BL-AF</cell><cell></cell><cell>BL</cell><cell>BL-AF</cell></row><row><cell>FACTUAL</cell><cell>54</cell><cell>48</cell><cell>123</cell><cell>0.44</cell><cell>0.39</cell></row><row><cell>PROCEDURE</cell><cell>22</cell><cell>15</cell><cell>76</cell><cell>0.28</cell><cell>0.20</cell></row><row><cell>CAUSE</cell><cell>43</cell><cell>44</cell><cell>102</cell><cell>0.42</cell><cell>0.43</cell></row><row><cell>REQUIREMENT</cell><cell>5</cell><cell>5</cell><cell>16</cell><cell>0.31</cell><cell>0.31</cell></row><row><cell>DEFINITION</cell><cell>16</cell><cell>12</cell><cell>106</cell><cell>0.16</cell><cell>0.11</cell></row><row><cell>OBJECTIVE</cell><cell>21</cell><cell>23</cell><cell>77</cell><cell>0.27</cell><cell>0.30</cell></row><row><cell>ALL</cell><cell>161</cell><cell>147</cell><cell>500</cell><cell>0.32</cell><cell>0.29</cell></row><row><cell>ALL -FACTUAL</cell><cell>107</cell><cell>99</cell><cell>377</cell><cell>0.28</cell><cell>0.26</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,106.32,457.41,345.92,8.74"><head>Table 3 :</head><label>3</label><figDesc>Results for temporal questions in the submitted runs and other configurations</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work has been partially supported by the <rs type="funder">Regional Government of Madrid</rs> by means of the <rs type="funder">Research Network MAVIR</rs> (<rs type="grantNumber">S-0505/TIC/000267</rs>) and by the <rs type="funder">Spanish Ministry of Education</rs> by means of the project <rs type="projectName">BRAVO</rs> (<rs type="grantNumber">TIN2007-67407-C3-01</rs>)</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_jb9vzBY">
					<idno type="grant-number">S-0505/TIC/000267</idno>
				</org>
				<org type="funded-project" xml:id="_emVQS8g">
					<idno type="grant-number">TIN2007-67407-C3-01</idno>
					<orgName type="project" subtype="full">BRAVO</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,91.74,314.91,432.83,8.74;8,95.28,326.43,429.21,8.74;8,95.28,338.01,429.13,12.21;8,95.28,353.37,43.41,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,127.26,326.43,315.00,8.74">The JRC-Acquis: A multilingual aligned parallel corpus with 20+ languages</title>
		<author>
			<persName coords=""><forename type="first">Steinberger</forename><surname>Ralf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruno</forename><surname>Pouliquen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anna</forename><surname>Widiger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Camelia</forename><surname>Ignat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tomaž</forename><surname>Erjavec</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Tufiş</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dániel</forename><surname>Varga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,449.46,326.43,75.03,8.74;8,95.28,338.01,342.55,12.21">Proceedings of the 5 th International Conference on Language Resources and Evaluation (LREC&apos;2006)</title>
		<meeting>the 5 th International Conference on Language Resources and Evaluation (LREC&apos;2006)<address><addrLine>Genoa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-05">2006. May 2006</date>
			<biblScope unit="page" from="24" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.74,370.89,432.62,8.74;8,95.28,382.35,429.14,8.74;8,95.28,393.87,429.22,8.74;8,95.28,405.39,400.64,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,232.02,382.35,292.39,8.74;8,95.28,393.87,69.84,8.74">The MIRACLE Team at the CLEF 2008 Multilingual Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Martínez-González</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>De Pablo-Sánchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Polo-Bayo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Vicente-Díez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Martinez-Fernández</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Martínez-Fernández</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,184.66,393.87,339.85,8.74;8,95.28,405.39,18.02,8.74">Proceedings of the 9th Workshop of the Cross-Language Evaluation Forum, CLEF 2008</title>
		<meeting>the 9th Workshop of the Cross-Language Evaluation Forum, CLEF 2008<address><addrLine>Aarhus, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-09-17">2008. September 17-19, 2008</date>
		</imprint>
	</monogr>
	<note>Revised Selected Papers. Series LNCS. to appear</note>
</biblStruct>

<biblStruct coords="8,91.74,423.41,432.70,10.46;8,95.28,436.65,48.09,8.74" xml:id="b2">
	<monogr>
		<ptr target="http://lucene.apache.org/,visited30/07/2009" />
		<title level="m" coord="8,91.74,424.71,264.98,8.74">The Apache Software Foundation</title>
		<imprint/>
	</monogr>
	<note>Apache Lucene project</note>
</biblStruct>

<biblStruct coords="8,91.74,454.17,432.67,8.74;8,95.28,465.69,216.68,8.74" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Saquete</surname></persName>
		</author>
		<title level="m" coord="8,141.42,454.17,349.31,8.74">Resolución de Información Temporal y su Aplicación a la Búsqueda de Respuestas</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>Universidad de Alicante</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Thesis in Computer Science</note>
</biblStruct>

<biblStruct coords="8,91.74,483.15,432.94,8.74;8,95.28,494.67,378.17,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,349.38,483.15,175.30,8.74;8,95.28,494.67,117.20,8.74">Splitting Complex Temporal Questions for Question Answering Systems</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Saquete</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Martínez-Barco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Muñoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Viñedo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,230.21,494.67,165.46,8.74">Proceedings of the ACL&apos;2004 Conference</title>
		<meeting>the ACL&apos;2004 Conference<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.74,512.13,380.88,8.74" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="8,151.98,512.13,206.67,8.74">Inference for temporal question answering Project</title>
		<author>
			<persName coords=""><surname>De Rijke</surname></persName>
		</author>
		<idno>OND1302977</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.74,529.65,432.70,8.74;8,95.28,541.17,429.03,8.74;8,95.28,552.63,296.55,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,257.28,529.65,267.16,8.74;8,95.28,541.17,157.56,8.74">University of Hagen at QA@CLEF 2006: Interpretation and normalization of temporal expressions</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hartrumpf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Leveling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,271.18,541.17,253.13,8.74;8,95.28,552.63,225.77,8.74">Results of the CLEF 2006 Cross-Language System Evaluation Campaign, Working Notes for the CLEF 2006 Workshop</title>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.74,570.15,432.68,8.74;8,95.28,581.67,247.22,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,224.22,570.15,167.73,8.74">Temporally Relevant Answer Selection</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,416.60,570.15,107.82,8.74;8,95.28,581.67,197.15,8.74">Proceedings of the 2005 International Conference on Intelligence Analysis</title>
		<meeting>the 2005 International Conference on Intelligence Analysis</meeting>
		<imprint>
			<date type="published" when="2005-05">May 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.74,599.13,432.74,8.74;8,95.28,610.65,418.02,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,235.98,599.13,288.50,8.74;8,95.28,610.65,143.68,8.74">Aplicación de técnicas de extracción de información temporal a los sistemas de búsqueda de respuestas</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Vicente-Díez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Martínez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,245.58,610.65,154.96,8.74">Procesamiento del lenguaje natural. N</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="25" to="30" />
			<date type="published" when="2009">marzo 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.74,628.17,432.65,8.74;8,95.28,639.63,143.18,8.74" xml:id="b9">
	<monogr>
		<idno>ISO8601</idno>
		<title level="m" coord="8,166.02,628.17,358.37,8.74;8,95.28,639.63,61.84,8.74">Data elements and interchange formats -Information interchange -Representation of dates and times</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>Third edition 2004</note>
</biblStruct>

<biblStruct coords="8,91.74,659.01,432.56,8.74;8,95.28,670.53,211.99,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,350.40,659.01,173.90,8.74;8,95.28,670.53,68.35,8.74">Information Retrieval Baselines for the ResPubliQA task</title>
		<author>
			<persName coords=""><forename type="first">Pérez</forename><forename type="middle">J</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,194.93,670.53,108.03,8.74">CLEF 2009 Working Notes</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
