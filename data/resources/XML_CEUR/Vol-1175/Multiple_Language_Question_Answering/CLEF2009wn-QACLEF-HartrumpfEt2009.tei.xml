<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,203.94,148.79,195.12,15.48;1,145.50,170.71,311.99,15.48;1,199.16,192.62,204.67,15.48">GIRSA-WP at GikiCLEF: Integration of Structured Information and Decomposition of Questions</title>
				<funder>
					<orgName type="full">Centre for Next Generation Localisation</orgName>
					<orgName type="abbreviated">CNGL</orgName>
				</funder>
				<funder ref="#_UsGYFnu">
					<orgName type="full">Science Foundation Ireland</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,267.25,226.91,64.32,8.64"><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
							<email>sven.hartrumpf@fernuni-hagen.dejohannes.leveling@computing.dcu.ie</email>
							<affiliation key="aff0">
								<orgName type="department">Intelligent Information and Communication Systems (IICS)</orgName>
								<orgName type="institution">University of Hagen (FernUniversit√§t in Hagen)</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,262.39,240.86,74.03,8.64"><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Centre for Next Generation Localisation (CNGL)</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<settlement>Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,203.94,148.79,195.12,15.48;1,145.50,170.71,311.99,15.48;1,199.16,192.62,204.67,15.48">GIRSA-WP at GikiCLEF: Integration of Structured Information and Decomposition of Questions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C12937AC3ED0DB7852AF7E074754905D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing-Indexing methods</term>
					<term>Linguistic processing</term>
					<term>H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-Query formulation</term>
					<term>Search process</term>
					<term>H.3.4 [Information Storage and Retrieval]: Systems and Software-Performance evaluation (efficiency and effectiveness) Experimentation, Measurement, Performance Geographic Information Retrieval, Question Answering, Questions Beyond Factoids, Temporal and Local Constraints, Cross-language Information Retrieval, Wikipedia</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the current GIRSA-WP system and the experiments performed for Gi-kiCLEF 2009. GIRSA-WP (GIRSA for Wikipedia) is a fully-automatic, hybrid system combining methods from question answering (QA) and geographic information retrieval (GIR). It merges results from InSicht, a deep (text-semantic) open-domain QA system, and GIRSA, a system for textual GIR.</p><p>For the second participation (the first participation was for the pilot task GikiP 2008), the GIR methods were adjusted by switching from a sentence-based retrieval to an abstract-based retrieval. Furthermore, geographic names and location indicators in Wikipedia articles were annotated before indexing. The QA methods were extended by allowing more general recursion with question decomposition. In this way, complex questions, which are frequent in GikiCLEF, can be answered by first answering several depending questions and exploiting their answers. Two new resources of structured information from Wikipedia were integrated, namely the categories assigned to articles and the infobox file from DBpedia, which is an automatic information extraction approach for Wikipedia data. Both resources were exploited by reformulating them in a restricted natural language form. In this way, they can be used as any other text corpus. A semantic filter in GIRSA-WP compares the expected answer type derived from the question parse to the semantics of candidate answers.</p><p>Three runs were submitted. The first one contained only results from the QA system; as expected it showed high precision, but low recall. The combination with results from the GIR system increased recall considerably, but reduced precision. The second run used a standard IR query, while the third run combined such queries with a Boolean query with selected keywords. The evaluation showed that the third run was significantly better than the second run. In both cases, the combination of the GIR methods and the QA methods was successful in combining their strengths (high precision of deep QA, high recall of GIR), but the overall performance leaves much room for improvements. For example, the multilingual approach is too simple. All processing is done in only one Wikipedia (the German one); results for the nine other languages are collected only by following the translation links in Wikipedia.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>GIRSA-WP (GIRSA for Wikipedia) is a fully-automatic, hybrid system combining methods from question answering (QA) and geographic information retrieval (GIR). It merges results from InSicht, an opendomain QA system <ref type="bibr" coords="2,169.88,353.04,10.58,8.64" target="#b2">[3]</ref>, and GIRSA, a system for textual GIR <ref type="bibr" coords="2,339.79,353.04,10.58,8.64" target="#b5">[6]</ref>. GIRSA-WP has already participated at the preceding pilot task, GikiP 2008 <ref type="bibr" coords="2,236.39,364.99,10.79,8.64" target="#b6">[7,</ref><ref type="bibr" coords="2,249.67,364.99,7.19,8.64" target="#b7">8]</ref>, and was improved based on this and other evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Description</head><p>The GIRSA-WP system used for GikiCLEF 2009 integrates two basic systems: a deep (text-semantic) QA system (InSicht) and a GIR system (GIRSA, GIR with semantic annotation). Each question is processed by both basic systems; GIRSA-WP filters their results semantically to improve precision and combines both result streams yielding a final result of Wikipedia article names, additional supporting article names (if needed), and supporting text snippets (the latter is not required by the GikiCLEF guidelines, but helpful for users).</p><p>The semantic filter checks whether the expected answer type (EAT) of the question and the title of a Wikipedia article are semantically compatible. This technique is widely known from QA for typical answer types such as PERSON, ORGANIZATION, or LOCATION. In our system, a concept (a disambiguated word) corresponding to the EAT is extracted from the question. This concept and the title of a candidate article are parsed by WOCADI <ref type="bibr" coords="2,214.98,540.43,10.58,8.64" target="#b1">[2]</ref>, a syntactico-semantic parser for German. The semantic representations (comprising the sort and the semantic features, see <ref type="bibr" coords="2,292.58,552.39,11.62,8.64" target="#b4">[5]</ref> for details on the semantic representation formalism MultiNet) of the semantic heads are unified. If this unification succeeds, the candidate article is kept; otherwise it is discarded. For example, from topic GC-2009-06 (Which Dutch violinists held the post of concertmaster at the Royal Concertgebouw Orchestra in the twentieth century?), the concept extracted as EAT is violinist.1.1, whose semantic representation belongs to the class human (human-object in MultiNet). There are 87 such semantic classes, which can also be disjunctively connected for underspecification or for so-called semantic molecules (or semantic families).</p><p>The retrieval in the GIR system works on the first few (two or three) sentences of the Wikipedia articles. Geographic names and location indicators in the articles were automatically annotated (see <ref type="bibr" coords="2,479.82,648.03,11.62,8.64" target="#b5">[6]</ref> for a discussion of this approach). As a result of our participation in GikiCLEF last year, we found that the full Wikipedia articles may be too long and indexing on a per-sentence basis does not provide enough context for matching. Therefore, we focused on the most important parts of the Wikipedia articles (to increase precision for GIRSA), and changed to full-document indexing.</p><p>For the GikiCLEF 2009 experiments, the questions were analyzed by the parser and sent to GIRSA and InSicht. In GIRSA, the top 1000 results were retrieved, with scores normalized to the interval [0, 1]; on average, GIRSA returned 153 and 395 documents per question for run 2 and 3, respectively (see Sect. 3).</p><p>For results returned by both GIRSA and InSicht, the maximum score was chosen (combMAX, <ref type="bibr" coords="3,476.49,112.34,10.45,8.64" target="#b0">[1]</ref>). Results whose score was below a given threshold were discarded and the semantic filter was applied to the remaining results. To obtain multilingual results, the German article names were 'translated' to the nine other languages using the Wikipedia linking between languages.</p><p>Besides the inter-wiki links, GIRSA-WP uses one further information type from Wikipedia: the categories assigned to articles. Note that other Wikipedia information types like intra-wiki (i.e. inter-article) links and Internet links are still ignored.</p><p>For the first time, two resources that contain structured information and are derived directly (categories) or indirectly (DBpedia) from Wikipedia were integrated into GIRSA-WP. The direct source of categories assigned to articles was exploited by extracting categories from the Wikipedia XML file. The resulting relations of the form in category(article title, category) were reformulated in the following form: article title ist ein/ist eine/ . . . category/'article title is a . . . category'. Some automatic corrections for frequent cases where the text would be syntactically and/or semantically incorrect were implemented. The remaining errors were largely unproblematic because the processing by InSicht's parser detects them and avoids incorrect semantic networks. In this way, 1.1 million semantic networks were generated for 1.5 million sentences derived from around 2 million in category relations.</p><p>The DBpedia data (more specifically: version 3.2 of the file infobox de.nt, the infobox information from the German Wikipedia encoded in N-Triples, a serialization of RDF; see http://wiki.dbpedia. org/ for details) is integrated similarly to the category data by rephrasing it in natural language. As there are many different relations in DBpedia only some frequent and relevant relations are covered currently. Each selected relation (currently 19) is linked to an abstract relation (currently 16) and a natural language pattern. For example, the triple &lt;http://dbpedia.org/resource/Andrea_Palladio&gt; &lt;http://dbpedia.org/property/geburtsdatum&gt; "1508-11-08"^^&lt;http://www.w3.org/2001/XMLSchema#date&gt; is translated to Andrea Palladio wurde geboren am 08.11.1508./'Andrea Palladio was born on 08.11.1508.' This generation process led to around 460,000 sentences derived from around 4,400,000 triples in the DBpedia file.</p><p>The detour via natural language for structured information resources is slower and can introduce some errors. But the advantage is that all resources are treated in the same way (and hence can be used in the same way to provide answer support etc.). In addition, the parser is able to deal with ambiguities (for example, names referring to different kinds of entities) that had to be resolved explicitly on the structured level otherwise.</p><p>The QA system (InSicht) compares the semantic representation of the question and the semantic representations of document sentences. To go beyond exact matching, InSicht applies many techniques, e.g. coreference resolution, query expansion by inference rules and lexicosemantic relations, and splitting the query semantic network at certain semantic relations. In the context of GikiCLEF, InSicht results (which are generated answers in natural language) must be mapped to Wikipedia article names; if this is not straightforward, the article name of the most important support is taken.</p><p>InSicht employed a new special technique called question decomposition (or query decomposition, see <ref type="bibr" coords="3,105.76,602.50,11.62,8.64" target="#b3">[4]</ref> for details) for GeoCLEF 2007, GeoCLEF 2008, and GikiP 2008. An error analysis showed that sometimes it is not enough to decompose a question once. For example, question GC-2009-07 (What capitals of Dutch provinces received their town privileges before the fourteenth century?) is decomposed into the subquestion Name capitals of Dutch provinces. and revised question Did subanswer-1 receive its town privileges before the fourteenth century? Unfortunately, the subquestion is still too complex and unlikely to deliver many (if any) answers. This situation changes if one decomposes the subquestion further into a subquestion (second level) Name Dutch provinces. and revised question (second level) Name capitals of subanswer-2 . InSicht's processing of question GC-2009-07 is illustrated in Fig. <ref type="figure" coords="3,464.66,686.19,3.74,8.64" target="#fig_0">1</ref>. Note that for readability the supporting texts are shortened and not translated. All subquestions and revised questions are shown in natural language, while the system operates mostly on the semantic (network) level.</p><p>Question decomposition, especially in its recursive form, is a very powerful technique that can provide answers and justifications for complex questions. However, the success rates at each decomposition combine in a multiplicative way. For example, if the QA system has an average success rate of 0. Middelburg (support from article Miniatuur Walcheren: . . . in Middelburg, der Hauptstadt von Seeland (Niederlande).) 1st answer to 2nd revised question level 2: . . . . . . 1st subanswer level 1:</p><p>Middelburg (note: answer to 1st revised question level 2 can be taken without change) 2nd subanswer level 1: . . . . . . 1st revised question level 1:</p><p>Wurde Middelburg vor dem vierzehnten Jahrhundert das Stadtrecht gew√§hrt? 'Did Middelburg receive its town privileges before the fourteenth century?' 2nd revised question level 1: . . . . . . answer to 1st revised question level 1:</p><p>Ja./'Yes.' (support from article Middelburg: 1217 wurden Middelburg durch Graf Willem I. . . . die Stadtrechte verliehen.) answer to 2nd revised question level 1: . . . . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1st answer:</head><p>Middelburg (support: three sentences, here from different articles, see supports listed in previous steps) 2nd answer: . . . . . .  decomposition as described above (leading to questions on three levels) will have an average success rate of 0.125 (= 0.5 ‚Ä¢ 0.5 ‚Ä¢ 0.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We produced three runs with the following experiment settings:</p><p>‚Ä¢ Run 1: only results from InSicht.</p><p>‚Ä¢ Run 2: results from InSicht and GIRSA, using a standard query formulation and a standard IR model (tf-idf) in GIRSA.</p><p>‚Ä¢ Run 3: results from InSicht and GIRSA, using a Boolean conjunction of the standard query formulation employed for GIRSA and (at most two) keywords extracted from the topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation and Discussion</head><p>InSicht achieved a higher precision than GIRSA-WP as a whole: 0.7895 compared to 0.1076 and 0.1442 for run 2 and 3, respectively (see Table <ref type="table" coords="5,249.77,430.29,3.88,8.64" target="#tab_1">1</ref>; the definition of the GikiCLEF score and other task details can be found in the GikiCLEF overview paper), but InSicht's low recall (30 correct answers compared to 107 and 142 for run 2 and 3, respectively) is still problematic as already seen in similar evaluations, e.g. GikiP 2008. As intended, InSicht aims for precision, GIRSA for recall, and GIRSA-WP tries to combine both in an advantageous way.</p><p>The overall performance of GIRSA-WP is not satisfying, yet. We made the following general observations:</p><p>‚Ä¢ On average, GikiCLEF questions seem to be harder than QA@CLEF questions from the years 2003 till 2008.</p><p>‚Ä¢ Especially the presence of temporal and spatial (geographical) constraints in GikiCLEF questions poses challenges for QA techniques.</p><p>‚Ä¢ As our question decomposition experiments indicate, correct answers can often not be found in one step; instead, subproblems must be solved or subquestions must be answered in the right order.</p><p>‚Ä¢ Indexing shorter (abstracted) Wikipedia articles returned a higher number of correct results (which was tested on some manually annotated data before submission). Similarly, the annotation of geographic entities in the documents (i.e. conflating different name forms etc.) ensured a relatively high recall.</p><p>‚Ä¢ The use of the query formulation which combines keywords extracted from the query with a standard IR query (run 3) increases precision (+34%) and recall (+33%) compared to the standard IR query formulation (run 2).</p><p>‚Ä¢ The system's multilingual approach is too simple because it relies only on the Wikipedia of one language (German) and adds results by following title translation links to other languages. Therefore for questions that have no or few articles in German, relevant articles in other languages cannot be found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Some resources</head><p>are not yet exploited to their full potential. For example, almost half of the category assignments are ignored (see Sect. 2). Similarly, many attribute-value pairs from infoboxes in DBpedia are not covered by GIRSA-WP currently. The cross-language aspect should be improved by processing at least one more Wikipedia version, preferably the largest one: the English Wikipedia.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,130.12,708.37,342.76,8.64"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of successful recursive question decomposition for GC-2009-07.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,168.30,119.13,266.41,71.56"><head>Table 1 :</head><label>1</label><figDesc>Evaluation results for all GIRSA-WP runs.</figDesc><table coords="5,168.30,141.19,266.41,49.50"><row><cell cols="5">Run Answers Correct answers Precision GikiCLEF score</cell></row><row><cell>1</cell><cell>38</cell><cell>30</cell><cell>0.7895</cell><cell>24.7583</cell></row><row><cell>2</cell><cell>994</cell><cell>107</cell><cell>0.1076</cell><cell>14.5190</cell></row><row><cell>3</cell><cell>985</cell><cell>142</cell><cell>0.1442</cell><cell>23.3919</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research was in part supported by the <rs type="funder">Science Foundation Ireland</rs> (Grant <rs type="grantNumber">07/CE/I1142</rs>) as part of the <rs type="funder">Centre for Next Generation Localisation (CNGL)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_UsGYFnu">
					<idno type="grant-number">07/CE/I1142</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,106.60,294.03,406.40,8.82;6,106.60,305.98,406.40,8.82;6,106.60,318.12,146.41,8.64" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,266.34,294.21,136.59,8.64">Combination of multiple searches</title>
		<author>
			<persName coords=""><forename type="first">Edward</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
		<idno>500-215</idno>
	</analytic>
	<monogr>
		<title level="m" coord="6,426.01,294.03,86.98,8.59;6,106.60,305.98,115.20,8.59">The Second Text REtrieval Conference (TREC-2)</title>
		<title level="s" coord="6,229.97,306.16,103.50,8.64">NIST Special Publication</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="243" to="252" />
		</imprint>
		<respStmt>
			<orgName>National Institute for Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="6,106.60,337.86,406.40,8.82;6,106.60,350.00,100.74,8.64" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="6,181.94,337.86,221.25,8.59">Hybrid Disambiguation in Natural Language Analysis</title>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Der Andere Verlag</publisher>
			<pubPlace>Osnabr√ºck, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,106.60,369.92,406.40,8.64;6,106.60,381.88,406.40,8.64;6,106.60,393.65,406.40,8.59;6,106.60,405.61,406.40,8.82;6,106.60,417.74,91.20,8.64" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,176.00,369.92,297.08,8.64">Question answering using sentence parsing and semantic network matching</title>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,106.60,393.65,406.40,8.59;6,106.60,405.61,102.24,8.59">Multilingual Information Access for Text, Speech and Images: 5th Workshop of the Cross-Language Evaluation Forum, CLEF</title>
		<title level="s" coord="6,303.04,405.61,140.21,8.59">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bernardo</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><surname>Magnini</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004. 2005</date>
			<biblScope unit="volume">3491</biblScope>
			<biblScope unit="page" from="512" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,106.60,437.67,406.40,8.64;6,106.60,449.45,406.40,8.82;6,106.60,461.40,323.19,8.82" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,178.21,437.67,191.33,8.64">Semantic decomposition for question answering</title>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,343.50,449.45,169.50,8.59;6,106.60,461.40,147.33,8.59">Proceedings of the 18th European Conference on Artificial Intelligence (ECAI)</title>
		<editor>
			<persName><forename type="first">Malik</forename><surname>Ghallab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Constantine</forename><forename type="middle">D</forename><surname>Spyropoulos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nikos</forename><surname>Fakotakis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nikos</forename><surname>Avouris</surname></persName>
		</editor>
		<meeting>the 18th European Conference on Artificial Intelligence (ECAI)<address><addrLine>Patras, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-07">July 2008</date>
			<biblScope unit="page" from="313" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,106.60,481.33,406.40,8.82;6,106.60,493.46,22.42,8.64" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="6,177.76,481.33,262.17,8.59">Knowledge Representation and the Semantics of Natural Language</title>
		<author>
			<persName coords=""><forename type="first">Hermann</forename><surname>Helbig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,106.60,513.39,406.40,8.64;6,106.60,525.34,406.40,8.64;6,106.60,537.12,406.40,8.82;6,106.60,549.07,406.40,8.82;6,106.60,561.03,302.18,8.82" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,268.67,513.39,240.81,8.64">Inferring location names for geographic information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,275.70,537.12,237.30,8.59;6,106.60,549.07,252.27,8.59">Advances in Multilingual and Multimodal Information Retrieval: 8th Workshop of the Cross-Language Evaluation Forum</title>
		<title level="s" coord="6,482.93,549.07,30.07,8.59;6,106.60,561.03,134.21,8.59">Lecture Notes in Computer Science (LNCS</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Valentin</forename><surname>Jijkoun</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thomas</forename><surname>Mandl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Henning</forename><surname>M√ºller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Anselmo</forename><surname>Pe√±as</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vivien</forename><surname>Petras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Diana</forename><surname>Santos</surname></persName>
		</editor>
		<meeting><address><addrLine>CLEF; Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007. 2008</date>
			<biblScope unit="volume">5152</biblScope>
			<biblScope unit="page" from="773" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,106.60,581.13,406.40,8.64;6,106.60,593.09,406.40,8.64;6,106.60,604.86,406.40,8.59;6,106.60,616.82,210.20,8.82" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,200.52,593.09,292.13,8.64">Getting geographical answers from Wikipedia: the GikiP pilot at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Diana</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nuno</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paula</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iustin</forename><surname>Dornescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yvonne</forename><surname>Skalban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,106.60,604.86,406.40,8.59;6,106.60,616.82,59.72,8.59">Results of the CLEF 2008 Cross-Language System Evaluation Campaign, Working Notes for the CLEF 2008 Workshop</title>
		<meeting><address><addrLine>Aarhus, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-09">September 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,106.60,636.92,406.40,8.64;6,106.60,648.88,406.40,8.64;6,106.60,660.65,406.40,8.82;6,106.60,672.61,345.53,8.82" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,196.93,648.88,311.67,8.64">GikiP at GeoCLEF 2008: Joining GIR and QA forces for querying Wikipedia</title>
		<author>
			<persName coords=""><forename type="first">Diana</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nuno</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paula</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iustin</forename><surname>Dornescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yvonne</forename><surname>Skalban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,117.08,660.65,317.27,8.59">Evaluating Systems for Multilingual and Multimodal Information Access, CLEF</title>
		<title level="s" coord="6,117.39,672.61,166.77,8.59">Lecture Notes in Computer Science (LNCS</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008. 2009</date>
			<biblScope unit="volume">5706</biblScope>
			<biblScope unit="page" from="894" to="905" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
