<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,110.52,135.22,354.45,15.49">The LIMSI participation to the QAst 2009 track</title>
				<funder ref="#_bCGt9KK">
					<orgName type="full">OSEO</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,128.88,168.98,75.06,8.97"><forename type="first">Guillaume</forename><surname>Bernard</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Spoken Language Processing Group</orgName>
								<orgName type="institution">LIMSI-CNRS</orgName>
								<address>
									<postBox>B.P. 133</postBox>
									<postCode>91403</postCode>
									<settlement>Orsay cedex</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,210.60,168.98,54.99,8.97"><forename type="first">Sophie</forename><surname>Rosset</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Spoken Language Processing Group</orgName>
								<orgName type="institution">LIMSI-CNRS</orgName>
								<address>
									<postBox>B.P. 133</postBox>
									<postCode>91403</postCode>
									<settlement>Orsay cedex</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,272.28,168.98,61.81,8.97"><forename type="first">Olivier</forename><surname>Galibert</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Spoken Language Processing Group</orgName>
								<orgName type="institution">LIMSI-CNRS</orgName>
								<address>
									<postBox>B.P. 133</postBox>
									<postCode>91403</postCode>
									<settlement>Orsay cedex</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,340.32,168.98,49.35,8.97"><forename type="first">Eric</forename><surname>Bilinski</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Spoken Language Processing Group</orgName>
								<orgName type="institution">LIMSI-CNRS</orgName>
								<address>
									<postBox>B.P. 133</postBox>
									<postCode>91403</postCode>
									<settlement>Orsay cedex</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,396.00,168.98,45.65,8.97"><forename type="first">Gilles</forename><surname>Adda</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Spoken Language Processing Group</orgName>
								<orgName type="institution">LIMSI-CNRS</orgName>
								<address>
									<postBox>B.P. 133</postBox>
									<postCode>91403</postCode>
									<settlement>Orsay cedex</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,110.52,135.22,354.45,15.49">The LIMSI participation to the QAst 2009 track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1AFD361B9542A438921E38E7A457032C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries Measurement, Performance, Experimentation Question answering, speech transcriptions</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present in this paper the three LIMSI question-answering systems on speech transcripts which participated to the QAst 2009 evaluation. These systems are based on a complete and multi-level analysis of both queries and documents. These systems use an automatically generated research descriptor. A score based on those descriptors is used to select documents and snippets. Three different methods are tried to extract and score candidate answers, and we present in particular a tree transformation based ranking method. We participated to all the tasks and submitted 30 runs (for 24 sub-tasks). The evaluation results for manual transcripts range from 27% to 36% for accuracy depending on the task and from 20% to 29% for automatic transcripts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Question Answering on Speech Transcripts track of the QA@CLEF task provides an opportunity to evaluate the specificity of speech transcriptions. In this paper, we present the work carried out on the QA system developed at LIMSI for the QAst evaluation. We especially describe an answer re-ranking method used in this system.</p><p>For the QAst 2009 evaluation <ref type="bibr" coords="1,193.68,638.30,10.60,8.97" target="#b6">[7]</ref>, 3 main tasks are defined :</p><p>-T1, QA in English European Parliament Plenary sessions -T2, QA in Spanish European Parliament Plenary sessions -T3, QA in French Broadcast News</p><p>In the previous QAst evaluations <ref type="bibr" coords="1,212.64,710.06,10.60,8.97" target="#b5">[6]</ref>, the questions were created by the evaluators from the documents. This year, the objective was to have more spontaneous questions. Native speakers were requested to read fragments of doucments and ask, using speech, questions about information related to but not content in the texts fragments.</p><p>For each of the tasks, four versions of the data collection were provided, consisting of one manual transcriptions and three different automatic transcription. Two different sets of questions were provided, one consisting of written questions and the other of manually transcribed semi-spontaneous oral questions. In total a minimum of 8 runs were expected per task, for a total of 24. LIMSI participated to the three tasks. Three systems were tested. Their main architecture is identical and they differ only in the answer scoring method :</p><p>-Distance-based answer scoring (primary method) -Answer scoring through bayesian modeling -Tree transformation-based answer re-ranking</p><p>The first method is used on all three tasks, the second is used on the T1 and T2 tasks and the third on the T3 task.</p><p>The section 2 presents the common architecture and the answer scoring methods. The section 3 is split into three parts : the description of the training and development data (section 3.1, a quick evaluation of the difficulty of the task (section 3.2), and finally the results of the three systems on the development and test data (sectiob 3.3). We compare these results to those obtained in the QAst 2008 evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The LIMSI QA systems</head><p>The common architecture is identical to the systems used in the previous evaluations and is fully described in <ref type="bibr" coords="2,120.36,435.02,10.69,8.97" target="#b3">[4]</ref>.</p><p>The same complete and multilevel analysis is carried out on both queries and documents. To do so, the query and the documents (which may come from different modalities -text, manual transcripts, automatic transcripts) are transformed into a common representation. This normalization process converts raw texts to a form where words and numbers are unambiguously delimited, punctuation is separated from words, and the text is split into sentence-like segments. Case and punctuation are reconstructed using a fully cased, punctuated four-gram language model <ref type="bibr" coords="2,245.64,518.78,11.60,8.97" target="#b0">[1]</ref> applied to a word graph covering all the possible variants (all possible punctuations permitted between words, all possible word cases). The general objective of this analysis is to find the bits of information that may be of use for search and extraction, called pertinent information chunks. These can be of different categories : named entities, linguistic entities (e.g., verbs, prepositions), or specific entities (e.g., scores). All words that do not fall into such chunks are automatically grouped into chunks via a longest-match strategy. The full analysis comprises some 100 steps and takes roughly 4 ms on a typical user or document sentence. The analysis identifies about 300 different types of entities. The analysis is hierarchichal, resulting in a set of trees. Both answers and important element of the questions are supposed to be annotated as one of these entities.</p><p>The first step of QA system itself is to build a search descriptor (SD) that contains the important elements of the question, and the possible answer types with associated weights. Some elements are marked as critical, which makes them mandatory in future steps, while others are secondary. The element extraction and weighting is based on an empirical classification of the element types in importance levels. Answer types are predicted through rules based on combinations of elements of the question. Documents are selected using this SD. Each element of the document is scored with the geometric mean of the number of occurrences of all the SD elements that appear in it, and sorted by score, keeping the n-best.</p><p>Snippets are extracted from the document using fixed-size windows and scored using the geometrical mean of the number of occurrences of all the DDR elements that appear in the snippet, smoothed by the document score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Distance-based answer scoring</head><p>In each snippet, all the elements whose type is one of the predicted possible answer types are candidate answers. A score S(r) is associated to each candidate answer r :</p><formula xml:id="formula_0" coords="3,158.52,252.07,29.71,9.96">S(r) =</formula><p>a∈Ar (w(a) max Ea (e,l)∈Ea </p><formula xml:id="formula_1" coords="3,274.80,240.65,139.78,28.66">w(l) (1+d(e,a)) α ) 1-γ S p (a) γ C d (r) β C p (r) δ</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Answer scoring through bayesian modeling</head><p>We tried a preliminary method of answer scoring built upon a bayesian modeling of the process of estimating the quality of an answer candidate. This approach relies on multiple elementary models including element co-occurrence probabilities, question element appearance probility in the context of a correct answer and out of context answer probability. This is a very preliminary work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Tree transformation-based answer re-ranking</head><p>Our second approach for the T3 task is built upon the results of the primary system. We stated that the method for finding and extracting the best answer to a given question in 2.1 is based on redundancy and distances between candidate answers and elements of the question. While this approach gives good results, it also has some limitations. Mainly, it does not take into account the structure of the snippet and the relations between the different critical elements detected.</p><p>Relations between the elements of the text fragments are needed to represent the information stated in the documents and the questions. However, most of the systems use complex syntactic representations which are not adapted to handle oral fragments <ref type="bibr" coords="3,230.56,662.18,12.45,8.97" target="#b1">[2]</ref>. However, some systems <ref type="bibr" coords="3,342.83,662.18,12.70,8.97" target="#b4">[5,</ref><ref type="bibr" coords="3,357.84,662.18,8.24,8.97" target="#b2">3]</ref> show that it is possible to identify local syntactic and semantic relations by using a segmentation of the documents into segments (chunks) and then detecting the relations between these segments.</p><p>From these conclusions, we defined a re-ranking method which computes a score for each of the answers to a question. That method takes as input the question tagged by the analysis module, the answers found by the answer extraction module, and the best snippets associated to each answer. The analysis trees of the question and the snippets are segmented into chunks, and relations are added between these chunks.</p><p>For each evaluated answer, the method compares the structure of the question with the snippet of the answer. The system tries to match the structure of the question by moving the chunks of the snippets with similar elements. The relations are used in these moves and allow the system to compute the score of the answer.</p><p>This system uses two sub-modules, the segmenting and annotation module and the relation labelling module. The questions and the snippets are processed through these modules, and then the tree transformation system computes the similarity score of each answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Segmentation and annotation module</head><p>The definition of the segmentation formalism was led by its use for the relation labelling module. We think that verbs have an important role in the structure of a sentence. Therefore, we have defined two types of chunks : verbal chunks (VC) and general chunks (GC). The general chunks can be divided into several subtypes : temporal (TC), spatial (SC) and question markers (QMC). Below is an exemple of a segmented sentence, "The Ebola virus was identified in 1976".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>"[GC] The Ebola virus [/GC] [VC] was identified [/VC] [TC] in 1976 [/TC]."</head><p>The segmentation and annotation module uses a Conditional Random Fields (CRF) based approach. Two models were generated : one for the documents, and one for the questions. We used the following features : analysis module of the main architecture and a Part Of Speech annotation. Two training corpus were used, one for the documents and one for the questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Relation labelling module</head><p>The aim of the relations is to represent the dependances between the chunks of the questions and the chunks of the snippets.</p><p>The relations are oriented and non-exclusive, ie there can be multiple relations between the same two chunks. For the moment, five relations are defined, which are described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Noun modifier relation ; this relation represents the dependance between two chunks containing noun groups, as in the following sentence : "[GC] Steven Spielberg [/GC] [VC] is [/VC] [GC] the man [/GC]</head><p>[GC] with the glasses [/GC]". In this example, there is a noun modifier relation between "the man" and "with the glasses". Verb to member relation ; this relation represents the dependance between a verbal chunk and the chunks containing its members. The members of a verb are its subject and its objects. In the following sentence,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>"[GC] The Ebola virus [/GC] [VC] was identified [/VC] [TC] in 1976 [/TC</head><p>].", there are two verb to member relations between the verbal chunk "was identified" and the two chunks "in 1976" and "The Ebola virus". Member to verb relation ; this type of relation is the same as the previous one, except this relation goes from the member to the verb.</p><p>Temporal relation ; this relation represents the dependance between a temporal chunk and another chunk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>In the following sentence, "[GC] The Ebola virus [/GC] [VC] was identified [/VC] [TC] in 1976 [/TC].",</head><p>there are two temporal relations between the temporal chunk "in 1976" and the two chunks "was identified" and "The Ebola virus". Spatial relation ; this type of relation is the same as the temporal relation, except that it concerns spatial chunks.</p><p>To label the different relations of each of the chunk of the documents and the questions, we use a rule-based system. Each type of relation has an associated rule, with the following parameters : the types of the chunks on which the rule applies, the types of chunks who can be in relation, the direction of the rule, and the context of application of the rule. Here is an exemple of the rule for temporal relations :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>temporal relation : {TC} {GC | VC | SC | QMC} {LEFT | RIGHT} {TC}</head><p>This rule means that we add a temporal relation between a chunk of TC type and a chunk of GC, VC, SC or QMC type. The target chunk can either be at the left or a the right of the temporal chunk. The relation is not allowed to cross over another temporal chunk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Text transformation module</head><p>As we said previously, before trying to transform the snippet into the question, the system finds the similarities between the chunks of the snippet and the chunks of the question. To find the similarities, we use the following information : lemma form, synonyms and morphological derivations. The system defines anchor points between comparable chunks. With these points, the system transforms the snippet into the question by using three types of operations : inserting a chunk, deleting a chunk and substituting a chunk. These types of operation are applied in a certain order. First, the system generates one substituting operation for each anchor point, and compute its cost. It depends on two values : the substitution cost and the displacement cost. The subtitution cost is computing by making the sum of a per word cost for each important word which is not found in the question. We decide wether a word is deemed important based on its type given by the analysis module. For example, verbs and nouns are important but determinants are not. The per word cost has been set empirically. The displacement is seen as a sequence of permutations between adjacent chunks. Each permutation has a cost depending on the relation between the two chunks and their types. Then, the system finds the sequence of substitutin operations with the lowest total score, which results in a similar structure between the question and the snippet. To finish the transformation, the remaining chunks are deleted, and the missing ones from the question are inserted. The sequence of operations with the smallest total cost measures the similarity between the question and the snippet, and by comparing these similarity scores a new ranking is computed.</p><p>The figure <ref type="figure" coords="5,116.88,626.30,4.98,8.97">1</ref> shows an example of how the transformation works. The relations are not shown for clarity. We evaluate the answer "Northern Ireland" for the question "What country is Annetta Flanigan from ?". The snippet of the answer is "One captive is Annetta Flanigan from my constituency of Northern Ireland".</p><p>As you can see on the figure, the system find three anchor points between the chunks of the question and those of the text fragment. The colours show the anchors between the chunks. Using these anchors, the system generates the operations. For this example, a list of transformation could be :</p><p>-Moving chunk "Annetta Flanigan" next to chunk "of Northern Ireland" FIGURE 1 -Transformation exemple between the question "What country is Annetta Flanigan from ?" and the snippet "One captive is Annetta Flanigan from my constituency of Northern Ireland" containing the answer "Northern Ireland" -Deleting "the captive" and "from my constituency"</p><p>We think that verbs in a sentence are important to the signification and the structure of this sentence. Thus, we do not allow a permutation between a verbal chunk and another general chunk. That is why in this example we do not allow a permutation between "Northern Ireland" and "is". Also, we do not insert the preposition "from" because it is not a chunk of the question associated to a critical element of the search descriptor (SD). This sequence of operations allows the system to compute a new score for this answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training and Development data</head><p>Each main task had a two 50 questions development corpus, one of manual transcriptions of spontaneous spoken questions and one of written reformulations of these questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>An overview of the different corpus used can be viewed in table 1. The numbers between the parenthesis indicate the number of documents of the corpus.</head><p>As part of the training data, we used the corpus of reformulated questions we developped last year in addition to the official development corpus and the test data from QAst 2008. Since the second approach for the T3 task, the re-ranking method, does not yet give better results on the whole development corpus, we decided to use it only on certain type of questions. In Table <ref type="table" coords="7,444.24,98.30,3.77,8.97" target="#tab_1">2</ref>, the LIMSI1 system is the distance-based method, and the LIMSI2-T3 is the re-ranking method. We found that this method gets better results on questions with a lot of search elements present in the search descriptor. The corpus of questions evaluated in this table is a fusion between the official and the supplementary development corpus. We can see that on questions with at least 5 search elements, the LIMSI2 on the T3 task gets better results than the LIMSI1 system. Thus, we decided that the re-ranking approach will only be applied on questions with at least 5 search elements.</p><p>LIMSI2 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Task difficulty evaluation</head><p>As stated in Section 1, the procedure for building the question corpus has changed this year. We try to evaluate whether the difficulty of the task had changed as a result.</p><p>Mainly, we wanted to compare the differences between the development corpus of QAst08 and QAst09. Therefore, we evaluated for each question of the two corpus the distance between the elements of the question and the answer in the documents containing the correct answer. For each questions, we computed four distance scores : the number of words, the number of nodes of the analysis module, the number of chunks and the number of sentences. Each score is an average of the distance of each element of the question from the answer. This evaluation was made on the corpus of the T3 main task (French corpus). The Table <ref type="table" coords="7,469.92,511.22,4.98,8.97">3</ref> shows the results of this evaluation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Words</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 3 -Mean and standard deviation (SD) of the distance between correct answers and elements of the questions in various units</head><p>This table shows some differences between the development corpus of QAst09 and QAst08. We see that the mean distance is roughly doubled in the QAst09 development corpus compared to the previous year. While such a difference is significant in absolute terms, we do not think that it by itself fully justifies a large difference in task difficulty. We need to also analyse the impact of lexical variations between the questions and the documents contents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">General results on manual transcripts</head><p>The results for the three tasks on manual transcribed data are presented in tables 4 to 6, with all the question types evaluated. For each task, two systems were used. There is also a difference between the LIMSI2 system in the T1 and T2 tasks (English and Spanish) and LIMSI2 system in the T3 task (French). For each case, only the Factual Answer Extraction procedure is changing : in LIMSI1, it uses a scoring of all the candidates of appropriate types given the question classification. In LIMSI2 for the T1 and T2 tasks, it uses the bayesian method explained before, and in the T3 task the tree transformation re-ranking method. As stated before, the LIMSI2 system in the T3 task is not used on all the questions, but only the questions with a lot of search elements. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">General results on automatic transcripts</head><p>The results obtained on the three tasks in automatically transcribed data are presented in tables 7 to 9. With the automatic transcripts, only the LIMSI1 system is used. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ASR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Analysis of the results</head><p>Tables 4 to 6 show a great loss between the recall and the accuracy of our systems. The LIMSI1 system gives a bad answer on half of the questions with the good answer in the candidates answers, and it is worst for the LIMSI2 system on the T1 and T2 tasks. The LIMSI2 system on the T3 task gives almost the same results that the LIMSI1 system by applying it only on a small set of questions, as stated previously. A study of the results of this system is showed next. Nevertheless, we can see that there are almost no differences between written and spoken questions. LIMSI2 system on the T1 and T2 tasks is a preliminary version that gives interesting results. As such, we are going to improve it. LIMSI2 system on the T3 task still needs work to improve it.</p><p>For the results obtained on the three different automatic speech transcription, as showed in tables 7 to 9, we Table <ref type="table" coords="10,134.88,542.66,10.02,8.97" target="#tab_7">12</ref> compared the results on the T3 task between the LIMSI1 system and the LIMSI2-T3 system. We show two sets of results for the LIMSI2 system : those obtained where all the questions of the corpus are re-ranked (LIMSI2-T3), and those obtained where only the questions with 5 or more search elements are re-ranked (LIMSI2-T3-SE).</p><p>As we can see, there is an huge loss between the QAst08 corpus and the test and development corpus of QAst09. One reason for these results could be the new methodology used to build the questions corpus. As stated in section 3.1, the distances between the elements of the question and the answer are greater in the development corpus of QAst09. The greater distance between an answer and its associated question elements does not seem to be the only cause of these results. In particular, we expect lexical variations between the questions and the elements as found in the documents to also have play a significant role.</p><p>Table <ref type="table" coords="10,134.88,686.06,10.02,8.97" target="#tab_7">12</ref> shows that the re-ranking of the questions with 5 or more search elements allows the LIMSI2-T3 system to get almost the same results than the LIMSI1 system. We can see that it also gets better results on the development corpus from QAst 2008. While these results are interesting, as stated before this approach needs to be improved. We evaluated the questions with 5 or more search elements which were re-ranked by the LIMSI2-T3 system.</p><p>Of the ten questions of the written question corpus with that many search elements, six did not have the correct answer ithin the candidates answers and one was a NIL question. Of the remaining three, one was answered correctly by both systems, one was answered correctly by the LIMSI1 but not the LIMSI2-T3 one.</p><p>And the correct answer for the last question was not found by either of the systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we presented the LIMSI question-answering systems on speech transcripts which participated to the QAst 2009 evaluation. These systems obtained state-of-the-art results on the different tasks and languages and the accuracy ranged from 27% for English to 36% for Spanish data). The results of the T1 ans T3 systems show a significant loss of results compared to the 2008 evaluation (6% for T1 and 17% for T3 in accuracy) inspite of the improvements og the systems. It can be explained by the new methodolgy used to build the questions corpus. A deeper analysis is ongoing to understand the origins of this loss.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,129.24,289.61,59.46,7.42;3,221.28,289.61,71.94,7.42;3,132.12,301.49,109.98,7.42;3,132.12,313.49,125.10,7.42;3,132.12,325.49,150.94,7.42;3,132.12,337.37,129.90,7.42;3,132.12,349.08,146.26,8.58;3,132.12,361.37,137.86,7.42;3,132.12,373.25,89.74,7.42"><head></head><label></label><figDesc>w(l) = line weight w(a) = answer weight d(e, a) = element-answer distance Ea = set of SD elements for instance a Ar = set of instances of the answer candidate r Sp(a) = score of the snippet including a C d (r) = instance count of r in the documents Cp(r) = instance count of r in the snippets α, β, γ, δ = tuning variables</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,110.16,617.78,428.46,89.61"><head>TABLE 1 -</head><label>1</label><figDesc>The corpus. Off. Dev : the official development data ; Supp. Dev : the QAst08 development and test data and reformulated questions based on the QAst08 development</figDesc><table coords="6,260.04,617.78,128.85,46.05"><row><cell cols="3">Task Off. Dev. Supp. Dev.</cell></row><row><cell>T1</cell><cell>2×50 (6)</cell><cell>377</cell></row><row><cell>T2</cell><cell>2×50 (6)</cell><cell>317</cell></row><row><cell>T3</cell><cell>2×50 (18)</cell><cell>450</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,73.44,192.38,428.73,163.29"><head>TABLE 2 -</head><label>2</label><figDesc>Results on the development data classified by number of search elements in the search descriptor, with #E being the number of search elements. LIMSI2-T3 is the re-ranking method and LIMSI1 the distancebased method.</figDesc><table coords="7,137.16,192.38,301.12,107.73"><row><cell></cell><cell>-T3</cell><cell></cell><cell>LIMSI1</cell><cell></cell><cell></cell></row><row><cell cols="6">#E. MRR Acc #Correct MRR Acc #Correct #Questions</cell></row><row><cell>1</cell><cell>0.62 48.6</cell><cell>53</cell><cell>0.71 67.0</cell><cell>73</cell><cell>109</cell></row><row><cell>2</cell><cell>0.56 42.2</cell><cell>73</cell><cell>0.66 61.8</cell><cell>107</cell><cell>173</cell></row><row><cell>3</cell><cell>0.74 67.4</cell><cell>145</cell><cell>0.79 77.9</cell><cell>166</cell><cell>215</cell></row><row><cell>4</cell><cell>0.72 65.5</cell><cell>74</cell><cell>0.79 77.9</cell><cell>88</cell><cell>113</cell></row><row><cell>5</cell><cell>0.73 65.5</cell><cell>38</cell><cell>0.71 60.3</cell><cell>35</cell><cell>58</cell></row><row><cell>6</cell><cell>0.85 85.7</cell><cell>18</cell><cell>0.81 76.0</cell><cell>16</cell><cell>21</cell></row><row><cell>7</cell><cell>0.60 60.0</cell><cell>3</cell><cell>0.60 60.0</cell><cell>3</cell><cell>5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,110.16,266.18,428.65,203.13"><head>TABLE 4 -</head><label>4</label><figDesc>Results for task T1, English EPPS, manual transcripts (75 factual questions and 25 definitional ones).</figDesc><table coords="8,227.04,266.18,194.65,203.13"><row><cell>Questions</cell><cell></cell><cell>Test 09</cell></row><row><cell></cell><cell cols="3">MRR Acc Recall</cell></row><row><cell>LIMSI1 Written</cell><cell cols="2">0.36 27%</cell><cell>53%</cell></row><row><cell>Spoken</cell><cell cols="2">0.33 23%</cell><cell>45%</cell></row><row><cell>LIMSI2 Written</cell><cell cols="2">0.32 23%</cell><cell>45%</cell></row><row><cell>Spoken</cell><cell cols="2">0.27 19%</cell><cell>41%</cell></row><row><cell>System Questions</cell><cell></cell><cell>Test 09</cell></row><row><cell></cell><cell>MRR</cell><cell>Acc</cell><cell>Recall</cell></row><row><cell>LIMSI1 Written</cell><cell cols="2">0.45 36.0%</cell><cell>61%</cell></row><row><cell>Spoken</cell><cell cols="2">0.45 36.0%</cell><cell>62%</cell></row><row><cell>LIMSI2 Written</cell><cell cols="2">0.34 24.0%</cell><cell>49%</cell></row><row><cell>Spoken</cell><cell cols="2">0.34 24.0%</cell><cell>49%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,110.16,492.02,428.65,108.57"><head>TABLE 5 -</head><label>5</label><figDesc>Results for task T2, Spanish EPPS, manual transcripts (44 factual questions and 56 definitional ones).</figDesc><table coords="8,227.04,528.62,194.65,71.97"><row><cell>System Questions</cell><cell></cell><cell>Test 09</cell></row><row><cell></cell><cell>MRR</cell><cell>Acc</cell><cell>Recall</cell></row><row><cell>LIMSI1 Written</cell><cell cols="2">0.39 28.0%</cell><cell>60%</cell></row><row><cell>Spoken</cell><cell cols="2">0.39 28.0%</cell><cell>59%</cell></row><row><cell>LIMSI2 Written</cell><cell cols="2">0.38 27.0%</cell><cell>60%</cell></row><row><cell>Spoken</cell><cell cols="2">0.39 28.0%</cell><cell>59%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,110.16,623.30,428.46,20.85"><head>TABLE 6 -</head><label>6</label><figDesc>Results for the T3 task, French Broadcast News, manual transcripts (68 factual questions and 32 definitional ones).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,73.44,97.10,428.65,462.69"><head>TABLE 9 -</head><label>9</label><figDesc>Results for the T3 task, French Broadcast News, manual transcripts (68 factual questions and 32 definitional ones).</figDesc><table coords="9,73.44,97.10,428.65,419.13"><row><cell></cell><cell>System Questions</cell><cell></cell><cell>Test 09</cell></row><row><cell></cell><cell></cell><cell>MRR</cell><cell>Acc</cell><cell>Recall</cell></row><row><cell cols="2">ASR_A LIMSI1 Written</cell><cell cols="2">0.31 26.0%</cell><cell>42%</cell></row><row><cell>10.6%</cell><cell>Spoken</cell><cell cols="2">0.30 25.0%</cell><cell>41%</cell></row><row><cell cols="2">ASR_B LIMSI1 Written</cell><cell cols="2">0.25 21.0%</cell><cell>32%</cell></row><row><cell>14.0%</cell><cell>Spoken</cell><cell cols="2">0.25 21.0%</cell><cell>33%</cell></row><row><cell cols="2">ASR_C LIMSI1 Written</cell><cell cols="2">0.24 21.0%</cell><cell>31%</cell></row><row><cell>24.1%</cell><cell>Spoken</cell><cell cols="2">0.24 20.0%</cell><cell>33%</cell></row><row><cell cols="5">TABLE 7 -Results for task T1, English EPPS, automatic transcripts (75 factual questions and 25 definitional</cell></row><row><cell>ones).</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ASR</cell><cell>System Questions</cell><cell></cell><cell>Test 09</cell></row><row><cell></cell><cell></cell><cell>MRR</cell><cell>Acc</cell><cell>Recall</cell></row><row><cell cols="2">ASR_A LIMSI1 Written</cell><cell cols="2">0.32 27.0%</cell><cell>42%</cell></row><row><cell>11.5%</cell><cell>Spoken</cell><cell cols="2">0.31 26.0%</cell><cell>41%</cell></row><row><cell cols="2">ASR_B LIMSI1 Written</cell><cell cols="2">0.29 25.0%</cell><cell>37%</cell></row><row><cell>12.7%</cell><cell>Spoken</cell><cell cols="2">0.29 25.0%</cell><cell>37%</cell></row><row><cell cols="2">ASR_C LIMSI1 Written</cell><cell cols="2">0.28 23.0%</cell><cell>37%</cell></row><row><cell>13.7%</cell><cell>Spoken</cell><cell cols="2">0.28 24.0%</cell><cell>37%</cell></row><row><cell cols="5">TABLE 8 -Results for task T2, Spanish EPPS, automatic transcripts (44 factual questions and 56 definitional</cell></row><row><cell>ones).</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ASR</cell><cell>System Questions</cell><cell></cell><cell>Test 09</cell></row><row><cell></cell><cell></cell><cell>MRR</cell><cell>Acc</cell><cell>Recall</cell></row><row><cell cols="2">ASR_A LIMSI1 Written</cell><cell cols="2">0.37 29.0%</cell><cell>52%</cell></row><row><cell>11.0%</cell><cell>Spoken</cell><cell cols="2">0.37 29.0%</cell><cell>50%</cell></row><row><cell cols="2">ASR_B LIMSI1 Written</cell><cell cols="2">0.32 27.0%</cell><cell>40%</cell></row><row><cell>23.9%</cell><cell>Spoken</cell><cell cols="2">0.30 25.0%</cell><cell>38%</cell></row><row><cell cols="2">ASR_C LIMSI1 Written</cell><cell cols="2">0.28 23.0%</cell><cell>38%</cell></row><row><cell>35.4%</cell><cell>Spoken</cell><cell cols="2">0.27 22.0%</cell><cell>35%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="11,154.92,96.74,265.65,139.05"><head>TABLE 12 -</head><label>12</label><figDesc>Results obtained on each system for the manual tasks.</figDesc><table coords="11,165.72,96.74,244.17,107.37"><row><cell></cell><cell></cell><cell>T3</cell></row><row><cell></cell><cell>LIMSI1</cell><cell>LIMSI2-T3</cell><cell>LIMSI2-T3-SE</cell></row><row><cell>Corpus</cell><cell cols="3">MRR Acc MRR Acc MRR</cell><cell>Acc</cell></row><row><cell cols="4">W. Test09 0.39 28% 0.24 18% 0.38</cell><cell>27%</cell></row><row><cell>S. Test09</cell><cell cols="3">0.39 28% 0.24 17% 0.39</cell><cell>28%</cell></row><row><cell cols="4">W. Dev09 0.44 40% 0.25 16% 0.44</cell><cell>40%</cell></row><row><cell>S. Dev09</cell><cell cols="3">0.44 36% 0.26 18% 0.42</cell><cell>34%</cell></row><row><cell cols="4">W. Dev08 0.81 76% 0.68 58% 0.85</cell><cell>80%</cell></row><row><cell cols="4">W. Test08 0.57 50% 0.50 40% 0.57</cell><cell>50%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work has been partially financed by <rs type="funder">OSEO</rs> under the <rs type="programName">Quaero program</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_bCGt9KK">
					<orgName type="program" subtype="full">Quaero program</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>can see that they are lower than the results of the manual transcriptions.</p><p>We show in table 10 the results obtained by the LIMSI on each task. We also show the best results of all the participants systems in column Best for each task. Except on the T1 Manual and the T1 ASR_A, the LIMSI obtains the best results. It should be noted that we were the only participants in the T3 task. Table <ref type="table" coords="10,135.00,330.86,10.02,8.97">11</ref> shows the results obtained by each system for the manual sub-tasks on the T1 and T2 tasks. The evaluated corpus are the development and the test corpus of QAst 2009 on both written and spoken questions, and the development and test corpus of QAst 2008, on written questions. As stated before, the LIMSI1 system used the distance-based approach, and the LIMSI2-T1 and LIMSI2-T2 the bayesian approach. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="11,90.00,601.82,412.21,8.97;11,90.00,613.82,137.25,8.97" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="11,315.00,601.82,187.21,8.97;11,90.00,613.82,28.01,8.97">Improved machine translation of speech-to-text outputs</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Déchelotte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Adda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-L</forename><surname>Gauvain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<pubPlace>Antwerp. Belgium</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,90.00,629.90,411.94,8.97;11,90.00,641.90,411.92,8.97;11,90.00,653.90,412.17,8.97;11,90.00,665.78,22.65,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,293.16,629.90,208.78,8.97;11,90.00,641.90,32.14,8.97">Easy, evaluation of parsers of french : what are the results ?</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Paroubek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vilnat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Grau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ayache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,140.52,641.90,209.49,8.97;11,387.24,641.90,114.68,8.97;11,90.00,653.90,241.03,8.97">Proceedings of the Sixth International Language Resources and Evaluation (LREC&apos;08)</title>
		<meeting>the Sixth International Language Resources and Evaluation (LREC&apos;08)<address><addrLine>Marrakech, Morocco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="2480" to="2486" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA)</note>
</biblStruct>

<biblStruct coords="11,90.00,681.98,412.09,8.97;11,90.00,693.86,227.61,8.97" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="11,351.48,681.98,150.61,8.97;11,90.00,693.86,59.19,8.97">Semantic role labeling using different syntactic views</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Hacioglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jurafski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="581" to="588" />
			<pubPlace>Ann Arbor, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,90.00,710.06,411.93,8.97;11,90.00,721.94,319.41,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,340.44,710.06,157.74,8.97">The limsi participation to the qast track</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Galibert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Bilinski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Adda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,100.80,721.94,158.01,8.97">Working Notes of CLEF 2008 Workshop</title>
		<meeting><address><addrLine>Aarhus, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-09">September 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,126.72,98.30,412.02,8.97;12,126.72,110.30,389.61,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,422.52,98.30,116.22,8.97;12,126.72,110.30,197.96,8.97">Askmi : A japanese question answering system based on semantic role analysis</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ichimura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kokubu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Manabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,342.84,110.30,105.00,8.97">Proceedings of RIAO 2004</title>
		<meeting>RIAO 2004<address><addrLine>Avignon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,126.72,125.78,412.02,8.97;12,126.72,137.78,348.81,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,406.56,125.78,132.18,8.97;12,126.72,137.78,138.73,8.97">Overview of qast 2008 -question answering on speech transcriptions</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Turmo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Comas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lamel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mostefa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,283.32,137.78,86.73,8.97">CLEF 2008 Workshop</title>
		<meeting><address><addrLine>Aarhus, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,126.72,153.26,412.07,8.97;12,126.72,165.26,412.05,8.97;12,126.72,177.26,63.81,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,499.68,153.26,39.11,8.97;12,126.72,165.26,236.77,8.97">Overview of qast 2009 -question answering on speech transcriptions</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Turmo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Comas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Galibert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mostefa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Buscaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,383.76,165.26,87.69,8.97">CLEF 2009 Workshop</title>
		<meeting><address><addrLine>Greece, Corfu</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
