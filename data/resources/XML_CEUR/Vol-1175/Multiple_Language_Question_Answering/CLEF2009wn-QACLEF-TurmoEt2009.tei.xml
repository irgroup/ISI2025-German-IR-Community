<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,198.24,148.63,206.66,15.51">Overview of QAST 2009</title>
				<funder ref="#_hZRuauX">
					<orgName type="full">Spanish Ministry of Science</orgName>
				</funder>
				<funder ref="#_NWPpXPC">
					<orgName type="full">OSEO</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,96.00,182.13,40.27,9.96"><forename type="first">J</forename><surname>Turmo</surname></persName>
							<email>turmo@lsi.upc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">TALP Research Centre (UPC)</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,146.88,182.13,51.57,9.96"><forename type="first">P</forename><forename type="middle">R</forename><surname>Comas</surname></persName>
							<email>pcomas@lsi.upc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">TALP Research Centre (UPC)</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,208.92,182.13,40.23,9.96"><forename type="first">S</forename><surname>Rosset</surname></persName>
							<email>rosset@limsi.fr</email>
							<affiliation key="aff1">
								<orgName type="institution">LIMSI. Paris</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,259.56,182.13,50.21,9.96"><forename type="first">O</forename><surname>Galibert</surname></persName>
							<email>olivier.galibert@limsi.fr</email>
							<affiliation key="aff1">
								<orgName type="institution">LIMSI. Paris</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,320.28,182.13,46.70,9.96"><forename type="first">N</forename><surname>Moreau</surname></persName>
							<email>moreau@elda.org</email>
							<affiliation key="aff2">
								<orgName type="institution">ELDA/ELRA. Paris</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,377.40,182.13,48.06,9.96"><forename type="first">D</forename><surname>Mostefa</surname></persName>
							<email>mostefa@elda.org</email>
							<affiliation key="aff2">
								<orgName type="institution">ELDA/ELRA. Paris</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,436.08,182.13,37.26,9.96"><forename type="first">P</forename><surname>Rosso</surname></persName>
							<email>prosso@dsic.upv.es</email>
							<affiliation key="aff3">
								<orgName type="laboratory">NLE Lab. -ELiRF Research Group (UPV)</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,500.41,182.13,50.77,9.96"><forename type="first">D</forename><surname>Buscaldi</surname></persName>
							<email>dbuscaldi@dsic.upv.es</email>
							<affiliation key="aff3">
								<orgName type="laboratory">NLE Lab. -ELiRF Research Group (UPV)</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,198.24,148.63,206.66,15.51">Overview of QAST 2009</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5ED39D8E6163CCD8B46DD430F00C2F94</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software Experimentation, Performance, Measurement Question Answering, Spontaneous Speech Transcripts</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the experience of QAST 2009, the third time a pilot track of CLEF has been held aiming to evaluate the task of Question Answering in Speech Transcripts. Four sites submitted results for at least one of the three scenarios (European Parliament debates in English and Spanish and broadcast news in French). In order to assess the impact of potential errors of automatic speech recognition, for each task manual transcripts and three different ASR outputs were provided. In addition an original method of question creation was tried in order to get spontaneous oral questions resulting in two sets of questions (spoken and written). Each participant who had chosen a task, was asked to submit a run for each condition. The QAST 2009 evaluation framework is described, along with descriptions of the three scenarios and their associated data, the system submissions for this pilot track and the official evaluation results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Question Answering (QA) technology aims at providing answers to natural language questions. Current QA technology is focused mainly on the mining of written text sources for extracting the answer to written questions from both open-domain and restricted-domain document collections <ref type="bibr" coords="2,90.00,181.05,10.57,9.96" target="#b7">[7,</ref><ref type="bibr" coords="2,103.56,181.05,7.05,9.96" target="#b3">3]</ref>. However, most human interaction occurs through speech, e.g. meetings, seminars, lectures, telephone conversations. All these scenarios provide large amounts of information that could be mined by QA systems. As a consequence, the exploitation of speech sources brings QA a step closer to many real world applications in which spontaneous oral questions or written questions can be involved. The QAST 2009 track aims at investigating the problem of answer spontaneous oral questions and written questions using audio documents.</p><p>Current text-based QA systems tend to use technologies that require text written in accordance with standard norms for written grammar. The syntax of speech is quite different than that of written language, with more local but less constrained relations between phrases, and punctuation, which gives boundary cues in written language, is typically absent. Speech also contains disfluencies, repetitions, restarts and corrections. Moreover, any practical application of search in speech requires the transcriptions to be produced automatically, and the Automatic Speech Recognizers (ASR)introduce a number of errors. Therefore current techniques for text-based QA need substantial adaptation in order to access the information contained in audio documents, and probably to analyse oral questions. Preliminary research on QA in speech transcriptions was addressed in QAST 2007 and QAST 2008, pilot evaluation tracks at CLEF in which systems attempted to provide answers to written factual and definitional written questions by mining speech transcripts of different scenarios <ref type="bibr" coords="2,182.88,396.21,10.57,9.96" target="#b5">[5,</ref><ref type="bibr" coords="2,196.68,396.21,7.05,9.96" target="#b6">6]</ref>.</p><p>This paper provides an overview of the third QAST pilot evaluation. Section 2 describes the principles of this evaluation track. Sections 3 and 4 present the evaluation framework and the systems that participated, respectively. Section 5 reports and discusses the achieved results, followed by some conclusions in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The QAST 2009 task</head><p>The aim of this third year of QAST is to provide a framework in which QA systems can be evaluated in a real scenario, where the answers of both spontaneous oral questions and written questions have to be extracted from speech transcriptions, these transcriptions being manually and automatically generated. There are five main objectives to this evaluation:</p><p>• Motivating and driving the design of novel and robust QA architectures for speech transcripts;</p><p>• Measuring the loss due to the inaccuracies in state-of-the-art ASR technology;</p><p>• Measuring this loss at different ASR performance levels given by the ASR word error rate;</p><p>• Measuring the loss when dealing with spontaneous oral questions</p><p>• Motivating the development of monolingual QA systems for languages other than English.</p><p>In the 2009 evaluation, as in the 2008 evaluation, an answer is structured as a simple [answer string, document id] pair where the answer string contains nothing more than the full and exact answer, and the document id is the unique identifier of the document supporting the answer. For the tasks on automatic speech transcripts, the answer string consisted of the &lt;start-time&gt; and the &lt;end-time&gt; giving the position of the answer in the signal.</p><p>Figure <ref type="figure" coords="3,120.49,569.37,4.98,9.96" target="#fig_0">1</ref> illustrates this point. Given the manually transcribed spontaneous oral question When did the bombing of Fallujah eee took take place? corresponding to the written question When did the bombing of Fallujah take place?, the figure compares the expected answer in a manual transcript (the text a week ago) and in an automatic transcript (the time segment 1081.588 1082.178). Note that Fallujah was wrongly recongnized as for the Chair by the ASR. A system can provide up to 5 ranked answers per question.</p><p>A total of six tasks were defined for this third edition of QAST covering three scenarios: English questions related to European Parliament sessions in English (T1a and T1b), Spanish questions related to European Parliament sessions in Spanish (t2a and T2b) and French questions related to French Broadcast News (t3a and T3b). The complete set of tasks is:</p><p>• T1a: QA of English written questions in the manual and automatic transcriptions of European Parliament Plenary sessions in English (EPPS English corpus).</p><p>• T1b: QA of manual transcriptions of English spontaneous oral questions in the manual and automatic transcriptions of European Parliament Plenary sessions in English (EPPS English corpus).</p><p>• T2a: QA of Spanish written questions in the manual and automatic transcriptions of European Parliament Plenary sessions in Spanish (EPPS Spanish corpus).</p><p>• T2b: QA of manual transcriptions of Spanish spontaneous oral questions in the manual and automatic transcriptions of European Parliament Plenary sessions in Spanish (EPPS Spanish corpus).</p><p>• T3a: QA of French written questions in manual and automatic transcriptions of broadcast news for French (ESTER corpus)</p><p>• T3b: QA of manual transcriptions of French spontaneous oral questions in manual and automatic transcriptions of broadcast news for French (ESTER corpus)</p><p>3 Evaluation protocol</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data collections</head><p>The QAST 2009 data is derived from three different resources, each one corresponding to a different language (English, Spanish and French):</p><p>• English parliament (EPPS EN): The TC-STAR05 EPPS English corpus <ref type="bibr" coords="4,462.60,431.61,10.45,9.96" target="#b4">[4]</ref> contains 3 hours of recordings in English corresponding to 6 sessions of the European Parliament.</p><p>The data was used to evaluated speech recognizers in the TC-STAR project. There are 3 different automatic speech recognition outputs with different word error rates (10.6%, 14% and 24.1%) . The manual transcriptions were done by ELDA.</p><p>• Spanish parliament (EPPS ES): The TC-STAR05 EPPS Spanish corpus <ref type="bibr" coords="4,465.96,499.29,10.57,9.96" target="#b4">[4]</ref> is comprised of three hours of recordings in Spanish corresponding to 6 sessions of the European Parliament. The data was used to evaluate Spanish ASR systems developed in the TC-STAR project. There are 3 different automatic speech recognition outputs with different word error rates (11.5%, 12.7% and 13.7%). The manual transcriptions were done by ELDA. These three collections are the same than the ones used last year for the QAST 2008 evaluation campaign.</p><p>European Parliament and Broadcast News data are usually referred to as prepared speech. Although they typically have few interruptions and turn-taking problems when compared to actual spontaneous speech, many of the characteristics of spoken language are still present (hesitations, breath noises, speech errors, false starts, mispronunciations and corrections).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Questions and answer types</head><p>For each of the three languages, two sets of manually transcribed spontaneous oral questions and their respective written questions have been created and provided to the participants, the first for development purposes and the second for the evaluation:</p><p>• Development sets (released on the 25th of March 2009):</p><p>-EPPS EN: 50 transcribed questions and their respective written questions.</p><p>-EPPS ES: 50 transcribed questions and their respective written questions.</p><p>-French BN: 50 transcribed questions and their respective written questions.</p><p>• Evaluation sets (released on the 1st of June 2009):</p><p>-EPPS EN: 100 transcribed questions and their respective written questions.</p><p>-EPPS ES: 100 transcribed questions and their respective written questions.</p><p>-French BN: 100 transcribed questions and their respective written questions.</p><p>For each language, both the development and evaluation sets were created from the whole document collection (i.e. the 6 European Parliament sessions for English and Spanish, and the 18 Broadcast News shows for French). In other words, there was no collection split between a development data set and an evaluation data set as was done last year.</p><p>As for last year, two types of questions were considered: factual questions and definitional ones.</p><p>The expected answer to a factual question is a named entity. There were 6 types of factual question this year, each corresponding to a particular category of named entities:</p><p>• Person: names of humans, real and fictional, fictional or real non-human individuals. Ex: Mirjam Killer, John, Jesus, etc.</p><p>• Organisation: names of business, multinational organizations, political parties, religious groups, etc. Ex: CIA, IBM, but also named entities like Washington when they display the characteristics of an organisation.</p><p>• Location: geographical, political or astronomical entities. Ex: California, South of California, Earth, etc.</p><p>• Time: a date or a specific moment in time, absolute and relative time expressions. Ex: March 28th, last week, at four oclock in the morning, etc.</p><p>• Measure: measures of length, width or weight, etc. Generally, a quantity and a unit of measurement.</p><p>Ex: five kilometers, 20 hertz, etc. But also ages, period of time, etc. This is less than the 10 categories used for the 2007 and 2008 evaluations. Some categories have not been considered this year because no occurence were found in the collected set of sponteaneous questions (Color, Shape, Language, System, Material).</p><p>The definition questions are questions such as What is the CDU? and the answer can be anything.</p><p>In this example, the answer would be political group. This year, the definition questions are subdivided into three types:</p><p>• Person: question about someone. Q: Who is George Bush? R: The President of the United States of America.</p><p>• Organisation: question about an organisation. Q: What is Cortes? R: Parliament of Spain.</p><p>• Other: questions about technology, natural phenomena, etc. Q: What is the name of the system created by AT&amp;T? R: The How can I help you system.</p><p>For each language a number of 'NIL' questions (i.e., questions having no answer in the document collection) have been selected. The distribution of the different types of questions across the three collections is shown in Table <ref type="table" coords="6,217.44,280.77,3.90,9.96" target="#tab_1">1</ref>. ). The procedure to generate the questions is described in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Question generation</head><p>A novel feature in QAST 2009 was the introduction of spontaneous oral questions. The main issue in the generation of this kind of questions was how to obtain spontaneity. The solution adopted was to set up the following procedure for question generation:</p><p>1. Passage generation: a set of passages was randomly extracted from the document collection.</p><p>A single passage was composed by the complete sentences included in a text window of 720 characters.</p><p>2. Question generation: human question generators were randomly assigned a number of passages (varying from 2 to 4). They had to read each passage and then to formulate one or more questions based on the passage they just read about information not present in it.</p><p>3. Question transcription: precise transcriptions of the oral spontaneous questions were made, including hesitations, etc. Ex: (%hesitation) What (%hesitation) house is the pres() the president elect being elected to?</p><p>4. Question filtering: some questions were filtered out from the set of generated questions because their answer types were not allowed or because they did not have answer in the document collection. The resulting questions were usable questions.</p><p>5. Written question generation: the usable questions were re-written by removing speech disfluencies, correcting the syntax and simplifying the sentence when necessary.</p><p>Ex: What house does the president run?</p><p>6. Question selection: the final set of development questions and test questions were selected by ELDA from the usable questions.</p><p>The allowed question types were the following:</p><p>• definition: person, organisation, object and other</p><p>• factoid : person, location, organisation, time (includes date), measure and language However, the types "language" for factual questions and "object" for definition questions did not occur among the generated questions.</p><p>A preliminary evaluation of the generated questions was carried out in order to determine how many usable questions could be produced by a human reader. The results of this evaluation show that the percentage of usable questions produced by the questions generator was between 47% and 58% of the total questions produced, depending on the speakers knowledge of the task guidelines. These figures show that the produced questions were more than the number of questions actually presented to participants in QAST 2009. Most unusable questions were due to the fact that human question generators forgot the guidelines many times while asking their questions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Human judgment</head><p>As in 2008, the answer files submitted by participants have been manually judged by native speaking assessors, who considered the correctness and exactness of the returned answers. They also checked that the document labeled with the returned document ID supports the given answer.</p><p>One assessor evaluated the results, and another assessor manually checked each judgment of the first one. Any doubts about an answer was solved through various discussions. The assessors used the QASTLE<ref type="foot" coords="7,148.68,634.06,3.97,4.84" target="#foot_1">2</ref> evaluation tool developed in Perl (at ELDA) to evaluate the systems' results. A simple window-based interface permits easy, simultaneous access to the question, the answer and the document associated with the answer.</p><p>After each judgment the submission files were modified by the interface, adding a new element in the first column: the answer's evaluation (or judgment). The four possible judgments (also used at TREC <ref type="bibr" coords="7,123.62,706.65,16.12,9.96" target="#b7">[7]</ref>) correspond to a number ranging between 0 and 3:</p><p>• 0 correct: the answer-string consists of the relevant information (exact answer), and the answer is supported by the returned document.</p><p>• 1 incorrect: the answer-string does not contain a correct answer.</p><p>• 2 inexact: the answer-string contains a correct answer and the docid supports it, but the string has bits of the answer missing or contains additional texts (longer than it should be).</p><p>• 3 unsupported: the answer-string contains a correct answer, but is not supported by the docid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Measures</head><p>The two following metrics (also used in CLEF) were used in the QAST evaluation:</p><p>1. Mean Reciprocal Rank (MRR): This measures how well the right answer is ranked in the list of 5 possible answers.</p><p>2. Accuracy: The fraction of correct answers ranked in the first position in the list of 5 possible answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Submitted runs</head><p>A total of four groups from four different countries submitted results for one or more of the proposed QAST 2009 tasks. Due to various reasons (technical, financial, etc.), eight other groups registered but were not be able to submit any results.</p><p>The four participating groups were:</p><p>• INAOE, Instituto Nacional de Astrofísica, Optica y Electríca, Mexico;</p><p>• LIMSI, Laboratoire d'Informatique et de Mécanique des Sciences de l'Ingénieur, France;</p><p>• TOK, Tokyo Institute of Technology, Japan;</p><p>• UPC, Universitat Politècnica de Catalunya, Spain.</p><p>All groups participated to task T1 (EPPS EN), UPC and LIMSI participated to task T2 (EPPS ES) and only LIMSI dealt with task T3 (French BN). Each participant could submit up to 48 submissions (2 runs per task and transcription). In order to allow comparisons on the performance of the systems when using different WER levels in the transcriptions, it was mandatory for each task to submit results for all the data: the manual transcriptions and the three ASR outputs (automatic transcriptions).</p><p>Table <ref type="table" coords="8,116.88,688.65,4.98,9.96" target="#tab_2">3</ref> shows the number of submitted runs per participant and task. The number of submissions ranged from 8 to 32. The characteristics of the systems used in the submissions are summarized in Table <ref type="table" coords="8,129.36,712.53,3.90,9.96" target="#tab_5">4</ref>. A total of 86 submissions were evaluated with the distribution across tasks shown in the bottom row of the table.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>The results for the three tasks in manual transcribed data are presented in Tables <ref type="table" coords="9,441.83,518.01,22.99,9.96" target="#tab_9">5 to 7</ref>, according to the question types (factual, definitional and all questions).    The results for the three tasks in automatically transcribed data are presented in Tables 8 to 10, according to the question types (factual, definitional and all questions).</p><p>7 systems participated in the T1 (English) task on manual transcripts and 6 on automatic transcripts.</p><p>On manual transcripts, the accuracy ranged from 28% to 5% (for written questions) and from 26% to 3% (for spoken questions).</p><p>For five of the systems, we observe a relatively small difference between written and spoken questions (from 2% to 5% loss going from written questions to spoken questions). The other two systems encountered a significant loss (13% and 16% of difference between written and spoken questions).</p><p>There were three approaches for QA on automatic speech transcripts used by the systems. The LIMSI and UPC on all ASRs and INAOE on ASR A and ASR B took the ASR output at the only available information. INAOE on ASR C used information extracted from all the ASR outputs, keeping ASR C as primary. This approach could represent an application where multiple ASR outputs from different systems are available. Combining outputs from varied systems is a standard method in speech recognition to obtain a better word error rate <ref type="bibr" coords="10,359.63,615.45,10.00,9.96" target="#b1">[1]</ref>, it is interesting to see if the same kind of method can be used at a more semantic level. The TOK system on the other hand used sentence segmentation information from the manual transcripts and applied it to the automatic transcripts. While such a segmentation information is not available in the transcriptions given, ASR systems do generate an acoustically motivated segmentation as a step of their processing. The TOK approach could then be considered as using an optimistic approximation of this automatically generated segmentation information. In any case, comparing systems and estimating the impact of WER can only be done on "pure" systems (LIMSI and UPC on all ASRs and INAOE on ASR A and ASR B).</p><p>On the ASR transcripts for the pure systems, the accuracy ranged for the best ASR (10.6%  We notice that the impact of written vs spoken questions is similar than for manual transcriptions, with two systems taking an heavy loss and the others not showing a significant difference.</p><p>Four systems (2 from LIMSI and 2 from UPC) participated in the T2 (Spanish) task on manual transcripts and 3 systems (1 from LIMSI and 2 from UPC) on automatic transcripts.</p><p>On manual transcripts, the accuracy ranged from 36% (written questions and spoken questions) to 14% (written questions) and 17% (spoken questions). The differences between written questions and spoken questions is very low (from 0% to 3%). The same kind of behaviour is observed on the automatic transcripts tasks, with a loss due to the speech recognition errors and no significant difference between written and spoken questions.   Only 2 systems (both from LIMSI) participated in the T3 (French) task on manual transcripts and one (from LIMSI) on automatic transcripts.</p><p>On manual transcripts, the accuracy ranged from from 28% (both written and spoken questions) to 27% (written questions). There is no significant differences between spoken and written questions (0% to 1% loss). The results for automatic transcriptions show very little loss compared to the manual transcriptions except for the worst ASR.</p><p>The overall absolute results were worse this year compared to last year which points to a globally harder task. The question development method produces requests which qualitatively seem to be more different to what is found in the documents compared to questions built after reading the documents. In our opinion that method, while giving an harder problem, puts us closer to a real, usable application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, the QAST 2009 evaluation has been described. Four groups participated in this track with a total of 86 submitted runs across 3 main tasks that included dealing with different languages (English, Spanish and French), different word error rates for automatic transcriptions (from 10.5% to 35.4%) and different question types (written and spoken questions). An original question creation method has been tried succesfully to generate spontaneous spoken questions. Qualitatively, the questions were harder and more different to the formulations found in the documents compared to those produced by the traditional method of consulting the documents first. The method used this year gives an harder problem but we think that it is a more realistic one, putting us closer to a real, usable application.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,93.84,489.21,415.45,9.96"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example query and response from manual (top) and automatic (bottom) transcripts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,90.00,301.22,423.17,119.23"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table coords="6,219.60,301.22,163.77,36.77"><row><cell>Type</cell><cell>Factual</cell><cell>Definition</cell><cell>NIL</cell></row><row><cell>T1 (English)</cell><cell>75%</cell><cell>25%</cell><cell>18%</cell></row><row><cell>T2 (Spanish)</cell><cell>55%</cell><cell>45%</cell><cell>23%</cell></row><row><cell>T3 (French)</cell><cell>68%</cell><cell>32%</cell><cell>21%</cell></row></table><note coords="6,132.24,360.09,380.80,9.96;6,90.00,372.09,21.13,9.96;6,90.00,398.49,423.17,9.96;6,90.00,410.49,45.46,9.96;6,135.48,409.54,3.97,4.84"><p>Distribution of question types per task: T1 (EPPS EN), T2 (EPPS ES), T3 (French BN). The question sets are formatted as plain text files, with one question per line (see the QAST 2008 Guidelines 1</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,90.00,390.33,423.05,92.76"><head>Table 3 .</head><label>3</label><figDesc>2.1 shows the number of questions recorded, the resulting usable questions and the average of the length in words per question for each language.</figDesc><table coords="7,128.40,436.89,346.29,46.20"><row><cell></cell><cell cols="4">#speaker #questions recorded #usable questions avg. #words</cell></row><row><cell>English</cell><cell>12</cell><cell>1096</cell><cell>616</cell><cell>9.1</cell></row><row><cell>French</cell><cell>7</cell><cell>485</cell><cell>335</cell><cell>7.7</cell></row><row><cell>Spanish</cell><cell>11</cell><cell>403</cell><cell>313</cell><cell>7.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,166.80,504.93,269.52,9.96"><head>Table 2 :</head><label>2</label><figDesc>Details of the questions generated for each language.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,96.00,109.94,446.47,320.81"><head>Table 3 :</head><label>3</label><figDesc>Submitted runs per participant and task.</figDesc><table coords="9,96.00,109.94,446.47,320.81"><row><cell></cell><cell></cell><cell>Participant</cell><cell>T1a</cell><cell>T1b</cell><cell>T2a</cell><cell>T2b</cell><cell>T3a</cell><cell>T3b</cell></row><row><cell></cell><cell></cell><cell>INAOE</cell><cell>8</cell><cell>8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>LIMSI</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell></row><row><cell></cell><cell></cell><cell>TOK</cell><cell>4</cell><cell>4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>UPC</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>Total</cell><cell>25</cell><cell>25</cell><cell>13</cell><cell>13</cell><cell>5</cell><cell>5</cell></row><row><cell>System</cell><cell>Enrichment</cell><cell>Question</cell><cell cols="3">Doc./Passage</cell><cell cols="2">Factual Answer</cell><cell>Def. Answer NERC</cell></row><row><cell></cell><cell></cell><cell>classification</cell><cell cols="2">Retrieval</cell><cell></cell><cell cols="2">Extraction</cell><cell>Extraction</cell></row><row><cell>inaoe1</cell><cell>words</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">passage</cell></row><row><cell></cell><cell>and NEs</cell><cell>hand-crafted</cell><cell cols="2">Indri</cell><cell></cell><cell cols="2">selection</cell><cell>-</cell><cell>regular</cell></row><row><cell>inaoe2</cell><cell>same plus</cell><cell>rules</cell><cell></cell><cell></cell><cell></cell><cell cols="2">based on NEs</cell><cell>expressions</cell></row><row><cell></cell><cell>phonetics</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">of the question type</cell></row><row><cell>limsi1</cell><cell>words, lemmas,</cell><cell></cell><cell cols="2">passage</cell><cell></cell><cell cols="2">ranking based on</cell></row><row><cell></cell><cell>morphologic</cell><cell></cell><cell cols="2">ranking</cell><cell></cell><cell cols="2">distance and</cell><cell>specific</cell><cell>hand-crafted</cell></row><row><cell></cell><cell>derivations,</cell><cell>hand-crafted</cell><cell cols="2">based on</cell><cell></cell><cell cols="2">redundancy</cell><cell>index</cell><cell>rules with</cell></row><row><cell>limsi2</cell><cell>synonymic</cell><cell>rules</cell><cell cols="2">search</cell><cell></cell><cell cols="2">ranking based on</cell><cell>for known</cell><cell>statistical POS</cell></row><row><cell></cell><cell>relations and</cell><cell></cell><cell cols="2">descriptors</cell><cell></cell><cell cols="2">bayesian</cell><cell>acronyms</cell></row><row><cell></cell><cell>extended NEs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">modelling</cell></row><row><cell>tok1</cell><cell>words and</cell><cell></cell><cell cols="2">sentence</cell><cell></cell><cell cols="2">ranking based on</cell></row><row><cell></cell><cell>word classes</cell><cell></cell><cell cols="2">ranking</cell><cell></cell><cell cols="2">analogy between</cell></row><row><cell></cell><cell>derived from</cell><cell>-</cell><cell cols="2">based on</cell><cell></cell><cell cols="2">input question</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>training data -</cell><cell></cell><cell cols="2">statistical</cell><cell></cell><cell cols="2">and question in</cell></row><row><cell></cell><cell>question-answer</cell><cell></cell><cell cols="2">models</cell><cell></cell><cell cols="2">the training data</cell></row><row><cell></cell><cell>pairs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>upc1</cell><cell>words, NEs</cell><cell></cell><cell cols="3">passage ranking</cell><cell cols="2">ranking based on</cell><cell>hand-crafted</cell></row><row><cell></cell><cell>lemmas and</cell><cell></cell><cell cols="3">through iterative</cell><cell cols="2">keyword distance</cell><cell>rules,</cell></row><row><cell></cell><cell>POS</cell><cell>perceptrons</cell><cell cols="3">query relaxation</cell><cell cols="2">and density</cell><cell>-</cell><cell>gazeetters</cell></row><row><cell>upc2</cell><cell>same plus</cell><cell></cell><cell cols="4">addition of approximated</cell><cell>and perceptrons</cell></row><row><cell></cell><cell>phonetics</cell><cell></cell><cell cols="3">phonetic matching</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,143.28,452.37,316.69,9.96"><head>Table 4 :</head><label>4</label><figDesc>Characteristics of the systems that participated in QAST 2009.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="9,90.00,729.81,423.07,21.96"><head>Table 5 :</head><label>5</label><figDesc>Results for task T1, English EPPS, manual transcripts (75 factual questions and 25 definitional ones).</figDesc><table coords="10,115.44,109.94,372.22,96.65"><row><cell>System</cell><cell>Questions</cell><cell></cell><cell>Factual</cell><cell></cell><cell cols="2">Definitional</cell><cell></cell><cell></cell><cell>All</cell></row><row><cell></cell><cell></cell><cell>#Correct</cell><cell>MRR</cell><cell>Acc</cell><cell>#Correct</cell><cell>MRR</cell><cell>Acc</cell><cell>MRR</cell><cell>Acc</cell></row><row><cell>LIMSI1</cell><cell>Written</cell><cell>32</cell><cell>0.56</cell><cell>45.5%</cell><cell>29</cell><cell>0.36</cell><cell>28.6%</cell><cell>0.45</cell><cell>36.0%</cell></row><row><cell></cell><cell>Spoken</cell><cell>32</cell><cell>0.56</cell><cell>45.5%</cell><cell>30</cell><cell>0.37</cell><cell>28.6%</cell><cell>0.45</cell><cell>36.0%</cell></row><row><cell>LIMSI2</cell><cell>Written</cell><cell>26</cell><cell>0.41</cell><cell>29.5%</cell><cell>23</cell><cell>0.28</cell><cell>19.6%</cell><cell>0.34</cell><cell>24.0%</cell></row><row><cell></cell><cell>Spoken</cell><cell>26</cell><cell>0.41</cell><cell>29.5%</cell><cell>23</cell><cell>0.28</cell><cell>19.6%</cell><cell>0.34</cell><cell>24.0%</cell></row><row><cell>UPC1</cell><cell>Written</cell><cell>16</cell><cell>0.24</cell><cell>15.9%</cell><cell>10</cell><cell>0.16</cell><cell>14.3%</cell><cell>0.20</cell><cell>15.0%</cell></row><row><cell></cell><cell>Spoken</cell><cell>20</cell><cell>0.34</cell><cell>27.3%</cell><cell>9</cell><cell>0.13</cell><cell>10.7%</cell><cell>0.22</cell><cell>18.0%</cell></row><row><cell>UPC2</cell><cell>Written</cell><cell>20</cell><cell>0.29</cell><cell>18.2%</cell><cell>10</cell><cell>0.14</cell><cell>10.7%</cell><cell>0.20</cell><cell>14.0%</cell></row><row><cell></cell><cell>Spoken</cell><cell>20</cell><cell>0.33</cell><cell>27.3%</cell><cell>9</cell><cell>0.13</cell><cell>8.9%</cell><cell>0.22</cell><cell>17.0%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="10,90.00,228.81,423.07,92.98"><head>Table 6 :</head><label>6</label><figDesc>Results for task T2, Spanish EPPS, manual transcripts (44 factual questions and 56 definitional ones).</figDesc><table coords="10,115.44,263.78,372.22,58.01"><row><cell>System</cell><cell>Questions</cell><cell></cell><cell>Factual</cell><cell></cell><cell cols="2">Definitional</cell><cell></cell><cell></cell><cell>All</cell></row><row><cell></cell><cell></cell><cell>#Correct</cell><cell>MRR</cell><cell>Acc</cell><cell>#Correct</cell><cell>MRR</cell><cell>Acc</cell><cell>MRR</cell><cell>Acc</cell></row><row><cell>LIMSI1</cell><cell>Written</cell><cell>38</cell><cell>0.35</cell><cell>23.5%</cell><cell>22</cell><cell>0.47</cell><cell>37.5%</cell><cell>0.39</cell><cell>28.0%</cell></row><row><cell></cell><cell>Spoken</cell><cell>39</cell><cell>0.36</cell><cell>23.5%</cell><cell>20</cell><cell>0.46</cell><cell>37.5%</cell><cell>0.39</cell><cell>28.0%</cell></row><row><cell>LIMSI2</cell><cell>Written</cell><cell>38</cell><cell>0.34</cell><cell>22.1%</cell><cell>22</cell><cell>0.47</cell><cell>37.5%</cell><cell>0.38</cell><cell>27.0%</cell></row><row><cell></cell><cell>Spoken</cell><cell>39</cell><cell>0.36</cell><cell>23.5%</cell><cell>20</cell><cell>0.46</cell><cell>37.5%</cell><cell>0.39</cell><cell>28.0%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="10,90.00,344.01,423.10,21.84"><head>Table 7 :</head><label>7</label><figDesc>Results for task T3, French Broadcast News, manual transcripts (68 factual questions and 32 definitional ones).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="11,90.00,498.45,423.15,102.12"><head>Table 8 :</head><label>8</label><figDesc>Results for task T1, English EPPS, automatic transcripts (75 factual questions and 25 definitional ones). of WER) from 26% (written questions) to 5% (spoken questions). Accuracy goes down with increased word error rate giving a roughly 5% loss for ASR B and ASR C compared to ASR A. It is interesting to note that the differences between ASR B (WER 14%) and ASR C (WER 24%) are negligible. The INAOE multi-ASR approach paid off by giving an overall result better than what was obtained by the same system on the best ASR only.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="12,90.00,327.09,423.08,116.26"><head>Table 9 :</head><label>9</label><figDesc>Results for task T2, Spanish EPPS, automatic transcripts (44 factual questions and 56 definitional ones).</figDesc><table coords="12,97.92,361.94,407.12,81.41"><row><cell>ASR</cell><cell>System</cell><cell>Questions</cell><cell></cell><cell>Factual</cell><cell></cell><cell cols="2">Definitional</cell><cell></cell><cell></cell><cell>All</cell></row><row><cell></cell><cell></cell><cell></cell><cell>#Correct</cell><cell>MRR</cell><cell>Acc</cell><cell>#Correct</cell><cell>MRR</cell><cell>Acc</cell><cell>MRR</cell><cell>Acc</cell></row><row><cell cols="2">ASR A LIMSI1</cell><cell>Written</cell><cell>33</cell><cell>0.33</cell><cell>25.0%</cell><cell>19</cell><cell>0.47</cell><cell>37.5%</cell><cell>0.37</cell><cell>29.0%</cell></row><row><cell>11.0%</cell><cell></cell><cell>Spoken</cell><cell>32</cell><cell>0.33</cell><cell>25.0%</cell><cell>18</cell><cell>0.45</cell><cell>37.5%</cell><cell>0.37</cell><cell>29.0%</cell></row><row><cell>ASR B</cell><cell>LIMSI1</cell><cell>Written</cell><cell>25</cell><cell>0.29</cell><cell>25.0%</cell><cell>15</cell><cell>0.38</cell><cell>31.3%</cell><cell>0.32</cell><cell>27.0%</cell></row><row><cell>23.9%</cell><cell></cell><cell>Spoken</cell><cell>25</cell><cell>0.27</cell><cell>22.1%</cell><cell>13</cell><cell>0.35</cell><cell>31.3%</cell><cell>0.30</cell><cell>25.0%</cell></row><row><cell>ASR C</cell><cell>LIMSI1</cell><cell>Written</cell><cell>25</cell><cell>0.26</cell><cell>20.6%</cell><cell>13</cell><cell>0.33</cell><cell>28.1%</cell><cell>0.28</cell><cell>23.0%</cell></row><row><cell>35.4%</cell><cell></cell><cell>Spoken</cell><cell>24</cell><cell>0.25</cell><cell>19.1%</cell><cell>11</cell><cell>0.31</cell><cell>28.1%</cell><cell>0.27</cell><cell>22.0%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" coords="12,90.00,465.57,423.10,21.84"><head>Table 10 :</head><label>10</label><figDesc>Results for task T3, French Broadcast News, manual transcripts (68 factual questions and 32 definitional ones).</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="6,105.24,720.86,136.01,7.97"><p>http://www.lsi.upc.edu/˜qast: News</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="7,105.24,726.26,106.07,7.97"><p>http://www.elda.org/qastle/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work has been jointly funded by the <rs type="funder">Spanish Ministry of Science</rs> (<rs type="projectName">TEXTMESS</rs> project <rs type="grantNumber">-TIN2006-15265-C06</rs>) and <rs type="funder">OSEO</rs> under the <rs type="programName">Quaero program</rs>. We thank to <rs type="person">Lori Lamel</rs>, <rs type="person">Erik Bilinski</rs>, <rs type="person">Manuel González</rs> and <rs type="person">Pere Vilarrubia</rs> their help to the organisation and data generation.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_hZRuauX">
					<idno type="grant-number">-TIN2006-15265-C06</idno>
					<orgName type="project" subtype="full">TEXTMESS</orgName>
				</org>
				<org type="funding" xml:id="_NWPpXPC">
					<orgName type="program" subtype="full">Quaero program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="3,90.00,114.09,413.62,9.95;3,90.00,127.77,323.51,9.95;3,93.36,153.21,396.71,9.96;3,93.36,165.21,396.72,9.96;3,93.36,177.21,381.78,9.96;3,93.36,190.77,91.84,9.96;3,93.36,210.09,308.92,9.96;3,93.36,222.09,16.00,9.96;3,93.36,233.97,248.10,9.96;3,93.36,245.97,258.54,9.96;3,93.36,257.85,258.06,9.96;3,93.36,269.85,278.47,9.96;3,93.36,281.85,251.35,9.96;3,93.36,293.73,279.30,9.96;3,93.36,305.73,284.83,9.96;3,93.36,317.73,262.26,9.96;3,93.36,329.61,253.63,9.96;3,93.36,341.61,248.82,9.96;3,93.36,353.61,258.90,9.96;3,93.36,365.49,254.35,9.96;3,93.36,377.49,249.43,9.96;3,93.36,389.37,251.35,9.96;3,93.36,401.37,261.91,9.96;3,93.36,413.37,253.63,9.96;3,93.36,425.25,256.63,9.96;3,93.36,437.25,286.28,9.96;3,93.36,449.25,16.00,9.96;3,93.36,461.13,123.67,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="3,224.35,165.21,265.74,9.96;3,93.36,177.21,381.78,9.96;3,93.36,190.77,91.84,9.96;3,93.36,210.09,209.88,9.96">) m() m() m() marched into Fallujah and they (%hesitation) bombarded (%hesitation) m() murdered and have been persecuting everyone in the city . Answer: a week ago Extracted portion of an automatic transcript</title>
		<idno>Answer: 1019.228 1019.858</idno>
	</analytic>
	<monogr>
		<title level="m" coord="3,90.00,114.09,413.62,9.95;3,90.00,127.77,323.51,9.95;3,93.36,153.21,396.71,9.96;3,93.36,165.21,130.99,9.96">Spontaneous oral question: When did the bombing of Fallujah eee took take place? Written question: When did the bombing of Fallujah take place? Manual transcript: (%hesitation) a week ago President the American (%hesitation) occupation forces (%hesitation</title>
		<imprint/>
	</monogr>
	<note>CTM file format</note>
</biblStruct>

<biblStruct coords="13,105.48,356.37,407.60,9.96;13,105.48,368.37,407.48,9.96;13,105.48,380.25,269.91,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="13,148.56,356.37,364.52,9.96;13,105.48,368.37,96.15,9.96">A post-processing system to yield reduced word error rates: Recogniser output voting error reduction (rover)</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fiscus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,220.92,368.37,292.04,9.96;13,105.48,380.25,80.61,9.96">Proceedings 1997 IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<meeting>1997 IEEE Workshop on Automatic Speech Recognition and Understanding<address><addrLine>Santa Barbara, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="347" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,105.48,400.17,407.63,9.96;13,105.48,412.17,407.81,9.96;13,105.48,424.17,279.25,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,481.94,400.17,31.18,9.96;13,105.48,412.17,407.81,9.96;13,105.48,424.17,20.64,9.96">Corpus description of the ESTER Evaluation Campaign for the Rich Transcription of French Broadcast News</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Galliano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Geoffrois</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Bonastre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mostefa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Choukri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,148.20,424.17,102.73,9.96">Proceedings of LREC&apos;06</title>
		<meeting>LREC&apos;06<address><addrLine>Genoa</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="315" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,105.48,444.09,407.56,9.96;13,105.48,455.97,407.41,9.96;13,105.48,467.97,99.39,9.96" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="13,221.16,455.97,287.59,9.96">Evaluation of Multilingual and Multi-modal Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</author>
		<editor>, J. Karlgren, B. M</editor>
		<editor>agnini, D.W. Oard, M. de Rijke, and M. Stempfhuber</editor>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,105.48,487.89,196.34,9.96" xml:id="b4">
	<monogr>
		<ptr target="http://www.tc-star.org" />
		<title level="m" coord="13,105.48,487.89,34.03,9.96">TC-Star</title>
		<imprint>
			<date type="published" when="2004">2004-2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,105.48,507.81,407.77,9.96;13,105.48,519.81,407.66,9.96;13,105.48,531.69,407.57,9.96;13,105.48,543.69,162.01,9.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,416.04,507.81,92.63,9.96">Overview of qast 2007</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Turmo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">R</forename><surname>Comas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mostefa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lamel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,140.64,531.69,269.43,9.96">8th workshop of the Cross Language Evaluation Forum (CLEF</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</editor>
		<editor>
			<persName><surname>Th</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Mandl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Oard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Pes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Petras</surname></persName>
		</editor>
		<editor>
			<persName><surname>Santos</surname></persName>
		</editor>
		<imprint>
			<publisher>LNCS</publisher>
			<date type="published" when="2007">2007. 2008</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
	<note>Revised Selected Papers</note>
</biblStruct>

<biblStruct coords="13,105.48,563.61,407.67,9.96;13,105.48,575.61,407.57,9.96;13,105.48,587.49,145.69,9.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,438.36,563.61,74.80,9.96">Overview of qast</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Turmo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">R</forename><surname>Comas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lamel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mostefa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,143.04,575.61,293.34,9.96">9th workshop of the Cross Language Evaluation Forum (CLEF 2008)</title>
		<imprint>
			<date type="published" when="2008">2008. 2009</date>
		</imprint>
	</monogr>
	<note>Revised Selected Papers. (to appear</note>
</biblStruct>

<biblStruct coords="13,105.48,607.41,407.49,9.96;13,105.48,619.41,110.29,9.96" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">L</forename><surname>Buckland</surname></persName>
		</author>
		<title level="m" coord="13,297.83,607.41,215.14,9.96;13,105.48,619.41,79.27,9.96">The Fifteenth Text Retrieval Conference Proceedings (TREC 2006)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
