<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,135.05,148.86,332.90,15.15;1,116.53,170.78,369.93,15.15">Robust Question Answering for Speech Transcripts: UPC Experience in QAst 2009</title>
				<funder>
					<orgName type="full">Spanish Ministry of Science and Technology (TEXTMESS project)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,230.10,204.67,65.39,8.74"><forename type="first">Pere</forename><forename type="middle">R</forename><surname>Comas</surname></persName>
							<email>pcomas@lsi.upc.edu</email>
						</author>
						<author>
							<persName coords="1,318.19,204.67,54.70,8.74"><forename type="first">Jordi</forename><surname>Turmo</surname></persName>
							<email>turmo@lsi.upc.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">TALP Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Technical University of Catalonia (UPC)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,135.05,148.86,332.90,15.15;1,116.53,170.78,369.93,15.15">Robust Question Answering for Speech Transcripts: UPC Experience in QAst 2009</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">54A16040AF40D8079B5A207130CC7E1F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.2.3 [Database Managment]: Languages-Query Languages Measurement, Performance, Experimentation Question Answering, Spoken Document Retrieval, Oral Question Answering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of the Technical University of Catalonia in the CLEF 2009 Question Answering on Speech Transcripts track. We have participated in the English and Spanish scenarios of QAst. For both manual and automatic transcripts we have used a robust factual Question Answering that uses minimal syntactic information. We have also developed a NERC designed to handle automatic transcripts. We perform a detailed analysis of our results and draw conclusions relating QA performance to word error rate and the difference between written and spoken questions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The CLEF 2009 Question Answering on Speech Transcripts (QAst) track <ref type="bibr" coords="1,407.77,600.64,10.52,8.74" target="#b6">[7]</ref> consists of four Question Answering (QA) tasks for three different languages: T1 English, T2 Spanish and T3 French. Task m is QA in manual transcripts of recorded European Parliament Plenary Sessions (EPPS). Tasks a, b, and c, use three different transcripts of the recorded audio using three Automatic Speech Recognizers (ASR). This transcriptions have an increasing percentage of errors. There are two sets of questions for each language: set B contains oral questions spontaneously asked by several human speakers, while set A consists of grammatically corrected transcriptions of the questions in set B. The questions are divided in two sets of development (50 questions) and test (100 questions). Given the languages, questions and transcripts, there is a total of 24 possible scenarios in the QAst evaluation. For example, we will refer as T2B-a to the scenario taking the best automatic transcripts of the Spanish EPPS using spontaneous questions. The automatic transcripts have different levels of word error rate (WER). WERs for T1 are 10.6%, 14%, and T1-m: "Abidjan is going going the way of Kinshasa Kinshasa which was of course a country in the past with skyscrapers and boulevards and now a country a city in ruins" T1-a: "average down is going to go in the way of Kinshasa other at Kinshasa which was of course a country in the past of skyscrapers and poorer parts and our country as a city in ruins" 24.1%. For T2 WERs are 11.5%, 12.7% and 13.7%. Figure <ref type="figure" coords="2,349.82,231.26,4.98,8.74" target="#fig_0">1</ref> shows a text sample extracted from T1 corpus.</p><p>This paper summarizes our methods and results in QAst. We have participated in scenarios T1 and T2 with all transcripts and question sets. Our QA system is based on our previous work in <ref type="bibr" coords="2,101.18,279.08,10.52,8.74" target="#b0">[1]</ref> and <ref type="bibr" coords="2,133.51,279.08,9.96,8.74" target="#b5">[6]</ref>. We have used the same system architecture for all the tasks, having interchangeable language-dependant parts and different passage retrieval algorithms for automatic transcripts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview of the System Architecture</head><p>The architecture of our QA system follows a commonly-used schema which splits the process into three phases performed in a sequential pipeline: Question Processing (QP), Passage Retrieval (PR), and Answer Extraction (AE) This QA system is designed to answer to factoid questions, those whose answer is a named entity (NE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Question Processing and Classification (QC)</head><p>The main goal of this component is to detect the type of the expected answer. We currently recognize the 53 open-domain answer types from <ref type="bibr" coords="2,313.79,439.90,9.96,8.74" target="#b3">[4]</ref>. The answer types are extracted using a multi-class Perceptron classifier and a rich set of lexical, semantic and syntactic features. This classifier obtains an accuracy of 88% on the corpus of <ref type="bibr" coords="2,318.62,463.81,9.96,8.74" target="#b3">[4]</ref>. Additionally, the QP component extracts and ranks relevant keywords from the question For scenario T2, he have developed an Spanish question classifier using human translated questions from the corpus of <ref type="bibr" coords="2,222.00,499.68,10.52,8.74" target="#b3">[4]</ref> following the same machine learning approach. This classifier obtains an accuracy of 74%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Passage Retrieval (PR)</head><p>This component retrieves a set of relevant passages from the document collection, given the previously extracted question keywords. The PR algorithm uses a query relaxation procedure that iteratively adjusts the number of keywords used for retrieval and their proximity until the quality of the recovered information is satisfactory (see <ref type="bibr" coords="2,295.92,593.78,10.30,8.74" target="#b5">[6]</ref>). In each iteration a Document Retrieval application (IR engine) fetches the documents relevant for the current query and a subsequent passage construction module builds passages as segments where two consecutive keyword occurrences are separated by at most t words.</p><p>When dealing with automatic transcripts, the incorrectly transcribed words may create a problem of word recognition to the IR engine, introducing false positives and false negatives to its input.</p><p>To overcome such drawbacks, we have used an IR engine relying on phonetic similarity for the automatic transcripts. This tool is called PHAST (after PHonetic Alignment Search Tool) and uses pattern matching algorithms to search for small sequences of phones (the keywords) into a larger sequence (the documents) using a measure of sound similarity. Then the PR algorithm may be applied to the words found with PHAST. A detailed description of PHAST can be found in <ref type="bibr" coords="2,499.71,725.28,9.96,8.74" target="#b1">[2]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Answer Extraction (AE)</head><p>Identifies the exact answer to the given question within the retrieved passages. First, answer candidates are identified as the set of named entities (NEs) that occur in these passages and have the same type as the answer type detected by QP. Then, these candidates are ranked using a scoring function based on a set of heuristics that measure keyword distance and density <ref type="bibr" coords="3,469.92,284.92,9.96,8.74" target="#b4">[5]</ref>. These heuristic measures use approximated matching for AE in automatic transcripts as shown in the passage retrieval module from the previous section. The same measure is used for T1 and T2.</p><p>3 Named Entity Recognition and Classification (NERC)</p><p>As described before, we extract candidate answers from the NEs that occur in the passages retrieved by the PR component. We detail below the strategies used for NERC in both manual and automatic transcripts.</p><p>We have taken a machine learning approach to this problem. First we apply learning at word level to identify NE candidates using a BIO tagging scheme. Then these candidates are classified into NE categories. Each function is modeled with voted perceptron <ref type="bibr" coords="3,386.14,423.37,9.96,8.74" target="#b2">[3]</ref>. As learning data we have manually labeled the NEs that occur in the QAst corpora T1 and T2 with their types (i.e. date, location, number, organization, person and time).</p><p>Our NERC uses a rich set of lexical and syntactic features which are standard to state-ofthe-art NERCs. This features include: words, lemmas, POS tags, word affixes, flags regarding presence of numerals and capitalization, use of gazetteers, and n-grams of this features within a certain window of words. New features specially designed for automatic transcripts have been added to the sets a, b and c. These features use phonetic transcription of the words:</p><p>• Prefixes and suffixes of phones.</p><p>• Phonetic similarity with words in the gazetteer.</p><p>• A clustering of the transcriptions of the words has been done by grouping words with similar pronunciation. This clustering reduces the sparseness of the word-based features by mapping the words in several smaller subsets of different coarseness.</p><p>• Features capturing the possibility of splitting or merging adjacent words due to ASR recognition errors.</p><p>The addition of this phonetic features improves the results by no more than 2 points of F β=1 score in datasets a, b and c.</p><p>Given that all there is no specific datasets for development, and we don't have more automatic transcripts of EPPS data, it is not possible to train our NERC in a dataset other than the test set. Therefore we have relabeled both corpora through a process of cross-validation. Both corpora have been randomly split in 5 segments, a NERC model has been learned for all subsets of 4 segments and the remaining segment has been labeled using this model. Thus we can train our NERC with documents from the same domain but test it on unseen data. Table <ref type="table" coords="4,196.68,380.85,3.87,8.74">3</ref>: Overall factoid results for our sixteen Spanish runs.</p><p>Table <ref type="table" coords="4,132.82,412.67,4.98,8.74">3</ref> shows the overall results of our NERC for the four datasets of both T1 and T2. As the transcript WER increases, the scores consequently drop. 1   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>UPC participated in 2 of the 3 scenarios, English (T1) and Spanish (T2) ones. We submitted two runs for each task, run number 1 uses the standard NERC described in Section 3 and run number 2 uses the hand-annotated NEs. Each scenario included 100 test questions, from which 20 do not have an answer in the corpora (these are nil questions). In T1 75 question are factoids for 44 in T2. Our QA system is designed to answer only factual questions, therefore the our experimental analysis will refer only to factual questions.</p><p>We report two measures: (a) TOPk, which assigns to a question a score of 1 only if the system provided a correct answer in the top k returned; and (b) Mean Reciprocal Rank (MRR), which assigns to a question a score of 1/k, where k is the position of the correct answer, or 0 if no correct answer is found. The official evaluation of QAst 2008 uses TOP1 and TOP5 measures <ref type="bibr" coords="4,499.72,586.99,6.64,8.74" target="#b6">[7</ref>]. An answer is considered correct by the human evaluators if it contains the complete answer and nothing more, and it is supported by the corresponding document. If an answer was incomplete or it included more information than necessary or the document did not provide the justification for the answer, the answer was considered incorrect.</p><p>Tables <ref type="table" coords="4,136.19,646.76,4.98,8.74" target="#tab_1">2</ref> and<ref type="table" coords="4,163.70,646.76,4.98,8.74">3</ref> summarizes our overall results for factual questions in English and Spanish. It shows MRR, TOP1 and TOP5 scores for each track and run as defined previously.</p><p>Table <ref type="table" coords="4,132.27,670.68,4.98,8.74">4</ref> contains a statistical error analysis of our system covering the QC, PR and AE parts. It deals only with factoid questions with non-nil answers. The meaning of each column is the following. Q: number of factual question. QC: number of questions with answer type correctly detected by QP. PR: number of question where at least on passage with the correct answer war retrieved. QC&amp;PR: number of questions with correct answer type and correct passage retrieval. Table <ref type="table" coords="5,207.57,613.28,3.87,8.74">4</ref>: Error analysis of the QA system components.</p><p>C.NE: number of questions where the retrieved passages contain the correct answer tagged as a NE of the right type (specified by the QC module), so it is a candidate answer for the AE module. TOP5 non-nil: number of question with non-nil answer correctly answered by our system in the TOP5 candidates. There is an "Avg. Loss" row for each task and run that shows the performance loss (averaged in all transcripts) introduced by each module in relation to the previous step. Note that this numbers have been gathered using an automatic application and some disagreement between in the selection of factoid questions may exist, therefor the TOP5 scores in this table may differ slightly from the official QAst scores.</p><p>In Table <ref type="table" coords="5,144.52,740.75,4.98,8.74" target="#tab_1">2</ref> we can see that moving from transcript m to a implies a loss of 10 points in TOP5 score for T1. For T2 this loss is as much as 50 points. But subsequent increases of WER in transcripts b and c have a low impact in our performance. According to the QAst 2009 overview paper <ref type="bibr" coords="6,118.18,135.93,9.96,8.74" target="#b6">[7]</ref>, the incidence of WER rates in our system is less severe than in other participants but our initial results in m track are also lower. We should note an important loss of performance in T1 scenario when using the question set B. Table <ref type="table" coords="6,117.21,171.80,4.98,8.74" target="#tab_1">2</ref> shows that TOP5 and TOP1 scores decrease by 50% or more when compared to question set A. Table <ref type="table" coords="6,149.85,183.75,4.98,8.74">4</ref> shows that the number of correctly classified questions drops from 49 to 22 in T1 (44% of the original), therefore QA&amp;PR drops about 30%. The AE module has comparable performance with both sets thus this poor performance is due solely to our QC module. In scenario T2 there is a smaller loss in QC (just 30%) and it has different repercussion. The specific distribution of errors among QC and PR leads a higher QC&amp;PR count in all T2 tracks than in T1 tracks, although T1 has 20 more questions than T2, and this yields a smaller performance drop in T2. This must be blamed on our machine learning question classifier. Although this is based on shallow textual analysis, the model doesn't generalize well to spontaneous questions. Probably it has an strong dependency on the usual well-formed question structure. Additionally, we note that the classification of written questions is better for T2 question set than T1 question set. This suggests that in this evaluation T1 questions are more domain specific than the others.</p><p>The difference between runs number 1 and 2 is that number 2 uses hand-tagged NEs instead of our automatic NERC. The results show that it has little impact on performance. In Table <ref type="table" coords="6,508.02,327.21,4.98,8.74">4</ref> we can see that most of the correct answers retrieved by our PR module are annotated with the correct entity. It is shown by the small difference between QC&amp;PR and C.NE columns. Using hand-tagged NEs improves slightly the results for TOP5 and TOP1, probably because it filters out incorrect candidates and the AE process becomes easier. As we have seen in Table <ref type="table" coords="6,486.97,375.03,3.87,8.74">3</ref>, the F β=1 score of our NERC models is below 70% but this poor performance doesn't reflect in the final QA results. We think that hand-tagged NEs doesn't improve the results due to two facts. On one hand, the NERC we have developed is useful enough for this task even having poor F β=1 scores, and on the other hand, there is a probable disagreement between the humans who tagged the NEs and the humans who wrote the questions.</p><p>One of the main issues of our QAst 2009 system is the poor performance of the AE module. More than 25% of the correctly retrieved and tagged answers aren't correctly extracted in T1, and more than 50% are lost in T2. In fact, AE is the main source of errors in T2, more than the combination of both PR and QC. This is a big difference with the results achieved in 2008 evaluation, where AE was of high accuracy. It shows that our answer selecting heuristics may be more domain-dependant than we knew and they should be tuned for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>This paper describes UPC's participation in the CLEF 2009 Question Answering on Speech Transcripts track. We submitted runs for all English and Spanish scenarios. In this evaluation we analyzed the impact of using gold-standard NEs with using a far from perfect NERC.</p><p>We have developed a new NERC designed for speech transcripts that shows results competitive with gold-standard NEs when used in Question Answering.</p><p>The results achieved in the different scenarios and tasks are not the top ones. But there is little degradation due to ASR effects thus showing that our QA system is highly robust to transcript errors, being this one of the main focuses of the QAst evaluation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,181.09,199.44,240.82,8.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sample of manual and automatic transcripts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,123.63,110.41,355.75,97.17"><head>Table 1 :</head><label>1</label><figDesc>NERC performance</figDesc><table coords="3,123.63,110.41,355.75,65.45"><row><cell cols="2">T1: English</cell><cell></cell><cell></cell><cell></cell><cell cols="2">T2: Spanish</cell><cell></cell></row><row><cell cols="3">Set WER Precision</cell><cell>Recall</cell><cell>F β=1</cell><cell cols="3">Set WER Precision</cell><cell>Recall</cell><cell>F β=1</cell></row><row><cell>m</cell><cell>∼ 0%</cell><cell>70.63%</cell><cell cols="2">68.19% 69.39</cell><cell>m</cell><cell>∼ 0%</cell><cell>76.11%</cell><cell>71.19% 73.57</cell></row><row><cell>a</cell><cell>10.6%</cell><cell>63.57%</cell><cell cols="2">55.26% 59.13</cell><cell>a</cell><cell>11.5%</cell><cell>72.40%</cell><cell>62.03% 66.81</cell></row><row><cell>b</cell><cell>14%</cell><cell>61.51%</cell><cell cols="2">52.28% 56.52</cell><cell>b</cell><cell>12.7%</cell><cell>64.33%</cell><cell>55.95% 59.85</cell></row><row><cell>c</cell><cell>24.1%</cell><cell>58.62%</cell><cell cols="2">43.92% 50.22</cell><cell>c</cell><cell>13.7%</cell><cell>67.61%</cell><cell>55.60% 61.02</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,106.84,110.81,389.33,247.06"><head>Table 2 :</head><label>2</label><figDesc>Overall factoid results for our sixteen English runs.</figDesc><table coords="4,106.84,110.81,389.33,247.06"><row><cell cols="5">Task, System #Q MRR TOP1 TOP5</cell><cell cols="5">Task, System #Q MRR TOP1 TOP5</cell></row><row><cell>T1A-m 1</cell><cell>75</cell><cell>0.27</cell><cell>14</cell><cell>32</cell><cell>T1A-m 2</cell><cell>75</cell><cell>0.31</cell><cell>17</cell><cell>35</cell></row><row><cell>T1B-m 1</cell><cell>75</cell><cell>0.15</cell><cell>7</cell><cell>19</cell><cell>T1B-m 2</cell><cell>75</cell><cell>0.15</cell><cell>7</cell><cell>18</cell></row><row><cell>T1A-a 1</cell><cell>75</cell><cell>0.27</cell><cell>14</cell><cell>29</cell><cell>T1A-a 2</cell><cell>75</cell><cell>0.26</cell><cell>14</cell><cell>30</cell></row><row><cell>T1B-a 1</cell><cell>75</cell><cell>0.08</cell><cell>4</cell><cell>11</cell><cell>T1B-a 2</cell><cell>75</cell><cell>0.09</cell><cell>4</cell><cell>12</cell></row><row><cell>T1A-b 1</cell><cell>75</cell><cell>0.24</cell><cell>13</cell><cell>26</cell><cell>T1A-b 2</cell><cell>75</cell><cell>0.26</cell><cell>15</cell><cell>29</cell></row><row><cell>T1B-b 1</cell><cell>75</cell><cell>0.08</cell><cell>3</cell><cell>11</cell><cell>T1B-b 2</cell><cell>75</cell><cell>0.08</cell><cell>3</cell><cell>12</cell></row><row><cell>T1A-c 1</cell><cell>75</cell><cell>0.21</cell><cell>12</cell><cell>22</cell><cell>T1A-c 2</cell><cell>75</cell><cell>0.24</cell><cell>13</cell><cell>26</cell></row><row><cell>T1B-c 1</cell><cell>75</cell><cell>0.08</cell><cell>4</cell><cell>10</cell><cell>T1B-c 2</cell><cell>75</cell><cell>0.08</cell><cell>3</cell><cell>11</cell></row><row><cell cols="5">Task, System #Q MRR TOP1 TOP5</cell><cell cols="5">Task, System #Q MRR TOP1 TOP5</cell></row><row><cell>T2A-m 1</cell><cell>44</cell><cell>0.24</cell><cell>7</cell><cell>16</cell><cell>T2A-m 2</cell><cell>44</cell><cell>0.29</cell><cell>8</cell><cell>20</cell></row><row><cell>T2B-m 1</cell><cell>44</cell><cell>0.34</cell><cell>8</cell><cell>20</cell><cell>T2B-m 2</cell><cell>44</cell><cell>0.33</cell><cell>12</cell><cell>20</cell></row><row><cell>T2A-a 1</cell><cell>44</cell><cell>0.15</cell><cell>6</cell><cell>8</cell><cell>T2A-a 2</cell><cell>44</cell><cell>0.20</cell><cell>8</cell><cell>12</cell></row><row><cell>T2B-a 1</cell><cell>44</cell><cell>0.14</cell><cell>6</cell><cell>6</cell><cell>T2B-a 2</cell><cell>44</cell><cell>0.24</cell><cell>10</cell><cell>12</cell></row><row><cell>T2A-b 1</cell><cell>44</cell><cell>0.18</cell><cell>6</cell><cell>12</cell><cell>T2A-b 2</cell><cell>44</cell><cell>0.20</cell><cell>7</cell><cell>13</cell></row><row><cell>T2B-b 1</cell><cell>44</cell><cell>0.20</cell><cell>7</cell><cell>12</cell><cell>T2B-b 2</cell><cell>44</cell><cell>0.20</cell><cell>7</cell><cell>12</cell></row><row><cell>T2A-c 1</cell><cell>44</cell><cell>0.22</cell><cell>9</cell><cell>12</cell><cell>T2A-c 2</cell><cell>44</cell><cell>0.20</cell><cell>8</cell><cell>11</cell></row><row><cell>T2B-c 1</cell><cell>44</cell><cell>0.13</cell><cell>5</cell><cell>8</cell><cell>T2B-c 2</cell><cell>44</cell><cell>0.21</cell><cell>9</cell><cell>10</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work has been partially funded by the <rs type="funder">Spanish Ministry of Science and Technology (TEXTMESS project)</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="7,105.50,133.84,407.51,8.74;7,105.50,145.80,407.51,8.74;7,105.50,157.75,94.60,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,221.45,133.84,291.56,8.74;7,105.50,145.80,18.04,8.74">Robust question answering for speech transcripts: Upc experience in qast</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">R</forename><surname>Comas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Turmo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,153.07,145.80,359.94,8.74;7,105.50,157.75,64.12,8.74">Proceedings of the CLEF 2008 Workshop on Cross-Language Information Retrieval and Evaluation</title>
		<meeting>the CLEF 2008 Workshop on Cross-Language Information Retrieval and Evaluation</meeting>
		<imprint>
			<date type="published" when="2008">2009. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,177.68,407.50,8.74;7,105.50,189.63,351.97,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,225.35,177.68,287.64,8.74;7,105.50,189.63,19.70,8.74">Spoken document retrieval based on approximated sequence alignment</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">R</forename><surname>Comas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Turmo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,134.54,189.63,291.72,8.74">11th International Conference on Text, Speech and Dialogue (TSD)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,209.56,407.50,8.74;7,105.50,221.51,407.50,8.74;7,105.50,233.47,22.69,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,234.86,209.56,257.95,8.74">Large margin classification using the perceptron algorithm</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,105.50,221.51,403.31,8.74">COLT&apos; 98: Proceedings of the eleventh annual conference on Computational learning theory</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,253.39,407.50,8.74;7,105.50,265.35,172.42,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,196.10,253.39,273.75,8.74">Learning question classifiers: The role of semantic information</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,479.89,253.39,33.11,8.74;7,105.50,265.35,141.87,8.74">Journal of Natural Language Engineering</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,285.27,407.50,8.74;7,105.50,297.23,270.57,8.74" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="7,162.37,285.27,346.69,8.74">High-performance, open-domain question answering from large text collections</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pacsca</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<pubPlace>Dallas, TX</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Southern Methodist University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="7,105.50,317.15,407.50,8.74;7,105.50,329.11,407.50,8.74;7,105.50,341.06,386.40,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,343.41,317.15,169.59,8.74;7,105.50,329.11,318.08,8.74">Design and performance analysis of a factoid question answering system for spontaneous speech transcriptions</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Dominguez-Sal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">R</forename><surname>Comas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,433.97,329.11,79.02,8.74;7,105.50,341.06,355.86,8.74">Proceedings of the International Conference on Spoken Language Processing (INTERSPEECH 2006)</title>
		<meeting>the International Conference on Spoken Language Processing (INTERSPEECH 2006)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.50,360.99,407.50,8.74;7,105.50,372.94,407.50,8.74;7,105.50,384.90,192.53,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,133.40,372.94,83.49,8.74">Overview of QAST</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Turmo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">R</forename><surname>Comas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Galibert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mostefa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Buscaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,247.63,372.94,265.37,8.74;7,105.50,384.90,162.04,8.74">Proceedings of the CLEF 2009 Workshop on Cross-Language Information Retrieval and Evaluation</title>
		<meeting>the CLEF 2009 Workshop on Cross-Language Information Retrieval and Evaluation</meeting>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
