<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,187.22,89.31,221.14,12.64">NLEL-MAAT at CLEF-ResPubliQA</title>
				<funder ref="#_meBUpj6">
					<orgName type="full">Universidad PolitÃ©cnica de Valencia &quot;MÃ³dulo de servicios semÃ¡nticos de la plataforma</orgName>
				</funder>
				<funder ref="#_TFwzxPz">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder>
					<orgName type="full">Maat Knowledge</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,201.05,128.22,62.86,8.96"><forename type="first">Santiago</forename><surname>Correa</surname></persName>
							<email>scorrea@dsic.upv.es</email>
							<affiliation key="aff0">
								<orgName type="department">DSIC</orgName>
								<orgName type="laboratory" key="lab1">NLE Lab</orgName>
								<orgName type="laboratory" key="lab2">ELiRF Research Group</orgName>
								<orgName type="institution">Universidad PolitÃ©cnica de Valencia</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,270.68,128.22,64.65,8.96"><forename type="first">Davide</forename><surname>Buscaldi</surname></persName>
							<email>dbuscaldi@dsic.upv.es</email>
							<affiliation key="aff0">
								<orgName type="department">DSIC</orgName>
								<orgName type="laboratory" key="lab1">NLE Lab</orgName>
								<orgName type="laboratory" key="lab2">ELiRF Research Group</orgName>
								<orgName type="institution">Universidad PolitÃ©cnica de Valencia</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,187.22,89.31,221.14,12.64">NLEL-MAAT at CLEF-ResPubliQA</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B4B028511A4EA337FECFBFFF73DCD9D0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>I.2 [Artificial Intelligence]: I.2.7 Natural Language Processing Measurement, Performance, Experimentation Question Answering, Information Retrieval and Passage Retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This report presents the work carried out at NLE Lab for the QA@CLEF-2009 competition. We used the JIRS passage retrieval system, which is based on redundancy, with the assumption that it is possible to find the response to a question in a large enough document collection. The retrieved passages are ranked depending on the number, length and position of the question n-grams structures found in the passages. The best results were obtained in monolingual English, while the worst results were obtained for French. We suppose the difference is due to the question style that varies considerably from one language to another.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>An open-domain Question Answering (QA) system can be viewed as a specific Information Retrieval (IR) system, in which the amount of information retrieved is the minimum amount of information required to satisfy a user information need expressed as a specific question, e.g.: "Where is the Europol Drugs Unit?". Many QA systems are based on Passage Retrieval (PR) <ref type="bibr" coords="1,254.33,525.35,10.91,8.96" target="#b0">[1,</ref><ref type="bibr" coords="1,265.24,525.35,7.27,8.96" target="#b1">2]</ref>. A PR system is an IR system that returns parts of documents (passages) instead of complete documents. Their utility in the QA task is based on the fact that in many cases the information needed to reply a question is usually contained in a small portion of the text <ref type="bibr" coords="1,425.36,549.35,11.04,8.96" target="#b2">[3]</ref>.</p><p>In the 2009 edition of CLEF, the competition ResPubliQA 1 has been organized, consisting in a narrow domain QA task, centered on the legal domain, given that the data is constituted by the body of European Union (EU) law. Our participation in this competition has been based on the JIRS 2 open source PR system, which has proved to be able to obtain better results than classical IR search engines in the previous open-domain CLEF QA tasks <ref type="bibr" coords="1,94.10,609.38,10.68,8.96" target="#b4">[4]</ref>. In this way we desired to evaluate the effectiveness of this PR system in this specific domain and to check our hypothesis that answers usually are formulated similarly to questions. In the next section we describe the characteristics of competition; furthermore, paragraphs 3 and 4 explain the main concepts of JIRS system and we discuss how it has been applied in solving the problem, continuing, in paragraph 5, with the results and the conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Multiple Language Question Answering Task</head><p>In this task the systems receive in input natural language questions about knowledge of European law, and these, in turn, should return a paragraph containing the response from the documents collection. This constitutes an important difference with respect to previous QA tasks where the answer had to be extracted by the system. For this reason we employed just the JIRS system instead of the complete QUASAR QA system we developed for previous participations.</p><p>The document collection is composed by the JRC-Acquis corpus 3 , containing the complete EU legislation, including texts between the years 1950 to 2006 (in total 10,700 documents); these documents have been aligned in parallel and were made available to the competition in the following languages: Bulgarian, Dutch, English, French, German, Italian, Portuguese, Romanian and Spanish. The corpus is encoded in XML format according to the TEI guidelines 4 . Each document have a title and is subdivided in a series of phrases, each one marked with the "&lt;p&gt;" tag.</p><p>The test set is composed of 500 questions that must be analyzed by the systems to return a paragraph that contains the answer to the formulated question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The passage retrieval engine JIRS</head><p>Nowadays, many passage retrieval systems are not targeted to the specific problem of finding answers, due to the fact that they only take into account the keywords of the question to find the relevant passages. The information retrieval system JIRS is a based on n-grams (an n-gram is a sequence of n adjacent words extracted from a sentence or a question.) instead than keywords. JIRS is based on the premise that in a large collection of documents, an n-gram associated with a question must be found in this collection at least once.</p><p>JIRS starts searching the candidate passage with a standard keyword search that retrieves an initial set of passages. These passages are ranked later depending on the number, position and length of the question n-grams that are found in the passages. E.g.: suppose you have a publications database of a newspaper, using the JIRS system and based on these documents you will find the answer to the question: "Who is the president of Colombia?"; The system could retrieve the following two passages: "... Ãlvaro Uribe is the president of Colombia ..." and "...Giorgio Napolitano is the president of Italy...". Of course, the first passage should have more relevance as it contains the 5-gram "is the president of Colombia", while the second passage contains only the 4-gram "is the president of". To calculate the n-grams weight of each passage, first of all you must identify the most relevant n-gram and assign to it an equal weight to the all weights sum of the terms. The weight of each term is set to:</p><formula xml:id="formula_0" coords="2,253.13,549.06,271.43,17.88">ğ‘¤ ğ‘˜ = 1 - log (ğ‘› ğ‘˜ ) 1+log (ğ‘) (1)</formula><p>Where n k is the number of passages in which the term appears and N is the total number of passages in the system.</p><p>The similarity between a passage d and a question q is determinated by:</p><formula xml:id="formula_1" coords="2,229.37,614.75,295.19,24.00">ğ‘ ğ‘–ğ‘š ğ‘‘, ğ‘ = â„ ğ‘¥ ,ğ· ğ‘— ğ‘¥ âˆˆğ‘„ ğ‘› ğ‘— =1 â„ ğ‘¥,ğ‘„ ğ‘— ğ‘¥ âˆˆğ‘„ ğ‘› ğ‘— =1 (2)</formula><p>Where â„(ğ‘¥, ğ· ğ‘— ) returns a weight for the j-gram x with respect to the set of j-grams (D j ) in the passage:</p><formula xml:id="formula_2" coords="2,224.21,669.93,300.35,26.11">â„ ğ‘¥, ğ· ğ‘— = ğ‘Š ğ‘¥ ğ‘–ğ‘“ ğ‘¥ âˆˆ ğ· ğ‘— ğ‘¥ ğ‘˜=1 0 ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’<label>(3)</label></formula><p>A more detailed description of the system JIRS can be found in <ref type="bibr" coords="2,343.21,708.86,10.82,8.96" target="#b5">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Adaptation of JIRS to the Task</head><p>The data had to be preprocessed, due to the format of the collection employed in the ResPubliQA@CLEF-2009 competition, a subset of the JRC-ACQUIS Multilingual Parallel Corpus, this Corpus containing the total body of European Union (EU) documents, of mostly legal nature. In particular, the subset is constituted by documents of 9 out of 22 languages. It consists of approximately of 10,700 parallel and aligned documents per language. The documents cover various subject domains: economy, health, information technology, law, agriculture, food, politics and more.</p><p>To be able to use the JIRS system in this task, the documents were analysed and transformed for proper indexing. Since JIRS uses passages as basic indexing unit, it was necessary to extract passages from the documents. We consider any paragraph included between &lt;p&gt; tags as a passage. Therefore, each paragraph was labelled with the name of the containing document and its paragraph number.</p><p>Once the collection was indexed by JIRS, the system was ready to proceed with the search for the answers to the questions provided by the competition. For each question, the system returned a list with the passages that most likely contained the answer to the question, according to the JIRS weighting scheme. In an additional experiment, we used the parallel collection to obtain a list of answers in different languages (Spanish, English, Italian and French), choosing as the best answer the one that obtained the best score by JIRS and subsequently taking the identifier of each paragraph (answer) for retrieving the aligned paragraph in the target language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>We submitted four "pure" monolingual runs for the following languages: English, French, Italian and Spanish, and in an additional experiment we exploited the parallel corpus to produce a monolingual Spanish run. This experiment consisted in searching the question in all languages, and selecting the passage with the highest similarity; finally, the returned passage was the Spanish alignment of this best passage. In Table <ref type="table" coords="3,465.50,436.91,4.98,8.96" target="#tab_0">1</ref> we show the official results for the submitted runs. From Figure <ref type="figure" coords="3,127.42,617.18,4.98,8.96" target="#fig_0">1</ref> we can see that the result obtained in English were particularly good, while in French and Spansish the percentage of wrong answers is very high. We did not expect this behaviour for the Spanish language, since JIRS was developed specifically for the Spanish QA task. On the other hand, we expected the French to be the language in which the system obtained the worst results, because of the results of the system at previous QA competitions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>The difference between the best results (for English) and the worst ones (in French) is of 22%. This may reflect the different way of formulating questions in each language. The abandonment of the Question Classification and Answer Extraction phases, with respect to our previous participation in the CLEF QA tasks, did not result particularly useful. In many cases, the right answer was available in a passage ranked lower, and this could have been detected by checking the question type and determining if the passage contained a candidate answer. We plan to re-introduce these modules for further participations, at least for the validation of the returned passage, that is, to check if it contains the answer or not.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,202.61,300.14,190.32,8.10;4,99.45,142.05,398.84,161.79"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Comparative graph for all the submitted runs.</figDesc><graphic coords="4,99.45,142.05,398.84,161.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,72.74,472.72,449.45,128.49"><head>Table 1 .</head><label>1</label><figDesc>Results for submitted runs. Ans.: Answered, Unans.: Unanswered, A.R.: Answered Rigth, A.W.:</figDesc><table coords="3,456.28,472.72,65.92,8.10"><row><cell>Answered Wrong,</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,76.22,751.77,440.10,8.10"><p>For more information about the competition ResPubliQA @ CLEF-2009, refer to page: http://celct.isti.cnr.it/ResPubliQA/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,76.22,762.69,124.67,8.10"><p>http://sourceforge.net/projects/jirs/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,76.22,751.77,87.53,8.10"><p>http://wt.jrc.it/lt/Acquis/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,76.22,762.69,117.43,8.10"><p>http://www.tei-c.org/Guidelines/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The work of the first author has been possible thanks to a scholarship funded by <rs type="funder">Maat Knowledge</rs> in the context of the joint project with the <rs type="funder">Universidad PolitÃ©cnica de Valencia "MÃ³dulo de servicios semÃ¡nticos de la plataforma</rs> G". We also thank the <rs type="grantNumber">TEXT-ENTERPRISE 2</rs>.<rs type="grantNumber">0 TIN2009-13391-C04-03</rs> research project.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_meBUpj6">
					<idno type="grant-number">TEXT-ENTERPRISE 2</idno>
				</org>
				<org type="funding" xml:id="_TFwzxPz">
					<idno type="grant-number">0 TIN2009-13391-C04-03</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="4,84.26,588.19,440.27,8.10;4,82.34,598.51,442.11,8.10;4,82.34,608.83,441.86,8.10;4,82.34,619.15,203.78,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,324.80,588.19,199.72,8.10;4,82.34,598.51,84.67,8.10">Quantitative evaluation of passage retrieval algorithms for question answering</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tellex</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Marton</surname></persName>
		</author>
		<idno type="DOI">10.1145/860435.860445</idno>
		<ptr target="http://doi.acm.org/10.1145/860435.860445" />
	</analytic>
	<monogr>
		<title level="m" coord="4,186.88,598.51,337.58,8.10;4,82.34,608.83,137.16,8.10;4,397.52,608.83,36.07,8.10">Proceedings of the 26th Annual international ACM SIGIR Conference on Research and Development in informaion Retrieval</title>
		<meeting>the 26th Annual international ACM SIGIR Conference on Research and Development in informaion Retrieval<address><addrLine>Toronto, Canada; New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003-07-28">2003. July 28 -August 01, 2003</date>
			<biblScope unit="page" from="41" to="47" />
		</imprint>
	</monogr>
	<note>SIGIR &apos;03</note>
</biblStruct>

<biblStruct coords="4,84.85,629.59,439.67,8.10;4,82.34,639.91,441.67,8.10;4,82.34,650.23,121.29,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,232.90,629.59,291.62,8.10;4,82.34,639.91,221.32,8.10">Experiments on robust NL question interpretation and multi-layered document annotation for a cross-language question/answering system</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Sacaleanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,323.87,639.91,200.14,8.10;4,82.34,650.23,23.64,8.10">Multilingual information access for text, speech and images</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3491</biblScope>
			<biblScope unit="page" from="411" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,84.37,660.55,439.67,8.10;4,82.34,670.99,416.97,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,154.94,660.55,164.22,8.10">Passage-level evidence in document retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,335.72,660.55,188.32,8.10;4,82.34,670.99,275.44,8.10">Proceedings of the 17th Annual international ACM SIGIR Conference on Research and Development in information Retrieval</title>
		<meeting>the 17th Annual international ACM SIGIR Conference on Research and Development in information Retrieval<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-07-03">1994. July 03 -06, 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,502.45,670.99,22.03,8.10;4,82.34,681.31,441.90,8.10;4,82.34,691.63,195.10,8.10" xml:id="b3">
	<monogr>
		<title level="m" coord="4,219.91,681.31,300.83,8.10">Annual ACM Conference on Research and Development in Information Retrieval</title>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</editor>
		<meeting><address><addrLine>New York, New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<biblScope unit="page" from="302" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,84.01,701.95,440.45,8.10;4,82.34,712.39,77.09,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,256.48,701.95,263.80,8.10">N-Gram vs. Keyword-Based Passage Retrieval for Question Answering</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Buscaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Sanchis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,82.34,712.39,21.91,8.10">CLEF</title>
		<imprint>
			<biblScope unit="page" from="377" to="384" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,84.74,722.71,439.70,8.10;4,82.34,733.05,194.89,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="4,262.73,722.71,257.75,8.10">Answering Questions with an n-gram based Passage Retrieval Engine</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Buscaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Sanchis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,82.34,733.05,152.75,8.10">Journal of Intelligent Information Systems</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
