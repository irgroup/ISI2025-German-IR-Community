<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,113.64,94.25,375.74,22.40;1,120.84,116.21,360.89,22.40;1,263.40,138.05,76.21,22.40">GikiCLEF: Crosscultural issues in an international setting: asking non-English-centered questions to Wikipedia</title>
				<funder ref="#_Fr6TPfP">
					<orgName type="full">FCCN</orgName>
				</funder>
				<funder ref="#_Da4CAy5">
					<orgName type="full">European Union (FEDER and FSE)</orgName>
				</funder>
				<funder>
					<orgName type="full">UMIC</orgName>
				</funder>
				<funder>
					<orgName type="full">Portuguese Government</orgName>
				</funder>
				<funder ref="#_A78nk3R">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,233.52,174.29,75.51,12.00"><roleName>Lu√≠s</roleName><forename type="first">Diana</forename><surname>Santos</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Oslo</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">SINTEF ICT</orgName>
								<address>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,311.55,174.29,57.78,12.00;1,209.76,188.21,42.70,12.01"><forename type="first">Miguel</forename><forename type="middle">Cabral</forename><surname>Linguateca</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Oslo</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,113.64,94.25,375.74,22.40;1,120.84,116.21,360.89,22.40;1,263.40,138.05,76.21,22.40">GikiCLEF: Crosscultural issues in an international setting: asking non-English-centered questions to Wikipedia</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">475A3447E31C3BCB9D9B66E1D2D5B368</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Managment]: Languages-Query Languages 1 Motivation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we provide a full overview of GikiCLEF, an evaluation contest (track) that was specifically designed to expose and investigate cultural and linguistic issues involved in multimedia collections and searching. In GikiCLEF, 50 topics were developed by a multilingual team with non-English users in mind. Answers should be found in Wikipedia, but not trivially, in the sense that the task should be difficult for human users as well. Crosslinguality was fostered and encouraged by the evaluation measures employed. We present the motivation and the organization process, the management system developed, dubbed SIGA, an overview of participation and results, concluding with what we have learned from the whole initiative.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Why Wikipedia</head><p>Wikipedia is here being used as the source of freely available multilingual data, semistructured and with some quality control, that is, as an invaluable resource to gather semantic data for natural language processing, as advocated by many as a solution to the knowledge acquisition bottleneck. We are obviously not the first to see Wikipedia in this light, cf. <ref type="bibr" coords="2,274.14,114.29,10.78,12.00" target="#b0">[1,</ref><ref type="bibr" coords="2,287.80,114.29,7.42,12.00" target="#b4">5,</ref><ref type="bibr" coords="2,297.98,114.29,12.45,12.00" target="#b22">21,</ref><ref type="bibr" coords="2,313.19,114.29,7.54,12.00" target="#b6">7,</ref><ref type="bibr" coords="2,323.49,114.29,11.86,12.00" target="#b21">20]</ref>. However, in our present case Wikipedia is rather seen as a user environment which has billions of users and to which -through GikiCLEF -we are contributing to provide a better user experience: one should be able to pose questions to Wikipedia and find a list of articles that provide the answer. Furthermore, in a third way, we are also looking at Wikipedia as providing raw material for an evaluation contest, as has been done by <ref type="bibr" coords="2,379.20,162.05,16.66,12.00" target="#b10">[11]</ref> some years ago. It should be however clear that the systems developed for querying Wikipedia in an intelligent way are not necessarily usable only in that context: on the contrary, we expect that the insights and techniques used could be generalized or adapted to all other sources of (multilingual) encyclopedic information as well as other large sized wiki-like sites, and should not be too dependent on particular Wikipedia idyosincrasies.</p><p>To our knowledge, Wikipedia snaphsots are by far the largest (partially aligned) multilingual corpora that have the highest number of crosslingual links. Most other Web pages have just one or two other languages to which they are linked, as can be appreciated e.g. in <ref type="bibr" coords="2,353.90,245.81,15.33,12.01" target="#b15">[15]</ref>. <ref type="foot" coords="2,373.44,244.98,3.48,8.40" target="#foot_0">1</ref> But we should hasten to say that we do not believe that the existence of crosslingual links means the existence of independently edited and equally reliable information: in fact, the more paralell the information in two language versions of the same topic, the more probable that one is the translation of the other. Also, we are quite aware that there is a Wikipedia bias in terms of subjects covered, as pointed e.g. by Veale <ref type="bibr" coords="2,396.37,293.57,15.53,12.00" target="#b21">[20]</ref>: there is a much higher population of science fiction and comics heros as compared for example with traditional desserts.</p><p>This said, and given that Wikipedia is something that evolves daily, it is challenging to process something real (and therefore with inconsistencies and problems), rather than a formal model which is an idealization, and keeps us closed in a lab.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task description</head><p>In GikiCLEF, systems need to answer or address geographically challenging topics, on the Wikipedia collections, returning list of answers in the form of Wikipedia document titles.</p><p>The "geographical domain" was chosen, not only on internal CLEF grounds (to maintain the tradition started by GeoCLEF and continued with GikiP) or because it is a hot topic nowadays, but because it displays a huge variety in natural language that current gazetteer compilers are often not aware of. We believed it made sense to look at geographically-related queries in order to highlight what is or may be different from language to language, or culture to culture. In fact, in other spheres of thought there have been strong claims for different spatial conceptualizations in languages, see <ref type="bibr" coords="2,394.07,481.01,10.78,12.00" target="#b2">[3,</ref><ref type="bibr" coords="2,407.49,481.01,11.86,12.00" target="#b19">18]</ref>, and this is a recurring theme in the GeoCLEF series papers <ref type="bibr" coords="2,238.77,492.89,10.78,12.00" target="#b8">[9,</ref><ref type="bibr" coords="2,251.95,492.89,12.45,12.00" target="#b9">10,</ref><ref type="bibr" coords="2,266.92,492.89,13.30,12.00" target="#b12">13]</ref> as well.</p><p>In practice, a system participating in GikiCLEF receives a set of topics representing valid and realistic user needs, coming from a range of different cultures and languages -in all GikiCLEF languages, namely Bulgarian, Dutch, English, German, Italian, Norwegian -both Bokm√•l and Nynorsk<ref type="foot" coords="2,431.16,527.94,3.48,8.40" target="#foot_1">2</ref> -, Portuguese, Romanian and Spanish, and its output is a list of answers, in all languages it can find answers. This kind of output seems to be appropriate, considering that it would be followed by a "output formatter module": For different kinds of human users, and depending on the languages those users could read, different possible output formats would filter the information per language, as well as rank it in order of preference. We are assuming here that people prefer to read answers in their native languages, but that most people are happier with answers (remember, answers are titles of Wikipedia entries) in other languages they also know or even just slightly understand, than with no answers at all.</p><p>Since we are aware that not all GikiCLEF participants have the resources and interest to process or give answers in the ten collections, we have added the option of "languages of participation" to the registration process, that is, languages of the users the systems want to please. However, as will be explained presently, systems not tackling all languages will at once have a lower score.</p><p>The evaluation measures are then as follows for a given run, and for each language:</p><p>¬¢ C: number of correct (that is, justified in at least one language) answers ¬¢ N: total number of answers provided by the system ¬¢ GikiCLEF score per language: C*C/N (so one has a score for de, pt, etc, as ¬£ ¬•¬§ ¬ß¬¶ ¬©¬£ ¬§ ¬ß¬¶ ¬§ ¬ß¬¶ , ¬£ ¬£! " ¬ß # , etc.)</p><p>The final score of any system is given by the sum of the scores for each individual language. So, the more languages a system returns answers in, the better its scores. Furthermore, a language with no answers for a particular topic (C=0) will not contribute for the relative ordering of the systems.</p><p>Note that a score for a particular language is the sum for all topics, not the average of the scores per topic. This is in order not to penalize languages which have no information on a particular topic in their Wikipedia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The organization of GikiCLEF</head><p>The Wikipedia collections for all GikiCLEF languages were released on 20 January, 2009, and correspond to the Wikipedia snapshots from June 2008. They were converted to XML with the WikiXML tool created by the University of Amsterdam, which is available from http://ilps.science.uva.nl/WikiXML/. Figure <ref type="figure" coords="3,507.49,318.89,5.03,12.01" target="#fig_0">1</ref> presents their relative sizes. Later on, due to some problems in the conversion, we allowed participants to use the HTML versions as well. This was the only task performed prior to the development of SIGA<ref type="foot" coords="3,387.12,521.70,3.48,8.40" target="#foot_2">3</ref> , which we then developed in order to assist both organizers and participants in the GikiCLEF task. In fact, four distinct roles had to be implemented, with different access modes and privileges: participant, topic manager, assessor, and administrator.</p><p>Briefly, the different tasks involved in the several phases of GikiCLEF, in a loose chronological order, were:</p><p>Topic management The process of developing topics, finding some answers (pre-determined, and mark if they were self-justified or required further information), translate the wording into other languages and provide a motivation for them (for topic managers);</p><p>Participation Fetching the topics, submitting answers and validating them , getting final individual scores (for participants);</p><p>Answer pool creation The process of merging all answers from all runs, come up with a pool of unique answers to be assessed, and attribute them to different assessors, with some overlap per language (for administrators);</p><p>Topic assessment The process of evaluating individual answers (and their justifications) as well as discuss hard cases (for assessors);</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict resolution</head><p>Comparing the assessments done by different assessors and proceed to a final decision (for administrators);</p><p>Results computation For each run, propagate the justification to other languages, do another (crosslingual) conflict resolution, obtain individual scores, and provide aggregated results (for administrators).</p><p>A system helping during all these phases was necessary since GikiCLEF had a really large (and geographically distributed) organizer committee, and the same was even more true of the assessors and participants masses. SIGA was developed in MySQL, Perl and PhP and its source code is available, under a Gnu license, from the GikiCLEF site. We will be presenting SIGA along with the description of the process followed in 2009.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Topics: their preparation and related issues</head><p>The final topics were released 15 May 2009, after a set of 24 example topics, displayed in Table <ref type="table" coords="4,487.23,326.81,3.77,12.01" target="#tab_0">1</ref>, had been made available some months before. EX08 What Belgians won the Tour de France exactly twice? EX09 Find Amazon tribes which have no written language EX10 Find Northern Europe companies which produce nano-electronic components for planes. EX11 Which MPB musicians are also distinguished Spanish guitar players? EX12 Which stave churches are still used for religious purposes in Norway? EX13 Name Brazilian theologists connected with Liberation Theology. EX14 Amusement parks in East Germany EX15 Technical universities in Germany with more than 10,000 students EX16 Carnival strongholds in the Rhineland EX17 Cathedral libraries in Germany EX18 Cities having an Olympic stadium EX19 Cities situated by the former Eifel Aqueduct EX20 Castles in Upper Bavaria, that are still inhabited EX21 German zoos with animals from the African savannah EX22 Cities, that have a womens's football (soccer) team, that won the Women's Football Bundesliga EX23 Terminal stations in Germany with more than 100,000 passengers a day EX24 Transalpine UNESCO World Heritage Sites, listed before 2000  <ref type="table" coords="6,273.30,286.37,3.77,12.01">2</ref>, let us present the topic creation guidelines and our expectations (not necessarily met by the final set, as will be hinted at in the final discussion):</p><p>¬¢ One should strive for realistic topics which can be answered in some Wikipedia covered by Giki- CLEF, chosen with a conscious cultural bias so that not all Wikipedia would have that information.</p><p>¬¢ Ideal topics for GikiCLEF may require knowledge of culture to understand the way they should be answered (or better, what it is that is being sought). This requirement entails that translation into other languages may require lengthy explanations. For example, Spanish guitar is a technical term in music that is probably not the best way to translate viol √£o, the Brazilian (original) term. Also, to render the Norwegian oppvekstroman requires the clarification that this is close, but not the same as what, in English, literature experts use the German (!) term Bildungsroman to express. Similary, Romanian balade is probably a false friend with Spanish ballada, and had to be translated by romance. Interestingly, this is again a false friend with Portuguese romance, denoting what in English is called novel.</p><p>¬¢ Answers to the questions had to be justified in at least one Wikipedia (that is, the string may be found as a entry in all Wikipedias, but the rest of the information has to be found in at least one). So, we are not looking for absolute truth, we are looking for answers which are justified in Wikipedia.</p><p>¬¢ Questions may include ambiguous concepts or names, especially when translated. In that case, participants were warned that only answers related to the proper disambiguation will be considered correct e.g. Which countries did Bush visit in the first two years of his mandate? will not be correctly answered by the singer Kate Bush's travels in whatever mandate she may have (had). Narratives<ref type="foot" coords="6,509.16,546.54,3.48,8.40" target="#foot_3">4</ref> should thus clearly specify and explain the exact user need.</p><p>¬¢ In case there appear ambiguities in the topic formulation that have not been discussed or clarified in the narrative, and which have more than one interpretation acceptable (with respect to the user model at stake), assessment will accept both. For example, in Award-winning Romanian actresses in international cinema festivals, one would have to accept not only those actresses actually receiving prizes, but also those just in the audience or even hosting the event, if that had not been made clear beforehand (in the Further clarification text).</p><p>¬¢ Different answers about the same subject are welcome, provided they have support in the mate- rial. Examples are "Who is (considered to be) the founder of mathematics?" or "Name the greatest scientific breakthroughs in the XIXth century", which are obviously open to different opinions. <ref type="foot" coords="7,493.80,82.74,3.48,8.40" target="#foot_4">5</ref>During the topic discussion phase, the topic creation group came up with 75 topics, from which the final 50 were chosen according to the following additional criteria: avoid repetition, avoid quizz-like flavour, avoid hard to interpret topics, and then removing randomly until the number 50 was reached.</p><p>As an integral part of topic choice and preparation, SIGA helped the topic managers to look for answers in titles of Wikipedia documents pertaining to the GikiCLEF collection, as illustrated by Figure <ref type="figure" coords="7,472.52,150.17,3.77,12.01" target="#fig_2">3</ref>.</p><p>We expected that this process of finding candidates by just looking in the titles would be of considerable help for topic managers, who would not need to deal with the large collections in order to list correct answers. However, we did not require that people stored the answers there during topic creation.  This was something we provided as a facility in order to avoid, later, much work during assessment. Interestingly, only half of the members of the topic group used this, and also for different topics and for different languages there were different policies. Some people did it for the topics they owned in almost all languages, others did it for all topics only in their language, some did no pre-storing at all, and the majority did just some and in some languages. In Figure <ref type="figure" coords="7,280.81,676.61,5.03,12.01" target="#fig_3">4</ref> one can see the result of this process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Expected answers in GikiCLEF</head><p>Systems were supposed to deliver as many answers (in as many languages) as possible, but answers had to be justified in at least one language. For an answer to be considered justified, it required simply that a person would accept the answer by reading it (the article) and further documents offered as additional material. Of course this is ultimately subjective, but all evaluation in information retrieval is. In order to ensure a maximum of fairness, guidelines for borderline cases had to be discussed among the assessors and normalized in the end, to the best of our abilities (and to the strain of the assessors, who had often to reassess their answers).</p><p>The Wikipedia page about the answer may be its own justification (the simplest case), but we imagined, and catered for, cases where other pages would have to be brought to bear (such as disambiguation or lists, or even images).</p><p>An answer without justification was never to be considered right.</p><p>Let us provide two examples in more detail: Question In which places did Italo Calvino live during adulthood? would require a system to go to the page(s) devoted to this writer, find that information, and get the places, namely e.g. Turin and Paris. In order to have these accepted as correct answers, the page about Italo Calvino which describes his life, e.g. http://en.wikipedia.org/wiki/ItaloCalvino, would have to be included as (further) justification for Paris and Turin.</p><p>Once there was a justification (in this example and to make it easier for the present paper, in English -although the most complete is probably in the Italian Wikipedia), any answers like Turim in Portuguese or Parisj in Dutch would be considered correct: in other words -once justified in a particular language, justified for all languages. Now to a more complex example, to show how the GikiCLEF format allows arbitrary chains of reasoning -which is not to say that we expected current systems to be able to do it. Take question Name American cities where people who killed presidents lived for more than one year. To answer it, in addition to the name of the city, systems would have to find the names of presidents who were killed, and -although in this particular case there is even a category in Wikipedia entitled "United States presidential assassination attempts" -this might require that systems go through all pages concerning presidents and investigate the deaths, their causes and the names of the assassins, then check the assassins' pages, and finally extract the cities where they lived.</p><p>In order for an answer to be justified, let us say the answer "Chicago", the justification would have to include the page of the assassin that mentions that place, and the name of the president killed as well if this is not mentioned in the assassin's page. So, in principle at least, one may have to include several pages in order to justify any given answer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Asssessment and evaluation</head><p>In GikiCLEF, as before in GikiP, only answers / documents of the correct type were considered correct. That is, if the question is about people, an answer of an organization is considered wrong, even if in that document whose title was an organization there is the person one would want as answer.</p><p>After pooling all answers returned by the participant systems, they were manually assessed by the assessors' group. SIGA's assessment interface, displayed in Figure <ref type="figure" coords="9,367.84,129.77,3.77,12.01" target="#fig_4">5</ref>, allows the assessors to judge the candidate answers, and check the correctness of their justifications.</p><p>Prior to this, to ease the assessment task, an automatic process assesses the answer documents that were listed as correct answers during the topic preparation period, as well as eliminates invalid document answers (such as redirects).</p><p>Assessment in GikiCLEF proceeds in several phases: A new process of crosslingual conflict resolution then ensues, with the net result that positively conflicting information for one topic brings about the inhibition of multilingual propagation: for those topics, only monolingually correct and justified answers will be considered correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Final scores are computed and displayed</head><p>It goes without saying that all these phases and checkpoints allowed us to find problems, inconsistencies and even wrongly pre-defined answers in the original topic set. Figure <ref type="figure" coords="9,133.05,434.09,5.03,12.01" target="#fig_5">6</ref> displays SIGA's assistance in conflict solving. The administrator can choose to send a question to the diverging assessors, or decide herself, if it is a straightforward case. The final scores are automatically computed after the assessment task and made available to the participants, who are granted access to several scores and the detailed assessment of their answers, as illustrated in Figure <ref type="figure" coords="9,128.72,693.29,3.77,12.01" target="#fig_6">7</ref>. While this seems a complete enough description of the assessment process, one should document that a lot of other more specific decisions and guidelines had to be decided during the process. By writing them down here we intend not only to illustrate the kinds of problems that arise, but also provide an initial set for further initiatives or GikiCLEF editions.</p><p>1. If the answer is already contained in the question, it is considered incorrect. For example, Italy is not a fair answer to a question starting by List the Italian places 2. If there is principled disagreement about vague, complex categories and different people have strong reasons for disagreement, for GikiCLEF we accept the union of all.</p><p>3. Speaking/writing poets in other languages than Romanian are Romanian poets? We decided for a yes.</p><p>4. Studying in a place, taking a short visit to another place and coming back in love to that place, does it qualify as a place where someone falls in love? Again, yes.</p><p>5. If a ciclist won the junior Tour de Flandres and then the adult one, is s/he considered a winner twice?</p><p>We decided for yes, although this is a recurrent issue in sports questions. Often, without further specification, only the major competition is meant.</p><p>6. Very slight differences which very strongly convey the probability of yes are accepted, because we would expect most people (except lawyers and logicians) to accept that:</p><p>¬¢ Eight thousanders accept a 50 m deviation (if a mountain is higher than 7950 m) ¬¢ Norwegian musicians convicted for burning (even if the article does not mention they burned churches) must be the ones looked for ¬¢ People wro wrote ballads and published a lot of volumes of poetry is expected to have published volumes with ballads although the article does not say so ¬¢ People who have two residences, one in Switzerland and another somewhere else, can be con- sidered to have moved to Switzerland some time in their lives.</p><p>7. If two of three sisters died of tuberculosis and for the third the cause of death is not certain, is a page entitled "the sisters Bront√´" correct? We relaxed the strict requirement that writers should be people and not a group of people, because beforehand we did not expect groups of writers to stand as an article. So we accepted it as correct.</p><p>8. No longer existing Austrian parties, provided they were founded after the Second World War and had -at some time -people in the National Council of Austria, were considered correct. This brings about the often noted fact that most questions are not independent on time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9.</head><p>Finally, what are American museums? This expression should be interpreted according to the natural meaning of the corresponding word (american, amerikansk, americanos etc.) in the corresponding language -at least this was how we asked people to translate the question. But apparently Canadian and Brazilian museums do not mention their Picassos in their Wikipedia pages (so, even if correct, these answers will turn out Not justified, hence Incorrect), and the only hits found corresponded to museums in the USA. In this case, it was probably the organization's fault not to emphasize to the participants that each language topics should be understood and answered in that language, even if the correct interpretation of the terms in the different languages turned out to be different. <ref type="foot" coords="11,471.84,142.50,3.48,8.40" target="#foot_5">6</ref>We do not want to convey the idea that anything goes, though. In fact there were several other cases which were negatively decided:</p><p>1. Gulag can be metonymically used for the places where people were imprisoned and most of them were above the Arctic. However, we did not consider it as a valid place.</p><p>2. Fictional countries were not considered as correct when asking for fictional works, even if they were created or presented in the scope of a written fictional work.</p><p>3. Cases where the expected answer type (EAT) was clearly different from the one returned were considered downright incorrect, notwithstanding our agreement that the answers could be useful. So, ¬¢ Flags were not accepted as answers to questions Which countries had a flag ...? ¬¢ Queens were not accepted as answers to questions Which countries have a queen...? ¬¢ Countries were not accepted as answers to questions Which national teams...? ¬¢ Reserves were not accepted as answers to questions Which mountains...?</p><p>This is in line with our belief that assessment would have become a nightmare if any answer, whatever its type, had to be investigated by the assessors to see whether it could be indirectly useful. However, we are also aware that different participants in GikiCLEF 2009 interpreted the task differently, which produced unwanted differences among the participants. Clearly, this issue has to be considered for future editions, and an intermediate solution could be that, for some topics, more than one EAT, previously agreed, could be accepted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Overview of participation</head><p>Although we had almost 30 registrations of interest for GikiCLEF, in the end only 8 participants were able to submit. For the record, they are displayed in Table <ref type="table" coords="11,303.46,488.21,5.03,12.01" target="#tab_2">3</ref> by registration order. We received 17 runs, and their results are presented in Figure <ref type="figure" coords="11,350.83,664.73,3.77,12.01" target="#fig_7">8</ref>. Figure <ref type="figure" coords="12,133.41,332.69,5.03,12.01">9</ref> presents the participation detailed for each language. The last row indicates how many participants per language, and the last column the number of languages tried in that run. Eight runs opted for all (10) languages, four tried solely 2 languages, and five one only.</p><p>While this seems a modest amount of work, in fact it produced a sizeable amount of material to deal with, as Table <ref type="table" coords="12,147.78,380.57,5.03,12.01" target="#tab_3">4</ref> shows. The reason why there were considerably more manual assessments than manually assessed answers is due to the important fact that 2,131 answers had more than one assessor (often two, but they may have been assigned up to four different ones), to test the soundness and coherence of the assessment process. Note, anyway, that this does not include repeated assessments by the same assessor, nor assessments done by the organizers during conflict resolution, so that in practice the work involved was substantial, even with 29 assessors.</p><p>Turning now to the comparative weight and/or performance of the different languages involved, at face value, all languages participated in the answer gathering.</p><p>Figure <ref type="figure" coords="12,134.01,652.37,10.06,12.01" target="#fig_0">10</ref> provides an overview of the total number of answers per language, while Figure <ref type="figure" coords="12,474.88,652.37,10.06,12.01" target="#fig_0">11</ref> shows the distribution of only the correct answers.</p><p>The number of answers provided per language, as well as the amount of the correct ones, seems to demonstrate that the GikiCLEF systems could be used with the same level of success in all GikiCLEF However, it does not say anything about whether there were languages which were necessary to check in order to have a (crosslingually) justified (and therefore correct answer). For this we tried to see if some languages had a large amount of correct answers due to other languages, that is, we wanted to check language dependence or interdependence. A language should be more authoritative the more answers it provided without requiring proof in other languages.</p><p>Figure <ref type="figure" coords="13,133.65,514.85,10.06,12.01" target="#fig_2">13</ref> presents those numbers, as well as contrasting the number of correct answers per language, with the pre-assessed (correct) ones.</p><p>Interestingly, and contrary to our expectations concerning English, that figure again does not allow one to infer that English has more information or more detailed justifications in pages written in that language. This must be an artifact of our topic choice, which was on purpose geared toward languages different from English. Still, and even with our initial guidelines, many of the topics chosen were more international than really national (even if they did relate to specific individuals of a non-English-speaking nationality), and therefore one would expect that they would have equally developed pages in English as well.</p><p>A more thorough investigation of the different GikiCLEF topics regarding language spread should thus take place, such as the one done by <ref type="bibr" coords="13,239.93,622.37,10.60,12.00" target="#b3">[4]</ref>, who claim that, of the 50 topics of this year's GikiCLEF only 25 had a (justified) answer in the Portuguese Wikipedia, vs. 47 in English. If this is true, systems that processed only the Portuguese Wikipedia and then tried to follow links into the other languages would be in definite disadvantage compared to others that did the opposite, even for answering in Portuguese.</p><p>Also, it remains to be investigated which topics might be popular (or even asked at all) regarding different language populations. Of course our organizers' sample was not representative, and, in addition, Figure <ref type="figure" coords="14,268.65,183.77,8.55,12.01" target="#fig_1">12</ref>: Language precision Figure <ref type="figure" coords="14,145.53,364.73,8.55,12.01" target="#fig_2">13</ref>: Language authority: Languages where answers were found and also justifications and due to the random choice, some topic owners (proposers) received more topics than others. In fact, a cursory examination of the final topics shows that language or culture distribution was quite skewed, with a predominance of Romanian and German topics, on the one hand, and a scarcity of Portuguese and Norwegian ones, on the other.</p><p>If we look at the topics per language, then the relative importance of English finally emerges: for the vast majority of topics the language with higher number of correct hits is English. Table <ref type="table" coords="14,471.90,456.53,5.03,12.01" target="#tab_4">5</ref> shows a selected sample of the topics per language. Most of the remaining ones did feature English as the decisive winner (the full table is available from the GikiCLEF site). One other thing that remains to be done is an investigation of how really different the several answers are, that is, are the answers relative to the same individuals or places, or rather different? So, while GikiCLEF was able to demonstrate that there are systems that can answer (although still with poor performance) questions in these nine languages, the real utility for each language of processing also the other nine collections has not yet been established.</p><p>We present, for comparison, at the end of the paper, a description of the different monolingual Giki-CLEFs, by presenting precision per topic for some of the languages. We would like to emphasize, however, that these views are somehow artificial on several counts: not only they represent the joint performance of the several participants, but for some languages, such as for example Italian or Norwegian, their collections were not processed in any way, that is; the hits came from processing e.g. the German, the Dutch and the English collections... So these could be called, in GikiCLEF parlance, "parasitic languages", and in fact it is interesting to note that they do attain better precision than other languages whose collections were processed, such as Spanish and Portuguese.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Investigating the difficulty of GikiCLEF 2009</head><p>As a general opinion it is fair to say that GikiCLEF was universally considered too difficult or ambitious, which resulted in that several prospective participants gave up and not even sent in results. Many people strove hard to just be able to process the huge collections and minimally parse the topic renderings, and did not even consider cultural differences and/or crosslinguality. Our impression is that most participants did the bulk of processing in one main language, and then used naive and straightforward procedures to get answers in other languages. So, neither crosslinguality (differences in conveying related information) or multilinguality (the fact that different Wikipedias might produce different results) were really investigated by the first GikiCLEF participants.  Another feature we were expecting people to make use of was the justification field, which could in a way display the reasoning done. However, very few participants (only two) used a justification field, and apparently it was not very successful either, see Figure <ref type="figure" coords="16,310.90,83.57,8.38,12.01" target="#fig_11">16</ref>. In fact, the proportion of justified answers was only considered correct ca. 50% of the times. But we believe that if further justifications had been given by the participants their score would increase. Several other things did not work out as expected, and in particular we believe now that some quizzlike or relatively strained topics ended up in the final topic list, while topic managers in general shied away from the formidable task to convey things peculiar to their own cultures to a set of foreigners, and decided for simpler topics to begin with.</p><p>Finally, we would have liked to see more practically oriented systems with a special purpose and an obvious practical utility to try their hand at GikiCLEF. Apparently most if not all participants were simply considering GikiCLEF too hard and had no independent system of their own to try out there. Again, this may prove the complexity of the task, or the fact that the audience was not appropriate. We hope that training with GikiCLEF materials, all of them made available on due course, may in any case help future systems to perform difficult tasks with semi-structured multilingual material, which we still believe is something required out there.</p><p>All resources compiled under GikiCLEF, as well as collections and Web access to SIGA, can be reached from http://www.linguateca.pt/GikiCLEF.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,175.32,496.61,252.23,12.00;3,131.77,367.59,339.66,116.02"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Size of the different GikiCLEF Wikipedia collections</figDesc><graphic coords="3,131.77,367.59,339.66,116.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,183.24,254.33,236.39,12.00;6,131.66,62.42,339.74,178.89"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: SIGA in topic creation mode: editing translations</figDesc><graphic coords="6,131.66,62.42,339.74,178.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,129.48,464.09,343.91,12.00;7,131.74,211.60,339.66,239.40"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: SIGA in topic creation mode: finding candidates in the GikiCLEF colection.</figDesc><graphic coords="7,131.74,211.60,339.66,239.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,207.36,606.17,188.30,12.00;7,245.04,500.30,113.01,92.88"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Pre determined answers per language</figDesc><graphic coords="7,245.04,500.30,113.01,92.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,156.00,640.13,290.69,12.00;8,131.67,491.35,339.73,135.76"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: SIGA in assessment mode: Is this answer correct and justified?</figDesc><graphic coords="8,131.67,491.35,339.73,135.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="9,155.52,647.09,291.63,12.00;9,131.65,471.33,339.76,162.88"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: SIGA in conflict resolution mode: choose between assessments</figDesc><graphic coords="9,131.65,471.33,339.76,162.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="10,90.00,187.49,422.41,12.00;10,90.00,199.49,108.93,12.00;10,103.40,62.39,396.36,112.03"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Displaying SIGA results: on the left a graphic with language score, and on the right the assessment of each answer given.</figDesc><graphic coords="10,103.40,62.39,396.36,112.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="12,219.24,300.65,164.60,12.00;12,160.24,62.54,282.77,224.96"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Final scores in GikiCLEF 2009</figDesc><graphic coords="12,160.24,62.54,282.77,224.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="13,228.60,268.49,145.90,12.00;13,131.81,62.53,339.57,192.93"><head>Figure 9 :Figure 10 :Figure 11 :</head><label>91011</label><figDesc>Figure 9: Results in GikiCLEF 2009</figDesc><graphic coords="13,131.81,62.53,339.57,192.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="15,184.56,443.33,233.73,12.00;15,105.00,467.21,407.65,12.00;15,90.00,479.09,422.60,12.00;15,90.00,491.09,259.97,12.00;15,131.71,297.82,339.72,132.45"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Number of (total and correct) answers per topicIf we make a more detailed inspection of the topics and the systems' behaviors, we can identify the easiest and most difficult topics, through the display, in Figures14 and 15, of the number of answers and the conjoined precision (taking all participants together) attained.</figDesc><graphic coords="15,131.71,297.82,339.72,132.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="15,242.04,681.89,118.91,12.00;15,131.72,517.96,339.71,150.98"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Precision per topic</figDesc><graphic coords="15,131.72,517.96,339.71,150.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="16,190.80,268.85,221.43,12.00;16,159.99,132.96,283.08,122.99"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Extra justification by GikiCLEF participants</figDesc><graphic coords="16,159.99,132.96,283.08,122.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12" coords="19,202.08,235.97,198.76,12.00;19,160.00,84.26,283.06,138.68"><head>Figure 18 :Figure 19 :</head><label>1819</label><figDesc>Figure 18: English GikiCLEF: precision per topic</figDesc><graphic coords="19,160.00,84.26,283.06,138.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13" coords="19,201.36,669.29,200.19,12.00;19,159.99,518.63,283.07,137.75"><head>Figure 20 :</head><label>20</label><figDesc>Figure 20: German GikiCLEF: precision per topic</figDesc><graphic coords="19,159.99,518.63,283.07,137.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14" coords="20,204.60,236.45,193.73,12.00;20,159.99,84.18,283.07,139.25"><head>Figure 21 :</head><label>21</label><figDesc>Figure 21: Italian GikiCLEF: precision per topic</figDesc><graphic coords="20,159.99,84.18,283.07,139.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15" coords="21,196.92,287.69,209.06,12.00;21,159.99,138.63,283.07,136.04"><head>Figure 24 :</head><label>24</label><figDesc>Figure 24: Romanian GikiCLEF: precision per topic</figDesc><graphic coords="21,159.99,138.63,283.07,136.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16" coords="21,197.76,614.93,207.39,12.00;21,159.99,462.91,283.08,139.00"><head>Figure 25 :</head><label>25</label><figDesc>Figure 25: Bulgarian GikiCLEF: precision per topic</figDesc><graphic coords="21,159.99,462.91,283.08,139.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,96.00,367.61,406.69,107.65"><head>Table 1 :</head><label>1</label><figDesc>Example topics in GikiCLEF.</figDesc><table coords="4,96.00,379.14,406.69,96.12"><row><cell>ID EX01 Name Portuguese-speaking Nobel prize winners Topic in English EX02 List Portuguese Pop/Rock groups created in the 90s. EX03 Which Brazilian football players play in clubs in the Iberian Pensinsula? EX04 What capitals of Dutch provinces received their town privileges during the sixteenth century? EX05 In which places did Italo Calvino live during adulthood? EX06 Name Mexican poets who published volumes with ballads until 1930. EX07 Name authors born in Alaska and who wrote fiction about it.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,102.48,206.33,410.28,156.96"><head></head><label></label><figDesc>1. All pre-stored correct answers which are self-justified are automatically classified. The ones which require a justification are marked as Correct, but are still presented to the assessors for them to assign a Justified (or Not Justified) verdict. 2. Assessors assess individual answers, assigning either Incorrect, Correct, or Unknown. If it is Correct, they have to indicate whether the individual answer they are assessing (which includes the justification chain) is Justified, or whether it is Not Justified.</figDesc><table /><note coords="9,102.48,290.69,410.28,12.00;9,114.96,302.69,397.68,12.00;9,114.96,314.69,244.99,12.00;9,102.48,332.93,250.84,12.00;9,102.48,351.29,7.54,12.00"><p><p>3. A process of conflict resolution among different assessments of the very same answer is then run, which allows people to discuss and get aware of complications and/or mistakes or mistaken assumptions. Only after all conflicts are resolved can one proceed to:</p>4. Evaluate runs, by propagating justification across languages 5.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="11,108.36,519.05,386.27,132.01"><head>Table 3 :</head><label>3</label><figDesc>Participants in GikiCLEF 2009</figDesc><table coords="11,108.36,531.05,386.27,120.01"><row><cell>Name</cell><cell>Institution</cell><cell>System name</cell></row><row><cell cols="3">Ray Larson Sven Hartrumpf &amp; &amp; Johannes Leveling Iustin Dornescu TALP Research Center Gosse Bouma &amp; Sergio Duarte Information Science, University of Groningen JoostER University of California, Berkeley cheshire FernUniversit√§t in Hagen &amp; GIRSA-WP &amp; Dublin City University University of Wolverhampton EQUAL Universitat Polit√©cnica de Catalunya GikiTALP Nuno Cardoso et al. GREASE/XLDB, Univ. Lisbon GreP Adrian Iftene Faculty of Computer Science UAICGIKI09 Richard Flemmings et al. Birkbeck College (UK) &amp; UFRGS (Brazil) bbk-ufrgs</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="12,180.00,410.69,243.07,131.53"><head>Table 4 :</head><label>4</label><figDesc>Numbers on the assessment process.</figDesc><table coords="12,180.00,422.57,243.07,119.65"><row><cell>No. of answers received No. of different answers No. of different answers with complex justification No. of different manually assessed answers No. of manual assessments No. of automatically assessed answers as incorrect 10,588 21,251 18,152 215 6,974 10,332 No. of automatically assessed answers as correct 283 No. of answers resulting in conflicts 383 No. of correct and justified answers 1,327 No. of correct but not justified answers 1,415</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="14,152.04,511.37,298.92,107.52"><head>Table 5 :</head><label>5</label><figDesc>Correct</figDesc><table coords="14,152.04,511.37,298.92,107.52"><row><cell>Topic</cell><cell cols="5">answers per language BG NL EN DE IT NO NN PT RO ES</cell></row><row><cell cols="2">GC-2009-07 GC-2009-09 GC-2009-19 GC-2009-27 GC-2009-34 15 8 3 0 1 GC-2009-48 1 GC-2009-50 0</cell><cell>24 8 2 0 20 0 0</cell><cell>23 7 17 1 21 0 4</cell><cell>16 15 15 8 7 7 5 3 1 0 0 0 22 20 20 1 0 0 10 0 0</cell><cell>12 16 16 16 8 7 7 7 0 6 0 7 0 0 0 0 20 27 5 20 0 0 0 0 0 0 0 0</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,104.40,656.26,298.94,9.60"><p>The exception is probably the Bible, but it is not so widely accessed as Wikipedia in our days.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,104.40,665.98,408.14,9.60;2,90.00,675.46,422.29,9.60;2,90.00,684.94,77.66,9.60"><p>Norwegian has two written standards, and Norwegians therefore decided to maintain Wikipedia in two "parallel" versions, so GikiCLEF covers nine languages and ten collections. We have therefore created questions in and/or translated them into both written standards of Norwegian.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,104.40,677.86,408.05,9.60;3,90.00,687.34,254.34,9.60"><p>SIGA stands for SIstema de Gest√£o e Avaliac ¬∏√£o do GIKICLEF, Portuguese for "Management and Evaluation System of Giki-CLEF". The word siga means "Go on!" (imperative of verb seguir, "continue").</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="6,104.40,658.54,408.01,9.60;6,90.00,668.02,90.98,9.60"><p>In fact, the term Further clarification was employed in SIGA instead. Participants did not have access to them during submission, only after their participation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="7,104.40,695.14,294.98,9.60"><p>For the record, no topic owner chose to do this kind of opinion questions in GikiCLEF 2009.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="11,104.40,684.58,407.79,9.60;11,90.00,694.06,294.62,9.60"><p>This raises the problem, not yet satisfactorily solved, that culturally-laden questions are not exactly parallel, and that therefore the set of (multilingual) answers ultimately depends on the language the question was asked.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We are very grateful to <rs type="person">Nuno Cardoso</rs> for preparing the colections, to the remaining organizers -<rs type="person">S√∂ren Auer</rs>, <rs type="person">Gosse Bouma</rs>, <rs type="person">Iustin Dornescu</rs>,<rs type="person">Corina Forascu</rs>, <rs type="person">Pamela Forner</rs>, <rs type="person">Fredric Gey</rs>, <rs type="person">Danilo Giampiccolo</rs>, <rs type="person">Sven Hartrumpf</rs>, <rs type="person">Katrin Lamm</rs>, <rs type="person">Ray Larson</rs>, <rs type="person">Johannes Leveling</rs>, <rs type="person">Thomas Mandl</rs>, <rs type="person">Constantin Orasan</rs>, <rs type="person">Petya Osenova</rs>, <rs type="person">Anselmo Pe√±as</rs>, <rs type="person">Erik Tjong Kim Sang</rs>, <rs type="person">Julia Schulz</rs>, <rs type="person">Yvonne Skalban</rs>, and <rs type="person">Alvaro Rodrigo Yuste -</rs>for hard work, supportive feedback and enthusiasm, and to the larger set of further assessors -including the organizers and further <rs type="person">Anabela Barreiro</rs>, <rs type="person">Leda Casanova</rs>, <rs type="person">Lu√≠s Costa</rs>, <rs type="person">Ana Engh</rs>, <rs type="person">Laska Laskova</rs>, <rs type="person">Cristina Mota</rs>, <rs type="person">Ros√°rio Silva</rs>, and <rs type="person">Kiril Simov -</rs>who helped with assessment. <rs type="person">Paula Carvalho</rs> and <rs type="person">Christian-Emil Ore</rs> helped in an initial phase by suggesting Portuguese and Norwegian-inspired topics, respectively.</p><p><rs type="person">Iustin Dornescu</rs> and <rs type="person">Sven Hartrumpf</rs> deserve further mention, the first for having performed an extremely large number of assessments, and the second for intelligent critical comments and revision throughout the whole process, as well as for pertinent discussions in all GikiCLEF lists. Finally, <rs type="person">Alexander Yeh</rs>'s testing and debugging of the Wikipedia collections was particularly useful.</p><p>The organization work, as well as the writing of this paper, were accomplished under the scope of the Linguateca project, jointly funded by the <rs type="funder">Portuguese Government</rs>, the <rs type="funder">European Union (FEDER and FSE)</rs>, under contract ref. <rs type="grantNumber">POSC/339/1.3/C/NAC</rs>, <rs type="funder">UMIC</rs> and <rs type="funder">FCCN</rs>. We also gratefully acknowledge support of the <rs type="projectName">TrebleCLEF</rs> Coordination Action. <rs type="grantNumber">ICT-1-4-1</rs> <rs type="projectName">Digital libraries and technology-enhanced learning</rs> (Grant agreement: <rs type="grantNumber">215231</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Da4CAy5">
					<idno type="grant-number">POSC/339/1.3/C/NAC</idno>
				</org>
				<org type="funded-project" xml:id="_Fr6TPfP">
					<idno type="grant-number">ICT-1-4-1</idno>
					<orgName type="project" subtype="full">TrebleCLEF</orgName>
				</org>
				<org type="funded-project" xml:id="_A78nk3R">
					<idno type="grant-number">215231</idno>
					<orgName type="project" subtype="full">Digital libraries and technology-enhanced learning</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="17,111.59,82.01,401.01,12.00;17,111.60,93.89,401.20,12.00;17,111.60,105.89,401.04,11.90;17,111.60,117.89,278.57,12.00" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="17,111.60,93.89,187.65,12.00">DBpedia: A Nucleus for a Web of Open Data</title>
		<author>
			<persName coords=""><forename type="first">S√∂ren</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Georgi</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zachary</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,321.26,93.89,191.54,11.90;17,111.60,105.89,335.40,11.90;17,209.61,117.89,47.67,11.90">The Semantic Web: 6th International Semantic Web Conference, 2nd Asian Semantic Web Conference, ISWC 2007 + ASWC 2007</title>
		<meeting><address><addrLine>Busan, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">November 11-15, 2007. 2008</date>
			<biblScope unit="page" from="722" to="735" />
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

<biblStruct coords="17,111.59,137.81,401.04,12.00;17,111.60,149.69,269.97,12.00" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="17,174.87,137.81,337.76,12.00;17,111.60,149.69,61.64,12.00">Galileo&apos;s revenge: Ways of construing knowledge and translation strategies in the era of globalization</title>
		<author>
			<persName coords=""><forename type="first">Karen</forename><surname>Bennett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,180.39,149.69,65.56,11.90">Social Semiotics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="171" to="193" />
			<date type="published" when="2007-06">June 2007 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,111.59,169.61,401.07,12.00;17,111.60,181.61,401.13,12.00;17,111.60,193.61,203.49,12.00" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="17,194.78,169.61,317.88,12.00;17,111.60,181.61,50.00,12.01">The origins of children&apos;s spatial semantic categories: cognitive versus linguistic determinants</title>
		<author>
			<persName coords=""><forename type="first">Melissa</forename><surname>Bowerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,341.58,181.61,119.88,11.90">Rethinking linguistic relativity</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Gumperz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Levinson</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="145" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,111.59,213.53,401.06,12.00;17,111.60,225.41,401.10,12.00;17,111.60,237.41,400.94,12.00;17,111.60,249.41,113.00,12.00" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="17,417.11,213.53,95.54,12.00;17,111.60,225.41,219.75,12.00">Where in the Wikipedia is that answer? the XLDB at the GikiCLEF 2009 task</title>
		<author>
			<persName coords=""><forename type="first">Nuno</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Francisco</forename><surname>Lopez-Pellicer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M√°rio</forename><forename type="middle">J</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,199.82,237.41,234.58,11.90">Cross Language Evaluation Forum CLEF 2009 Workshop</title>
		<editor>
			<persName><forename type="first">Francesca</forename><surname>Borri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alessandro</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2009-10-02">30 September -2 October 2009</date>
		</imprint>
	</monogr>
	<note>This volume</note>
</biblStruct>

<biblStruct coords="17,111.59,269.33,400.98,12.00;17,111.60,281.21,223.17,12.00" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="17,187.59,269.33,285.46,12.00">Large Scale Named Entity Disambiguation Based on Wikipedia Data</title>
		<author>
			<persName coords=""><forename type="first">Silviu</forename><surname>Cucerzan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,497.58,269.33,14.98,11.90;17,111.60,281.21,133.97,11.90">The EMNLP-CoNLL Joint Conference</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="708" to="716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,111.59,301.13,401.06,12.00;17,111.60,313.13,135.65,12.00" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Marieke</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mooij</forename></persName>
		</author>
		<title level="m" coord="17,197.91,301.13,314.74,11.90;17,111.60,313.13,44.49,11.90">Consumer Behaviour and Culture: Consequences for Global Marketing and Advertising</title>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>Sage</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,111.59,333.05,401.06,12.00;17,111.60,345.05,401.07,12.00;17,111.60,356.93,249.98,12.00" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="17,301.33,333.05,211.32,12.00;17,111.60,345.05,133.33,12.00">Computing Semantic Relatedness using Wikipediabased Explicit Semantic Analysis</title>
		<author>
			<persName coords=""><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shaul</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,265.52,345.05,247.15,11.90;17,111.60,356.93,145.67,11.90">Proceedings of The Twentieth International Joint Conference for Artificial Intelligence IJCAI 2007</title>
		<meeting>The Twentieth International Joint Conference for Artificial Intelligence IJCAI 2007</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1606" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,111.59,376.85,401.10,12.00;17,111.60,388.85,291.09,12.00" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="17,175.84,376.85,336.85,12.00;17,111.60,388.85,39.15,12.01">Estimating Linguistic Diversity on the Internet: A Taxonomy to Avoid Pitfalls and Paradoxes</title>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Gerrand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,158.34,388.85,188.34,11.90">Journal of Computer-Mediated Communication</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,111.59,408.77,400.96,12.00;17,111.60,420.77,401.04,12.00;17,111.60,432.65,400.91,12.00;17,111.60,444.65,401.18,12.00;17,111.60,456.53,401.37,12.00;17,111.60,468.53,91.31,12.00" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="17,425.49,408.77,87.06,12.00;17,111.60,420.77,250.37,12.00">GeoCLEF: the CLEF 2005 Cross-Language Geographic Information Retrieval Track</title>
		<author>
			<persName coords=""><forename type="first">Frederic</forename><surname>Gey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ray</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hideo</forename><surname>Joho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,162.53,444.65,350.25,11.90;17,111.60,456.53,104.17,11.90">Acessing Multilingual information Repositories: 6th Workshop of the Cross-Language Evaluation Forum, CLEF</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Frederic</forename><surname>Gey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Henning</forename><surname>M√ºller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bernardo</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maarten</forename><surname>Magnini</surname></persName>
		</editor>
		<editor>
			<persName><surname>De Rijke</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005. 2006</date>
			<biblScope unit="volume">4022</biblScope>
			<biblScope unit="page" from="908" to="919" />
		</imprint>
	</monogr>
	<note>Revised Selected papers</note>
</biblStruct>

<biblStruct coords="17,111.58,488.45,401.16,12.00;17,111.60,500.45,401.19,12.01;17,111.60,512.33,401.18,12.00;17,111.60,524.33,401.05,12.00;17,111.60,536.33,401.08,12.00;17,111.60,548.21,401.15,12.00;17,111.60,560.21,218.21,12.00" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="17,380.92,500.45,131.87,12.01;17,111.60,512.33,270.99,12.00">GeoCLEF 2006: the CLEF 2006 Cross-Language Geographic Information Retrieval Track Overview</title>
		<author>
			<persName coords=""><forename type="first">Fredric</forename><surname>Gey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ray</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kerstin</forename><surname>Bishoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Mandl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christa</forename><surname>Womser-Hacker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Diana</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paulo</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Giorgio</forename><surname>Di Nunzio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,227.86,536.33,284.82,11.90;17,111.60,548.21,240.69,11.90">Evaluation of Multilingual and Multi-modal Information Retrieval: 7th Workshop of the Cross-Language Evaluation Forum, CLEF</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Fredric</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jussi</forename><surname>Karlgren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maximilian</forename><surname>Stempfhuber</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006. 2007</date>
			<biblScope unit="volume">4730</biblScope>
			<biblScope unit="page" from="852" to="876" />
		</imprint>
	</monogr>
	<note>Revised selected papers</note>
</biblStruct>

<biblStruct coords="17,111.58,580.13,401.00,12.00;17,111.60,592.01,401.19,12.00;17,111.60,604.01,83.39,12.00" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="17,292.83,580.13,219.75,12.00;17,111.60,592.01,39.69,12.01">WiQA: Evaluating Multi-lingual Focused Access to Wikipedia</title>
		<author>
			<persName coords=""><forename type="first">Valentin</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,172.49,592.01,305.32,11.90">The First International Workshop on Evaluating Information Access (EVIA)</title>
		<meeting><address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-05-15">May 15 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,111.58,623.93,401.04,12.00;17,111.60,635.93,58.58,12.00" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="17,251.20,623.93,87.71,11.90">Metaphors We Live By</title>
		<author>
			<persName coords=""><forename type="first">George</forename><surname>Lakoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980">1980</date>
			<publisher>University of Chicago Press</publisher>
			<pubPlace>Chicago and London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,111.58,655.85,401.03,12.00;17,111.60,667.73,401.29,12.00" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="17,111.60,667.73,396.74,12.00">GeoCLEF 2008: the CLEF 2008 Cross-Language Geographic Information Retrieval Track Overview</title>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Mandl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paula</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fredric</forename><surname>Gey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ray</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Diana</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christa</forename><surname>Womser-Hacker</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="17,111.60,679.73,401.17,12.00;17,111.60,691.73,401.19,12.00;18,111.60,59.69,400.94,11.90;18,111.60,71.69,361.52,12.00" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="17,365.10,691.73,147.69,11.90;18,111.60,59.69,369.12,11.90">Evaluating Systems for Multilingual and Multimodal Information Access 9th Workshop of the Cross-Language Evaluation Forum</title>
		<author>
			<persName coords=""><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tomas</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mikko</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Kurimo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Mandl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Viviane</forename><surname>Pe√±as</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Petras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,488.20,59.69,24.35,11.90;18,111.60,71.69,18.11,11.90">CLEF 2008</title>
		<meeting><address><addrLine>Aarhus, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">September 17-19, 2008. 2009</date>
		</imprint>
	</monogr>
	<note>Revised Selected Papers</note>
</biblStruct>

<biblStruct coords="18,111.58,91.61,401.06,12.00;18,111.60,103.61,401.04,12.00;18,111.60,115.49,48.65,12.00" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="18,212.40,91.61,188.63,12.00">The need for culturally adaptive agent behavior</title>
		<author>
			<persName coords=""><forename type="first">O'</forename><surname>Patricia</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Neill-Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,420.71,91.61,91.93,11.90;18,111.60,103.61,310.43,11.90">Computational Models for Mixed Initiative Interaction: Papers from the 1997 AAAI Spring Symposium</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="117" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,111.58,135.41,401.28,12.00;18,111.60,147.41,87.59,12.00" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="18,249.18,135.41,113.29,12.00">The Web as a parallel corpus</title>
		<author>
			<persName coords=""><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="18,369.40,135.41,104.38,11.90">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="349" to="380" />
			<date type="published" when="2003-09">September 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,111.58,167.33,401.04,12.00;18,111.60,179.21,121.27,12.00" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="18,175.23,167.33,337.39,11.90;18,111.60,179.21,56.66,11.90">Translation-based corpus studies: Contrasting English and Port uguese tense and aspect systems</title>
		<author>
			<persName coords=""><forename type="first">Diana</forename><surname>Santos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Rodopi</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,111.58,199.25,401.12,12.00;18,111.60,211.13,401.05,12.00" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="18,200.28,211.13,307.97,12.00">GikiP at GeoCLEF 2008: Joining GIR and QA forces for querying Wikipedia</title>
		<author>
			<persName coords=""><forename type="first">Diana</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nuno</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paula</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iustin</forename><surname>Dornescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yvonne</forename><surname>Skalban</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="18,111.60,223.13,401.17,12.00;18,111.60,235.01,401.19,12.00;18,111.60,247.01,400.94,11.90;18,111.60,259.01,401.18,12.00;18,111.60,270.89,22.64,12.00" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="18,365.10,235.01,147.69,11.90;18,111.60,247.01,369.12,11.90">Evaluating Systems for Multilingual and Multimodal Information Access 9th Workshop of the Cross-Language Evaluation Forum</title>
		<author>
			<persName coords=""><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tomas</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mikko</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Kurimo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Mandl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Viviane</forename><surname>Pe√±as</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Petras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,488.20,247.01,24.35,11.90;18,111.60,259.01,18.11,11.90">CLEF 2008</title>
		<meeting><address><addrLine>Aarhus, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">September 17-19, 2008. 2009</date>
			<biblScope unit="page" from="894" to="905" />
		</imprint>
	</monogr>
	<note>Revised Selected Papers</note>
</biblStruct>

<biblStruct coords="18,111.58,290.81,401.12,12.00;18,111.60,302.81,401.14,12.00;18,111.60,314.81,22.64,12.00" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="18,162.51,290.81,216.79,12.00">From &quot;thought and language&quot; to &quot;thinking for speaking</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">I</forename><surname>Slobin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,163.73,302.81,120.96,11.90">Rethinking linguistic relativity</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Gumperz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Levinson</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="70" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,111.58,334.73,401.19,12.00;18,111.60,346.61,401.03,12.00;18,111.60,358.61,290.36,12.00" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="18,181.69,334.73,238.90,12.00">Lexicalization patterns: semantic structure in lexical forms</title>
		<author>
			<persName coords=""><forename type="first">Leonard</forename><surname>Talmy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,141.08,346.61,367.31,11.90">Language Typology and Semantic Description: Grammatical Categories and the Lexicon</title>
		<editor>
			<persName><forename type="first">Timothy</forename><surname>Shopen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1985">1985</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="57" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,111.58,378.53,401.04,12.00;18,111.60,390.53,141.41,12.00" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="18,164.79,378.53,347.83,12.00;18,111.60,390.53,36.23,12.01">Enriched Lexical Ontologies: Adding new knowledge and new scope to old linguistic resources</title>
		<author>
			<persName coords=""><forename type="first">Tony</forename><surname>Veale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,170.86,390.53,53.02,11.90">ESSLLI 2007)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,111.58,410.45,400.96,12.00;18,111.60,422.33,397.52,12.00;18,195.24,613.73,212.42,12.00" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="18,227.98,410.45,157.75,12.00">Autonomously Semantifying Wikipedia</title>
		<author>
			<persName coords=""><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,405.25,410.45,107.29,11.90;18,111.60,422.33,312.78,11.90">CIKM &apos;07: Proceedings of the sixteenth ACM on Conference on Information and Knowledge Management</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="41" to="50" />
		</imprint>
	</monogr>
	<note>Figure 17: Portuguese GikiCLEF: precision per topic</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
