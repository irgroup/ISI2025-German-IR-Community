<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,105.00,148.73,393.14,15.51;1,210.96,170.69,181.34,15.51">Approaching Question Answering by means of Paragraph Validation</title>
				<funder ref="#_HvHXtkV">
					<orgName type="full">Regional Government of Madrid</orgName>
				</funder>
				<funder ref="#_GzdWwKX">
					<orgName type="full">European Commission</orgName>
				</funder>
				<funder ref="#_avBy5CB">
					<orgName type="full">Spanish Ministry of Science and Innovation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,119.16,201.55,63.95,12.48"><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
							<email>alvarory@lsi.uned.es</email>
							<affiliation key="aff0">
								<orgName type="department">Sistemas Informáticos</orgName>
								<orgName type="institution">UNED</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,190.94,204.07,60.08,9.96"><forename type="first">Joaquín</forename><surname>Pérez</surname></persName>
							<email>joaquin.perez@lsi.uned.es</email>
							<affiliation key="aff0">
								<orgName type="department">Sistemas Informáticos</orgName>
								<orgName type="institution">UNED</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,258.51,204.07,64.40,9.96"><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
							<email>anselmo@lsi.uned.es</email>
							<affiliation key="aff0">
								<orgName type="department">Sistemas Informáticos</orgName>
								<orgName type="institution">UNED</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,330.75,204.07,78.79,9.96"><forename type="first">Guillermo</forename><surname>Garrido</surname></persName>
							<email>ggarrido@lsi.uned.es</email>
							<affiliation key="aff0">
								<orgName type="department">Sistemas Informáticos</orgName>
								<orgName type="institution">UNED</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,417.26,204.07,67.88,9.96"><forename type="first">Lourdes</forename><surname>Araujo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sistemas Informáticos</orgName>
								<orgName type="institution">UNED</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,193.68,218.11,73.06,9.96"><forename type="first">Dpto</forename><surname>Lenguajes</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sistemas Informáticos</orgName>
								<orgName type="institution">UNED</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,105.00,148.73,393.14,15.51;1,210.96,170.69,181.34,15.51">Approaching Question Answering by means of Paragraph Validation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AFFE81DCC9FE86506A81B517E457D149</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.3 [Information Search and Retrieval]; H.3.4 [Systems and Software]: [Questionanswering (fact retrieval) systems] Question Answering</term>
					<term>Answer Validation</term>
					<term>Lexical Entailment</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we describe the system we developed for taking part in monolingual Spanish and English tasks at ResPubliQA 2009. Our system was composed by an IR phase focused on improving QA results, a validation step for removing not promising paragraphs and a module based on ngrams overlapping for selecting the final answer. Furthermore, a selection module that used lexical entailment and ngram overlapping was developed in English. While the IR module has achieved very promising results, the performance of the validation module has to be improved. On the other hand, the ngram ranking improved the one given by the IR module and it worked better for English than for Spanish. Finally, the ranking was slightly improved when lexical entailment was used.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The first participation of UNED at QA@CLEF (called this year ResPubliQA) is based on our experience as participants and organizers of the Answer Validation Exercise<ref type="foot" coords="1,424.56,562.88,3.97,4.84" target="#foot_0">1</ref> (AVE) <ref type="bibr" coords="1,464.65,563.71,15.49,9.96" target="#b10">[11,</ref><ref type="bibr" coords="1,483.85,563.71,12.73,9.96" target="#b11">12,</ref><ref type="bibr" coords="1,500.29,563.71,12.73,9.96" target="#b16">17,</ref><ref type="bibr" coords="1,90.00,575.71,12.73,9.96" target="#b17">18,</ref><ref type="bibr" coords="1,105.72,575.71,11.62,9.96" target="#b18">19]</ref>. Our motivation for using Answer Validation (AV) in this task comes from the conclusions obtained in AVE, where it was shown that AV systems could contribute towards improving results in Question Answering (QA). In fact, some systems that took part in the last edition of QA@CLEF improved their results including an AV module <ref type="bibr" coords="1,298.36,611.59,10.45,9.96" target="#b1">[2,</ref><ref type="bibr" coords="1,312.16,611.59,11.62,9.96" target="#b19">20]</ref>.</p><p>This year a paragraph containing a correct answer had to be returned for each question instead of the exact answer string of last editions. Since answer extraction was not necessary, we could concentrate on the validation of candidate paragraphs. Nevertheless, most of the experiments in AV have been performed with short answers but not with paragraphs containing an answer. Thus, checking the viability of performing paragraph validation in QA was one motivation for taking part in this task.</p><p>On the other hand, although according to the guidelines all the questions have an answer in the document collection, if a system is not sure about the correctness of an answer, the system can choose not to give any answer. In fact, the evaluation in this edition gives a higher reward for not giving an answer than for returning an incorrect one. Because of this modification in the evaluation we think it is important to carry out a validation step in order to decide if a correct answer can be returned or if no answer has been found. Thus, if our QA system is not sure about the correctness of all the candidate answers, no answer is returned.</p><p>In this paper we describe the main features of our QA system and the results obtained in monolingual English and Spanish. The rest of this paper is structured as follows: In Section 2 we describe the main components of our system. The description of the submitted runs is given in Section 3, while the results and their analysis are shown in Section 4. Finally, some conclusions and future work are given in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Overview</head><p>The main steps performed by our system are shown in Figure <ref type="figure" coords="2,364.94,261.91,4.98,9.96" target="#fig_0">1</ref> and they are described in detail in the following subsections. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Retrieval Phase</head><p>In this phase a first selection of paragraphs that are considered relevant for the proposed question are selected, therefore it will put focus on obtaining a first set of paragraphs ordered according to their relevance with the question. The precision in terms of retrieving the correct answer for the query within the top k<ref type="foot" coords="2,186.00,620.84,3.97,4.84" target="#foot_1">2</ref> paragraphs, delimits in some sense the overall quality of the full system. In order to retrieve the most relevant paragraphs, the full collection has been indexed by paragraphs removing stopwords, specifically by language.</p><p>We used BM25 <ref type="bibr" coords="2,173.76,657.55,14.70,9.96" target="#b15">[16]</ref>, which can be adapted to fit the specific characteristics of the data in use. More information about the selected retrieval model can be found in <ref type="bibr" coords="2,387.86,669.55,14.60,9.96" target="#b12">[13]</ref>. In this ranking function the effect of term frequency and document length to the final score of a document can be specified by setting up two parameters (b,k 1 ). The expression for BM25 ranking function<ref type="foot" coords="2,438.84,692.60,3.97,4.84" target="#foot_2">3</ref> for a document d and query q is as follows:</p><formula xml:id="formula_0" coords="3,169.56,119.35,343.48,27.06">R(q,d) = t en q f req t,d k 1 ((1 -b) + b • l d avl d ) + f req t,d • N -df t + 0.5 df t + 0.5 (1)</formula><p>Where N is the total number of documents in the collection; df t is the number of documents in the collection that contain the term t; f req t,d is the frequency of the term t within document d; l d is the length of the document and avl d is the average length of documents within the collection. The values of the parameters should be fixed according to the data, as appears next:</p><formula xml:id="formula_1" coords="3,105.00,209.59,49.45,9.96">• b ∈ [0, 1].</formula><p>Assigning 0 to b is equivalent to avoid the process of normalisation and therefore the document length will not affect the final score. If b takes 1, we will be carrying out a full normalisation dl avdl .</p><p>• k 1 , where ∞ &gt; k 1 &gt; 0, allow us to control the effect of frequency in final score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Retrieval Settings</head><p>For both languages stopwords have been removed and a stemming pre-process based on Snowball implementation of Porter algorithm has been applied. This implementation can be downloaded from http://snowball.tartarus.org/algorithms/. The stopwords lists applied can be found at http://members.unine.ch/jacques.savoy/clef/.</p><p>The BM25 parameters for both languages were fixed after a training phase with the English development data supplied by the organisation (No development Spanish data was released). These values are as next:</p><p>1. b: 0.6. Those paragraphs with a length over the average will obtain a slightly higher score.</p><p>2. k 1 : 0.1. The effect of term frequency over final score will be minimised.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pre-processing</head><p>In this step each question and each paragraph returned by the IR engine is pre-processed with the purpose of obtaining the following data:</p><p>• Name Entities (NEs): the Freeling NE recognizer <ref type="bibr" coords="3,353.65,491.47,10.57,9.96" target="#b0">[1]</ref> is applied in order to tag proper nouns, numeric expressions and temporal expressions for each question and each candidate paragraph. Besides, it is also included information regarding the type of the NE found. That is, for proper nouns we have types PERSON, ORGANIZATION and LOCATION (since Freeling does not supply this classification in English, these three types are grouped in the ENAMEX type when the QA system is used for English texts); NUMEX for numeric expressions and TIMEX for time expressions.</p><p>• Lemmatisation: the Freeling PoS tagger in Spanish and TreeTagger<ref type="foot" coords="3,421.44,582.20,3.97,4.84" target="#foot_3">4</ref> in English are used for obtaining the lemmas of paragraphs and questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Paragraph Validation</head><p>This component receives as input the original questions and paragraphs as well as the pre-processed ones obtained in the previous step. The objective is to remove paragraphs that do not satisfy a set of constraints imposed by a question since, in that case, it is not likely to find a correct answer for this question in these paragraphs. A set of modules for checking constraints have been implemented (3 in this edition, but more modules can be easily added to the system) and they are applied in a pipeline processing. That is, only paragraphs able to satisfy a certain constraint are checked against the following constraint. Finally, only paragraphs that satisfy all the implemented constraints are given to the following step. In fact, it is possible to obtain no paragraph as output, what means that no paragraph is a candidate for containing a correct answer (according to this component). This situation changes the way of selecting the final answer as it is explained in Section 2.4.</p><p>The constraints implemented in this edition are explained in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Expected Answer Type matching</head><p>Traditional QA systems typically apply as a first processing step an analysis of the input question where the expected answer type represents an important and useful information for the following steps <ref type="bibr" coords="4,114.83,215.47,10.45,9.96" target="#b4">[5,</ref><ref type="bibr" coords="4,128.52,215.47,7.69,9.96" target="#b8">9,</ref><ref type="bibr" coords="4,139.32,215.47,11.62,9.96" target="#b14">15]</ref>. Besides, there are some AV systems that use also information about the expected answer type with the purpose of checking whether the expected answer type matches the type of the answer to be validated <ref type="bibr" coords="4,209.53,239.47,15.49,9.96" target="#b21">[22,</ref><ref type="bibr" coords="4,228.50,239.47,12.73,9.96" target="#b9">10,</ref><ref type="bibr" coords="4,244.58,239.47,12.73,9.96" target="#b20">21,</ref><ref type="bibr" coords="4,260.78,239.47,7.05,9.96" target="#b5">6]</ref>. While some AV systems use this checking as a feature <ref type="bibr" coords="4,90.00,251.35,14.60,9.96" target="#b20">[21]</ref>, others use it as a constraint that must be satisfied by an answer in order to be validated <ref type="bibr" coords="4,90.00,263.35,14.60,9.96" target="#b21">[22]</ref>. We think that a correct answer paragraph must contain at least one element whose type matches the expected answer type. This is why we decided to validate only paragraphs that contain elements of the expected answer type. Firstly, the expected answer type is detected for each question. We based our taxonomy on the one used in the last editions of QA@CLEF. Thus, the defined types were: count, time, location, organization, person, definition and other (this is chosen when none of the previous categories is given). Then, although several machine learning methods have been successfully applied for question classification <ref type="bibr" coords="4,188.16,346.99,9.91,9.96" target="#b7">[8]</ref>, given the small size of our taxonomy we decided to use the traditional approach based on hand-made patterns.</p><p>Secondly, for performing the matching process we took advantage of the fact that all the types in our taxonomy (except definition and other ) match the possible NE types given by the pre-processing step. That is, count questions must be answered by numeric expressions, time questions must be answered by temporal expressions, etc. Then, the module validates paragraphs that contain at least a NE of the expected answer type and rejects the other paragraphs. In case of the expected answer type is definition or other, all the input paragraphs are validated because the system does not have enough evidences for rejecting them.</p><p>Our system can perform two kinds of expected answer type matching: the coarse grained matching and the fine grained matching. In the fine grained matching all the possible expected answer types and all the possible NE types are used. Thus, only paragraphs with at least one NE of the same type that the expected answer type will be validated. For example, if the expected answer type of a question is person, only paragraphs containing at least one NE of PERSON type will be validated.</p><p>However, in the coarse grained matching some types are grouped. That is, the expected answer types organization, person and location are grouped into a single one called enamex, which means that any NE of one of these types (PERSON, ORGANIZATION and LOCATION) can match with any enamex question. For example, if the expected answer type is location and the only NE in a paragraph is of type PERSON, the paragraph will be validated (while it would not be validated using the fine grained matching). In a similar way, time and count questions are grouped in a unique type and they can be answered by either numeric expressions or time expressions.</p><p>We decided to allow this double matching based on the intuition that NE types sometimes can be wrongly classified as for example the expression in 1990, which can be classified as a numeric expression when in fact can be also a temporal expression. Moreover, since the NE recognizer used in English did not give us a fine grained classification of enamex NEs, we needed to use the coarse grained matching in this language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">NE entailment</head><p>The validation process performed by this module follows the intuition that the NEs of a question are a so important information that they must appear in some way in the text that supports an answer <ref type="bibr" coords="4,123.00,726.07,14.60,9.96" target="#b16">[17]</ref>. This year the supporting snippet is the paragraph given as answer and following the previous intuition, the NEs of the question must appear in any answer paragraph. This module receives as input the NEs of the question and the candidate paragraphs before being pre-processed (we explain the reason below). Then, only paragraphs that contain all the NEs of the question are validated by this module and returned as output.</p><p>The idea of containment used is a simple text matching of the NEs of the question in the paragraphs. It is not important if the matched element in the paragraph is or not a NE, because the important NEs are the ones of the question. In fact, this kind of matching allows to avoid possible errors in the recognition of NEs in the paragraphs. Another difference with the work performed in <ref type="bibr" coords="5,149.53,195.19,15.49,9.96" target="#b16">[17]</ref> is that we did not use the Levensthein distance <ref type="bibr" coords="5,378.60,195.19,10.57,9.96" target="#b6">[7]</ref> because in the development period it produced worse results.</p><p>If a question does not have any NE, then all the paragraphs are validated by this module because there are no evidences for rejecting them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Acronym checking</head><p>This module works only over questions that ask about the meaning of a certain acronym as for example What is NATO? or What does NATO stand for?. The objective is to validate only paragraphs that could contain an explanation for these acronyms.</p><p>Firstly, the module checks whether the question is of definition type and whether it is asking about a word that only contains capitalized letters, which we called acronym. If the question satisfies these constraints, then the acronym is extracted.</p><p>Secondly, only paragraphs that can contain a possible definition for the extracted acronym are validated. In the current implementation it is considered that if a paragraph contains the acronym inside a pair of brackets then it might contain a definition of the acronym. For example, for question What does ECSC stand for?, where the acronym is ECSC, the paragraph in Figure <ref type="figure" coords="5,508.00,382.87,4.98,9.96">2</ref> contains an explanation of the acronym and it would be validated by this module.</p><p>on the consequences of the expiry of the European Coal and Steel Community (ECSC) Treaty on international agreements concluded by the ECSC Figure <ref type="figure" coords="5,154.93,463.27,3.90,9.96">2</ref>: An example of a paragraph containing the explanation to an acronym.</p><p>Similar to the other validation modules, if the restriction cannot be applied, that is, if the question is not asking about the definition of an acronym, all the input paragraphs are validated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Paragraph Selection</head><p>Once all the restrictions have been applied, the system selects one paragraph among the ones validated in the previous step. Since AV has been successfully applied for performing the selection of answers in QA <ref type="bibr" coords="5,168.97,569.35,14.60,9.96" target="#b20">[21]</ref>, it could be a natural option to use it since our system is already using an AV module. However, when an AV module is used for selection, it usually produces a confidence value that is considered for performing the selection. Since our AV module does not produce any confidence value, we decided to discard this option. Then, after some experiments performed at the development period we based the decision of which paragraph to select on the overlapping between question and answer paragraphs.</p><p>The paragraph selection works only when the validation process returns more than one candidate paragraph. If there is only one candidate paragraph, then it is the one selected. If there is no candidate paragraph, that means that no candidate paragraph was suitable for containing a correct answer. In these cases, it is considered that the system cannot find an answer and the system does not answer the question (an option that is allowed this year). Since in case of not giving any answer an hypothetical answer must be given for evaluation purposes, in this situation it is returned the paragraph that was chosen by the IR engine at the first position.</p><p>We have two modules for selecting the final answer: one based only on lemmas overlapping; and another one based on lemmas overlapping and Lexical Entailment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Setting 1</head><p>As a way of avoiding different formulations of similar expressions we discarded stop words and measured overlapping using lemmas. Thus, the selection process is as follows:</p><p>1. Overlapping using 1-grams (lemmas) is measured. If the maximum overlapping with the question is achieved for only one paragraph, then that paragraph is selected. If the maximum overlapping is achieved for more than one paragraph, then the next step is performed.</p><p>2. The overlapping using 2-grams (lemmas) is measured over the paragraphs with the maximun overlapping using 1-grams. If the maximum overlapping with the question is achieved for only one paragraph, then that paragraph is selected. If the maximum overlapping is achieved for more than one paragraph, then the process is repeated with 3-grams, 4-grams and 5-grams stopping when there is still more than one paragraph with the maximun overlapping using 5-grams (lemmas) to perform the next step.</p><p>3. If there is more than one paragraph with the maximun overlapping using 5-grams (lemmas), then it is selected among the paragraphs with the maximum overlapping the one which obtains the higher ranking in the IR process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Setting 2</head><p>Furthermore, for English we developed another version for this selection process that is based on Lexical Entailment. For this purpose we took advantage of a module based on WordNet relations and paths for checking the entailment between lexical units <ref type="bibr" coords="6,349.68,379.39,10.57,9.96" target="#b2">[3,</ref><ref type="bibr" coords="6,363.25,379.39,7.05,9.96" target="#b3">4]</ref>. The same process performed in setting 1 is applied, but there can be overlapping between a word in a paragraph and a word in a question if the two words are the same or the word in the paragraph entails (according to the entailment module based on WordNet) the word in the question. This new idea of overlapping is used with all the lengths of ngrams (from 1-grams to 5-grams).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Runs Submitted</head><p>In the first edition of ResPubliQA we took part in two monolingual tasks (English and Spanish), sending two runs for each of these tasks with the aim of checking different settings. All the runs applied the same IR process and the main differences are in the validation and selection steps. The characteristics of each run are as follows:</p><p>• Monolingual English runs: both runs applied for the validation process the coarse grained expected answer type matching (because with the NE recognizer used in English we can only perform this kind of matching), the NE entailment module and the acronym checking module. The differences come in the paragraph selection process:</p><p>-Run 1: paragraph selection was performed by the module based on lemmas overlapping (setting 1) that was described in Section 2.4.1.</p><p>-Run 2: paragraph selection was performed by the module based on lemmas overlapping and Lexical Entailment (setting 2) that was described in Section 2.4.2. The motivation for using this selection module was to study the effect of Lexical Entailment for ranking candidate answer paragraphs.</p><p>• Monolingual Spanish runs: in both runs the selection process was based on lemmas overlapping (setting 1 described in Section 2.4.1). Both runs applied the validation step in the same way for both the NE entailment module and the acronym checking module. The differences come in the use of the expected answer type matching module:</p><p>-Run 1: it was applied the fine grained expected answer type matching.</p><p>-Run 2: it was applied the coarse grained expected answer type matching. The objective was to study the influence of using a fine grained or a coarse grained matching. It may be thought that the best option is the fine grained matching. However, possible errors in the classification given by the NE recognizer could contribute to obtain better results using the coarse grained option and we wanted to analyze it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis of the Results</head><p>The runs submitted to ResPubliQA 2009 were evaluated by human assessors who tagged each answer as correct (R) or incorrect (W). This year it was allowed to leave unanswered a question when there are no evidences about the correctness of the answer. In order to evaluate the performance of systems rejecting answers, the task allowed to return an hypothetical candidate answer when it was chosen not to answer a question. This answer could be the answer given if it was mandatory to answer all the questions. These answers were evaluated as unanswered with a correct candidate answer (UR), or unanswered with an incorrect candidate answer (UI). The main measure used for evaluation is c@1 (2). Moreover, accuracy (3) was also used as a secondary measure.</p><formula xml:id="formula_2" coords="7,227.28,317.35,285.76,23.07">c@1 = #R n + #R n * #U R + #U I n<label>(2)</label></formula><formula xml:id="formula_3" coords="7,248.04,348.91,265.00,23.07">accuracy = #R + #U R n<label>(3)</label></formula><p>The results obtained for the runs described in Section 3 are shown in Table <ref type="table" coords="7,437.77,375.31,4.98,9.96" target="#tab_0">1</ref> for English and Table <ref type="table" coords="7,117.72,387.19,4.98,9.96" target="#tab_1">2</ref> for Spanish. The results of a baseline system based only on the IR process described in Section 2.1 also appear in each Table . In this baseline, the answer given to each question was the first one according to the IR ranking. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results in English</head><p>Regarding English results, run 2 achieves a slightly higher amount of correct answers than run 1. Since the only difference between both runs was that run 2 used Lexical Entailment for ranking the candidate answers, the improvement was a consequence of using entailment. Although this is not a remarkable result for showing the utility of using entailment for ranking results in QA, it encourages us to explore more complex ways of using entailment for answer paragraphs ranking.</p><p>Comparing English runs with the English baseline it can be seen how the results of the submitted runs are about 10% better according to the given evaluation measures. A preliminary study showed us that most of this variation in the results was a consequence of the different ways for ranking paragraphs and not the inclusion of the validation step. Then, the lemmas overlapping ranking used for the selection of paragraphs in the submitted runs has shown to be more appropriate for this task than the one based only on IR ranking when the QA system is working in English. Therefore, results suggest that it is useful to include information on lemmas when ranking the candidate paragraphs of a system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results in Spanish</head><p>The results of the Spanish submitted runs are quite similar as it can be seen in Table <ref type="table" coords="8,461.40,217.51,3.90,9.96" target="#tab_1">2</ref>. Since the only difference between both runs was the expected answer type matching used, results suggest that there are no big differences between using one or another expected answer type matching. In fact, there were only 11 questions in which the given answers differ. In these 11 questions, 9 questions were incorrectly answered by both runs and in the other 2 ones, there was a correct answer for each run. Nevertheless, we detected that some of the errors obtained when the fine grained expected answer type matching was applied were caused by errors in the NE classification given by the NE recognizer. The possibility of having these errors was one of the motivations for using also coarse grained matching. However, when there was this kind of errors with the fine grained matching, the coarse grained matching did not help to find a right answer. Then, the preliminary analysis of the results show that the fine grained matching could contribute towards improving results, but it depends too much on the classification given by the used NE recognizer.</p><p>On the other hand, if we compare both submitted runs with the baseline run, we can see that the results according to the two evaluation measures are quite similar. This is different to the results obtained in English, in which the submitted runs performed better than the baseline. This means that the lemmas overlapping used for the selection process worked better in English than in Spanish. We want to perform a deeper analysis in order to study why there is such difference between the two languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis of Validation</head><p>Given that one of our objectives for taking part at ResPubliQA was to study the impact of using validation, we find important to study the contribution of the validation modules in our QA system. Table <ref type="table" coords="8,116.76,490.99,4.98,9.96" target="#tab_2">3</ref> shows for each language the number of questions where each of the validation modules was applied. Despite the fact that the basic ideas of the modules where the same in both languages and the question set was also the same (the same questions but translated to each language), it can be seen in Table <ref type="table" coords="8,186.96,526.75,4.98,9.96" target="#tab_2">3</ref> how the numbers differ between languages. This was a consequence of different question formulations for each language and little variations in the implementation of modules for different languages. However, the number of questions that were leaved unanswered was almost the same in both languages as it can be seen in Tables <ref type="table" coords="8,382.68,562.63,4.98,9.96" target="#tab_0">1</ref> and<ref type="table" coords="8,410.29,562.63,3.90,9.96" target="#tab_1">2</ref>. Since the candidate answers given to unanswered questions were also evaluated, it can be measured the precision of systems validating answers (4). Table <ref type="table" coords="8,373.80,684.67,4.98,9.96" target="#tab_3">4</ref> shows the validation precision of the submitted runs for English and Spanish. In each language, the validation precision obtained was the same for both runs. As it can be seen in Table <ref type="table" coords="9,218.88,205.63,3.90,9.96" target="#tab_3">4</ref>, the validation precision achieved by the submitted runs is close to 50% (slightly better in Spanish and slightly worse in English). Therefore, the validation process applied by our QA system has not behaved very well.</p><formula xml:id="formula_4" coords="8,217.32,728.47,295.72,23.52">validation precision = #U W #U R + #U W<label>(4)</label></formula><p>We studied the errors produced by the validation process and we found that most of the errors were produced by the NE entailment module. On one hand, the constraint of having all the NEs of the question into the answer paragraph seemed to be very strict because a paragraph sometimes can omit some NEs that have been referred before in the document. Therefore, in the future we would like to study a way of relaxing these constraints that can allow us to improve results.</p><p>On the other hand, we found in Spanish some errors due to incorrect translations of the questions from English. For example, the NE EEC (which means European Economic Community) in question 17 <ref type="foot" coords="9,152.52,324.32,3.97,4.84" target="#foot_4">5</ref> was kept as EEC in Spanish, but the correct translation is CEE (which means Comunidad Económica Europea). This kind of errors in the translations caused that our system denied paragraphs that could contain correct answers.</p><p>Regarding the acronym checking, we found that its behaviour was quite good in Spanish but not in English. In fact, some questions were leaved unanswered in English because the acronym module was incorrectly applied. Therefore, we have to outperform this module in English.</p><p>Finally, the expected answer type matching was applied in a low amount of questions for both languages and we did not observe several problems in its performance. Now, we want to focus in improving its coverage so that it can help us in a higher amount of questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>In this paper we have described our QA system and the results obtained in both English and Spanish monolingual tasks at ResPubliQA. The main steps of our system were an IR phase focused on improving QA results, a validation step for rejecting no promising paragraphs and a selection of the final answer based on ngrams overlapping.</p><p>The IR ranking has provided a good performance obtaining better results in English than in Spanish, while the validation process was not very helpful. On the other hand, the ranking based on ngrams was able to improve results of the IR module in English, while it maintains the performance in Spanish. Besides, Lexical Entailment has shown to be informative for creating the answers ranking in English.</p><p>Future work is focused on solving the errors detected in each module, as well as developing modules for a broader range of questions. Furthermore, we want to perform a deeper study about the ranking of answers using ngrams in combination with Lexical Entailment.</p><p>search Network MAVIR (S-0505/TIC-0267), the Education Council of the Regional Government of Madrid and the European Social Fund.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,229.56,527.59,139.70,9.96;2,128.71,319.84,345.21,193.32"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: System's architecture.</figDesc><graphic coords="2,128.71,319.84,345.21,193.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,176.52,441.19,249.98,64.35"><head>Table 1 :</head><label>1</label><figDesc>Results for English runs.</figDesc><table coords="7,176.52,463.38,249.98,42.16"><row><cell>Run</cell><cell cols="6">#R #W #UR #UI accuracy c@1</cell></row><row><cell>run 1</cell><cell>282</cell><cell>190</cell><cell>15</cell><cell>13</cell><cell>0.59</cell><cell>0.6</cell></row><row><cell>run 2</cell><cell>288</cell><cell>184</cell><cell>15</cell><cell>13</cell><cell>0.61</cell><cell>0.61</cell></row><row><cell cols="2">baseline 263</cell><cell>236</cell><cell>0</cell><cell>1</cell><cell>0.53</cell><cell>0.53</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,176.52,549.31,249.98,64.35"><head>Table 2 :</head><label>2</label><figDesc>Results for Spanish runs.</figDesc><table coords="7,176.52,571.50,249.98,42.16"><row><cell>Run</cell><cell cols="6">#R #W #UR #UI accuracy c@1</cell></row><row><cell>run 1</cell><cell>195</cell><cell>275</cell><cell>13</cell><cell>17</cell><cell>0.42</cell><cell>0.41</cell></row><row><cell>run 2</cell><cell>195</cell><cell>277</cell><cell>12</cell><cell>16</cell><cell>0.41</cell><cell>0.41</cell></row><row><cell cols="2">baseline 199</cell><cell>301</cell><cell>0</cell><cell>0</cell><cell>0.4</cell><cell>0.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,144.00,593.47,315.02,52.95"><head>Table 3 :</head><label>3</label><figDesc>Number of question where each validation module was applied.</figDesc><table coords="8,175.44,615.54,252.09,30.88"><row><cell cols="4">Language Answer Type NE entailment Acronym</cell></row><row><cell>English</cell><cell>55</cell><cell>209</cell><cell>23</cell></row><row><cell>Spanish</cell><cell>44</cell><cell>179</cell><cell>6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,136.32,118.39,330.39,52.95"><head>Table 4 :</head><label>4</label><figDesc>Validation precision of the submitted runs in English and Spanish.</figDesc><table coords="9,241.56,140.58,119.90,30.76"><row><cell cols="2">Language Val. precision</cell></row><row><cell>English</cell><cell>0.46</cell></row><row><cell>Spanish</cell><cell>0.57</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,105.24,738.24,112.08,7.97"><p>http://nlp.uned.es/clef-qa/ave</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,105.24,724.44,158.05,7.97"><p>In the final system k has been fixed to 100</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,105.24,733.92,407.52,7.97;2,90.00,743.40,26.43,7.97"><p>An implementation of BM25 ranking function over Lucene can be found at :http://nlp.uned.es/ jperezi/Lucene-BM25/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="3,105.24,744.12,235.70,7.97"><p>http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="9,105.24,717.36,407.75,7.97;9,90.00,726.72,51.03,7.97"><p>Why is it necessary to provide for information about certain foodstuffs in addition to those in Directive 79/112/EEC?</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work has been partially supported by the <rs type="funder">Spanish Ministry of Science and Innovation</rs> within the project <rs type="projectName">QEAVis-Catiex</rs> (<rs type="grantNumber">TIN2007-67581-C02-01</rs>), the <rs type="projectName">TrebleCLEF Coordination Action</rs>, within FP7 of the <rs type="funder">European Commission</rs>, Theme <rs type="grantNumber">ICT-1-4-1</rs> <rs type="projectName">Digital Libraries and Technology Enhanced Learning</rs> (Contract <rs type="grantNumber">215231</rs>), the <rs type="funder">Regional Government of Madrid</rs> under the Re-</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_avBy5CB">
					<idno type="grant-number">TIN2007-67581-C02-01</idno>
					<orgName type="project" subtype="full">QEAVis-Catiex</orgName>
				</org>
				<org type="funded-project" xml:id="_GzdWwKX">
					<idno type="grant-number">ICT-1-4-1</idno>
					<orgName type="project" subtype="full">TrebleCLEF Coordination Action</orgName>
				</org>
				<org type="funded-project" xml:id="_HvHXtkV">
					<idno type="grant-number">215231</idno>
					<orgName type="project" subtype="full">Digital Libraries and Technology Enhanced Learning</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,110.52,178.15,402.61,9.96;10,110.52,190.15,402.50,9.96;10,110.52,202.03,268.21,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,393.61,178.15,119.52,9.96;10,110.52,190.15,121.10,9.96">FreeLing: An Open-Source Suite of Language Analyzers</title>
		<author>
			<persName coords=""><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Isaac</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lluís</forename><surname>Padró</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Muntsa</forename><surname>Padró</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,251.28,190.15,261.74,9.96;10,110.52,202.03,158.45,9.96">Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC04)</title>
		<meeting>the 4th International Conference on Language Resources and Evaluation (LREC04)<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.52,222.07,402.64,9.96;10,110.52,233.95,402.57,9.96;10,110.52,245.95,402.57,9.96;10,110.52,257.83,51.01,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,360.84,222.07,152.33,9.96;10,110.52,233.95,402.57,9.96;10,110.52,245.95,33.08,9.96">University of Hagen at QA@CLEF 2008: Efficient Question Answering with Question Decomposition and Multiple Answer Streams</title>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ingo</forename><surname>Glöckner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,166.80,245.95,198.59,9.96">Working Notes for the CLEF 2008 Workshop</title>
		<meeting><address><addrLine>Aarhus, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-09-19">17-19 September. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.52,275.23,402.58,12.48;10,110.52,289.75,402.25,9.96;10,110.52,301.75,48.62,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,399.27,277.75,113.83,9.96;10,110.52,289.75,40.94,9.96">UNED at PASCAL RTE-2 Challenge</title>
		<author>
			<persName coords=""><forename type="first">Jesús</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Felisa</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,171.60,289.75,336.09,9.96">Proceedings of the Second PASCAL Recognizing Textual Entailment Workshop</title>
		<meeting>the Second PASCAL Recognizing Textual Entailment Workshop</meeting>
		<imprint>
			<date type="published" when="2006-04">April 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.52,321.67,402.51,9.96;10,110.52,333.55,402.72,9.96;10,110.52,345.55,402.45,9.96;10,110.52,357.55,198.03,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,332.05,321.67,180.99,9.96;10,110.52,333.55,154.25,9.96">Textual Entailment Recognition Based on Dependency Analysis and WordNet</title>
		<author>
			<persName coords=""><forename type="first">Jesús</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Felisa</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="10,413.88,345.55,99.09,9.96;10,110.52,357.55,56.50,9.96">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Joaquin</forename><surname>Quiñonero Candela</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Florence D'alché</forename><surname>Buc</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">3944</biblScope>
			<biblScope unit="page" from="231" to="239" />
			<date type="published" when="2005">2005</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.52,377.47,402.63,9.96;10,110.52,389.35,402.36,9.96;10,110.52,401.35,63.13,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,474.26,377.47,38.90,9.96;10,110.52,389.35,115.67,9.96">Question Answering in Webclopedia</title>
		<author>
			<persName coords=""><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Laurie</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Junk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,251.88,389.35,228.71,9.96">Proceedings of the Ninth Text REtrieval Conference</title>
		<meeting>the Ninth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="655" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.52,421.27,402.66,9.96;10,110.52,433.27,245.05,9.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,320.33,421.27,192.85,9.96;10,110.52,433.27,43.75,9.96">Answer Validation on English and Romanian Languages</title>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Iftene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandra</forename><surname>Balahur-Dobrescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,175.68,433.27,23.56,9.96">LNCS</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer Verlag</publisher>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct coords="10,110.52,453.19,402.56,9.96;10,110.52,465.07,290.41,9.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,211.22,453.19,301.87,9.96;10,110.52,465.07,14.79,9.96">Binary Codes Capable of Correcting Deletions, Insertions and Reversals</title>
		<author>
			<persName coords=""><forename type="first">Vladimir</forename><surname>Levensthein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,145.68,465.07,105.62,9.96">Soviet Physics -Doklady</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="707" to="710" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.52,484.99,402.50,9.96;10,110.52,496.99,207.49,9.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,222.15,484.99,129.24,9.96">Learning Question Classifiers</title>
		<author>
			<persName coords=""><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,377.88,484.99,135.14,9.96;10,110.52,496.99,176.95,9.96">Proceedings 19th International conference on Computacional Linguistics</title>
		<meeting>19th International conference on Computacional Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.52,516.91,402.64,9.96;10,110.52,528.91,402.63,9.96;10,110.52,540.79,402.46,9.96;10,110.52,552.79,191.17,9.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,236.18,528.91,276.97,9.96;10,110.52,540.79,78.30,9.96">The Structure and Performance of an Open-Domain Question Answering System</title>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marius</forename><surname>Pasca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roxana</forename><surname>Girju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Goodrum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vasile</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,211.92,540.79,301.06,9.96;10,110.52,552.79,92.59,9.96">Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 39th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="563" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.52,572.71,402.56,9.96;10,110.52,584.71,402.47,9.96;10,110.52,596.59,320.65,9.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,447.38,572.71,65.70,9.96;10,110.52,584.71,317.05,9.96">Justification of Answers by Verification of Dependency Relations -The French AVE Task</title>
		<author>
			<persName coords=""><forename type="first">Véronique</forename><surname>Moriceau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xavier</forename><surname>Tannier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arnaud</forename><surname>Grappy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brigitte</forename><surname>Grau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,448.20,584.71,64.79,9.96;10,110.52,596.59,127.43,9.96">Working Notes for the CLEF 2008 Workshop</title>
		<meeting><address><addrLine>Aarhus, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-09-19">17-19 September. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.52,613.99,402.53,12.48;10,110.52,628.51,269.53,9.96" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="10,409.36,616.51,103.69,9.96;10,110.52,628.51,84.24,9.96">Overview of the Answer Validation Exercise</title>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Valentín</forename><surname>Sama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Felisa</forename><surname>Verdejo</surname></persName>
		</author>
		<editor>Peters et al.</editor>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="257" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.52,645.91,402.51,12.48;10,110.52,660.31,402.45,9.96;10,110.52,672.31,402.49,9.96;10,110.52,684.31,402.49,9.96;10,110.52,696.19,90.97,9.96" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,355.11,648.43,157.92,9.96;10,110.52,660.31,36.10,9.96">Overview of the Answer Validation Exercise</title>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Felisa</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,187.80,660.31,325.17,9.96;10,110.52,672.31,259.31,9.96">Advances in Multilingual and Multimodal Information Retrieval, 8th Workshop of the Cross-Language Evaluation Forum, CLEF 2007</title>
		<title level="s" coord="10,353.40,684.31,155.25,9.96">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09-19">2007. September 19-21, 2007. 2007</date>
			<biblScope unit="volume">5152</biblScope>
			<biblScope unit="page" from="237" to="248" />
		</imprint>
	</monogr>
	<note>Revised Selected Papers</note>
</biblStruct>

<biblStruct coords="10,110.52,713.59,402.65,12.48;10,110.52,728.11,351.13,9.96" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="10,110.52,728.11,249.15,9.96">Information Retrieval Baselines for the ResPubliQA Task</title>
		<author>
			<persName coords=""><forename type="first">Joaquín</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guillermo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lourdes</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>In this Volume</note>
</biblStruct>

<biblStruct coords="11,110.52,111.43,402.52,9.96;11,110.52,123.43,402.43,9.96;11,110.52,135.31,402.55,9.96;11,110.52,147.31,402.66,9.96;11,110.52,159.31,236.67,9.96" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,382.80,123.43,130.16,9.96;11,110.52,135.31,397.28,9.96">Evaluation of Multilingual and Multi-modal Information Retrieval, 7th Workshop of the Cross-Language Evaluation Forum</title>
	</analytic>
	<monogr>
		<title level="m" coord="11,110.52,147.31,49.05,9.96">CLEF 2006</title>
		<title level="s" coord="11,121.80,159.31,152.14,9.96">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Fredric</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jussi</forename><surname>Karlgren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maximilian</forename><surname>Stempfhuber</surname></persName>
		</editor>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">September 20-22, 2006. 2007</date>
			<biblScope unit="volume">4730</biblScope>
		</imprint>
	</monogr>
	<note>Revised Selected Papers</note>
</biblStruct>

<biblStruct coords="11,110.52,179.23,402.47,9.96;11,110.52,191.11,397.33,9.96" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,410.52,179.23,102.47,9.96;11,110.52,191.11,94.76,9.96">Question-Answering by Predictive Annotation</title>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anni</forename><surname>Coden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dragomir</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,226.92,191.11,181.89,9.96">Proceedings of the 23rd SIGIR Conference</title>
		<meeting>the 23rd SIGIR Conference</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="184" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.52,211.03,402.87,9.96;11,110.52,223.03,402.52,9.96;11,110.52,235.03,232.11,9.96" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="11,275.31,211.03,238.08,9.96;11,110.52,223.03,189.29,9.96">Some Simple Effective Approximations to the 2-Poisson Model for Probabilistic Weighted Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steve</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,146.04,235.03,25.91,9.96">SIGIR</title>
		<editor>
			<persName><forename type="first">W</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</editor>
		<imprint>
			<publisher>ACM/Springer</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="232" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,111.72,252.43,401.40,12.48;11,110.52,266.83,311.40,9.96" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="11,421.56,254.95,91.55,9.96;11,110.52,266.83,147.62,9.96">The Effect of Entity Recognition on Answer Validation</title>
		<author>
			<persName coords=""><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jesús</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Felisa</forename><surname>Verdejo</surname></persName>
		</author>
		<editor>Peters et al.</editor>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="483" to="489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,111.72,284.23,401.52,12.48;11,110.52,298.75,402.65,9.96;11,110.52,310.75,402.62,9.96;11,110.52,322.63,258.63,9.96" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="11,346.95,286.75,166.29,9.96">UNED at Answer Validation Exercise</title>
		<author>
			<persName coords=""><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Felisa</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="11,481.68,310.75,31.47,9.96;11,110.52,322.63,117.10,9.96">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Valentin</forename><surname>Jijkoun</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thomas</forename><surname>Mandl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vivien</forename><surname>Petras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Diana</forename><surname>Santos</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">5152</biblScope>
			<biblScope unit="page" from="404" to="409" />
			<date type="published" when="2007">2007. 2007</date>
			<publisher>Springer</publisher>
		</imprint>
		<respStmt>
			<orgName>CLEF</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="11,111.72,340.03,401.31,12.48;11,110.52,354.55,258.73,9.96" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="11,355.11,342.55,157.92,9.96;11,110.52,354.55,36.10,9.96">Overview of the Answer Validation Exercise</title>
		<author>
			<persName coords=""><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Felisa</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,189.24,354.55,23.56,9.96">LNCS</title>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2008">2008. 2009</date>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct coords="11,110.52,374.47,402.45,9.96;11,110.52,386.47,402.59,9.96;11,110.52,398.35,402.55,9.96;11,110.52,410.35,68.65,9.96" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="11,180.85,386.47,332.26,9.96;11,110.52,398.35,63.94,9.96">INAOE at QA@CLEF 2008: Evaluating Answer Validation in Spanish Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Alberto</forename><surname>Téllez-Valero</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Juárez-González</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manuel</forename><surname>Montes Y Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luis</forename><surname>Villaseñor-Pineda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,195.96,398.35,196.19,9.96">Working Notes for the CLEF 2008 Workshop</title>
		<meeting><address><addrLine>Aarhus, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-09-19">17-19 September. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.52,430.27,402.45,9.96;11,110.52,442.27,402.54,9.96;11,110.52,454.15,146.89,9.96" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="11,183.73,442.27,275.88,9.96">Using Non-Overlap Features for Supervised Answer Validation</title>
		<author>
			<persName coords=""><forename type="first">Alberto</forename><surname>Téllez-Valero</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Juárez-González</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manuel</forename><surname>Montes Y Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luis</forename><surname>Villaseñor-Pineda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,483.60,442.27,23.56,9.96">LNCS</title>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct coords="11,110.52,474.07,402.53,9.96;11,110.52,486.07,146.89,9.96" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="11,264.64,474.07,195.21,9.96">Information Synthesis for Answer Validation</title>
		<author>
			<persName coords=""><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Günter</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,483.60,474.07,23.56,9.96">LNCS</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer Verlag</publisher>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
