<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,128.71,74.03,337.85,12.33">Multiple Sequence Alignment for Morphology Induction</title>
				<funder ref="#_rwaGfH3">
					<orgName type="full">DOD/NGIA</orgName>
				</funder>
				<funder ref="#_PrbXApM">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,256.45,118.05,82.37,8.72"><forename type="first">Tzvetan</forename><surname>Tchoukalov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postBox>P.O. Box 17469</postBox>
									<postCode>94309</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,227.41,197.25,71.84,8.72"><forename type="first">Christian</forename><surname>Monson</surname></persName>
							<email>monsonc@ohsu.com</email>
							<affiliation key="aff1">
								<orgName type="department">Center for Spoken Language Understanding (CSLU) OGI School of Science &amp; Engineering</orgName>
								<orgName type="institution">Oregon Health &amp; Science University (OHSU)</orgName>
								<address>
									<addrLine>20000 NW Walker Road Beaverton</addrLine>
									<postCode>97006</postCode>
									<region>Oregon</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,318.77,197.25,49.06,8.72"><forename type="first">Brian</forename><surname>Roark</surname></persName>
							<email>roark@cslu.ogi.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Center for Spoken Language Understanding (CSLU) OGI School of Science &amp; Engineering</orgName>
								<orgName type="institution">Oregon Health &amp; Science University (OHSU)</orgName>
								<address>
									<addrLine>20000 NW Walker Road Beaverton</addrLine>
									<postCode>97006</postCode>
									<region>Oregon</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,128.71,74.03,337.85,12.33">Multiple Sequence Alignment for Morphology Induction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">34E7BB16E1065F9DF63B6D961C70097E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I.2 [Artificial Intelligence]: I.2.7 NaturalLanguage Processing Algorithms</term>
					<term>Experimentation</term>
					<term>Languages Natural Language Morphology</term>
					<term>Unsupervised Learning</term>
					<term>Morphology Induction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>MetaMorph is a novel application of multiple sequence alignment (MSA) to natural language morphology induction. Given a text corpus in any language, we sequentially align a subset of the words of the corpus to form an MSA using a probabilistic scoring scheme. We then segment the MSA to produce output analyses. We used this algorithm to compete in the 2009 Morpho Challenge. The F-measure of the analyses produced by MetaMorph are low for the full development corpus, but high for the corpus subsets used to generate the MSA, even surpassing the F-measure of another system used to aid MSA segmentation. This suggests that MSA is an effective algorithm for unsupervised morphology induction and may yet outperform the state-ofthe-art morphology induction algorithms. Future research directions are discussed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this paper we describe the newly developed MetaMorph algorithm and report on its performance. The MetaMorph algorithm is an unsupervised natural language morphology induction algorithm utilizing multiple sequence alignments (MSA). MSAs are frequently implemented in biological sequence processing in order to capture information about long-distance dependencies and three-dimensional structures of protein or nucleotide sequences without resorting to polynomial complexity context-free models <ref type="bibr" coords="1,373.86,738.45,80.51,8.72" target="#b1">(Durbin et al., 1998)</ref>. However, MSA</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Introduction to Multiple Sequence Alignment</head><p>MSAs are frequently used in biological sequence processing, where sets of evolutionarily related sequences are collectively aligned. The resulting alignment is in table format comprised of sequences containing gaps. Depending on the corpus of sequences being aligned, sequences are constructed from an alphabet. In the case of aligning RNA sequences, the alphabet is {A, C, T, G}, for example.</p><p>Simple statistical alignment models, typically called profile models, can be constructed for a MSA. Position specific score matrices (PSSM) can assign a cost to each of the symbols in the alphabet as well as to the gap symbol. Given a MSA, the PSSM values can be estimated as the negative logarithm of the probability of a symbol given a column. Typically Laplace's rule (add one observation per symbol per column) is used to smooth such distributions, along with relative frequency estimation. However, this model does not provide a mechanism to insert symbols between columns, which may be necessary. Such a mechanism is provided by a profile hidden Markov model and can be estimated from a given MSA, using relative frequency estimation and simple smoothing.</p><p>Dynamic programming can be implemented to efficiently achieve optimal alignment of a sequence with a PSSM or profile HMM. The dynamic programming table would have dimensions defined by the number of columns on one side and the length of the sequence on the other. The resulting alignment of a sequence of length N with a MSA which has M columns takes O(NM) in space and time.</p><p>In our application of MSA, we align large numbers of sequences obtained from a text corpus. The alphabet used is the set of characters encountered in the language corpus, which is typically the alphabet of the language and a few additional characters such as numbers or punctuation marks depending on the purity of the corpus. We implement a profile HMM in aligning sequences to generate a MSA. Figure <ref type="figure" coords="2,377.55,705.09,4.92,8.72">1</ref> depicts a MSA. The tabular format of the MSA can be seen in the left portion of the figure. It is composed of 8 columns and 10 sequences. The table in the figure depicts the probability distributions of each of the columns of the MSA. A column distribution is composed of the MSA alphabet and a count for each of the characters of that alphabet and also for the gap character. These counts are the occurrences of the character in the column plus the Leplace smoothing constant of one. The probability of a character given a column is the ratio of the character count in that column's distribution to the distribution total. For example, the probability of the character 'd' given column 1 is the ratio 5/26; the cost for aligning the character 'd' to column 1 would therefore be the negative logarithm of this probability, or approximately 1.65.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The MetaMorph Algorithm</head><p>The MetaMorph algorithm implements progressive MSA for unsupervised natural language morphology induction. An ordered list of similar words with respect to Levinstein distance is obtained from the language corpus. Words from the ordered list are sequentially aligned to the probability distribution of the MSA using a linear gap scoring method. When alignment is complete for all words in the ordered list, multiple cycles of realignment are performed, where each word in the MSA is sequentially extracted from the alignment and aligned to the remaining alignment anew. These cycles are halted when the MSA remains unchanged or when, after an initial number of iterations, the MSA's end-of-cycle sum of column probability distribution entropy scores increases from the previous cycle's sum. At this phase, certain columns of the MSA are chosen to segment the alignment into chunks of columns. Each chunk represents a distinct morpheme of the word in each row of the MSA. The morphological analysis of the corpus is generated by first recording the words contained in the MSA and their respective analyses based on the alignment segmentation into word morpheme boundaries. Finally, the remaining words of the corpus are individually aligned to the MSA to produce their analyses based on the morpheme boundaries generated by the alignment's existing segmentation. Each of the phases of the MetaMorph algorithm are discussed in more detail in sections 3.1 through 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Step 1: Reordering</head><p>When generating a MSA, words are sequentially aligned to the MSA, which is initialized with a single word. To increase the morphological information stored within the MSA, words that are more similar with respect to Levinstein distance are selected earlier in the generation phase. We also wish to use only a small subset of the Chars 1 2 3 4 5 6 7 8</p><formula xml:id="formula_0" coords="3,283.92,89.19,157.32,188.77">a 1 2 5 1 1 1 5 1 c 1 1 1 1 5 1 1 1 d 5 1 1 1 1 1 1 3 e 1 1 1 1 1 1 1 1 g 1 1 1 2 1 1 1 5 h 1 1 1 1 2 1 1 1 i 1 1 1 1 1 5 1 1 j 5 1 1 1 1 1 1 1 l 2 1 1 1 1 1 1 1 m 1 1 1 5 1 1 1 1 n 1 1 1 6 2 1 5 1 p 1 1 1 1 5 1 1 1 r 2 1 1 1 1 1 1 1 s 1 1 1 1 1 1 2 2 u 1 1 7 1 1 1 1 1 gap 1 10 1 1 1 7 2 4</formula><p>12345678 d-anc-es d-anc-ed d-anc-ed-ancing r-unning j-umping j-ump-ed j-ump-sj-ump--laughing</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure1:ExampleMSAwithColumnDistributions</head><p>text corpus in generating the MSA. We therefore use a greedy heuristic to generate an ordered corpus subset consisting of similar words to later be aligned. The basic steps in this heuristic are to, first, choose an initial pair of words to include in the subset, and second, iteratively add in one additional word until the subset of ordered words is of the desired size. We implement dynamic programming to generate the Levinstein distance between all pairs of words. The corpus of words that we use is sorted by word occurrence frequency in the source literature. In order to obtain an initial, locally closest pair of words to add to the subset we calculate pair-wise Levinstein distances for a small number of the corpus and choose the closest pair. The first 1000 words of the frequency sorted words of the corpus, constrained to be between 5 and 16 characters in length and containing no hyphens or numbers, constitute this set, in an attempt to target words with meaningful morphological structure. Because our corpus is sorted by word frequency with most frequent words appearing earlier in the corpus, the initial pair will in most cases contain properly spelled words. However, the algorithm will likely include misspellings of words in the corpus subset since the Levinstein distance is very small between a properly spelled word and its misspelling.</p><p>In order to complete the second step of the process we maintain a list of potential words from which to choose the next best word to add to the corpus subset. We impose a bound on its size to be the difference between the desired size of the corpus subset and the corpus subset's current size. Once a word is added to the corpus subset, we calculate the Levinstein distances between that word and each candidate word, or rather, each remaining word in the corpus that does not yet appear in the subset. If the list of potential words has not reached its size bound, a candidate word is added to the potential words list with its corresponding Levinstein distance value. If the list of potential words is full, a candidate word is only added if its Levinstein distance is less than that of the word in the potential words list with the greatest Levinstein distance, in which case that word is removed from the list. When all candidate words have been examined, the word in the list of potential words with the lowest Levinstein distance is removed and added to the corpus subset.</p><p>We note some strategies we use in our implementation of this heuristic. For the sake of algorithmic efficiency we implement two priority queues utilizing a heap-based structure to represent the list of potential words. Both priority queues contain the list of potential words but one is ordered in increasing Levinstein distance while the other in decreasing. Thus the best and worst words, or the words with the least and greatest Levinstein distance values, are easily accessible because the priority queues impose a sorting on the words. Adding words to the queues and removing the best and worst words takes logarithmic time. Since searching in the queues and removing any other words besides the best and worst takes linear time, we simply do not check to see if a candidate word appears in the list when considering it as a new candidate word, thus a candidate word may appear multiple times in this list. Similarly, we do not remove all occurrences of a word in this list when we add it to the corpus subset. Instead, if a word is returned from the priority queues as the next best word to add to the corpus subset and the word is already included in the subset, we simply ignore it and retrieve the next best. We also do not remove the best word from the priority queue we use to return the worst word, because this would take linear time. Similarly, we do not remove the worst word from the priority queue that we use to return the best word. In summary, if a word is returned by the queues that is already in the subset, we ignore that word and retrieve the next word. This results in substantial speedups of the algorithm.</p><p>Finally, the other technique we use to substantially reduce algorithm run time is if we are calculating Levinstein distance between two words to see if it is less than a certain value, at each step in the dynamic programming we determine if there is a possibility of the result being less than the value. If there is no such possibility, we terminate the dynamic programming early. Also, before we begin dynamic programming, we check to see if the difference in the size of the words will produce a value greater than our tolerance value and do not perform dynamic programming if it thus has no chance of returning a value less than the tolerance. This short-circuiting of the dynamic programming approach in determining Levinstein distance substantially decreases algorithm run time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Step 2: Alignment</head><p>We next implement progressive alignment <ref type="bibr" coords="4,263.56,731.49,110.37,8.72" target="#b2">(Feng and Doolittle, 1987)</ref> to produce a MSA from the corpus subset we generated in the reordering phase above. Each word is considered as a sequence of characters. We implement a probabilistic scoring method with linear gap scoring, where gaps in a sequence are treated as characters in the word. The MSA is initialized to the first word in the sorted set where each character in the word is a new column of length one. Column distributions are created for each new column by initializing the counts for each alphabetical character to a predefined Leplace smoothing value and then incrementing the count of each character that appears in the column by one, and normalizing.</p><p>Then we proceed to sequentially align the remaining words in the corpus subset to the MSA. The cost of a character-to-column match or of inserting a gap in the new sequence is the negative logarithm of the probability of the specific character or gap in the column's distribution. The cost of inserting a column of gaps in the MSA is equivalent to the cost of matching a character to a column in which the character does not yet occur. Since the column probability distribution totals increase as more sequences are aligned to and inserted in the MSA, this cost increases dramatically with the size of the MSA. The score of an alignment of a sequence to the MSA is the additive combination of these character or gap to column matches or gap column insertions. This allows the use of dynamic programming with back-tracing to efficiently determine an optimal alignment, minimizing the alignment score. The modified sequence is then incorporated into the MSA and the MSA's column distributions are updated so the sequence's character counts are incremented in the respective MSA column distributions. When all words in the corpus subset have been inserted into the MSA in this fashion, the initial alignment cycle is complete.</p><p>Using a corpus of 500,000 Hungarian words paired with a gold standard morphological analysis, we ran the MetaMorph algorithm on various corpus subset sizes, column distribution smoothing values, and even implemented a modified scoring scheme that included entropy scores in determining alignment scores. We generated corpus subsets of 5,000, 10,000, and 20,000 words and used the MetaMorph algorithm to generate MSAs of these subsets using smoothing values of 1, 0.3, 0.1, 0.01, and 0.001 and also using an alternate scoring scheme besides the standard scoring scheme described above. We used this development set to establish working parameters.</p><p>The alternate scoring scheme attempts to take column probability distribution entropy into consideration when assigning a score to a character or gap to column match. In these two cases, the score of a match is calculated in the same manner as in the scoring scheme mentioned before, except that the column probability distribution entropy is subtracted from the cost. Thus a high entropy, corresponding to a uniform probability distribution, would result in a low cost for matching. We hoped this would influence the MSA in exhibiting a root region, or a region of columns with uniform probability distributions, resulting in high entropy values, which would contain the roots of the corpus subset words and thus allow the prefixes and suffixes to align more directly in the periphery of the MSA, resulting in low entropy values. Our main purpose in doing this was to better enhance the success of our implicit segmentation strategies, which we will discuss later.</p><p>This alternate scoring scheme resulted in identical MSAs as the original scoring scheme. Furthermore, varying the Leplace smoothing value used in the column probability distributions resulted in insignificant changes in the MSA. MSAs with various smoothing values would result in approximately the same MSA columns but with gap columns, or columns predominantly composed of the gap character, located in slightly different areas of the alignment. In all cases, gap columns were found uniformly scattered in the MSAs, thus the differences in the MSAs resulting from various smoothing values were insignificant. We thus used the standard scoring scheme with a smoothing value of one to produce our final results. The equivalence of the two scoring methods also suggests that the involvement of entropy in the scoring scheme was overshadowed by the magnitude of the character or gap costs in column probability distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Step 3: Re-alignment</head><p>Once we have aligned all words in the subset to a MSA we perform iterative refinement <ref type="bibr" coords="5,444.67,678.69,55.17,8.72" target="#b3">(Gotoh, 1996)</ref>. This is done by repeatedly performing leave-one-out realignment cycles until certain termination conditions are met. Each leave-one-out realignment cycle consists of removing each sequence of the MSA in turn, decrementing the sequence's character count in the respective column distributions, finding the best alignment of the word represented by the sequence, and reinserting this modified, currently optimal sequence back into the MSA.</p><p>At the end of each alignment cycle (both initial alignment and realignment cycles), the MSA is given a score comprising of the sum of the entropy of each column's distribution. Typically this score decreases with each cycle, but may increase in some initial realignment cycles. Therefore, the condition for terminating the execution of a subsequent realignment cycle is set to be an increase in total entropy score of the MSA after a predefined number of unhindered realignment cycles. The other condition for termination is if the MSA remains unchanged after a realignment cycle. Originally, this last condition was our only condition for terminating realignment cycles, but we observed that after many realignment cycles, the MSA occasionally converged to a cyclic reoccurrence of a number of alignment configurations. We therefore add the entropy condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Step 4: Segmentation</head><p>Having obtained a final MSA, we must now produce a morphological analysis of the language corpus. We had many ideas about how to do this, but settled on a simple baseline technique due to time constraints. We implement a greedy algorithm to iteratively generate segmentation schemes using a third-party <ref type="bibr" coords="6,462.52,255.57,62.22,8.72;6,70.56,268.77,23.27,8.72" target="#b4">(Monson et al., 2009)</ref> analysis provided by a different unsupervised morphology induction algorithm as a guide. This approach is expected to utilize the third-party analysis in identifying columns whose column boundaries likely mark morpheme boundaries. The approach also relies on the MSA alignment of sequences to properly segment a large portion of the corpus. Each segmentation scheme consists of a set of column boundaries at which we segment the MSA. The greedy algorithm is comprised of the following steps. We begin with an empty segmentation scheme, which imposes no morpheme boundaries, as the current optimal segmentation scheme. First, for each column in the MSA in turn, we produce a potential segmentation scheme that consists of this column together with those in the current optimal segmentation scheme. We then score the analysis induced on either the whole corpus or on the corpus subset against the third-party analysis. Second, we select the segmentation column which most improves the F-measure and add this column to the current optimal segmentation scheme. If no such column is found, then the algorithm terminates and the current optimal segmentation scheme is used to produce the final analysis. Third, if the process was not terminated in step two then we repeat this process from step one. The two potential scoring methods mentioned in step one produce two distinct segmentation schemes. If we score the analyses induced on the whole corpus by the potential segmentations, the segmentation scheme we receive from the algorithm is optimized for an analysis on the whole corpus. Similarly, the segmentation scheme resulting from scoring the analyses induced on the corpus subset by the potential segmentations is optimized for analyzing the corpus subset.</p><p>Once a segmentation strategy is given, words in the corpus subset are morphologically analyzed by imposing morpheme boundaries where the segmentation column boundaries in the segmentation scheme divide the sequences. Similarly, we align each word of the corpus not contained in the subset to the MSA and, as before, impose morpheme boundaries where the segmentation column boundaries in the segmentation scheme divide the word's sequence.</p><p>We developed segmentation strategies that only relied on the MSAs to define a segmentation scheme. These initial segmentation strategies involve segmenting at the left column boundaries of columns with the maximal gap probability in their distributions and columns with minimal probability distribution entropies. We defined four such segmentation strategies. Strategy one segmented at columns with locally maximum gap probability. Strategy two segmented at columns with globally maximum gap probability. Strategy three segmented at columns with locally minimum probability distribution entropy. Finally, strategy four segmented at columns with globally minimum probability distribution entropy. Figure <ref type="figure" coords="6,310.37,639.09,4.92,8.72" target="#fig_1">2</ref> depicts the first six sequences of one of the MSAs of the Hungarian corpus, where sequence gaps are represented as dashes. The two lines of asterisks and ampersands represent the four segmentation strategies for this MSA. The asterisks and ampersand in the first of these two rows indicate a column with locally maximum gap probability and the ampersand indicates the column with globally maximal gap probability. The asterisks and ampersand in the second row indicate columns with locally minimum probability distribution entropy and the ampersand indicates the column with the globally minimum entropy. Sequences two through five are variations of the first sequence, which is a postposition, and are reasonably aligned. The final 'i' and 't' in the third sequence are distinct morphemes while the endings on the other variations of the first sequence are single morphemes. The last sequence is a verb and is linguistically unrelated to the other sequences. As can be observed, it was found that strategies one and three always tended to be almost identical. Furthermore, these segmentation strategies exhibited no correlation with actual morpheme boundaries in the words. Because gap columns were uniformly spread out in the MSAs, segmentation strategies one and three over-segmented, producing analyses that would define morpheme boundaries between every character or every few characters of each word. While segmentation strategies two and four under-segmented, and were often different from each other, they would frequently define a morpheme boundary in the roots of words, as is the case above. It can therefore be seen that these segmentation strategies were not successful in producing meaningful morphological analyses. Due to the failure of these implicit segmentation strategies, we developed the two explicit segmentation strategies that depend on a third-party morphological analysis of the corpus data. We utilized the morphological analysis produced by this year's new unsupervised Paramor-Morfessor union morphology induction algorithm, cited in section 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>We ran the MetaMorph algorithm on a Hungarian corpus of 500,000 words with corpus subsets of 5,000, 10,000, and 20,000 words and a Leplace smoothing value of one. Our results suggest that there was a difference in F-measure between the two segmentation strategies. Both segmentation strategies produced potential segmentations yielding analyses scored against the third-party analysis to determine an optimal segmentation strategy. The difference between these two strategies is that the first scores full potential analyses while the second only scores the portion of the potential analyses corresponding to the corpus subset. Thus the first strategy finds the segmentation scheme that optimizes the analysis score over the whole corpus, while the second only over the corpus subset. We also observed that when using the first segmentation strategy, the corpus subset size influences F-measure. Besides this case, corpus subset size and F-measure seemed unrelated. Finally, there is a substantial difference between F-measures of the entire corpus analyses and the F-measures of only the corpus subset analyses against the gold standard.</p><p>We will refer to the first segmentation strategy mentioned in the previous paragraph as the full corpus analysis segmentation strategy, or just "full" in the figures, and the second segmentation strategy as the corpus subset analysis segmentation strategy, or just "subset" in the figures. The corpus subset analysis segmentation strategy is intended to optimize the morphological analysis of the words contained in the corpus subset while the full corpus analysis segmentation strategy is intended to optimize the morphological analysis of all words in the corpus. These predictions are observable in the actual data. As can be seen in figure <ref type="figure" coords="7,437.78,718.53,3.74,8.72">3</ref>, the morphological analysis produced using the full corpus analysis segmentation strategy resulted in a better F-measure than the analysis produced using the corpus subset analysis segmentation strategy, especially in the case where the corpus  <ref type="figure" coords="7,95.28,109.39,411.67,12.22;7,86.88,125.23,420.04,12.22;7,86.88,141.07,420.03,12.22;7,86.88,156.91,420.04,12.22;7,86.88,172.75,420.03,12.22;7,86.88,188.83,403.22,12.22">----k----ö---z-----ö-------t-----------t--------------k----ö---z-----ö-------t-----------t----i---------k----ö---z-----ö-------t-----------t----i-t-------k----ö---z-----ö-------t-----------t----e---------k----ö---z-----ö-------t-----------t----e-m-------k----ö---t-----ö-------t-----------t----e-m</ref> F-measure</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subset Size</head><p>Figure3:FullCorpusAnalysis Using3CorpusSubsetSizesand2 SegmentaEonStrategies subset full subset size is 10,000 words. Analogously, as can be seen in figure <ref type="figure" coords="8,337.94,658.05,3.73,8.72">4</ref>, the morphological analyses produced on the corpus subsets alone using the two segmentation strategies also reflect the implemented strategy. The analysis produced using the corpus subset analysis segmentation strategy is substantially more accurate than that produced by the full corpus analysis segmentation strategy.</p><p>The final and more significant result, however, is that even though the corpus subset analysis segmentation scheme was produced by scoring morphological analyses of the corpus subset against the third-party analysis, the resulting morphological analysis of the corpus subset using MetaMorph greatly outperformed the third-party analysis on the corpus subset for the 5,000 and 10,000 corpus subset sizes. The degree to which the MetaMorph analysis on the corpus subsets outperforms the third-party analysis can be observed in figure <ref type="figure" coords="9,449.93,86.61,3.73,8.72">5</ref>. This final result suggests that the MetaMorph algorithm's analysis of words in the MSA is highly successful for smaller sets of words, while the analysis of words not included in the MSA is not as successful. These data suggest that MetaMorph is an effective method for aligning words in a MSA that reflects morpheme boundaries with significant accuracy and thus is an effective algorithm for morphology induction in principle, but fails to effectively utilize this MSA to accurately predict the morpheme boundaries of words in the language corpus not included in the corpus subset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Directions</head><p>We conclude by discussing directions into which this research can be further extended. The two main directions we suggest include, first, modifying the existing MetaMorph algorithm to take advantage of its accurate analysis of smaller sized corpus subsets and, second, modifying the scoring scheme to define prefix, root, and suffix regions in the MSA.</p><p>In the first method, we would impose a partition of the corpus into equivalence classes or words that are mutually within a predefined Levinstein distance tolerance apart. We would then construct a MSA using each equivalence class as described in the MetaMorph algorithm. Finally, each MSA is segmented by using the corpus subset analysis segmentation strategy described in the MetaMorph algorithm. Alternatively, an implicit segmentation strategy may be discovered if the Levinstein distance tolerance used in creating the partition is adequately small. In that case, words may tend to be very similar, and perhaps column probability distribution entropy or column gap probability may successfully indicate segmentation columns correlating to morpheme boundaries.</p><p>The corpus subset analysis F-measure using the corpus subset analysis segmentation strategy was found to be high. Similarly, we anticipate that implicit segmentation strategies will also provide similar efficacy with an appropriately set Levinstein distance tolerance. These suggest that the analyses of the individual equivalence classes will also yield a large F-measure. Extrapolation of the results of figure <ref type="figure" coords="9,403.11,427.41,4.92,8.72">5</ref> suggest that smaller corpus subset sizes yield larger F-measures. Therefore, by setting a low Levinstein distance tolerance we would expect the equivalence classes to be smaller in word number and the resulting analyses to thus yield a larger F-measure than that produced by the corpus subset of size 5,000.</p><p>The corpus may contain words that have a large Levinstein distance from almost all other words in the corpus. This may result in a number of equivalence classes in the partition with low or single word counts. Therefore, if the corpus subset analysis segmentation strategy is used, the analysis of these words will likely be identical or worse than the analysis of these words in the third-party analysis. However, the F-measure of the analysis of the words in the larger equivalence classes is expected to be much higher than the third-party analysis. This suggests that the overall F-measure will still be higher than that of the third-party analysis and thus this modified MetaMorph algorithm will outperform the third-party algorithm over the full corpus.</p><p>The second foreseeable alteration to the MetaMorph algorithm in hopes of improving its performance may be the modification of the scoring scheme. We would not focus on whether we use one or more MSAs as done above. Instead we would focus on redefining the construction algorithm of the MSA in hopes of better isolating prefixes and suffixes of words. By initializing the MSA to a certain size, we may allocate a certain number of the initial and final columns to be prefix and suffix areas and allocate the middle portion to be a root area. We may define two different scoring schemes to be used when aligning a sequence (word) to the existing MSA. In both schemes we would attempt to emphasize the role of column probability distribution entropy, which may be done by various approaches. We may do this by normalizing the negative logarithm of the probability of a character or gap in a column probability distribution to the potential range of entropy in order for this value to not overshadow the entropy's involvement in the cost function. We may altogether not assign a cost for inserting a gap into the sequence and matching this gap to a column. Or we may simply not consider a gap as a character at all and allow the column probability distribution sizes to vary from column to column depending on the number of gaps they contain.</p><p>In any case, our overall objective with the scoring schemes would be to maximize probability distribution entropies of columns in the root region and minimize those in the prefix and suffix regions. Thus the overall scoring approach is not the only one to consider. One of the two schemes is to be used if we are matching a character or gap to a column contained in the root region of the MSA. In this scoring scheme, it is intended that a high column entropy value desirably affect the cost of matching. Or rather, we may subtract the current column entropy from the column entropy with the character or gap inserted into the column. A positive result would indicate an increase in column entropy while a negative result would indicate a decrease in column entropy. Since we desire the root region of the alignment to contain characters more uniformly and thus be something of a mixing pot, a positive result would be a desirable cost, while a negative result would be an undesirable cost. The other scoring scheme is the reverse of the one discussed. We wish the probability distribution entropies of columns in the prefix and suffix regions to decrease, and thus specialize in one or a couple of characters, thus locking re-occurring prefixes and suffixes of words in these regions. Thus, by taking the difference in entropies as above, a positive result is undesirable while a negative result is desirable. The two scoring schemes just mentioned may be used alone or incorporated into the overall scoring schemes mentioned earlier. Otherwise, one of the overall scoring schemes may be used and modified to respectively take column probability distribution entropy into account when aligning sequences.</p><p>In summary, the performance of the current version of the MetaMorph algorithm is mediocre on complete language corpora. However, its results on the morphology induction of the Hungarian corpus subset suggest that the technique is very effective in principle. We suggest a couple of directions which can be followed to improve the MetaMorph algorithm.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,103.44,225.74,399.03,9.81;7,117.64,241.11,106.94,9.80"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: First Six Sequences of a Hungarian Corpus MSA with Indicated Implicit SegmentationSchemes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,356.64,295.27,159.41,10.87;8,353.11,309.29,166.49,10.86;8,375.86,324.29,121.04,10.86"><head></head><label></label><figDesc>Figure4:CorpusSubsetAnalysisUsing3CorpusSubsetSizesand2SegmentaEonStrategies</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research was supported in part by <rs type="funder">NSF</rs> Grant #<rs type="grantNumber">IIS-0811745</rs> and <rs type="funder">DOD/NGIA</rs> grant #<rs type="grantNumber">HM1582-08-1-0038</rs>. Any opinions, findings, conclusions or recommendations expressed in this publication are those of the authors and do not necessarily reflect the views of the <rs type="funder">NSF</rs> or DOD.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_PrbXApM">
					<idno type="grant-number">IIS-0811745</idno>
				</org>
				<org type="funding" xml:id="_rwaGfH3">
					<idno type="grant-number">HM1582-08-1-0038</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,70.56,477.33,454.07,8.72;10,84.76,490.53,439.95,8.72;10,84.76,503.73,439.88,8.72;10,84.76,516.93,229.66,8.72" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,312.53,477.33,212.10,8.72;10,84.76,490.53,234.21,8.72">Unsupervised Discovery of Morphologically Related Words Based on Orthographic and Semantic Similarity</title>
		<author>
			<persName coords=""><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Matiasek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Harald</forename><surname>Trost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,327.90,490.53,196.81,8.72;10,84.76,503.73,439.88,8.72;10,84.76,516.93,82.84,8.72">ACL Special Interest Group in Computational Phonology in Cooperation with the ACL Special Interest Group in Natural Language Learning (SIGPHON/SIGNLL)</title>
		<meeting><address><addrLine>Philadelphia, Pennsylvania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="48" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.56,530.13,454.15,8.72;10,84.76,543.33,439.88,8.72;10,84.76,556.53,40.23,8.72" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="10,400.20,530.13,124.51,8.72;10,84.76,543.33,223.26,8.72">Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids</title>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Durbin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sean</forename><forename type="middle">R</forename><surname>Eddy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anders</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Graeme</forename><surname>Mitchison</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge, United Kingdom</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.56,569.97,454.17,8.72;10,84.76,583.17,270.17,8.72" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,267.44,569.97,257.28,8.72;10,84.76,583.17,71.93,8.72">Progressive Sequence Alignment as a prerequisite to correct phylogenetic trees</title>
		<author>
			<persName coords=""><forename type="first">Da-Fei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Russell</forename><forename type="middle">F</forename><surname>Doolittle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,162.79,583.17,125.25,8.72">Journal of Molecular Evolution</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="351" to="360" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.56,596.37,454.00,8.72;10,84.76,609.57,440.01,8.72;10,84.76,622.77,22.46,8.72" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,139.25,596.37,385.31,8.72;10,84.76,609.57,244.17,8.72">Significant improvement in accuracy of multiple protein sequence alignments by iterative refinement as assessed by reference to structural alignments</title>
		<author>
			<persName coords=""><forename type="first">Osamu</forename><surname>Gotoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,336.73,609.57,120.16,8.72">Journal of Molecular Biology</title>
		<imprint>
			<biblScope unit="volume">264</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="823" to="838" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.56,635.97,454.15,8.72;10,84.76,649.17,92.46,8.72" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,338.84,635.97,86.65,8.72">Probabilistic ParaMor</title>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Monson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristy</forename><surname>Hollingshead</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,433.61,635.97,91.10,8.72;10,84.76,649.17,87.68,8.72">Working Notes for the CLEF 2009 Workshop</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.56,662.37,454.14,8.72;10,84.76,675.33,367.64,8.72" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="10,195.25,662.37,329.46,8.72;10,84.76,675.33,90.58,8.72">Modelling and Learning Multilingual Inflectional Mor-phology in a Minimally Supervised Framework</title>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Wicentowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<pubPlace>Baltimore, Maryland</pubPlace>
		</imprint>
		<respStmt>
			<orgName>The Johns Hopkins University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
