<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,128.75,148.72,345.56,15.51;1,243.29,170.63,116.49,15.51">Unsupervised Morpheme Discovery with Allomorfessor</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,226.04,204.44,60.91,9.62"><forename type="first">Sami</forename><surname>Virpioja</surname></persName>
							<email>sami.virpioja@tkk.fi</email>
							<affiliation key="aff0">
								<orgName type="department">Adaptive Informatics Research Centre</orgName>
								<orgName type="institution">Helsinki University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,309.66,204.44,67.33,9.62"><forename type="first">Oskar</forename><surname>Kohonen</surname></persName>
							<email>oskar.kohonen@tkk.fi</email>
							<affiliation key="aff0">
								<orgName type="department">Adaptive Informatics Research Centre</orgName>
								<orgName type="institution">Helsinki University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,128.75,148.72,345.56,15.51;1,243.29,170.63,116.49,15.51">Unsupervised Morpheme Discovery with Allomorfessor</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">04BEAA7AFA967619B7CD653E625A399E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I.2 [Artificial Intelligence]: I.2.6 Learning; I.2.7 Natural Language Processing Algorithms</term>
					<term>Experimentation</term>
					<term>Languages Morphology</term>
					<term>Morphological Analysis</term>
					<term>Unsupervised Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe Allomorfessor, which extends the unsupervised morpheme segmentation method Morfessor to account for the linguistic phenomenon of allomorphy, where one morpheme has several different surface forms. The method discovers common base forms for allomorphs from an unannotated corpus by finding small modifications, called mutations, for them. Using Maximum a Posteriori estimation, the model is able to decide the amount and types of the mutations needed for the particular language. The method is evaluated in Morpho Challenge 2009.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Morphological analysis is crucial to many modern natural language processing applications, especially when dealing with morphologically rich languages. The enormous number of inflected word forms may lead to severe problems with data sparsity and computational efficiency. There are several successful methods for unsupervised segmentation of word forms into smaller, morpheme-like units (see, e.g., <ref type="bibr" coords="1,162.35,598.41,10.52,9.62" target="#b6">[7,</ref><ref type="bibr" coords="1,177.22,598.41,7.76,9.62" target="#b3">4,</ref><ref type="bibr" coords="1,189.33,598.41,7.20,9.62" target="#b1">2]</ref>). However, the phenomenon of allomorphy limits the quality of morpheme analysis achievable by segmentation alone. Allomorphy is defined in linguistics as when an underlying morpheme-level unit has two or more morph-level surface realizations which only occur in a complementary distribution: only one of the different allomorphs of a given morpheme appear may appear in a certain morpho-and phonotactical context. For example, in Finnish, the singular genitive case is marked with a suffix n, e.g. auto (car)auton (car's). Many Finnish nouns undergo a stem change when producing the genitive: kenkä (shoe)kengän (shoe's), pappi (priest)papin (priest's), tapa (habit)tavan (habit's). A segmentation based approach models changed stems as distinct morphemes.</p><p>In Morpho Challenge 2008, we introduced an unsupervised model for morpheme segmentation and allomorphy learning <ref type="bibr" coords="1,222.12,717.97,14.61,9.62" target="#b9">[10]</ref>. In <ref type="bibr" coords="1,259.23,717.97,14.61,9.62" target="#b10">[11]</ref>, some modifications to the model (now referred to as Allomorfessor Alpha) were suggested. In this paper we describe and evaluate the modified Allomorfessor model (referred to as Allomorfessor Baseline). As indicated by the name, the model is an extension to the Morfessor Baseline model by Creutz and Lagus <ref type="bibr" coords="2,386.48,135.70,9.96,9.62" target="#b2">[3]</ref>.</p><p>There are two main problems in literature on the unsupervised learning of allomorphy: finding morphologically related words (e.g. <ref type="bibr" coords="2,248.37,159.61,15.51,9.62" target="#b12">[13,</ref><ref type="bibr" coords="2,267.37,159.61,7.20,9.62" target="#b0">1]</ref>), and learning a morphological analyzer (e.g. <ref type="bibr" coords="2,479.62,159.61,15.50,9.62" target="#b13">[14,</ref><ref type="bibr" coords="2,498.62,159.61,7.20,9.62" target="#b4">5]</ref>). We try to solve the latter, which is more complex and general-as morphologically related words can be determined from the analyses. In contrast to the work by Yarowsky and Wicentowski <ref type="bibr" coords="2,90.00,195.48,14.61,9.62" target="#b13">[14]</ref>, the framework based on Morfessor allows concatenative morphology, rather than only stemsuffix pairs. In the work by Dasgupta and Ng <ref type="bibr" coords="2,297.30,207.44,9.96,9.62" target="#b4">[5]</ref>, concatenative morphology is allowed to some extent, but the approach is not as general and cannot find, e.g., suffixes between stems. Another difference is related to what information sources are used for finding the allomorphs. In addition to the orthographic similarity, word frequencies <ref type="bibr" coords="2,300.09,243.30,15.50,9.62" target="#b13">[14]</ref> and word contexts <ref type="bibr" coords="2,402.33,243.30,15.45,9.62" target="#b12">[13,</ref><ref type="bibr" coords="2,421.10,243.30,7.76,9.62" target="#b0">1]</ref> have been applied. We currently use only orthographic similarity.</p><p>This paper proceeds as follows: Section 2 presents the framework of the model and the learning task, both based on Morfessor. Section 3 describes how Allomorfessor models allomorphy by including new operations, mutations, to the model. Section 4 defines the model probabilities needed by the Maximum a Posteriori estimation. Section 5 describes the applied learning algorithm for the model, and Section 6 how the model can be used to analyze new words. Section 7 includes the initial results for the Morpho Challenge 2009. Finally, Section 8 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model Framework</head><p>To define our framework for learning morphology, we start with a probabilistic generative model M for a text corpus. With Maximum a Posteriori (MAP) estimation, we try to select the model that is the most probable given the training corpus:</p><formula xml:id="formula_0" coords="2,164.60,424.73,348.40,17.58">M MAP = arg max M P (M|corpus) = arg max M P (M)P (corpus|M)<label>(1)</label></formula><p>P (M) is the Bayesian prior probability for the model and P (corpus|M) is the likelihood of the training corpus. Compared to Maximum Likelihood estimation, MAP provides a systematic way of balancing the model complexity and accuracy, and thus helps with the problem of overlearning (see, e.g., Chapter 3 in <ref type="bibr" coords="2,200.70,486.57,10.29,9.62" target="#b5">[6]</ref>). This MAP formulation can alternatively be formulated using a two-part coding approach of the Minimum Description Length (MDL) principle. Modeling a corpus with a morphological model is not straightforward. For example, the occurrences of the words in a corpus follow power law distributions (Zipf's law), any realistic model should abide by that phenomenon. Instead of using an explicit model for the corpus, as in, e.g., <ref type="bibr" coords="2,90.00,546.34,9.96,9.62" target="#b7">[8]</ref>, we separate word-level model M W and morpheme-level model M M , and estimate only the latter. Word-level model is assumed to be a constant given a word lexicon L W , which contains all the word forms in the corpus. In addition, we divide M M into two parts: morpheme lexicon L M and morpheme grammar G M . The former models word-internal syntax and the latter provides the morphemes that from which the words are constructed. The optimization task is thus:</p><formula xml:id="formula_1" coords="2,194.77,613.60,213.46,17.58">M MAP = arg max GM ,LM P (L W |G M , L M )P (G M )P (L M ).</formula><p>(</p><formula xml:id="formula_2" coords="2,504.51,613.94,8.48,9.62">)<label>2</label></formula><p>This is equivalent to the approach used in Morfessor <ref type="bibr" coords="2,332.57,640.93,9.96,9.62" target="#b3">[4]</ref>, but instead of modeling the original corpus, we are now modeling a lexicon of the words in the corpus. <ref type="foot" coords="2,378.09,651.44,3.97,6.72" target="#foot_0">1</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Modeling Allomorphy with Mutations</head><p>Our morpheme-level model is Morfessor Baseline extended with operations that can make minor modifications to the surface forms of the morphemes. These operations are called mutations.</p><p>In many cases, the mutations are empty, i.e., they do not affect the surface form. If all of the mutations are empty, the model is equivalent to Morfessor Baseline.</p><p>When designing the mutation model for allomorphy we strive to: (1) Make wrong analyses costly by favoring mutations close to the suffix. E.g., the edit distance between blue and glue is only one, but they are not allomorphs of the same morpheme. (2) Use mutation types general enough to allow statistical analysis. I.e., similar variations in different words should be modeled with the same mutation. The mutation type used in Allomorfessor is a special case of the standard edit distance. We allow only substitution and deletion operations, and make the mutation position independent. The affected position is found by matching to k:th instance of a target letter, that is scanned for starting from the end of the virtual prefix (or previous operation). Examples are shown in Table <ref type="table" coords="3,159.24,231.34,3.87,9.62" target="#tab_0">1</ref>.</p><p>To calculate the smallest mutation of this kind between two arbitrary strings we apply the dynamic programming based algorithm for minimum edit distance (see, e.g., <ref type="bibr" coords="3,429.74,255.25,14.73,9.62" target="#b11">[12]</ref>), which can be modified to return also the edit operations needed. We want the optimal sequence of operations not containing insertions, so we set the cost of insertions to be larger than what the other operations may yield for the given string lengths. In this way, we can always find sequences of operations not containing insertions, if such sequences exist, by discarding candidates with too high costs. It is trivial to transform the edit operations into the Allomorfessor mutation format. To verify the suitability of the approach, we examined how well this kind of mutations are able to find the allomorphic variations in linguistic gold standard segmentations. The tests were performed on English, Finnish and Turkish, based on the gold standards used in Morpho Challenge. <ref type="foot" coords="3,508.51,513.51,3.97,6.72" target="#foot_1">2</ref>Statistics were calculated separately for a word lexicon and for a corpus, where the common words had more weight. The results are in Table <ref type="table" coords="3,282.39,538.88,3.87,9.62" target="#tab_1">2</ref>. The first column shows the number of morphs in the data. The second column shows how many of the morphs have allomorphs. The third column shows how many of the allomorphs can be constructed with mutations. We applied similar restrictions to those that were in our model; in practice, variations in affixes and other short morphemes were excluded from the search. Mutations provide reasonable good coverage for English and Finnish. E.g., for English, we can find at most 82% of the real allomorphs in the gold standard segmentation. The percentages for the corpora are lower, as affixes are more common than stems. For Turkish, where most of the allomorphy seems to be in affixes or other short morphemes, only 2% of the cases with allomorphic variants in a corpus can be found using mutations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model Probabilities</head><p>Next, we give a formal description of the probabilities in Equation <ref type="formula" coords="3,376.29,689.29,4.98,9.62" target="#formula_2">2</ref>for the Allomorfessor Baseline model. Again, the formulation follows the work by Creutz and Lagus <ref type="bibr" coords="3,392.85,701.24,9.96,9.62" target="#b3">[4]</ref>, especially the Morfessor Baseline model. </p><formula xml:id="formula_3" coords="4,204.30,268.10,308.70,31.99">P (L W |G M , L M ) = MW j=1 nj k=1 P (µ jk )P (δ jk |µ jk ),<label>(3)</label></formula><p>where n j is the number of morphs in word j. The probabilities of the morphs and the (conditional) probabilities of the mutations are estimated from the observed frequencies. I.e., if there is 10000 morph tokens in the word lexicon, and µ jk occurs 200 times, its probablity will be P (µ jk ) = 200/10000 = 0.02. The probability of the morph lexicon L M is based on the properties of the morphs:</p><formula xml:id="formula_4" coords="4,153.98,372.97,359.02,10.71">P (L M ) = P (size(L M ) = M )P (properties(µ 1 ) . . . properties(µ M ))M !<label>(4)</label></formula><p>If a non-informative prior is used for the probability of the lexicon size M , its effect is minimal and it can be neglected. The factor M ! is explained by the fact that there are M ! possible orderings of M items, and the lexicon is the same regardless of the order in which the morphs are discovered. The properties of the morphs are divided into two parts, usage and form. The usage includes properties of the morph itself and the properties of its context. Here we include only the frequency distribution of the morphs. For the probability of the distribution, we use a non-informative, implicit frequency prior</p><formula xml:id="formula_5" coords="4,148.79,481.65,359.97,23.54">P (usage(µ 1 ) . . . usage(µ M )) = P (freq(µ 1 ) . . . freq(µ M )) = 1/ N -1 M -1 , (<label>5</label></formula><formula xml:id="formula_6" coords="4,508.76,488.73,4.24,9.62">)</formula><p>where N is the sum of the counts of the morphs. The form of a morph is its representation in the model. Forms of the morphs are assumed to be independent. They are represented by a string of characters c ij :</p><formula xml:id="formula_7" coords="4,217.10,556.61,295.90,31.53">P (form(µ i )) = P (len(µ i )) len(µi) j=1 P (c ij ),<label>(6)</label></formula><p>where c ij is the jth character of the morph. The lengths of the morphs are modeled explicitly using an appropriate probability distribution, such as an exponential (geometric) or a gamma distribution.</p><p>Grammar G M of the model contains the set of mutations ∆. Similarly to the lexicons,</p><formula xml:id="formula_8" coords="4,153.49,651.62,359.51,11.04">P (G M ) = P (size(∆) = M δ )P (properties(δ 1 ) . . . properties(δ M δ ))M δ !,<label>(7)</label></formula><p>and properties can be divided into usage and form. Usage features include the frequencies of the mutations and their co-occurrences with the suffix morphs (needed in Equation <ref type="formula" coords="4,445.04,683.04,3.87,9.62" target="#formula_3">3</ref>). We apply a condition that each morph has to have at least one co-occurrence with an empty mutation ǫ. In consequence, the count of the empty mutation n ǫ is at most N (number of morph tokens) and at least M (number of morph types). Applying the the uniform distribution,</p><formula xml:id="formula_9" coords="4,245.72,735.99,267.28,23.20">P (freq(ǫ)) = 1 N -M + 1 .<label>(8)</label></formula><p>The other M δ -1 mutation types have Nn ǫ occurrences in total, as there are as many mutation tokens as there are morph tokens in the data. We apply the same non-informative prior as in Equation <ref type="formula" coords="5,133.53,135.70,3.87,9.62" target="#formula_5">5</ref>. Finally, we determine the probability of the co-occurrences of mutations and suffix morphs. For each non-empty mutation δ we divide its occurrences with the M possible morphs. There are freq(δ)+M -1</p><formula xml:id="formula_10" coords="5,154.77,165.20,18.54,6.72">M -1</formula><p>possibilities, so a non-informative prior for the co-occurrences is</p><formula xml:id="formula_11" coords="5,191.59,181.74,317.16,28.07">P (co-freqs(∆, L M )) = δ∈∆\ǫ 1/ freq(δ) + M -1 M -1 . (<label>9</label></formula><formula xml:id="formula_12" coords="5,508.76,188.81,4.24,9.62">)</formula><p>Note that after the others are determined, the co-occurrences with the empty mutation are:</p><formula xml:id="formula_13" coords="5,207.80,243.87,300.76,21.34">co-freq(µ, ǫ) = freq(µ) - δ∈∆\ǫ co-freq(µ, δ). (<label>10</label></formula><formula xml:id="formula_14" coords="5,508.56,244.21,4.43,9.62">)</formula><p>The prior probability for the form of a mutation δ i with len(δ i ) operations is given by:</p><formula xml:id="formula_15" coords="5,174.61,298.64,338.38,31.54">P (form(δ i )) = P (len(δ i )) len(δi) j=1 P (k ij )P (op ij )<label>(11)</label></formula><formula xml:id="formula_16" coords="5,192.80,334.54,315.77,26.28">P (op ij ) = P (del) 1 Σ if op ij is a deletion P (sub) 1 Σ 2 if op ij is a substitution (<label>12</label></formula><formula xml:id="formula_17" coords="5,508.56,342.59,4.43,9.62">)</formula><p>For the weights we use P (del) = P (sub) = 0.5, Σ is the alphabet size, and k ij tells which instance of the target letter of the operation op ij is matched. P (len(δ i )) and P (k ij ) can be taken from any suitable prior distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Algorithm for Model Learning</head><p>The model is learned by iteratively improving the model posterior P (M|corpus), processing one word at a time and selecting the analysis of that word that maximizes the probability, as shown in Algorithm 1. In the algorithm, A w is a list and we use + to denote the append operation. The algorithm considers analyzing the word w (1) without splits, (2) with all possible splits of w and an empty mutation, and (3) with all possible splits and a base form similar to the virtual prefix and the required mutation. The cases (1) and ( <ref type="formula" coords="5,299.76,508.64,4.24,9.62" target="#formula_2">2</ref>) are the same as in Morfessor Baseline and (3) is our extension, with details shown in Algorithm 2.</p><p>Since each word has 2 (len(w)-1) possible analyses without considering mutations, we search greedily for the best split at any time, reducing the search space to O(len(w) 2 ). When considering mutations, any word w could potentially be the base form for any other word w * . Thus, a naive algorithm would have time complexity O(N 2 ), which is unfeasible for large datasets. Therefore, we constrain the candidates in heuristic ways, such as limiting the number of analyses to K per morph and iteration, as can be seen in Algorithm 2. Since finding the baseforms can be done as a range search, it requires O(K log(N )) time, and thus the time complexity for the whole learning algorithm is O(N K log(N )).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 The learning algorithm</head><p>while P (M | corpus) increases do for w ∈ L W in random order do optimize(w,len(w)) end while function optimize(w,n)</p><p>A w ← w + (w 1..i , w (i+1)..n ) : i ∈ 1, ..., n -1 + mutated analyses(w, n) Apply the analysis a * w of the first K elements of A w that maximizes P (M | corpus) if a * w involved a split then optimize(w 1..i , i); optimize(w (i+1)..n , ni)</p><formula xml:id="formula_18" coords="6,90.00,111.75,264.49,60.68">Algorithm 2 mutated analyses(w, n) for i ∈ 1, ..., n -1 do if n &gt;= 4 ∧ len(w (i+1)..n ) &lt;= 5 ∧ w (i+1)..n ∈ L M then if n &gt; 6 then difflen ← 4 else difflen ← 3 baseforms ← {v ∈ L W : v 1..(n-difflen) = w 1..(n-difflen) }</formula><p>Calculate mutations δ j between each baseforms j and w (i+1)..n A w ← A w + (v j , w (i+1)..n , δ j ) : v j ∈ baseforms end if end for return A w sorted by i and descending len(v j )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Algorithm for Analyzing New Data</head><p>After the model has been trained, it can be used to analyze words with a variant of the Viterbi algorithm, which is a dynamic programming algorithm that finds the most probable state sequences for Hidden Markov models <ref type="bibr" coords="6,212.59,304.40,9.96,9.62" target="#b8">[9]</ref>. In our case, the observation is the sequence of |W | letters that form the word w, and the hidden states are the morphemes of the word. We need a grid s of length |W | to fill with the best probability values α(s i ) and paths. Without mutations, the model is 0th order Markov model, and the grid is a one dimensional table. The grid position s i indicates that the first i letters are observed. At each time step, we proceed with one letter and insert the probability α(s i ) = max j α(s j )P (µ ji ) and path indicator ψ(s i ) = arg max j α(s j )P (µ ji ) to the grid. We can come to s i from any of the positions s j between s 1 and s i-1 : the letters between j and i form the next morpheme µ ij . The time complexity is of the algorithm is thus O(|W | 2 ).</p><p>The mutations make things a bit more complicated. As they are conditioned on the suffixes, it is easier to run the algorithm from right to left. The grid has to be two dimensional: for each s i there can be several states (morphemes) with their own costs and paths. The rule for updating the grid value for s i is</p><formula xml:id="formula_19" coords="6,166.92,455.90,341.64,16.10">α(s i , μij ) = max j∈[i+1,|W |] max µ∈sj max δ∈∆ α(s j , µ)P (δ|µ)P (μ ij ) , (<label>13</label></formula><formula xml:id="formula_20" coords="6,508.56,456.24,4.43,9.62">)</formula><p>where μij is a morpheme that produces the letters between i and j when modified by the mutation δ. Only those mutations that are observed before µ need to be tested, otherwise P (δ|µ) = 0. For the morphemes that are not observed before, we use an approximate cost of adding them into the lexicon. The worst case time complexity for the algorithm is O(M M δ |W | 2 ). In practice, however, the number of morphemes and mutations tested in each position is quite limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments and Evaluation</head><p>For Morpho Challenge 2009 Competitions 1 and 2 we trained the model with the Competition 1 data, where all words occurring only once were filtered out 3 , with the exception of Arabic data sets, where the number of words was very low to start with. After training the model, we analyzed all the words in both data sets with the Viterbi algorithm (Section 6). For Competition 3, we used the Europarl data set for training, without any filtering. After training the model, the final analysis was calculated with the Viterbi algorithm. The following parameter settings were used: Morpheme length distribution in Equation <ref type="formula" coords="6,276.77,656.20,4.98,9.62" target="#formula_7">6</ref>was geometric with parameter p = M W /(M W + M c ), where M W is the number of words and M c the number of characters in the training corpus. The number of candidates considered for each virtual morph was K = 20. For the mutation lengths and k ij in Equation <ref type="formula" coords="6,182.24,692.06,8.48,9.62" target="#formula_15">11</ref>, we used gamma distribution with scale and shape parameters equal to one, preferring short mutations. In Table <ref type="table" coords="6,144.08,715.97,3.87,9.62" target="#tab_2">3</ref>, the performance of the Allomorfessor Baseline (the current algorithm) is compared to Allomorfessor Alpha (the algorithm presented in Challenge 2008 <ref type="bibr" coords="6,388.75,727.93,15.50,9.62" target="#b9">[10]</ref>) and Morfessor Baseline [3] in the Competition 1 of Morpho Challenge. The improvement over the previous algorithm is remarkable. As indicated by the improved recall measures, the algorithm no longer undersegments. This can also be seen in Figure <ref type="figure" coords="7,224.58,453.35,3.87,9.62" target="#fig_0">1</ref>, where the average number of morphemes per word form is shown for the three algorithms. Compared to Morfessor, the results are roughly at the same level. For English, Allomorfessor has both higher recall and higher precision. For all the other tasks, one is higher and the other is lower. Note that whenever the recall is higher, also the F-measure is higher, as improving the lower measure (in this case, recall) has more effect on the geometric mean of the measures. Figure <ref type="figure" coords="7,508.03,513.13,4.98,9.62" target="#fig_0">1</ref> shows that on average, Morfessor always segments word forms to smaller parts. This usually leads to a higher recall. However, for English, Allomorfessor obtains higher recall while segmenting less than Morfessor, which implies that the majority of the common base forms extracted by Allomorfessor are correct. Also, Allomorfessor achieved the winning F-measure for English in Morpho Challenge 2009.</p><p>In Competition 2, the algorithms were applied in an information retrieval system for English, Finnish and German. The results for Allomorfessor and Morfessor Baseline, shown in Table <ref type="table" coords="7,488.95,596.81,3.87,9.62" target="#tab_3">4</ref>, are roughly on the same level. Notably, Allomorfessor is better for Finnish and Morfessor for English in contrast to Competition 1; rigorous error analysis would be needed to find an explanation. Overall, Allomorfessor performed reasonably well in this task, being second in English and Finnish and third in German.</p><p>The number of non-empty mutations found by the algorithm (in the final analysis of all the word forms) is shown in Table <ref type="table" coords="7,223.95,668.55,3.87,9.62" target="#tab_4">5</ref>. Generally, mutations are not used as much as linguistic analysis would prefer. One reason is that the model seems to favor storing frequent morphs directly, instead of deriving them using mutations. The method finds, e.g., the morph pretti instead of deriving it as pretty (y|i). Therefore mutations are mostly used for morphs that occur only in few different word forms. When comparing languages, the most striking figures are in the Arabic sets: If the vowels are excluded (as usual in Arabic script), the model finds no useful mutations. However, when the vowels are in the text, the model finds 70 mutations, more than for any other tested  language. This nicely demonstrates the method's ability to adapt to the particular languages and data sets. The fact that Arabic morphology is not concatenative, and thus does not fit well into the Morfessor framework, emphasizes the flexibility of the model. In Table <ref type="table" coords="8,147.18,465.35,3.87,9.62" target="#tab_5">6</ref>, the mutations found by the algorithm are shown for English and Finnish. As can be seen, a large part of the mutations correspond to linguistic analysis. The most common error, especially for Finnish, is having a derived form as the base form. This is because an unsupervised algorithm has trouble finding the correct base form. However, if the analysed morph is semantically related to the induced base form, such analyses can be useful in applications. Other errors include not finding the correct suffix, using a more complex mutation and suffix combination than necessary, and using a semantically unrelated base form. Mutations are also used commonly on misspelled word forms. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>We have described the Allomorfessor Baseline method for unsupervised morphological analysis. It attempts to find the morphemes of the input data by segmenting the words into morphs and finding modifications that can restore allomorphic variations in stems back to their base forms. In the Morpho Challenge 2009 evaluations, significant improvements were obtained over the previous version of the method. The results are now close to those of the Morfessor Baseline method. In comparison to the methods by the other participants, Allomorfessor performed especially well in the linguistic evaluation for English (the best result in the task), and in the information retrieval evaluation for English (second), Finnish (second) and German (third). </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="8,90.00,283.19,423.01,9.62;8,90.00,295.14,151.93,9.62"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The average amount of morphemes per word indicated by the algorithms. The error bars show the standard deviations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,133.15,345.80,336.72,129.31"><head>Table 1 :</head><label>1</label><figDesc>The allowed operations in mutations and some examples in Finnish.</figDesc><table coords="3,152.22,357.11,225.97,82.15"><row><cell>Operation</cell><cell cols="2">Notation Description</cell></row><row><cell>substitution</cell><cell>kx|y</cell><cell>Change k:th x to y</cell></row><row><cell>deletion</cell><cell>-kx</cell><cell>Remove k:th x</cell></row><row><cell cols="2">(k is omitted when k = 1)</cell><cell></cell></row><row><cell>Source</cell><cell cols="2">Mutation Target</cell></row><row><cell>kenkä (shoe)</cell><cell>(k|g)</cell><cell></cell></row></table><note coords="3,295.26,429.63,140.53,9.62;3,152.22,441.59,55.20,9.62;3,249.31,441.81,26.15,9.20;3,295.26,441.59,121.21,9.62;3,152.22,453.54,59.99,9.62;3,241.46,453.54,209.34,9.62;3,152.22,465.49,77.29,9.62;3,246.69,465.71,31.38,9.20;3,295.26,465.49,83.98,9.62"><p>kengä (e.g. kengä+ssä, in shoe) tanko (pole) (k|g) tango (e.g. tango+t, poles) ranta (shore) (-a t|n) rann (e.g. rann+oi+lla, on shores) ihminen (human) (2n|s) ihmisen (human's)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,90.00,118.71,423.01,148.71"><head>Table 2 :</head><label>2</label><figDesc>The portion of morphemes with allomorphs and how many of the allomorphic variations can be modeled with mutations for English, Finnish and Turkish lexicon and corpus.First, every word form w j in the word lexicon is represented by a sequence of morphs µ jk and mutations δ jk</figDesc><table coords="4,135.98,141.97,331.02,81.75"><row><cell></cell><cell>Morphemes</cell><cell cols="2">Allomorphs</cell><cell cols="2">Mutation found</cell></row><row><cell>English lexicon</cell><cell>21 173</cell><cell cols="2">10 858 (51.3%)</cell><cell cols="2">8 912 (82.1%)</cell></row><row><cell>Finnish lexicon</cell><cell>68 743</cell><cell cols="2">56 653 (82.4%)</cell><cell cols="2">36 210 (63.9%)</cell></row><row><cell>Turkish lexicon</cell><cell>23 376</cell><cell>646</cell><cell>(2.8%)</cell><cell cols="2">102 (15.8%)</cell></row><row><cell>English corpus</cell><cell cols="5">76 968 382 42 282 837 (54.9%) 14 706 543 (34.8%)</cell></row><row><cell>Finnish corpus</cell><cell cols="5">73 512 023 61 583 251 (83.8%) 18 751 022 (30.5%)</cell></row><row><cell>Turkish corpus</cell><cell cols="3">23 288 821 11 978 142 (51.4%)</cell><cell>225 708</cell><cell>(1.9%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,90.00,119.26,423.01,286.33"><head>Table 3 :</head><label>3</label><figDesc>Results from the Morpho Challenge linguistic evaluation (Competition 1) for Arabic (nv = non-vowelized, vow = vowelized), English, Finnish, German and Turkish. Allomorfessor Baseline and Morfessor Baseline are trained with the same data sets. Allomorfessor Alpha has no implementation for Viterbi segmentation, so it is trained on the full data sets.</figDesc><table coords="7,142.79,166.43,317.42,239.16"><row><cell>Language</cell><cell>Measure</cell><cell cols="3">Allomorfessor Allomorfessor Morfessor</cell></row><row><cell></cell><cell></cell><cell>Alpha</cell><cell>Baseline</cell><cell>Baseline</cell></row><row><cell>Arabic (nv)</cell><cell>precision</cell><cell>-</cell><cell>91.62%</cell><cell>91.77%</cell></row><row><cell></cell><cell>recall</cell><cell>-</cell><cell>6.59%</cell><cell>6.44%</cell></row><row><cell></cell><cell>F-measure</cell><cell>-</cell><cell>12.30%</cell><cell>12.03%</cell></row><row><cell cols="2">Arabic (vow) precision</cell><cell>-</cell><cell>88.28%</cell><cell>86.87%</cell></row><row><cell></cell><cell>recall</cell><cell>-</cell><cell>4.37%</cell><cell>4.90%</cell></row><row><cell></cell><cell>F-measure</cell><cell>-</cell><cell>8.33%</cell><cell>9.28%</cell></row><row><cell>English</cell><cell>precision</cell><cell>83.31%</cell><cell>68.98%</cell><cell>68.43%</cell></row><row><cell></cell><cell>recall</cell><cell>15.84%</cell><cell>56.82%</cell><cell>56.19%</cell></row><row><cell></cell><cell>F-measure</cell><cell>26.61%</cell><cell>62.31%</cell><cell>61.71%</cell></row><row><cell>Finnish</cell><cell>precision</cell><cell>92.64%</cell><cell>86.51%</cell><cell>86.07%</cell></row><row><cell></cell><cell>recall</cell><cell>8.65%</cell><cell>19.96%</cell><cell>20.33%</cell></row><row><cell></cell><cell>F-measure</cell><cell>15.83%</cell><cell>32.44%</cell><cell>32.88%</cell></row><row><cell>German</cell><cell>precision</cell><cell>87.82%</cell><cell>77.78%</cell><cell>76.47%</cell></row><row><cell></cell><cell>recall</cell><cell>8.54%</cell><cell>28.83%</cell><cell>30.49%</cell></row><row><cell></cell><cell>F-measure</cell><cell>15.57%</cell><cell>42.07%</cell><cell>43.60%</cell></row><row><cell>Turkish</cell><cell>precision</cell><cell>93.16%</cell><cell>85.89%</cell><cell>85.43%</cell></row><row><cell></cell><cell>recall</cell><cell>9.56%</cell><cell>19.53%</cell><cell>20.03%</cell></row><row><cell></cell><cell>F-measure</cell><cell>17.35%</cell><cell>31.82%</cell><cell>32.45%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,90.00,324.53,423.01,81.10"><head>Table 4 :</head><label>4</label><figDesc>Results from the Morpho Challenge information retrieval evaluation (Competition 2). Allomorfessor Baseline versus Morfessor Baseline, trained with the same data sets.</figDesc><table coords="8,215.96,347.79,171.07,57.84"><row><cell>Language</cell><cell cols="2">Average Precision (%)</cell></row><row><cell></cell><cell cols="2">Allomorfessor Morfessor</cell></row><row><cell>English</cell><cell>0.3852</cell><cell>0.3873</cell></row><row><cell>Finnish</cell><cell>0.4601</cell><cell>0.4475</cell></row><row><cell>German</cell><cell>0.4388</cell><cell>0.4728</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,90.00,579.80,422.99,57.20"><head>Table 5 :</head><label>5</label><figDesc>The number of non-empty mutations found by Allomorfessor. Mutation usage is the number of non-empty mutation tokens divided by the number of morph tokens.</figDesc><table coords="8,111.36,603.07,380.30,33.93"><row><cell>Language</cell><cell cols="6">Arabic (nv) Arabic (vow) English Finnish German Turkish</cell></row><row><cell>Mutation types</cell><cell>0</cell><cell>69</cell><cell>15</cell><cell>66</cell><cell>26</cell><cell>55</cell></row><row><cell>Mutation usage</cell><cell>0.0%</cell><cell>4.61%</cell><cell>0.18%</cell><cell>0.44%</cell><cell>0.17%</cell><cell>0.12%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="10,95.98,122.00,448.74,625.84"><head>Table 6 :</head><label>6</label><figDesc>Mutation types with example usage for English and Finnish.</figDesc><table coords="10,95.98,133.08,117.27,7.16"><row><cell>Mutation</cell><cell>Count</cell><cell>Examples</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,105.25,737.47,407.69,7.24;2,90.00,746.93,174.86,7.24"><p>This has been recommended to be done also with Morfessor by setting all the word counts to one. Otherwise, frequent word forms are often undersegmented.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,105.25,730.35,254.28,7.43"><p>See http://www.cis.hut.fi/morphochallenge2009/datasets.shtml.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,110.48,190.07,402.52,9.62;9,110.48,202.02,402.53,9.62;9,110.48,213.98,402.54,9.62;9,110.48,225.93,75.97,9.62" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,357.12,190.07,155.88,9.62;9,110.48,202.02,302.60,9.62">Unsupervised discovery of morphologically related words based on orthographic and semantic similarity</title>
		<author>
			<persName coords=""><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Matiasek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Harald</forename><surname>Trost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,434.69,202.02,78.31,9.62;9,110.48,213.98,266.35,9.62">Proceedings of the ACL-02 workshop on Morphological and phonological learning</title>
		<meeting>the ACL-02 workshop on Morphological and phonological learning<address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="48" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,244.92,402.52,9.62;9,110.48,256.88,402.53,9.62;9,110.48,268.83,402.53,9.62;9,110.48,280.79,47.90,9.62" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,200.89,244.92,275.23,9.62">Simple morpheme labelling in unsupervised morpheme analysis</title>
		<author>
			<persName coords=""><forename type="first">Delphine</forename><surname>Bernhard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,497.45,244.92,15.55,9.62;9,110.48,256.88,396.65,9.62">Advances in Multilingual and Multimodal Information Retrieval, 8th Workshop of the CLEF</title>
		<title level="s" coord="9,178.67,268.83,151.19,9.62">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin / Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">5152</biblScope>
			<biblScope unit="page" from="873" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,299.79,402.53,9.62;9,110.48,311.74,402.52,9.62;9,110.48,323.69,137.21,9.62" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,265.28,299.79,163.71,9.62">Unsupervised discovery of morphemes</title>
		<author>
			<persName coords=""><forename type="first">Mathias</forename><surname>Creutz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krista</forename><surname>Lagus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,451.52,299.79,61.49,9.62;9,110.48,311.74,298.43,9.62">Proceedings of the Workshop on Morphological and Phonological Learning of ACL&apos;02</title>
		<meeting>the Workshop on Morphological and Phonological Learning of ACL&apos;02<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="21" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,342.69,402.54,9.62;9,110.48,354.64,402.54,9.62;9,110.48,366.59,22.68,9.62" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,271.73,342.69,241.29,9.62;9,110.48,354.64,88.09,9.62">Unsupervised models for morpheme segmentation and morphology learning</title>
		<author>
			<persName coords=""><forename type="first">Mathias</forename><surname>Creutz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krista</forename><surname>Lagus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,207.29,354.64,239.08,9.62">ACM Transactions on Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2007-01">January 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,385.59,402.51,9.62;9,110.48,397.54,402.53,9.62;9,110.48,409.50,52.87,9.62" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,252.54,385.59,260.45,9.62;9,110.48,397.54,42.07,9.62">High-performance, language-independent morphological segmentation</title>
		<author>
			<persName coords=""><forename type="first">Sajib</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,185.10,397.54,327.91,9.62;9,110.48,409.50,21.49,9.62">the annual conference of the North American Chapter of the ACL (NAACL-HLT)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,428.50,355.90,9.62" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="9,204.13,428.50,151.07,9.62">Unsupervised Language Acquisition</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Carl</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>De Marcken</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
		<respStmt>
			<orgName>MIT</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="9,110.48,447.49,402.53,9.62;9,110.48,459.44,170.44,9.62" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,187.59,447.49,273.73,9.62">Unsupervised learning of the morphology of a natural language</title>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Goldsmith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,470.22,447.49,42.78,9.62;9,110.48,459.44,73.86,9.62">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="153" to="189" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,478.43,402.52,9.62;9,110.48,490.39,402.53,9.62;9,110.48,502.34,138.02,9.62" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,373.47,478.43,139.54,9.62;9,110.48,490.39,187.01,9.62">Interpolating between types and tokens by estimating power-law generators</title>
		<author>
			<persName coords=""><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,321.59,490.39,191.42,9.62;9,110.48,502.34,67.79,9.62">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,521.33,402.54,9.62;9,110.48,533.29,22.68,9.62" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,204.89,521.33,94.56,9.62">The Viterbi algorithm</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">David</forename><surname>Forney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jr</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,308.02,521.33,102.49,9.62">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1973-03">March 1973</date>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="268" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,552.28,402.54,9.62;9,110.48,564.23,402.53,9.62" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,348.05,552.28,164.98,9.62;9,110.48,564.23,80.79,9.62">Allomorfessor: Towards unsupervised morpheme analysis</title>
		<author>
			<persName coords=""><forename type="first">Sami</forename><surname>Oskar Kohonen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mikaela</forename><surname>Virpioja</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Klami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,210.28,564.23,190.15,9.62">Working notes for the CLEF 2008 Workshop</title>
		<meeting><address><addrLine>Aarhus, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,583.23,402.54,9.62;9,110.48,595.18,402.52,9.62;9,110.48,607.14,402.53,9.62;9,110.48,619.10,74.19,9.62" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,348.05,583.23,164.98,9.62;9,110.48,595.18,83.18,9.62">Allomorfessor: Towards unsupervised morpheme analysis</title>
		<author>
			<persName coords=""><forename type="first">Sami</forename><surname>Oskar Kohonen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mikaela</forename><surname>Virpioja</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Klami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,221.16,595.18,291.83,9.62;9,110.48,607.14,158.89,9.62">Evaluating Systems for Multilingual and Multimodal Information Access -9th Workshop of the CLEF</title>
		<title level="s" coord="9,279.16,607.14,155.25,9.62">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct coords="9,110.48,638.09,402.52,9.62;9,110.48,650.04,79.14,9.62" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,196.23,638.09,208.40,9.62">A guided tour to approximate string matching</title>
		<author>
			<persName coords=""><forename type="first">Gonzalo</forename><surname>Navarro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,417.58,638.09,91.16,9.62">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="88" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,669.04,402.52,9.62;9,110.48,680.99,402.52,9.62;9,110.48,692.94,402.54,9.62;9,110.48,704.90,75.97,9.62" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,278.13,669.04,234.87,9.62;9,110.48,680.99,74.02,9.62">Knowledge-free induction of morphology using latent semantic analysis</title>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Schone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,205.28,680.99,307.71,9.62;9,110.48,692.94,261.03,9.62">Proceedings of the 2nd workshop on Learning language in logic and the 4th conference on Computational natural language learning</title>
		<meeting>the 2nd workshop on Learning language in logic and the 4th conference on Computational natural language learning<address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="67" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,723.89,402.55,9.62;9,110.48,735.85,402.51,9.62" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,304.17,723.89,208.85,9.62;9,110.48,735.85,94.22,9.62">Minimally supervised morphological analysis by multimodal alignment</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Wicentowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,226.09,735.85,186.98,9.62">Proceedings of the 38th Meeting of the ACL</title>
		<meeting>the 38th Meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="207" to="216" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
