<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,103.59,148.78,395.82,16.18;1,220.23,170.70,162.54,15.22">Promodes: A probabilistic generative model for word decomposition</title>
				<funder ref="#_fSKHmyK">
					<orgName type="full">EPSRC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,198.82,204.60,77.85,8.80"><forename type="first">Sebastian</forename><surname>Spiegler</surname></persName>
							<email>spiegler@cs.bris.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="laboratory">Machine Learning Group</orgName>
								<orgName type="institution">University of Bristol</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,284.12,204.60,61.85,8.80"><forename type="first">Bruno</forename><surname>Golénia</surname></persName>
							<email>goleniab@cs.bris.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="laboratory">Machine Learning Group</orgName>
								<orgName type="institution">University of Bristol</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,353.80,204.60,50.38,8.80"><forename type="first">Peter</forename><surname>Flach</surname></persName>
							<email>flach@cs.bris.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="laboratory">Machine Learning Group</orgName>
								<orgName type="institution">University of Bristol</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,103.59,148.78,395.82,16.18;1,220.23,170.70,162.54,15.22">Promodes: A probabilistic generative model for word decomposition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D2C8F79BD46C1C3895C53B649400E208</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I.2 [Artificial Intelligence]: I.2.6 Learning</term>
					<term>Parameter learning</term>
					<term>I.2.7 Natural Language Processing</term>
					<term>Language models word morphology, word decomposition, probabilistic generative model, expectation maximization, committee of unsupervised learners</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For the Morpho Challenge 2009 we present an algorithm for unsupervised morphological analysis called Promodes 1 which is based on a probabilistic generative model. The model considers segment boundaries as hidden variables and includes probabilities for letter transitions within segments. Promodes purely concentrates on segmenting words whereas its labeling method is simplistic. Morpheme labels are the segments themselves. The algorithm can be employed in different degrees of supervision. For the challenge, however, we demonstrate three unsupervised versions. The first one uses a simple segmenting algorithm on a small subset of the data which is based on letter succession probabilities in substrings and then estimates the model parameters using a maximum likelihood approach. The second version estimates its parameters through expectation maximization. Independently of the parameter estimation, we utilized each model to decompose words from the original language data. A third method is a committee of unsupervised learners where each learner corresponds to the second version, however, with different initializations of the expectation maximization. The solution is then found by majority vote which decides whether to segment in a word position or not. In this paper, we describe the details of the probabilistic model, how parameters are estimated and how the most likely decomposition of an input word is found. We have tested Promodes on Arabic (vowelized and non-vowelized), English, Finnish, German and Turkish. All three methods achieved competitive results in the Morpho Challenge 2009.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of the Morpho Challenge is to advance algorithms for unsupervised morphological analysis. In general, morphological analysis is a subdiscipline of linguistics and can be defined as the study of the internal structure of words <ref type="bibr" coords="1,267.31,707.66,9.96,8.80" target="#b2">[3]</ref>. There are four tasks assigned to it: 1) decomposing words into morphemes, 2) building a morpheme dictionary, 3) defining morphosyntactical rules which state how morphemes can be combined to valid words and 4) defining morphophonological rules which specify phonological changes when morphemes are combined <ref type="bibr" coords="2,434.72,123.92,9.96,8.80" target="#b8">[9]</ref>. Unsupervised means absence of supervision in machine learning. Most generally, machine learning refers to a computational program which improves its results with increasing experience in terms of data and intervention of an external teacher <ref type="bibr" coords="2,243.45,159.78,14.61,8.80" target="#b12">[13]</ref>. If there is no intervention of a teacher and the data does not contain any information towards the task to be solved, the learning process is referred to as unsupervised. For the Morpho Challenge, the task is to perform unsupervised morpheme analysis for words contained in a word list using a generic algorithm without any further information. Test languages are Arabic, English, German, Finnish and Turkish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related work</head><p>Over the recent years, a number of algorithms for unsupervised morphological analysis have been produced. Goldsmith <ref type="bibr" coords="2,186.11,265.83,10.51,8.80" target="#b7">[8]</ref> presents a morphological analyzer called Linguistica which learns signatures from a word list. Signatures are sets of suffixes which are used with certain sets of stems. They are built by initially splitting words into stem and affix candidates and then by heuristically changing stem-affix boundaries until the description length of the signatures is minimized. A similar approach has been chosen by <ref type="bibr" coords="2,235.24,313.65,52.50,8.80">Monson [14]</ref> who developed Paramor, an algorithm which learns paradigmatic structures of morphology as sets of mutually substitutable morphological operations. Another frequently cited morphological analyzer is Morfessor developed by Creutz et al. <ref type="bibr" coords="2,490.74,337.56,10.51,8.80" target="#b3">[4,</ref><ref type="bibr" coords="2,505.26,337.56,7.74,8.80" target="#b4">5]</ref> who implemented a model family for unsupervised morphology induction. The two main methods are Morfessor baseline and Morfessor Categories-MAP. <ref type="foot" coords="2,330.73,359.92,3.97,6.16" target="#foot_1">2</ref> The first one is a recursive algorithm for morphological decomposition based on minimum description length (MDL) and the latter method is based on a probabilistic maximum a posteriori (MAP) framework and furthermore distinguishes between different categories of morphemes (prefixes, stems, suffixes). Linguistica, Paramor and Morfessor carry out morphological analysis in terms of word decomposition, learning a morpheme dictionary and finding morphosyntactical rules. Other approaches <ref type="bibr" coords="2,388.72,421.25,10.51,8.80" target="#b1">[2,</ref><ref type="bibr" coords="2,403.43,421.25,7.75,8.80" target="#b5">6]</ref> focus on word decomposition by analyzing words based on transition probabilities or letter successor variety which originates in Harris' approach <ref type="bibr" coords="2,223.27,445.16,14.61,8.80" target="#b10">[11]</ref>. Snover <ref type="bibr" coords="2,278.60,445.16,15.49,8.80" target="#b14">[15,</ref><ref type="bibr" coords="2,297.49,445.16,12.73,8.80" target="#b15">16]</ref> describes a generative model for unsupervised learning of morphology, however, it differs from ours in the fact that Snover searches, similar to Monson, for paradigms and we are interested in word decomposition based on the probability of having a boundary in a certain position and the resulting letter transition of morphemes.</p><p>The remainder of this paper is structured as follows. In Section 2 we will present our algorithm Promodes, its mathematical model, different ways of estimating its parameters and different methods for word decompositions. In Section 3 and 4 experiments are explained, results analysed and conclusions drawn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Algorithm</head><p>In this section we will explain our algorithm Promodes. Its name stands for probabilistic generative model for different degrees of supervision. It can be applied in an unsupervised manner where model parameters are estimated using expectation maximization (EM) <ref type="bibr" coords="2,406.16,607.51,10.51,8.80" target="#b6">[7]</ref> or (semi-) supervised by computing maximum likelihood estimates (MLE) from a pre-segmented training set. For the Morpho Challenge, we used a simple unsupervised segmenting algorithm to segment words in the training set and then estimated the model parameters by MLE. Independently of the parameter estimation, we applied the generative model to the original data and decomposed all words. Apart from the two different ways of estimating the model parameters, we also carried out experiments with a committee of unsupervised learners where we combined η different results by majority vote.</p><p>In Figure <ref type="figure" coords="2,148.00,691.20,3.87,8.80">1</ref>, an overview over the algorithm is given. The probabilistic generative model which will be introduced in Section 2.1 supplies us with an objective function which assigns a probability to a single word analysis. In Section 2.2 two ways of estimating parameters will be explained. In Section 2.3 we will show how we used a committee of unsupervised learners to decompose words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter estimation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Objective function</head><p>Search method In morphological analysis, we try to decompose words into morphemes. Therefore, the observables correspond to the original words and the hidden variables to their segmentation or boundaries. Knowing the parameters of the model we can find the best segmentation of a given word. However, we can also learn the parameters from a word list by using either maximum likelihood estimates or expectation maximization as described in Section 2.2.1 and 2.2.2. Subsequently, we will describe how the PGM has been constructed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Words and segmentations</head><p>The data we are dealing with is a list W of words w j with 1 ≤ j ≤ |W |. A word consists of n letters and has m = n -1 positions where a boundary can be inserted. We denote a segmentation of a word with b which describes a binary vector b 1 , . . . , b m with 1 ≤ i ≤ m. In position i a boundary value b i can be {0, 1} for putting a boundary or not.</p><p>Letter transition probability distribution In the Markovian spirit we describe a word by transitions from a letter x to a letter y within a morpheme where y is drawn from an alphabet A and x from A B = A ∪ B where B is a silent start symbol pointing to the first letter of the morpheme. By introducing such a silent start symbol it is guaranteed that different segmentations of a word have the same number of transitions.</p><formula xml:id="formula_0" coords="3,145.00,568.40,368.00,20.12">p x,y = P r(X i+1 = y|X i = x) with y∈A p x,y = 1 ∀x ∈ A B and 1 ≤ i ≤ m.<label>(1)</label></formula><p>Probability distribution over boundary and non-boundary positions A simple way of describing a segmentation is in a position-independent and identically distributed manner. However, assuming that boundaries are equally likely over the segmentation is a strong simplification.</p><p>For this reason we chose a position-dependent and non-identical distribution. Each position i is therefore assigned to a Bernoulli random variable Z i and the existence of a boundary corresponds to a single trial with positive outcome.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P r(Z</head><formula xml:id="formula_1" coords="3,155.40,686.16,357.60,9.71">i = 1|m) = p zi,m with P r(Z i = 0|m) + P r(Z i = 1|m) = 1 and 1 ≤ i ≤ m<label>(2)</label></formula><p>with</p><formula xml:id="formula_2" coords="3,113.34,708.14,100.73,9.65">Z i ∈ Z = [Z 1 , . . . , Z m ]</formula><p>for the entire segmentation. Both above distributions describe the parameters of our generative model which can be summarized as</p><formula xml:id="formula_3" coords="3,277.19,741.96,235.81,8.80">θ = {X, Z}<label>(3)</label></formula><p>with X being the probability distribution over transitions and Z the probability distribution over boundary and non-boundary positions in a segmentation.</p><p>Position-based perspective and probability of segmenting or not in position i Instead of looking for the best segmentation b jk given a word w j in an exponential search space with 1 ≤ k ≤ 2 m , we make a decision in each position i and therefore turn the search into a linear problem. The probability of putting a boundary in position i or not is then defined as</p><formula xml:id="formula_4" coords="4,259.66,207.59,253.34,9.71">P r(b i |m, θ) = p bi,m<label>(4)</label></formula><p>where p bi,m is the probability of having a boundary value b i = {0, 1} in position i given the length of the segmentation m. For later derivations we rewrite this equation as</p><formula xml:id="formula_5" coords="4,234.82,261.95,278.18,60.75">P r(b i |m, θ) = 1 r=0 p r,m µ r,b i (5) µ r,bi = 1, if b i = r, 0, otherwise<label>(6)</label></formula><p>where we iterate over possible boundary values r = {0, 1} and have a function µ r,i which eliminates all r's in the product which do not correspond to b i .</p><p>Probability of a letter transition in position i If the segmentation of the word is known we can assign a probability to its letter structure in terms of letter transitions (which are lengthindependent).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P r(w</head><formula xml:id="formula_6" coords="4,282.98,420.91,230.02,9.71">ji |b i , θ) = p x,y<label>(7)</label></formula><p>where in the jth word the letter x is in position i and y in i + 1 given the information whether there is a boundary or not in position i expressed by b i . For later derivations, we rewrite this equation such that we iterate over the alphabet using x and y , and eliminate all probabilities which do not correspond to the original x and y by using the function µ xy,x y .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P r(w</head><formula xml:id="formula_7" coords="4,183.95,500.95,329.05,22.13">ji |b i , θ) = y ∈A p x,y µ xy,x y x ∈ A B<label>(8)</label></formula><p>µ xy,x y = 1, if x = x and y = y in w j at ith position, 0, otherwise.</p><p>Finding the best segmentation of a word is done as follows.</p><formula xml:id="formula_9" coords="4,93.83,596.44,414.75,37.41">b max,i = 1, if P r(Z i = 1|m) • P r(X i+1 = y|X i = B) &gt; P r(Z i = 0|m) • P r(X i+1 = y|X i = x) 0, otherwise. (<label>10</label></formula><formula xml:id="formula_10" coords="4,99.01,625.06,413.99,25.65">) b max = b max,1 , . . . , b max,m<label>(11)</label></formula><p>If the product of the transition probability from the abstract start symbol B to letter i + 1 and the probability for segmenting in i is higher than product of the transition from letter i to i + 1 and not segmenting in i, segment, otherwise do not segment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Parameter estimation</head><p>Before applying the probabilistic model described in the previous section, its parameters have to be estimated. For the Morpho Challenge we present two ways of doing this, at first by maximum likelihood estimates and then by expectation maximization. A training set was prepared which constituted a subset of the original data. Approximately 100,000 word forms<ref type="foot" coords="5,431.50,122.36,3.97,6.16" target="#foot_2">3</ref> were selected for each language which did not contain special characters like apostrophes or hyphens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Maximum likelihood estimates</head><p>We segmented each training set using a heuristic similar to the successor variety <ref type="bibr" coords="5,440.37,179.62,15.49,8.80" target="#b10">[11]</ref> in a separate pre-processing step. All possible substrings of every word were collected in a forward trie <ref type="foot" coords="5,481.78,190.02,3.97,6.16" target="#foot_3">4</ref> along with statistical information as their frequency. A particular word was then analysed from the first letter onwards where the conditional probability P (l t+1 |l 1 , . . . , l t ) with l being any letter from the alphabet was evaluated. If the conditional probability dropped in t + 1, the word was segmented in position t and the evaluation process was reinitiated in position t + 1. Since this is a rather crude method, it tends to over-segment and solutions for similar words might vary a lot. From the segmentations we estimated the parameters for Promodes 1 using maximum likelihood estimates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Expectation maximization</head><p>Parameter estimation by expectation maximization (EM) <ref type="bibr" coords="5,343.71,307.06,10.51,8.80" target="#b6">[7]</ref> was used in Promodes 2. The EM algorithm iteratively alternates between two distinctive steps, the expectation or E-step and the maximization or M-step, until a convergence criterion is met. In the E-step the log-likelihood of the current estimates for the model parameters are computed. In the M-step the parameters are updated such that the log-likelihood is maximized. First, we will describe how the likelihood is calculated and then we will show how the re-estimation is done using partial derivatives. The Q function as the expected value of the log likelihood function is defined as follows:</p><formula xml:id="formula_11" coords="5,177.00,396.73,336.00,31.40">Q(θ, θ t ) = |W | j=1 mj i=1 1 r=0 P (b i = r|w ji , θ) log P (w ji , b i = r|θ t )<label>(12)</label></formula><formula xml:id="formula_12" coords="5,200.66,432.61,312.34,18.67">θ * t = arg max θt Q(θ, θ t )<label>(13)</label></formula><p>Our objective function L which we want to maximize during the M-step is built of the Q function from Equation 12 and includes constraints c 1 and c 2 and Lagrange multiplier λ 1 and λ 2 :</p><formula xml:id="formula_13" coords="5,143.09,489.18,369.91,30.48">c 1 = y ∈A p x ,y = 1 with x ∈ A B , and c 2 = 1 r =0 p z=r ,m = 1<label>(14)</label></formula><formula xml:id="formula_14" coords="5,145.00,541.15,368.01,87.91">L = Q(θ, θ t ) -λ 1 (c 1 -1) -λ 2 (c 2 -1) = |W | j=1 mj i=1 1 r=0 P (b i = r|w ji , θ) •   log 1 r=0 p r,m µ r,b i + log y ∈A p x,y µ xy,x y   -λ 1     y ∈A p x,y   -1   -λ 2 1 r =0 p z=r ,m -1<label>(15)</label></formula><p>In order to re-estimate transition probabilities we use the partial derivative ∂L ∂px,y and get the following result:</p><formula xml:id="formula_15" coords="5,152.50,672.29,360.50,35.22">p x,y = |W | j=1 mj i=1 1 r=0 P (b i = r|w ji , θ) y ∈A µ xy,x y y ∈A |W | j=1 mj i=1 1 r=0 P (b i = r|w ji , θ) y ∈A µ x y ,x y<label>(16)</label></formula><p>For re-estimating the probabilities for segmenting in position i the partial derivative ∂L ∂pz i ,m has to be calculated:</p><formula xml:id="formula_16" coords="6,160.22,141.30,352.78,35.22">p zi,m = |W | j=1 mj i=1 1 r=0 P (b i = r|w ji , θ) 1 r =0 µ r ,zi 1 r =0 |W | j=1 mj i=1 1 r=0 P (b i = r|w ji , θ) 1 r =0 µ r ,r<label>(17)</label></formula><p>Although both derivatives are bulky, they have an intuitive interpretation. In Equation <ref type="formula" coords="6,487.49,185.95,9.96,8.80" target="#formula_15">16</ref>we first count the occurrence of a certain letter transition from x to y multiplied by the probability P (b ir |w ji , θ t ) and divide it by the weighted sum of all transitions from x. In Equation 17 we calculate the weighted sum for putting a boundary in position i of words with length m j and divide it by the weighted sum of all boundaries and non-boundaries in position i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Committee of unsupervised learners</head><p>Since different initializations of the above described EM algorithm may converge in different local optima, the actual models might give slightly different analyses for a single word. Therefore it seems natural to average results from a set of different initializations and combine them to a single solution. Our third method for word decomposition Promodes Committee does exactly this by using a committee of unsupervised learners. This approach is similar to Atwell's <ref type="bibr" coords="6,437.41,327.23,10.51,8.80" target="#b0">[1]</ref> in the Morpho Challenge 2006. In general, a committee is a group of individuals which have been appointed to perform a certain task or to arrive at a certain decision. In machine learning, a committee can combine results from different algorithms or in our case different initializations. Each member of the committee can vote for a certain partial or complete solution. The weight of each vote can be either uniform or non-uniform, e.g. based on the algorithm's performance or the confidence in the algorithm. The approach we are presenting is completely unsupervised and purely based on majority vote for putting a boundary in a certain position. Given η analyses for a single word w j in position i we introduce score j,i as</p><formula xml:id="formula_17" coords="6,185.42,439.11,327.58,31.02">score j,i = η h=1 π h,j,i<label>(18)</label></formula><formula xml:id="formula_18" coords="6,195.94,473.96,43.80,39.25">π h,j,i =     </formula><p>1, if analysis h contains a boundary in position i for word w j , -1, otherwise. and put a boundary at the ith position of word w j if score j,i &gt; 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental results</head><p>For the Morpho Challenge 2009 we applied three methods, Promodes 1, Promodes 2 and Promodes Committee, to five languages. Two of these languages, Turkish and Finnish, are agglutinating languages where words are composed mostly by joining morphemes, however, in certain situations morphemes also undergo phonological changes during the word building process. One of these phenomena is called vowel harmony which is defined as long-distance phonological interactions constraining what kind of vowels can co-occur in a word. The other three languages, namely German, English and Arabic, are rather fusional languages where morphemes are tightly fused together and are more difficult to separate. The Promodes methods are intended for agglutinating languages. They decompose words into their morphemes. Morphosyntactic rules are implicitly stored as statistics in terms of probabilities for segmenting in certain word positions and resulting letter transitions within morphemes. There is no further grammatical analysis like building signatures or paradigms. Morpheme labels are either the morphemes themselves or simple labels consisting of morpheme+index number which we generated during a post-processing step. The results across all language are listed in Figure <ref type="figure" coords="6,320.17,733.63,4.98,8.80">2</ref> and in Table <ref type="table" coords="6,389.93,733.63,4.98,8.80" target="#tab_1">1</ref> where the best results for precision, recall and F-measure are written in bold. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">General setup of the experiments</head><p>For method Promodes 1 (P1) we estimated the algorithm's parameters from a pre-segmented subset which was generated with a simple segmenting algorithm based on a forward trie that contained all possible n-grams of words. Boundaries were put in word positions where the conditional probability of seeing a certain letter after a substring suddenly dropped. This kind of pre-processing did not give precise morpheme boundaries for various reasons, mostly because it is a crude heuristic and it does not distinguish between common substrings from different locations in a word. By using maximum likelihood estimates we averaged the statistics across the subset. Subsequently, the model was applied to the entire data set to decompose all words. Promodes 2 (P2) was the version which used expectation maximization (EM) to estimate its parameters. Initially, words from a subset were randomly segmented and then the EM algorithm improved the parameter estimates until a convergence criterion was met. As convergence criterion we used the Kullback-Leibler divergence <ref type="bibr" coords="8,268.96,261.85,15.49,8.80" target="#b11">[12]</ref> which measures the difference between the model's probability distributions before and after each iteration of the EM. The resulting probabilistic model was then applied to the entire data set. Since the EM converges to local optima we restarted it with different initializations varying the rate of the random segmentation.</p><p>Promodes Committee (PC) made use of the different initializations and the resulting analyses of Promodes 2. Instead of having to choose a single result it combined different solutions into one by using majority vote for either segmenting or not in a certain position of a word. The idea was that when different initializations led to a word boundary in the same position, it should be placed and if they disagreed, the boundary can be omitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Analysis of results</head><p>Comparing our three methods to each other, we can say that P1 had the highest precision aside from Finnish and Turkish where PC came first. For the recall, P2 had the highest results except for English and Turkish where P1 had higher values. Looking at the F-measure as the harmonic mean of precision and recall, P2 had the highest performance for Arabic (non-vowelized, vowelized), German and Turkish. The highest performance for English was achieved by P1 and for Finnish by PC. Comparing our methods to other participant's methods in this year's challenge, we can state that P2 ranked first for vowelized and non-vowelized Arabic, and PC came third for Finnish. For the other languages, P1, P2 and PC were in top or mid ranks.</p><p>Since the analysis of precision and recall also depends on the distribution of the underlying data, we compared our methods to two default approaches shown in Table <ref type="table" coords="8,399.41,511.37,3.87,8.80" target="#tab_2">2</ref>, segmenting words after each letter called the segment-all (SA) approach and only assigning a common morpheme to all words without further analysis called the one-common-morpheme-only (CM) approach. SA shows whether words which share a morpheme label in the gold standard also have letters in common. CM shows the different underlying distributions in terms of average number of morphemes in each language. For CM, we would expect a recall close to 100% if word pairs of the gold standard have on average a morpheme in common. For SA, we also expect a recall near 100% if words labeled with the same morpheme tag in the gold standard have a similar spelling. For the languages English, Finnish, German and Turkish the precision of all our methods were significantly above the SA and CM method. Arabic was a challenging language because it contained the most morpheme labels per word as shown in Table <ref type="table" coords="8,244.13,630.92,3.87,8.80" target="#tab_3">3</ref>, independently from being vowelized (vw) or non-vowelized (nv). The winning method P1 still managed to have a higher precision, however, P2 and PC were slightly below the default solution. For the recall, SA and CM represented an upper bound. Since the recall has been calculated by looking at word pairs in the gold standard which share a common morpheme and should share a morpheme in the result file as well, CM would always return a correct answer because all words have a common morpheme. SA would return a correct answer if two words share at least a letter which is the case if they share a morpheme with the same or similar spelling. Expectedly, all our methods have recall values below the default solutions by SA and CM.</p><p>Looking at the overall performance expressed by the F-measure, results were significantly better than default for English, Finnish and German. For Turkish, the F-measure of our methods were slightly lower than the one of the CM approach. For vowelized and non-vowelized Arabic, our methods were also slightly below the SA approach. SA's higher F-measure has been caused by the number of morphemes per word which exceeds the number of letters. There is also a difference between the vowelized and the non-vowelized version where non-vowelized Arabic yields a lower F-measure. This can be explained by the fact that non-vowelized words are shorter and contain therefore less linguistical information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>We have presented different settings of an algorithm called Promodes which is based on a probabilistic generative model. Promodes has been developed from a machine learning perspective with little linguistical considerations and a simple labeling scheme where each morpheme has a unique morpheme label, either the morpheme itself or an index number. Two ways of estimating parameters from a subset of each language have been described, maximum likelihood estimates (Promodes 1) and expectation maximization (Promodes 2). Subsequently, we applied the two models to decompose an entire data set. As a third method we presented a committee of unsupervised learners (Promodes Committee) which combined different analyses of Promodes 2. All three methods achieved competitive results in the Morpho Challenge 2009 whereas Promodes 2 ranked first for vowelized and non-vowelized Arabic, and Promodes Committee came third for Finnish. For the other languages, our methods were in top or mid ranks.</p><p>In general, we believe that Promodes has the following strengths. The algorithm focuses on decomposing words and it predicts well how many morphemes two words have in common. The algorithm does not make assumptions about the structure of the language. It does not matter whether a language uses only suffixes or also prefixes. Furthermore, it can be applied to different degrees of supervision. In a (semi-) supervised or unsupervised setting it can be utilized to average over a small segmented set by learning its parameters from it. Another advantage is that instead of building a morpheme dictionary and morphosyntactic rules which are likely to be incomplete, it applies the statistics of a small training set to a larger test data set. Furthermore, the deployment of expectation maximization showed another way of estimating the model parameters. Doing so, we achieved similar or slightly better results than with the first method and we can choose a model, which meets our expectations the best, from different initializations. Another interesting method was the committee of unsupervised learners which combined results from different analyses.</p><p>Our future work includes investigating the behaviour of the committee in terms of number of members and how members should be best initialized. Moreover, we want to examine in greater detail what the impact of the training set size is and to continue in our ongoing optimization of the probabilistic model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,91.56,111.19,412.24,467.82"><head>Table 1 :</head><label>1</label><figDesc>Results of Promodes 1, Promodes 2, Promodes Committee in Competition 1</figDesc><table coords="7,452.63,146.40,50.26,83.19"><row><cell>Precision_P1</cell></row><row><cell>Precision_P2</cell></row><row><cell>Precision_PC</cell></row><row><cell>Recall_P1</cell></row><row><cell>Recall_P2</cell></row><row><cell>Recall_PC</cell></row><row><cell>F!measure_P1</cell></row><row><cell>F!measure_P2</cell></row><row><cell>F!measure_PC</cell></row></table><note coords="7,91.56,186.84,8.29,46.20;7,99.19,334.92,404.62,9.36"><p><p>Performance</p>Figure 2: Results of Promodes 1, Promodes 2, Promodes Committee in Competition 1</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,117.60,592.62,367.81,104.14"><head>Table 2 :</head><label>2</label><figDesc>Default segment-all (SA) and one-common-morpheme-only (CM) approach</figDesc><table coords="7,172.94,615.84,257.13,80.93"><row><cell>Language</cell><cell cols="2">average # morpheme average word length</cell></row><row><cell>Arabic (nv)</cell><cell>8.80</cell><cell>5.77</cell></row><row><cell>Arabic (vw)</cell><cell>8.75</cell><cell>9.90</cell></row><row><cell>English</cell><cell>2.25</cell><cell>8.70</cell></row><row><cell>Finnish</cell><cell>3.58</cell><cell>13.50</cell></row><row><cell>German</cell><cell>3.26</cell><cell>11.12</cell></row><row><cell>Turkish</cell><cell>3.63</cell><cell>10.80</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,240.07,709.77,122.87,8.80"><head>Table 3 :</head><label>3</label><figDesc>Language statistics</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,105.24,739.09,341.21,7.48"><p>Promodes stands for PRObabilistic generative MOdel for different DEgrees of Supervision.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,105.24,745.99,316.56,7.04"><p>Both morphological analyzers are the reference algorithms for the Morpho Challenge.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="5,105.24,718.54,407.76,7.04;5,90.00,728.01,86.12,7.04"><p>For Arabic we used the whole data set since there were less than 20,000 word forms given in the vowelized and non-vowelized data set.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="5,105.24,737.49,407.76,7.04;5,90.00,746.96,422.99,7.04"><p>A trie is a tree structure where each node corresponds to a single letter. If a set of strings contain a common prefix, they hang off the same node and the path from the root to the last common node corresponds to the prefix.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We would like to thank <rs type="person">Aram Harrow</rs> for various fruitful discussions on the mathematical background of this paper, and our team colleagues <rs type="person">Roger Tucker</rs> and <rs type="person">Ksenia Shalonova</rs> for consulting us on general issues in morphological analysis. The work was sponsored by <rs type="funder">EPSRC</rs> grant <rs type="grantNumber">EP/E010857/1</rs> Learning the morphology of complex synthetic languages.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_fSKHmyK">
					<idno type="grant-number">EP/E010857/1</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,110.47,133.78,402.53,8.80;10,110.48,145.73,402.53,8.80;10,110.48,157.69,160.56,8.80" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,264.21,133.78,244.61,8.80">Combinatory hybrid elementary analysis of text (cheat)</title>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Atwell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,110.48,145.73,402.53,8.80;10,110.48,157.69,68.18,8.80">Proceedings of the PASCAL Challenges Workshop on Unsupervised Segmentation of Words into Morphemes</title>
		<meeting>the PASCAL Challenges Workshop on Unsupervised Segmentation of Words into Morphemes<address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,177.61,402.52,8.80;10,110.48,189.57,302.12,8.80" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,198.70,177.61,270.57,8.80">Simple morpheme labelling in unsupervised morpheme analysis</title>
		<author>
			<persName coords=""><forename type="first">Delphine</forename><surname>Bernhard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,476.71,177.61,36.29,8.80;10,110.48,189.57,153.20,8.80">Working notes for the CLEF 2007 Workshop</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,209.49,402.53,8.80;10,110.48,221.45,99.68,8.80" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="10,171.90,209.49,298.90,8.80">The Grammar of Words: An Introduction to Linguistic Morphology</title>
		<author>
			<persName coords=""><forename type="first">Geert</forename><surname>Booij</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,241.37,402.52,8.80;10,110.48,253.33,402.52,8.80;10,110.48,265.29,354.41,8.80" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,264.30,241.37,248.69,8.80;10,110.48,253.33,97.38,8.80">Inducing the morphological lexicon of a natural language from unannotated text</title>
		<author>
			<persName coords=""><forename type="first">Mathias</forename><surname>Creutz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krista</forename><surname>Lagus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,215.97,253.33,297.02,8.80;10,110.48,265.29,274.34,8.80">Proceedings of the International and Interdisciplinary Conference on Adaptive Knowledge Representation and Reasoning (AKRR&apos;05)</title>
		<meeting>the International and Interdisciplinary Conference on Adaptive Knowledge Representation and Reasoning (AKRR&apos;05)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="106" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,285.21,402.52,8.80;10,110.48,297.17,309.19,8.80" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,271.71,285.21,241.28,8.80;10,110.48,297.17,88.00,8.80">Unsupervised models for morpheme segmentation and morphology learning</title>
		<author>
			<persName coords=""><forename type="first">Mathias</forename><surname>Creutz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krista</forename><surname>Lagus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,207.08,297.17,150.86,8.80">ACM Trans. Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,317.09,402.53,8.80;10,110.48,329.05,402.53,8.80;10,110.48,341.00,214.03,8.80" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,284.60,317.09,228.40,8.80;10,110.48,329.05,33.46,8.80">Simple unsupervised morphology analysis algorithm (sumaa)</title>
		<author>
			<persName coords=""><forename type="first">Minh</forename><surname>Thang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dang</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Saad</forename><surname>Choudri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,155.06,329.05,357.94,8.80;10,110.48,341.00,110.57,8.80">Proceedings of the PASCAL Challenges Workshop on Unsupervised Segmentation of Words into Morphemes</title>
		<meeting>the PASCAL Challenges Workshop on Unsupervised Segmentation of Words into Morphemes<address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,360.93,402.53,8.80;10,110.48,372.88,339.64,8.80" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,328.31,360.93,184.69,8.80;10,110.48,372.88,93.87,8.80">Maximum likelihood from incomplete data via the em algorithms</title>
		<author>
			<persName coords=""><forename type="first">Arthur</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nan</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,213.19,372.88,168.02,8.80">Journal of the Royal Statistical Society</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,392.81,402.53,8.80;10,110.48,404.76,157.67,8.80" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,187.57,392.81,273.76,8.80">Unsupervised learning of the morphology of a natural language</title>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Goldsmith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,470.24,392.81,42.76,8.80;10,110.48,404.76,73.85,8.80">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="153" to="198" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,424.69,402.53,8.80;10,110.48,436.64,126.29,8.80" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="10,192.98,424.69,320.02,8.80;10,110.48,436.64,47.29,8.80">The Handbook of Computational Linguistics, chapter Segmentation and morphology</title>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Goldsmith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Blackwell</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,456.57,402.53,8.80;10,110.48,468.52,189.00,8.80" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,239.08,456.57,210.63,8.80">Word segmentation by letter successor varieties</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Hafer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">F</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,460.93,456.57,52.08,8.80;10,110.48,468.52,92.17,8.80">Information Storage and Retrieval</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="371" to="385" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,488.45,341.63,8.80" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,183.34,488.45,123.21,8.80">From phoneme to morpheme</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zellig</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Harris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,316.33,488.45,38.56,8.80">Language</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="190" to="222" />
			<date type="published" when="1955">1955</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,508.37,402.52,8.80;10,110.48,520.33,111.94,8.80" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,258.19,508.37,136.54,8.80">On information and sufficiency</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kullback</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A</forename><surname>Leibler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,406.94,508.37,106.06,8.80;10,110.48,520.33,38.38,8.80">Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="79" to="86" />
			<date type="published" when="1951">1951</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,540.25,371.39,8.80" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="10,191.25,540.25,76.68,8.80">Machine Learning</title>
		<author>
			<persName coords=""><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>McGraw-Hill Science/Engineering/Math</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,560.18,402.52,8.80;10,110.48,572.13,402.53,8.80;10,110.48,584.09,177.79,8.80" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="10,193.97,560.18,319.03,8.80;10,110.48,572.13,30.48,8.80">ParaMor: From Paradigm Structure To Natural Language Morphology Induction</title>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Monson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<pubPlace>Pittsburgh, PA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Language Technologies Institute, School of Computer Science, Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="10,110.48,604.02,402.53,8.80;10,110.48,615.97,400.44,8.80" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,302.08,604.02,210.92,8.80;10,110.48,615.97,48.32,8.80">A probabilistic model for learning concatenative morphology</title>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><forename type="middle">G</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Brent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,168.04,615.97,234.12,8.80">Proceedings of Neural Information Processing Systems</title>
		<meeting>Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="1513" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,635.90,402.53,8.80;10,110.48,647.85,402.53,8.80;10,110.48,659.81,336.61,8.80" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,379.82,635.90,133.19,8.80;10,110.48,647.85,311.96,8.80">Unsupervised learning of morphology using a novel directed search algorithm: Taking the first step</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gaja</forename><forename type="middle">E</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Jarosz</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Brent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,433.52,647.85,79.49,8.80;10,110.48,659.81,267.70,8.80">Proceedings of the ACL-02 workshop on Morphological and phonological learning</title>
		<meeting>the ACL-02 workshop on Morphological and phonological learning</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
