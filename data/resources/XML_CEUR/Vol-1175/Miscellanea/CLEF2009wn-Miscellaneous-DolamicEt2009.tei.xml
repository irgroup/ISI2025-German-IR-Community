<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,206.59,148.86,189.82,15.15;1,160.24,170.78,282.52,15.15">UniNE at CLEF 2009: Persian Ad Hoc Retrieval and IP</title>
				<funder ref="#_MrWjtra">
					<orgName type="full">Swiss National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,196.55,204.67,70.45,8.74"><forename type="first">Ljiljana</forename><surname>Dolamić</surname></persName>
							<email>ljiljana.dolamic@unine.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of Neuchatel</orgName>
								<address>
									<addrLine>Rue Emile Argand 11</addrLine>
									<postCode>2009</postCode>
									<settlement>Neuchâtel</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,275.14,204.67,61.25,8.74"><forename type="first">Claire</forename><surname>Fautsch</surname></persName>
							<email>claire.fautsch@unine.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of Neuchatel</orgName>
								<address>
									<addrLine>Rue Emile Argand 11</addrLine>
									<postCode>2009</postCode>
									<settlement>Neuchâtel</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,344.27,204.67,62.18,8.74"><forename type="first">Jacques</forename><surname>Savoy</surname></persName>
							<email>jacques.savoy@unine.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of Neuchatel</orgName>
								<address>
									<addrLine>Rue Emile Argand 11</addrLine>
									<postCode>2009</postCode>
									<settlement>Neuchâtel</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,206.59,148.86,189.82,15.15;1,160.24,170.78,282.52,15.15">UniNE at CLEF 2009: Persian Ad Hoc Retrieval and IP</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6A1E27B5BE263F29DBB7AD9D4D40DF4A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Management]: Languages-Query Languages Experimentation, Performance, Measurement, Algorithms Intellectual Property, Persian Language, Stemming</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of the University of Neuchâtel to the CLEF 2008 evaluation campaign. In the Persian ad hoc task, we suggest using a light suffixstripping algorithm for the Farsi language and the evaluations demonstrated that such an approach performs better than a simple light stemmer, an approach ignoring the stemming stage or a language independent approach (n-gram). The use of a blind query expansion (e.g., Rocchio's model) may improve the retrieval effectiveness. Combining different indexing and search strategies may further enhance the corresponding MAP. In the Intellectual Property (IP) task, we try different strategies to select and weight pertinent words to be extracted from a patent description in order to form an effective query. We also evaluated different search models and found that probabilistic models tend to perform better than vector-space schemes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Our participation to the CLEF 2009 evaluation campaign was motivated by our objective to design and evaluate indexing and search strategies for other languages than English (e.g., Persian ad hoc retrieval track) on the one hand, and on the other, by developing effective domain-specific IR (patent retrieval in the current case, also called "Intellectual Property" or CLEF-IP).</p><p>If the English language was studied since 1960, other natural languages may reveal different linguistic constructions having an impact on the retrieval effectiveness. For some languages (e.g., Chinese, Japanese), word segmentation is not an easy task, while for others (e.g., German), the frequent use of different compound constructions to express the same object or idea may hurt the retrieval quality. The presence of numerous inflectional suffixes (e.g., Hungarian, Finnish), even for names (e.g., Czech, Russian) as well as numerous derivational suffixes must be taken into account for an effective retrieval. In this context, the Persian language is member of the Indo-European family but it is written using Arabic letters. The underlying morphology is more complex than the English one but we cannot qualify it as hard compared to some languages such as Turkish or Finnish. In Persian (or Farsi) language, various suffixes are used to indicate the plural, the accusative or genitive cases as well as other suffixes (or prefixes) are employed to derive new words.</p><p>In the Intellectual Property ad hoc task, we face clearly a domain-specific IR problem. Based on a large set of patent descriptions written in part in three different languages (English, German and French), we need to retrieve patents that are similar to a submitted one. This task could be viewed as detecting conflict between patent claims or as claim validation. In such case and contrary to other search environments, the query formulation contains a large number of terms (e.g., full patent description) and the determination and extraction of the most useful terms for an effective search are a hard task. Due to the fact that a patent is composed of different parts with different relative importances from an IR point of view, a critical problem is to define the most pertinent passages to form the query. We think also that a language shift may occur between the language used in the submitted patent and the language used in other patents. For example, a patent proposal may not employ directly the term "pump" but may simply describe it to avoid direct and evident conflict with existing patents. As an additional problem, the submitted patent may concern only a subpart of a given object (e.g., injection in a pump system) and pertinent items must be related not on the general object (e.g., pump) but on the specific targeted element.</p><p>Of course we can automatically enlarge the query by extracting related terms provided by a general or specialized thesauri, by using commercial search engines or by using given web sites (e.g., Wikipedia). Using the citation information could be a way to improve the quality of the search (and such a search strategy may also cross easily the language barriers). The presence of drawings could also be used to enhance the retrieval effectiveness in some cases and under the assumption that an effective image-based search function is available (which is not the case for our participation).</p><p>The rest of this paper is organized as follows. Section 2 describes the main characteristics of the test-collections while Section 3 presents briefly the various IR models used in our evaluation. The evaluation of the different indexing and search models with our test-collections are described and analyzed in Section 4. Section 5 reports our official results for both the Persian ad hoc and the CLEF-IP track. Our main findings are regrouped in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview of Test-Collections</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Persian</head><p>The Persian test-collection used in this year's evaluation consists of newspaper articles extracted from Hamshahri (covering years 1996 to 2002). This corpus is the same one made available during the CLEF 2008 evaluation campaign and contains 611 MB of data with exactly 166,477 documents. In mean, we can find 202 terms per document (after stopword removal). All documents contain only &lt;text&gt; information without further division. The last column of the Table <ref type="table" coords="2,457.70,605.60,4.98,8.74">1</ref> gives basic statistics for this collection. The collection also contains 50 new topics (e.g., Topic #600 -Topic #650) having total of 4,464 relevant items, with mean of 89.28 relevant items per query (median 81.5 and standard deviation 55.63). The Topic #610 has the smallest number of relevant items (e.g., 8) while the largest number of relevant items (e.g., 266) was found for the Topic #649.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Intellectual Property (IP)</head><p>For the CLEF-IP track a data collection of more than 1 million patent documents was made available. The patents are derived from sources from the European Patent Office (EPO) and cover English, German and French patents, with at least 100,000 documents in each language. The patent can be divided into four main parts, namely "front page" (biblio and abstract), state  <ref type="table" coords="3,132.68,406.86,4.98,8.74">1</ref> shows collection statistics. For the IP collection, the number of documents indicates the number of patents containing at least one entry in the given language. We observe that while 99.22% of the documents contain some information in English, only 89.51% contain German information and 89.56% French information.</p><p>Among the various information available in each patent document, we decided to keep only following information: revised international patent classification number (tag &lt;classification-ipcr&gt;), abstract (&lt;abstract&gt;), patent description (&lt;description&gt;), claims (&lt;claims&gt;) and the invention title (&lt;invention-title&gt;) for our experiments. Not each patent contains necessarily all of these fields and each document might be written using more than one language. We also kept the language information for each field, in order to apply language specific indexing strategies such as stemming, stopword removal or decomposition.</p><p>For this collection three different sets of topics were available, a small set containing 500 queries (denoted S bundle), a medium set (1,000 or M bundle) and a large set (10,000 or XL bundle). Each topic consist of an entire patent document stored in the same format as described previously, but not included in the corpus.</p><p>Since the the whole document could not be used as query, one of the main challenges of this task was to formulate an appropriate query out of a patent document. To generate the query we applied following procedure. For each term t j contained in the abstract, description, claim or invention title of the patent, we computed a weight w(t j ) based on Equation <ref type="formula" coords="3,422.58,622.05,3.87,8.74" target="#formula_0">1</ref>. The m terms with the highest weights are then chosen as query terms.</p><formula xml:id="formula_0" coords="3,244.74,653.82,268.26,25.86">w(t j ) = tf j • idf j k (tf k • idf k ) 2<label>(1)</label></formula><p>where tf j is the frequency of the term t j in the patent and idf j the inverse document frequency of t j in the document collection. For our experiments we fixed m = 100. We reference to this query formulation as "Q". For some runs we added the classification numbers contained in the patent.</p><p>In such cases, the query formulation will be referenced as "QC".</p><p>Relevance assessments for this collection provide two levels of relevancy. For each topic documents considered relevant (level 1) and documents highly relevant (2) are given. If we consider both relevancy levels, we have an average of 6.22 relevant items per query, with a maximum of 56 and a minimum of 3. If however we consider only highly relevant patents, we have an average of 3.46 relevant items per query, with a maximum of 18 and a minimum of 1. All evaluations in this paper are done considering documents of level 1 and 2 as relevant. We ignore evaluation done only on the highly relevant items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">IR Models</head><p>In order to analyze the retrieval effectiveness under different conditions, we adopted various retrieval models for weighting the terms included in queries and documents. To be able to compare the different models and analyze their relative merit, we first used a classical tf idf model. We would thus take into account the occurrence frequency of the term t j in the document D i (tf ij ) as well as the inverse document frequency of term t j in the collection (idf j = ln( n dfj ) with n the number of documents in the corpus and df j the number of documents in which the term t j occurs). Furthermore we normalized each indexing weight using the cosine normalization. Additionally to this classical vector-space model, we used other models issued from both the vector-space and probabilistic model families.</p><p>We implemented the "doc=Lnu, query=ltc" (or Lnu-ltc) and "doc=Lnc, query=ltc" (Lnc-ltc) weighting schemes proposed by Buckley et al. <ref type="bibr" coords="4,298.54,359.35,10.52,8.74" target="#b0">[1]</ref> issued from the class of vector-space models. For these models the document score for document D j for the query Q is calculated by applying following formula:</p><formula xml:id="formula_1" coords="4,237.95,396.66,275.05,20.06">score(D i , Q) = tj ∈Q w ij • w Qj<label>(2)</label></formula><p>where w ij represents the weight assigned to the term t j in the document D i and w Qj the weight assigned to t j in the query Q. For the Lnu-ltc model the weight assigned to the document term ("doc=Lnu") is defined by Equation 3 while Equation <ref type="formula" coords="4,334.31,447.14,4.98,8.74" target="#formula_3">4</ref>gives the weight assigned to the query term ("query=ltc").</p><formula xml:id="formula_2" coords="4,221.06,469.85,291.94,28.56">w ij = ln(tfij )+1 ln(mean tf )+1 (1 -slope) • pivot + slope • nt i<label>(3)</label></formula><formula xml:id="formula_3" coords="4,218.86,503.51,294.14,28.84">w qj = ln(tf qj + 1) • idf j t k ∈Q ((ln(tf qk ) + 1) • idf k ) 2<label>(4)</label></formula><p>where nt i is the number of distinct indexing terms in the document D i , pivot and slope are constants, and mean tf is the mean term frequency in the document D i (values are given in Table <ref type="table" coords="4,117.40,563.09,3.87,8.74">1</ref>).</p><p>For the "doc=Lnc, query=ltc" model, the weight for the query is calculated by Formula 4 while for the document ("doc=Lnc") it is calculated using following equation:</p><formula xml:id="formula_4" coords="4,218.25,604.28,294.75,28.99">w ij = ln(tf ij + 1) t k ∈Di ((ln(tf ik ) + 1) • idf k ) 2<label>(5)</label></formula><p>To complete the vector-space models, we implemented several probabilistic approaches. As a first probabilistic approach, we implemented the Okapi model (BM25) as proposed by Robertson et al. <ref type="bibr" coords="4,115.47,666.39,10.52,8.74" target="#b1">[2]</ref> evaluating the document score by applying following formula:</p><formula xml:id="formula_5" coords="4,182.57,683.89,330.43,26.80">score(D i , Q) = tj ∈Q qtf j • log n -df j df j • (k 1 + 1) • tf ij K + tf ij<label>(6)</label></formula><p>with</p><formula xml:id="formula_6" coords="4,112.65,719.69,116.86,13.64">K = k 1 • ((1 -b) + b • li avdl )</formula><p>where qtf j denotes the frequency of term t j in the query Q, and l i the length of the document D i . Average document length is represented by avdl ((values are given in Table <ref type="table" coords="4,155.04,745.64,4.43,8.74">1</ref>) while b and k 1 are constants.</p><p>As second probabilistic approach, we implemented several models issued from the Divergence of Randomness (DFR) paradigm as proposed by Amati et al. <ref type="bibr" coords="5,344.53,123.98,9.96,8.74" target="#b2">[3]</ref>. In this framework, two information measures are combined to compute the weight w ij attached to the term t j in the document D i . The weight is then calculated using following formula:</p><formula xml:id="formula_7" coords="5,169.30,167.73,264.40,12.69">w ij = Inf 1 ij • Inf 2 ij = -log 2 (P rob 1 ij (tf ij )) • (1 -P rob 2 ij (tf ij ))</formula><p>As a first model, we implemented the DFR-PL2 scheme, defined by the following equations:</p><formula xml:id="formula_8" coords="5,253.18,211.50,259.82,27.86">P rob 1 ij = e -λj • λ tf nij j tf n ij !<label>(7)</label></formula><formula xml:id="formula_9" coords="5,259.48,252.71,249.28,23.22">P rob 2 ij = tf n ij tf n ij + 1 (<label>8</label></formula><formula xml:id="formula_10" coords="5,508.76,259.45,4.24,8.74">)</formula><p>with</p><formula xml:id="formula_11" coords="5,112.29,282.44,188.97,14.60">λ j = tcj n and tf n ij = tf ij • log 2 (1 + c•mean dl li</formula><p>) where tc j represents the number of occurrences of term t j in the collection. The constants c and mean dl (average document length) are fixed according to the underlying collection (see Table <ref type="table" coords="5,305.42,309.36,3.87,8.74">1</ref>).</p><p>As second model issued from the DFR framework, we implemented the DFR-InL2 model where P rob 2 ij is defined as in Equation <ref type="formula" coords="5,233.21,333.27,4.98,8.74" target="#formula_9">8</ref>and Inf 1 is defined as follows</p><formula xml:id="formula_12" coords="5,233.77,355.88,279.24,23.23">Inf 1 ij = tf n ij • log 2 n + 1 df j + 0.5<label>(9)</label></formula><p>where df j is the number of documents in which the term t j appears and tf n ij defined as before.</p><p>As third and last DFR model, we implemented the DFR-In e C2 model defined by following equations:</p><formula xml:id="formula_13" coords="5,234.94,429.40,82.90,12.69">Inf 1 ij = tf n ij • log 2</formula><p>n + 1 n e + 0.5 (10)</p><formula xml:id="formula_14" coords="5,237.13,454.33,275.88,23.22">P rob 2 ij = 1 - tc j + 1 df j • (tf n ij + 1)<label>(11)</label></formula><p>with</p><formula xml:id="formula_15" coords="5,112.69,484.62,245.69,13.47">n e = n • (1 -( n-1 n ) tcj ) and tf n ij = tf ij • ln(1 + c•mean dl li</formula><p>). Finally we also used a non-parametric probabilistic model based on a statistical language model. Both Okapi and DFR are viewed as parametric probabilistic models. In this study we adopted a model proposed by Hiemstra <ref type="bibr" coords="5,265.02,522.36,10.52,8.74" target="#b3">[4]</ref> combining an estimate based on document (P (t j |D i )) and on corpus (P (t j |C)) and defined by following equation</p><formula xml:id="formula_16" coords="5,174.52,558.59,338.47,20.06">P (D i |Q) = P (D i ) • tj ∈Q [λ j • P (t j |D i ) + (1 -λ j ) • P (t j |C)]<label>(12)</label></formula><p>with P (t j |D i ) = tfij li and P (t j |C) = dfj lc with lc = k df k where λ j is a smoothing factor, l i the length of document D i , and lc an estimate of the size of the corpus C. In our experiments λ j is constant for all indexing terms t j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>To measure retrieval performance of our different runs, we adopted MAP values computed using the TREC EVAL program. Using this tool, the MAP values are computed on the basis of 1,000 retrieved documents per query. In the following tables, the best performance under the given conditions are listed in bold. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query T Mean Average Precision</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Persian</head><p>The Persian language, belonging to the Indo-Aryan language family is written using 28 Arabic letters, with additional 4 letters ( ) being added to express sounds not present in classical Arabic. Suffixes predominate Persian morphology. Even thought this language does not have the definite article in the strict sense, it can be said that the relative suffix ( , the book which) and suffix ( , the son, informal writing) perform this function. The plurals in the Persian are formed by means of two suffixes, namely for animate ( , father, , fathers) and for inanimate ( , flower, , flowers) nouns, while the plural of Arabic nouns in this language is formed according to Arabic grammar rules (e.g., or for "sound" plurals). The "light" stemmer we propose for this language removes the above mentioned suffixes with addition of certain number of possessive and comparative suffixes, while the second stemmer we proposed, named "plural", detects and removes only the plural suffixes from Persian nouns together with any suffix that might follow them.</p><p>Table <ref type="table" coords="6,131.84,443.14,4.98,8.74" target="#tab_1">2</ref> shows the MAP achieved by five IR models as well as different indexing strategies with the short query formulation. Second column in Table <ref type="table" coords="6,324.09,455.09,4.98,8.74" target="#tab_1">2</ref> (marked "none") depicts the performance obtained by the word based indexing strategy without stemming, followed by the MAP achieved by our two stemmers, namely "plural" and "light". In the column marked "perstem" the results obtained using publicly available stemmer and morphological analyzer for the Persian language<ref type="foot" coords="6,508.53,489.38,3.97,6.12" target="#foot_1">2</ref> are given. This stemmer is based on numerous regular expressions in order to remove the corresponding suffixes. Finally the last column of the table depicts the performance of the language independent 5-gram indexing strategy. It can be seen from this table that the best performing models for all indexing strategies are the models derived from the DFR paradigm (marked bold in the table). The best performing indexing strategy proves to be the "light" stemming approach with the exception of the tf idf IR model for which the best performance was obtained by "plural" indexing approach. In all experiments presented in this paper the stoplist 3 for the Persian language containing 884 terms has been used.</p><p>Table <ref type="table" coords="6,132.57,598.55,4.98,8.74" target="#tab_3">3</ref> shows the MAP obtained using two different indexing strategies, namely "none" and "light" over five IR models with three query formulations (short or T, medium or TD and long or TDN). It can be seen from Table <ref type="table" coords="6,238.64,622.46,4.98,8.74" target="#tab_3">3</ref> that augmenting the query size ameliorates the MAP over T query formulation by 8% in average for TD queries and 15% for TDN queries.</p><p>Upon inspection of obtained results, we have found that the pseudo-relevance feedback (PRF or blind-query expansion) seemed to be a useful technique for enhancing retrieval effectiveness for this language. Table <ref type="table" coords="6,184.71,670.28,4.98,8.74" target="#tab_4">4</ref> depicts MAP obtained by using Rocchio's approach (denoted "Roc") <ref type="bibr" coords="6,502.48,670.28,10.52,8.74" target="#b0">[1]</ref> whereby the system was allowed to add m terms extracted from the k best ranked documents from the original query results. The MAP enhancement spans from +2.35% (light, Okapi, 0.4169 vs. 0.4267) to +11.1% (light, DFR-PL2, 0.4247 vs. 0.4718). We have also applied another idf -based query expansion model <ref type="bibr" coords="6,193.59,718.10,10.52,8.74" target="#b4">[5]</ref> in our official runs (see </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Intellectual Property</head><p>For indexing the patent documents, we applied different strategies depending on the language in order to improve retrieval effectiveness. To eliminate very frequent terms having no impact on sense-matching between topic and document we used a language specific stopword list for each language. Furthermore for each language the diacritics were replaced by their corresponding nonaccented equivalent. We also applied a language specific stemming strategy. For the English language we used the S-stemmer as proposed by Harmann <ref type="bibr" coords="7,348.43,502.22,10.52,8.74" target="#b5">[6]</ref> and a stopword list containing 571 terms, while for the German language, we applied our light stemmer<ref type="foot" coords="7,389.46,512.60,3.97,6.12" target="#foot_2">4</ref> , a stopword list containing 603 words and a decompounding algorithm <ref type="bibr" coords="7,282.15,526.13,9.96,8.74" target="#b6">[7]</ref>. Finally for the French language we also used our light stemmer and a stopword list containing 484 words. Table <ref type="table" coords="7,131.66,550.04,4.98,8.74" target="#tab_5">5</ref> shows the MAP achieved by the seven different models used on the IP collection as well as the different indexing strategies and query formulations. The evaluations were done using the small topic set. The last line indicates the MAP average computed for all IR models. For previous art search in the patents, we tried three different techniques. First we weighted search terms for each field (abstract, description, title, ...) separately and then added the results to obtain the final score for the given document. For example if one query term appears once in the abstract and once in the title, the term would have tf one for each field and idf related to the corresponding field. The weight is then calculated for each field and added up. We reference to this strategy as "Separated Fields". Second we weighted the search terms considering the whole document, i.e., if a term t occurs once in two different fields it has tf of two and to compute idf we consider the whole patent document. To this strategy we reference as "Single Field". The third and last strategy consist in searching only the description of the patent ("Description"). Furthermore we applied two query formulations either taking into account classification numbers ("QC") or not ("Q").</p><p>We observe that for all searching strategies except if we take only into consideration the description part of the patent, the language modeling approach (LM) shows the best performance. We can also see that keeping the various fields separated (index "Separated Fields") shows slightly better performance then if we index everything together. Searching only in the the description field of the patent, we obtain similar performances as when searching in the whole patent document. We furthermore observe that except if searching only in the description, vector-space models are generally outperformed by probabilistic models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Official Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Persian</head><p>Table <ref type="table" coords="8,117.15,645.34,4.98,8.74" target="#tab_2">6</ref> gives description and results of the four official runs submitted to the CLEF 2009 Persian ad hoc track. Each run is a fusion of several single runs using different IR models (DFR, Okapi, statistical language model(LM)), indexing strategies (word with and without stemming, 5-gram), query expansion strategies (Rocchio, idf -based or none) and query formulation (T, TD and TDN). The fusion was performed for all four runs using a Z-score operator <ref type="bibr" coords="8,393.93,693.16,9.96,8.74" target="#b7">[8]</ref>. In all cases we can see that combining different models, indexing and search strategies using Z-score approach improves clearly the retrieval effectiveness. In these different combinations, we however did not use our "light" stemmer showing a relatively hight retrieval effectiveness as depicted in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Intellectual Property</head><p>Table <ref type="table" coords="9,117.17,329.55,4.98,8.74" target="#tab_8">7</ref> shows our eight official runs submitted to the CLEF 2009 Intellectual Property task (IP).</p><p>Each run is either one single run or a fusion of several single runs, created using a Z-score fusion operator as described in <ref type="bibr" coords="9,199.97,353.46,9.96,8.74" target="#b7">[8]</ref>. For the first seven strategies we used only the small topic set (500 queries or S bundle) while for the last strategy, we used all 10,000 available topics (XL bundle). We observe that combining various single runs with the Z-score method at best improves retrieval effectiveness. The best performing strategy (UniNE strat3) is a combination of two probabilistic models, namely DFR-InL2 and Okapi and two different indexing strategies. The results for all runs lie relatively close together and present rather low MAP values. We do not consider expanding automatically query formulation due to the fact that the original topic expression was already unusually long compared to other ad hoc search done in past CLEF evaluation campaigns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>From our past experiences in various evaluation campaigns, the results achieved this year in CLEF confirm the retrieval effectiveness of the Divergence from Randomness probabilistic model family.</p><p>In particular the DFR-PL2 or the DFR-In e C2 implementation tends to produce high MAP when facing different test-collection. In both tracks, we found that using our Z-score operator to combine different indexing and search strategies tends to improve the resulting retrieval effectiveness.</p><p>For the Persian ad hoc task, we notice three main differences between results achieved last year and those obtained this year. First, using very short (title-only or T) query formulation, we achieved the best results in 2008. This is the contrary this year with results based on TDN topic formulation depicting the best MAP (see Table <ref type="table" coords="9,325.82,587.56,3.87,8.74" target="#tab_3">3</ref>). Second, unlike last year, the use of our stemmers was effective this year (see Table <ref type="table" coords="9,283.79,599.51,3.87,8.74" target="#tab_1">2</ref>), and particularly the "light" stemming approach. Third, applying a pseudo-relevance feedback enhance the retrieval effectiveness of the proposed ranked list (see Table <ref type="table" coords="9,186.73,623.42,3.87,8.74" target="#tab_4">4</ref>). For the moment, we do not have found a pertinent explanation to such difference between the two years. However, during both evaluation campaigns we found that a word-based indexing scheme using our "light" stemmer tends to perform better than a n-gram scheme.</p><p>In the Intellectual Property task (CLEF-IP), we were not able to propose an effective procedure to extract the most useful search terms or passages able to discriminate between the relevant and non-relevant patents. In our case, we fixed an arbitrary and fixed limit of 100 search terms to be extracted from the submitted patent description based on their tf idf weights. We experiment different indexing strategies and search models. It seems that building separate index for each field (title, abstract, description, . . . ) and then combining the resulting ranked list may improve the MAP. This task is particularly challenging knowing that only one participating group achieved a MAP clearly higher than 0.1 demonstrating also that additional evaluation campaigns are needed in this domain-specific task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,124.17,110.43,354.67,127.99"><head>Table 2 :</head><label>2</label><figDesc>MAP of Various Indexing Strategies and IR models (Persian Collection)</figDesc><table coords="6,384.62,110.43,31.13,8.74"><row><cell>(MAP)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,310.54,718.10,39.02,8.74"><head>Table 6 )</head><label>6</label><figDesc>.</figDesc><table coords="7,115.70,110.43,371.60,117.53"><row><cell></cell><cell></cell><cell></cell><cell cols="2">Mean Average Precision</cell><cell></cell><cell></cell></row><row><cell>Query</cell><cell>T</cell><cell>TD</cell><cell>TDN</cell><cell>T</cell><cell>TD</cell><cell>TDN</cell></row><row><cell>Stemmer</cell><cell>none</cell><cell>none</cell><cell>none</cell><cell>light</cell><cell>light</cell><cell>light</cell></row><row><cell>Okapi</cell><cell>0.3687</cell><cell>0.3960</cell><cell>0.4233</cell><cell>0.3894</cell><cell>0.4169</cell><cell>0.4395</cell></row><row><cell>DFR-PL2</cell><cell>0.3765</cell><cell>0.4057</cell><cell>0.4326</cell><cell>0.3983</cell><cell>0.4247</cell><cell>0.4521</cell></row><row><cell>DFR-In e C2</cell><cell>0.3762</cell><cell>0.4051</cell><cell>0.4284</cell><cell>0.4226</cell><cell>0.4226</cell><cell>0.4417</cell></row><row><cell>LM</cell><cell>0.3403</cell><cell>0.3727</cell><cell>0.4078</cell><cell>0.3559</cell><cell>0.3867</cell><cell>0.4268</cell></row><row><cell>tf idf</cell><cell>0.2521</cell><cell>0.2721</cell><cell>0.2990</cell><cell>0.2521</cell><cell>0.2687</cell><cell>0.2928</cell></row><row><cell>mean</cell><cell>0.3428</cell><cell>0.3703</cell><cell>0.3982</cell><cell>0.3582</cell><cell>0.3839</cell><cell>0.4106</cell></row><row><cell>% over T</cell><cell></cell><cell>+8%</cell><cell>16.17%</cell><cell></cell><cell>+7.2%</cell><cell>14.62%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,108.81,241.63,388.70,126.40"><head>Table 3 :</head><label>3</label><figDesc>MAP of Various IR Models and Query Formulations (Persian Collection)</figDesc><table coords="7,292.24,274.41,105.41,8.74"><row><cell>Mean Average Precision</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,159.01,381.71,284.99,8.74"><head>Table 4 :</head><label>4</label><figDesc>MAP using Blind-Query Expansion (Persian Collection)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,95.98,110.43,434.25,345.08"><head>Table 5 :</head><label>5</label><figDesc>MAP of Various IR Models and Query Formulations (CLEF-IP)</figDesc><table coords="8,95.98,110.43,434.25,345.08"><row><cell></cell><cell></cell><cell></cell><cell cols="3">Mean Average Precision (MAP)</cell><cell></cell></row><row><cell>Query</cell><cell></cell><cell>Q</cell><cell></cell><cell>QC</cell><cell>Q</cell><cell>Q</cell></row><row><cell>Index</cell><cell></cell><cell cols="6">Separated Fields Separated Fields Single Field Description</cell></row><row><cell cols="2">Model / # of queries</cell><cell>500</cell><cell></cell><cell>500</cell><cell>500</cell><cell>500</cell></row><row><cell>Okapi</cell><cell></cell><cell>0.0832</cell><cell></cell><cell>0.0832</cell><cell>0.0843</cell><cell>0.0856</cell></row><row><cell cols="2">DFR-InL2</cell><cell>0.0849</cell><cell></cell><cell>0.092</cell><cell>0.0645</cell><cell>0.0872</cell></row><row><cell cols="2">DFR-PL2</cell><cell>0.083</cell><cell></cell><cell>0.0909</cell><cell>0.0515</cell><cell>0.0673</cell></row><row><cell>LM</cell><cell></cell><cell cols="2">0.0886</cell><cell>0.0952</cell><cell>0.0891</cell><cell>0.0787</cell></row><row><cell>Lnc-ltc</cell><cell></cell><cell>0.0735</cell><cell></cell><cell>0.0839</cell><cell>0.0554</cell><cell>0.0884</cell></row><row><cell>Lnu-ltc</cell><cell></cell><cell>0.0675</cell><cell></cell><cell>0.0782</cell><cell>0.0695</cell><cell>0.0589</cell></row><row><cell>tf idf</cell><cell></cell><cell>0.0423</cell><cell></cell><cell>0.0566</cell><cell>0.0380</cell><cell>0.0337</cell></row><row><cell>Mean</cell><cell></cell><cell>0.0748</cell><cell></cell><cell>0.0829</cell><cell>0.0464</cell><cell>0.0714</cell></row><row><cell>Run name</cell><cell>Query</cell><cell>Index</cell><cell>Model</cell><cell cols="2">Query exp.</cell><cell>MAP</cell><cell>Comb.MAP</cell></row><row><cell></cell><cell>T</cell><cell>word</cell><cell>DFR-PL2</cell><cell>none</cell><cell></cell><cell>0.3765</cell></row><row><cell>UniNEpe1</cell><cell>T</cell><cell>5-gram</cell><cell>LM</cell><cell cols="2">idf 10 docs/50 terms</cell><cell>0.3726</cell><cell>0.4380</cell></row><row><cell></cell><cell>T</cell><cell>plural</cell><cell>Okapi</cell><cell cols="2">Roc 10 docs/70 terms</cell><cell>0.4197</cell></row><row><cell></cell><cell>TD</cell><cell>5-gram</cell><cell>DFR-In e C2</cell><cell>none</cell><cell></cell><cell>0.4113</cell></row><row><cell>UniNEpe2</cell><cell>TD</cell><cell>word</cell><cell>DFR-PL2</cell><cell>none</cell><cell></cell><cell>0.4057</cell><cell>0.4593</cell></row><row><cell></cell><cell>TD</cell><cell>plural</cell><cell>Okapi</cell><cell cols="2">Roc 5 docs/70 terms</cell><cell>0.4311</cell></row><row><cell></cell><cell>TD</cell><cell>word</cell><cell>DFR-PL2</cell><cell cols="2">idf 10 docs/50 terms</cell><cell>0.4466</cell></row><row><cell></cell><cell>TD</cell><cell>word</cell><cell>Okapi</cell><cell cols="2">Roc 5 docs/50 terms</cell><cell>0.4228</cell></row><row><cell>UniNEpe3</cell><cell>TD</cell><cell>plural</cell><cell>Okapi</cell><cell cols="2">Roc 5 docs/70 terms</cell><cell>0.4311</cell><cell>0.4663</cell></row><row><cell></cell><cell>TD</cell><cell>perstem</cell><cell>DFR-PB2</cell><cell cols="2">idf 10 docs/50 terms</cell><cell>0.4462</cell></row><row><cell></cell><cell>TDN</cell><cell>word</cell><cell>LM</cell><cell cols="2">Roc 10 docs/50 terms</cell><cell>0.4709</cell></row><row><cell>UniNEpe4</cell><cell>TDN</cell><cell>plural</cell><cell>Okapi</cell><cell cols="2">Roc 5 docs/70 terms</cell><cell>0.4432</cell><cell>0.4937</cell></row><row><cell></cell><cell>TDN</cell><cell>perstem</cell><cell>DFR-PL2</cell><cell cols="2">Roc 10 docs/20 terms</cell><cell>0.4769</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="8,178.79,468.63,242.09,8.74"><head>Table 6 :</head><label>6</label><figDesc>Description and MAP of Official Persian Runs</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="8,437.97,729.03,35.15,8.74"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table coords="9,95.98,110.82,373.23,155.39"><row><cell>Run name</cell><cell>Query</cell><cell>Index</cell><cell>#Queries</cell><cell>Model</cell><cell cols="2">MAP Comb.MAP</cell></row><row><cell>UniNE strat1</cell><cell>Q</cell><cell>Single</cell><cell>500</cell><cell>Lnu-ltc</cell><cell>0.0695</cell><cell>0.0695</cell></row><row><cell>UniNE strat2</cell><cell>Q</cell><cell>Single</cell><cell>500</cell><cell>LM</cell><cell>0.0891</cell><cell>0.0891</cell></row><row><cell>UniNE strat3</cell><cell>QC</cell><cell>Separated</cell><cell>500</cell><cell cols="2">DFR-InL2 0.092</cell><cell>0.1024</cell></row><row><cell></cell><cell>Q</cell><cell>Description</cell><cell></cell><cell>Okapi</cell><cell>0.0856</cell><cell></cell></row><row><cell>UniNE strat4</cell><cell>Q</cell><cell>Separated</cell><cell>500</cell><cell>LM</cell><cell>0.0886</cell><cell>0.0961</cell></row><row><cell></cell><cell>QC</cell><cell>Separated</cell><cell></cell><cell>Okapi</cell><cell>0.0832</cell><cell></cell></row><row><cell></cell><cell>Q</cell><cell>Single</cell><cell></cell><cell>LM</cell><cell>0.0891</cell><cell></cell></row><row><cell>UniNE strat5</cell><cell>Q</cell><cell>Description</cell><cell>500</cell><cell>Okapi</cell><cell>0.0856</cell><cell>0.0856</cell></row><row><cell>UniNE strat6</cell><cell>QC</cell><cell>Separated</cell><cell>500</cell><cell cols="2">DFR-PL2 0.0909</cell><cell>0.0955</cell></row><row><cell></cell><cell>Q</cell><cell>Single</cell><cell></cell><cell>Lnu-ltc</cell><cell>0.0554</cell><cell></cell></row><row><cell>UniNE strat7</cell><cell>QC</cell><cell>Separated</cell><cell>500</cell><cell>Lnc-ltc</cell><cell>0.0839</cell><cell>0.0839</cell></row><row><cell>UniNE strat8</cell><cell>QC</cell><cell>Separated</cell><cell>10,000</cell><cell>Okapi</cell><cell>0.0994</cell><cell>0.0994</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="9,191.29,279.34,220.42,8.74"><head>Table 7 :</head><label>7</label><figDesc>Description and MAP of Official IP Runs</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,105.24,744.49,209.60,6.99"><p>http://www.ir-facility.org/pdf/clef/patent-document.dtd</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="6,105.24,737.50,153.54,6.99"><p>http://sourceforge.net/projects/perstem/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="7,105.24,736.34,117.15,6.99"><p>http://www.unine.ch/info/clef/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments The authors would like to also thank the CLEF-2009 organizers for their efforts in developing test-collections. This research was supported in part by the <rs type="funder">Swiss National Science Foundation</rs> under Grant #<rs type="grantNumber">200021-113273</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_MrWjtra">
					<idno type="grant-number">200021-113273</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,105.50,228.55,407.50,9.30;10,105.50,240.50,146.46,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,279.47,228.55,169.99,9.30">New retrieval approaches using SMART</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,477.09,228.55,35.91,8.74;10,105.50,240.50,66.62,8.74">Proceedings of TREC-4</title>
		<meeting>TREC-4</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="25" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.50,260.43,407.50,8.74;10,105.50,272.38,357.57,9.30" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,319.58,260.43,193.42,8.74;10,105.50,274.07,25.85,7.61">Experimentation as a way of life: Okapi at TREC</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beaulieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,147.59,272.38,168.49,8.74">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="95" to="108" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.50,292.31,407.51,8.74;10,105.50,304.26,407.51,8.74;10,105.50,316.22,108.48,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,258.82,292.31,254.18,8.74;10,105.50,304.26,167.57,8.74">Probabilistic models of information retrieval based on measuring the divergence from randomness</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J V</forename><surname>Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,286.02,304.26,187.82,8.74">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="357" to="389" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.50,336.14,407.50,8.74;10,105.50,348.10,407.50,8.74;10,105.50,360.06,356.79,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,171.84,336.14,341.16,8.74;10,105.50,348.10,185.99,8.74">Term-specific smoothing for the language modeling approach to information retrieval: The importance of a query term</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,317.15,348.10,195.84,8.74;10,105.50,360.06,277.86,8.74">Proceedings of the 25th ACM Conference on Research and Development in Information Retrieval (SIGIR&apos;02)</title>
		<meeting>the 25th ACM Conference on Research and Development in Information Retrieval (SIGIR&apos;02)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="35" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.50,379.98,407.50,9.30;10,105.50,391.94,326.00,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,215.88,379.98,297.12,9.30;10,105.50,391.94,21.35,8.74">Searching in Medline: Query expansion and manual indexing evaluation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Abdou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,138.71,391.94,168.49,8.74">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="781" to="789" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.50,411.86,407.51,8.74;10,105.50,423.82,138.90,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,169.63,411.86,110.23,8.74">How effective is suffixing</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,292.91,411.86,220.09,8.74;10,105.50,423.82,30.49,8.74">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="7" to="15" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.50,443.74,407.50,9.30;10,105.50,455.70,407.50,8.74;10,105.50,467.65,407.51,8.74;10,105.50,479.61,220.33,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,153.58,443.74,359.42,9.30;10,105.50,455.70,123.93,8.74">Report on CLEF-2003 monolingual tracks: Fusion of probabilistic models for effective monolingual retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,252.97,455.70,260.03,8.74;10,105.50,467.65,34.63,8.74">Comparative Evaluation of Multilingual Information Access Systems</title>
		<title level="s" coord="10,453.71,467.65,59.29,8.74;10,105.50,479.61,89.18,8.74">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3237</biblScope>
			<biblScope unit="page" from="322" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.50,499.53,407.50,8.74;10,105.50,511.49,407.50,8.74;10,105.50,523.44,407.51,8.74;10,105.50,535.40,198.15,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,230.40,499.53,282.60,8.74;10,105.50,511.49,27.35,8.74">Selection and merging strategies for multilingual information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P.-Y</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,154.52,511.49,261.36,8.74">Multilingual Information Access for Text, Speech and Images</title>
		<title level="s" coord="10,439.98,523.44,73.03,8.74;10,105.50,535.40,76.96,8.74">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3491</biblScope>
			<biblScope unit="page" from="27" to="37" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
