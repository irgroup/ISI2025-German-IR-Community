<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,127.92,148.73,347.04,15.51">LIC2M experiments at ImageCLEF 2004</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,145.68,182.23,78.81,9.96"><forename type="first">Romaric</forename><surname>Besançon</surname></persName>
							<email>romaric.besancon@cea.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">CEA-LIST/LIC2M</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,232.47,182.23,54.51,9.96"><forename type="first">Patrick</forename><surname>Hède</surname></persName>
							<email>patrick.hede@cea.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">CEA-LIST/LIC2M</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,295.16,182.23,86.36,9.96"><forename type="first">Pierre-Alain</forename><surname>Moellic</surname></persName>
							<email>pierre-alain.moellic@cea.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">CEA-LIST/LIC2M</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,389.04,182.23,68.00,9.96"><forename type="first">Christian</forename><surname>Fluhr</surname></persName>
							<email>christian.fluhr@cea.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">CEA-LIST/LIC2M</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,127.92,148.73,347.04,15.51">LIC2M experiments at ImageCLEF 2004</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0CE8EB306CE1994F373F985EACD53DC3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For its first participation in the ImageCLEF campaign, the LIC2M participated in both the ad hoc task and the medical task. Using our cross-language information retrieval system and content-based image retrieval system, our goal was to perform some first experiments on merging the results of the two systems. The results show that the performance of each system highly depends on the corpus and the task: with the systems we used, text retrieval alone performs better on the ad hoc task, whereas image retrieval alone performs better on the medical task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The purpose of the ImageCLEF campaign is to study cross-language image retrieval, which includes the study of cross-language text retrieval on corpora that are specially associated with images (captions for instance) and the interactions between text and image retrieval. Since the LIC2M develops both cross-language text retrieval systems and content-based image retrieval systems, our goal was to perform some first experiments on merging strategies to integrate the information retrieved from both systems.</p><p>The goal of the ImageCLEF ad hoc task is to retrieve relevant images based on a text query. In this case, we tried to improve the results by merging the results of the cross-language text retrieval with a content-based image retrieval, using the example image given with the topic.</p><p>The goal of the ImageCLEF medical task is to retrieve relevant images based on an image query. The medical CasImage corpus is composed of a series of medical cases, associated with a text description and a set of images such as x-rays or scans. The task is to retrieve images similar to the query image with respect to its modality and the anatomic region shown. We tried, for this task, to improve the results by using an automatic feedback on the text description associated with images retrieved by a CBIR system.</p><p>We present in section 2 the retrieval systems for text and image. We then present the strategies used for the ad hoc task and the medical task and their results in section 3 and 4 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Retrieval Systems</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Cross-language Text Retrieval System</head><p>The cross-language text retrieval system used for these experiments is the same as the one used for the CLEF multilingual task, and a more detailed description can be found in the working notes corresponding to this task or in the proceedings of the CLEF 2003 campaign <ref type="bibr" coords="1,427.26,672.91,9.89,9.96" target="#b0">[1]</ref>. The system has not been specially adapted to work on the text of the ImageCLEF corpora, and has simply been used as is. This system is a weighted boolean search engine based on a linguistic analysis of the query and the documents. Its basic principle is briefly described here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document processing</head><p>The documents are processed through a linguistic analyzer, that performs in particular a part-of-speech tagging, a lemmatization, and extracts compounds and named entities from the text. All these elements are indexed into inverted files. For both the StAndrews and CasImage corpora, no special treatment has been performed to take into account the structure of the documents (such as photographer's name, location, date for the captions and description, diagnosis, clinical presentation in the medical cases): all fields have been taken as a single text to be analyzed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query processing</head><p>The query is first processed through a similar analyzer (corresponding to the query language) to extract the informative elements of the text. These elements are used as query "concepts". Each concept is reformulated into a set of search terms, either using a monolingual expansion dictionary (that introduces synonyms and related words), or using a bilingual dictionary, depending on the index languages.</p><p>Search and merging Each search term is searched in the index, and documents containing the term are retrieved. All retrieved documents are then associated to a concept profile, indicating the presence of query concepts in the document. Documents sharing the same concept profile are clustered together, and a weight is associated to each cluster according to its concept profile and to the weight of the concepts (the weight of a concept depends on the weight of each of its reformulated term in the retrieved documents). The clusters are sorted according to their weights and the first 1000 documents in this sorted list are retrieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Content-base Image Retrieval System</head><p>For Image retrieval, we used a system developed at our lab, the LIC2M, called PIRIA (Program for the Indexing and Research of Images by Affinity) <ref type="bibr" coords="2,316.08,412.75,11.54,9.96" target="#b1">[2]</ref>. A user query is submitted to the system, which returns a list of images ranked by their similarity to the query image. The similarity is obtained by a metric distance that operates on every image signature. These indexed images are compared according to several classifiers : principally Color, Texture and Form if the segmentation of the images is relevant. The system takes into account geometric transformations and variations like rotation, symmetry, mirroring, etc. PIRIA is a global one-pass system, feedback or "relevant/non relevant" learning methods are not used.</p><p>Color Indexing PIRIA uses a global normalized color histogram. The choice of the color space is very important for a good color division. The model based on Hue, Saturation and Value is used to obtain a strong semantic content. Global histogram is used for the global image or after the segmentation of the image in several blocks. Splitting the image by blocks enables computation of spatial relationship. A more complex color analysis can be used with a region based segmentation. Color information of each region are mixed with form analysis (Fourier descriptors). The distance uses for the color indexing is a classical L1 norm.</p><p>Texture Indexing A global texture histogram is used for the texture analysis. The histogram is computed from the Local Edge Pattern descriptors <ref type="bibr" coords="2,333.16,619.99,9.89,9.96" target="#b2">[3]</ref>. These descriptors describe the local structure according to the edge image computed with a Sobel filtering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Merge of results</head><p>The merging of results from several indexers is computed with a boundary fusion based on the position of the result's images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Ad hoc task</head><p>For the ad hoc task, we used topics in English and French. For each of these topic languages, we submitted two runs. The first one (lic2mSA*1t) was a simple text retrieval, with no use of the image retrieval system. The second one (lic2mSA2*ti) uses a simple merging strategy integrating the results of both text and image retrieval: in this case, the image used for the image retrieval was the example image provided for each topic. The merging strategy was quite straightforward: each image is given a score that is a weighted sum of the scores given by each retrieval systems.</p><p>The results of the four runs<ref type="foot" coords="3,225.36,158.55,3.95,6.37" target="#foot_0">1</ref> are presented in Figure <ref type="figure" coords="3,337.36,159.31,5.03,9.96">1</ref>  From these results, the merging strategy using both text and image does not show much better results than the direct text search. In the case of English topics, a little improvement of the average precision is noticed (not significant), which is mostly the effect of a reordering of retrieved documents (the total number of relevant documents retrieved actually decreases). This is mainly due to the fact that the image retrieval does not perform well on this corpus (indeed, the images need a complex local analysis -based on interest points) The image retrieval alone (using the example images from the queries) retrieves only 122 relevant images out of the 829 relevant images of the partial-isec-total assessments. Of these 122 images, only 8 were not found by the original text retrieval, based on English topics (11 for French topics). Hence, the merging strategy seems to give too much importance to the image result and add noise to the text retrieval (removing relevant images retrieved). Nevertheless, this merging causes a reordering of relevant documents already retrieved that seems to be interesting (at least in the case of English topics).</p><p>Further experiments for merging the results of text and image are planned to try to minimize the introduction of non-relevant images in the retrieval results.</p><p>The image indexers will also be adapted to treat images such as the old photographs of the StAndrews collection: this image base is particularly difficult for the kind of image indexers we used in this experiments since most of the images are old photographs that are in a kind of monochrome color (with not always the same tone), so that a color segmentation of the image cannot be performed to identify the interesting elements of the images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Medical task</head><p>For the medical task, we submitted two runs. The first one (lic2mCA1i) was a simple image retrieval search from the image query. The second one (lic2mCA2it) is based on the first image retrieval search and implements an automated feedback using text information contained in the cases associated with the retrieved images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Text feedback strategy for the CasImage corpus</head><p>The process of this feedback is the following (a schema presenting the outline of this feedback strategy is proposed in Figure <ref type="figure" coords="4,223.92,272.23,3.88,9.96" target="#fig_1">2</ref>):  we first take the images retrieved by the CBIR system: these images are given a score by the CBIR system (we call it the image score, denoted s i );</p><p>2. we collect the cases associated with the images retrieved by the CBIR system (we used only the top five cases for this feedback);</p><p>3. we then use these cases as queries to the text retrieval system to retrieve similar cases, based on the textual description of the cases: we retrieve the 20 most similar cases. These cases are given a score by the text retrieval system (text score, denoted s t ); 4. we collect the images associated to the cases retrieved by the text retrieval system: these images are candidate images for feedback;</p><p>5. since the images retrieved must have the same modality than the query image, we filter these images associated to the similar cases by their similarity to the corresponding images collected in step 1<ref type="foot" coords="5,195.96,110.67,3.95,6.37" target="#foot_2">2</ref> . The similarity with the original image give a score to the candidate images (filtering score, denoted s f );</p><p>6. The set of feedback images is then used to enrich the first set of retrieved images (step 1), either by increasing the score of an already retrieved image (function of its image score, the text score and the filtering score) or by adding new images, with an associated score that is a function of the image score of the image that lead to the new image, the text score and the filtering score of the new image. Since the scoring of the different systems are not easily comparable, the merging of the three scores is not obvious: we used in the submitted runs an arbitrary function defined as follows: if the image was already retrieved the score is α</p><formula xml:id="formula_0" coords="5,176.83,226.99,336.17,10.33">× s i + (1 -α) × f (s t , s f ), otherwise, the score attributed is α × g(s i , s t , s f ),</formula><p>where f (s t , s f ) and g(s i , s t , s f ) are weighted sums of the different scores. The α parameter has been introduced to make sure that images retrieved by the first step are still given more importance (in the experiments, α = 0.9).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">results</head><p>The results of both runs are presented in Figure <ref type="figure" coords="5,303.44,309.19,5.03,9.96" target="#fig_2">3</ref> (precision/recall graph) and Table <ref type="table" coords="5,464.21,309.19,3.90,9.96">2</ref>.  <ref type="table" coords="5,116.98,647.95,3.90,9.96">2</ref>: Results for the ad hoc task: average precision, R-precision, number of relevant document retrieved (with percentage)</p><p>In this case, the image retrieval alone performs better than the use of the text retrieval for feedback and enrichment of the retrieved images.</p><p>A deeper analysis of the feedback process show that this process produces 17656 image candidates for feedback (step 4: images from similar cases), in which 3195 images were already found by direct image retrieval (of which 829 were relevant images) and in the 14461 other images, only 243 are relevant images.</p><p>Furthermore, the merging strategy does not succeed in including these 243 documents: only 74 new relevant documents are added, and on the other hand, 82 relevant documents previously included in the initial retrieval are lost. The scoring function we tested introduce too many nonrelevant images. Further testing will be performed on the scoring function, and on the number of cases to consider for feedback and the number of similar cases to look at.</p><p>Also, the task imposes that the retrieved images are of the same modality than the query images. Hence, a general similarity on the textual description of the cases is not enough: it can retrieve cases relative to the same kind of pathology but it is not obvious that the images associated to these cases will be similar to the original image We tried to avoid this problem using a second step of image similarity, but a deeper analysis of the text could be needed so that informations on the image modality and anatomic region are extracted from the case description.</p><p>Another possible reason for these results is that our text retrieval system is very general. A specialized corpus such as this medical corpus contains many technical words that are treated by the system as unknown words. A more adapted processing of the medical text, giving special importance to terms such as disease names, anatomic regions, medical acts should increase the relevance of the cases similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>These first experiments in the ImageCLEF campaign are very interesting: with the same two general purpose systems (no particular adaptation of the systems was made for the two tasks), the results lead to very different conclusions according to the task and corpus.</p><p>The ad hoc task with the StAndrews collection of old photographs is not well adapted the kind of image indexers we used, that relies mostly on color for segmentation. On the other hand, this task is easier for text retrieval, since the descriptions of the images in the captions are small and precise and the elements in the queries are often found as is in the documents (even without treating the structure of the captions).</p><p>The medical task offers a better field for image retrieval, the images being "easier" to index (at least, to separate the images by their modality, quite different in nature and colors) but in that case, and given the particularity of the task and the specialization of the corpus, the feedback strategy using text information did not improve the results. More experiments should be undertaken to improve the feedback strategy. Other experiments using a training process to associate text to the medical images and use the results for adding text to the image query (textual query expansion) can also be imagined.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,184.92,545.35,232.92,9.96"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Text feedback strategy for the medical task</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,218.64,544.51,165.70,9.96"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Results for the medical task</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,90.00,159.31,422.75,348.96"><head>Table 1 :</head><label>1</label><figDesc>(precision/recall graph) and Table1. Results for the ad hoc task: average precision, number of relevant document retrieved (with percentage)</figDesc><table coords="3,155.64,185.44,291.54,279.04"><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">lic2mSAen1t.eval</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">lic2mSAen2ti.eval</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">lic2mSAfr1t.eval</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">lic2mSAfr2ti.eval</cell><cell></cell></row><row><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Precision</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell>Recall</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Figure 1: Precision/Recall results for the ad hoc task</cell><cell></cell></row><row><cell cols="6">lic2mSAen1t lic2mSAen2ti lic2mSAfr1t lic2mSAfr2ti</cell></row><row><cell>avg p</cell><cell>0.42</cell><cell>0.423</cell><cell>0.314</cell><cell cols="2">0.295</cell></row><row><cell cols="2">relret 757 (91.3%)</cell><cell cols="4">674 (81.3%) 720 (86.8%) 565 (68.1%)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,105.24,732.48,407.36,7.97;3,90.00,741.96,262.33,7.97"><p>The official results for the merging strategy (lic2mSA*2ti) are erroneous due to a misordering of the queries in the submitted runs (only 17 queries were taken into account out of the</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1" coords="3,355.33,741.96,14.02,7.97"><p>25).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2" coords="5,105.24,739.32,388.25,7.97"><p>We could have used directly the query image for filtering: the impact of such a choice should be studied.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,105.47,591.55,407.17,9.96;6,105.48,603.55,407.12,9.96;6,105.48,615.55,201.27,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,171.18,603.55,139.10,9.96">The LIC2Mś CLEF 2003 system</title>
		<author>
			<persName coords=""><forename type="first">Romaric</forename><surname>Besançon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Olivier</forename><surname>Gaël De Chalendar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Ferret</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Olivier</forename><surname>Fluhr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hubert</forename><surname>Mesnard</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Naets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,331.23,604.00,181.37,9.07;6,105.48,616.00,17.54,9.07">Working Notes for the CLEF 2003 Workshop</title>
		<meeting><address><addrLine>Trondheim, Norway</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-08-22">21-22 August 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.47,635.47,407.26,9.96;6,105.48,647.35,407.18,9.96;6,105.48,659.35,184.95,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,394.04,635.47,118.69,9.96;6,105.48,647.35,225.85,9.96">PIRIA : A general tool for indexing, search and retrieval of multimedia content</title>
		<author>
			<persName coords=""><forename type="first">Magali</forename><surname>Joint</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre-Alain</forename><surname>Moëllic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Hède</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pascal</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,352.73,647.80,135.68,9.07">SPIE Electroning Imaging 2004</title>
		<meeting><address><addrLine>San Jose, California USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-01">January 2004</date>
			<biblScope unit="page" from="18" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.47,679.27,407.07,9.96;6,105.48,691.27,226.84,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,221.44,679.27,224.57,9.96">Image classification using color, texture and regions</title>
		<author>
			<persName coords=""><forename type="first">Y.-C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S.-Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,467.10,679.72,45.43,9.07;6,105.48,691.72,76.90,9.07">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2003-09">September 2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
