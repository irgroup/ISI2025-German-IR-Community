<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,97.44,148.73,407.84,15.51;1,105.36,170.69,392.22,15.51;1,208.92,192.53,184.88,15.51">Report on the imageCLEF Experiment: How to visually retrieve images from the St. Andrews collection using GIFT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,171.60,226.03,68.11,9.96"><forename type="first">Henning</forename><surname>Müller</surname></persName>
							<email>henning.mueller@sim.hcuge.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Service of Medical Informatics</orgName>
								<orgName type="institution">University and University Hospitals of Geneva</orgName>
								<address>
									<addrLine>24 Rue Micheli-du-Crest</addrLine>
									<postCode>CH-1211</postCode>
									<settlement>Geneva 14</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,250.32,226.03,88.12,9.96"><forename type="first">Antoine</forename><surname>Geissbühler</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Service of Medical Informatics</orgName>
								<orgName type="institution">University and University Hospitals of Geneva</orgName>
								<address>
									<addrLine>24 Rue Micheli-du-Crest</addrLine>
									<postCode>CH-1211</postCode>
									<settlement>Geneva 14</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,365.63,226.03,57.30,9.96"><forename type="first">Patrick</forename><surname>Ruch</surname></persName>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Swiss Federal Institute of Technology</orgName>
								<address>
									<addrLine>LITH IN-Ecublens</addrLine>
									<postCode>CH-1015</postCode>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,97.44,148.73,407.84,15.51;1,105.36,170.69,392.22,15.51;1,208.92,192.53,184.88,15.51">Report on the imageCLEF Experiment: How to visually retrieve images from the St. Andrews collection using GIFT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DC37B72BFD49F258C846B510081CE070</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The imageCLEF task of the Cross Language Evaluation forum has as its main goal the retrieval of images from multi-lingual test collections, or retrieval of images where the query is in a different language than the collection itself. The 2003 imageCLEF task saw no group using the visual information of the images that is inherently language independent. In 2004, this changed and a few groups among them the university hospitals of Geneva are submitting visual runs for the queries.</p><p>The query topics are definitely defined in a way that makes visual retrieval extremely hard as pure visual similarity plays a marginal role whereas semantics and background knowledge are extremely important, that can only be obtained from textual captions. This article describes the submission of an entirely visual result set to the task. This article will also define possible improvements for visual retrieval systems with the current data. Most important is Section 4 that explains possible ways to make this query task more appealing to visual retrieval research groups, explaining problems of content-based retrieval and what such a task could do to help overcome the present problems. A benchmarking event is needed for visual information retrieval to lower current barriers in retrieval performance. ImageCLEF can help to define such an event and identify areas where visual retrieval might be better than textual and vice-versa. The combination of visual and textual features together is another important field where research is needed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Visual retrieval of images has been an extremely active research area for more then ten years now <ref type="bibr" coords="1,112.30,600.67,10.55,9.96" target="#b4">[5,</ref><ref type="bibr" coords="1,124.53,600.67,11.69,9.96" target="#b15">16]</ref>. Still, there has not been neither a benchmarking event nor the use of standard datasets to compare the performance of several systems or techniques. Despite efforts such as the Benchathlon<ref type="foot" coords="1,162.00,623.83,3.95,4.85" target="#foot_0">1</ref>  <ref type="bibr" coords="1,170.04,624.67,10.55,9.96" target="#b5">[6]</ref> and several articles on evaluation <ref type="bibr" coords="1,334.28,624.67,10.55,9.96" target="#b7">[8,</ref><ref type="bibr" coords="1,346.39,624.67,12.82,9.96" target="#b10">11,</ref><ref type="bibr" coords="1,360.77,624.67,12.82,9.96" target="#b11">12,</ref><ref type="bibr" coords="1,375.16,624.67,11.69,9.96" target="#b16">17]</ref>, no common framework has been created, yet. This is different in textual information retrieval where several initiatives such as TREC<ref type="foot" coords="1,131.16,647.71,3.95,4.85" target="#foot_1">2</ref>  <ref type="bibr" coords="1,139.32,648.55,10.55,9.96" target="#b6">[7]</ref> (Text REtrieval conference) and CLEF<ref type="foot" coords="1,325.80,647.71,3.95,4.85" target="#foot_2">3</ref>  <ref type="bibr" coords="1,333.96,648.55,15.58,9.96" target="#b14">[15]</ref> (Cross Language Evaluation Forum) exist. In 2003, CLEF added a cross language image retrieval task <ref type="bibr" coords="1,377.35,660.55,10.55,9.96" target="#b0">[1]</ref> using a collection of historic photographs. The task in 2004 uses the same collection but adds an interactive and a medical task <ref type="bibr" coords="1,111.33,684.43,9.98,9.96" target="#b1">[2]</ref>. Figure <ref type="figure" coords="1,160.50,684.43,5.03,9.96" target="#fig_0">1</ref> shows a few examples from the St Andrews collection.</p><p>Images are annotated in English and query topics are formulated in another language containing a textual description of the query and an example image. English retrieval performance is taken as a baseline. The topics for which results can be submitted look as follows (a French example for image 1(a)):</p><p>&lt;title&gt; Portraits photographiques de pasteurs d'église par Thomas Rodger &lt;/title&gt; &lt;narr&gt; Les images pertinentes sont des portraits photographiques de pasteurs ou de leaders d'église pris par Thomas Ridger. Les images de nimporte quelle époque sont pertinentes, mais ne doivent montrer qu'une personne dans un studio, c'est-à-dire posant pour la photo. Des photos de groupes ne sont pas pertinentes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt;/narr&gt;</head><p>From this topic description we only took the image to start queries with our system, the textual information was discarded. No manual relevance feedback or automatic query expansion was used. This means that important information on the query task has not been obtained. With the visual information only, we do not know that we are searching for church ministers and we do not know who actually took the picture. Only a very good domain expert might be able to get this information from the image alone. Actually, all this information is only findable if the annotation is of a very high quality and is known to be complete. It has to be assured that all images with church ministers have these words in the text, otherwise we can not be sure whether the person is a church minister or might have a similar function. The producer (photographer) of the images also needs to be marked, otherwise a relevance judge would not be able to mark a result as relevant, although two images might be extremely similar in style. What about images where we do not have any name of the photographer but that look very similar to images from "Thomas Ridger"? What about collections with a mediocre text quality such as those that we often find in the real world, for example the Internet? Some retrieval tasks led to subjectively good results with a visual retrieval system whereas others did not manage to show any relevant images in the top 20 results. Figure <ref type="figure" coords="2,459.06,612.91,5.03,9.96" target="#fig_1">2</ref> shows one example result of a visual retrieval system. The first image is the query image and we can see that the same image was found as well as a few other images with the queen that apparently show the same scene.</p><p>Although this might look like a reasonable retrieval results, we can definitely tell that the system had no idea that we were looking for the queen or a military parade. The images were basically retrieved because they have very similar properties with respect to the grey levels contained, and especially with respect to the frame around the image. These images were most likely taken with the same camera and digitised with the same scanner. These properties can be found with a visual retrieval system. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Basic technologies used for the task</head><p>The technology used for the content-based image retrieval is mainly taken from the Viper<ref type="foot" coords="3,475.08,560.47,3.95,4.85" target="#foot_3">4</ref> project of the University of Geneva. Much information is available on the system <ref type="bibr" coords="3,419.84,573.31,14.67,9.96" target="#b17">[18]</ref>. Outcome of the Viper project is the GNU Image Finding Tool, GIFT <ref type="foot" coords="3,325.92,584.35,3.95,4.85" target="#foot_4">5</ref> . We used a version that slightly modifies the feature space and is called medGIFT<ref type="foot" coords="3,273.48,596.35,3.95,4.85" target="#foot_5">6</ref> as it was mainly developed for the medical domain. These software tools are open source and can consequently also be used by other participants of imageCLEF. Demonstration versions for participants were made available as well as not everybody can be expected to install an entire Linux tool for such a benchmarking event, only. The feature sets that are used by medGIFT are:</p><p>• Local colour features at different scales by partitioning the images successively four times into four subregions and taking the mode colour of each region as a feature;</p><p>• global colour features in the form of a colour histogram;</p><p>• local texture features by partitioning the image and applying Gabor filters in various scales and directions. Gabor responses are quantised into 10 strengths;</p><p>• global texture features represented as a simple histogram of the responses of the local Gabor filters in various directions and scales and with various strengths.</p><p>A particularity of GIFT is that it uses many techniques from text retrieval. Visual features are quantised/binarysed, and open a feature space that is very similar to the distribution of words in texts (similar to a Zipf distribution). A simple tf/idf weighting is used and the query weights are normalised by the results of the query itself. The histogram features are calculated based on a simple histogram intersection. This allows us to apply a variety of techniques that are common in text retrieval to the retrieval of images. Experiments show that especially relevance feedback queries on images are much better using this feature space whereas one-shot queries might be done more performant with other techniques.</p><p>3 Runs submitted for evaluation</p><p>Unfortunately, there was not enough time this year to submit a mixed visual and textual run for imageCLEF but we are working on this for next year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Only visual retrieval with one query image</head><p>For the visual queries, the medGIFT system was used. This system allows to fairly easy change a few system parameters such as the configuration of the Gabor filters and the grey level and colour quantisations. Input for these queries were only the query images. No feedback or automatic query expansion was used. The following system parameters were submitted:</p><p>• 18 hues, 3 saturations, 3 values, 4 grey levels, 4 directions and 3 scales of the Gabor filters, the GIFT base configuration made available to all participants of imageCLEF; (GE 4g 4d vis)</p><p>• 9 hues, 2 saturations, 2 values, 16 grey levels, 4 directions and 5 scales of the Gabor filters.</p><p>(GE 16g 4d vis) Some queries delivered surprisingly good results but this was not due to a recognition of image features with respect to the topic but rather due to the fact that images from a relevance set were taken at a similar time and have a very similar appearance. Content-based image retrieval can help to retrieve images that were taken with the same camera or scanned with the same scanner if they are similar with respect to their colour properties. Mixing text and visual features for retrieval will need a fair amount of work to optimise parameters and really receive good results. The evaluation results show the very low performance of all visual only runs that were submitted. Mean average precision (MAP) is 0.0919 for the GIFT base system and 0.0625 for the modified version. It is actually surprising that the system with only four grey levels performed better than a system having a larger number. Most of the images are in grey and brown tones so we expected to obtain better results when giving more flexibility to this aspect. It will have to be show whether other techniques might obtain better results such as a normalisation of the images or even a change of the brown tones into grey tones to make images better comparable. Still, these results will be far away from the best systems that reach a MAP of 0.5865 such as the Daedalus system suing text retrieval only. Several systems include some visual information into the retrieval and some of these systems are indeed ranked high. All systems that relied on visual features, only, receive fairly bad results, in general the worst results in the competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Techniques to improve visual retrieval results</head><p>Some techniques might be of help to further increase the performance of the retrieval results. One such techniques is a pre-processing of images to bring all images to a standard grey level distribution and maybe removing colour completely. At least the brown levels should be changed to grey levels so images can be retrieved based on real content and not based on general appearance.</p><p>Another possibility is the change of the colour space of the image. Several spaces have been analysed with respect to invariance regarding lighting conditions with good results <ref type="bibr" coords="5,460.60,147.31,9.98,9.96" target="#b3">[4]</ref>. For the tasks of imageCLEF it might be useful to reduce the number of colours and slightly augment the number of grey levels for best retrieval. Some form of normalisation could also be used as some images used the entire grey spectrum whereas others only use an extremely limited number of grey levels. A proper evaluation will have to show what actually works best.</p><p>Mixed visual/textual strategies can lead to a better result. If in a first step only the textual information is taken as a query and then the first N images are visually fed back to the system the results can be much better and can manage to find images that are without text or with a bad annotation and that would not have been found otherwise. More research is definitely needed on mixed textual/visual strategies for retrieval to find out which influence each one can have. It might also be possible to have a small influence of the visually most similar images in a first query step as well but the text will need to be the dominating factor for best results as the query topics are semantics-based.</p><p>4 How to make the queries more appealing to visual retrieval research groups?</p><p>Although CLEF is on cross-language retrieval and thus mainly on text, image information should exploited in this context for the retrieval of visual data. Images are inherently language-independent and they can provide important additional information for cross-language retrieval tasks. To foster these developments it might even be the best to have an entirely visual task to attract the content-based retrieval community and later come back to a combination of visual/textual techniques. This can also help to develop partnerships between visual and textual retrieval groups to submit common runs for such a benchmark. Techniques for visual information retrieval are currently not good enough to respond properly to semantic tasks <ref type="bibr" coords="5,171.16,459.07,9.98,9.96" target="#b2">[3]</ref>. Sometimes the results look indeed good but this is most often linked to secondary parameters and not really to the semantic concepts being searched for or the low-level features being used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">More visual information for the current topics</head><p>The easiest way to make the St. Andrews cross-language retrieval task more attractive to visual retrieval groups is simply to supply more visual information as task description. Having three to five example images instead of one might help visual retrieval significantly as systems can search for the really important information that these images have in common. A single image for retrieval is a little bit "a shot in the dark" but several images do supply a fair amount of information.</p><p>Besides positive examples, an important improvement would be to supply several negative examples to have an idea of what not to look for. Negative relevance feedback has shown to be extremely important in visual information retrieval <ref type="bibr" coords="5,331.24,613.03,15.58,9.96" target="#b9">[10]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Topics based on the visual "appearance" of an image</head><p>It has been discussed a lot what visual image retrieval cannot do but there are quite a few things that visual image retrieval can indeed do. Although search on semantics seems currently infeasible, similarity based on the appearance of the images can be obtained in a fairly good quality. Visual appearance is often described as a first impression of an image or preattentive similarity of images <ref type="bibr" coords="6,178.24,111.43,14.67,9.96" target="#b13">[14]</ref>. Tasks can also contain fairly easy semantics that are basically modelled by the visual appearance. Possible topics could be:</p><p>• Sun sets -modelled by a yellow round object somewhere in the middle and mainly variations of red.</p><p>• Mountain views -upper part blue and in the middle sharp changes, in grey/white tones, bottom sometimes/often green.</p><p>• Beach -Lower part yellow and the upper part in blue with a clear line between the two.</p><p>• City scenes -very symmetric structures with a large number of horizontal lines and right angles.</p><p>It will need to be analysed whether these queries do actually respond to what real users are looking for in retrieval systems, but they have the potential to attract a much larger number of visual information retrieval groups to participate and compare their techniques in such a benchmarking event.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Easy semantic topics</head><p>TRECVID<ref type="foot" coords="6,137.16,327.67,3.95,4.85" target="#foot_6">7</ref> introduced in 2003 several topics for video retrieval that can also be used for visual image retrieval, maybe with slight variations. These are fairly easy semantic topics such as finding out whether there are people in images. Some examples for topics are:</p><p>• People: segment contains at least three humans.</p><p>• Building: segment contains a building. Buildings are walled structures with a roof.</p><p>• Road: segment contains part of a road -any size, paved or not.</p><p>• Vegetation: segment contains living vegetation in its natural environment.</p><p>• Animal: segment contains an animal other than a human . ImageCLEF could define topics similar in style for the image collections being available (topics that actually do correspond to the images in the collection). Retrieval systems can then try to find as many of the images with respect to the topic as possible based on visual features only or based on visual and textual features. This could also help to find out the influence of text and visual information on fairly low-level semantic concepts.</p><p>This can especially stimulate the creation of simple detectors for simple semantic concepts. These detectors can later be combined for the retrieval of higher-level semantic retrieval, so they do deliver important intermediary results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">An easier image collection</head><p>The St. Andrews collection is definitely a very hard collection for purely visual analysis. The images do not contain many clearly separated objects and the small amount of colour pictures and variances in sharpness/quality make automatic analysis extremely hard. Other collections such as the Corel Photo CDs are much easier for automatic analysis and query/retrieval <ref type="bibr" coords="6,475.55,623.47,9.98,9.96" target="#b8">[9]</ref>. This collection contains 100 images each for a large number of topics (tigers, planes, eagles, ...). Often the collections have a distinct object in each of the sets, sometimes the sets also correspond to regions (Paris, California, Egypt, ...). Only problem might be to get a collection without to strong copyright constraints. As the Corel Photo CDs are not sold anymore, this might be a possibility if Corel agrees to make the images in a lower resolution available to participants. The Corbis<ref type="foot" coords="6,508.68,682.39,3.95,4.85" target="#foot_7">8</ref> image archive also offers a limited selection of around 15.000 images for research purposes that are annotated in a hierarchical code. Such a collection might be an easier topic for visual and combined visual/textual retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Interactive tasks evaluated by users</head><p>A different idea is the evaluation of interactive systems based on real users performing queries. Normally, image retrieval is not extremely good in a first query step but with feedback, very good results can be obtained <ref type="bibr" coords="7,199.24,153.79,15.58,9.96" target="#b9">[10,</ref><ref type="bibr" coords="7,216.38,153.79,11.69,9.96" target="#b12">13]</ref>. Similar to the interactive task using text introduced in 2004 we can imagine a task with only a visual query description with an example image. Users can subsequently perform queries until they are satisfied with the results. Evaluation could be done directly by the users, for example by counting how many relevant images they found with which system, and how many refinement steps were necessary to find a satisfactory result. It has to be stated that the user satisfaction can vary considerable with respect to his knowledge of the content of the database. When not knowing anything about the total number of relevant images, users tend to be satisfied fairly easily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>This article explained a simple submission to the imageCLEF task using the St. Andrews historical image collection. The two submitted runs were based on visual features of the images only, without using the text supplied for the queries. No other techniques were used such as manual relevance feedback or automatic query expansion. The results show the problems of purely visual image retrieval: no semantics are currently included in the visual low-level features and as a consequence the performance is low.</p><p>Still, visual information retrieval based on low-level non-semantic features can be an important part in the general information retrieval picture. Visual information retrieval can be used to find images with a similar visual appearance or with simple semantic concepts if learning data for these concepts are available. Thus, it is important for evaluation events such as imageCLEF to create topics that are more suitable to visual retrieval groups and that correspond to desires of real users as well. Visual and textual retrieval need to be brought together with overlapping retrieval tasks to find out where each one works best and where the two can be combined for optimal results. Currently, there is no experience in this domain, hence the importance of benchmarking events such as imageCLEF but also the creation of retrieval tasks suitable for visual retrieval. This article gives a few ideas on how to make the imageCLEF task more appealing for visual retrieval groups. Hopefully, these changes will be able to attract more attention in the visual retrieval community so people start working on the same data sets and start comparing systems and techniques. To advance retrieval systems, a critical evaluation and comparison of existing systems is currently more needed than new techniques. ImageCLEF might be an important factor in advancing information retrieval and especially visual information retrieval.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,166.80,242.35,269.30,9.96;2,224.76,120.15,62.36,77.95"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Some example images of the St. Andrews database.</figDesc><graphic coords="2,224.76,120.15,62.36,77.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,144.12,507.67,314.59,9.96;3,131.40,118.93,340.16,368.37"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example for a "good" query result based on visual properties.</figDesc><graphic coords="3,131.40,118.93,340.16,368.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,350.18,613.03,162.57,9.96;5,90.00,624.91,422.62,9.96;5,90.00,636.91,422.63,9.96;5,90.00,648.91,205.77,9.96"><head></head><label></label><figDesc>and feedback with negative examples substantially changes the result sets whereas positive examples only do a slight reordering of the highest-ranked results. Finding three to five negative examples per query task in addition to the positive examples should not be a big problem.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,105.24,727.77,113.31,7.35"><p>http://www.benchathlon.net/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,105.24,737.37,88.04,7.35"><p>http://trec.nist.gov/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="1,105.24,746.85,121.82,7.35"><p>http://www.clef-campaign.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="3,105.24,716.25,88.04,7.35"><p>http://viper.unige.ch</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="3,105.24,725.85,138.71,7.35"><p>http://www.gnu.org/software/gift/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="3,105.24,735.33,134.51,7.35"><p>http://www.sim.hcuge.ch/medgift/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="6,105.24,737.49,180.99,7.35"><p>http://www-nlpir.nist.gov/projects/trecvid/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="6,105.24,746.97,92.23,7.35"><p>http://www.corbis.com/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,110.51,586.03,401.97,9.96;7,110.52,598.03,324.05,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,238.10,586.03,206.19,9.96">The clef 2003 cross language image retrieval task</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,462.98,586.03,49.50,9.96;7,110.52,598.03,238.74,9.96">Proceedings of the Cross Language Evaluation Forum (CLEF 2004)</title>
		<meeting>the Cross Language Evaluation Forum (CLEF 2004)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>submitted</note>
</biblStruct>

<biblStruct coords="7,110.51,617.95,402.25,9.96;7,110.52,629.95,401.92,9.96;7,110.52,641.83,153.71,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,287.63,617.95,225.13,9.96;7,110.52,629.95,73.36,9.96">A proposal for the clef cross language image retrieval track (imageclef)</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,224.27,629.95,248.28,9.96">The Challenge of Image and Video Retrieval (CIVR 2004)</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer LNCS</publisher>
			<date type="published" when="2004-07">2004. July 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.51,661.75,402.00,9.96;7,110.52,673.75,402.15,9.96;7,110.52,685.75,324.52,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,176.33,661.75,265.49,9.96">Benchmarks for storage and retrieval in multimedia databases</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,461.81,661.75,50.70,9.96;7,110.52,673.75,130.04,9.96;7,319.32,673.75,74.76,9.96">Storage and Retrieval for Media Databases</title>
		<title level="s" coord="7,284.86,685.75,141.09,9.96">SPIE Photonics West Conference</title>
		<meeting><address><addrLine>San Jose, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">January 21-22 2002</date>
			<biblScope unit="volume">4676</biblScope>
			<biblScope unit="page" from="240" to="247" />
		</imprint>
	</monogr>
	<note>SPIE Proceedings</note>
</biblStruct>

<biblStruct coords="7,110.51,705.67,402.37,9.96;7,110.52,717.55,401.96,9.96;7,110.52,729.55,251.28,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,276.36,705.67,236.52,9.96;7,110.52,717.55,110.02,9.96">A comparative study of several color models for color image invariants retrieval</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,242.76,717.55,264.30,9.96">Proceedings of the First International Workshop ID-MMS&apos;96</title>
		<meeting>the First International Workshop ID-MMS&apos;96<address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-08">August 1996</date>
			<biblScope unit="page" from="17" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.51,111.43,402.00,9.96;8,110.52,123.43,189.51,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,178.51,111.43,275.33,9.96">Image information retrieval: An overview of current research</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Goodrum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,466.49,111.43,46.03,9.96;8,110.52,123.43,127.23,9.96">Journal of Information Science Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.51,143.35,402.28,9.96;8,110.52,155.35,402.25,9.96;8,110.52,167.23,112.53,9.96" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="8,252.72,143.35,260.06,9.96;8,110.52,155.35,117.59,9.96">A benchmark for image retrieval using distributed systems over the internet: BIRDS-I</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">J</forename><surname>Gunther</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Beretta</surname></persName>
		</author>
		<idno>HPL- 2000-162</idno>
		<imprint>
			<date type="published" when="2001">2001</date>
			<pubPlace>Palo Alto; San Jose</pubPlace>
		</imprint>
		<respStmt>
			<orgName>HP Labs</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="8,110.51,187.15,401.97,9.96;8,110.52,199.15,386.96,9.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,167.95,187.15,259.24,9.96">Overview of the first Text REtrieval Conference (TREC-1)</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,450.99,187.15,61.48,9.96;8,110.52,199.15,199.76,9.96">Proceedings of the first Text REtrieval Conference (TREC-1)</title>
		<meeting>the first Text REtrieval Conference (TREC-1)<address><addrLine>Washington DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.51,219.07,402.29,9.96;8,110.52,230.95,401.99,9.96;8,110.52,242.95,402.23,9.96;8,110.52,254.95,98.17,9.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,197.94,219.07,249.18,9.96">Benchmarking for content-based visual information search</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,144.09,230.95,363.27,9.96">Fourth International Conference On Visual Information Systems (VISUAL&apos;2000)</title>
		<title level="s" coord="8,180.55,242.95,151.34,9.96">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Laurini</surname></persName>
		</editor>
		<meeting><address><addrLine>Lyon, France</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1929-11">1929. November 2000</date>
			<biblScope unit="page" from="442" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.51,274.87,402.31,9.96;8,110.52,286.75,401.92,9.96;8,110.52,298.75,231.84,9.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,316.91,274.87,195.91,9.96;8,110.52,286.75,34.73,9.96">The truth about corel -evaluation in image retrieval</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Marchand-Maillet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Pun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,163.99,286.75,348.45,9.96;8,110.52,298.75,97.86,9.96">Proceedings of the International Conference on the Challenge of Image and Video Retrieval (CIVR 2002)</title>
		<meeting>the International Conference on the Challenge of Image and Video Retrieval (CIVR 2002)<address><addrLine>London, England</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-07">July 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.50,318.67,402.32,9.96;8,110.52,330.67,402.14,9.96;8,110.52,342.55,402.01,9.96;8,110.52,354.55,402.27,9.96;8,110.52,366.55,132.50,9.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,432.51,318.67,80.32,9.96;8,110.52,330.67,247.18,9.96">Strategies for positive and negative relevance feedback in image retrieval</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Squire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Marchand-Maillet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Pun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,410.77,342.55,101.76,9.96;8,110.52,354.55,270.50,9.96">Proceedings of the 15th International Conference on Pattern Recognition (ICPR 2000)</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Sanfeliu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Villanueva</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vanrell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Alcézar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-O</forename><surname>Eklundh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Aloimonos</surname></persName>
		</editor>
		<meeting>the 15th International Conference on Pattern Recognition (ICPR 2000)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000-09">September 2000</date>
			<biblScope unit="page" from="1043" to="1046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.50,386.47,402.31,9.96;8,110.52,398.35,402.22,9.96;8,110.52,410.35,114.93,9.96" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,424.35,386.47,88.47,9.96;8,110.52,398.35,271.25,9.96">Performance evaluation in content-based image retrieval: Overview and proposals</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Squire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Marchand-Maillet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Pun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,390.34,398.35,118.32,9.96">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="593" to="601" />
			<date type="published" when="2001-04">April 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.50,430.27,402.32,9.96;8,110.52,442.27,228.68,9.96" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,347.83,430.27,160.48,9.96">Benchmarking multimedia databases</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">D</forename><surname>Narasimhalu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,110.52,442.27,149.36,9.96">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="333" to="356" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.50,462.19,402.26,9.96;8,110.52,474.07,401.97,9.96;8,110.52,486.07,402.08,9.96;8,110.52,497.95,123.72,9.96" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,327.30,462.19,185.45,9.96;8,110.52,474.07,157.01,9.96">Relevance feedback: A power tool for interactive content-based image retrieval</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mehrotra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,275.71,474.07,236.78,9.96;8,110.52,486.07,45.32,9.96">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="644" to="655" />
			<date type="published" when="1998-09">September 1998</date>
		</imprint>
	</monogr>
	<note>Special Issue on Segmentation, Description, and Retrieval of Video Content</note>
</biblStruct>

<biblStruct coords="8,110.50,517.99,402.19,9.96;8,110.52,529.87,402.18,9.96;8,110.52,541.87,193.61,9.96" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="8,223.97,517.99,267.29,9.96">Gabor space and the development of preattentive similarity</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Santini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,110.52,529.87,369.00,9.96">Proceedings of the 13th International Conference on Pattern Recognition (ICPR&apos;96)</title>
		<meeting>the 13th International Conference on Pattern Recognition (ICPR&apos;96)<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996-08">August 1996</date>
			<biblScope unit="page" from="40" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.50,561.79,402.07,9.96;8,110.52,573.67,402.32,9.96" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="8,153.29,561.79,140.94,9.96">Report on clef-2001 experiments</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,315.86,561.79,196.71,9.96;8,110.52,573.67,123.34,9.96">Report on the CLEF Conference 2001 (Cross Language Evaluation Forum)</title>
		<meeting><address><addrLine>Darmstadt, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer LNCS</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page">2406</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.50,593.59,402.33,9.96;8,110.52,605.59,401.84,9.96;8,110.52,617.59,170.89,9.96" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="8,419.56,593.59,93.27,9.96;8,110.52,605.59,160.64,9.96">Content-based image retrieval at the end of the early years</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Santini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,279.43,605.59,232.93,9.96;8,110.52,617.59,48.01,9.96">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1349" to="1380" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.50,637.51,401.99,9.96;8,110.52,649.39,402.18,9.96;8,110.52,661.39,22.88,9.96" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="8,173.96,637.51,114.01,9.96">Image retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,313.63,637.51,198.86,9.96;8,110.52,649.39,176.36,9.96">IEEE Workshop on Content-based Access of Image and Video Libraries (CBAIVL&apos;98)</title>
		<meeting><address><addrLine>Santa Barbara, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-06-21">June 21 1998</date>
			<biblScope unit="page" from="112" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.50,681.31,402.29,9.96;8,110.52,693.31,401.95,9.96;8,110.52,705.19,374.10,9.96" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="8,331.77,681.31,181.02,9.96;8,110.52,693.31,133.05,9.96">Content-based query of image databases: inspirations from text retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Squire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Pun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,379.68,693.31,132.79,9.96;8,110.52,705.19,246.34,9.96">Selected Papers from The 11th Scandinavian Conference on Image Analysis SCIA &apos;99)</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1193" to="1198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,492.18,705.19,20.38,9.96;8,110.52,717.19,114.16,9.96" xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">K</forename><surname>Ersboll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Johansen</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
