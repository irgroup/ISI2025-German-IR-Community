<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,71.16,75.73,453.01,12.19">NCTU-ISU&apos;s Evaluation for the User-Centered Search Task at ImageCLEF 2004</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,112.32,108.93,70.84,8.74"><forename type="first">Pei-Cheng</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer &amp; Information Science</orgName>
								<orgName type="institution">National Chiao Tung University</orgName>
								<address>
									<addrLine>1001 Ta Hsueh Rd</addrLine>
									<postCode>30050</postCode>
									<settlement>Hsinchu</settlement>
									<region>O.C</region>
									<country>TAIWAN, R</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,191.10,108.93,54.98,8.74"><forename type="first">Jen-Yuan</forename><surname>Yeh</surname></persName>
							<email>jyyeh@cis.nctu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer &amp; Information Science</orgName>
								<orgName type="institution">National Chiao Tung University</orgName>
								<address>
									<addrLine>1001 Ta Hsueh Rd</addrLine>
									<postCode>30050</postCode>
									<settlement>Hsinchu</settlement>
									<region>O.C</region>
									<country>TAIWAN, R</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,253.98,108.93,50.28,8.74"><forename type="first">Hao-Ren</forename><surname>Ke</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">University Library</orgName>
								<orgName type="institution" key="instit2">National Chiao Tung University</orgName>
								<address>
									<addrLine>1001 Ta Hsueh Rd</addrLine>
									<postCode>30050</postCode>
									<settlement>Hsinchu</settlement>
									<region>O.C</region>
									<country>TAIWAN, R</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,312.52,108.93,74.12,8.74"><forename type="first">Been-Chian</forename><surname>Chien</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">I-Shou University 1</orgName>
								<address>
									<addrLine>Sec. 1, Hsueh Cheng Rd., Ta-Hsu Hsiang</addrLine>
									<postCode>84001</postCode>
									<settlement>Kaohsiung</settlement>
									<country>TAIWAN, R.O</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National University of Tainan</orgName>
								<address>
									<addrLine>33, Sec. 2, Su Line St</addrLine>
									<postBox>O.C</postBox>
									<postCode>70005</postCode>
									<settlement>Tainan</settlement>
									<country>TAIWAN, R</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,413.61,108.93,62.01,8.74"><forename type="first">Wei-Pang</forename><surname>Yang</surname></persName>
							<email>wpyang@cis.nctu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer &amp; Information Science</orgName>
								<orgName type="institution">National Chiao Tung University</orgName>
								<address>
									<addrLine>1001 Ta Hsueh Rd</addrLine>
									<postCode>30050</postCode>
									<settlement>Hsinchu</settlement>
									<region>O.C</region>
									<country>TAIWAN, R</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Department of Information Management</orgName>
								<orgName type="institution">National Dong Hwa University</orgName>
								<address>
									<addrLine>1, Sec. 2, Da Hsueh Rd., Shou-Feng</addrLine>
									<postCode>97401</postCode>
									<settlement>Hualien</settlement>
									<region>O.C</region>
									<country>TAIWAN, R</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,71.16,75.73,453.01,12.19">NCTU-ISU&apos;s Evaluation for the User-Centered Search Task at ImageCLEF 2004</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0EA52D1F307780728DB502C8F16EA7DA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Interactive search</term>
					<term>Cross-Language image retrieval</term>
					<term>Relevance feedback</term>
					<term>User behavior</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We participated in the user-centered search task at ImageCLEF 2004. In this paper, we proposed two interactive Cross-Language image retrieval systems -T_ICLEF and VCT_ICLEF. The first one is implemented with a practical relevance feedback approach based on textual information while the second one combines textual and image information to help users find a target image. The experimental results show that VCT_ICLEF has a better performance than T_ICLEF in almost all cases. Overall, VCT_ICLEF helps users find the image within a fewer iterations with a maximum of 2 iterations saved.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ImageCLEF campaign under the CLEF<ref type="foot" coords="1,256.32,478.93,3.24,5.65" target="#foot_0">1</ref> (Cross-Language Evaluation Forum) is conducting a series of evaluations on systems which are built to accept a query in a language and to find relevant images with captions in different languages. In this year, three tasks<ref type="foot" coords="1,258.48,501.91,3.24,5.65" target="#foot_1">2</ref> are proposed based on different domains, scenarios, and collections. They are 1) the bilingual ad hoc retrieval task, 2) the medical retrieval task, and 3) the user-centered search task. The first is to perform bilingual retrieval against a photographic collection in which images are accompanied with captions in English. The second is, given an example image, to find out similar images from a medical image database which consists of images such as scans and x-rays. The last one aims to assess user interaction for a known-item or target search. This paper concentrates on the user-centered search task. The task follows the scenario that a user is searching with a specific image in mind, but without any key information about it. Previous work (e.g, ) has shown that interactive search helps improve recall and precision in the retrieval task. However, in this paper, the goal is to determine whether the retrieval system is being used in the manner intended by the designers as well as to determine how the interface helps users reformulate and refined their search topics. We proposed two systems: 1) T_ICLEF, and 2) VCT_ICLEF to address the task. T_ICLEF is a Cross-Language image retrieval system, which is enhanced with the relevance feedback mechanism; VCT_ICLEF is practically T_ICLEF but provides a color table which allows users to indicate color information about the target image.</p><p>In the following sections, the design of our systems is first described. We then introduce the proposed methods for the interactive search task, and present our results. Finally, we finish with a conclusion and a discus-sion of future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview of the Interactive Search Process</head><p>The overview of the interactive search process is shown in Fig. <ref type="figure" coords="2,350.26,131.85,3.77,8.74" target="#fig_0">1</ref>. Given an initial query, ) , (</p><formula xml:id="formula_0" coords="2,457.74,128.37,46.49,14.02">I T Q Q Q =</formula><p>, in which Q T denotes a Chinese text query, and Q I stands for a query image, the system performs the Cross-Language image retrieval, and returns a set of "relevant" images to the user. The user then evaluates the relevance of the returned images, and gives a relevance value to each of them. The process is called relevance feedback. In our system, the user is only able to indicate as an image "non-relevant," "neutral," and "relevant." At the following stage, the system invokes the query reformulation process to derive a new query, ) , (</p><formula xml:id="formula_1" coords="2,72.42,200.25,49.09,14.62">I T Q Q Q ′ ′ = ′</formula><p>. The new query is believed to be more corresponding to the user's need. Finally, the system performs again image retrieval according to Q′ . The process iterates until the user finds the target image. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Cross-Language Image Retrieval</head><p>In this section, we describe how to create the representation for an image or a query, and how to compute the similarity between an image and the query on the basis of their representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Image/ Query Representations</head><p>We create both an image and a query representation by representing them as a vector in the vector space model <ref type="bibr" coords="2,70.92,507.51,40.69,8.74" target="#b4">[Salton83]</ref>. First of all, we explain the symbols used in the following definitions of representations. ) , (</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I T P P P =</head><p>denotes an image where P T and P I stand for the captions of P and the image P respectively, and ) , (</p><formula xml:id="formula_2" coords="2,72.42,531.33,46.55,14.02">I T Q Q Q =</formula><p>represents a query, which is defined as mentioned before. In our proposed approach, a textual vector representation, such as P T and Q T , is modeled in terms of three distinct features -term, category, and temporal information while an image vector representation, for example, P I and Q I , is represented with color histogram.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Textual Vector Representation</head><p>Let W (|W| = n) the set of significant keywords in the corpus, C (|C| = m) the set of categories which are defined in the corpus, and Y (|Y| = k) the set of publication years of all images, for an image P, it textual vector representation (i.e., P T ) is defined as Eq. (1), (1) where the first n dimensions the weighting of a keyword t i in P T , which is measured by TF-IDF <ref type="bibr" coords="2,468.12,693.51,40.61,8.74" target="#b4">[Salton83]</ref>, as computed in Eq. (2); the following n+1 to n+m dimensions indicate whether P belongs to a category c i , which is shown as Eq. (3); the final n+m+1 to n+m+k dimensions present whether P was published in y i , which is defined as Eq. (4). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query</head><formula xml:id="formula_3" coords="2,108.48,248.80,380.73,14.61">I T Q Q Q = ) , ( I T Q Q Q ′ ′ = ′ i T i i t P t T t n N tf tf P w log max ) ( , × =</formula><p>(2) In the above, we introduce how to create a textual vector representation for P T . As for a query Q, one problem is that since Q T is given in Chinese, it is necessary to translate Q T into English, which is the language used in the image collection. We first perform the word segmentation process to obtain a set of Chinese words. For each Chinese word, it is then translated into one or several corresponding English words by looking up it in a dictionary. The dictionary that we use is pyDict 3 . Up to now, it is hard to determine the translation as correctly as possible. We tend to keep all English translations in order not to lose the consideration of any correct word.</p><formula xml:id="formula_4" coords="3,98.46,106.00,420.34,62.44">⎩ ⎨ ⎧ = otherwise 0 , to belongs if 1 ) ( i T c c P P w i (3) ⎩ ⎨ ⎧ =</formula><p>Another problem is the so-called short query problem. A short query usually can not cover as many useful search terms as possible because of the lack of sufficient words. We address this problem by performing the query expansion process to add new terms to the original query. The additional search terms is taken from a thesaurus -WordNet <ref type="bibr" coords="3,146.07,399.99,40.09,8.74" target="#b2">[Miller95]</ref>. For each English translation, we include its synonyms, hypernyms, and hyponyms into the query. Now, it comes out a new problem. Assume } ,..., { ) (</p><formula xml:id="formula_5" coords="3,275.28,434.38,124.50,14.01">1 h T e e Q sion AfterExpan =</formula><p>the set of all English words obtained after query translation and query expansion, it is obvious that</p><formula xml:id="formula_6" coords="3,365.40,452.87,81.51,10.52">) ( T Q sion AfterExpan</formula><p>may contain a lot of words which are not correct translations or useful search terms. To resolve the translation ambiguity problem, we exploit word co-occurrence relationships to determine final query terms. If the co-occurrence frequency of e i and e j in the corpus is greater than a predefined threshold, both e i and e j are regarded as useful search terms for monolingual image retrieval. So far, we have a set of search terms,</p><formula xml:id="formula_7" coords="3,348.78,502.37,93.45,10.52">) ( T Q biguity AfterDisam</formula><p>, which is presented as Eq. ( <ref type="formula" coords="3,101.44,516.51,3.55,8.74" target="#formula_8">5</ref>),</p><formula xml:id="formula_8" coords="3,99.18,534.28,419.61,30.74">} ce cooccurren t significan a have , &amp; ) ( , | , { ) ( j i T j i j i T e e Q sion AfterExpan e e e e Q biguity AfterDisam ∈ =<label>(5)</label></formula><p>After giving the definition of</p><formula xml:id="formula_9" coords="3,216.00,579.83,93.51,10.52">) ( T Q biguity AfterDisam</formula><p>, for a query Q, its textual vector representation (i.e., Q T ) is defined in Eq. ( <ref type="formula" coords="3,159.18,593.97,3.54,8.74">6</ref>),</p><formula xml:id="formula_10" coords="3,70.92,611.90,453.55,57.95">&gt; =&lt; ) ( ),..., ( ), ( ),..., ( ), ( ),..., ( 1 1 1 T y T y T c T c T t T t T Q w Q w Q w Q w Q w Q w Q k m n (6) where ) ( T t Q w i is the weighting of a keyword t i in Q T , which is measured as Eq. (7), ) ( T c Q w i indicates whether there exists an ) ( T j Q biguity AfterDisam e ∈</formula><p>and it also occurs in a category c i , which is shown as Eq.</p><p>(8), and</p><formula xml:id="formula_11" coords="3,109.32,672.88,273.03,15.32">) ( T y Q w i presents whether there is an ) ( T j Q biguity AfterDisam e ∈</formula><p>, e j is a temporal term, and e j satisfies a condition caused by a predefined temporal operator.</p><p>3 An English/Chinese dictionary written by D. Gau, which is available at http://sourceforge.net/projects/pydict/.</p><p>In Eq. ( <ref type="formula" coords="4,123.06,82.35,3.54,8.74">7</ref>), W is the set of significant keywords as defined before,</p><formula xml:id="formula_12" coords="4,70.92,74.33,453.63,40.58">tf tf T i Q t max , stands for the normalized fre- quency of t i in ) ( T Q biguity AfterDisam</formula><p>, maxtf is the maximum number of occurrences of any keyword in</p><formula xml:id="formula_13" coords="4,73.56,119.33,93.51,10.52">) ( T Q biguity AfterDisam</formula><p>, N indicates the number of images in the corpus, and i t n denotes the number of images in whose caption t i appears. Similar to Eq. (3) and Eq. (4), both Eq. (8) and Eq. ( <ref type="formula" coords="4,421.27,135.51,3.89,8.74" target="#formula_14">9</ref>) compute the weighting of the category and the temporal feature as a Boolean value.</p><formula xml:id="formula_14" coords="4,98.46,167.00,420.34,129.67">⎪ ⎩ ⎪ ⎨ ⎧ × = i T i i t Q t T t n N tf tf Q w log max ) ( , (7) ⎩ ⎨ ⎧ ∈ ∃ = otherwise 0 , in occurs and ) ( , if 1 ) ( i j T j T c c e Q biguity AfterDisam e j Q w i (8) ⎪ ⎪ ⎩ ⎪ ⎪ ⎨ ⎧ = otherwise 0 Y, before is and , Y contains if 1 Y, before is and , Y " contains if 1 Y, before is and , Y contains if 1 ) ( i T i T i T T y y " " Q y " Q y " " Q Q w i 年以後 年之中 年以前<label>(9)</label></formula><p>To be mentioned, with regard to ) ( T y Q w i , three operators -BEFORE, IN, and AFTER -are defined to take into account a query such as "1900 年以前拍攝的愛丁堡城堡的照片 (Pictures of Edinburgh Castle taken before 1900)," which also concerns about the time dimension. Take, for example, the above query which targets only images taken before 1900; a part of the textual vector of the above query about the temporal feature is given in Table <ref type="table" coords="4,107.16,362.49,3.77,8.74" target="#tab_3">1</ref>, it gives an idea that P 1 will be retrieved since its publication year was in 1899 while P 2 will not be retrieved because of its publication year, 1901. Note that in this year, we only consider years for the temporal feature. Hence, for a query like "1908 年四月拍攝的羅馬照片 (Photos of Rome taken in April 1908)," "四月 (April)" is treated as a general term, which only contributes its effect to the term feature. </p><formula xml:id="formula_15" coords="4,89.46,454.47,413.49,40.50">P 0 0 0 1 0 0 0 0 2 P 0 0 0 0 0 1 0 0 Q T 1 1 1 1 0 0 0 0</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Vector Representation</head><p>Color histogram <ref type="bibr" coords="4,141.10,537.03,41.80,8.74" target="#b5">[Swain91]</ref> is a basic method and has good performance for representing image content. The color histogram method gathers statistics about the proportion of each color as the signature of an image. In our work, the colors of an image are represented in the HSV (Hue/ Saturation/ Value) space, which is believed closer to human perception than other models, such as RGB (Red/ Green/ Blue) or CMY (Cyan/ Magenta/ Yellow). We quantize the HSV space into 18 hues, 2 saturations, and 4 values, with additional 4 levels of gray values; as a result, there are a total of 148 (i.e., 18×2×4+4) bins. Let C (|C| = m) a set of colors (i.e., 148 bins), P I (Q I ) is represented as Eq. ( <ref type="formula" coords="4,141.32,607.53,7.66,8.74">10</ref>), which models the color histogram H(P I ) (H(Q I )) as a vector, in which each bucket i c h counts the ratio of pixels of</p><formula xml:id="formula_16" coords="4,98.04,623.73,420.76,50.19">P I (Q I ) in color c i . &gt; =&lt; ) ( ),..., ( 1 I c I c I P h P h P m , &gt; =&lt; ) ( ),..., ( 1 I c I c I Q h Q h Q m (10)</formula><p>Previous work models that each pixel is only assigned into a single color. Consider the following situation: I 1 , I 2 are two images, all pixels of I 1 and I 2 fall into c i and c i+1 respectively; I 1 and I 2 are indeed similar to each other, but the similarity computed by the color histogram will regard them as different images. To address the problem, we set an interval range δ to extend the color of each pixel and introduce the idea of a partial pixel as shown in Eq. ( <ref type="formula" coords="4,129.75,734.01,7.51,8.74">11</ref> . It is clear that a pixel has its contributions not only to c i but also to its neighboring bins.</p><p>Fig. <ref type="figure" coords="5,209.56,349.94,3.77,8.74">2</ref>. The illustration of the idea of the partial pixel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Similarity Metric</head><p>While a query ) , (</p><formula xml:id="formula_17" coords="5,134.40,404.78,46.43,14.03">I T Q Q Q =</formula><p>and an image ) , (</p><formula xml:id="formula_18" coords="5,249.42,404.78,42.17,14.03">I T P P P =</formula><p>are represented in terms of a textual and an image vector representation, we proposed two strategies to measure the similarity between the query and each image in the collection. In the following, we briefly describe the proposed strategies: Strategy 1, which is exploited in the system, T_ICLEF, only takes into account the textual similarity while Strategy 2 4 , which combines the textual and the image similarity, is employed in the system, VCT_ICLEF. </p><formula xml:id="formula_19" coords="5,98.04,511.34,420.76,59.57">T y T y T c T c T t T t T Q w Q w Q w Q w Q w Q w Q k m n , | || | ) , ( 1 T T T T Q P Q P Q P Sim r r r r ⋅ = (12)</formula><p>Strategy 2 (VCT_ICLEF): Based on both the textual and the image similarity The system then returns a set of "relevant" images on the basic foundation of the similarity of the query and each image in the collection. In our design, the system first returns 80 images for the initial search, but 40 images in the later iterations. This is because that in the initial search the system does not catch an idea about what the user want exactly, a set of more images may induce the user to mark more relevant images, and it will give helps to the system when reformulating the query. In the display area, a pull-down menu below each image assists users in feedback of the relevance of each image. In fact, it is the color table shown in VCT_ICLEF which distinguishes the two systems. Users can provide color information, which is considered as an image in the system, to help the system determine the best query strategy. According to the experimental results, VCT_ICLEF has a better performance by exploiting color information for searching.</p><formula xml:id="formula_20" coords="5,98.46,611.55,240.96,114.46">∑ ∑ = = &gt; =&lt; &gt; =&lt; ⋅ + ⋅ = i I c i I c I c I I I I c I c I I c I c I Q h Q h P h Q H Q H P H Q P Sim Q h Q h Q P h P h P Q P Sim Q P Sim Q P Sim i i i m m ) ( )) ( ), ( min( | ) ( | ) ( ) ( ) ,<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Query Reformulation</head><p>As mentioned in Section 2, in the relevance feedback process, the user evaluates the relevance of the returned images, and gives a relevance value (i.e., non-relevant, neutral, and relevant) to each of them. At the next stage, the system performs query reformulation to modify the original query on the basis of the user's relevance judgments, and invokes again Cross-Language image retrieval based on the new query.</p><p>Recall that we denote as the original query ) , (</p><formula xml:id="formula_21" coords="7,273.96,234.33,46.49,14.02">I T Q Q Q =</formula><p>and the new query ) , (</p><formula xml:id="formula_22" coords="7,415.92,233.73,49.09,14.62">I T Q Q Q ′ ′ = ′</formula><p>; as for T Q′ , we exploit a practical method, as shown in Eq. ( <ref type="formula" coords="7,269.57,252.02,7.68,8.74">14</ref>), for query reformulation. This mechanism, which has been suggested by <ref type="bibr" coords="7,127.22,263.48,48.14,8.74" target="#b3">[Rocchio65]</ref>, is achieved with a weighted query by adding useful information which is extracted from relevant images as well as decreasing useless information which is derived from non-relevant images to the original query. Regarding I Q′ , it is computed as the centroid of the relevant images which is defined as the average of the relevant images. We do not take into account the irrelevant images for I Q′ since in our observations, there is always a large difference among the non-relevant images in which we believe that adding the irrelevant information to I Q′ will make no contribution.</p><formula xml:id="formula_23" coords="7,98.04,348.56,420.76,63.51">∑ ∑ ∈ ∈ - + ⋅ = ′ NREL P T REL P T T T T T P NREL P REL Q Q | | | | γ β α (14) ∑ ∈ = ′ REL P I I I P REL Q | | 1<label>(15)</label></formula><p>In Eq. ( <ref type="formula" coords="7,103.05,425.66,8.32,8.74">14</ref>) and Eq. (15), α, β, γ ≥ 0 are parameters, REL and NREL stands for the sets of relevant ad irrelevant images which are marked by the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation Results</head><p>In this section, we present our evaluation results for the user-centered search task at ImageCLEF 2004.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">The St. Andrews Collection</head><p>At the ImageCLEF 2004, the St. Andrews Collection<ref type="foot" coords="7,287.52,549.97,3.24,5.65" target="#foot_2">5</ref> is used for the evaluation purpose in which the majority of images (82%) are in black and white. It is indeed a subset of the St. Andrews University Library photographic collection from which 10% (i.e., 28,133) images have been used. All images have an accompanying textual description which is composed of 8 distinct fields<ref type="foot" coords="7,260.58,584.47,3.24,5.65" target="#foot_3">6</ref> , including 1) Record ID, 2) Title, 3) Location, 4) Description, 5) Date, 6) Photographer, 7) Categories, and 8) Notes. In total, the 28,133 captions consist of 44,085 terms and 1,348,474 word occurrences; the maximum caption length is 316 words, but on average 48 words in length. All captions are written in British English, and around 81% of captions contain text in all fields. In most cases, the caption is a grammatical sentence of about 15 words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">The User-Centered Search Task</head><p>The goal of the user-centered search task is to study whether the retrieval system is being used in the manner intended by the system designers and how the interface helps users reformulate and refine their search requests, given that a user searches with a specific image in mind but without knowing key information thereby requiring them to describe the image instead. At the ImageCLEF 2004, the interactive task is using an experimental procedure similar to iCLEF 2003<ref type="foot" coords="8,190.32,105.85,3.24,5.65" target="#foot_4">7</ref> .</p><p>In brief, given two interactive Cross-Language image retrieval systems -T_ICLEF and VCT_ICLEF, and the 16 topics shown in Fig. <ref type="figure" coords="8,182.23,140.00,3.76,8.74" target="#fig_4">5</ref>, 8 users are asked to test each system with 8 topics. For a system/topic combination, a total of 4 searchers will test the system. Users are given a maximum of 5 mins only to find each image. Topics and systems will be presented to the user in combinations following a latin-square design to ensure user/topic and system/topic interactions are minimized. Moreover, in order to know the search strategies used by searchers, we also conducted an interview after the task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Searcher Background</head><p>There are 8 people involved in the task, including 5 male and 3 female searchers. The average age of them is 23.5, with the youngest of 22 and the oldest of 26. Three of them major in computer science, two major in social science, and the others are librarians. In particular, three searchers have experiences in participating in projects about image retrieval. All of them have an average of 3.75 years (with a minimum of 2 years and a maximum of 5 years) accessing online search services, specifically, in Web search. In average, they search about 4 times a week, with a minimum of once and a maximum of 7 times. However, only a half of them have experiences in using image search services, such as Google images search. In addition, all of them are native speaker of Chinese, but all learned English before. Most of them report that his/her English ability is acceptable or good.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results</head><p>We are interested in which system helps searchers find a topic image efficiently. We summarize the average number of iterations<ref type="foot" coords="8,152.70,651.73,3.24,5.65" target="#foot_5">8</ref> and the average time spent by a searcher for each topic in Fig. <ref type="figure" coords="8,416.21,653.90,3.74,8.74" target="#fig_6">6</ref>. In the figure, it does not give information in the case that all searchers did not find the target image. (For instance, regarding topic 2, all searchers failed to complete the task by using T_ICLEF within the definite time.) The figure shows that overall VCT_ICLEF helps users find the image within a fewer iterations with a maximum of 2 iterations saved. For top-ics 2, 5, 7, 11, 15 and 16, no searcher can find the image by making use of T_ICLEF. Furthermore, with regard to topics 10 and 12, VCT_ICLEF has a worse performance. In our observations, it is because that most images (82%) in the corpus are in black and white, once the user gives imprecise color information, VCT_ICLEF needs to cost more iterations to find the image consequently.</p><p>Table <ref type="table" coords="9,117.49,133.52,5.01,8.74" target="#tab_5">2</ref> presents the number of searchers who failed to find the image for each topic. It is clear that VCT_ICLEF outperforms T_ICLEF in almost all cases. Considering topic 3, we believe that it is caused by the same reason we mentioned above for topics 10 and 12. Finally, we give a summary of our proposed systems in Table <ref type="table" coords="9,97.01,168.02,3.77,8.74">3</ref>. The table illustrates that while considering those topics that at least one search completed the task, T_ICLEF cost additional 0.4 iterations and 76.47 secs. Besides, by using VCT_ICLEF, on average, 89% of searchers successfully found the image while by using T_ICLEF, there is around 56.25% of searchers who did not fail the task.  To show the effect of color information used in VCT_ICLEF, we take Fig. <ref type="figure" coords="9,401.63,577.10,5.01,8.74" target="#fig_3">3</ref> and Fig. <ref type="figure" coords="9,446.93,577.10,5.01,8.74">4</ref> for example. Regarding topic 6, the query used was "燈塔 (Lighthouse)." For T_ICLEF, it returned a set of images corresponding to the query; however, the target image could not be found in the top 80 images. Since topic 6 is a color image, while we searched the image with color information by using VCT_ICLEF, the image was found in the first iteration. We conclude that color information can help a user indicate to the system what he is searching for. For an interactive image retrieval system, it is necessary to provide users not only an interface to issue a textual query but also an interface to indicate the system the visual information of the target.</p><p>Finally, we give an example as Fig. <ref type="figure" coords="9,240.52,673.16,5.01,8.74">7</ref> to show that the proposed interactive mechanism works effectively. The query is "長鬍鬚的男人 (A man with a beard)". The evaluated system is T_ICLEF; after 2 iterations of relevance feedback, it is obviously that we can improve the result by our feedback method.  In our survey of search strategies exploited by searchers, we found that 5 searchers thought that additional color information about the target image was helpful to indicate the system what they really wanted. Four searchers preferred to search the image with a text query first, even though by using VCT_ICLEF. They then considered color information for the next iteration in the situation that the target image was in color but the system returned images all in black and white. When searching for a color image, 3 searchers preferred to use color information first. Moreover, 2 searchers hoped that in the future, users can provide a textual query to indicate color information, such as "黃色 (Yellow)." Finally, to be mentioned, in our systems, the user is allowed to provide a query consisting of temporal conditions. However, since it is hard to decide in which year the image was published, no one used a query in which temporal conditions were contained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We participated in the user-centered search task at ImageCLEF 2004. In this paper, we proposed two interactive Cross-Language image retrieval systems -T_ICLEF and VCT_ICLEF. The first one is implemented with a practical relevance feedback approach based on textual information while the second one combines textual and image information to help users find a target image. The experimental results show that VCT_ICLEF has a better performance than T_ICLEF in almost all cases. Overall, VCT_ICLEF helps users find the image within a fewer iterations with a maximum of 2 iterations saved.</p><p>In the future, we plan to investigate user behaviors to understand in which cases users prefer a textual query as well as in which situations users prefer to provide visual information for searching. Besides, we also intend to implement a SOM (Self-Organizing Map) <ref type="bibr" coords="10,245.52,546.02,53.33,8.74" target="#b0">[Kohonen98]</ref> on image clustering, which we believe that it can provide an effective browsing interface to help searchers find a target image.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,183.54,369.51,228.18,8.74"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The overview of the user-centered search process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,137.27,734.01,7.51,8.74;5,174.90,107.14,2.00,8.72;5,159.54,107.14,2.00,8.72;5,196.02,74.14,2.00,8.72;5,156.54,74.14,2.00,8.72;5,123.18,99.34,3.33,8.72;5,110.04,99.34,3.33,8.72;5,168.84,111.57,2.33,6.11;5,147.54,95.85,4.28,6.11;5,138.90,95.85,3.50,6.11;5,189.48,78.57,3.50,6.11;5,167.76,78.57,3.50,6.11;5,118.86,103.83,2.33,6.11;5,102.90,103.83,3.11,6.11;5,163.92,107.14,6.11,8.72;5,113.94,99.34,6.11,8.72;5,98.34,99.34,5.00,8.72;5,151.32,98.90,1.67,4.36;5,106.26,106.88,1.39,4.36;5,139.92,73.65,12.82,22.03;5,142.44,93.40,4.98,8.57;5,174.48,70.64,5.48,12.25;5,129.00,95.84,5.48,12.25;5,173.82,85.56,4.94,12.94;5,182.28,70.08,5.48,12.94;5,159.72,70.08,6.30,12.94;5,502.50,76.95,16.30,8.74;5,70.92,137.31,24.49,8.74;5,132.96,137.38,3.33,8.72;5,116.76,137.38,2.50,8.72;5,101.40,137.38,3.33,8.72;5,128.10,141.81,3.50,6.11;5,112.08,141.81,3.50,6.11;5,120.96,133.33,5.47,12.92;5,104.04,133.33,6.29,12.92;5,143.34,137.31,61.66,8.74;5,285.54,137.32,3.33,8.71;5,279.00,145.12,5.00,8.71;5,257.52,137.32,2.50,8.71;5,249.90,145.12,5.00,8.71;5,228.84,137.32,3.33,8.71;5,277.86,127.03,4.94,12.93;5,248.70,127.03,4.94,12.93;5,269.82,133.84,5.48,12.23;5,240.84,133.84,5.48,12.23;5,262.62,137.32,5.00,8.71;5,233.64,137.32,5.00,8.71;5,211.62,137.32,4.44,8.71;5,216.00,141.81,1.94,6.10;5,220.98,135.91,6.59,12.97;5,290.94,137.31,233.56,8.74;5,70.92,156.51,453.48,9.48;5,70.92,174.81,22.56,8.74;5,157.56,174.82,3.33,8.71;5,150.96,182.62,5.00,8.71;5,129.48,174.82,2.50,8.71;5,121.86,182.62,5.00,8.71;5,100.80,174.82,3.33,8.71;5,149.82,164.53,4.94,12.93;5,120.72,164.53,4.94,12.93;5,141.78,171.34,5.48,12.23;5,112.80,171.34,5.48,12.23;5,134.58,174.82,5.00,8.71;5,105.60,174.82,5.00,8.71;5,169.08,171.31,208.07,12.26;5,416.04,174.88,3.33,8.71;5,399.84,174.88,2.50,8.71;5,384.48,174.88,3.33,8.71;5,411.18,179.31,3.50,6.10;5,395.16,179.31,3.50,6.10;5,404.04,170.83,5.48,12.93;5,387.12,170.83,6.30,12.93;5,422.70,174.81,101.71,8.74;5,128.76,202.86,3.32,8.69;5,122.16,210.60,4.98,8.69;5,100.68,202.86,2.49,8.69;5,93.06,210.60,4.98,8.69;5,72.00,202.86,3.32,8.69;5,121.02,192.57,4.92,12.90;5,91.92,192.57,4.92,12.90;5,112.98,199.38,5.46,12.20;5,84.00,199.38,5.46,12.20;5,105.78,202.86,4.98,8.69;5,76.80,202.86,4.98,8.69;5,140.46,202.83,310.44,9.48;5,476.88,206.57,4.94,12.95;5,485.34,190.97,5.48,12.95;5,462.78,190.97,6.30,12.95;5,499.14,195.04,2.00,8.73;5,459.60,195.04,2.00,8.73;5,492.54,199.52,3.50,6.11;5,470.82,199.52,3.50,6.11;5,477.54,191.54,5.48,12.25;5,510.00,202.83,14.42,8.74;5,103.02,234.51,4.95,12.97;5,123.24,218.97,6.32,12.97;5,96.00,218.97,4.95,12.97;5,137.88,223.04,2.01,8.74;5,111.42,223.04,3.34,8.74;5,106.38,223.04,5.01,8.74;5,77.10,223.04,3.34,8.74;5,73.14,223.04,2.01,8.74;5,131.28,227.53,3.51,6.12;5,81.90,223.04,5.01,8.74;5,116.70,219.54,5.50,12.27;5,89.10,219.54,5.50,12.27"><head></head><label></label><figDesc>. Fig.2gives an example to explain what we called a partial pixel. In the figure, c i-1 , c i , c i+1 stand for a color bin, a solid line indicate the boundary of c i , p is the value of a , each contribution of the pixel to c i and c i-1 is computed as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,70.92,123.50,453.59,8.74;6,70.92,135.02,453.42,8.74;6,70.92,146.48,453.43,8.74;6,70.92,158.00,453.44,8.74;6,70.92,169.52,453.56,8.74;6,70.92,180.98,453.52,8.74;6,70.92,192.50,151.32,8.74"><head>4</head><label></label><figDesc>Fig.3and Fig.4demonstrate the user interfaces which are designed for the user-centered search task at Image-CLEF 2004. Both systems have a search panel on the top, which allows users to type a Chinese query. The system then returns a set of "relevant" images on the basic foundation of the similarity of the query and each image in the collection. In our design, the system first returns 80 images for the initial search, but 40 images in the later iterations. This is because that in the initial search the system does not catch an idea about what the user want exactly, a set of more images may induce the user to mark more relevant images, and it will give helps to the system when reformulating the query.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,230.16,465.02,134.93,8.74;6,156.12,210.54,283.08,251.88"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The interface of T_ICLEF.</figDesc><graphic coords="6,156.12,210.54,283.08,251.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,167.34,443.84,260.64,8.74;8,160.20,203.82,274.86,224.94"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The 16 topic images used in the user-centered search task.</figDesc><graphic coords="8,160.20,203.82,274.86,224.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="9,108.90,401.24,377.62,8.74"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. The average number of iterations and the average time spent by a search for each topic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,70.92,141.89,453.63,122.29"><head></head><label></label><figDesc>are set to 1 if and only if P belongs to c i and P was published in y i respectively.</figDesc><table coords="3,70.92,141.89,453.63,122.29"><row><cell></cell><cell>w</cell><cell>y</cell><cell>i</cell><cell>(</cell><cell>T P</cell><cell>)</cell><cell>0 1</cell><cell cols="2">otherwise published was if P</cell><cell>in</cell><cell>y</cell><cell>i</cell><cell>,</cell><cell>(4)</cell></row><row><cell cols="7">In Eq. (2),</cell><cell cols="2">tf T i P max tf t ,</cell><cell cols="5">stands for the normalized frequency of t i in P T , maxtf is the maximum number of oc-</cell></row><row><cell cols="14">currences of any keyword in P T , N indicates the number of images in the corpus, and</cell><cell>i n denotes the number of t</cell></row><row><cell cols="14">images in whose caption t i appears. Regarding Eq. (3) and Eq. (4), both of them compute the weighting of the</cell></row><row><cell cols="14">category and the temporal feature as a Boolean value. In other words, considering a category c i and a year y i ,</cell></row><row><cell>( T c P w i</cell><cell>)</cell><cell></cell><cell></cell><cell cols="2">and</cell><cell>w</cell><cell>( T y P</cell><cell>)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note coords="3,142.80,259.84,1.38,4.34"><p>i</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,70.92,419.01,434.63,46.85"><head>Table 1</head><label>1</label><figDesc></figDesc><table coords="4,70.92,430.53,434.63,35.33"><row><cell cols="7">An example which shows how time operators work while considering the time dimension</cell><cell></cell><cell></cell></row><row><cell>Year</cell><cell>…</cell><cell>1897</cell><cell>1898</cell><cell>1899</cell><cell>1900</cell><cell>1901</cell><cell>1902</cell><cell>…</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,70.92,422.90,448.00,136.96"><head>Table 2</head><label>2</label><figDesc></figDesc><table coords="9,70.92,434.42,448.00,125.44"><row><cell cols="10">Number of searchers who did not find the target image for each topic</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Topic</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell cols="7">10 11 12 13 14 15 16</cell></row><row><cell>T_ICLEF</cell><cell>1</cell><cell>4</cell><cell>1</cell><cell>0</cell><cell>4</cell><cell>0</cell><cell>4</cell><cell>1</cell><cell>0</cell><cell>1</cell><cell>4</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>4</cell><cell>4</cell></row><row><cell cols="2">VCT_ICLEF 1</cell><cell>0</cell><cell>2</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>2</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>2</cell><cell>0</cell></row><row><cell>Table 3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">Average steps to find the target image, and the average spent time</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Avg. Iterations</cell><cell></cell><cell></cell><cell cols="5">Avg. Spent Time for</cell><cell cols="6">Avg. percent of searchers who</cell></row><row><cell></cell><cell></cell><cell cols="4">(Not including not found)</cell><cell cols="2">each topic</cell><cell></cell><cell></cell><cell></cell><cell cols="6">found the target image (#/4×100%)</cell></row><row><cell>T_ICLEF</cell><cell></cell><cell>2.24</cell><cell></cell><cell></cell><cell></cell><cell cols="2">208.67s</cell><cell></cell><cell></cell><cell></cell><cell>56.25%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VCT_ICLEF</cell><cell></cell><cell>1.84</cell><cell></cell><cell></cell><cell></cell><cell cols="2">132.20s</cell><cell></cell><cell></cell><cell></cell><cell>89.00%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,79.20,736.47,252.22,8.74"><p>The official website is available at http://clef.iei.pi.cnr.it:2002/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,79.20,747.99,293.24,8.74"><p>Please refer to http://ir.shef.ac.uk/imageclef2004/ for further information.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2" coords="7,79.20,724.94,330.53,8.74"><p>Please refer to http://ir.shef.ac.uk/imageclef2004/guide.pdf for a detail description.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3" coords="7,79.20,736.46,414.90,8.74;7,70.92,747.98,136.93,8.74"><p>In this paper, almost all fields were used for indexing, except for 1 and 8. The words were stemmed and stop-words were removed as well.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4" coords="8,79.20,724.94,360.28,8.74"><p>Please refer to http://terral.lsi.uned.es/iCLEF/2003/guidelines.htm for further information.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5" coords="8,79.20,736.46,443.84,8.74;8,70.92,747.98,371.40,8.74"><p>Please note that our system does not have a efficient performance; since for each iteration it spent about 1 minute to retrieve relevant images, approximately 5 iterations is performed within the time limit.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,126.59,614.96,365.80,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,183.64,614.96,99.46,8.74">The Self-Organizing Map</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,295.62,614.96,64.78,8.74">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,119.15,626.48,405.32,8.74;10,115.92,638.00,408.60,8.74;10,115.92,649.46,49.80,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,419.47,626.48,105.01,8.74;10,115.92,638.00,91.21,8.74">Query Feedback for Interactive Image Retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kushki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Androutsos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">N</forename><surname>Plataniotis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Venetsanopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,218.34,638.00,266.45,8.74">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,115.01,660.98,408.34,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,160.50,660.98,165.04,8.74">WordNet: A Lexical Database for English</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,336.30,660.98,112.77,8.74">Communications of the ACM</title>
		<imprint>
			<biblScope unit="page" from="39" to="45" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,123.64,672.50,400.86,8.74;10,115.92,683.96,356.59,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,243.70,672.50,272.08,8.74">Information Search Optimization and Iterative Retrieval Techniques</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Rocchio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,115.92,683.96,106.97,8.74">Proc. of AFIPS 1965 FJCC</title>
		<meeting>of AFIPS 1965 FJCC<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Spartan Books</publisher>
			<date type="published" when="1965">1965</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="293" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,115.67,695.48,394.43,8.74" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="10,235.22,695.48,182.62,8.74">Introduction to Modern Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Mcgill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>McGraw-Hill</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,115.68,707.00,408.77,8.74;10,115.92,718.46,63.02,8.74;10,121.62,224.48,132.56,8.74;10,332.74,224.48,150.35,8.74;10,159.48,241.88,275.82,8.75" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,252.43,707.00,59.71,8.74">Color Indexing</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Swain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">H</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,323.64,707.00,167.14,8.74">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="11" to="32" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
	<note>a) Results after the initial search (b) Results after 2 feedback iterations Fig. 7. Results for the query &quot;長鬍鬚的男人 (A man with a beard)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
