<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,91.32,98.73,420.02,15.51;1,263.04,120.69,76.84,15.51">Visual Features for Content-based Medical Image Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,165.48,154.07,61.61,9.96"><forename type="first">Peter</forename><surname>Howarth</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Multimedia Information Retrieval Team</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>km</settlement>
									<country>UK. http</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,235.37,154.07,70.02,9.96"><forename type="first">Alexei</forename><surname>Yavlinsky</surname></persName>
							<email>alexei.yavlinsky@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Multimedia Information Retrieval Team</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>km</settlement>
									<country>UK. http</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,313.04,154.07,59.47,9.96"><forename type="first">Daniel</forename><surname>Heesch</surname></persName>
							<email>daniel.heesch@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Multimedia Information Retrieval Team</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>km</settlement>
									<country>UK. http</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,380.43,154.07,56.94,9.96"><forename type="first">Stefan</forename><surname>Rüger</surname></persName>
							<email>s.rueger@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Multimedia Information Retrieval Team</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>km</settlement>
									<country>UK. http</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,91.32,98.73,420.02,15.51;1,263.04,120.69,76.84,15.51">Visual Features for Content-based Medical Image Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B3A4A3FF0444E3432D422558AB1CE147</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe our experiments for the Image CLEF medical retrieval task. Our efforts were focused on the initial visual search. A content based approach was followed using global features that would perform well with the medical image collection. We used structure, texture, localisation and colour features that have been proven by previous experiments. The retrieval results showed that this simple approach was successful.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Content based image retrieval (CBIR) aims to provide a way to search generic image collections. Traditionally, for highly constrained domains, such as medical images, CBIR has been viewed as being too imprecise. Our aim was to determine if the CBIR approach could be viable for an initial search or filtering step in a medical image collection. We focused on choosing high quality visual features that have good discriminatory power for the collection.</p><p>In this paper we first present a brief overview of our system. Section 3 explains the rationale for using specific visual features and details how they are computed. Results of our run are presented in Section 4, followed by a postmortem analysis. Some of our ideas for future work are presented in Section 5. We would like to have applied classification methods to the collection but the lack of training data precluded this. We also discuss the use of a browsing paradigm for this type of collection. Our conclusions round off the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Overview</head><p>The initial visual search task was very straightforward, with 26 single image queries. With no training data it was not possible to use any learning classifiers. We therefore used a simple system to tackle the retrieval task, using the features described in the next section. The following steps were carried out:</p><p>1. Features were generated for the test collection and query images; 2. For each feature the Manhattan distance between the query images and the test set was calculated;</p><p>3. The set of distances from each feature was normalised by dividing by their median. This ensured that each feature would have an equal weighting;</p><p>4. The distances for each query were summed over all features. This gave the overall distance from each query image to the test set. These were then sorted to produce a ranked list of retrieval results.</p><p>The initial step in our work was to look at the collection and determine its characteristics. As a relatively specific domain it displayed a large degree of homogeneity. We realised this could be exploited by choosing features that would differentiate the image types.</p><p>The collection contained a large number of monochrome images, such as x-rays and CT scans, with very specific layout. The patients are positioned very precisely to show the area under investigation at the centre of the image. The layout can be used to indicate both modality and anatomic region. For this reason a localisation feature, thumbnail, was used to detect images with similar layouts. Within the modalities the images could be discriminated by structure and texture. We therefore chose to use a convolution feature to discriminate structure and two texture features, co-occurrence matrices and Gabor filters. The two texture features were applied to nonoverlapping image tiles. This gives an additional structural element to the feature. Finally, for the relatively small number of colour images we deployed a colour structure descriptor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Thumbnail</head><p>This is perhaps the simplest feature in our feature set, yet it is highly effective in detecting images with a near identical layout. Each image is converted to grey scale and then scaled down to a thumbnail of fixed size. For these experiments we used 40×30 pixels. The pixel values of this new image then make up the feature vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Convolution</head><p>This feature is based on Tieu and Viola's method <ref type="bibr" coords="2,303.80,343.19,9.89,9.96" target="#b0">[1]</ref>. It relies on a large number of highly selective features that can determine structure within an image and capture information about texture and edges. A vast set of features are defined such that each feature will have a high value for only a small proportion of images. This enables an effective search by matching the features that are defined by the query. Due to the nature of the image collection we applied the feature to grey level images rather than RGB.</p><p>The feature generation process starts with a set of 25 primitive features that are applied to the grey level image. This generates 25 feature maps. Each of these is rectified and down-sampled before being filtered again by each of the 25 primitive filters. This gives 625 feature maps. The second stage of the process 'discovers' arrangements of features in the previous levels. The values of each feature map are summed to give a single number. These are combined into a feature vector of 625 values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Co-occurrence</head><p>Haralick <ref type="bibr" coords="2,130.76,521.03,10.55,9.96" target="#b1">[2]</ref> suggested the use of grey level co-occurrence matrices (GLCM) to extract second order statistics from an image. They have been used very successfully for texture classification. The GLCM of an image is defined as a matrix of frequencies at which two pixels, separated by a certain vector, occur in the image. The distribution in the matrix will depend on the angular and distance relationship between pixels. Varying the vector used allows the capturing of different texture characteristics. Once the GLCM has been created, various features can be computed from it. These have been classified into four groups: visual texture characteristics, statistics, information theory and information measures of correlation <ref type="bibr" coords="2,352.45,604.67,10.43,9.96" target="#b1">[2,</ref><ref type="bibr" coords="2,366.24,604.67,6.95,9.96" target="#b2">3]</ref>.</p><p>Using the results of our recent evaluation <ref type="bibr" coords="2,288.20,616.67,10.55,9.96" target="#b3">[4]</ref> we chose the following configuration for creating the GLCM:</p><p>• The original image was split into 7×7 non-overlapping tiles and the feature run for each of these;</p><p>• The colour image was quantised into 64 grey levels;</p><p>• 16 GLCMs were created for each image tile using vectors of length 1, 2, 3, and 4 pixels and orientations 0, π/4, π/2 and 3π/4;</p><p>• For each normalised co-occurrence matrix P (i, j) we calculated a homogeneity feature H p ,</p><formula xml:id="formula_0" coords="3,258.24,113.03,111.36,27.37">H p = i j P (i, j) 1 + |i -j| .</formula><p>This feature was chosen as it had performed consistently well in previous evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Gabor</head><p>One of the most popular signal processing based approaches for texture feature extraction has been the use of Gabor filters. These enable filtering in the frequency and spatial domain. It has been proposed that Gabor filters can be used to model the responses of the human visual system. Turner <ref type="bibr" coords="3,123.69,232.43,10.55,9.96" target="#b4">[5]</ref> first implemented this by using a bank of Gabor filters to analyse texture. A range of filters at different scales and orientations allows multichannel filtering of an image to extract frequency and orientation information. This can then be used to decompose the image into texture features.</p><p>Our implementation is based on that of Manjunath et al <ref type="bibr" coords="3,347.34,280.31,9.98,9.96" target="#b5">[6]</ref>. The feature is built by filtering the image with a bank of orientation and scale sensitive filters and computing the mean and standard deviation of the output in the frequency domain.</p><p>Filtering an image I(x, y) with Gabor filters g mn designed according to <ref type="bibr" coords="3,415.71,316.19,10.43,9.96" target="#b5">[6]</ref> results in its Gabor wavelet transform W mn , W mn (x, y) = I(x, y)g * mn (x -x 1 , y -y 1 )dx 1 dy 1 .</p><p>The mean and standard deviation of the magnitude |W mn | are used for the feature vector. The outputs of filters at different scales have different ranges. For this reason each element of the feature vector is normalised using the standard deviation of that element across the entire database. From our evaluation <ref type="bibr" coords="3,197.44,415.07,10.43,9.96" target="#b3">[4]</ref> we found that a filter bank with 2 scales and 4 orientations gave the best retrieval performance. We used this configuration and applied it to 7×7 non-overlapping tiles created from the original image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Colour Structure Descriptor</head><p>For the colour images in the collection we used a feature that is good at capturing local colour image structure. It is defined in the HMMD (hue, min, max diff) colour space. This is used in the MPEG-7 standard and is derived from both RGB and HSV spaces. The hue component is taken from HSV and the min and max components are from the maximum and minimum values in the RGB space. The diff component is the difference between min and max. We follow the MPEG-7 standard and quantise this space non-uniformly into 184 bins in the 3 dimensional hue, sum and diff colour space, see Manjunath and Ohm <ref type="bibr" coords="3,278.51,556.79,10.43,9.96" target="#b6">[7]</ref> for details of the quantisation.</p><p>To calculate the colour structure descriptor an 8 × 8 window is slid over the image. Each of the 184 bins of the HMMD histogram contains the number of window positions for which there is at least one pixel falling into the bin under consideration. This feature is capable of discriminating between images with the same global colour distribution but differing local colour structures. For this reason it is suited to colour medical images which tend to have similar overall colour but differing structure depending on the detail of the photograph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Our system had 34.5% mean average precision retrieval across all queries. This put us in third place for the visual retrieval task. The two runs performing better than ours, SUNY and KIDS,  These results showed that our simple approach, using good quality visual features, produced results comparable with the top systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Postmortem</head><p>With the relevance judgements available it was possible to look at how individual features had performed. Fig. <ref type="figure" coords="4,162.79,549.11,5.03,9.96" target="#fig_0">1</ref> shows the precision recall graph for the individual features together with that for the combined features of the submitted run. Table <ref type="table" coords="4,325.28,561.11,5.03,9.96" target="#tab_0">1</ref> shows the mean average precisions for the same features. From these results it is clear that all features performed reasonably well. The Gabor feature on its own slightly outperformed the submitted run. Some additional feature combinations were tested, including adding the Gabor feature to each of the others in turn. However, none of these improved the mean average precision above that of Gabor on its own.</p><p>Reviewing the performance for individual queries showed that there was a wide variation. Different features performed better on queries where they had better discrimination. An example of this was query 7, a colour image, where HDS gave a mean average precision of 91.5% and Gabor 33.6%. This variation indicates that it could be possible to improve performance by weighting features differently depending on the query. In addition to the search task we also used put the data set into a novel browsing network, NN k , developed in our group <ref type="bibr" coords="5,197.32,432.47,9.98,9.96" target="#b7">[8]</ref>. Although we did not carry out a formal evaluation we found that through browsing it was possible to rapidly access similar images in the collection. A medical expert would be presented with a range of images to review and identify those that they were particularly interested in. It is clear that the browsing paradigm is an effective way of searching data collections of this size and complexity.</p><p>This system can be accessed through our group demo page at http://km.doc.ic.ac.uk; open the iBase application and then select the CasImage collection on the settings page. A screen shot of the application is shown in Fig <ref type="figure" coords="5,239.30,516.11,3.90,9.96" target="#fig_1">2</ref>.</p><p>The experiments carried out used very effective features. However, they were combined in a simple way, by summing the distances obtained from each feature. As shown shown in the postmortem analysis there was a wide variation in performance of different features for different queries. This would indicate that when querying for certain modalities or anatomic regions different combinations of features may perform better. By varying the weights applied to features we can introduce a degree of plasticity into our system and then use machine learning techniques to improve retrieval performance.</p><p>Given training data we would like to train a support vector machine as a meta classifier. We have deployed this technique in other contexts, see Yavlinsky et al. <ref type="bibr" coords="5,384.09,623.75,9.89,9.96" target="#b8">[9]</ref>. We propose that it would be possible to learn the optimal weights for retrieving a specific modality, such as CT scan or x-ray.</p><p>Our experiments showed that it is possible to achieve good retrieval performance on a medical image collection using a CBIR approach. We used global features, which is in contrast to the highly specialised features normally used for medical imaging. Given the constrained domain we were able to choose visual features that had good discriminatory power for the collection. In addition, the analysis of individual features indicates that there is scope for applying machine learning classification methods to further improve retrieval performance.</p><p>These results show that CBIR can be used as a useful first step in medical image retrieval.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,150.84,425.15,301.20,9.96"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Precision-recall graph for combined and individual features</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,189.96,356.87,139.92,9.96;5,330.00,355.90,4.20,6.26;5,338.28,356.87,74.82,9.96;5,125.11,59.23,352.50,283.08"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: iBase showing the NN k browsing window</figDesc><graphic coords="5,125.11,59.23,352.50,283.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,149.28,68.39,304.19,340.95"><head>Table 1 :</head><label>1</label><figDesc>Mean average precision for combined and individual features</figDesc><table coords="4,186.84,80.03,229.30,82.08"><row><cell>Feature</cell><cell>Mean average precision</cell></row><row><cell>Submitted run (combined)</cell><cell>34.50%</cell></row><row><cell>Gabor</cell><cell>35.31%</cell></row><row><cell>Thumbnail</cell><cell>26.33%</cell></row><row><cell>Co-occurrence</cell><cell>19.79%</cell></row><row><cell>HDS</cell><cell>19.50%</cell></row><row><cell>Convolution</cell><cell>18.11%</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,105.47,209.75,407.13,9.96;6,105.48,221.75,120.35,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,194.20,209.75,103.30,9.96">Boosting image retrieval</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Tieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,315.64,210.09,196.96,9.18;6,105.48,222.09,43.37,9.18">International Conference on Spoken Language Processing</title>
		<imprint>
			<date type="published" when="2000-12">December 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.47,241.67,407.03,9.96;6,105.48,253.55,89.27,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,167.81,241.67,215.99,9.96">Statistical and structural approaches to texture</title>
		<author>
			<persName coords=""><surname>Haralick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,398.62,242.01,108.54,9.18">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="786" to="804" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.47,273.59,407.13,9.96;6,105.48,285.47,243.56,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,240.23,273.59,222.19,9.96">Texture descriptors based on co-occurrence matrices</title>
		<author>
			<persName coords=""><forename type="first">C C</forename><surname>Gotlieb</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H E</forename><surname>Kreyszig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,469.80,273.93,42.80,9.18;6,105.48,285.81,169.24,9.18">Computer Vision, Graphics and Image Processing</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="70" to="86" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.47,305.39,407.26,9.96;6,105.48,317.39,407.43,9.96;6,105.48,329.27,118.56,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,225.61,305.39,283.26,9.96">Evaluation of texture features for content-based image retrieval</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Howarth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rüger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,117.96,317.73,322.27,9.18">Proceedings of the International Conference on Image and Video Retrieval</title>
		<meeting>the International Conference on Image and Video Retrieval</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2004-07">July 2004</date>
			<biblScope unit="page" from="326" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.47,349.31,407.35,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,153.92,349.31,183.77,9.96">Texture discrimination by Gabor functions</title>
		<author>
			<persName coords=""><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,345.89,349.65,93.05,9.18">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="71" to="82" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.47,369.23,407.07,9.96;6,105.48,381.11,323.39,9.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,225.64,369.23,252.36,9.96">Texture features for browsing and retrieval of image data</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Manjunath</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,488.56,369.57,23.98,9.18;6,105.48,381.45,226.76,9.18">IEEE Trans on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="837" to="842" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.47,401.03,407.08,9.96;6,105.48,413.03,204.81,9.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,247.23,401.03,128.42,9.96">Color and texture descriptors</title>
		<author>
			<persName coords=""><forename type="first">B S</forename><surname>Manjunath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J-R</forename><surname>Ohm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,385.59,401.37,126.97,9.18;6,105.48,413.37,120.51,9.18">IEEE Transs on circuits and systems for video technology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="703" to="715" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.47,432.95,125.41,9.96;6,231.00,431.98,4.20,6.26;6,240.00,432.95,272.73,9.96;6,105.48,444.95,356.39,9.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,216.02,432.95,14.86,9.96;6,231.00,431.98,4.20,6.26;6,240.00,432.95,186.39,9.96">NN k networks for content-based image retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Heesch</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Rüger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,449.81,433.29,62.92,9.18;6,105.48,445.29,158.54,9.18">26th European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2004-04">April 2004</date>
			<biblScope unit="page" from="253" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.47,464.87,407.27,9.96;6,105.48,476.75,406.99,9.96;6,105.48,488.75,174.99,9.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,328.35,464.87,184.39,9.96;6,105.48,476.75,59.89,9.96">A comparative study of evidence combination strategies</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yavlinsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Pickering</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Heesch</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Rüger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,184.37,477.09,323.77,9.18">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2004-05">May 2004</date>
			<biblScope unit="volume">III</biblScope>
			<biblScope unit="page" from="1040" to="1043" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
