<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,96.42,148.86,410.16,15.15;1,263.06,170.78,76.88,15.15">Toward Cross-Language and Cross-Media Image Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,148.64,204.67,68.69,8.74"><forename type="first">Carmen</forename><surname>Alvarez</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Montreal</orgName>
								<address>
									<addrLine>CP, 6128, succursale Centre-ville</addrLine>
									<postCode>H3C 3J7</postCode>
									<settlement>Montreal</settlement>
									<region>Quebec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,225.08,204.67,94.27,8.74"><forename type="first">Ahmed</forename><surname>Id Oumohmed</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Montreal</orgName>
								<address>
									<addrLine>CP, 6128, succursale Centre-ville</addrLine>
									<postCode>H3C 3J7</postCode>
									<settlement>Montreal</settlement>
									<region>Quebec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,328.58,204.67,60.32,8.74"><forename type="first">Max</forename><surname>Mignotte</surname></persName>
							<email>mignotte@iro.umontreal.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Montreal</orgName>
								<address>
									<addrLine>CP, 6128, succursale Centre-ville</addrLine>
									<postCode>H3C 3J7</postCode>
									<settlement>Montreal</settlement>
									<region>Quebec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,396.93,204.67,57.43,8.74;1,236.13,218.62,23.03,8.74"><roleName>DIRO</roleName><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
							<email>nie@iro.umontreal.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Montreal</orgName>
								<address>
									<addrLine>CP, 6128, succursale Centre-ville</addrLine>
									<postCode>H3C 3J7</postCode>
									<settlement>Montreal</settlement>
									<region>Quebec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,96.42,148.86,410.16,15.15;1,263.06,170.78,76.88,15.15">Toward Cross-Language and Cross-Media Image Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2AD7B496214BFAC581A335E2EC5835BB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This report describes the approach used in our participation of ImageCLEF. Our focus is on image retrieval using text, i.e. Cross-Media IR. To do this, we first determine the strong relationships between keywords and types of visual features. Then the subset of images retrieved by text retrieval is used as examples to match other images according to the most important types of features of the query words.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The RALI group at University of Montreal has participated in several CLEF experiments on Cross-Language IR (CLIR). Our participation in this year's ImageCLEF experiments is to see how our approach can be extended to Cross-Media IR. Two research groups are involved in this task: one on image processing and the other on text retrieval. Our CLIR approach is similar to that used in our previous participation in CLEF, i.e. we use statistical translation models trained on parallel web pages for French to English translations. For the translation from other languages, we use bilingual dictionaries. Our focus is on image retrieval from text queries.</p><p>Different approaches have been used for image retrieval. 1) A user can submit a text query, and the system can search for images using image captions. 2) A user can submit an image query (using an example image -either selected from a database or drawn by the user). In this case, the system tries to determine the most similar images to the example image by comparing various visual features such as shape, texture, or color. 3) There is still a third group of approaches which tries to assign some semantic meaning to images. This approach is often used to annotate images by concepts or by keywords <ref type="bibr" coords="1,214.35,580.36,9.97,8.74" target="#b0">[1]</ref>. Once images have been associated with different keywords, they can be retrieved for a textual query.</p><p>The above three approaches have their own advantages and weaknesses. The first approach is indeed text retrieval. There is no particular image processing. The coverage of the retrieval is limited to images with captions.</p><p>The second approach does not require the images to be associated with captions. However, the user is required to provide an example image and a visual feature or a combination of some features to be used for image comparison. This is often difficult for a non-expert user.</p><p>The third approach, if successful, would allow us to automatically recognize the semantics of images, thus allow users to query images by keywords. However, the development up to now only allows us to annotate images according to some typical components or features. For example, according to a texture analysis, one can recognize a region of image as corresponding to a tiger because of the particular texture of tigers <ref type="bibr" coords="1,277.50,723.82,9.96,8.74" target="#b1">[2]</ref>. It is still impossible to recognize all the semantic meanings of images. Some recent studies <ref type="bibr" coords="2,193.89,112.02,10.52,8.74" target="#b2">[3]</ref> have tried to automatically create associations between visual features and keywords. The basic idea is to use a set of annotated images as a set of learning examples, and to extract strong associations between annotation keywords and the visual features of the images. In our study, we initially tried to use a similar approach in ImageCLEF. That is, we wanted to extract strong relationships between the keywords in the captions and the visual features of the images. If such relationships could be created, then it would be possible to use them to retrieve non-annotated images by a textual query. In this case, the relationships play a role of translation between media. However, we discovered that this approach is extremely difficult in the context of ImageCLEF for several reasons:</p><p>1. The annotations (captions) of the images in the ImageCLEF corpus often contain keywords that are not strongly associated with particular visual features. They correspond to abstract concepts. Examples of such keywords are "Scotland", "north", and "tournament". Therefore, if we use the approach systematically, there will be many noisy relationships.</p><p>2. Even if there are some relationships between keywords and visual features, these relationships may be difficult to be extracted because there are a huge number of possible visual features.</p><p>In fact, visual features are continuous. Even if we use some discretization techniques, their number is still too high to be associated with some keywords. For example, for a set of images associated with the keyword "water", one would expect to extract strong relationships between the keyword and the color and texture features. However, "water" in images may only take up a small region of the image. There may be various other objects in the same images, making it difficult to automatically isolate the typical features for "water".</p><p>Due to these reasons, we take a more flexible approach. We also use the images with captions as a set of training examples, but we do not try to create relationships between keywords and particular visual features (such as a particular shade of blue for the word "water"). We only try to determine which type(s) of feature is (are) the most important for a keyword. For example, "water" may be associated with "texture" and "color". Only strong relationships are retained. During the retrieval process, a text query is first matched with a set of images using image captions. This is a text retrieval step. Then the retrieved images are used as examples to retrieve other images, which are similar according to the determined types of features associated with the query keywords. The whole processes of our system is illustrated in figure <ref type="figure" coords="2,388.07,486.62,3.88,8.74" target="#fig_0">1</ref>.</p><p>In the following sections, we will first describe the image processing developed in this project. In particular, we will describe the way that relationships between keywords and visual features are extracted, as well as image retrieval with example images. In section 3, we will describe the CLIR approach used. In section 4, both approaches are combined to perform image retrieval. Section 5 will describe the experimental results and some conclusions.</p><p>Our approach is much less ambitious than that of <ref type="bibr" coords="2,330.14,590.23,9.96,8.74" target="#b2">[3]</ref>, but it is more feasible in practice. In fact, in many cases, image captions contain abstract keywords that cannot be strongly associated with visual features, and even if they can, it is impossible to associate a single vector to a keyword. Our approach does not require determining such a single feature vector for a given keyword. It abandons the third approach mentioned earlier, but combines the first two families of approaches. The advantage of extracting keyword-feature associations is to avoid the burden of requiring the user to indicate the appropriate types of features to be used in image comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Image processing-based learning procedure</head><p>The objective of the automatic image processing-based learning procedure that we propose in this section is twofold: • First, this procedure aims at estimating the most discriminant type(s) of high-level visual features for each annotated keyword. In our application, we have considered the three fundamental visual characteristics; namely, texture (including color information), edge and shape. For example, the keyword "animal" could belong to the shape class since the measure using shape information will be the most discriminant to identify images with animals (but the more specific keywords "zebra" and "tiger" will more probably belong to the edge and texture classes respectively due to the characteristic coat of these animals).</p><p>A discriminant measure, belonging to each of these classes of visual features has then been defined. We have considered :</p><p>1. The mean and the standard deviation of the energy distribution in each of the sub-band of an Harr wavelet <ref type="bibr" coords="3,221.00,496.92,10.51,8.74" target="#b3">[4]</ref> decomposition of the image as discriminant measure of the edge class.</p><p>2. The coarseness measure proposed by Tamura et al. <ref type="bibr" coords="3,364.99,524.34,10.52,8.74" target="#b4">[5]</ref> as discriminant measure of the texture class.</p><p>3. The histogram of the edge orientation of the different shapes extracted from the input image (after a region-based segmentation) as discriminant measure of the shape class.</p><p>• The second objective is to identify a set of candidate images that are the most representative for each annotated keyword, in the sense of similarity distance combining one or several preestimated visual feature classes.</p><p>The type of high-level visual feature (along with its discriminant measure) and a set of candidate images along with its associated normalized similarity distance will be used with crosslanguage Information, to refine the retrieval process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Edge class and its measure</head><p>Wavelet-based measures have often been used in content-based image retrieval system because of the appealing ability to describe the local texture and the distribution of the edges of a given image at multiple scales. In our application we use the Harr wavelet transform <ref type="bibr" coords="3,419.98,721.73,10.52,8.74" target="#b3">[4]</ref> for the luminance (i.e., grey-level) component of the image. There are several other wavelet transforms but the Harr wavelet transform has better localization properties and requires less computation compared to other wavelets (e.g., Daubechies' wavelet). The procedure of image decomposition into wavelets involves recursive numeric filtering. It is applied to the set of pixels of the digital image which is decomposed with a family of orthogonal basis functions obtained through translation and dilatation of a special function called mother wavelet. At each scale (or step) in the recursion, we obtain four sub-bands (or sets of wavelet coefficients), which we refer to as LL, LH, HL and HH according to their frequency characteristics (L : Low and H : High, see Figure <ref type="figure" coords="4,383.60,171.80,3.88,8.74">2</ref>). The LL sub-band is then decomposed into four sub-bands at the next scale decomposition. For each scale decomposition (three considered in our application), we compute the mean and the standard deviation of the energy distribution (i.e., the average and the square of each set of wavelet coefficients) in each of the sub-bands. This leads to a vector of 20 (i.e., (2 × 3 × 3) + 2) components or attributes which can be viewed as the descriptor (or the signature) of the edge information/characteristics of the image. For example, an image containing a zebra thus has high energy in the HL sub-band and low energy in the LH sub-band due to the vertical strips of the coat of this animal. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Texture class and its measure</head><p>Tamura et al. <ref type="bibr" coords="4,155.32,526.29,10.52,8.74" target="#b4">[5]</ref> have proposed to characterize image texture along the dimensions of contrast, directionality, coarseness, line-likeness, regularity and roughness. These properties correspond to the human texture perception.</p><p>-Contrast is a scalar value related to the amount of local intensity variations present in an image and involves the standard deviation of the grey-level probability distribution.</p><p>-Directionality is a global texture property which is computed from the oriented edge histogram, obtained by an edge detector like the Sobel detector <ref type="bibr" coords="4,357.55,598.02,9.96,8.74" target="#b5">[6]</ref>. The directionality measures the sharpness of the peaks in this edge histogram.</p><p>-In this class of visual features, we have utilized only the coarseness property which yields a histogram of six bins, for the following reasons :</p><p>• The contrast is not very discriminant for textural retrieval,</p><p>• The edge information has been already treated in the wavelet and shape class,</p><p>• The line-likeness, regularity and roughness properties are correlated to the coarseness, contrast and directionality properties.</p><p>Coarseness refers to the size of the texton; i.e., the smallest unit of a texture. This measure thus depends on the resolution of the texture. With this measure, we can compute a histogram with 6 bins (i.e., a 6-component attribute vector) which will be used as the descriptor of the texture characteristics of a given image. The procedure for computing the coarseness histogram is outlined below, 1. At each pixel with coordinates (x, y) in the image, and for each value k (k taking its value in {1, 2, . . . , 6}), we compute the average over its neighborhood of size 2 k × 2 k i.e.,</p><formula xml:id="formula_0" coords="5,220.56,204.35,157.29,33.66">A k (x, y) = i=(x+2 k-1 -1) i=(x-2 k-1 ) j=(y+2 k-1 -1)</formula><p>j=(y-2 k-<ref type="foot" coords="5,364.99,231.27,4.22,4.37" target="#foot_0">1</ref> )</p><formula xml:id="formula_1" coords="5,380.71,210.53,25.44,22.31">I(i, j) 2 2k</formula><p>where I(i, j) is the intensity pixel of the image at pixel (i, j).</p><p>2. At each pixel, and for the horizontal and vertical directions, we compute the differences between pairs of averages corresponding to pairs of nonoverlapping neighborhoods just on opposite sides of the pixel. The horizontal and vertical differences are expressed as:</p><formula xml:id="formula_2" coords="5,136.51,326.66,294.28,51.66">E k,horizontal = |A k (x + 2 k-1 , y) -A k (x -2 k-1 , y)| E k,vertical = |A k (y + 2 k-1 , y) -A k (y -2 k-1 , y)| 3.</formula><p>At each pixel, the value of k that maximizes E k (i, j), in either direction (horizontal or vertical), is used to set the best size S best (i, j) = 2 k . At this stage we can consider, as descriptor, the scalar measure of coarseness which is the average of S best over the entire image, or consider, as in our application, the histogram (i.e., the empirical probability distribution) of S best which is more precise for discrimination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Shape class and its measure</head><p>Description and interpretation of shapes contained in an input image remains a difficult task. Several methods use a contour detection of the images (such as the Canny or Sobel edge detectors) as a preliminary step in the shape extraction. But these methods remain dependent on certain parameters as thresholds (on the magnitude of the image gradient).</p><p>In image compression, some approaches <ref type="bibr" coords="5,286.43,524.13,10.52,8.74" target="#b6">[7]</ref> use a vector quantization method on the set of vectors of dimension K 2 of grey-levels corresponding to K × K blocks extracted from the image. By using a clustering procedure into K classes, we can obtain an image with separate regions (a set of connected pixels belonging to a same class) from which we extract the contours of the different regions. These edges are connected and obtained without any parameter adjustment and the noise is taken into consideration in this procedure. Figure <ref type="figure" coords="5,322.31,583.91,4.98,8.74">3</ref> shows an example of edge detection using three regions, i.e., three clusters in the vector quantization.</p><p>In our application, we use this strategy of edge detection and we use, as clustering procedure, the Generalized LLoyd <ref type="bibr" coords="5,192.67,619.77,10.52,8.74" target="#b7">[8]</ref> [9] (generally used in this context). In our implementation, we use the code provided from the QccPack Library 1 .</p><p>For each edge pixel, we define a direction (horizontal, vertical, first or second diagonal) depending on the disposition of its neighboring edge pixels. For each direction we count the number of edge pixels associated with it, which yields a 4 bin histogram.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">The learning procedure</head><p>Given a training database, we first pre-compute and store off-line for each image its three descriptors (related to each of the three visual features). These set of three vectors simplify the</p><formula xml:id="formula_3" coords="6,160.19,209.50,320.45,8.74">(a) (b) (c)</formula><p>representation of each image, by giving maximal information about its content (according to each considered feature). We now define a similarity measure between two images given a visual feature class. This measure is simply the euclidean distance between two vectors. The learning procedure which allows us to determine the type of high-level visual feature (and its measure) that is the most representative for each annotated keyword, is outlined below: 4. We normalize in [0, 1] all the similarity distances of each sample of each selected set I class k,w .</p><p>5. We combine the similarity distance measures of the selected sets I class k,w , with an identical weighting factor, in order to find a final set of images i associated with each annotated keyword w. The similarity measures of these final images are then normalized, and the noramlized similarity measure of an image i for the given word w is represented as R cluster (i, w) for retrieval, described in section 4.</p><p>The first 24 images of the set of images associated to the word garden are shown in Figure <ref type="figure" coords="7,505.25,112.02,3.88,8.74">4</ref>. We can see that, even if most images are not annotated by the word garden (the word does not exist in any field of the text associated with the image), we can visually count about 9 images which are related to gardens from the 14 non-annotated images.</p><p>Figure <ref type="figure" coords="7,120.61,503.80,3.88,8.74">4</ref>: Result of learning procedure applied to the word "garden". Below each image, we can read its identifier key in the database and the score (similarity measure) obtained after normalization. The images which are not annotated by the word "garden" have their identifier key written in a gray box.</p><p>3 Cross-language text retrieval</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Translation models</head><p>Two approaches are used for query translation, depending on the ressources available for the different languages. For Spanish, Italian, German, Dutch, Swedish, and Finnish, FreeLang bilingual dictionaries<ref type="foot" coords="7,143.82,646.85,3.97,6.12" target="#foot_1">2</ref> are used in a word-for-word translation approach. The foreign language words in the dictionaries are stemmed using Snowball stemmers<ref type="foot" coords="7,331.53,658.80,3.97,6.12" target="#foot_2">3</ref> , and the English words are left in their original form. The queries are also stemmed, and stop words are removed with a stoplist in the foreign language. The translated query consists of the set of all possible English word translations for each query term, each translated word having equal weight.</p><p>For French, a translation model trained on a web-aligned corpus is used <ref type="bibr" coords="7,438.21,708.20,14.62,8.74" target="#b9">[10]</ref>. The model associates a list of English words and their corresponding probabilities with a French word. As with the bilingual dictionaries, the French words are stemmed, and the English words are not. Word-for-word translation is done. For a given French root, all possible English translations are added to the translated query. The translation probabilities determine the weight of the word in the translated query. The term weights are represented implicitly by repeating a given translated word a number of times according to its translation probability. For French as well as for the other languages, the words in the translated query are stemmed using the Porter stemming algorithm.</p><p>This query translation approach was found to be optimal, using training data described in the following section. The parameters evaluated were:</p><p>• Whether to use a bilingual dictionary, or the translation model, for French.</p><p>• For a given query term, whether to pick just one translation from the dictionary or translation model, all translations, or in the case of the translation model, the first n probable translations.</p><p>• When to stem the English words: The English words could be stemmed in the dictionary, rather than after translation. This affects the number times a particular word appears, and therefore its implicit weight, in the final translated query. Without stemming English words in the dictionary, multiple forms of a word may appear as a possible translation for a foreign language stem. After the translated query is stemmed, the English root appears several times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CLIR process</head><p>For retrieval, the Okapi retreival algorithm <ref type="bibr" coords="8,284.88,387.44,15.50,8.74" target="#b10">[11]</ref> is used, implemented by the Lemur Toolkit for Language Modeling and Information Retrieval. In particular, the BM 25 weighting function is used. The following parameters contribute to the relevance score of a document (an image annoatation) for a query:</p><p>• BM 25 k1</p><p>• BM 25 b</p><p>• BM 25 k3</p><p>• FeedbackDocCount: the number of documents (image captions) to use for relevance feedback</p><p>• FeedbackTermCount: the number of terms to add to the expanded query</p><p>• qtf: the weight of the query terms added during relevance feedback</p><p>The training data used to optimize each of these parameters, as well as the translation approaches described in section 3.1 was the TREC-6 AP89 document collection and 53 queries in English, French, Spanish, German, Italian, and Dutch. Since no training data was available for Finnish and Swedish, the average of the optimal values found for the other languages is used.</p><p>While the training collection, consisting of news articles about 200-400 words in length, is quite different from the test collection of image captions, the volume of the training data (163000 documents, 25 or 53 queries, depending on the language, and 9403 relevance assessments) is much greater than the training data provided from the image collection (5 queries, 167 relevance assessments).</p><p>Once the parameters for relevance feedback and the BM 25 weighting function are optimized with the training data, retreival is performed on the test data, producing a list of images and their relevance scores, for each query. We annotate this image relevance score for a query, based on textual retrieval, as R text (i, q). A preliminary analysis shows that our image retrieval works well. In particular, using the French queries, our system produced the best results among the participants. This may be related to two factors: -The method of query translation used for these queries is reasonable. For French queries, we used a statistical translation model trained on parallel web pages. This translation model has produced good results in our previous CLIR experiments.</p><p>-The method based on keyword-feature type association we used in these experiments may be effective. However, further analysis has to be carried out to confirm this.</p><p>For the experiments with other languages, our results are relatively good -they are often among the top results. However, the absolute MAP is lower than for the French queries.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,219.60,338.97,163.80,8.74;3,90.00,108.86,422.00,215.00"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Workflow of image retrieval</figDesc><graphic coords="3,90.00,108.86,422.00,215.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,136.51,357.91,342.16,9.68;6,149.24,367.55,90.07,12.35;6,211.45,375.95,8.56,6.12;6,239.81,371.17,12.67,8.74;6,252.76,366.38,16.49,8.74;6,252.49,376.14,8.56,6.12;6,269.75,371.17,12.67,8.74;6,282.71,366.38,20.80,8.74;6,282.43,376.14,8.56,6.12;6,304.01,371.17,174.65,8.74;6,149.24,383.09,290.90,9.68;6,136.51,403.05,185.27,9.05;6,153.45,422.97,325.21,8.74;6,171.16,434.93,222.81,8.74;6,394.24,433.36,17.96,6.12;6,393.96,439.71,8.56,6.12;6,412.70,434.93,2.77,8.74;6,152.89,450.87,325.77,8.74;6,171.16,462.82,57.96,8.74;6,229.39,461.25,17.96,6.12;6,229.12,467.61,8.56,6.12;6,251.76,462.82,226.91,8.74;6,171.16,476.02,150.68,8.74;6,322.12,474.45,17.96,6.12;6,321.84,480.53,12.10,6.12;6,340.58,476.02,25.41,8.74;6,366.26,474.45,17.96,6.12;6,365.98,480.88,12.53,6.12;6,384.72,476.02,93.94,8.74;6,171.16,487.98,307.50,8.74;6,171.16,499.93,151.57,8.74;6,323.01,498.36,17.96,6.12;6,322.73,504.72,8.56,6.12;6,341.47,499.93,2.77,8.74;6,179.27,517.11,153.00,9.05;6,332.55,515.54,17.96,6.12;6,332.27,521.62,12.10,6.12;6,351.01,517.11,25.41,8.74;6,376.69,515.54,17.96,6.12;6,376.42,521.97,12.53,6.12;6,397.51,517.11,4.98,8.74;6,196.76,531.06,281.90,8.74;6,206.73,543.01,91.67,8.74;6,298.67,541.44,17.96,6.12;6,298.39,547.88,12.53,6.12;6,317.13,543.01,161.54,8.74;6,206.73,556.62,22.19,8.77;6,228.92,555.07,17.96,6.12;6,228.92,561.51,12.53,6.12;6,250.70,556.65,92.05,8.74;6,196.76,572.24,263.44,8.77;6,460.21,570.70,17.96,6.12;6,460.21,577.13,12.53,6.12;6,206.73,584.19,271.94,9.68;6,206.73,596.18,25.85,8.74;6,233.67,594.61,17.96,6.12;6,232.58,601.04,12.53,6.12;6,255.45,596.18,68.71,8.74;6,136.51,617.75,145.53,9.08;6,282.04,616.21,17.96,6.12;6,282.04,622.64,12.53,6.12;6,304.24,617.78,91.07,8.74;6,396.40,616.21,17.96,6.12;6,395.32,622.64,12.53,6.12;6,418.60,617.78,60.07,8.74;6,149.24,629.74,51.39,8.74"><head>1 . 3 .</head><label>13</label><figDesc>Let I w be the set of all images I w (each described by its three vectors or descriptors [D texture Iw , D edge Iw , D shape Iw ]) in the training database that are annotated with the keyword w and |I w |, the number of images in I w . 2. For each class { Texture, Edge, Shape } (a) We use a K-mean clustering procedure [6] (with a euclidean distance for the similarity measure) on the set of samples D class Iw . (b) This clustering allows us to approximate the distribution of the set of samples D class Iw by K spherical distributions (with identical radius) and to give K prototype vectors [D class 1,w , ..., D class k,w ] corresponding to the centers of these distributions. Several values of K are used to find the best clustering representation of D class Iw . i. For each prototype vector { D class 1,w , ..., D class k,w } • We search in the whole training database for the closest descriptors (or images) of D class k,w , according to the euclidean distance. Let I class k,w be this set of images. • We compute the number of the first top-level T samples of I class k,w also belonging to I w (best results are obtained with T = 10). Let N class k,w be this number. We retain the class(es) and I class k,w for which we have N class k,w above a given threshold ξ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="7,103.08,167.00,396.85,311.72"><head></head><label></label><figDesc></figDesc><graphic coords="7,103.08,167.00,396.85,311.72" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,105.24,745.31,117.44,6.99"><p>http://qccpack.sourceforge.net/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="7,105.24,737.50,90.33,6.99"><p>http://www.freelang.net</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="7,105.24,747.00,104.56,6.99"><p>http://snowball.tartarus.org</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The image relevance score based on clustering</head><p>The image analysis based on clustering, described in section 2.4, provides a list of relevant images i for a given word w, with a relevance score for each image, R cluster (i, w). The relevance score of an image for a query, based on clustering, is then a weighted sum of the relevance scores for that image for each query term:</p><p>In our approach, each word has the same weight, and the relevance score for the query is normalized, with λ w = 1 |q| , where |q| is the number of words in the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Combining the five image relevance scores</head><p>We now have 5 lists of images for each query, with the following relevance scores:</p><p>• R text (i, q)</p><p>• R cluster (i, q)</p><p>• R edge (i, q): The similarity between the query image q and a collection image i, according to the wavelet measure described in section 2.1.</p><p>• R texture (i, q): The similarty according to the texture class measure from section 2.2.</p><p>• R shape (i, q): The similarity according to the shape class measure from section 2.3.</p><p>Each of these relevance scores contributes to a final relevance score as follows :</p><p>R(i, q) = λ text R text (i, q) + λ cluster R cluster (i, q) + λ edge R edge (i, q) + λ texture R texture (i, q) + λ shape R shape (i, q)</p><p>The coefficients we chose for the contribution of each approach are as follows:</p><p>• λ text = 0.8</p><p>These values have been determined empirically using the training data provided in ImageCLEF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Filtering the list of images based on location, photographer, and date</head><p>A final filtering is applied to the list of images for a given query. A "dictionary" of locations is extracted from the location field in the entire collection's annotations. Similarly, a "dictionary" of photographers is extracted. If a query contains a term in the location dictionary, then the location of a potential image, if it is known, must match this location. Otherwise, the image is removed from the list. The same approach is applied to the photographer. Similarly, if a date is specified in the query, then the date of the image, if it is known, must satisfy this date constraint.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="10,110.48,284.25,402.52,8.74;10,110.48,296.20,75.83,8.74" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="10,415.88,284.25,97.12,8.74;10,110.48,296.20,45.29,8.74">Semi-automatic image annotation</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Czerwinski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Field</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,316.13,402.52,8.74;10,110.48,328.08,402.52,8.74;10,110.48,340.04,258.63,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,110.48,328.08,295.07,8.74">Blobworld: A system for region-based image indexing and retrieval</title>
		<author>
			<persName coords=""><forename type="first">Chad</forename><surname>Carson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Megan</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joseph</forename><forename type="middle">M</forename><surname>Hellerstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,428.04,328.08,84.96,8.74;10,110.48,340.04,185.27,8.74">Third International Conference on Visual Information Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,359.97,402.52,8.74;10,110.48,371.92,230.42,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,300.25,359.97,212.75,8.74;10,110.48,371.92,125.03,8.74">Automatic image annotation and retrieval using cross-media relevance models</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,257.09,371.92,52.63,8.74">ACM SIGIR</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,391.85,402.52,8.74;10,110.48,403.80,369.28,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,167.59,391.85,341.10,8.74">A theory for multiresolution signal decomposition : The wavelet representation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">G</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,110.48,403.80,285.57,8.74">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="674" to="693" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,423.73,402.52,8.74;10,110.48,435.68,315.49,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,288.16,423.73,220.43,8.74">Texture features corresponding to visual perception</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Yamawaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,110.48,435.68,236.37,8.74">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="460" to="473" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,455.61,392.03,8.74" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="10,156.05,455.61,251.97,8.74">Signal processing image processing and pattern recognition</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Banks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,475.53,402.51,8.74;10,110.48,487.49,379.66,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,290.89,475.53,222.10,8.74;10,110.48,487.49,25.15,8.74">Image compression using adaptative vector quantization</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Boucher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shlien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,144.23,487.49,208.41,8.74">Communications, IEEE Transactions on [legacy</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="180" to="187" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
	<note>pre -1988</note>
</biblStruct>

<biblStruct coords="10,110.48,507.41,385.83,8.74" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="10,162.49,507.41,146.22,8.74">Last square quantization in pcm&apos;s</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">P</forename><surname>Lloyd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957">1957</date>
		</imprint>
		<respStmt>
			<orgName>Bell Telephone Laboratories Paper</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,527.34,402.51,8.74;10,110.48,539.29,221.35,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,270.50,527.34,177.12,8.74">An algorithm for vector quantizer design</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Linde</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Buzo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,456.79,527.34,56.21,8.74;10,110.48,539.29,119.17,8.74">IEEE Transactions on Communications</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="84" to="95" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,559.22,402.54,8.74;10,110.48,571.17,402.52,8.74;10,110.48,583.13,344.05,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,267.64,559.22,225.79,8.74">Using statistical translation models for bilingual ir</title>
		<author>
			<persName coords=""><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michel</forename><surname>Simard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,110.48,571.17,402.52,8.74;10,110.48,583.13,245.23,8.74">Revised Papers from the Second Workshop of the Cross-Language Evaluation Forum on Evaluation of Cross-Language Information Retrieval Systems</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="137" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,603.05,402.53,8.74;10,110.48,615.01,402.52,8.74;10,110.48,626.96,62.90,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,474.04,603.05,38.96,8.74;10,110.48,615.01,23.75,8.74">Okapi at trec-3</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,154.15,615.01,241.67,8.74">Proc. of the Third Text REtrieval Conference (TREC-3)</title>
		<meeting>of the Third Text REtrieval Conference (TREC-3)</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="500" to="225" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
