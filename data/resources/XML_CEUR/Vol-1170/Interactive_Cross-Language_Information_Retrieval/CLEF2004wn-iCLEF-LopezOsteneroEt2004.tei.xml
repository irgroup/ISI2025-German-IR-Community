<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,93.13,148.86,413.62,15.15;1,96.90,170.78,409.22,15.15">Interactive Cross-Language Question Answering: Searching Passages versus Searching Documents</title>
				<funder>
					<orgName type="full">UNED (Universidad Nacional de Educación a Distancia)</orgName>
				</funder>
				<funder ref="#_Sy5z7jA">
					<orgName type="full">Spanish Government</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,134.96,204.67,109.95,8.74"><forename type="first">Fernando</forename><surname>López-Ostenero</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Departamento de Lenguajes y Sistemas Informáticos</orgName>
								<orgName type="institution">UNED</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,252.97,204.67,57.97,8.74"><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Departamento de Lenguajes y Sistemas Informáticos</orgName>
								<orgName type="institution">UNED</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,319.04,204.67,65.48,8.74"><forename type="first">Víctor</forename><surname>Peinado</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Departamento de Lenguajes y Sistemas Informáticos</orgName>
								<orgName type="institution">UNED</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,407.20,204.67,60.83,8.74"><forename type="first">Felisa</forename><surname>Verdejo</surname></persName>
							<email>felisa@lsi.uned.es</email>
							<affiliation key="aff0">
								<orgName type="department">Departamento de Lenguajes y Sistemas Informáticos</orgName>
								<orgName type="institution">UNED</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,93.13,148.86,413.62,15.15;1,96.90,170.78,409.22,15.15">Interactive Cross-Language Question Answering: Searching Passages versus Searching Documents</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1746DC81CB202B37CA857D8C02EA5842</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>iCLEF 2004 is the first comparative evaluation of interactive Cross-Language Question Answering systems. The UNED group has participated in this task comparing two strategies to help users in the answer finding task: the baseline system is just a standard document retrieval engine searching machine-translated versions of the documents; the contrastive system is identical, but searching passages which contain expressions of the appropriate answer type.</p><p>Although the users prefer the passage system because searching is faster and simpler, it leads to slightly worse results, because the document context (which is not available in the passage retrieval system) turns out to be useful to verify the correctness of candidate answers; this makes an interesting difference with automatic Q&amp;A systems. In addition, our experiment sets a strong baseline of 69% strict accuracy, showing that Cross-Language Question Answering can be efficiently accomplished by users without using dedicated Q&amp;A technology.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="611.998" lry="791.997"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In spite of its long name, "Interactive Cross-Language Question Answering" is not an exotic task, but rather a quite natural problem in the context of e.g. web searches: we want to know the answer to a question, and if the answer is out there in some web document, we want to find it as fast and easily as possible, and we do not want to miss the possibility of finding the answer just because it is written in a foreign language.</p><p>For our participation in the interactive CLEF track <ref type="bibr" coords="1,334.67,535.56,9.97,8.74" target="#b2">[3]</ref>, which is for the first time devoted to study Cross-Language Question Answering (CL-QA) from a user inclusive perspective, we have designed an experiment aiming at:</p><p>• Establishing a reasonable baseline giving initial quantitative and qualitative data about the nature and difficulty of the task.</p><p>• Finding out whether passages are more adequate than full documents for interactive answer finding.</p><p>• Experimenting with interactive features specifically aimed at improving answer finding processes.</p><p>To achieve these goals, we have designed and compared two CL-QA assistants:</p><p>• A reference search system which uses a document retrieval engine (Inquery <ref type="bibr" coords="1,449.64,697.82,10.80,8.74" target="#b1">[2]</ref>) to retrieve machine-translated versions (into the user's native language) of the target language documents. Our hypothesis is that standard Document Retrieval and Machine Translation technologies, coupled together, can be efficient tools to help users in the answer location task.</p><p>• A contrastive search system which is identical to the reference system, except for two issues:</p><p>1. It retrieves machine-translated passages rather than documents. The possibility of examining the context of a passage is intentionally excluded.</p><p>2. At the beginning of the search, the user is asked to specify the type of answer (named entity, date, quantity), and only passages containing possible answers are retrieved and shown to the user.</p><p>In order to compare both systems, we have recruited eight Spanish native speakers with low English skills, which have searched the CLEF English collection to find answers for 16 questions extracted from the CLEF QA 2004 question set. Answers have been collected in Spanish, manually translated into English, and sent to CLEF QA assessors for evaluation (see <ref type="bibr" coords="2,423.74,241.53,10.52,8.74" target="#b3">[4]</ref> for details on the evaluation criteria).</p><p>Section 2 describes our experimental design, Section 3 discusses the results, and finally we draw some conclusions in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Experiment design</head><p>Our experiment follows the iCLEF 2004 experiment design <ref type="bibr" coords="2,352.75,332.17,9.97,8.74" target="#b0">[1]</ref>, which prescribes how to conduct searches with eight subjects, 16 fixed questions, fixed document collections for each available target language, and the two search systems being compared:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Test data</head><p>We have used the Spanish version of the question set, and the English text collection, which comprises news data from 1994 and 1995 taken from the Los Angeles Times and the Glasgow Herald. News were translated with Systran Professional 3.0 (as provided by the iCLEF organization).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">User profiles</head><p>Our eight users were between 20 and 43 years old, had low or medium-low English skills, all were very familiarized with graphical interfaces and search engines, and in average they had little familiarity with Machine Translation systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Reference and Contrastive systems</head><p>Our Reference system is a straightforward document retrieval system (see Figure <ref type="figure" coords="2,470.30,542.73,3.88,8.74">1</ref>). Users type in queries in Spanish, and the system performs monolingual retrieval (using the Inquery API) against Systran translations of the original English news. The ranked list of results displays the document title, and the user can click to access the document contents. The interface has additional buttons to storage a document, to view storaged documents, and to end the search marking a document when an answer has been found.</p><p>The Contrastive system (Figure <ref type="figure" coords="2,259.65,614.46,4.43,8.74">2</ref>) begins by asking the user to select, from a radio button menu, which type of answer is appropriate for the question (a named entity, a date or a quantity). Then the search interface is similar, but 1. It retrieves machine-translated passages rather than documents. The possibility of examining the context of a passage is intentionally excluded, to test whether context is necessary or not to find and validate answers.</p><p>2. only passages containing the type of possible answers are retrieved and shown to the user.</p><p>The filter that discards unappropriate passages is straightforward and does not involve using any NLP tool: • A passage (sentences in our case) contains a named entity if there are expressions in uppercase where uppercase is not prescribed by punctuation rules. Locations are looked up in a gazeteer and filtered out, because "location" questions are excluded from the iCLEF question set.</p><p>• A passage contains a temporal reference if there is a match with a list of words denoting dates or a number between 1900 and 1999 (this temporal restriction is ad-hoc for CLEF data).</p><p>• Similarly, a passage contains a quantity if there is a number or a word from a given list.</p><p>Note that the aim of the filter is deciding whether there are named entities, quantities or dates, not finding them: that makes the task much easier. Note also that recall is much more important than precision, because we do not want to miss any potential answer. That makes our simple filter effective for our purposes, and its potential mistakes relatively harmless.</p><p>Overall, the filter identifies named entities in 75% of the sentences, which is too permisive to be useful. A real Named Entity Recognizer, able to distinguish between people, organizations, locations, etc., would be a useful substitute of our naive filter. In the other two categories, however, the filter is useful: only 21% of the sentences contain dates, and 43% contain quantities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Search sessions</head><p>Every subject searches all 16 questions, eight with each system, according to the latin-square matrix design prescribed by the iCLEF guidelines (see Table <ref type="table" coords="5,357.41,363.35,3.88,8.74" target="#tab_0">1</ref>). They filled in a pre-search questionnaire, two post-system questionnaires, and a final post-search questionnaire. The maximum search time per question was five minutes. Once time expired, the system stops the search, and the user has a last chance of writing an answer. user search order (system: A|B, question: 1 . . . 16) 1 A1 , A4 , A3 , A2 , A9 , A12, A11, A10, B13, B16, B15, B14, B5 , B8 , B7 , B6 2 B2 , B3 , B4 , B1 , B10, B11, B12, B9 , A14, A15, A16, A13, A6 , A7 , A8 , A5 3 B1 , B4 , B3 , B2 , B9 , B12, B11, B10, A13, A16, A15, A14, A5 , A8 , A7 , A6 4 A2 , A3 , A4 , A1 , A10, A11, A12, A9 , B14, B15, B16, B13, B6 , B7 , B8 , B5 5 A15, A14, A9 , A12, A7 , A6 , A1 , A4 , B3 , B2 , B5 , B8 , B11, B10, B13, B16 6 B16, B13, B10, B11, B8 , B5 , B2 , B3 , A4 , A1 , A6 , A7 , A12, A9 , A14, A15 7 B15, B14, B9 , B12, B7 , B6 , B1 , B4 , A3 , A2 , A5 , A8 , A11, A10, A13, A16 8 A16, A13, A10, A11, A8 , A5 , A2 , A3 , B4 , B1 , B6 , B7 , B12, B9 , B14, B15 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Comparison between systems</head><p>The main differences (in search results and search behaviour) between systems can be seen in Table <ref type="table" coords="5,118.88,721.73,3.88,8.74" target="#tab_1">2</ref>. The average (strict) accuracy is 5% higher for the baseline system, and the search behaviour (average searching time, confidence in the answers, average number of refinements) is very similar for both systems. The absolute performance (between .66 and .69 strict accuracy) is remarkably high: there is room for improvement, but it is fair to say that users can find answers efficiently without QA-specific machinery. This accuracy is obtained in an average time of only 3, 5 minutes, and with only 1.6 average refinements per question. Why our contrastive system, which has some QA-specific features, performs worse than the baseline system? Our observational studies, together with the questionnaires filled by our users, give some hints: • The results of post-system questionnaires (where users evaluated each system separately) can be seen in Figure <ref type="figure" coords="6,208.97,529.43,3.88,8.74" target="#fig_1">3</ref>. For all individual questions, the passage-retrieval system had better results: according to users, it was easier to search with, faster, it was easier to recognize answers, and even the translation quality (which is the same) was perceived as better. We can conclude that users felt more comfortable when searching with the contrastive system. Then why the performance is worse?</p><p>• The results of the post-search questionnaire (where users were explicitly instructed to compare can be seen in Figure <ref type="figure" coords="6,268.24,609.13,3.88,8.74" target="#fig_2">4</ref>. In this explicit comparison, again the passage-retrieval system is perceived as "easier to learn" and "easier to use". But when asked for the better system overall, both systems receive half of the votes. Why?</p><p>• The written comments made by our subjects, together with our observational study, give a clear answer: most subjects wrote that the passage retrieval system was easier and faster to use, but only the document retrieval system permitted looking at the full content of documents containing potential answers; and the context was perceived as a key factor to ensure that a potential answer was correct. In addition, the document context was also used to refine the query and/or search for similar documents that might verify a potential answer. This is a factor related not only to a document content, but also to the translation quality, which often creates doubts about the correctness of an apparent answer. From the comments made by our subjects, and from their search behaviour, it seemed clear that the preferred search facility would do passage retrieval, but with the possibility of accessing the context of a passage when desired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Failure analysis</head><p>Out of 148 answers, there were 33 judged as "W" (wrong), 7 as "inexact" and 1 as "unsupported". 21 wrong answers were simply time outs: the user was not able to find an answer in five minutes. In the remaining 12 + 7 + 1 = 20 cases, users gave an answer that was not correct. The sources of error are:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Misleading translations In most cases, an incorrect or misleading translation is the responsible</head><p>for an incorrect answer. In some cases, users were doubtful about an answer, and looked for additional evidence supporting the answer. Once time has expired, they preferred to give an answer with a low level of confidence, than no answer at all.</p><p>Human errors In a few cases, the user just made a mistake when writing the answer. For instance, a user stated that the Channel Tunnel costed "15,000 millions", without specifying the currency. In other occasion, a cut-and-paste error repeated the answer given for a former question.</p><p>Responsiveness criteria Occasionally, the user and the assessor had different opinions about the responsiveness or focus of an answer. For instance, when asking for the number of missing people caused by the typhoon Angela, a user said "more than 500 killed and 280 still missing", which was judged as inexact.</p><p>It is worth noticing that, while automatic Q&amp;A systems may avoid translating documents (by translating only selected query terms), in an interactive system it is unavoidable to translate documents if the user does not have target-language skills. Thus, accurate targeted translation is a specific requirement of interactive CL-QA systems.</p><p>It is also worth noticing that, in some occasions, users were able to jump over significant translation problems. For instance, When did Latvia gain independence? was pretty hard to answer, because Systran did not have "Latvia" in its bilingual vocabulary; thus, it remained untranslated in all documents. Users were looking for "Letonia" (Spanish translation of Latvia), but nothing was found. Some documents, however, spoke about "Latvia, Estonia y Lituania", and users were able to "disambiguate" Latvia from the context.</p><p>It is also worth mentioning that users were able to make more inferences than current Q&amp;A systems. An interesting example is When did Lenin die? Some users found a document talking about the beginning of the celebrations of the 70 th anniversary of Lenin's death. Substracting from the date of the document (Sunday 22 January, 1994), users correctly deduced 1924. A couple of users, however, answered "20 January, 1924" (because the document asserts that "celebrations started last Friday") which was incorrect, because the celebrations started on January 20 th but the real anniversary was on January 21 st . The accuracy by user, and its correlation with average search time, can be seen in Figure <ref type="figure" coords="8,505.25,656.57,3.88,8.74" target="#fig_3">5</ref>. Both accuracy and average search time are rather homogeneous across users; more than in our previous experiences in interactive CL Document Retrieval. Our impression is that users find the Q&amp;A task simpler to understand, easier and more amusing than document retrieval; thus, fatigue effects are less relevant. In some cases, a priori knowledge of the question domain permitted a better selection of query terms, but the effect on average accuracy is small. The accuracy by topic, and its correlation with average search time, can be seen in Figure <ref type="figure" coords="9,505.25,460.24,3.88,8.74" target="#fig_4">6</ref>. Obviously, the effect of topic difficulty is the predominant factor on accuracy. The most difficult topics are:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">User effects</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Topic effects</head><p>• What is Charles Millon's political party? (accuracy 0). No reference to Charles Millon was found by any user, probably due to mistranslation by Systran.</p><p>• How many people were declared missing in the Philippines after the Thyphoon "Angela"? (accuracy 1/8). The source of difficulty here is not in translation quality. Some users answered with preliminary, vague information; others mixed dead and dissapeared people.</p><p>• When did Latvia gain independence? (accuracy 3/8). As commented above, the source of difficulty is that Systran left "Latvia", which is the key term in this question, untranslated.</p><p>• Who committed the terrorist attack in the Tokyo underground? (accuracy 3/8). Apparently the source of difficulty was cross-linguality: it was hard to find good query terms, specially because "Tokyo" was misspelled in the Spanish question (it used the English spelling, Tokyo, instead of the Spanish spelling, Tokio) and Systran mixed both spellings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>Our first experiment in interactive Cross-Language Question Answering has produced some interesting results:</p><p>• First, we have set a strong baseline (69% strict accuracy) for the task, using standard Document Retrieval and Machine Translation technologies. Can automatic CL-QA systems be adapted to interactive settings to achieve significantly higher user performance? This is an interesting research question for upcoming iCLEF editions.</p><p>• The main source of difficulty is the cross-language nature of the search, rather than the idiosincrasy of the QA task. Task-specific term suggestion and machine translation techniques might be useful for interactive CL-QA.</p><p>• Users prefer passage retrieval to document retrieval for the CL-QA task, partly because full Machine Translated documents are noisy and discouraging. But once a potential answer is found, the context is sometimes helpful to validate it.</p><p>• Interactivity can effectively be used to add Q&amp;A specific restrictions to focus a passage search. In our contrastive system, users were asked to specify which type of answer was required for the question at hand. When, for instance, the answer had to be a date, only 21% of the sentences in the collection had to be searched.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,203.84,596.87,195.31,8.74;3,90.00,255.69,425.21,326.06"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: Documents system: Main interface</figDesc><graphic coords="3,90.00,255.69,425.21,326.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,220.94,483.60,161.13,8.74;6,145.59,192.49,311.79,275.99"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Post-system questionnaires</figDesc><graphic coords="6,145.59,192.49,311.79,275.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,181.45,407.11,240.08,8.74;7,159.77,108.86,283.45,283.13"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Post-search questionnaire comparing systems</figDesc><graphic coords="7,159.77,108.86,283.45,283.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,246.32,630.72,110.36,8.74;8,159.77,332.47,283.45,283.13"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Results by user</figDesc><graphic coords="8,159.77,332.47,283.45,283.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="9,244.42,437.25,114.16,8.74;9,159.77,139.01,283.45,283.13"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Results by topic</figDesc><graphic coords="9,159.77,139.01,283.45,283.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="4,90.00,108.86,425.21,326.06"><head></head><label></label><figDesc></figDesc><graphic coords="4,90.00,108.86,425.21,326.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="4,90.00,435.92,425.21,326.06"><head></head><label></label><figDesc></figDesc><graphic coords="4,90.00,435.92,425.21,326.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,90.00,516.53,363.20,120.00"><head>Table 1 :</head><label>1</label><figDesc>iCLEF 2004 Latin-Square Experiment Design 3 Results and discussion</figDesc><table coords="5,149.79,591.53,303.41,45.00"><row><cell>System</cell><cell cols="2">Accuracy</cell><cell>Time</cell><cell cols="3">Confidence # Refinements</cell></row><row><cell></cell><cell cols="2">strict lenient</cell><cell>(av.)</cell><cell cols="2">High Low</cell><cell>(av.)</cell></row><row><cell>Documents</cell><cell>.69</cell><cell>.73</cell><cell>209.05</cell><cell>44</cell><cell>20</cell><cell>1.6</cell></row><row><cell>Passages</cell><cell>.66</cell><cell>.72</cell><cell>195.20</cell><cell>41</cell><cell>23</cell><cell>1.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,196.27,649.66,210.45,8.74"><head>Table 2 :</head><label>2</label><figDesc>Comparison of results for both systems</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This research has been partially supported by a grant from the <rs type="funder">Spanish Government</rs>, project <rs type="projectName">R2D2</rs> (<rs type="grantNumber">TIC2003-07158-C04-01</rs>), and a grant by the <rs type="funder">UNED (Universidad Nacional de Educación a Distancia)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_Sy5z7jA">
					<idno type="grant-number">TIC2003-07158-C04-01</idno>
					<orgName type="project" subtype="full">R2D2</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,105.49,400.88,189.16,8.74" xml:id="b0">
	<monogr>
		<ptr target="http://nlp.uned.es/iCLEF" />
		<title level="m" coord="10,105.49,400.88,63.11,8.74">iCLEF website</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.49,420.80,407.50,8.74;10,105.50,432.76,314.14,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,271.37,420.80,139.27,8.74">The INQUERY retrieval system</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,434.16,420.80,78.84,8.74;10,105.50,432.76,283.98,8.74">Proceedings of the 3rd Int. Conference on Database and Expert Systems applications</title>
		<meeting>the 3rd Int. Conference on Database and Expert Systems applications</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.49,452.68,407.51,8.74;10,105.50,464.64,261.45,8.74" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="10,228.40,452.68,284.60,8.74;10,105.50,464.64,157.77,8.74">iCLEF 2004 Track Overview: Pilot Experiments in Interactive Cross-Language Question Answering</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Oard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>In This volume</note>
</biblStruct>

<biblStruct coords="10,105.49,484.57,407.52,8.74;10,105.50,496.52,407.51,8.74;10,105.50,508.48,59.08,8.74" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="10,164.91,496.52,303.65,8.74">Overview of the CLEF 2004 Multilingual Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vallin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Erbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sutcliffe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>In This volume</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
