<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,92.04,74.32,411.24,14.50;1,232.32,90.88,130.62,14.50">Cross-Language French-English Question Answering using the DLT System at CLEF 2004</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,252.72,114.68,89.86,8.96"><forename type="first">Richard</forename><forename type="middle">F E</forename><surname>Sutcliffe</surname></persName>
							<email>richard.sutcliffe@ul.iemichael.mulcahy@ul.ie</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Documents and Linguistic Technology Group Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Information Systems</orgName>
								<orgName type="institution">University of Limerick Limerick</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,273.24,126.44,49.14,8.96"><forename type="first">Igal</forename><surname>Gabbay</surname></persName>
							<email>igal.gabbay@ul.ie</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Documents and Linguistic Technology Group Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Information Systems</orgName>
								<orgName type="institution">University of Limerick Limerick</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,262.56,138.20,70.50,8.96"><forename type="first">Michael</forename><surname>Mulcahy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Documents and Linguistic Technology Group Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Information Systems</orgName>
								<orgName type="institution">University of Limerick Limerick</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,264.48,149.96,66.54,8.96"><forename type="first">Aoife</forename><surname>O'gorman</surname></persName>
							<email>aoife.ogorman@ul.ie</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Documents and Linguistic Technology Group Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Information Systems</orgName>
								<orgName type="institution">University of Limerick Limerick</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,92.04,74.32,411.24,14.50;1,232.32,90.88,130.62,14.50">Cross-Language French-English Question Answering using the DLT System at CLEF 2004</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D5ED6744F03948D0CDEB2BF358E82C56</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This article outlines the participation of the Documents and Linguistic Technology (DLT) Group in the Cross Language French-English Question Answering Task of the Cross Language Evaluation Forum (CLEF). Following our experiences last year <ref type="bibr" coords="1,216.24,360.68,161.95,8.96" target="#b2">(Sutcliffe, Gabbay and O'Gorman, 2003)</ref>, our aim was to improve the system particularly in the early stages of processing, and to make further refinements to other components. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Architecture of the CLEF 2004 DLT System</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Outline</head><p>The basic architecture of our system is standard in nature and comprises query type identification, query analysis and translation, retrieval query formulation, document retrieval, text file parsing, named entity recognition and answer entity selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Query Type Identification</head><p>As last year, simple keyword combinations and patterns were used to classify the query. This was accomplished by using the CLEF 03 queries and translated TREC queries <ref type="bibr" coords="2,309.72,104.96,50.22,8.96">(RALI, 2004</ref>) as a model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Query Analysis and Translation</head><p>This stage differed greatly from last year. We started off by tagging the query for part-of-speech using XeLDA (2004). We then carried out shallow parsing looking for various types of phrase. Each phrase was then translated using three different methods. Two translation engines and one dictionary were used. The engines were Reverso (2004) and <ref type="bibr" coords="2,122.64,183.80,81.56,8.96">WorldLingo (2004)</ref> which were chosen because we had found them to give the best overall performance in various experiments. The dictionary used was the Grand Dictionnaire Terminologique <ref type="bibr" coords="2,498.36,195.56,26.49,8.96;2,70.56,207.32,23.48,8.96">(GDT, 2004)</ref> which is a very comprehensive terminological database for Canadian French with detailed data for a large number of different domains. The three candidate translations were then combined -if a GDT translation was found then the Reverso and WorldLingo translations were ignored. The reason for this is that if a phrase is in GDT the translation for it is nearly always correct. It is an excellent resource. For example 'equipe de football' becomes 'football team' and not 'team of football', 'salle d'opera' (Canadian dialect for 'opera') becomes 'opera house' not 'room of opera' and so on. In the case where words or phrases are not in GDT, then the Reverso and WorldLingo translations were simply combined.</p><p>The types of phrase recognised were determined after a study of the constructions used in French queries together with their English counterparts. The aim was to group words together into sufficiently large sequences to be independently meaningful but to avoid the problems of structural translation, split particles etc which tend to occur in the syntax of a question, and which the engines tend to analyse incorrectly.</p><p>The structures used were number, quote, cap_nou_prep_det_seq, all_cap_wd, cap_adj_cap_nou, cap_adj_low_nou, cap_nou_cap_adj, cap_nou_low_adj, low_nou_low_adj, low_nou_prep_low_nou, low_adj_low_nou, nou_seq and wd. These were based on our observations that (1) Proper names usually only start with a capital letter with subsequent words uncapitalised, unlike English; (2) Adjective-Noun combinations either capitalised or not can have the status of compounds in French and hence need special treatment; (3) Certain noun-preposition-noun phrases are also of significance.</p><p>As part of the translation and analysis process, weights were assigned to each phrase in an attempt to establish which parts were more important in the event of query simplification being necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Retrieval Query Formulation</head><p>The starting point for this stage was a set of possible translations for each of the phrases recognised above. For each phrase, a boolean query was created comprising the various alternatives as disjunctions. In addition, alternation was added at this stage to take account of morphological inflections (e.g., 'go'&lt;-&gt;'went', 'company'&lt;-&gt;'companies' etc) and European English vs. American English spelling ('neighbour'&lt;-&gt;'neighbor', 'labelled'&lt;-&gt;'labeled' etc). The reason for this last step was the addition for this year of the Glasgow Herald collection to the existing LA Times. The list of the above components was then ordered by the weight assigned during the previous stage and the ordered components were then connected with AND operators to make the complete boolean query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Document Retrieval</head><p>During document retrieval, the boolean query was submitted to the DTSearch search engine <ref type="bibr" coords="2,451.56,623.72,73.28,8.96">(DTSearch, 2000)</ref> which had previously been indexed on the LA Times and Glasgow Herald collections, with each sentence in the collection being considered as a separate document for indexing purposes. This followed our observation that in most cases the search keywords and the correct answer appear in the same sentence.</p><p>In the event that no documents were found, the conjunction in the query (corresponding to one phrase recognised in the query) with the lowest weight was eliminated and the search was repeated. Some attempts were made this year to avoid the situation in which the query is inadvertently simplified to something insufficiently selective and highly frequent in the corpus (e.g. United States).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Text File Parsing</head><p>This stage is straightforward and simply involves retrieving the matching 'documents' (i.e. sentences) from the corpus and extracting the text from the markup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Named Entity Recognition</head><p>Named Entity recognition was carried out in the standard way using a mixture of grammars and lists. The number of types was increased to 75 by studying previous CLEF and TREC question sets and these were incorporated into the query categoriser also.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8">Answer Entity Selection</head><p>We used the highest-scoring method of answer selection. In this, the named-entity instance is selected which occurs in the vicinity of the maximum number of keywords taken from the translated query, across all document passages. We also experimented with Google re-ordering using a Magnini-type method <ref type="bibr" coords="3,424.68,219.08,100.18,8.96;3,70.56,230.84,68.59,8.96" target="#b0">(Magnini, Negri, Prevete and Tanev, 2002)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Runs and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Two Experiments</head><p>We submitted two runs which differed slightly in their term translation strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Type</head><p>Classif.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Correct Classification Incorrect Classification Run 1</head><p>Run 2 </p><formula xml:id="formula_0" coords="3,72.72,343.37,449.94,221.22">Ru n 1 Run 1 C NC R X U W R X U W R X U W R X U W animal 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 colour 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 company 3 0 1 0 0 2 1 0 0 2 0 0 0 0 0 0 0 0 def_org 9 0 2 1 0 6 2 1 0 6 0 0 0 0 0 0 0 0 def_person 10 1 3 2 0 5 3 2 0 5 0 0 0 0 0 0 0 1 distance 1 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 how_did_die 3 0 3 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 how_many3 11 2 1 0 0 10 0 0 0 11 0 0 0 2 0 0 0 2 how_old 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 name_part 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 nationality 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 pol_party 3 0 0 0 0 3 0 1 0 2 0 0 0 0 0 0 0 0 population 2 0 0 0 0 2 0 0 0 2 0 0 0 0 0 0 0 0 team 2 0 0 0 0 2 0 0 0 2 0 0 0 0 0 0 0 0 what_capital 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 what_country 4 0 1 0 0 3 0 0 0 4 0 0 0 0 0 0 0 0 what_mountain 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 what_river 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 when<label>17</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>Results are summarised by query type in Table <ref type="table" coords="3,266.64,722.12,3.77,8.96">2</ref>. Concerning query classification it shows for each query type the number of queries assigned to that type which were correctly categorised along with the number incorrectly categorised. The overall rate of categorisation success was 85% which is identical to the one achieved in TREC <ref type="bibr" coords="4,70.56,84.92,184.15,8.96" target="#b3">(Sutcliffe, Gabbay, Mulcahy and White, 2004)</ref>. The number of queries classified as unknown was 70.</p><p>The performance of question answering in Run 1 can be summarised as follows. Out of the 170 queries classified correctly, 35 were answered correctly. Out of the remaining 30 queries classified incorrectly a further three were answered correctly. Overall performance was thus 38 / 200 i.e. 19%. Results for Run 2 were as follows. 28 of the 170 queries were answered correctly along with two of the 30 queries giving a total of 30 / 200 i.e. 15%. In both runs 63 questions were answered NIL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Platform</head><p>We used a Dell PC running Windows 2000 and having 256 Mb RAM. The whole system was ported this year to SICStus Prolog 3.11.1 <ref type="bibr" coords="4,162.60,222.56,64.40,8.96">(SICStus, 2004)</ref> which is much faster than Quintus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions</head><p>The overall performance this year was 19% compared to the 11.5% we achieved last year. We can attribute this improvement mostly to a superior translation strategy although much further work on this is required.</p><p>If we exclude query types which were represented by just one example, the best performance (100%) was on how_did_die queries. However, there were only three queries of this type. On the more common types the best performance was achieved when answering 'when <ref type="bibr" coords="4,281.04,327.44,219.56,8.96">' and 'where' queries (47% and 41%, respectively)</ref>. The performance on the relatively common types how_many and when_year was poor (9%, 10%, respectively). A more sophisticated answer selection strategy may improve performance on these. It is hard to assess performance on many of the other query types due to the small number of each.</p><p>Query categorisation for this year stood at 85% compared to 79.5% last year. The reduction in the number of (correctly classified) unknown queries from 58 last year to 47 in the current run may reflect an improvement in the coverage of categorisation. However, among the unknown queries several categories emerged which should be added in future systems. These include queries which are similar in nature to list questions but ask for a single hyponym (e.g., Query 17 'name a cetacean'), queries about materials (e.g. Query 70 'What are fibre-optic cables made of?'), queries of the type 'What does company X sell/produce?' and queries about diseases (treatment, way of transmission), newspaper names, wars, and musical bands. This year the test set included 'how' and 'why' queries (e.g., Query 8 'How is the pope?', Query 87 'Tell me a reason for teenage suicide', Query 107 'How does acupuncture work?') which our system cannot answer A significant proportion of the queries are likely to remain unknown in increasingly difficult evaluations. The poor performance on unknown queries highlights the need to develop an alternative to our current strategy of answering such queries (i.e., finding a sequence of capitalised words). Less than a quarter of the correctly classified unknown queries this year can be answered by the current strategy.</p><p>Simple heuristics may improve performance on existing categories. For example, year numbers could often be eliminated from answers to how_many queries.</p><p>Our boolean search query formulation strategy was a big improvement on last year but was not without its problems. In particular the combination for each phrase of translation alternatives, inflection alternatives and spelling alternatives could result on occasion in highly complex queries which were a problem for our relatively lightly engineered search engine.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,172.56,555.13,4.50,8.10;3,194.16,555.13,4.50,8.10;3,215.76,555.13,4.50,8.10;3,237.36,555.13,4.50,8.10;3,258.96,555.13,4.50,8.10;3,280.56,555.13,4.50,8.10;3,302.16,555.13,4.50,8.10;3,323.76,555.13,26.10,8.10;3,366.96,555.13,4.50,8.10;3,388.56,555.13,4.50,8.10;3,410.16,555.13,4.50,8.10;3,431.76,555.13,4.50,8.10;3,453.36,555.13,4.50,8.10;3,474.96,555.13,4.50,8.10;3,496.56,555.13,4.50,8.10;3,518.16,555.13,4.50,8.10;3,72.72,565.85,55.02,9.30;3,150.96,565.69,4.50,8.10;3,172.56,565.69,4.50,8.10;3,194.16,565.69,4.50,8.10;3,215.76,565.69,4.50,8.10;3,237.36,565.69,4.50,8.10;3,258.96,565.69,4.50,8.10;3,280.56,565.69,4.50,8.10;3,302.16,565.69,4.50,8.10;3,323.76,565.69,4.50,8.10;3,345.36,565.69,4.50,8.10;3,366.96,565.69,4.50,8.10;3,388.56,565.69,4.50,8.10;3,410.16,565.69,4.50,8.10;3,431.76,565.69,4.50,8.10;3,453.36,565.69,4.50,8.10;3,474.96,565.69,4.50,8.10;3,496.56,565.69,4.50,8.10;3,518.16,565.69,4.50,8.10;3,72.72,576.41,42.04,9.30;3,146.40,576.25,9.06,8.10;3,172.56,576.25,4.50,8.10;3,194.16,576.25,4.50,8.10;3,215.76,576.25,4.50,8.10;3,237.36,576.25,4.50,8.10;3,258.96,576.25,4.50,8.10;3,280.56,576.25,4.50,8.10;3,302.16,576.25,4.50,8.10;3,323.76,576.25,4.50,8.10;3,345.36,576.25,4.50,8.10;3,366.96,576.25,4.50,8.10;3,388.56,576.25,4.50,8.10;3,410.16,576.25,4.50,8.10;3,431.76,576.25,4.50,8.10;3,453.36,576.25,4.50,8.10;3,474.96,576.25,4.50,8.10;3,496.56,576.25,4.50,8.10;3,518.16,576.25,4.50,8.10;3,72.72,586.97,23.56,9.30;3,146.40,586.81,9.06,8.10;3,172.56,586.81,4.50,8.10;3,194.16,586.81,4.50,8.10;3,215.76,586.81,4.50,8.10;3,237.36,586.81,26.10,8.10;3,280.56,586.81,4.50,8.10;3,302.16,586.81,4.50,8.10;3,323.76,586.81,26.10,8.10;3,366.96,586.81,4.50,8.10;3,388.56,586.81,4.50,8.10;3,410.16,586.81,4.50,8.10;3,431.76,586.81,4.50,8.10;3,453.36,586.81,4.50,8.10;3,474.96,586.81,4.50,8.10;3,496.56,586.81,4.50,8.10;3,518.16,586.81,4.50,8.10;3,72.72,597.53,16.14,9.30;3,146.40,597.37,9.06,8.10;3,172.56,597.37,4.50,8.10;3,194.16,597.37,4.50,8.10;3,215.76,597.37,4.50,8.10;3,237.36,597.37,26.10,8.10;3,280.56,597.37,4.50,8.10;3,302.16,597.37,4.50,8.10;3,323.76,597.37,26.10,8.10;3,366.96,597.37,4.50,8.10;3,388.56,597.37,4.50,8.10;3,410.16,597.37,4.50,8.10;3,431.76,597.37,4.50,8.10;3,453.36,597.37,4.50,8.10;3,474.96,597.37,4.50,8.10;3,496.56,597.37,4.50,8.10;3,518.16,597.37,4.50,8.10;3,72.72,608.09,35.72,9.30;3,146.40,607.93,30.66,8.10;3,194.16,607.93,4.50,8.10;3,215.76,607.93,4.50,8.10;3,237.36,607.93,26.10,8.10;3,280.56,607.93,4.50,8.10;3,302.16,607.93,4.50,8.10;3,323.76,607.93,26.10,8.10;3,366.96,607.93,4.50,8.10;3,388.56,607.93,4.50,8.10;3,410.16,607.93,26.10,8.10;3,453.36,607.93,4.50,8.10;3,474.96,607.93,4.50,8.10;3,496.56,607.93,26.10,8.10;3,72.72,618.65,23.90,9.30;3,141.84,618.65,56.82,9.30;3,215.76,618.65,4.50,9.30;3,237.36,618.65,47.70,9.30;3,302.16,618.65,4.50,9.30;3,323.76,618.65,26.10,9.30;3,366.96,618.65,4.50,9.30;3,388.56,618.65,4.50,9.30;3,410.16,618.65,26.10,9.30;3,453.36,618.65,4.50,9.30;3,474.96,618.65,4.50,9.30;3,496.56,618.65,26.10,9.30;3,106.56,641.87,382.30,10.29;3,106.56,653.72,382.30,8.96;3,106.56,665.48,382.26,8.96;3,106.56,677.24,323.97,8.96"><head></head><label></label><figDesc>by Query Type. The columns C and NC show the numbers of queries of a particular type which were classified correctly and not correctly. Those classified correctly are then broken down into Right, ineXact, Unsupported and Wrong for each of the two runs Run 1 and Run 2. Finally, those classified incorrectly are broken down in the same way.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="1,83.28,414.47,426.22,220.01"><head>Table 1 : Some of the Question Types used in the DLT system</head><label>1</label><figDesc></figDesc><table coords="1,83.28,414.47,426.22,160.37"><row><cell>Type</cell><cell>Example Question</cell><cell>Translation</cell></row><row><cell>what_capital</cell><cell>130 Quelle est la capitale du Vénézuela?</cell><cell>What is the capital of Venezuela?</cell></row><row><cell>company</cell><cell>149 Qui fabrique Invirase?</cell><cell>Who manufactures Invirase?</cell></row><row><cell>what_country</cell><cell>37 Dans quel pays européen est située la</cell><cell>In which European country is the town of</cell></row><row><cell></cell><cell>ville de Galway?</cell><cell>Galway located?</cell></row><row><cell>mountain</cell><cell>162 Quelle est la plus haute montagne du</cell><cell>What is the highest mountain of the world?</cell></row><row><cell></cell><cell>monde?</cell><cell></cell></row><row><cell>where</cell><cell>166 Où se trouve Halifax?</cell><cell>Where is Halifax?</cell></row><row><cell>how_did_die</cell><cell>47 Comment est mort River Phoenix?</cell><cell>How did River Phoenix die?</cell></row><row><cell>who</cell><cell>40 Qui a réalisé "Braveheart"?</cell><cell>Who directed "Braveheart"?</cell></row><row><cell>when</cell><cell>30 Quand est-ce que le prince Charles et</cell><cell>When did prince Charles and Diana get</cell></row><row><cell></cell><cell>Diana se sont mariés?</cell><cell>married?</cell></row><row><cell>unknown</cell><cell>36 Citez une unité de radioactivité</cell><cell>Name a unit of radioactivity</cell></row></table><note coords="1,368.01,601.91,120.85,10.29;1,106.56,613.76,382.38,8.96;1,106.56,625.52,68.13,8.96"><p>. The second column shows a sample question for each type. Translations based on submission to WorldLingo are listed in the third column.</p></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,70.56,73.16,454.28,8.96;5,70.56,84.92,454.29,9.49;5,70.56,96.68,160.77,9.49" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,299.88,73.16,224.96,8.96;5,70.56,84.92,74.63,8.96">Is it the Right Answer? Exploiting Web Redundancy for Answer Validation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Prevete</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tanev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,152.76,85.07,368.23,9.34">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-07-06">2002. July 6-12, 2002</date>
			<biblScope unit="page" from="425" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,70.56,120.20,283.81,8.96;5,70.56,143.72,259.02,8.96;5,70.56,167.24,261.37,8.96" xml:id="b1">
	<analytic>
		<title/>
		<ptr target="http://www.sics.se/isl/sicstuswww/site/index.html" />
	</analytic>
	<monogr>
		<title level="j" coord="5,70.56,120.20,23.00,8.96">RALI</title>
		<imprint>
			<date type="published" when="2004">2004. 2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,70.56,190.76,435.30,8.96;5,70.56,202.52,433.05,9.49;5,70.56,214.28,208.17,9.49" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,293.16,190.76,212.70,8.96;5,70.56,202.52,146.52,8.96">Cross-Language French-English Question Answering using the DLT System at CLEF 2003</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">F E</forename><surname>Sutcliffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gabbay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>O'gorman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,224.16,202.67,269.88,9.34">Proceedings of the Cross Language Evaluation Forum, CLEF 2003</title>
		<meeting>the Cross Language Evaluation Forum, CLEF 2003<address><addrLine>Trondheim, Norway</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-08-21">2003. August 21-22, 2003</date>
			<biblScope unit="page" from="373" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,70.56,237.80,443.89,8.96;5,70.56,249.56,450.93,9.49;5,70.56,261.32,396.09,9.49" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,332.88,237.80,181.57,8.96;5,70.56,249.56,46.20,8.96">Question Answering using the DLT system at TREC 2003</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">F E</forename><surname>Sutcliffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gabbay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mulcahy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,304.08,249.71,217.41,9.34;5,70.56,261.47,45.00,9.34">Proceedings of the Twelfth Text Retrieval Conference, TREC 2003</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<meeting>the Twelfth Text Retrieval Conference, TREC 2003<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>NIST Special Publication</publisher>
			<date type="published" when="2003">2004. November 18-21, 2003</date>
			<biblScope unit="page" from="500" to="255" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
