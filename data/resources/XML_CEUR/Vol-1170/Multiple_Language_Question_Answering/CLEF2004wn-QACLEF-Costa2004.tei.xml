<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,151.80,69.83,341.26,11.88;1,274.20,84.95,62.98,11.88">First evaluation of Esfinge -a question answering system for Portuguese</title>
				<funder ref="#_CyWPhez">
					<orgName type="full">POSI</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,285.48,109.65,40.40,8.43"><surname>Luís Costa</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Linguateca at SINTEF ICT</orgName>
								<address>
									<addrLine>Pb ; 124 Blindern</addrLine>
									<postCode>0314</postCode>
									<settlement>Oslo</settlement>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,151.80,69.83,341.26,11.88;1,274.20,84.95,62.98,11.88">First evaluation of Esfinge -a question answering system for Portuguese</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B1A0CA213E60A639C4530A22215A9840</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper I will start by describing Esfinge -a general domain Portuguese question answering system, and then the strategies I used to participate in the CLEF-2004 QA track. Then I will present and discuss the results obtained and finally describe some of the work planned for the near future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Esfinge</head><p>With a question answering system we want, for a given question, that the system be able of returning answers with the help of an information repository. This task requires the processing of the question and of the information repository. Existing systems use in this processing various linguistic resources like taggers, named entities extractors, semantic relations, dictionaries, thesauri, etc… Esfinge (http://acdc.linguateca.pt/Esfinge/) is based on the architecture described by Eric Brill in <ref type="bibr" coords="1,458.52,312.69,45.35,8.43" target="#b2">(Brill, 2003)</ref>. Brill tried to check the results that could be obtained by investing less in the resources to process the question and the information repository and more in the volume of the information repository itself. The Web, as the biggest free information repository that we know is a good candidate for these experiences. Brill's approach was never tried for Portuguese and this language is quite used in the Web <ref type="bibr" coords="1,353.88,355.89,85.53,8.43" target="#b0">(Aires &amp; Santos, 2002)</ref>. The motivation to start developing Esfinge was to check the results that could be obtained by applying Brill's approach to Portuguese.</p><p>The planned architecture has four modules: Question reformulation N-grams harvesting N-grams filtering N-Grams composition</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Question reformulation</head><p>In this module, patterns of plausible answers to a given question are obtained. These patterns are based on the words in the question. As an example, for the question: "In which year did Vasco da Gama arrived in India?" a plausible pattern would be "Vasco da Gama arrived in India in". It's too optimistic to expect the existence of pages with answers in "friendly" formats for all the questions (with the exact format as the result of the question reformulation module). Therefore, patterns of plausible answers with less ambitious strings, like for example the simple conjunction of the question words are also considered. Each one of these patterns is scored according to the probability of helping to find correct answers. The patterns were initially scored according to my intuition. At the moment the scores range from 1 to 20.</p><p>The linguistic information of this module is encapsulated in a text file using the regular expression syntax of the computer programming language Perl. Each triple (question pattern, answer pattern, score) is defined in a line separated by a slash (/).</p><p>Here follows a sample of the referred text file (it's actually a simplification for clarity sake):</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>O que ([^\s?]*) ([^?]*)\??/"$2 $1"/10 O que ([^?]*)\??/$1/1</head><p>The first rule says that for a question starting with "O que X Y?" (What X Y?), answers with the pattern "Y X" should be granted the score 10 (since Y and X are enclosed in double quotes, it means this is a phrase pattern -Y must appear just before X). For the question "O que é a MTV?" (What is MTV?), this rule generates the pattern "a MTV é" with the score 10. The second rule says that for a question starting with "O que X?" (What X?), answers with the pattern X should be granted the score 1. For the question in the previous example, this rule generates the pattern "é" "a" "MTV" with the score 1. Since the words in the pattern are not all enclosed in a pair of double quotes -this means they don't need to appear in this order or even in the same sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">N-grams harvesting</head><p>In this module, the resulting patterns of the Question Reformulation module are queried against an information repository. For that purpose they are submitted to a web search engine (for the moment I've been using Google <ref type="bibr" coords="2,92.64,211.17,9.90,8.43" target="#b5">[6]</ref>).</p><p>In Figure <ref type="figure" coords="2,129.60,232.77,4.68,8.43" target="#fig_0">1</ref> we can see the results of querying the pattern "a antiga capital da Polónia" (the former capital of Poland) in Google. There is some hope that the correct answer will be among the extracted N-grams Next, these N-grams of different lengths will be scored accordingly to their frequency, length and the scorings of the patterns that originated them. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">N-grams filtering</head><p>This module re-evaluates the scorings obtained in the module "N-grams harvesting". In this module the N-grams will be analysed by their particular features. To a given question, even if we don't know the answer, we can predict the type of expected answer. For example:</p><p>-A "When?" question implies an answer of type "date". It can be more or less precise: a year (like 1973) or a complete date (like 11/10/1973), but answers like "Lisboa" or "George W. Bush" don't make sense in this context.</p><p>-A "How many?" question implies an answer of type "number". Answers like "Oslo" or "5/8/2004" are not acceptable answers.</p><p>Analysing the N-Grams about the presence of digits, capitalization and typical patterns may allow reclassifying those N-Grams or even discarding them. The PoS information provided by a morphologic analyser may also be used to enhance the scorings of N-grams with interesting sequences of PoS categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">N-grams composition</head><p>This module tries to deal with questions with a set of answers, like "Who were the musicians in Queen?". The complete answer to this question demands the composition of the word N-grams "Freddy Mercury", "Brian May", "Roger Taylor", "John Deacon" that can be expected to be among the top scored word N-grams obtained from the three previous modules. The first task in this module is to determine whether the type of answer is singular (ex: "Who was the first king of Norway?"), plural with a known number of items (ex: "Which are the three largest cities in Portugal?") or plural with an unknown number of items (ex: What are the colours of Japan's flag?"). For the first type this module will return the best scored word N-gram resulting from the previous modules. For the second type it will return the required number of best scored word N-grams (three, in the example above). For the third type, it will need to decide which word N-grams will be part of the answer. This can be done using a threshold that will define which word N-grams will be part of the answer according to their scoring. The proximity of the scoring values can also be used as a decisive factor.</p><p>Esfinge is still in its first stages of development, but participating in the CLEF-2004 QA track seemed a good way of evaluating the work done so far, feel some of the difficulties in this IR field and get in touch with the state-of-the-art of actual QA systems and their approaches. For the QA-CLEF monolingual track, one had to supply, along with each answer, one document in the document collection that supported it. As said above, my system originally used Google's search results and was mainly statistical (tried to use the redundancy existing in the Web), so I knew I would need to add some extra functionalities. I tested three different strategies. In the first, the system searched the answers in the CLEF document collection (Run1). In the second, it searched the answers in the Web and used the CLEF document to confirm these answers (Run 2). Finally, in the third strategy my system searched the answers in the Web (this one was not submitted to the organization).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Run 1</head><p>The first thing I needed was some way of searching in the document collection. I have some experience in encoding corpora using IMS Workbench <ref type="bibr" coords="4,248.40,260.85,73.32,8.43" target="#b4">(Christ et al., 1999)</ref> as well as using its querying capabilities. So, it seemed a good idea to use it to encode the CLEF document collection and to use its querying capabilities to search for desired patterns. Another important decision to be made was the size of the text unit to be considered when searching for patterns -the entire text of each document, or a passage: a fixed number of sentences or a fixed number of words. I had not a definitive answer for this question, so I chose to do some experiments. Since the document length seemed too big for a unit, I tried the three following strategies:</p><p>-Considered the text unit as 50 contiguous words. This is done dynamically: it is possible to query corpora encoded using IMS Workbench for the context (in terms of words) in which the required patterns co-occur.</p><p>-Divided each document into sentences. Those sentences were considered as the text unit. . To segment the document collection into sentences, I used the Perl Module Lingua::PT::Segmentador <ref type="bibr" coords="4,417.12,379.65,10.13,8.43" target="#b6">[7]</ref>. The resulting sentences had an average of 28 words per sentence.</p><p>-Divided each document into sets of three sentences. Those sets of three sentences were considered as the text unit.</p><p>For each question in the QA track, Esfinge did the following steps:</p><p>1. Question reformulation -Submitted the question to the question reformulation module. The result was a set of pairs (answer pattern, score).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Passage extraction</head><p>-Searched each of these patterns in the document collection and extracted the text units (50 contiguous words, one sentence or three sentences) where the pattern was found. The system discards stop-words without context. For example in the query "a" "antiga" "capital" "da" Polónia", the words "a" and "da" are discarded while in the query "a antiga capital da Polónia" (phrase pattern) they are not discarded. Currently I'm discarding the 22 most frequent words in the CETEMPúblico corpus <ref type="bibr" coords="4,289.20,574.05,89.13,8.43" target="#b9">(Santos &amp; Rocha, 2001)</ref>. At this stage the system retrieved a set of document passages {P 1 , P 2 … P n }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">N-grams harvesting</head><p>-Computed the distribution of word n-grams (from length 1 to length 3) of the document excerpts.</p><p>-Ordered the list of word n-grams according to a score based on the frequency, length and scorings of the patterns that originated the document excerpts where the n-grams were found (formula in section 1.2). At this stage the system had an ordered set of possible answers (A 1 , A 2 … A n ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">N-grams filtering</head><p>-The next step was to discard some of these possible answers using a set of filters.</p><p>The filters used were:</p><p>-First, a filter to discard answers that are contained in the questions. Ex: for the question "Qual é a capital da Rússia" (What is the capital of Russia?), the answer "capital da Rússia" (capital of Russia) is not desired and should be discarded.</p><p>-Then, a filter that used the morphologic analyser jspell <ref type="bibr" coords="5,308.88,165.81,101.88,8.43" target="#b10">(Simões &amp; Almeida, 2001)</ref> to check the PoS of the various words in each answer. The analyser returns a set of possible PoS tags for each word. I erroneously assumed that the order in which the PoS tags were returned was related to their frequency. With that in mind, I was using only the first PoS for each word. Recently I found out that this assumption was wrong. This filter considered some PoS as "interesting": adjectives (adj), common nouns (nc), numbers (card) and proper nouns (np). All answers whose first and final word didn't belong to one of these "interesting" PoS were discarded. It's worthwhile to say that most probably my misinterpretation of the analyser's results led to a poor performance by this filter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example:</head><p>For the question "Quem é Andy Warhol?" (Who is Andy Warhol?), the system had the following answers among the highest scored: After applying the filter the set of highest scored answers will be:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>artista: nc cola em garrafa: nc prep nc</head><p>-The final answer will be the candidate answer with the highest score in the set of candidate answers which were not discarded by any of the filters above. If all the answers were discarded by the filters then the final answer is NIL (meaning the system is not able to find an answer in the document collection).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Run 2</head><p>It was possible to send two sets of results to the organization. I wanted to do some experiments using also the Web as source since that's the line of work where I expect to get better results. For that purpose I selected one of the previous experiences to send to the organization (my run1): the one considering sets of three sentences as the text unit because it seemed the one with best results, even though the results were quite similar in all three experiments.</p><p>The next experiment used the strategy described in <ref type="bibr" coords="6,289.44,211.17,65.89,8.43" target="#b3">(Brill et al., 2001)</ref>. First, it looked for answers in the Web, and then tried to find documents in the document collection supporting those answers. It submitted the patterns obtained in the question reformulation module to Google. Then the document snippets {S 1 , S 2 … S n ) were extracted from Google's results pages. These snippets are usually composed by fragments of the different sentences in the recovered documents that contain the query words and have approximately 25 words. The next step was to compute the distribution of word n-grams (from length 1 to length 3) existing in this document snippets. From this point the algorithm followed the one described as run 1, with an extra filter in the N-grams filtering module: a filter that searched the document collection for documents supporting the answercontaining both the candidate answer and a pattern obtained from the question reformulation module. This filter is necessary because it was stated in the task guidelines that the system should return the code of a document supporting each answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Brazilian Portuguese. A problem?</head><p>Using texts in Brazilian web pages may enlarge the corpus the system uses to find answers, but may also bring some problems. The system may return an answer in the Brazilian variant which is not possible to support in the document collection, which was built with newspaper texts written in European Portuguese.</p><p>Example: For the question "Qual é a capital da Rússia?" (What is the capital of Russia?), my system returned the answer "Moscou" (in the Brazilian variant). It would be much easier to support the answer "Moscovo" (same word in the European variant).</p><p>Another problem may occur when the scoring gets diluted by the two variants (like "Moscou" and "Moscovo" in the example), thus allowing other answers to get better scores. Searching only in Portuguese pages can obviate this problem, but will diminish the corpus to search into.</p><p>Yet another example can be illustrated by the query in section 1.2: "a antiga capital da Polónia". Even though, using the word "Polónia" (Portuguese variant) in the query, this word is not on the top 10 of harvested n-grams. On the other hand, "Polônia" (in the Brazilian variant) is third placed on the n-gram ranking. The reason for this is that Google doesn't differentiate between accentuated and non-accentuated characters, so the characters "ó", "ô" and "o" are exactly the same thing to this search engine. This can be a serious problem, when one is processing a language with the variety and heavy use of accentuation as present in Portuguese. One way to obviate this problem is to develop a post-Google filter to discard non-interesting documents, thus overcoming Google's limitations regarding the Portuguese.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Web-only experiment</head><p>For the present paper, I did an extra "run" using the Web as document collection and without crosschecking the answers in CLEF's document collection. I thought this experience could give some insight on whether there are advantages in combining two different information sources (Web and CLEF's document collection) or whether one can get better results using only one of these information sources.  <ref type="table" coords="7,316.08,343.77,4.68,8.43">1</ref> In Table <ref type="table" coords="7,126.60,359.85,4.68,8.43">1</ref> we can see that the results in run2 (the one which used the Web crosschecking the results in the document collection) are slightly better. However we can also see that the type of question is not irrelevant to the results. For example run1 had better results for questions of type "Qual" (Which). There are also some relatively frequent questions types without any right answer in either of the runs (like "Como", "Quando", "De que"). This probably means that there is something in these types of questions that Esfinge doesn't deal properly in the answer-finding procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Results by type of question</head><p>Both run1 and run2 were evaluated by the organization. To evaluate my Web-only experience I needed to know the right answers. For that purpose I created a list with my "right answers" to the question set. The Web-only experience is in some aspects a different task from the one proposed in CLEF. For example it was stated in CLEF's guidelines <ref type="bibr" coords="7,216.60,467.97,10.80,8.43" target="#b8">[9]</ref> that some questions might have no answer in the document collection (NIL answer), but it's much more difficult to say such thing when using the Web as the document collection. For that reason I considered not answered questions as wrong when evaluating this experience. Since my system was not recording the addresses of the documents it used to get the answers in the Web, it was not possible to check if the answers were supported or not.</p><p>Globally, we can see that the best results were obtained combining the use of the document collection and the Web. The worst results are the ones obtained using solely the Web. It is somehow surprising that the results using solely the document collection are better than the ones using solely the Web, since the approach I'm trying to test was designed to take advantage of the redundancy in larger corpora. Possible explanations for this are:</p><p>-My system is not extracting efficiently text from the Web. Possibly it is getting control symbols and documents in other languages -according to Nuno Cardoso (p.c.), it is common for search engines to mistake UTF for iso8859-1 character encoding. Also, the snippets resulting from the search engine are most probably not good enough to extract good answers, since most of those snippets are formed by truncated sentences.</p><p>-Some documents in the Web, rather than helping to find answers, do the exact opposite (jokes, blogs, …).</p><p>-The text size unit of 3 sentences 90 words gives a larger context, while many Google snippets do not even include all the words in the query.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,288.84,527.97,33.72,8.43;2,92.64,263.16,425.76,256.80"><head>Figure 1</head><label>1</label><figDesc>Figure 1</figDesc><graphic coords="2,92.64,263.16,425.76,256.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,92.64,219.81,126.32,8.43;3,92.64,241.41,338.28,8.43;3,92.64,263.01,22.88,8.43;3,92.64,284.61,82.32,8.43;3,92.64,295.41,231.92,8.43;3,92.64,306.21,68.64,8.43"><head>I</head><label></label><figDesc>'m using the following equation: N-gram score = (F * S * L) through the first 100 snippets resulting from the web search where F = n-gram frequency S = score of the search pattern which recovered the document L = n-gram length</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>I thank my colleague <rs type="person">Diana Santos</rs> (<rs type="affiliation">Linguateca / SINTEF</rs>) for all the valuable suggestions and for helping me to write this paper in a much more understandable way than it was written in the preliminary versions. I thank <rs type="person">Nuno Cardoso</rs> (<rs type="affiliation">Linguateca/XLDB</rs>) for the final revision of this paper. I thank <rs type="person">Alberto Simões</rs> (<rs type="affiliation">Linguateca/Universidade do Minho</rs>) for the hints on using the Perl Modules "jspell" [11], "Lingua::PT:: Atomizador" [7] and "Lingua::PT::Segmentador" [7]. I also thank the <rs type="institution">Fundação para a Ciência e Tecnologia</rs> for the grant <rs type="grantNumber">POSI/PLP/43931/2001</rs>, co-financed by <rs type="funder">POSI</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_CyWPhez">
					<idno type="grant-number">POSI/PLP/43931/2001</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results by question length</head><p>In table 2, I try to study the influence of the question length in the results of "run1" and "run2".</p><p>In order to determine the length of the questions, I used the Perl Module Lingua::PT::Atomizador <ref type="bibr" coords="8,461.64,316.05,11.04,8.43" target="#b6">[7]</ref> to tokenize the questions.. In "run1" the most significant results are obtained in questions from length 6 to 8, while in "run2" the system gets better results in questions from length 5 to 6. This difference can be explained by the different length of the passages recovered from the Web and from the document collection. The passages recovered from the Web being shorter, may be more suited to answer shorter questions, while the passages recovered from the document collection being longer, needs the questions to be longer in order to get the appropriate context. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results considering 5 answers per question</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3</head><p>The system can be configured to give more than one answer to a question. It returns these answers ordered by their probability of being the right answer. In this table a question is considered rightly answered if a right answer is found in the top 5 answers returned by the system.</p><p>To do this evaluation I needed also to know if the answers were supported in the document collection. For that purpose I included in my list of "right answers" the list of questions to which I couldn't find any answer supported by the document collection (NIL answers). The results are obviously better than the ones present in the previous table, but not dramatically. This suggests that most problems are located before the scoring of the candidate answers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Causes for wrong answers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4</head><p>In the table above I tried to find out why the system produces wrong answers. To find the causes takes some time, so I started with the run with best results (run 2) and did the evaluation only for the first 30 questions of the question set. For some questions I counted more than one reason for failure. This sort of evaluation can give some insight into the system modules that are causing more errors and therefore should be looked into more in detail.</p><p>4 Future work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Development of Esfinge</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Question reformulation</head><p>In this module the linguistic information is encapsulated in a text file using Perl's regular expression syntax. This syntax is quite powerful, however it is much more suited to the thought processes of computer-scientists than to linguists' ones. In case it is intended to include professionals in that area to improve the question reformulation patterns in a more advanced stage of development, it would be better to use a friendlier syntax.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">N-grams harvesting</head><p>There are planned experiences about extracting word N-grams not from the snippets returned by the search engine, but from the actual pages. Other planned experiences are related to the type of web pages to be considered: only European Portuguese pages, pages written in other languages, only news sites…</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Machine learning techniques</head><p>An interesting experience/refinement that is planned is to use a set of questions associated with their answers as a training set for the system The results of the system on the training set questions can be compared with the correct answers. The scorings of the patterns and/or the word n-grams can then be changed and the system executed again against the training set, the new results compared with the right answers and the results checked again to understand if the system is improving.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Further evaluation of Esfinge</head><p>I plan to use a multitude of sources to further evaluate Esfinge:</p><p>• The questions and answers created by QA@CLEF • A set of real questions and answers found on the web, created by humans, using several distinct methods for collecting them (oráculo) • A set of questions posed by real users (from Esfinge' s logs) • A set of questions with answers, created and validated by myself</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="10,105.96,308.73,403.26,8.43;10,92.64,319.65,387.56,8.43" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,227.98,308.73,125.53,8.43">Measuring the Web in Portuguese</title>
		<author>
			<persName coords=""><forename type="first">Rachel</forename><forename type="middle">&amp;</forename><surname>Aires</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Diana</forename><surname>Santos</surname></persName>
		</author>
		<ptr target="http://www.linguateca.pt/Diana/download/AiresSantosEuroWeb2002.html" />
	</analytic>
	<monogr>
		<title level="m" coord="10,374.04,308.73,98.72,8.43">Euroweb 2002 conference</title>
		<meeting><address><addrLine>Oxford, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-12-18">17-18 December 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.96,341.13,394.88,8.43;10,92.64,351.93,384.12,8.43;10,92.64,362.85,265.08,8.43" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,255.14,341.13,245.70,8.43;10,92.64,351.93,38.24,8.43">The Design, Implementation, and Use of the {N}gram {S}tatistic {P}ackage</title>
		<author>
			<persName coords=""><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ted</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,151.92,351.93,324.84,8.43;10,92.64,362.85,98.51,8.43">Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics</title>
		<meeting>the Fourth International Conference on Intelligent Text Processing and Computational Linguistics<address><addrLine>Mexico City</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-02">February 2003</date>
			<biblScope unit="page" from="370" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.96,384.33,385.02,8.43;10,92.64,395.13,306.06,8.43" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,152.36,384.33,252.69,8.43">Processing Natural Language without Natural Language Processing</title>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Brill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,92.64,395.13,52.01,8.43">CICLing 2003</title>
		<title level="s" coord="10,151.32,395.13,23.93,8.43">LNCS</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2588</biblScope>
			<biblScope unit="page" from="360" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.96,416.85,408.54,8.43;10,92.64,427.53,425.16,8.43;10,92.64,438.33,205.62,8.43" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,371.12,416.85,134.70,8.43">Data-Intensive Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Susan</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,250.92,427.53,266.88,8.43">Information Technology: The Tenth Text Retrieval Conference, TREC</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="393" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.96,460.05,406.08,8.43;10,92.64,470.73,93.27,8.43;10,180.36,470.73,233.94,8.43" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="10,344.64,460.05,167.40,8.43;10,92.64,470.73,93.27,8.43;10,180.36,470.73,32.61,8.43">The IMS Corpus Workbench: Corpus Query Processor (CQP): User&apos; s Manual</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Christ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">M</forename><surname>Schulze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Koenig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999-03-08">1999. March 8, 1999</date>
		</imprint>
		<respStmt>
			<orgName>University of Stuttgart</orgName>
		</respStmt>
	</monogr>
	<note>CQP V2.2</note>
</biblStruct>

<biblStruct coords="10,105.96,492.45,234.08,8.43" xml:id="b5">
	<monogr>
		<ptr target="http://www.google.com/help/index.html" />
		<title level="m" coord="10,105.96,492.45,76.73,8.43">Google Help Central</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.96,513.93,221.34,8.43;10,92.64,524.73,396.78,8.43;10,92.64,535.65,196.28,8.43" xml:id="b6">
	<monogr>
		<ptr target="http://search.cpan.org/dist/Lingua-PT-Segmentador/" />
		<title level="m" coord="10,109.96,513.93,93.33,8.43;10,218.55,513.93,97.33,8.43">Lingua::PT:: Atomizador</title>
		<imprint/>
	</monogr>
	<note>Lingua::PT::Segmentador</note>
</biblStruct>

<biblStruct coords="10,105.96,557.13,378.68,8.43;10,92.64,568.05,410.60,8.43;10,92.64,578.85,248.94,8.43" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,470.28,557.13,14.36,8.43;10,92.64,568.05,210.28,8.43">The Multiple Language Question Answering Track at CLEF</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Romagnoli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vallin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Peinado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Verdejo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,418.44,568.05,84.80,8.43;10,92.64,578.85,83.19,8.43">Working Notes for the CLEF 2003 Workshop</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<meeting><address><addrLine>Trondheim, Norway</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-08-22">2003. 21-22 August. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.96,600.45,264.92,8.43" xml:id="b8">
	<monogr>
		<ptr target="http://clef-qa.itc.it/2004/guidelines.html" />
		<title level="m" coord="10,105.96,600.45,108.47,8.43">QA@CLEF-2004 Guidelines</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.76,622.05,406.96,8.43;10,92.64,632.85,418.20,8.43;10,92.64,643.65,18.72,8.43" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,231.22,622.05,219.03,8.43">Evaluating CETEMPúblico, a free resource for Portuguese</title>
		<author>
			<persName coords=""><forename type="first">Diana</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paulo</forename><surname>Rocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,470.76,622.05,46.96,8.43;10,92.64,632.85,286.82,8.43">Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 39th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toulouse</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-07-11">9-11 July 2001</date>
			<biblScope unit="page" from="442" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.76,665.25,405.44,8.43;10,92.64,676.05,405.60,8.43;10,92.64,686.85,424.20,8.43;10,92.64,697.65,18.72,8.43" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,294.00,665.25,222.21,8.43;10,92.64,676.05,141.73,8.43">Jspell.pm -um módulo de análise morfológica para uso em Processamento de Linguagem Natural</title>
		<author>
			<persName coords=""><forename type="first">Alberto</forename><surname>Simões</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">&amp;</forename><surname>Manuel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">João</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Almeida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,445.68,676.05,52.56,8.43;10,92.64,686.85,238.19,8.43">Actas do XVII Encontro da Associação Portuguesa de Linguística (APL 2001)</title>
		<editor>
			<persName><forename type="first">Anabela</forename><surname>Gonçalves</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Clara</forename><forename type="middle">Nunes</forename><surname>Correia</surname></persName>
		</editor>
		<meeting><address><addrLine>Lisboa; Lisboa</addrLine></address></meeting>
		<imprint>
			<publisher>APL</publisher>
			<date type="published" when="2001-04-02">2-4 Outubro 2001</date>
			<biblScope unit="page" from="485" to="495" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
