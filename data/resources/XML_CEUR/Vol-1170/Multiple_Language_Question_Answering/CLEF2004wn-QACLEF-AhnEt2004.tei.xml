<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,144.66,148.91,313.67,15.49">Cross-lingual Question Answering with QED</title>
				<funder ref="#_9Nk2CDt">
					<orgName type="full">Scottish Enterprise Edinburgh-Stanford Link</orgName>
				</funder>
				<funder>
					<orgName type="full">Economic and Social Research Council, UK</orgName>
				</funder>
				<funder ref="#_Xcma6pA">
					<orgName type="full">Linguit GmbH</orgName>
				</funder>
				<funder>
					<orgName type="full">School of Informatics, University of Edinburgh</orgName>
				</funder>
				<funder ref="#_5QeWu4h">
					<orgName type="full">German Academic Exchange Service (DAAD)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,161.37,181.39,52.24,10.76"><forename type="first">Kisuh</forename><surname>Ahn</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,222.83,181.39,61.82,10.76"><forename type="first">Beatrix</forename><surname>Alex</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,292.89,181.39,50.23,10.76"><forename type="first">Johan</forename><surname>Bos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,351.50,181.39,84.20,10.76"><forename type="first">Tiphaine</forename><surname>Dalmas</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,192.38,195.34,89.73,10.76"><forename type="first">Jochen</forename><forename type="middle">L</forename><surname>Leidner</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,312.66,195.34,97.96,10.76"><forename type="first">Matthew</forename><forename type="middle">B</forename><surname>Smillie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,144.66,148.91,313.67,15.49">Cross-lingual Question Answering with QED</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6D5B18159574823A15CEA8DC4F7F8F9F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present improvements and modifications of the QED open-domain question answering system developed for TREC-2003 to make it cross-lingual for participation in the Cross-Linguistic Evaluation Forum (CLEF) Question Answering Track 2004 for the source languages French and German and the target language English. We use rule-based question translation extended with surface pattern-oriented pre-and post-processing rules for question reformulation to create and English query from its French or German original. Our system uses deep processing for the question and answers, which requires efficient and radical prior search space pruning. For answering factoid questions, we report an accuracy of 16% (German to English) and 20% (French to English), respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This report describes QED, a question answering (Q&amp;A) system developed at the University of Edinburgh, and its performance at <ref type="bibr" coords="1,182.48,446.99,45.08,8.97">CLEF-2004</ref>. QED [LBD + 04] was originally developed for monolingual (English) Q&amp;A tasks, so we needed to extend it with a machine translation (MT) component in order to be able to participate in the CLEF evaluation exercise. We concentrated on the languages French and German for the cross-language QA task and used the 200 French and German questions from CLEF-2003 [MRV + 03] as developement data. As we aimed at English as target language, we only required an MT component to translate the questions.</p><p>The CLEF evaluation exercise for QA is based on that of TREC <ref type="bibr" coords="1,364.77,518.72,30.59,8.97" target="#b10">[Voo04]</ref>. The task is to give possibly exact answers for factoid and definition questions, and back these up with a document that supports the answer. Questions for which no answer can be found in the document collection have to be answered with the string "NIL". Each answer needs to be associated with a confidence value (a number between 0 and 1), in order to reward systems that are able to evaluate their own performance.</p><p>In the remainder of this paper we describe the general architecture of the cross-lingual QED question answering system as well as its individual components (Section 2). Most of the QED system is similar to that described in [LBD + 04], minus the more elaborate question-typing, the use of Lemur instead of MG for Information Retrieval (IR), and several minor enhancements in the various components. We present our results obtained in the CLEF-2004 evaluation in Section 3, and conclude in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The QED System</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Architecture</head><p>The translation component was added as a front-end to the existing English QED open domain question answering (QA) system. We chose this system architecture in order to exploit the already available end-toend QA system which was developed for TREC-2003 [LBD + 04].</p><p>The questions are translated using our MT module, tokenized, and optionally reformulated. After stemming, POS-tagging and parsing, the question is parsed. A semantic representation is generated from the grammatical relations, which is used to construct a query. The query is posed to the document retrieval module to obtain documents. A passage segmenting and ranking tool is used to prune the search space and find document regions likely to contain answers. Its output is parsed and a semantic representation for answer candidates is created likewise. An answer extraction module attempts to match and score representations of question and answer candidates. Finally, evidence from the Web in the form of cooccurrence counts is used to check answer candidates for validity and the best answer is output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Machine Translation</head><p>Our translation component consists of Babelfish<ref type="foot" coords="2,284.02,241.06,3.69,6.63" target="#foot_0">1</ref> , an online machine translation (MT) engine based Systran. This is a rule-based MT engine, which makes use of both bilingual dictionaries and linguistic rules designed empirically for specific language pairs. Perhaps unsurprisingly, we initially observed several errors specific to language pairs that occurred regularly for various types of questions. Using Babelfish, we translated 200 CLEF-2003 questions DE→EN automatically and let a linguist judge the results for acceptability. Only 29% were found to be acceptable by the human subject. Many of the errors were caused by foreign words and literally translated Named Entities.</p><p>We decided to develop automatic pre-and post-processing rules to improve the quality of the MT output. As the English MT output serves as input into the QA system, our aim was to produce MT output as correct as possible. We therefore invested some time in examining the types of errors that occurred in the Systran output for both language pairs, and devised sets of pre-and post-correction rules.<ref type="foot" coords="2,459.96,360.61,3.69,6.63" target="#foot_1">2</ref> Pre-correction After an extensive analysis of the MT output of the development data, we identified such instances and designed pre-processing rules to reformulate certain questions into simpler constructions. For example, we reformulated French questions starting with " À quel moment" into "Quand" (when) questions.</p><p>Post-correction Similarly, we devised a set of post-processing rules to correct regular errors in the MT output. For example, in the case of French questions that are distinguished by the inversion of subject pronoun and verb such as "Où X travaille-t-il?", the English MT output is "Where X does it work?" instead of "Where does X work?". German questions such as "Wie heißt X?" are literally translated into "How is X called?" rather than "What is X called?". The surface pattern-oriented pre-and postprocessing rules enabled us to correct such errors automatically and thus considerably improve the MT output.</p><p>These pre-and post-processing rules improved the MT component considerably, and although the results were far from perfect, we expected them to be good enough for our purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Document Retrieval, Passage Extraction and Ranking</head><p>We used the Lemur toolkit<ref type="foot" coords="2,195.65,591.12,3.69,6.63" target="#foot_2">3</ref> to realize document retrieval using the Vector-Space Model. The question was analyzed syntactically and semantically and a weighted set of phrases were constructed from the Discourse Representation Structures, which were converted into structured queries for Lemur. The most relevant 300 documents were retrieved for subsequent processing.</p><p>Our passage segmentation and ranking component qtile takes a query and a set of retrieved documents and extracts n-sentence passages (called "tiles"), and assigns a score to them. This is done by sliding an n-sentence window over the document stream at a time in a sentence-wise fashion, retaining all window tiles that contain at least one of the words in the query and also always must contain all upper-case query words. The score is based on heuristics like • number of non-stopword query word tokens (as opposed to types) found in the tile;</p><p>• a comparison of the capitalization of query occurrence and tile occurrence of a term;</p><p>• the occurrence of 2-grams and 3-grams in both question and tile. Each tile's score s is multiplied with a slightly asymmetric triangular window function w to weights sentences in the centre of a window higher than in the periphery and to break ties (W is the number of word tokens):</p><formula xml:id="formula_0" coords="4,235.27,215.43,131.26,29.47">w(s) = 1.1 × s |W | s ≤ |W | 1.0 × -s |W | otherwise</formula><p>The qtile component has linear asymptotic time complexity and requires constant space. For CLEF-2004 we use a window size of 3 sentences and output the top-scoring 100 tiles (duplicates are eliminated) for further processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Question Typing</head><p>We used a taxonomy of eleven basic question types (Figure <ref type="figure" coords="4,337.90,322.84,3.60,8.97" target="#fig_1">2</ref>), based on the strategies used for finding suitable answers within the large variety of question patterns. This division is based on answers in the form of sentences (S), adjectives (ADJ), and noun phrases (NP). Some of the question-types are further divided into subtypes, where C is a concept, R a relation, and U a unit of measurement. The question types are determined after the semantic analysis of the question using a rule-based system. For instance, "How how is the sun?" gets assigned the question type MEASURE:TEMPERATURE, and "Who is Janis Joplin?", the question type DEFINITION:PERSON. The question types are used by the answer selection component to constrain the set of potential answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Linguistic Analysis</head><p>The C&amp;C maximum entropy POS tagger <ref type="bibr" coords="4,254.41,697.82,34.30,8.97" target="#b2">[CC03a]</ref> is used to tag the question words and the text segments returned by the tiler. The C&amp;C NE-tagger <ref type="bibr" coords="4,269.17,709.78,34.87,8.97" target="#b3">[CC03b]</ref> is also applied to the question and text segments, identifying named entities from the standard MUC-7 data set (locations, organisations, persons, dates, times and monetary amounts). The POS tags and NE-tags are used to construct a semantic representation from the output of the parser. We used the RADISP system <ref type="bibr" coords="5,221.92,112.02,29.88,8.97" target="#b0">[BC02]</ref> to parse the question and the text segments returned by the tiler. The RADISP parser returns syntactic dependencies represented by grammatical relations such as ncsubj (non-clausal subject), dobj (direct object), ncmod (non-clausal modifier), and so on. The set of dependencies for a sentence are annotated with POS and NE information and converted into a graph in Prolog format.</p><p>To increase the quality of the parser output, we reformulated imperatives in "list questions" (e.g. Name countries in Europe) into proper question form (What are countries in Europe?). The RADISP parser was much better at returning the correct dependencies for such questions, largely because the RADISP POS tagger typically assigned the incorrect tag to Name in the imperative form. We applied a similar approach to other question types not handled well by the parser.</p><p>The output of the parser, a set of dependency relations (describing a graph) between syntactic categories, is used to build a semantic representation-both for the question under consideration and for the text passages that might contain an answer to the question. Categories contain the following information: the surface word-form, the lemmatized word-form, the word position in the sentence, the sentence position in the text, named-entity information, and a POS tag defining the category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Semantic Interpretation</head><p>Our semantic formalism is based on Discourse Representation Theory <ref type="bibr" coords="5,381.13,326.59,28.22,8.97" target="#b5">[KR93]</ref>, but we use an enriched form of Discourse Representation Structure (DRS), combining semantic information with syntactic and sortal information. DRSs are constructed from the dependency relations in a recursive way, starting with an empty DRS at the top node of the dependency graph, and adding semantic information to the DRS as we follow the dependency relations in the graph, using the POS information to decide on the nature of the semantic contribution of a category.</p><p>Following DRT, DRSs are defined as ordered pairs of a set of discourse referents and a set of DRSconditions. The following types of basic DRS-conditions are considered: pred(x,S), named(x,S), card(x,S), event(e,S), and argN(e,x), rel(x,y,S), mod(x,S), where e, x, y are discourse referents, S a constant, and N a number between 1 and 3. Questions introduce a special DRS-condition of the form answer(x,T) for a question type T. We call this the answer literal; answer literals play an important role in answer selection.</p><p>Implemented in Prolog, we reached a recall of around 80%. (By recall we mean the percentage of categories that contributed to semantic information in the DRS). Note that each passage or question is translated into one single DRS; hence DRSs can span several sentences. Some basic techniques for pronoun resolution are implemented as well. However, to avoid complicating the answer extraction task too much, we only considered non-recursive DRSs in our TREC-2003 implementation, i.e. DRSs without complex conditions introducing nested DRSs for dealing with negation, disjunction, or universal quantification.</p><p>Finally, a set of DRS normalisation rules are applied in a post-processing step, thereby dealing with active-passive alternations, question typing, inferred semantic information, and the disambiguating of noun-noun compounds. The resulting DRS is enriched with information about the original surface wordforms and POS tags, by co-indexing the words, POS tags, the discourse referents, and DRS-conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Answer Selection</head><p>The answer extraction component takes as input a DRS for the question, and a set of DRSs for selected passages. The task of this component is to extract answer candidates from the passages. This is realised by performing a match between the question-DRS and a passage-DRS, by using a relaxed unification method and a scoring mechanism indicating how well the DRSs match each other.</p><p>Taking advantage of Prolog unification, we use Prolog variables for all discourse referents in the question-DRSs, and Prolog atoms in passage-DRSs. We then attempt to unify all terms of the question DRSs with terms in a passage-DRS, using an A * search algorithm. Each potential answer is associated with a score, which we call the DRS-score. High scores are obtained for perfect matches (i.e., standard unification) between terms of the question and passage, low scores for less perfect matches (i.e., obtained by "relaxed" unification). Less perfect matches are granted for different semantic types, predicates with different argument order, or terms with symbols that are semantically familiar according to WordNet <ref type="bibr" coords="5,481.18,744.40,27.84,8.97" target="#b4">[Fel98]</ref>.</p><p>After a successful match the answer literal is identified with a particular discourse referent in the passage-DRS. Recall that the DRS-conditions and discourse referents are co-indexed with the surface word-forms of the source passage text. This information is used to generate an answer string, simply by collecting the words that belong to DRS-conditions with discourse referents denoting the answer. Finally, all answer candidates are output in an ordered list. Duplicate answers are eliminated, but answer frequency information is added to each answer in this final list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation and Results</head><p>We submitted two runs for each language pair (edin041deen, edin042deen, edin041fren, edin042fren), differing in the way reranking of answers was executed. The answers of the first runs for each language pair were ranked using the formula Rank = 0.2 * S + 0.8 * F, the answers of the second runs were ranked using the formula Rank = 0.8 * S + 0.2 * F for location and measure question types, and on Rank = 1.0 * S for all other question types. (Here S is the normalised DRS-score and F the normalised frequency.) The weights were estimated on the basis of running QED on TREC-2003 data. The second runs were expected to perform better.</p><p>For both languages, the second runs preformed the best, with an overal accuracy of 17.00% for German and 20.00% for French. The better scores for French are due to the differences in accuracy of the machine translation components (more time was invested in the French to English MT). Separate results for the factoid and definition questions are listed in Table <ref type="table" coords="6,291.10,347.24,4.98,8.97" target="#tab_0">1</ref> and<ref type="table" coords="6,315.45,347.24,28.12,8.97">Table 2</ref> For the German edin041deen and edin042deen runs, the answer-string "NIL" was returned 47 times, and correctly returned 7 times (14.89%). For the French edin041fren and edin042fren, the answer-string "NIL" was returned 70 times, and correctly returned 11 times (15.71%). The confidence-weighted score for the four runs varied between 0.04922 and 0.05889, which is probably low compared to other systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Future Work</head><p>We have presented our extensions to QED to enable it for cross-lingual Q&amp;A. Our approach consisted of composing existing software (with minor enhancements) for machine translation and question answering in a sequential pipeline. The translation was enhanced using pattern replacements to correct systematic mistakes. We obtained an accuracy of 16% (German to English) and 20% (French to English), respectively, for answering factoid questions. For definition questions, obtained an accuracy of 25% (German to English) and 15% (French to English), respectively. Definition questions constituted a minor portion of the test set.</p><p>For future work, we consider using several competing MT systems in a parallel architecture. Automatic MT evaluation scores like Bleu <ref type="bibr" coords="7,219.98,147.89,43.73,8.97" target="#b8">[PRWZ01]</ref> could also be considered to select the best translation from a set of candidate machine translation if multiple engines are available. Questions translated by multiple MT systems could also be used together as query expansions. Another proposed extension is recognition (and alignment) of Named Entities in source and target questions to avoid literal translations of proper nouns (for instance, Spielberg→play mountain and Neufeld→new field).</p><p>With regards to the IR component of QED, answer recall after information retrieval and tiling was found low (about 30% of correct answers were not contained after these phases). This is most likely due to impedance mismatch between retrieval and tiling components and the current lack of question-type specific query expansion, and the absense of query relaxation in case no appropriate answers can be found.</p><p>The ability to process a large number of highly ranked passages is bound by the time taken by the parser. We are planning to accelerate parsing using a supertagging-based statistical parser [BCS + 04] in the next version of QED. This parser, based on CCG, will not only give us a gain in speed, but is also expected to increase the coverage and accuracy of the parser.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,90.00,672.24,423.00,8.97;3,90.00,684.20,278.92,8.97"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The QED system architecture for CLEF-2004 (dataflow graph). Normal arrows represent processing of the question, bold arrows represents processing of answers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,386.44,459.76,32.32,4.45;4,387.96,444.60,25.71,4.45;4,392.51,429.43,16.04,4.45;4,387.96,414.26,26.66,4.45;4,390.99,399.09,16.98,4.45;4,387.96,383.93,25.24,4.45"><head>Figure 2 :</head><label>2</label><figDesc>ABBREVIATION MEASURE:U DATE:C LOCATION:C NAME:C GENERAL:C</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,167.67,347.24,267.66,233.88"><head>Table 1 :</head><label>1</label><figDesc>. CLEF-2003 Performance of QED on Factoid Questions</figDesc><table coords="6,167.67,399.60,267.66,181.51"><row><cell>Run</cell><cell cols="4">Right Inexact Unsupported Accuracy</cell></row><row><cell>edin041deen</cell><cell>24</cell><cell>4</cell><cell>1</cell><cell>13.33%</cell></row><row><cell>edin042deen</cell><cell>29</cell><cell>5</cell><cell>0</cell><cell>16.11%</cell></row><row><cell>edin041fren</cell><cell>32</cell><cell>4</cell><cell>0</cell><cell>17.78%</cell></row><row><cell>edin042fren</cell><cell>37</cell><cell>6</cell><cell>0</cell><cell>20.56%</cell></row><row><cell cols="5">Table 2: CLEF-2003 Performance of QED on Definition Questions</cell></row><row><cell>Run</cell><cell cols="4">Right Inexact Unsupported Accuracy</cell></row><row><cell>edin041deen</cell><cell>4</cell><cell>1</cell><cell>0</cell><cell>20.00%</cell></row><row><cell>edin042deen</cell><cell>5</cell><cell>2</cell><cell>0</cell><cell>25.00%</cell></row><row><cell>edin041fren</cell><cell>1</cell><cell>2</cell><cell>0</cell><cell>5.78%</cell></row><row><cell>edin042fren</cell><cell>3</cell><cell>1</cell><cell>0</cell><cell>15.00%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,104.35,706.62,131.26,6.64"><p>http://babelfish.altavista.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,104.35,715.85,408.65,7.17;2,90.00,725.32,101.82,7.17"><p>We used the GNU recode utility to convert the CLEF test questions from UTF-8 character encoding into ISO 8859-1 (Latin-1) encoding required by Babelfish.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,104.35,735.70,131.75,6.64"><p>http://www-2.cs.cmu.edu/~lemur/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We are grateful to <rs type="person">Steve Clark</rs>, <rs type="person">James Curran</rs>, <rs type="person">Malvina Nissim</rs>, and <rs type="person">Bonnie Webber</rs> for assistance and helpful discussions, and would like to thank the system administrators <rs type="person">Bill Hewitt</rs> and <rs type="person">Andrew Woods</rs> for their computing support. Special thanks to <rs type="person">John Carroll</rs> for his help with the RADISP parser, and in general to all authors of all external progams we utilized for making them available.</p><p>Alex is supported by <rs type="funder">Scottish Enterprise Edinburgh-Stanford Link</rs> (<rs type="grantNumber">R36759</rs>), the <rs type="funder">Economic and Social Research Council, UK</rs> and the <rs type="funder">School of Informatics, University of Edinburgh</rs>. <rs type="person">Dalmas</rs> is supported by the <rs type="funder">School of Informatics, University of Edinburgh</rs>. Leidner is supported by the <rs type="funder">German Academic Exchange Service (DAAD)</rs> under scholarship <rs type="grantNumber">D/02/01831</rs> and by <rs type="funder">Linguit GmbH</rs> (research contract <rs type="grantNumber">UK-2002/2</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_9Nk2CDt">
					<idno type="grant-number">R36759</idno>
				</org>
				<org type="funding" xml:id="_5QeWu4h">
					<idno type="grant-number">D/02/01831</idno>
				</org>
				<org type="funding" xml:id="_Xcma6pA">
					<idno type="grant-number">UK-2002/2</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,139.71,486.27,373.30,8.97;7,139.70,498.22,373.30,8.97;7,139.70,510.18,181.09,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,265.57,486.27,209.94,8.97">Robust accurate statistical annotation of general text</title>
		<author>
			<persName coords=""><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,495.19,486.27,17.81,8.97;7,139.70,498.22,343.31,8.97">Proceedings of the 3rd International Conference on Language Resources and Evaluation</title>
		<meeting>the 3rd International Conference on Language Resources and Evaluation<address><addrLine>Las Palmas, Gran Canaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="1499" to="1504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,139.70,529.50,373.29,8.97;7,139.70,541.45,373.30,8.97;7,139.70,553.41,342.83,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,488.50,529.50,24.50,8.97;7,139.70,541.45,209.71,8.97">Widecoverage semantic representations from a CCG parser</title>
		<author>
			<persName coords=""><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,365.74,541.45,147.26,8.97;7,139.70,553.41,226.62,8.97">Proceedings of the 20th International Conference on Computational Linguistics (COLING &apos;04)</title>
		<meeting>the 20th International Conference on Computational Linguistics (COLING &apos;04)<address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,139.71,572.73,373.29,8.97;7,139.70,584.68,373.30,8.97;7,139.70,596.64,329.89,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,290.18,572.73,222.82,8.97;7,139.70,584.68,27.36,8.97">Investigating GIS and smoothing for maximum entropy taggers</title>
		<author>
			<persName coords=""><forename type="first">James</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,183.57,584.68,329.44,8.97;7,139.70,596.64,164.10,8.97">Proceedings of the 11th Annual Meeting of the European Chapter of the Association for Computational Linguistics (EACL&apos;03)</title>
		<meeting>the 11th Annual Meeting of the European Chapter of the Association for Computational Linguistics (EACL&apos;03)<address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,139.71,615.96,373.29,8.97;7,139.70,627.91,373.30,8.97;7,139.70,639.87,168.79,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,291.12,615.96,221.88,8.97;7,139.70,627.91,23.01,8.97">Language independent NER using a maximum entropy tagger</title>
		<author>
			<persName coords=""><forename type="first">James</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,179.68,627.91,328.62,8.97">Proceedings of the Seventh Conference on Natural Language Learning (CoNLL-03)</title>
		<meeting>the Seventh Conference on Natural Language Learning (CoNLL-03)<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="164" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,139.71,659.19,373.29,8.97" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="7,254.12,659.19,164.13,8.97">WordNet. An Electronic Lexical Database</title>
		<editor>Christiane Fellbaum</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,139.71,678.50,373.29,8.97;7,139.70,690.46,332.01,8.97" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="7,264.16,678.50,248.84,8.97;7,139.70,690.46,220.84,8.97">From Discourse to Logic; An Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and DRT</title>
		<author>
			<persName coords=""><forename type="first">Hans</forename><surname>Kamp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Uwe</forename><surname>Reyle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Kluwer</publisher>
			<pubPlace>Dordrecht</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,139.70,709.78,373.29,8.97;7,139.70,721.73,373.29,8.97;7,139.70,733.69,373.30,8.97;7,139.70,745.64,307.61,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,333.64,721.73,179.35,8.97;7,139.70,733.69,57.96,8.97">The QED open-domain answer retrieval system for TREC</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jochen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johan</forename><surname>Leidner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tiphaine</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><forename type="middle">R</forename><surname>Dalmas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Colin</forename><forename type="middle">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Bannard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bonnie</forename><surname>Steedman</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Webber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,239.43,733.69,269.28,8.97">Proceedings of the Twelfth Text Retrieval Conference (TREC 2003)</title>
		<title level="s" coord="7,139.70,745.64,101.84,8.97">NIST Special Publication</title>
		<meeting>the Twelfth Text Retrieval Conference (TREC 2003)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003. 2004</date>
			<biblScope unit="page" from="595" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,139.70,112.02,373.30,8.97;8,139.70,123.98,373.29,8.97;8,139.70,135.93,373.30,8.97;8,139.70,147.89,174.53,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,362.45,123.98,150.55,8.97;8,139.70,135.93,155.33,8.97">Creating the DISEQuA corpus: a test set for multilingual question answering</title>
		<author>
			<persName coords=""><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Simone</forename><surname>Romagnoli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alessandro</forename><surname>Vallin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jesús</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Víctor</forename><surname>Peinado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Felisa</forename><surname>Verdejo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,397.68,135.93,115.32,8.97;8,139.70,147.89,59.72,8.97">Working Notes for the CLEF 2003 Workshop</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<meeting><address><addrLine>Trondheim, Norway</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,139.71,167.81,373.29,8.97;8,139.70,179.77,329.57,8.97" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="8,397.01,167.81,115.99,8.97;8,139.70,179.77,131.80,8.97">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno>W0109-022</idno>
		<imprint/>
		<respStmt>
			<orgName>IBM</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="8,471.90,179.77,41.10,8.97;8,139.70,191.72,124.69,8.97" xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Watson Research</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Center</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,139.71,211.65,373.29,8.97;8,139.70,223.60,373.29,8.97;8,139.70,235.56,22.42,8.97" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,223.84,211.65,79.10,8.97">Overview of TREC</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,346.03,211.65,166.97,8.97;8,139.70,223.60,99.47,8.97">Proceedings of the Twelfth Text Retrieval Conference (TREC 2003)</title>
		<title level="s" coord="8,245.87,223.60,101.64,8.97">NIST Special Publication</title>
		<meeting>the Twelfth Text Retrieval Conference (TREC 2003)<address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003. 2004</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
