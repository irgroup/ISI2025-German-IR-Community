<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,110.58,74.83,374.11,12.19">The Use of Lexical Context in Question Answering for Spanish</title>
				<funder>
					<orgName type="full">Human Language Technologies Laboratory of INAOE</orgName>
				</funder>
				<funder ref="#_yMapEkF">
					<orgName type="full">SNI-Mexico</orgName>
				</funder>
				<funder ref="#_EXtxyQv">
					<orgName type="full">CONACYT</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,186.48,103.71,74.24,8.74"><forename type="first">M</forename><surname>Pérez-Coutiño</surname></persName>
						</author>
						<author>
							<persName coords="1,267.85,103.71,40.77,8.74"><forename type="first">T</forename><surname>Solorio</surname></persName>
						</author>
						<author>
							<persName coords="1,315.30,103.34,89.52,9.10"><forename type="first">M</forename><surname>Montes-Y-Gómez</surname></persName>
						</author>
						<author>
							<persName coords="1,208.86,115.23,67.91,8.74"><forename type="first">A</forename><surname>López-López</surname></persName>
						</author>
						<author>
							<persName coords="1,297.97,115.23,88.48,8.74"><forename type="first">L</forename><surname>Villaseñor-Pineda</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Óptica y Electrónica (INAOE)</orgName>
								<orgName type="institution">Instituto Nacional de Astrofísica</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Luis Enrique Erro No</orgName>
								<address>
									<addrLine>1, Sta Ma Tonantzintla</addrLine>
									<postCode>72840</postCode>
									<settlement>Puebla</settlement>
									<region>Pue, México</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,110.58,74.83,374.11,12.19">The Use of Lexical Context in Question Answering for Spanish</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">44364137818248FF32AF4559D51FF25F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Question Answering for Spanish</term>
					<term>Lexical Context</term>
					<term>Natural Language Processing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the prototype developed by the Language Technologies Laboratory at INAOE for Spanish monolingual QA evaluation task at CLEF 2004. Our approach is centered in the use of context at a lexical level in order to identify possible answers to factoid questions. Such method is supported by an alternative one based on pattern recognition in order to identify candidate answers to definition questions. The methods applied at different stages of the system and prototype architecture for question answering are described. The paper shows and discusses the results achieved with this approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Question Answering (QA) systems has become an alternative to traditional information retrieval systems because of its capability to provide concise answers to questions stated by the user in natural language. This fact, along with the inclusion of QA evaluation as part of the Text Retrieval Conference (TREC) 1 in 1999, and recently <ref type="bibr" coords="1,105.76,329.85,11.68,8.74" target="#b5">[6]</ref> in Multilingual Question Answering as part of the Cross Language Evaluation Forum (CLEF) 2 , have arisen a promising and increasing research field.</p><p>The Multilingual Question Answering evaluation track at CLEF 2004 is similar to last year edition. For each subtask, participants are provided with 200 questions requiring short answers. Some questions may not have known answer, and systems should be able to recognize them. However there are some important differences, this year answers included fact based instances or definitions, and systems must return exactly one response per question, and up to two runs.</p><p>Our laboratory has developed a prototype system for Spanish monolingual QA task. Two important things to consider about it are, a) this is our first QA prototype and has been developed from scratch, and b) this the first time that our laboratory participates in an evaluation forum.</p><p>The prototype described in this document relies in the fact that several approaches of QA systems like <ref type="bibr" coords="1,498.26,444.81,10.87,8.74" target="#b7">[8,</ref><ref type="bibr" coords="1,511.90,444.81,12.55,8.74" target="#b12">13,</ref><ref type="bibr" coords="1,70.92,456.33,7.55,8.74" target="#b3">4,</ref><ref type="bibr" coords="1,83.10,456.33,13.42,8.74" target="#b9">10]</ref> use named entities at different stages of the system in order to find a candidate answer. Generally speaking, the use of named entities is performed at the final stages of the system, i.e., either in the passage selection or as a discriminator in order to select a candidate answer at the final stage. Another interesting approach is the use of Predictive Annotation which was first presented at TREC-8 by Prager et al. <ref type="bibr" coords="1,489.64,490.83,10.66,8.74" target="#b7">[8]</ref>. One meaningful characteristic of this approach is the indexing of anticipated semantic types, identifying the semantic type of the answer sought by the question, and extracting the best matching entity in candidate answer passages. In their approach, the authors used no more than simple pattern matching to get the entities. Our prototype was developed to process both, questions and source documents in Spanish. Our system is based on approach just described but differs in the following: i) Semantic classes' identification relies in the preprocessing of the whole document collection by a POS tagger that simultaneously works as named entity recognizer and classifier. ii) The indexing stage takes as item the lexical context associated to each single named entity contained in every document of the collection. iii) The searching stage selects as candidate answers those named entities whose lexical contexts match better the context of the question. iv) At the final stage, candidate answers are compared against a second set of candidates gathered from the Internet. v) Final answers are selected based on a set of relevance measures which encompass all the information collected in the searching process.</p><p>The rest of this paper is organized as follows; section two describes the architecture and functionality of the system; section three details the process of question processing; section four details the process of indexing; section five shows the process of searching; section six describe the process of answer selection; section seven discusses the results achieved by the system; and finally section eight exposes our conclusions and discusses further work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Overview</head><p>The system adjusts to a typical QA system architecture <ref type="bibr" coords="2,296.40,95.31,15.35,8.74" target="#b13">[14]</ref>. Figure <ref type="figure" coords="2,347.37,95.31,5.01,8.74" target="#fig_0">1</ref> shows the main blocks of the system. The system could be divided into the following stages: question processing, which involves the extraction of named entities and lexical context in the question, as well as question classification to define the semantic class of the answer expected to respond to the question; indexing, where a preprocessing of the supporting document collection is done, building the representation of each document that become the searching space to find candidate answers to the question; searching, where a set of candidate answers is obtained from the index and the Internet, (here candidate answers are classified by a machine learning algorithm, and provides information to perform different weighting schemes); and finally answer selection where candidate answers are ranked and the final answer recommendation of the system is produced. Next sections describe each of these stages. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Question Processing</head><p>MACO <ref type="bibr" coords="2,104.53,589.65,11.69,8.74" target="#b2">[3]</ref> is a POS tagger and lemmatizer capable of recognizing and classifying named entities (NEs). The possible categories for NEs are the following: person, organization, geographic place, date, quantity and miscellaneous. In order to reduce the possible candidate answers provided by our system we perform a question classification process. The purpose of this classification is to match each question with one of the six named entities provided by MACO.</p><p>We use a straightforward approach, where the attributes for the learning task are the prefixes of the words in the question and additional information acquired by an Internet search engine.</p><p>The procedure for gathering this information from Internet is first we use a set of heuristics in order to extract from the question the first noun word or words w. We then employ a search engine, in this case Google, submitting queries using the word w in combination with the five possible semantic classes. For instance, for the question Who is the President of the French Republic? President is extracted as the noun in the question using our heuristics, and run 5 queries in the search engine, one for each possible class. The queries take the following forms: </p><formula xml:id="formula_0" coords="3,100.26,70.75,115.47,48.92">• "President is a person" • "President is a place" • "President is a date" • "President is a measure"</formula><p>• "President is an organization" For each query (q i ) the heuristic takes the number of results (Cr i ) returned by Google and normalizes them according to equation 1. This means that for each question, the summatory of their five performed queries is 1. Normalized values (Iw(q i )) are taken as attributes values for the learning algorithm. As it can be seen is a very direct approach, but experimental evaluations showed that this information gathered from Internet is quite useful <ref type="bibr" coords="3,70.92,180.57,15.39,8.74" target="#b10">[11]</ref>.</p><formula xml:id="formula_1" coords="3,225.84,192.69,145.76,29.83">( ) ∑ = = n i i i i Cr Cr q Iw 0 Equation 1.</formula><p>The machine learning technique used was Support Vector Machines <ref type="bibr" coords="3,379.35,237.57,16.74,8.74" target="#b11">[12]</ref> implemented in WEKA <ref type="bibr" coords="3,505.31,237.57,15.37,8.74" target="#b14">[15]</ref>. Section 7 discusses the performance of question classification process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Indexing</head><p>Each document in the collection is modeled by the system as a factual text object whose content refers to several named entities even when it is focused on a central topic. As mentioned, named entities could be one of these objects: persons, organizations, locations, dates, quantities and miscellaneous. The model assumes that the named entities are strongly related to their lexical context, especially to nouns (subjects) and verbs (actions). Thus, a document can be seen as a set of entities and their contexts. For details about the document model we refer the reader to <ref type="bibr" coords="3,148.16,347.91,10.63,8.74" target="#b6">[7]</ref>. In order to obtain the representation of the documents, the system begins preprocessing each document with MACO, where this process is performed off-line. Once the document collection has been tagged, the system extracts the lexical contexts associated to named entities. The context considered for this experiment consists of the four verbs or nouns, both at the left and right of its corresponding NE (table <ref type="table" coords="3,484.91,382.35,5.01,8.74" target="#tab_1">1</ref> shows a sample). The final step in the indexing stage is the storage of the extracted contexts, populating a relational database <ref type="foot" coords="3,105.42,405.02,3.00,5.23" target="#foot_1">3</ref> which preserves several relations between each named entity, its semantic class, associated contexts, and the documents where they appeared. In other words, the index is an adaptation of the well knows inverted file structure used in several information retrieval systems. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Searching</head><p>The search engine developed for the system and the searching process differ in several aspects from traditional search engines. This process relies on two information sources: first the information gathered from question processing, i.e., the expected semantic class of the answer to the question, and the named entities and lexical context of the question; and second, the index of named entities, contexts and documents created during indexing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Searching Algorithm</head><p>With the document representation, all the name entities mentioned in a given document can be known beforehand. Thus, the name entities from the question become key elements in order to define the document set more likely to provide the answer. For instance, in the question "¿Dónde se entregan los Oscar?", the named entity "Oscar" narrows the set of documents to only those containing such name entity. At the same time, another assumption is that the context in the neighborhood of the answer has to be similar to the lexical context of the question. Once more, from the question of the example, the fragment "…reciben esta noche, en la sexagésimasexta edición de los Oscar, el homenaje de Hollywood…" contains a lexical context next to the answer which is similar to that of the question.</p><p>Following is the algorithm in detail: 1. Identify the set of relevant documents according to the named entities in the question. 2. Retrieve all contexts in each relevant document. 3. Compute the similarity between question context and those obtained in step 2.</p><p>3.1. Preserve only those contexts whose associated named entity corresponds to the semantic class of the question. 3.2. Compute a similarity function based on frequencies to perform further ranking and answer selection. This function is based on the number of question's named entities found in each pair (NE,Context) retrieved and the number of similar terms in both contexts. 4. Rank the candidate named entities in decreasing order of similarity. 5. Store similarity and named entity classification information (step 3.2) for next stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Answer Selection</head><p>Analyzing the output from the local index we find out that we had a lot of possible answers with the same values for similarity and named entity classification information. Thus, we develop a method for selecting the final possible answer based on answers retrieved from Internet and automated classification of answers using a bagged ensemble of J48 <ref type="bibr" coords="4,170.22,266.31,15.37,8.74" target="#b14">[15]</ref>.</p><p>The final answer presented by our system was selected by calculating the intersection among words between the local index candidate answers and the answers provided by the Internet search. We consider the candidate answer with highest intersection value to be more likely to be the correct answer. However, in some cases all the candidate answers have the same intersection values. In this case we selected from the candidates the first one classified by the learning algorithm as belonging to the positive class. When no positive answer was found among the candidates for a question, then we selected the first candidate answer with highest value from the local index.</p><p>The following sections briefly describe the Internet search and the answer classification processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Internet Searching</head><p>As mention earlier, at the final stage, the system uses information from the Internet in order to get more evidence of the possible accuracy of each candidate answer. From the perspective of the overall system, Internet searching occurs simultaneously to the local search. This subsection reviews the process involved in such task.</p><p>The module used at this step was originally developed at our laboratory to research the effectiveness of a statistical approach to web question answering in Spanish. Such approach lies in the concept of redundancy in the web, i.e, the module applies a several transformations in order to convert the question into a typical query and then this query along to some query reformulations are sent to a search engine with the hypothesis that the answer would be contained -several times-in the snippets retrieved by the search engine <ref type="foot" coords="4,447.66,481.40,3.00,5.23" target="#foot_2">4</ref> . The selection of candidate answers from Internet is based on computing all the n-grams, from unigrams to pentagrams, as possible answers to the given question. Then, using some statistical criteria the n-grams are ranked by decreasing likelihood of being the correct answer. The top ten are used to validate the candidates gathered from the local searching process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Answer Classification</head><p>Discriminating among possible answers was posed as a learning problem. Our goal was to train a learning algorithm capable of selecting from a set of possible candidates the answer that most likely satisfies the question. We selected as features the values computed by the local indexing. We use five attributes: 1) the number of times the possible answer was labeled as the entity class of the question; 2) the number of times the possible entity appeared labeled as a different entity class; 3) number of words in common in the context of the possible answer and the context of the question, excluding named entities; 4) the number of entities that matched the entities in the question, and 5) the frequency of the possible answer along the whole collection of documents. With these attributes, we then trained a bagged ensemble of classifiers using as base learning algorithm the rule induction algorithm J48 <ref type="bibr" coords="4,169.27,662.79,10.64,8.74" target="#b8">[9]</ref>.</p><p>In this work we build the ensemble using the bagging technique which consists of manipulating the training set <ref type="bibr" coords="4,84.56,685.77,10.63,8.74" target="#b0">[1]</ref>.</p><p>Given that we had available only one small set of questions, we evaluate the classification process in two parts. We divided the set of questions into two subgroups of the same size and performed two runs. In each run, we trained on one half and tested on the other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Answering Definitions</head><p>Due to the length and elements in a definition answer, our approach considers the treatment of these questions as a special case. In order to reach accurate definition answers, we have implemented a set of heuristics able to find patterns like those described in <ref type="bibr" coords="5,197.74,114.03,15.36,8.74" target="#b9">[10]</ref>. Table <ref type="table" coords="5,244.83,114.03,5.01,8.74" target="#tab_2">2</ref> shows some samples of applying such heuristics.</p><p>The heuristics are based in punctuation and some stopwords (articles, pronouns and prepositions) which provide evidence for identification of pairs &lt;Answer&gt;&lt;Name&gt;. Thus could be easily gathered by regular expressions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Evaluation</head><p>We participate in the evaluation with two runs. The first one inao041eses was gathered applying all components of the system, while our second run inao042eses didn't make use of heuristics for definition answers. Table <ref type="table" coords="5,519.48,371.85,5.01,8.74" target="#tab_3">3</ref> shows our results. Results show that overall performance of the system with evaluation questions was over 60% lower than training results. A preliminary analysis of the errors has let us note some obvious mistakes like the length and elements considered as part of the context (which couldn't be fixed before questions' release due to time constraints). However there are some other issues to take into account besides those errors, for instance, question classification. Figure <ref type="figure" coords="5,158.07,589.17,5.01,8.74" target="#fig_1">2</ref> shows the accuracy of the classifier, from a total of 200 questions, the classifier only can assign an accurate semantic class to 157 questions, which represents a precision of 78.5%. Besides classifier performance, searching and candidate answers selection were also very low, only 29.41% of questions right classified as person were answered, 63.63 % of organizations, 39.10% of localities, 37.50% of dates, 28.57% of quantities and 18.18% of miscellaneous were answered.</p><p>We have begun a detailed analysis looking for inconsistencies in the overall approach, as well as programming bugs. The initial step is to get an improved configuration of the POS tagger and NE classifier (MACO) in order to label the corpus and rebuild our indexes (databases) with a non restricted version of document model, i.e. without pre-established elements and length in the context. Thus repeat some experiments with a refinement method for candidates and answer selection. This work has presented a lexical-context approach for QA in Spanish. The strength of this work lies in the model used for the source documents. The identification and annotation in advance of named entities and their associated contexts serves as key information in order to select possible answers to a given factoid question. On the other hand, the discrimination of candidate answers is a complex task that requires more research and experimentation of different methods. In this work we have experimented with the merging of evidence coming from three main sources: a ranked list of candidate answers gathered by a similarity measure, answer classification by a bagged ensemble of classifiers, and a set of candidate answers gathered from the Internet. Further work includes exploring the inclusion of more information as part of the context, the refinement of the semantic classes for questions and named entities, and the improvement of answer selection methodology.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,232.86,526.84,140.98,7.85;2,144.12,537.16,307.07,7.85"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Block diagram of the system. There are four stages: question processing, indexing, searching and answer selection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,85.74,421.54,435.32,7.85"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Question classifier accuracy. Numbers in data labels refers to total number of questions classified or answered.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,117.00,449.56,356.88,67.07"><head>Table 1</head><label>1</label><figDesc>Context associated to named entity "CFC". Verbs and common nouns in cursive are gathered from a preprocessing with a POS tagger. &lt;DOCNO&gt;EFE19941219-11009&lt;/DOCNO&gt; … Los CFC son usados en los productos anticongelantes, de insuflación y como refrigerantes, que tienen al cloro como un ingrediente común. "Los CFC son los responsables del agujero de la capa de ozono",…</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,153.90,169.66,289.74,140.99"><head>Table 2</head><label>2</label><figDesc>Examples of definition questions and their answers.</figDesc><table coords="5,153.90,187.66,289.74,122.99"><row><cell>Question</cell><cell>Text fragment containing the answer</cell></row><row><cell cols="2">¿Quién es Arabella Kiesbauer? …otra carta-bomba dirigida, al parecer, a una</cell></row><row><cell></cell><cell>conocida periodista austriaca de raza negra,</cell></row><row><cell></cell><cell>Arabella Kiesbauer, y que fue enviada desde</cell></row><row><cell></cell><cell>Austria...</cell></row><row><cell>¿Qué es UNICEF?</cell><cell>Naciones Unidas, 3 ene (EFE).-El Fondo de</cell></row><row><cell></cell><cell>las Naciones Unidas para la Infancia</cell></row><row><cell></cell><cell>(UNICEF), formuló hoy, lunes, una</cell></row><row><cell></cell><cell>petición…</cell></row><row><cell>¿Quién es Andrew Lack?</cell><cell>…Tanto es así, que el presidente del</cell></row><row><cell></cell><cell>departamento de noticias de la cadena NBC,</cell></row><row><cell></cell><cell>Andrew Lack, confesó en una entrevista…</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,192.60,404.50,210.24,122.91"><head>Table 3</head><label>3</label><figDesc>Results of submitted runs.</figDesc><table coords="5,192.60,420.43,210.24,106.97"><row><cell>Run</cell><cell cols="2">inao041eses inao042eses</cell></row><row><cell>Right</cell><cell>45</cell><cell>37</cell></row><row><cell>Wrong</cell><cell>145</cell><cell>152</cell></row><row><cell>ineXact</cell><cell>5</cell><cell>6</cell></row><row><cell>Unsupported</cell><cell>5</cell><cell>5</cell></row><row><cell>Overall Accuracy</cell><cell>22.50%</cell><cell>18.50%</cell></row><row><cell>Factoid Questions</cell><cell>19.44%</cell><cell>17.78%</cell></row><row><cell>Definition Questions</cell><cell>50%</cell><cell>25%</cell></row><row><cell>"NIL" Accuracy</cell><cell>19.61%</cell><cell>21.74%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0" coords="1,72.42,710.90,447.97,8.88;1,70.92,722.32,22.78,7.85;1,70.92,732.26,74.06,8.88;1,70.92,743.30,73.16,8.88"><p>† This work was done while visiting the Dept. of Information Systems and Computation Polytechnic University of Valencia, Spain. 1 http://trec.nist.gov/ 2 http://clef-qa.itc.it/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="3,76.20,744.34,318.90,7.85"><p>Due to performance constraints, the index has been distributed over a cluster of 5 CPUs.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="4,76.20,744.34,271.52,7.85"><p>The search engine used by this module is Google (http://www.google.com)</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was done under partial support of <rs type="funder">CONACYT</rs> (Project Grants <rs type="grantNumber">U39957-Y</rs> and <rs type="grantNumber">43990</rs>), <rs type="funder">SNI-Mexico</rs>, and the <rs type="funder">Human Language Technologies Laboratory of INAOE</rs>. We also like to thanks to the CLEF as well as <rs type="institution">EFE agency</rs> for the resources provided.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_EXtxyQv">
					<idno type="grant-number">U39957-Y</idno>
				</org>
				<org type="funding" xml:id="_yMapEkF">
					<idno type="grant-number">43990</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,74.69,516.57,306.74,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,133.20,516.57,76.32,8.74">Bagging predictors</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,216.03,516.57,72.62,8.74">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.68,529.05,449.80,8.74;6,85.14,540.57,80.07,8.74" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="6,156.18,529.05,368.30,8.74;6,85.14,540.57,25.04,8.74">Issues, Tasks and Program Structures to Roadmap Research in Question &amp; Answering (Q&amp;A)</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Burger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note type="report_type">NIST</note>
</biblStruct>

<biblStruct coords="6,74.69,553.05,449.75,8.74;6,85.14,564.57,291.02,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,209.70,553.05,296.32,8.74">A Flexible Distributed Architecture for Natural Language Analyzers</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Padró</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,85.14,564.57,113.53,8.74">Proceedings of the LREC&apos;02</title>
		<meeting>the LREC&apos;02<address><addrLine>Las Palmas de Gran Canaria, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.68,577.05,449.78,8.74;6,85.14,588.57,173.74,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,151.44,577.05,124.21,8.74">Automatic Question Answering</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cowie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,283.13,577.05,241.33,8.74;6,85.14,588.57,140.41,8.74">Proceedings of the International Conference on Multimedia Information Retrieval (RIAO 2000)</title>
		<meeting>the International Conference on Multimedia Information Retrieval (RIAO 2000)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.68,601.05,449.77,8.74;6,85.14,612.57,126.50,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,226.32,601.05,258.86,8.74">Natural Language Question Answering: The View from Here</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Hirshman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,494.38,601.05,30.07,8.74;6,85.14,612.57,91.26,8.74">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.69,625.05,449.80,8.74;6,85.14,636.57,379.89,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,473.10,625.05,51.39,8.74;6,85.14,636.57,187.71,8.74">The Multiple Language Question Answering Track at CLEF</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Romagnoli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vallin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Peinado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Verdejo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,300.54,636.57,89.22,8.74">CLEF 2003 Workshop</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.69,649.05,449.76,8.74;6,85.14,660.57,439.37,8.74;6,85.14,672.03,52.49,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,484.50,649.05,39.95,8.74;6,85.14,660.57,204.75,8.74">Toward a Document Model for Question Answering Systems</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pérez-Coutiño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Solorio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes-Y-Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>López-López</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Villaseñor-Pineda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,309.27,660.57,122.64,8.74">Advances in Web Intelligence</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3034</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.68,684.57,449.78,8.74;6,85.14,696.03,137.79,8.74" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="6,326.76,684.57,197.71,8.74;6,85.14,696.03,82.43,8.74">The Use of Predictive Annotation for Question Answering in TREC8</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Coden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Samn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note type="report_type">NIST</note>
</biblStruct>

<biblStruct coords="6,74.69,708.51,395.32,8.74" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="6,140.64,708.51,151.07,8.74">4.5: Programs for machine learning</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Morgan Kaufmann</publisher>
			<pubPlace>San Mateo, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,79.26,721.05,445.18,8.74;6,85.14,732.51,73.97,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,214.50,721.05,269.85,8.74">Learning Surface Text Patterns for a Question Answering System</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,504.38,721.05,20.06,8.74;6,85.14,732.51,44.38,8.74">ACL Conference</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,79.28,73.53,445.19,8.74;7,85.14,84.99,375.76,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,518.34,73.53,6.12,8.74;7,85.14,84.99,226.59,8.74">A language independent method for question classification</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Solorio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pérez-Coutiño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes-Y-Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Villaseñor-Pineda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>López-López</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,328.91,84.99,48.46,8.74">COLING-04</title>
		<meeting><address><addrLine>Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,79.27,97.53,288.75,8.74" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="7,132.06,97.53,166.69,8.74">The Nature of Statistical Learning Theory</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,79.26,110.01,445.18,8.74;7,85.14,121.53,67.80,8.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,297.66,110.01,125.23,8.74">Question Answering in Spanish</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Vicedo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Izquierdo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Llopis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Muñoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,429.91,110.01,89.60,8.74">CLEF 2003 Workshop</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,79.26,134.01,445.15,8.74;7,85.14,145.53,436.02,8.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="7,312.90,134.01,211.51,8.74;7,85.14,145.53,70.85,8.74">Los sistemas de Búsqueda de Respuestas desde una perspectiva actual</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Vicedo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Massot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,162.30,145.53,307.02,8.74">Revista de la Sociedad Española para el Procesamiento del Lenguaje Natural</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,79.28,158.01,445.22,8.74;7,85.14,169.53,400.09,8.74" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="7,212.76,158.01,311.74,8.74;7,85.14,169.53,65.07,8.74">Data Mining, Practical Machine Learning Tools and Techniques with Java Implementations</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,157.08,169.53,242.08,8.74">The Morgan Kaufmann Series in Data Management Systems</title>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
