<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,121.76,97.29,351.83,12.64">The DIOGENE Question Answering System at CLEF-2004</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,170.60,135.36,50.35,8.96"><forename type="first">Hristo</forename><surname>Tanev</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">ITC-irst</orgName>
								<orgName type="department" key="dep2">Centro per la Ricerca Scientifica e Tecnologica Via Sommarive</orgName>
								<address>
									<postCode>38050</postCode>
									<settlement>Povo (</settlement>
									<region>TN)</region>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,228.08,135.36,51.84,8.96"><forename type="first">Matteo</forename><surname>Negri</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">ITC-irst</orgName>
								<orgName type="department" key="dep2">Centro per la Ricerca Scientifica e Tecnologica Via Sommarive</orgName>
								<address>
									<postCode>38050</postCode>
									<settlement>Povo (</settlement>
									<region>TN)</region>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,286.66,135.36,71.53,8.96"><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">ITC-irst</orgName>
								<orgName type="department" key="dep2">Centro per la Ricerca Scientifica e Tecnologica Via Sommarive</orgName>
								<address>
									<postCode>38050</postCode>
									<settlement>Povo (</settlement>
									<region>TN)</region>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,365.29,135.36,70.83,8.96"><forename type="first">Milen</forename><surname>Kouylekov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">ITC-irst</orgName>
								<orgName type="department" key="dep2">Centro per la Ricerca Scientifica e Tecnologica Via Sommarive</orgName>
								<address>
									<postCode>38050</postCode>
									<settlement>Povo (</settlement>
									<region>TN)</region>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,121.76,97.29,351.83,12.64">The DIOGENE Question Answering System at CLEF-2004</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">09185FE231C694F96D50885D54CAD9AF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Question Processing Component Search Component Answer Extraction Component WEB Question Tokenization and PoS Tagging Multiwords Recognition Answer Type Identification Expansion Search Engine Query Composition Query Reformulation Answer Validation and Ranking</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the ITC-irst Multilingual Question Answering system DIOGENE. The system was used successfully on the CLEF-2003, TREC-2003, TREC-2002 and TREC-2001 QA  tracks. DIOGENE relies on a classical three-layer architecture: question processing, document retrieval, answer extraction and validation. DIOGENE uses MultiWordNet [Pianta et.al. 2002]   (http://multiwordnet.itc.it) which facilitates the transfer of knowledge between languages. For answer validation we used the Web. This year we used also a set of linguistic templates for answering specific questions like definition questions, location questions, and a subset of who-is and what-is questions. DIOGENE participated in both monolingual Italian-Italian task and in the cross-language Italian-English task. A co-operative participation together with the Bulgarian Academy of Sciences was carried out in the cross-language Bulgarian-English QA task.</p><p>high-level description of the basic components of the DIOGENE architecture. Section 3 elaborates on the use of linguistic templates for answer extraction. Section 4 describes the Web validation approach which we used this year. Section 5 discusses the results in all the tasks where we participated. Finally, we describe the future directions for system development.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Research in Question Answering (QA) has received a strong boost in recent years by the QA track organized within the TREC conferences <ref type="bibr" coords="1,192.65,451.32,61.47,8.96" target="#b11">[Voorhees 2004</ref>], which aims at assessing the capability of systems to return exact answers to open-domain English questions. However, the TREC conferences are concentrated exclusively on the English language. In contrast, the CLEF conferences provide a multilingual forum for evaluation of NLP systems in languages other than English. Multilinguality has been recognized as an important issue for the future of QA <ref type="bibr" coords="1,99.29,497.28,75.67,8.96">[Burger et.al.2001]</ref>. In CLEF-2003 a multilingual QA task was introduced for the first time. Our system showed promising results in CLEF-2003 in the monolingual Italian and cross-language Italian-English tasks. This encouraged us to participate also in the Bulgarian-English cross-language task, promoted this year.</p><p>The multilingual version of DIOGENE was built upon the same well tested three-layers architecture of the English version <ref type="bibr" coords="1,136.04,560.76,85.71,8.96">[Magnini et.al.2002a]</ref>. Figure <ref type="figure" coords="1,256.84,560.76,4.98,8.96">1</ref> shows the main constituents of this common backbone: these are the question processing component, the document retrieval component, and the answer extraction and validation component. In all the monolingual and cross-language modalities DIOGENE relies on the knowledge in multilingual ontology MultiWordNet <ref type="bibr" coords="1,226.97,595.32,73.78,8.96">[Pianta et.al.2002]</ref>, manually created rules for named entity recognition and question type identification, a set of handcrafted answer extraction templates and statistical information collected from the Web and off-line multilingual corpora. DIOGENE CLEF-2004 architecture was similar to the CLEF-2003 version <ref type="bibr" coords="1,153.25,629.76,72.45,8.96">[Negri et.al. 2003]</ref>. A novel feature for the <ref type="bibr" coords="1,327.10,629.76,47.65,8.96">CLEF-2004</ref> version is the answer extraction and validation via linguistic templates. Linguistic templates were particularly important for the definition questions.</p><p>The following sections will provide a general overview of our participation to the monolingual Italian (M-I), bilingual Italian/English (B-I/E) and bilingual Bulgarian/English (B-B/E) tasks of the multiple-language QA track at <ref type="bibr" coords="1,106.96,678.72,45.21,8.96">CLEF-2004</ref>. In all the three tasks 200 questions were posed to the QA systems. For the M-I task questions were posed in Italian and the answer had to be sought in an Italian text collection (the 193Mb corpus of the 1994 year of La Stampa newspaper and the 172Mb corpus of the 1994 and 1995 SDA press agency),. For the bilingual B-I/E and B-B/E tasks questions were posed in Italian, respectively Bulgarian, and the answer had to be recovered from English corpus (the 425Mb corpus of the 1994 year of Los Angeles Times and 157Mb corpus of the 1995 year of Glasgow Herald ). The rest of the paper is structured as follows: Section 2 provides B-B/E</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linguistic Templates</head><p>Translations Selection Figure1: The architecture of the DIOGENE system</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>System Architecture Overview</p><p>The overall system architecture remained the same as the architecture with which we participated in CLEF-2003 <ref type="bibr" coords="2,70.76,641.76,66.70,8.96">[Negri et.al.2003</ref>], apart of the linguistic templates which we plugged in the system and the new Web validation procedure which we used. DIOGENE is capable of processing questions in Italian and English and to search the answers in text collections in Italian and English. The system has multilingual architecture -the same modules work both for English and Italian, using language-specific rules and resources when necessary. In the bililngual Bulgarian-English task (B-B/E) the questions were processed in the Bulgarian Academy of Sciences. For each question in Bulgarian, DIOGENE obtained a question type (what the question is about -person, location, etc.) list of keywords with all the possible translations in English. Next, DIOGENE chose the right keyword translations using a statistical approach described earlier in <ref type="bibr" coords="2,277.40,722.28,68.74,8.96">[Negri et.al.2003</ref>]. In both cross-language tasks after the question processing, all the other modules work in the same manner as the monolingual English version of DIOGENE (see <ref type="bibr" coords="2,128.87,745.20,83.12,8.96">[Magnini et.al.2002a</ref>] and <ref type="bibr" coords="2,235.41,745.20,67.35,8.96">[Negri et.al.2003</ref>] for more details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Question processing</head><p>Question processing has five basic stages: question pre-processing (part-of-speech tagging, multiword recognition); answer type identification which defines what is the question about: person, location, date, definition, etc.; keyword extraction from the question; keyword translation for cross-language tasks; keyword expansion with synonyms and morphological derivations. All these components were described earlier in our CLEF-2003 system report (see <ref type="bibr" coords="3,199.76,148.68,73.90,8.96" target="#b6">[Negri et al. 2003]</ref>). Here we will just sketch the most interesting stages of the question processing.</p><p>Answer type identification. For Italian we used hand-crafted, language specific rules for answer type identification similar to the English ones. These rules operate on part-of-speech tagged questions. Semantic predicates defined over the synsets of the MultiWordNet hierarchy are used in the answer type identification. For instance, the rule described in (1) matches any question starting with "quale" ("what"), whose first noun, if any, is a person. For instance, the rule matches the question"Quale presidente americano è stato renitente alla leva?" ("Which American President failed to report for military service?"), since the predicate person-p returns true for "presidente". However, the rule will not be activated on the question. "Qual è il partito di Charles Millon?" ("What is the party of Charles Milton?"), since "partito"("party") in MultiWordNet is not a hyponym of the concept "person" . Rule (1) gives on its output the type of entity which the question asks for ("person" in this case).</p><p>Since in MultiWordNet both English and Italian synsets are aligned, the predicates in the answeridentification rules like person-p can be used for both the languages. Keyword translation. Both B-I/E and B-B/E require translation of the question in the target language (English). Since state-of-the-art translation systems are not optimized for translation of short pieces of text as the questions are, we have developed a methodology specifically designed for keyword translation. First, for each keyword from the question, all the possible English translations are found using bilingual dictionaries and MultiWordNet (for Bulgarian this has been done at the Bulgarian Academy of Sciences). Next, the most plausible combination of keyword translations is chosen. We chose the combination of keyword translations (k 1 , k 2 ,...,k n ) with the highest frequency of co-occurrence in an English corpus (we used the AQUAINT and TIPSTER collections). The main assumption is that the more frequently a keyword translation combination appears with the translations close to each other (in one and the same paragraph) , the more plausible this combination is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Search Component</head><p>DIOGENE at the QA tasks within CLEF-2004 relied on the same search component developed for the English version of DIOGENE, as it is described in <ref type="bibr" coords="3,243.31,584.16,86.35,8.96">[Magnini et al. 2002a</ref>]. Such component first combines the question keywords and their lexical expansions in a Boolean query; then performs document retrieval accessing the target document collections.</p><p>The search is performed by Managing Gigabytes (MG) <ref type="bibr" coords="3,320.70,618.60,72.51,8.96">[Witten et.al.1999</ref>], an open-source indexing and retrieval system for text, images, and textual images covered by a GNU public license and available from http://www.cs.mu.oz.au/mg/. MG allows for a fast and customizable indexing. We opted to index the document collection at the paragraph level, using the paragraph markers provided in the SGML format of the document collection. This way, although no proximity operator is implemented in MG, the paragraph index makes the "AND" Boolean operator perform proximity search. In order to divide very long paragraphs into short passages, we set 20 text lines as the limit for paragraphs' length.</p><p>The document retrieval module uses the Boolean query mode of MG. At the first step of the search phase all the basic keywords are connected in a complex "AND" clause, where the term variants (morphological derivations and synonyms) are combined in an "OR" clause. As an example, given the question "Quando morì Lenin?" ("When did Lenin die?"), the basic keywords resulting from the translation process (i.e. "die" and "Lenin") are expanded and combined into:</p><p>[Lenin AND (die OR dies OR died OR dying OR death OR deaths)] However, Boolean queries often tend to return too many or too few documents. To cope with this problem, we implemented a feedback loop which starts with a query containing all the relevant keywords and gradually simplifies it by ignoring some of them. Several heuristics are used by the algorithm. For example, a word is removed if the resulting query does not produce more than a fixed number of hits. Other heuristics consider the capitalization of the query terms, their part of speech, their position in the question, WORDNET class, etc. <ref type="bibr" coords="4,70.76,172.20,87.63,8.96">[Magnini et al. 2002a]</ref>.</p><p>A post-processing procedure finally orders the paragraphs on the basis of the number and proximity of the keywords and their synonyms which are present.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Answer Extraction Component</head><p>Two types of questions were present in the questions set this year: factoid questions, which usually ask for a named entity: person, location, organization, etc., and definition questions (e.g. "Who is Valentina Tereshkova?") which ask for person or concept definitions. For the factoid questions, the answer extraction component first performs a selection of the answer candidates through named entities recognition and linguistic templates; then, a Web-based procedure for answer validation is applied over the selected named entities to choose the best one. As for definition questions, DIOGENE extracts the answers using linguistic templates and then chooses the most plausible definition using semantic and syntactic clues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Named Entities Recognition (NER).</head><p>The named entities recognition module is in charge of identifying, within the relevant passages returned by the search engine, all the entities that match the answer type category (e.g. person, organization, location, measure, etc.). The Italian version of the NER module tested on a 77Kb text corpus<ref type="foot" coords="4,97.40,386.97,3.24,5.83" target="#foot_0">1</ref> revealed a performance comparable to the English NER <ref type="bibr" coords="4,346.61,389.16,91.33,8.96">[Magnini et al 2002c]</ref>, with an overall F-Measure score of 83%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linguistic templates.</head><p>In order to increase the system precision we have applied linguistic templates to several question types: definition questions (both for English and Italian), dove-è (where-is) questions (only for Italian) and quale/chi-è (what/who-is) questions (only for Italian). Regarding the definition questions, the templates were the only source of information for answer extraction. The other two types of templates were applied for the respective question types, and if an answer was extracted, it was validated on the Web through answer validation templates; if no candidate was captured by the patterns, the classical DIOGENE answer extraction and validation was applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer Validation.</head><p>In CLEF-2003 we used AltaVista for answer validation. Since this year AltaVista has changed its access interface and has not provided any further support of the proximity search on which we base our statistical approach, we opted for an alternative method based on the analysis of the snippets returned by AllTheWeb. Each named entity returned as a candidate answer to a factoid question was tested for close cooccurrence with the question keywords. This provides DIOGENE with clues for the plausibility of the candidate answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Linguistic Templates for Answer Extraction and Validation</head><p>Linguistic templates has proved to be an appropriate technique for certain question types such as definition and location questions. For our CLEF-2004 participation we plugged in DIOGENE linguistic templates which perform answer extraction and validation for: (i) definition questions, (ii) location questions of the type "Where is &lt;LOCATION&gt;?", and (iii) what-is or who-is questions of the type "(What | Who) is &lt;NOUN PHRASE&gt;?" (e.g. "What is the Iraq currency?"). For definition questions we created bi-lingual templates which work both for English and Italian. Regarding the other two classes of questions we created manually templates for Italian.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Definition Questions</head><p>In the CLEF-2004 QA track 10% of the questions (20) were definition questions (e.g. "What is UNICEF?" "What is yakuza?" "Who is Jorge Amado?"). While definition questions are among the most natural and frequent kinds of queries posed by humans, they raise specific issues in the development of the QA systems.</p><p>First, the answer of a definition question is not a named entity. Next, while for most of the questions we have many content words whose co-occurrence may indicate the position of the answer, for the definition questions we have only one content word or multiword (i.e.the focus of the question, that is the entity for which the question asks for a definition).</p><p>CLEF-2004 QA track organizers stated in the guidelines that questions will be about persons and organizations. This makes the definition extraction more feasible, since capturing information about organization and persons is usually easier than finding definition of random concepts. We adopted the approach described in <ref type="bibr" coords="5,256.97,217.44,78.27,8.96" target="#b10">[Tanev et al. 2004</ref>]. Our approach relies on linguistic regular expressions, much more expressive than the string templates introduced by [Ravichandran&amp;Hovy 2002]. Since the syntactic structure of the definitions in Italian and English is not much different, we aligned the templates for English and Italian, obtaining multilingual templates. For example the following bilingual template (2) was used for capturing canonical definitions in English and Italian:</p><p>(</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) [~ Prep ] &lt;FOCUS&gt; [~ Noun](1) [eng: lemma:be | ita: lemma:essere] [~ Prep Verb Conj](3) Noun</head><p>This pattern captures the following sequence of words: a word which is not a preposition ( [~ Prep] ); followed by the focus ( &lt;FOCUS&gt; ) of the question; possibly followed by one word which is not a noun ( [~ Noun](1) ); followed by the auxiliary verb "be" appearing in one of the considered languages; followed by at most 3 words ( [~ Prep Verb Conj](3) ), none of which is a preposition, a finite verb form or a conjunction; followed by a noun (Noun). This patter captures a broad range of canonical definitions of the type "yakuza is the Japanese mafia" (English), or "yakuza è la mafia giapponese" (Italian). Correctly, it will not capture "The members of yakudza are…", since no preposition is allowed before the focus.</p><p>Different templates have different levels of reliability, therefore each extracted definition obtains a syntactic score depending on the template with which it was extracted. Currently, we have manually defined the reliability score for the definition extraction templates; however, we consider learning it automatically.</p><p>Our experiments revealed that the syntactic score does not provide reliable ranking. Therefore, we applied complementary scoring strategy based on MultiWordNet (this multilingual ontology gave us possibility to work both for Italian and English):</p><p>1. If the question is about a person we search in the extracted definition a concept which is a hyponym of the concept "person" in MultiWordNet. 2. If the question is not about a person, it will be about organization, so we search for hyponyms of the concept "organization". 3. If the focus of the definition question is present in MultiWordNet, an additional score is given if terms from the gloss of the focus or its hypernyms appear in the candidate definition.</p><p>In case the focus of the definition question was not present in MultiWordNet, we searched in the Wickipedia database and assigned additional score if a term with hyperlink from the Wickipedia article (usually these hyperlinks mean that the term is important) appeared in the candidate definition.</p><p>On the definition questions we gained 40% accuracy in the monolingual Italian task and 25% on the crosslanguage Italian-English task. Although there is a lot of space for improvement, the definition question accuracy was higher than the accuracy of the answers of the factoid questions, especially in the monolingual Italian task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Location questions</head><p>We have developed templates for location questions only for Italian. We noted that in many cases the answer of questions such as: "Dove si trova la Valle dei Re?" ("Where is the Valley of the Kings?") or "Dove si tiene il Motorshow?" ("Where does Motorshow take place?") is expressed through phrases like "La Valle dei Re in Egitto" ("Valley of the Kings in Egypt") or "Motorshow a Bologna" ("Motorshow in Bologna"). Such answers are captured easily through superficial patterns like:</p><p>(3) &lt;FOCUS&gt; (in|nel|nella) &lt;LOCATION&gt;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt;FOCUS&gt; a &lt;LOCATION&gt; &lt;FOCUS&gt; si trova in &lt;LOCATION&gt; &lt;FOCUS&gt; ed in tutt(a|o|i) (il| l'| la | gli | i)? &lt;LOCATION&gt;</head><p>The extraction of the focus was carried out through specific question processing patterns.</p><p>For each answer we count also how many times it appears in a location template; if it appears too little or many candidates are extracted, we further validate the answers querying the Web: "&lt;FOCUS&gt; in &lt;LOCATION&gt;" OR "&lt;FOCUS&gt; nel &lt;LOCATION&gt;" OR "&lt;FOCUS&gt; nella &lt;LOCATION&gt;"</p><p>For example: "Motorshow in Bologna" OR "Motorshow nel Bologna" OR "Motorshow nella Bologna" OR "Motorshow a Bologna"</p><p>The number of documents returned by the search engine as a response to this query together with repetition of the answer within templates in the local corpus are clues to the reliability of the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Questions of the type "(what|who) is &lt;NOUN PHRASE&gt;"</head><p>We implemented templates for these questions only for Italian where these questions begin with "Chi è" or "Qual è" . Questions of this class do not contain any other verb except the auxiliary one. Examples of such questions from CLEF-2004 monolingual Italian test set are:</p><p>Qual è la unita di frequenza? ("What is the frequency unit?") Qual è la capitale della Russia?("What is the capital of Russia?") Chi è il ministro delle finanze russo?("Who is the Russian minister of finances?") These questions are somehow opposite to the definition questions. In effect, such questions represent short definitions and the answers are entities which can be described via these definitions. Such kind of questions we call inverted definition questions, since if you ask a definition question about their answer, the focus of the inverted definition question will represent correct definition. For example Question: Chi è il ministro delle finanze russo?("Who is the Russian minister of finances?") Answer: Boris Fiodorov Definition question: Chi è Boris Fiodorov? ("Who is Boris Fiodorov?") Answer: il ministro delle finanze russo ("the Russian minister of finances") Taking this into account, we applied patterns similar to the definition question patterns described in Section 3.1. For this question type we used also Web validation via patterns when necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Using the Web to Validate Answers</head><p>When the named entity recognizer returns a list of candidate answers, those which are closer to the question keywords (and therefore considered more reliable) are passed to the answer validation algorithm which chooses the best candidate (if such exists).</p><p>The basic idea behind our approach to answer validation is to identify semantic relations between the question and each candidate answer by searching for their co-occurrences in a large document collection. In this framework, we consider the Web as the largest open domain text corpus containing information about almost all the different areas of the human knowledge.</p><p>In our previous participation at CLEF-2003 we used a Web validation method based on a co-occurrence statistical formula (see <ref type="bibr" coords="6,167.21,730.44,88.82,8.96">[Magnini et al. 2002d</ref>] and <ref type="bibr" coords="6,282.39,730.44,72.61,8.96" target="#b6">[Negri et al. 2003</ref>] for details). The frequency information used in this formula was taken from AltaVista. We used AltaVista's proximity operator "NEAR" which allowed for identifying the number of pages in which certain words co-occur close to each other. However, AltaVista changed its interface, providing no further support for proximity searches, nor we were able to find a public available search engine which offers the same feature. Therefore, we opted for content based answer validation, whose main idea we described earlier in <ref type="bibr" coords="7,245.00,114.60,90.11,8.96">[Magnini et al. 2002b</ref>] and we used the AllTheWeb search engine (www.alltheweb.com). Our Web validation algorithm performs the following basic steps:</p><p>1. It queries the Web with the question keywords QK and the answer a. For example, for the question "Quanti anni di prigionia ha subito Nelson Mandela?" ("How many years did Nelson Mandel spend in the prison?") and the (correct) candidate answer 27, we have:</p><p>QK={anni, prigionia, subito, Nelson, Mandela}; a=27</p><p>2. The top 100 hits returned by AllTheWeb are explored and for each text fragment where the answer a cooccurs with some of the QK words we calculate a score on the basis of the distance between a and the number of keywords present in QK which also appear in the snippet, according with the following formula:</p><formula xml:id="formula_0" coords="7,84.32,248.16,138.08,28.90">∏ ∩ ∈ + = QK snippet k k a snippet score | | 1 2 ) (</formula><p>, where |a k| is the distance in tokens between the candidate answer a and a question keyword k which appears in the snippet.</p><p>For example for the text fragment: "Nelson Mandela viene liberato dopo 27 anni di dura prigionia e di torture" this formula assigns a score of 55.08, while for the snippet "Nelson Mandela, che a Robben Island, l'isola-prigione a largo di Citta' del Capo dove ha trascorso 18 dei suoi 27 anni di prigionia" we assign 43.14, since the distance between 27 and both keywords Nelson and Mandela is greater.</p><p>3. The score gained from different fragments are summed up for each candidate answer. 4. The candidate answer which gains the highest score is chosen.</p><p>DIOGENE returns as answer the candidate for which the answer validation returns the highest score. If the answer validation module returns zero for all the candidate answers, DIOGENE returns NIL as answer. After some normalization the answer validation score is returned as a confidence score as required this year by the CLEF QA track guidelines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>DIOGENE was evaluated in five runs: two in the monolingual Italian QA task, two in the Italian-English task, and one in the Bulgarian-English task. In each task the system had to answer to 200 questions, returning one answer per question. For some questions NIL was allowed, which is interpreted as "no answer exists to the question". The following table shows the results in all these tasks. The first two rows of Table <ref type="table" coords="8,194.45,91.68,4.98,8.96" target="#tab_0">1</ref> show our results in the monolingual Italian task, the second two show the results from the cross-language Italian-English task, and the last row shows the results from our joint participation at the Bulgarian-English task. The first runs in both tasks (irst041itit and irst041iten) use the Web validation to calculate the final score of the answer candidates. The second runs (irst042itit and irst042iten) combine the results from the Web validation with the keyword density in the paragraph of the local text collection where the answer was found. The first column in table 1 is the run tag, the second column (R) shows the number of correct answers, the third column (W) contains the number of the wrong answers, the fourth column (U) indicates the number of unsupported answers (these are correct answers but extracted from document which do not support them), the fifth column (X) contains the number of inexact answers, the sixth column (Factoid) contains the number of the correctly answered factoid questions against all the factoid questions and the precision, the seventh column (DEF) contains the number and the precision of the correctly answered definition questions, the eighth column (NIL) contains the number and precision of the correctly returned NIL, the ninth column (Overall Accuracy) shows the main evaluation criteria -the percent of the correctly answered questions and the last column (Conf. Score) shows the value of the confidence weighted measure calculated by the CLEF judges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>The table shows that our results for this year QA tracks are lower than our CLEF-2003 results. We can explain that with the increased difficulty of this year questions. The main obstacle in front of DIOGENE were factoid questions which do not require a named entity as an answer (e.g. "Quale animale tuba?" ("What animal coos?")) .</p><p>The difference of the accuracy for monolingual Italian and Italian-English tasks is not big, which means that our translation mechanism works satisfactory.</p><p>Results show that pure Web based validation works 5-6% better than the combination with the keyword density.</p><p>Overall accuracy in the Bulgarian-English task was 10% under our best run on the Italian-English test set. However, accuracy on definition questions is equal for both cross-language tasks. Taking into account that this is one of the first QA experiments for Bulgarian (only one similar work has been currently reported - <ref type="bibr" coords="8,495.90,367.68,28.43,8.96;8,70.76,379.08,22.21,8.96" target="#b9">[Tanev 2003]</ref>) and the difficulty of this year questions, results are encouraging. Definition question accuracy is better than the accuracy on the factoid questions. In particular, the results on the definition questions in the monolingual Italian task (40%) are much higher than the corresponding factoid accuracy (26.67% in the best run). There is a significant difference in the precision on the definition questions between monolingual and cross-language runs (monolingual run was 15% better). We did not study thoroughly the reasons for this difference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>We described the architecture of our multilingual system DIOGENE, stressing on the improvements for CLEF-2004. The system combines the multilingual knowledge in MultiWordNet, a set of linguistic templates and information mined on the fly from the Web.</p><p>Our best run was on the monolingual Italian task (28% overall accuracy); on the definition questions we have 40% accuracy in the same task. While these figures leave a lot of space for improvement, they demonstrate the feasibility of our multilingual approach. In particular, our results on the definition questions show that putting together the knowledge encoded in an ontology and a set of linguistic templates can be an efficient strategy which should be studied further.</p><p>In the future development of our QA system we intend to strengthen the linguistic infrastructure for the Italian language by trying to integrate parsing and by using more extensively MultiWordNet. We would like DIOGENE to answer also to questions which do not have named entity as answers. We intend to create more templates which cover broader range of questions and to evaluate their performance.</p><p>Finally, we would like to extend the applicability of our QA system into the multilingual dimension by considering other languages.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,85.28,646.44,416.98,103.88"><head>Table 1 :</head><label>1</label><figDesc>Results of DIOGENE at the CLEF-2004 QA track.</figDesc><table coords="7,85.28,646.44,416.98,80.60"><row><cell></cell><cell cols="2">R W U X Factoid</cell><cell>DEF</cell><cell>NIL</cell><cell>Overall</cell><cell>Conf.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Accuracy</cell><cell>Score</cell></row><row><cell>irst041itit</cell><cell cols="4">56 131 2 11 48/180(26.67%) 8/20 (40%) 6/22 (27%)</cell><cell>28.00%</cell><cell>0.156</cell></row><row><cell>irst042itit</cell><cell>44 147 0 9</cell><cell>36/180(20%)</cell><cell cols="3">8/20 (40%) 4/6 (66.7%) 22.00%</cell><cell>0.107</cell></row><row><cell>irst041iten</cell><cell>45 146 3 6</cell><cell cols="3">40/180(22.22%) 5/20(25%) 6/25(24%)</cell><cell>22.50%</cell><cell>0.122</cell></row><row><cell>irst042iten</cell><cell>35 158 2 5</cell><cell cols="3">30/180(16.67%) 5/20(25%) 6/25(24%)</cell><cell>17.50%</cell><cell>0.075</cell></row><row><cell cols="2">bgas041bgen 26 168 1 5</cell><cell cols="4">21/180(11.67%) 5/20(25%) 8/59(13.6%) 13.00%</cell><cell>0.056</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,77.36,725.76,446.94,8.96;4,79.28,736.80,445.02,8.96;4,79.28,747.84,126.57,8.96"><p>Reference transcripts of two broadcast news shows, including a total of about 7,000 words and 322 tagged named entities, were manually produced for evaluation purposes and have been kindly provided by Marcello Federico and Vanessa Sandrini.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,88.88,108.00,435.48,8.96;9,88.88,119.40,435.55,8.96;9,88.88,130.92,435.74,8.96;9,88.88,142.44,162.04,8.96;9,88.88,153.96,322.86,8.96" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Chaudhri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Israel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Jacquemin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Maiorano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ogden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Shrihari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strzalkowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weishedel</surname></persName>
		</author>
		<ptr target="http://www-nlpir.nist.gov/projects/duc/papers/qa.Roadmap-paper_v2.doc" />
		<title level="m" coord="9,282.31,130.92,242.31,8.96;9,88.88,142.44,131.68,8.96">Issues Tasks and Program Structures to Roadmap Research in Question &amp; Answering (Q&amp;A)</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,88.88,177.00,435.59,8.96;9,88.88,188.40,435.56,8.96;9,88.88,199.92,27.06,8.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,280.46,177.00,244.01,8.96;9,88.88,188.40,7.22,8.96">Mining Knowledge from repeated Co-occurrences: DIOGENE at</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Prevete</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tanev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,99.73,188.40,336.95,8.96">TREC-2002 Proceedings of the Eleventh Text Retrieval Conference (TREC-2002)</title>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,88.88,222.96,435.54,8.96;9,88.88,234.48,378.93,8.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,289.95,222.96,234.48,8.96;9,88.88,234.48,121.62,8.96">Comparing Statistical and Content-Based Techniques for Answer Validation on the Web</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Prevete</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tanev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,218.41,234.48,163.93,8.96">Proceedings of the VIII Convegno AI*IA</title>
		<meeting>the VIII Convegno AI*IA<address><addrLine>Siena, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,88.88,257.40,435.48,8.96;9,88.88,268.92,435.42,8.96;9,88.88,280.44,92.45,8.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,309.07,257.40,215.29,8.96;9,88.88,268.92,46.97,8.96">A WORDNET-Based Approach to Named Entities Recognition</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Prevete</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tanev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,143.51,268.92,376.21,8.96">Proceedings of SemaNet02, COLING Workshop on Building and Using Semantic Networks</title>
		<meeting>SemaNet02, COLING Workshop on Building and Using Semantic Networks<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,88.88,303.48,435.71,8.96;9,88.88,315.00,185.96,8.96;9,274.88,312.81,5.04,5.83;9,286.16,315.00,238.28,8.96;9,88.88,326.40,204.83,8.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,289.16,303.48,235.43,8.96;9,88.88,315.00,77.85,8.96">Is It the Right Answer? Exploiting Web Redundancy for Answer Validation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Prevete</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tanev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,176.94,315.00,97.90,8.96;9,274.88,312.81,5.04,5.83;9,286.16,315.00,238.28,8.96;9,88.88,326.40,94.63,8.96">Proceedings of the 40 th Annual Meeting of the Association for Computational Linguistics (ACL-2002)</title>
		<meeting>the 40 th Annual Meeting of the Association for Computational Linguistics (ACL-2002)<address><addrLine>Philadelphia, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,88.88,349.44,399.58,8.96" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Shutze</surname></persName>
		</author>
		<title level="m" coord="9,191.41,349.44,220.84,8.96">Foundations of Statistical Natural Language Processing</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,88.88,360.96,435.40,8.96;9,88.88,372.48,255.47,8.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,241.16,360.96,283.12,8.96">Bridging Languages for Question Answering: DIOGENE at CLEF-2003</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tanev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,88.88,372.48,107.38,8.96">Proceedings of CLEF-2003</title>
		<meeting>CLEF-2003<address><addrLine>Trondheim, Norway</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-08">August 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,88.88,395.40,435.46,8.96;9,88.88,406.92,81.76,8.96;9,170.72,404.73,4.32,5.83;9,176.72,406.92,257.84,8.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,252.46,395.40,267.53,8.96">MULTIWORDNET: Developing an Aligned Multilingual Database</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Pianta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Girardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,88.88,406.92,81.76,8.96;9,170.72,404.73,4.32,5.83;9,176.72,406.92,167.16,8.96">Proceedings of the 1 st International Global WordNet Conference</title>
		<meeting>the 1 st International Global WordNet Conference<address><addrLine>Mysore, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,88.88,429.96,435.61,8.96;9,88.88,441.48,86.81,8.96;9,175.76,439.29,5.04,5.83;9,183.44,441.48,262.51,8.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,220.71,429.96,285.24,8.96">Learning Surface Text Patterns for a Question Answering System</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,88.88,441.48,86.81,8.96;9,175.76,439.29,5.04,5.83;9,183.44,441.48,66.55,8.96">Proceedings of the 40 th ACL Conference</title>
		<meeting>the 40 th ACL Conference<address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="9,88.88,464.40,435.79,8.96;9,88.88,475.92,150.91,8.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,137.68,464.40,249.94,8.96">Socrates -a Question Answering Prototype for Bulgarian</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tanev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,397.49,464.40,121.86,8.96">Proceedings of RANLP-2003</title>
		<meeting>RANLP-2003<address><addrLine>Borovets, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-09">September, 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,88.88,498.96,435.59,8.96;9,88.88,510.48,435.51,8.96;9,88.88,521.88,323.14,8.96" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,347.23,498.96,177.25,8.96;9,88.88,510.48,200.81,8.96">Multilingual Pattern Libraries for Question Answering: a Case Study for Definition Questions</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tanev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kouylekov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Coppola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,296.72,510.48,227.67,8.96;9,88.88,521.88,172.87,8.96">Fourth International Conference on Language Resources and Evaluation ( LREC-2004 ) Proceedings</title>
		<meeting><address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">May 26-28, 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,88.88,544.92,435.51,8.96;9,88.88,556.44,215.05,8.96" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
		<title level="m" coord="9,147.77,544.92,376.62,8.96;9,88.88,556.44,104.07,8.96">Overview of the TREC 2003 Question Answering Track Proceedings of the Sixth Retrieval Conference (TREC-2003)</title>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,88.88,579.36,435.49,8.96;9,88.88,590.88,248.83,8.96" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="9,228.10,579.36,296.27,8.96">Managing Gigabytes: Compressing and Indexing Documents and Images</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Bell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Morgan Kaufmann Publishers</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>second ed.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
