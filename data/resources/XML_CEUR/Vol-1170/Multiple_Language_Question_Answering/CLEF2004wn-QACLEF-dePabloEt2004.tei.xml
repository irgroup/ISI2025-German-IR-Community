<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,139.50,74.74,312.80,12.60">miraQA: Initial experiments in Question Answering</title>
				<funder ref="#_gf2T4Ac">
					<orgName type="full">MIRACLE (Regional Government of Madrid, Regional Plan for Research</orgName>
				</funder>
				<funder ref="#_Tx66VSB">
					<orgName type="full">OmniPaper (European Union</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,74.00,109.07,46.50,9.00"><forename type="first">C</forename><surname>De Pablo</surname></persName>
							<email>cdepablo@uc3m.es</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="laboratory">Advanced Databases Group</orgName>
								<orgName type="institution">Universidad Carlos III de Madrid</orgName>
								<address>
									<addrLine>Avda. Universidad 30</addrLine>
									<postCode>28911</postCode>
									<settlement>Leganés, Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,129.70,109.07,98.24,9.00"><forename type="first">J</forename><forename type="middle">L</forename><surname>Martínez-Fernández</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="laboratory">Advanced Databases Group</orgName>
								<orgName type="institution">Universidad Carlos III de Madrid</orgName>
								<address>
									<addrLine>Avda. Universidad 30</addrLine>
									<postCode>28911</postCode>
									<settlement>Leganés, Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Decisiond and Language</orgName>
								<orgName type="laboratory">DAEDALUS -Data</orgName>
								<orgName type="institution">S.A. Centro de Empresas &quot;La Arboleda&quot;</orgName>
								<address>
									<addrLine>Ctra. N-III km. 7</addrLine>
									<postCode>300, 28031</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,243.10,109.07,46.64,9.00"><forename type="first">P</forename><surname>Martínez</surname></persName>
							<email>jmartinez@daedalus.es</email>
						</author>
						<author>
							<persName coords="1,298.90,109.07,38.24,9.00"><forename type="first">J</forename><surname>Villena</surname></persName>
							<email>jvillena@it.uc3m.es</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="laboratory">Advanced Databases Group</orgName>
								<orgName type="institution">Universidad Carlos III de Madrid</orgName>
								<address>
									<addrLine>Avda. Universidad 30</addrLine>
									<postCode>28911</postCode>
									<settlement>Leganés, Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Telematic Engineering</orgName>
								<orgName type="institution">Universidad Carlos III de Madrid</orgName>
								<address>
									<addrLine>Avda. Universidad 30</addrLine>
									<postCode>28911</postCode>
									<settlement>Leganés, Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Decisiond and Language</orgName>
								<orgName type="laboratory">DAEDALUS -Data</orgName>
								<orgName type="institution">S.A. Centro de Empresas &quot;La Arboleda&quot;</orgName>
								<address>
									<addrLine>Ctra. N-III km. 7</addrLine>
									<postCode>300, 28031</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,352.50,109.07,86.90,9.00"><forename type="first">A</forename><forename type="middle">M</forename><surname>García-Serrano</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Artificial Intelligence Department</orgName>
								<orgName type="institution">Universidad Politécnica de Madrid</orgName>
								<address>
									<addrLine>Campus de Montegancedo s/n, Boadilla del Monte</addrLine>
									<postCode>28660</postCode>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,448.60,109.07,42.88,9.00"><forename type="first">J</forename><forename type="middle">M</forename><surname>Goñi</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Department of Mathematics Applied to Information Techmologies</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">E.T.S.I. Telecomunicación</orgName>
								<orgName type="institution">Universidad Politécnica de Madrid</orgName>
								<address>
									<addrLine>Avda. Ciudad Universitaria s/n</addrLine>
									<postCode>28040</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,515.00,109.07,3.20,9.00;1,271.00,122.77,49.34,9.00"><forename type="first">J</forename><forename type="middle">C</forename><surname>González</surname></persName>
							<email>sgonzalez@isys.dia.fi.upm.es</email>
							<affiliation key="aff3">
								<orgName type="department">Decisiond and Language</orgName>
								<orgName type="laboratory">DAEDALUS -Data</orgName>
								<orgName type="institution">S.A. Centro de Empresas &quot;La Arboleda&quot;</orgName>
								<address>
									<addrLine>Ctra. N-III km. 7</addrLine>
									<postCode>300, 28031</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,139.50,74.74,312.80,12.60">miraQA: Initial experiments in Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A8F056C237D6A3A6E60E1C8486112BBC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present the miraQA system that constitutes MIRACLE first experience in Question Answering for monolingual Spanish and has been developed for QA@CLEF 2004. The architecture of the system is described and details of our approach to Statistical Answer Extraction based on Hidden Markov Models are presented. One run that uses last year question set for training purposes has been submitted. The results are presented together with ideas for improvement.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Question Answering has received a lot of attention during the last years due to the advances in IR and NLP. As in other applications, the bulk of the research has mainly been centered around English, while perhaps, one of the most interesting applications of QA systems could be in cross and multilingual scenarios. Access to concrete quality information in a language that is not spoken or just poorly understood could be advantageous in lots of situations. QA@CLEF has encouraged the development of QA systems in other languages than English and in crosslingual scenarios.</p><p>QA systems are complex because of the number of different modules that they use, and the need for a good integration between them. Even in the case when questions are expecting a simple fact or a short definition as an answer, the requirement of more precise information has entailed the use of language and domain specific modules. On the other hand, some other approaches relying on data-intensive <ref type="bibr" coords="1,413.10,621.77,10.57,9.00" target="#b2">[3]</ref>, machine learning and statistical techniques have achieved wide spread and relative success. Moreover, the interest of these approaches for multilingual QA systems lies on the possibilitiy of adapt them quickly to other target languages.</p><p>In this paper we present our first approach to the QA task. As we have not taken part before in any of the QA evaluation forums, most of the work has been spent in putting together a system from available resources. So far, the system we present is targeted only to the monolingual Spanish task. The system explores the use of Hidden Markov Models for Answer Extraction and uses Google to collect training data. The results show that further improvements in the method and appropiate tuning is needed but it remains promising. We expect to continue working on this system to enhance its results and inspect the suitability of the approach for different languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System Description</head><p>miraQA, the system developed for QA@CLEF 2004 by the MIRACLE group represents our first attempt to face Question Answering . As Spanish is our mother tongue, we have developed the system for the monolingual Spanish task, where the group is familiar with available tools. Despite the system was developed for Spanish, we had in mind that it should be easily adapted to other target languages. For that reason, we have explored the potential of statistical models for Answer Extraction. Besides, most of the tools that we are using, like POS taggers or partial parsers, are available for almost every other european language.</p><p>The general architecture of miraQA system for QA follows the classical structure in three modules and is presented in the following figure : 

A fourth module for Answer Evaluation would be required to address this year novelty of providing a confidence measure for every answer. Although we appreciate the usefulness of that feature for the final user, we have not been able to include that module in our system due to time constraints.</p><p>In addition to the QA system, we have also developed another system to train the used Hidden Markov Models in our answer extraction phase. The system uses questions and answers to build queries that are posed to Google. Snippets of the results are extracted and used to build a model for the co-ocurrence of question terms and answers. In order to build the models we have used CLEF 2003 evaluation question set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question analysis</head><p>This module classifies the questions according to a manual taxonomy shown in Table <ref type="table" coords="2,430.80,577.87,5.00,9.00" target="#tab_0">1</ref> and composed of 17 classes. The taxonomy was decided considering mainly answer types. For some of them we decided to split or clonflate the classes depending on the frequency of appearance question-answer in QA@CLEF 2003 evaluation set. Questions are partially analysed using ms-tools <ref type="bibr" coords="2,285.90,612.77,10.65,9.00" target="#b1">[2]</ref>. We used MACO tagger and TACAT parser (slightly modified to avoid attacchment of PP chunks). Once the questions are partially parsed, a set of simple rules is applied to classify the questions determining its type, the type of the answer that is expected and assign a set of semantic tags to some of the chunks according to the relations they have with the answer. A simple example for the question "¿Cual es la capital de Croacia?" is shown in Figure <ref type="figure" coords="2,338.00,659.37,3.89,9.00">2</ref>:  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document retrieval</head><p>The IR module retrieves the top most relevant documents for a query and extracts the sentences that contain any of the words expressed in a query. After the question is analyzed, words that have a semantic tag assigned, are used in the query. For robustness purposes, the semantic tags are scanned again to remove stopwords and a query with all the terms is built and given to the IR engine. Our system uses Xapian [10] probabilistic engine to search for the most relevant documents. The last step consists on tokenizing the document using Daedalus Tokenizer <ref type="bibr" coords="3,114.70,481.37,11.63,9.00" target="#b3">[4]</ref> to extract those sentences that contain any of the words or stems that appeared in the query. The system assigns two scores to every sentence, the relevance measure provided by Xapian to the document and another figure proportional to the number of terms that were found in the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer extraction</head><p>The answer extraction module uses a statistical approach to answer pinpointing that is based on a syntacticsemantic context model of the answer built for any of the question-answer types. The following operations are performed:</p><p>1. Parsing and Anchor Searching. The sentences provided by the IR modules containing terms from the questions are parsed in a similar way as questions and training sentences using the ms-tools. Once parsed, the chunks containing the question terms are substituted by their semantic tags and constitute what we have called anchor terms. Finally, sentences are chunked in pieces that form a window of words around anchor terms and passed to the next module. 2. Answer Recognition. Pieces built in this way are passed to the answer extraction module that uses the HMM model. A variant of the N-best recognition strategy is used to identify the most probable sequence of states that originated the POS sequence and identifies an answer as the sequence of words that has been generated from the answer state. The recognition algorithm is guided by the semantic information in order to find a path that passes through the answer state.</p><p>Besides, the algorithm provides a score for every path computed as the sum of the log of the probabilities for that path and sequence. weighted score that takes into account the score of the document, the sentence, the path followed in recognition and their lengths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training of Answer Context</head><p>Models that are used in the answer extraction phase are previously trained from examples. For the training of the models we used the question-answer set provided for QA at CLEF 2003. Questions are analyzed in the same way that they are in the main QA system. Question terms and the answers strings are combined in queries that we send to Google using the Google API. Snippets for the top 100 results are retrieved and stored to build the model. They are splitted in sentences, analyzed and chunks having questions and answer terms are retagged. The tag is either the semantic class assigned to that term in the question or the answer tag (##ANSWER##).</p><p>Only sentences containing the answer and at least one of the other semantic tags are selected to train the model.</p><p>The machine that we built to extract answers is a Hidden Markov Model in wich the states are the syntacticsemantic tags assigned to the chunks while the emitted symbols are the POS tags assigned to the classes. To estimate the transition and emission probabilities we have counted the frequencies of the bigrams for POS-POS and POS-CHUNKS. In order to account for states or symbols that were not seen in the Google sentences we have used the simple add-one smoothing technique. We build a model like this one for every question-answer type that uses a closed set of one to three semantic tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results analysis</head><p>We submitted one run for the monolingual Spanish task (mira041eses) that provides one exact answer to every question. Our system is unable to compute the confidence measure and we limited us to assign the default value of 0. There are two main kinds of questions, factoid and definition and we have tried the same approach for both of them. Besides, the question set contains some questions whose answer could not be found in the document corpus and the valid answer in that case is the NIL string.</p><p>The results obtained for our run mira041eses are outlined in Table <ref type="table" coords="4,339.30,600.67,5.00,9.00">2</ref> Question Type Right Wrong IneXact Unsupported </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2 Result form mira041eses</head><p>Results are fairly low if we compare them with other systems. We attribute these bad results to the fact that the system is in a very early stage of development and tuning. We have obtained several conclusions from the analysis of correct and wrong answers that will guide our future work. The extraction algorithm is working better for factoid questions than definitional. Obviusly, among factoid questions results are also better for certain question-answer classes (DATE,NAME...) which are found often in the training set of questions. This is remarkable as the algorithm extract answers of the proper type even if they are incorrect. We were aware that such effect could appear as the amount of questions in each of the question-answers classes were unevenly distributed in QA@CLEF 2003 question set. There were specially few questions that we could classify as definitions wich diminish the amount of training data available and therefore the accuracy of the probabilities. Another fact noteworthy in our HMM algorithm is that is somewhat greedy when trying to identify answer and in that case shows some preference for words appearing near anchor terms . Finally, the algorithm is actually doing two jobs at a time as it identifies answers and, in some way, analyses entities according to patterns that were present in training answers of the same kind.</p><p>Another important source of errors in our system is induced by the document retrieval process and the way we posed questions and score documents. In our system all the terms that have been assigned a semantic tag will be used in queries and as anchors. Some terms are not very discriminating, specially if they are considered against proper names, and therefore lot of noisy documents are retrieved. As well, the simple scoring schema that we used for sentences (one token-one point) contributes to mask some of the useful fragments.</p><p>Errors are also generated by the question classification step as it is unable to handle some of the new surface forms introduced in this year question set. For that reason a catch all classification was also defined and used as a ragbag, but results were not expected to be good for that class.</p><p>The evaluation also provides results for the percentage of NIL answers that we have returned. In our case we returned 74 NIL answers and only 11 of them were correct (14.86%). NIL values were returned when the process did not provided any answer and their high value is due to the chaining of the other problems mentioned above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Future work</head><p>Several lines for further research are open along with the deficiencies that we have detected in our system. We are also intending to extend the same approach to other languages both in the source and the target language. Some attempts to address different language for the question have already been done by translating questions, but the low quality of the translations would have obligued us to extend the set of question patterns or to develop correction mechanisms. These problems as well as the errors caused by new questions not addressed in our schema are claiming for a more robust approach to question classification and analysis.</p><p>One of the most straightful improvements we should introcuce in miraQA is a module for specific answer type recognition that address Named Entity Recognition but also other common answer types as dates, time, amounts, etc. With such extension the answer extraction task would be reduce to identify proper units based on context. We believe that with this improvement the method would be able to reduced the inexact ratio and address short definitional questions.</p><p>Results show that for the answer extraction mechanism to work properly, a thorough training is needed. We are already carrying out experiments to determine the amount of training data that would be needed in order to improve recognition results. We would likely need to acquire or generate larger question-answer corpus. Several improvements in the learning and recognition machine would be definetily beneficial and therefore several extensions to Hidden Markov Model and other statistical finite state approaches are under study, as well as more effective methods for learning the structure and parameters of these machines.</p><p>Besides the previous improvements a more careful look at the interfaces and dependencies between the different subsystems is also needed. In that sense, the main work involves developing better strategies to query de document database and retrieve the most meaningful passages. We also need to estimate more precisely the contribution of any of the modules and elaborate a method to combine this information in a succesful measure of answer confidence as this would greatly increase the acceptance of QA systems by the final user.</p><p>Finally, as we have stated from the beginning, we believe that an statistical approach could be practical from a software engineering approach and would allow the rapid development of baseline QA systems for different languages and domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,246.40,429.19,124.72,11.07"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: miraQA arquitecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,234.10,377.39,144.92,11.07"><head>Figure 3</head><label>3</label><figDesc>Figure 3: HMM Training subsystem</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,82.90,215.31,459.67,207.45"><head>Table 1</head><label>1</label><figDesc>Question answer classes</figDesc><table coords="2,82.90,215.31,459.67,207.45"><row><cell></cell><cell cols="3">Question Analysis</cell><cell></cell><cell></cell><cell>Document</cell></row><row><cell>Question</cell><cell>POS +Parsing</cell><cell cols="2">Question classifier</cell><cell cols="2">Qclass Term: SemTag Term: SemTag ....</cell><cell>retrieval IR engine</cell><cell>EFE94/95</cell></row><row><cell></cell><cell></cell><cell cols="2">Answer Extraction</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Answer</cell><cell>Answer ranking</cell><cell>Answer Recog.</cell><cell>Anchor searching</cell><cell>POS +Parsing</cell><cell>Sen tence</cell><cell>Sentence extractor</cell></row><row><cell></cell><cell></cell><cell>Qclass</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Answer</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>model</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,99.30,76.61,407.66,670.36"><head></head><label></label><figDesc>3.  Ranking. Candidate answers are conflated when they present small differences in the outer form due to stopwords, for example. Finally, the candidate answers are ranked attending to a</figDesc><table coords="3,119.00,76.61,387.96,161.68"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>S</cell><cell></cell><cell></cell></row><row><cell>P</cell><cell>NP</cell><cell>VP</cell><cell></cell><cell cols="4">##REL#(NP) ##COUNTRY##(PP) P</cell><cell>states</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">espec grup-nom</cell><cell cols="2">prep grup-nom</cell></row><row><cell cols="2">Fia pt</cell><cell cols="2">vsip da</cell><cell>nc</cell><cell>sps</cell><cell>np</cell><cell>Fit</cell><cell>symbols</cell></row><row><cell cols="2">¿ Cuál</cell><cell>es</cell><cell cols="2">la capital</cell><cell cols="2">de Croacia</cell><cell>?</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work has been partially supported by the projects <rs type="funder">OmniPaper (European Union</rs>, <rs type="programName">5th Framework Programme for Research and Technological Development</rs>, <rs type="grantNumber">IST-2001-32174</rs>) and <rs type="funder">MIRACLE (Regional Government of Madrid, Regional Plan for Research</rs>, <rs type="grantNumber">07T/0055/2003</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Tx66VSB">
					<idno type="grant-number">IST-2001-32174</idno>
					<orgName type="program" subtype="full">5th Framework Programme for Research and Technological Development</orgName>
				</org>
				<org type="funding" xml:id="_gf2T4Ac">
					<idno type="grant-number">07T/0055/2003</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,74.65,226.57,449.53,9.00;6,85.10,238.17,150.90,9.00" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,236.50,226.57,247.30,9.00">Answer Extraction Applied Natural Language Processing</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Abney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,493.77,226.57,30.41,9.00;6,85.10,238.17,121.38,9.00">ANLP): Proceedings of the Conference</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.65,249.87,449.85,9.00;6,85.10,261.77,439.40,9.00;6,85.10,273.37,438.90,9.00;6,85.10,285.07,89.40,9.00" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,198.80,261.77,321.32,9.00">Turmo Morphosyntactic Analysis and Parsing of Unrestricted Spanish Text</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Atserias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carmona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Castellón</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Cervell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Civit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Martí</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Padró</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Placer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Taulé</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,85.10,273.37,434.26,9.00">Proceedings of the 1st International Conference on Language Resources and Evaluation (LREC&apos;98)</title>
		<meeting>the 1st International Conference on Language Resources and Evaluation (LREC&apos;98)<address><addrLine>Granada, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.65,296.97,376.35,9.00" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="6,246.00,296.97,129.30,9.00">Modern Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Addison Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.65,308.87,448.73,9.00;6,85.10,320.47,46.90,9.00;6,70.90,332.17,185.69,9.00" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,300.30,308.87,152.46,9.00">Data-Intensive Question Answering</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Banco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<ptr target="http;//www.daedalus.es" />
	</analytic>
	<monogr>
		<title level="m" coord="6,461.60,308.87,61.78,9.00;6,85.10,320.47,46.90,9.00;6,70.90,332.17,3.75,9.00">Proceedings of TREC 2001 5</title>
		<meeting>TREC 2001 5</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.65,344.07,338.25,9.00" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="6,211.60,344.07,139.34,9.00">Speech and Language Processing</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.65,355.97,423.74,9.00" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="6,211.30,355.97,234.28,9.00">Foundations of Statistical Natural Language Processing</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.65,365.79,449.29,11.08;6,85.10,379.77,261.60,9.00" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="6,507.30,367.87,16.64,9.00;6,85.10,379.77,239.01,9.00">The Multiple Language Question Answering Track at CLEF</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Romagnoli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vallin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Peinado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Verdejo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.65,391.67,449.69,9.00;6,85.10,403.57,201.10,9.00;6,70.90,413.09,176.78,11.08" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,175.60,391.67,198.87,9.00">Recuperando información de alta precisión</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Vicedo</surname></persName>
		</author>
		<ptr target="http://www.xapian.org/" />
	</analytic>
	<monogr>
		<title level="s" coord="6,385.20,391.67,139.14,9.00;6,85.10,403.57,45.00,9.00">Los sistemas de Búsqueda de Respuestas</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2003">2003</date>
			<pubPlace>Universidad de Alicante</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Phd Thesis</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
