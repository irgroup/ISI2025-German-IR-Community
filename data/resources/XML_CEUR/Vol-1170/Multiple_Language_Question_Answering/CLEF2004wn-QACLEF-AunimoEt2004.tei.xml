<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,117.72,148.73,367.41,15.51;1,209.04,170.69,184.88,15.51">Cross-Language Question Answering at the University of Helsinki</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,189.72,204.07,49.85,9.96"><forename type="first">Lili</forename><surname>Aunimo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Helsinki</orgName>
								<orgName type="institution" key="instit2">UNIVERSITY OF HELSINKI</orgName>
								<address>
									<postBox>P.O. Box 68</postBox>
									<postCode>FIN-00014</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,248.26,204.07,73.12,9.96"><forename type="first">Reeta</forename><surname>Kuuskoski</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Helsinki</orgName>
								<orgName type="institution" key="instit2">UNIVERSITY OF HELSINKI</orgName>
								<address>
									<postBox>P.O. Box 68</postBox>
									<postCode>FIN-00014</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,343.80,204.07,69.41,9.96"><forename type="first">Juha</forename><surname>Makkonen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Helsinki</orgName>
								<orgName type="institution" key="instit2">UNIVERSITY OF HELSINKI</orgName>
								<address>
									<postBox>P.O. Box 68</postBox>
									<postCode>FIN-00014</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,117.72,148.73,367.41,15.51;1,209.04,170.69,184.88,15.51">Cross-Language Question Answering at the University of Helsinki</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E4B7B8150D522C9B796CD743EFB5C33C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Tikka is a cross-language question answering system developed at the University of Helsinki for the purposes of the QA@CLEF 2004 evaluation campaign, Tikka was configured to answer Finnish questions using a text corpus in English, but it is designed so that it can be configured to work with any other languages as well. Tikka is the first general domain question answering system ever reported to have used Finnish. The question type classifier, the translator, the answer extractor and the answer scorer are the components of Tikka that are especially developed for question answering.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Question answering (QA) is a task where the information need of the user is formulated as a natural language question and where the answer is given in natural language as well. In general, the length of the answer varies from one word to a couple of sentences, depending on the question. Also those situations where the system is not able to provide an answer should be detected. QA systems can be designed to work on a specific domain only (e.g. an aid for a company help desk <ref type="bibr" coords="1,90.00,491.23,10.55,9.96" target="#b0">[1,</ref><ref type="bibr" coords="1,103.55,491.23,7.13,9.96" target="#b1">2]</ref>), or they can be general purpose systems (e.g. TREC <ref type="foot" coords="1,345.60,490.07,3.95,5.05" target="#foot_0">1</ref> and CLEF QA Tracks<ref type="foot" coords="1,451.68,490.07,3.95,5.05" target="#foot_1">2</ref> ). In general, question answering systems use unstructured text documents as their database, but, in addition, they can use lists of FAQs (Frequently Asked Questions) and structured databases. In general open domain QA systems, the Web can be used as a source of information. Much of the research on QA systems is concentrated in building systems for the CLEF and TREC evaluation campaigns. In these campaigns, the main database is newspaper text.</p><p>Cross-language question answering means that the question is expressed in another language than that in which the documents from which the answer is extracted are written. In this case, the user can use one language to search information from documents written in one or more other languages. This is useful, because it would be tiresome to write the question over and over again in many languages, and also because many users have a good passive knowledge of several languages, but their active knowledge is more restricted <ref type="bibr" coords="1,338.05,622.63,9.89,9.96" target="#b2">[3]</ref>. In our system, the questions can be expressed in Finnish and the document collection is in English. The system could be extended to handle questions and documents in other languages using the same methodology as presented in this paper. For the simplicity of presentation, we expect from here onwards that the questions and documents are in only one language, and that these languages are different. Cross-language QA is usually implemented either by first applying machine translation to the question and then passing it on to a monolingual QA system or by integrating cross-language processing into the QA system. Our approach is the latter one, because there is no reliable off-the-shelf machine translation software for Finnish. In addition, we expect to improve our results by using the original question as the basis of processing for as long as possible, because when translation is performed, the information content of the question is almost always altered.</p><p>In the following chapter we will descibe the overall architecture of our QA system. After that each of the main components of the system are described in detail. Section 3 descibes the processing of questions, that is, question classification and translation. In Section 4, the information retrieval component of our system is detailed. Answer processing, which consists of answer extraction pattern creation and instantiation and of answer selection and scoring, is described in Section 5. Section 6 is about evaluation and it presents our official results at QA@CLEF 2004. It also contains some discussion on the effects of translation on the overall performance of a QA system. Finally, Section 7 concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Architecture</head><p>The name of our QA system is Tikka (Woodpecker). It has three modules: Question Processor, Information retrieval (IR) Engine and Answer Processor. A system architecture is shown in Fig. <ref type="figure" coords="2,504.82,285.79,3.90,9.96">1</ref>. The figure also shows the configuration of Tikka for Finnish-English QA and for the document database used in the QA@CLEF evaluation initiative. The Question and Answer Processors are the modules which are especially developed for QA. The IR Engine, which is described in more detail in section 4, is a standard search engine. The Question Processor, which is described in section 3, first produces a syntactic parse of the question, then it classifies the question and finally it translates the relevant terms of the question. The Answer Processor first instantiates the answer extraction pattern prototypes with the translated words of the question. Then it applies the patterns to the documents retrieved by the IR Engine and finally it selects the best answer among the candidates extracted and gives it a confidence value. The Answer Processing module is described in detail in section 5.</p><p>When Tikka was used for the Finnish-English experiments of QA@CLEF, its document database consisted of 670 megabytes of newspaper text (The Glasgow Herald from 1995 and Los Angeles Times from 1994). Other external knowledge sources that the system used were the MOT dictionary software from Kielikone Ltd. <ref type="foot" coords="2,243.12,452.03,3.95,5.05" target="#foot_2">3</ref> , the functional dependency grammar parser from Connexor Ltd. <ref type="foot" coords="2,111.24,464.03,3.95,5.05" target="#foot_3">4</ref> and a Country and Capital Translation Database extracted from the web site of Statistics Finland<ref type="foot" coords="2,126.84,476.03,3.95,5.05" target="#foot_4">5</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Question Processing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Question Classifier for Finnish</head><p>The question processing commences by determination of the question type. The possible types were defined already in Multisix corpus <ref type="bibr" coords="2,260.16,564.19,9.89,9.96" target="#b4">[5]</ref>: date, location, measure, object, organization, other, and person. In addition, CLEF 2004 introduced new two types: manner (answering how-questions), abstraction and definition. The last type was tagged in the evaluation corpus, and thus the number of types to be recognized was ten.</p><p>Obviously, the question type can often be determined just by looking at the question word. However, in Finnish this is not always a straight-forward task as the language is morphologically rich. Typically, instead of prepositions there are agglutinated morphemes denoting the inflected cases, and within a noun phrase, for example, the words comply to congruity, i.e., attributes follow the case of the head word. As a simple example, consider the following uses of 'who' (kuka): There are 15 cases for each noun, adjective, pronoun and numeral in singular, and another 15 in plural. Furthermore, many morphemes produce changes also in the word body, and thus merely stripping morphemes at the end of the word is often of little avail. Without a morphological analysis it would be very difficult to take any further steps, because words are seldom used in their baseforms. We employ Connexor's functional dependency parser <ref type="bibr" coords="3,397.54,557.11,10.55,9.96" target="#b3">[4]</ref>  From this we see that, for instance, the pronoun minä is the essive of mikä (what) and that it is an attribute of the nominal head vuosi (year). The time-related questions in the test corpus typically fell into one of three categories: a general 'when' (milloin, koska), a specific interval, e.g., 'what year|month|time' (minä vuonna?, missä kuussa?, mihin aikaan? ) and a duration, 'how long' (kuinka kauan, kauanko, miten kauan, kuinka pitkän aikaa ). The first two are date-questions and the last a measure-question.</p><p>Likewise, many measure-questions are somewhat straight-forward to recognize. The question is scanned for occurrence of quantity-related question words ( e.g., kuinka|miten moni|kauan|paljon, montako|moniko|paljonko|kauanko). Then there are 'what-is'-questions, such as Mikä on Suomen väkiluku? (What is the population of Finland?) The classification of the question relies in identifying the complement (population) as a measure-related word. Same technique is used with person, location, and organization related question: the type is based on classifying the object or the complement. Sometimes verbs are a helpful indicators of the question type.</p><p>The question classified in other-type, if the complement is a verb or if the complement or the object does not relate to person, location or organization. The manner-questions typically start with either miten|kuinka (how) or millä tavoin|tavalla|keinoin (in what manner|way). It has been difficult to identify object-questions as they vary considerably. Hence, we regard them as other-type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Translator</head><p>Once the question has been classified, it is passed on to the Translator. It decides which of the words are translated, how to deal with proper names, homonyms and polysemous words and with words that have no translation in the dictionary. The Translator also decides which words are used in the query that is given to the IR Engine, and in answer extraction pattern prototype instantiation. For these decisions, it uses the syntactic parse tree of the question.</p><p>Once a question and its type is received by the Translator, it checks for country and capital names in the Country and Capital Translation Database. It contains 244 country and capital names in Finnish and their translations into English. The country and capital information is up to date as a new database is fetched from the web pages of Statistics Finland every once in a while. The version that we used in the CLEF evaluation exercise dates from 16.4.2004. This caused some problems, because the World has changed since 1994 and 1995 from where the CLEF newspaper text database dates. For example, two CLEF questions were about Yugoslavia, which our Country and Capital Translation Database naturally did not contain.</p><p>If the question contains a name that is in the database, it is given a translation and taken off from the list of words that will be passed on to the dictionary software. It is crucial that the proper names have been transformed into their baseforms before their existence in the database is checked because the database naturally does not contain any inflected proper names. For example, among the 34 country and capital names occurring in this year's questions, only 2 were uninflected.</p><p>After the Country and Capital Translation Database checking routine the translator determines which words are passed on to the dictionary software. All nouns are translated. If no translation is found, and the noun is a compound word, it is split into two parts both of which are used in the search from the dictionary. If there are more than two parts in the compound, then the last part forms the first search word and all the rest of the parts form the second search word. This is sensible, because quite often the preceding parts together are a modifier of the last part. For example (compound boundaries are marked with #): In kori#pallo#joukkue (basketball team) kori#pallo (basketball) modifies joukkue (team). This very coarse heuristic also has many counterexamples. One of them is kulttuuri#pää#kaupunki (Capital of Culture) where kulttuuri (culture) modifies pää#kaupunki (capital). In those cases where the noun is a compound word containing at least three parts and where the first part begins with a capital and ends with a hyphen, we split the word into dictionary search words from the hyphen, because the first part is most probably a proper noun and an uninflected modifier of the latter part and the latter part is the main part of the compound and it is inflected. For example in Andrew-#pyörre#myrsky (Hurricane Andrew) Andrew is a modifier for pyörre#myrsky (Hurricane). The proper noun could also contain several parts, for example La# Scala -#ooppera#talo (La Scala opera house), where La# Scala modifies ooppera#talo (opera house).</p><p>In addition to nouns, all adjectives that are attributes to nouns are translated. For example, in How many Japanese students were there in the United States in 1990?, Japanese is translated because it is a modifier of students.</p><p>If a word has no translation in the dictionary, and it looks like a proper name (begins with a capital and is not the first word of the question), its case is checked. If it is not nominative, but one of the other fourteen cases in which a noun can be, the baseform is passed on. Otherwise, the original word in the question is passed on. This is because in the nominative case, no inflection is added to the proper name, while in the other cases, a suffix is added to the end of the word. In order to be able to use an inflected proper name as an English query term, we have to find its baseform.</p><p>The main reason for only translating nouns and their attributes is that the verbs used in the questions tend to be highly polysemious and they tend to have one or more homonymes. For example, in the case of this year's question number 40: Who directed "Braveheart"?, in Finnish Kuka ohjasi elokuvan "Braveheart -Taipumaton"? the verb ohjata (to direct) has 22 different senses in English, and only the seventh is the correct sense. However, the problem of polysemious words and homonyms also exists for nouns. For example, in this years question set, question 192 contained the word laivasto (navy), for which our dictionary software gave 3 different senses and 4 different translations (fleet, naval, forces and navy). If the different translations represent the same sense, they are often synonyms or regional variants. An example of synonyms: the translation candidates for laulaja are: singer, songster, vocalist which all represent the same sense according to our dictionary software. An example of regional variants: the translation candidates for maanalainen are: metro, tube (br; the tube), underground (br; the underground), subway (yl am) where br means British English and am means American English.</p><p>There are two main problems that could be studied further in the Translator. First, we should investigate whether query terms and answer extraction pattern prototype instantiation terms should be different. At the moment, the same terms are used for both.</p><p>The second area for further investigations is that of finding the correct translation or translations for a word in a given question. At the moment we take at most the two first translations and hope that the correct one is among these. Usually it is, because in general, the dictionary software lists the translation alternatives in the order of their frequency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Information Retrieval</head><p>After the query terms have been selected, tey are given to the information retrieval engine. We used Managing Gigabytes (MG)<ref type="foot" coords="5,231.00,475.79,3.95,5.05" target="#foot_5">6</ref> for IR task in Tikka. MG is an open source text indexing and retrieval engine developed as a joint venture of multiple Australian universities.</p><p>Prior to indexing, the documents were split so that each document was in its own file. The more fine-grained segmentation was not applied, since some of the answers to the training questions were not within one sentence, or even one paragraph. The files were then fed to MG for indexing. The contents of the documents were not otherwise preprocessed, although it might have enhanced the results, since the special characters caused some problems in retrieval. For instance, changing the dollar signs to corresponding strings might have been worthwhile.</p><p>The maximum number of retrieved documents was limited to one hundred, since we did not want the document sets to be processed in Answer Selection phase grow too large. In our experiments we noticed that if found at all, the document containing the correct answer was generally within the first 100. By default, MG was run in boolean query mode.</p><p>From our point of view, MG has some drawbacks. Firstly, it does not support phrase search or proximity constraints. This made it difficult to search for compound terms. It would also have been nice to be able to weight the terms according to their importance. For instance, one would have wanted to tell that the proper names occurring in the question are obligatory and they must be present in the retrieved documents, but other terms are less important. Now each of the terms were treated individually, and given the same relevance.</p><p>Especially with the questions that included proper names the boolean mode proved to work better than the ranked query. Since the query terms could not be weighted, the ranked query could sometimes give lots of irrelevant results. In the boolean mode at least the presence of the most fundamental terms can be required. Sometimes the query conditions were too strict, however, and the result set became empty, in which case the mode was switched. This might cause the amount of the result document set to grow so large that the document with the correct answer could be left out of the set of 100 best and , hence, not be processed at all.</p><p>According to our experiments with the training data set, it seemed worthwhile to include also the corresponding adjective to the question as an alternative in case there was a name of a nation in the question. This is because the translations are in some situations more natural if the part of speech is altered. For example, question 29 in the test set was in English What is the official German airline called? The corresponding Finnish question is Mikä on Saksan virallisen lentoyhtiön nimi? Here German is an adjective, but Saksan is the genetive form of the noun Saksa (Germany).</p><p>Another motivation for adding the corresponding adjectives/nouns is the fact that even within one natural language, both of these expressions occur in sentences that have the same meaning. For instance, question 90 in the training set was How many people in U.S. do not have health insurance?, where U.S. is a noun. The correct answer to it was 37 million, which existed in the following snippet: ... the existing system, which leaves 37 million Americans without health insurance and ... There the triggering term is American, which is an adjective.</p><p>The expansion of the query terms with synonyms would probably have improven the results. The disambiguity of the query terms, especially in bilingual question answering task, enlargens the expansion term candidate set notably, however. Some proper names could have quite easily be expanded, though, such as United States, which might have been worth expanding with terms US, America and American, as discussed above.</p><p>The most important terms in the query seemed to be the proper nouns, as one might expect. After that came the common nouns, possibly expanded with their synonyms. Next to the common nouns were verbs, outside of some verbs that were so common that they didn't actually mean anything (such as do, be). The least important group of words were generally the adjectives, though there were some questions in which the adjectives were very significant, for instance in question 79: What is the highest active volcano in Europe?. This has been taken into account in query term selection, as was described in section 3.2.</p><p>The seach results are passed onward to Answer Selection module for the execution of the next phrase, the answer extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Answer Processing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Answer Extraction Patterns</head><p>Answer extraction pattern instantiation is the first step in Answer processing. This is done by creating instances of pattern prototypes. Each question type has a set of pattern prototypes that have been induced from the 1994 L.A. Times and the 1995 Glasgow Herald using the Multisix Corpus <ref type="bibr" coords="6,125.86,569.11,9.89,9.96" target="#b4">[5]</ref>. The pattern prototypes have slots where translated words from the question are inserted in order to form pattern instances.</p><p>Tikka contains pattern prototypes for six question types. They are: date, definition, location, measure, person and other. Based on the question types in the Multisix Corpus <ref type="bibr" coords="6,455.83,604.99,9.98,9.96" target="#b4">[5]</ref>, we could have developed pattern prototypes also for the classes object and organization. However, we picked the most common categories for pattern prototype development and left the rest for future development. Other is a class where we classify all those questions that do not belong to the other five classes. In addition, the CLEF-2004 Question Answering Track Guidelines 7 contained classes abstraction and manner, but we did not develop pattern prototypes for these since we had no training material.</p><p>Below are 3 examples of the 11 instantiated location patterns for question 116 Where is the Reichstag? </p><formula xml:id="formula_0" coords="6,90.01,721.26,307.81,14.35">[Ii]n (([A-Z][a-z]+ ){1,5}[A-Z][a-z]+), a [a-z]* [a-z]+,[^a-zA-Z0-9]+Reichstag[,\.] at ([A-Z][a-z]+,? ([A-Z][a-z]+)?), [^\.\?\!0-9\",]*</formula><formula xml:id="formula_1" coords="7,286.29,230.34,185.50,6.43">[A-Z][a-z\']+,? ){0,3}[A-Z][a-z\']+)[\.,][^0-9]{2}</formula><p>The prototypes of these patterns are identical to the instantiated patterns, except that the word Reichstag is replaced with a wildcard denoting any noun from the question. The third pattern is the one that matched both of the answers that were found. Both of the answers are Berlin, and here is their context:</p><p>Two matches for question 116: WORKERS lower a giant panel of cloth over the entrance to the Reichstag in Berlin, helping Hungarian artist Christo to fulful a dream of 24 years. Reichstag in Berlin, he He will use 160 assistants to wrap the Reichstag with 90,000 square yards of a silver propylene fabric, chosen "because it fits with the building, the heaven and light in Berlin." Christo Reichstag with 90,000 square yards of a silver propylene fabric, chosen "because it fits with the building, the heaven and light in Berlin." <ref type="bibr" coords="7,90.00,394.03,35.13,9.96">Table 2:</ref> The two text snippets that were matched for question 116. Above is a bigger window of text and below is the exact text snippet that was matched by the pattern.</p><p>The answer pattern prototypes consist of regular expressions and of slots for proper names and other words that have been picked from the question. The answer pattern prototypes do not contain any syntactic or morphological information at the moment. Table <ref type="table" coords="7,419.36,455.35,5.03,9.96" target="#tab_2">1</ref> lists all the pattern classes and the number of prototype patterns that each class contains. In future research, it would be interesting to incorporate at least part of speech information into the patterns. Examples of pattern instances that are derived from the same location pattern prototype: the city of ([^,\.\?\!0-9]+), Mike Kelley[^\.\?\!0-9]* the town of ([^,\.\?\!0-9]+), Mike Kelley[^\.\?\!0-9]* In the above example, the word kaupunki has two translations, city and town, and the pattern prototype is expanded with both.</p><p>Another example:</p><formula xml:id="formula_2" coords="7,94.20,584.37,303.79,7.35">PROPER NAME[^,\.\?\!0-9]* TITLE,? [^A-Z]*(([A-Z][a-z]+[ -])*[A-Z][a-z]+)</formula><p>In the above person pattern prototype the slots for PROPER NAME and TITLE are filled with words from the question. For example, in the question 2 from 2003, Kuka on YK:n pääsihteeri?, Who is the head of the United Nations?, the slot for PROPER NAME is filled by UN, United Nations and UN (United Nations). The slot for TITLE is filled by Sectretary General and secretarygeneral. When all these instantiations are combined, we end up with 6 different pattern instances. The different variations for the slots except for the combination UN (United Nations) are retrieved from the dictionary. For all acronyms that have the longer form listed in the dictionary, the system performs the same type of expansion as for UN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Answer Selection and Scoring</head><p>Answer selection is based on frequency, which means simply that among the answer candidates, the answer that appears most often is selected. If there are several answer candidates with the same frequency, the one appearing first in the results retrieved by the IR Engine, is selected. This is a reasonable approach, because the IR Engine search results are ranked in the order of relevance.</p><p>Confidence measure generation is a function of both the total number of candidates retrieved and of the frequency of the selected candidate. This function is illustrated as an area plot in Figure <ref type="figure" coords="8,122.73,159.31,3.90,9.96" target="#fig_0">2</ref>. In Tikka, the frequencies and numbers of different candidates are discrete and not continuous as shown in the figure. The confidence score is 1, if the number of different candidates is a number between 1 and 5, or if the number of different candidates is a number between 6 and 14 and the frequency of the candidate is greater than 1 (the area marked with tiles in figure <ref type="figure" coords="8,501.02,195.19,3.88,9.96" target="#fig_0">2</ref>). The confidence score is 0.5 if the number of different candidates is between 6 and 10 and the frequency is 1 (the area marked with diagonal lines in figure <ref type="figure" coords="8,361.95,219.07,3.88,9.96" target="#fig_0">2</ref>). The confidence score is 0.25 if the number of different candidates is between 11 and 14 and the frequency is 1, or if the number of different candidates is over 14 (the area marked blank in figure <ref type="figure" coords="8,382.61,242.95,3.88,9.96" target="#fig_0">2</ref>). All those answers that we detected as not having an answer in the text database (answers of type NIL) had a confidence score of 0. Detecting the degree of confidence for answers of type NIL is a goal for future research.  The Table <ref type="table" coords="8,154.28,445.15,5.03,9.96">3</ref> lists the number of occurrences of each confidence measure and the number of correct answers in these classes. Only answers that are not NIL are considered. As can be seen from the table 3, the confidence function should have been more strict, i.e. the score 1 should have been given to fewer answers. However, the confidence function depends heavily on the data and questions at hand and on how well the answer extraction patterns match to that data. We trained Tikka with questions and answers from QA@CLEF from 2003, and it seems that the answer extraction pattern prototypes were too specific to those answers. With the 2003 questions we got 132 NIL answers, but with this years material, the number of NIL answers was 159. The distribution of confidence scores from 2003 is shown in table <ref type="table" coords="8,417.81,669.79,3.90,9.96">3</ref>.</p><formula xml:id="formula_3" coords="8,233.40,335.50,32.20,37.00">¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¡ ¢ ¡¢</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation</head><p>6.1 Our Results at QA@CLEF Our results in the 2004 Finnish-English QA@CLEF are shown in figure <ref type="figure" coords="8,404.65,744.91,3.90,9.96">4</ref>. We had 21 right answers, among which 20 were factoid questions and 1 was a definition question. One of our answers was inexact and there were no unsupported answers in our answer set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Inter-Translator Agreement</head><p>The questions for Finnish-English QA were translated from English. The assessor of the evaluation campaign compared the English questions against the results given by Tikka. However, the translation process is not straightforward, because for most questions, there seem to be as many translations as there are translators. In addition, not all questions are sensible when translated. For example, the question 86 (What does a luthier make?) became pointless in Finnish, because our word for luthier (soitinrakentaja) tells what a luthier does. Another example of the influence of translation on the questions is question 85 What did the artist Christo wrap up?. This can be translated in two ways which have a completely different meaning due to the ambiguity of the verb to wrap up. To wrap up can be translated as denoting concrete wrapping up, which was the correct meaning according to the correct answer, which is that The artist Chisto wrapped up the Reichstag in silver fabric tied with blue rope. The other meaning of to wrap up is an abstract one, and it means finishing something. The translations 1 and 2 translated to wrap up with its concrete sense paketoida, but translation 2 has the abstract sense saattaa päätökseen. We did two more translations of the English questions by translators who had not seen the official translation in order to measure the difficulty of the translation task and the inter-translator agreement rate. The amount of inter-translator agreement is illustrated in Table <ref type="table" coords="9,378.61,467.83,3.90,9.96" target="#tab_6">5</ref>. Translation 1 is the official translation where the errors have been corrected<ref type="foot" coords="9,304.56,478.79,3.95,5.05" target="#foot_6">8</ref> .  The most common translator disagreement types are lexicografic disagreement, word order disagreement and disagreement in the use of conventions. Lexicographic disagreement means a different choice of words where the words are synonyms or semantically very closely related. For example: manufacture translated as valmistaa or tuottaa. Word order disagreement means that the words in the question are in a different order. For example: Missä on Hyde Park? (Where is Hyde Park?) and Missä Hyde Park on? (Where Hyde Park is?). Disagreement on the use of conventions means that there are many, equally correct, different conventions on how to express a concept. For example, there are several conventions for expressing names of movies that have originally appeared in another language than Finnish. For example, the question 175 is about the movie Nikita. Nikita was translated into Finnish in three different ways: Nikita, elokuva "Tyttö nimeltä Nikita" and Nikita (La Femme Nikita). The translation of names of movies is problematic because some movies have an official translation into Finnish and some don't. In the case of Nikita, there were two official translations, Nikita and Tyttö nimeltä Nikita. Proper names are often typed, elokuva "Tyttö nimeltä Nikita" (movie "Nikita"), because then the type (movie) gets inflected and there is no need to inflect the proper name. One convention in expressing movie names is that of first writing the official translation in Finnish and then adding the name of the original movie in parenthesis after it, as in Nikita (La Femme Nikita). It will be interesting to compare the QA results with translations 1, 2 and 3 once we have the correct answers for this year's questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Future Work</head><p>To the best of our knowledge, the work presented in this paper is the first time cross-language QA has been done using Finnish as a source language. Altogether, there has been very little work on any type of QA for Finnish. Keeping this in mind, it was interesting to get the system up and running and to observe that it could answer 10,88 % of the questions presented to it correctly.</p><p>Due to the very different nature of Finnish in comparison to any of the other languages participating in the QA@CLEF, special attention has been paid to question translation and to the effects of the translation phase to the overall performance of the system. This is also a subfield on which we plan to focus our attention in the future.</p><p>Another interesting subfield is that of answer extraction patterns. We plan to study carefully which patterns matched well and which didn't and to find out the reasons for this. We are also planning to investigate the use of POS tags and possibly surface syntactic tas in the answer extraction patterns. The results obtained in this evaluation showed that by developing further the question and answer processing modules, as well as by tuning the IR engine more carefully, the performance of Tikka is very likely to improve.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="8,90.00,411.19,422.62,9.96;8,90.00,423.07,135.80,9.96"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Area plot of the confidence score of Tikka as a function of candidate frequency and total number of different candidates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,150.48,479.71,47.61,9.96;8,230.49,479.71,89.21,9.96;8,352.10,479.71,100.25,9.96;8,210.00,492.07,52.86,9.96;8,291.06,492.07,32.84,9.96;8,352.10,492.07,98.90,9.96;8,171.72,504.43,5.03,9.96;8,231.35,504.43,10.06,9.96;8,274.65,504.43,65.57,9.96;8,373.46,504.43,10.06,9.96;8,429.60,504.43,10.06,9.96;8,167.88,516.31,12.82,9.96;8,233.86,516.31,5.03,9.96;8,304.89,516.31,5.03,9.96;8,376.04,516.31,5.03,9.96;8,432.20,516.31,5.03,9.96;8,165.36,528.31,17.85,9.96;8,233.85,528.31,5.03,9.96;8,304.88,528.31,5.03,9.96;8,373.52,528.31,10.06,9.96;8,432.18,528.31,5.03,9.96;8,171.72,540.31,5.03,9.96;8,233.87,540.31,5.03,9.96;8,304.90,540.31,5.03,9.96;8,376.05,540.31,5.03,9.96;8,432.20,540.31,5.03,9.96;8,90.00,562.15,422.95,9.96;8,90.00,574.15,179.48,9.96"><head>Table 3 :</head><label>3</label><figDesc>Number of occurrences of different confidence measures and number of correct answers. No figures are given for the NIL-answers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,107.88,110.17,387.06,388.34"><head>Question in Finnish Answer in English and its confidence or no answer</head><label></label><figDesc>Architecture of Tikka and its configuration in the Finnish-English QA@CLEF.</figDesc><table coords="3,107.88,128.73,363.45,369.78"><row><cell></cell><cell></cell><cell></cell><cell>Finnish Parser</cell></row><row><cell></cell><cell>Finnish -Dictionary English</cell><cell>Question Processing</cell><cell>Translator</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Question Classifier</cell></row><row><cell></cell><cell></cell><cell></cell><cell>for Finnish</cell></row><row><cell></cell><cell></cell><cell>English</cell></row><row><cell></cell><cell>Country and Capital</cell><cell>Query Terms</cell></row><row><cell></cell><cell>Translation</cell><cell></cell></row><row><cell></cell><cell>Database</cell><cell></cell></row><row><cell cols="2">670 MB</cell><cell>IR Engine</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Question</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Information</cell></row><row><cell>L.A. Times</cell><cell>Glasgow Herald</cell><cell>Relevant Documents</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Answer Extraction</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Pattern Instantiator</cell></row><row><cell></cell><cell></cell><cell></cell><cell>for English</cell></row><row><cell></cell><cell></cell><cell>Answer Processing</cell><cell>Pattern Matcher</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Answer Selector</cell></row><row><cell></cell><cell></cell><cell></cell><cell>and Scorer</cell></row><row><cell>Figure 1: English</cell><cell>Finnish</cell><cell></cell><cell>question word baseform + case</cell></row><row><cell>Who is that man?</cell><cell cols="2">Kuka on tuo mies?</cell><cell>kuka + nominative</cell></row><row><cell>Who do you mean?</cell><cell cols="2">Ketä tarkoitat?</cell><cell>kuka + partitive</cell></row><row><cell>Who is he with?</cell><cell cols="3">Kenen kanssa hän on? kuka + genetive</cell></row><row><cell cols="3">Who do you think he is? Keneksi häntä luulet?</cell><cell>kuka + translative</cell></row><row><cell>Who has it?</cell><cell cols="2">Kenellä se on?</cell><cell>kuka + adessive</cell></row><row><cell>Who do you trust?</cell><cell cols="2">Kehen luotat?</cell><cell>kuka + illative</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,90.00,557.11,422.66,94.76"><head></head><label></label><figDesc>for Finnish. Consider a sentence: Minä vuonna se alkoi? ('In what year did it start?'). It would be parsed as:</figDesc><table coords="3,152.16,588.40,298.41,63.47"><row><cell cols="2"># text</cell><cell cols="3">baseform dependency morphology</cell></row><row><cell>1</cell><cell>Minä</cell><cell>mikä</cell><cell>attr:&gt;2</cell><cell>&amp;A&gt; PRON SG ESS</cell></row><row><cell>2</cell><cell cols="2">vuonna vuosi</cell><cell>tmp:&gt;4</cell><cell>&amp;NH N SG ESS</cell></row><row><cell>3</cell><cell>se</cell><cell>se</cell><cell>subj:&gt;4</cell><cell>&amp;NH PRON SG NOM</cell></row><row><cell>4</cell><cell>alkoi</cell><cell>alkaa</cell><cell>main:&gt;0</cell><cell>&amp;+MV V ACT IND PAST SG3</cell></row><row><cell>5</cell><cell>?</cell><cell>?</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,286.19,729.18,33.45,6.43"><head>Table 1 :</head><label>1</label><figDesc>Question type classes and number of prototype patterns in each class.</figDesc><table coords="6,286.19,729.18,33.45,6.43"><row><cell>Reichstag</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,209.50,282.60,78.20,89.90"><head></head><label></label><figDesc>¡¢ ¡¢ ¡¢ ¡¢ ¡¢ ¡¢ ¢ ¡¢ ¡¢ ¡¢ ¡¢ ¡¢ ¡¢ ¡¢ ¢ ¡¢ ¡¢ ¡¢ ¡¢ ¡¢ ¡¢ ¡¢ ¢ ¡¢ ¡¢ ¡¢ ¡¢ ¡¢ ¡¢ ¡¢ ¢ ¡¢ ¡¢ ¡¢ ¡¢ ¡¢ ¡¢ ¡¢ ¢ ¡¢ ¡¢ ¡¢ ¡¢ ¡¢ ¡¢ ¡¢ ¢ ¡¢ ¡¢ ¡¢ ¡¢ ¡¢ ¡¢ ¡¢ ¢ ¡¢ ¡¢ ¡¢ ¡¢ ¡¢ ¡¢ ¡¢ ¢ ¡¢ ¡¢ ¡¢ ¡¢ ¡¢ ¡¢ ¡¢ ¢ ¡¢ ¡¢ ¡¢ ¡¢ ¡¢ ¡¢ ¡¢ ¢ ¡¢ ¡¢ ¡¢ ¡¢ ¡¢ ¡¢ ¡¢ ¢ ¡¢ ¡¢ ¡¢ ¡¢ ¡¢ ¡¢ ¡¢ ¢ ¡¢ ¡¢ ¡¢ ¡¢ ¡¢ ¡¢ ¡¢ £ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ £ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ £ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ £ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ £ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ £ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ £ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ £ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ £ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ £ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ £ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ £ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ £ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ £ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ £ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ £ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ £ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ £ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¡£ ¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤ ¡¤</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,176.76,110.04,249.13,86.71"><head>Table 4 :</head><label>4</label><figDesc>Our results at 2004 QA@CLEF.</figDesc><table coords="9,176.76,110.04,249.13,65.09"><row><cell></cell><cell>Absolute numbers</cell><cell>Percentage</cell></row><row><cell>Accuracy</cell><cell>21/193</cell><cell>10.88</cell></row><row><cell>Accuracy of factoid questions</cell><cell>20/173</cell><cell>11.56</cell></row><row><cell>Accuracy of definition questions</cell><cell>1/20</cell><cell>5</cell></row><row><cell>Number of N IL answers</cell><cell>159/193</cell><cell>82.38</cell></row><row><cell>Accuracy of N IL answers</cell><cell>17/159</cell><cell>10.69</cell></row><row><cell>Confidence-weighted score</cell><cell></cell><cell>4.65</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,184.92,558.43,233.09,9.96"><head>Table 5 :</head><label>5</label><figDesc>Number of indentically translated questions.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,105.24,737.04,124.31,7.97"><p>http://trec.nist.gov/data/qa.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,105.24,746.52,95.28,7.97"><p>http://clef-qa.itc.it/2004/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,105.24,679.44,103.12,7.97"><p>http://www.kielikone.fi/en/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,105.24,688.92,101.65,7.97"><p>http://www.connexor.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="2,105.24,698.40,156.37,7.97"><p>http://www.tilastokeskus.fi/index en.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="5,105.24,746.52,122.89,7.97"><p>http://www.mds.rmit.edu.au/mg</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6" coords="9,105.24,746.52,224.01,7.97"><p>http://clef-qa.itc.it/2004/down/clef04-test-FI-EN-correct.txt</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,105.47,436.15,407.34,9.96;10,105.48,448.15,407.18,9.96;10,105.48,460.15,406.82,9.96;10,105.48,472.03,147.78,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,473.96,436.15,38.84,9.96;10,105.48,448.15,389.68,9.96">Question answering system for incomplete and noisy data: Methods and measures for its evaluation</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Aunimo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Heinonen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kuuskoski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Makkonen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Petit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,105.48,460.15,402.33,9.96">Proceedings of the 25th European Conference on Information Retrieval Research (ECIR 2003)</title>
		<meeting>the 25th European Conference on Information Retrieval Research (ECIR 2003)<address><addrLine>Pisa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="193" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.47,491.95,407.23,9.96;10,105.48,503.95,406.98,9.96;10,105.48,515.83,22.88,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,303.84,491.95,168.97,9.96">Message classification in the call center</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Busemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Schmeier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">G</forename><surname>Arens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,493.65,491.95,19.06,9.96;10,105.48,503.95,280.56,9.96">Proceedings of 6th Applied Natural Language Processing Conference</title>
		<meeting>6th Applied Natural Language Processing Conference<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.47,535.75,407.10,9.96;10,105.48,547.75,407.05,9.96;10,105.48,559.75,372.26,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,161.82,535.75,247.89,9.96">Scenarios for interactive cross-language retrieval systems</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,433.58,535.75,78.99,9.96;10,105.48,547.75,407.05,9.96;10,105.48,559.75,238.52,9.96">Proceedings of the Workshop 1: Cross-Language Informaion Retrieval: A Research Roadmap Workshop held at the 25th Annual International ACM SIGIR Conference</title>
		<meeting>the Workshop 1: Cross-Language Informaion Retrieval: A Research Roadmap Workshop held at the 25th Annual International ACM SIGIR Conference<address><addrLine>Tampere, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-08">aug 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.47,579.67,407.25,9.96;10,105.48,591.55,279.82,9.96" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="10,253.75,579.67,141.71,9.96">A dependency parser for english</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Järvinen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Tapanainen</surname></persName>
		</author>
		<idno>TR-1</idno>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
		<respStmt>
			<orgName>Department of General Linguistics, University of Helsinki</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="10,105.47,611.47,407.23,9.96;10,105.48,623.47,407.29,9.96;10,105.48,635.47,369.46,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,163.53,623.47,285.08,9.96">The Multiple Language Question Answering Track at CLEF 2003</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Romagnoli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vallin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Penas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Peinado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Verdejo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,137.02,635.47,195.71,9.96">Working Notes for the CLEF 2003 Workshop</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<meeting><address><addrLine>Trondheim, Norway</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-08">aug 2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
