<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,92.34,74.83,410.56,12.19">Overview of the CLEF 2004 Multilingual Question Answering Track</title>
				<funder ref="#_VR9EafP #_gPUEb4V">
					<orgName type="full">Netherlands Organization for Scientific Research (NWO)</orgName>
				</funder>
				<funder ref="#_Zm5g9VJ">
					<orgName type="full">Portuguese Fundação para a Ciência e Tecnologia</orgName>
				</funder>
				<funder ref="#_U4j8YBQ">
					<orgName type="full">Autonomous Province of Trento</orgName>
				</funder>
				<funder ref="#_BmJY8NY #_php7xSM #_A7jNbvf #_YWb2Bju #_h6cs3sd #_n6aZruT">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_er8CkHJ">
					<orgName type="full">German Federal Ministry of Education and Research (BMBF)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,108.84,101.13,73.67,8.74"><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
							<email>magnini@itc.</email>
							<affiliation key="aff0">
								<orgName type="institution">ITC-Irst</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,190.65,101.13,72.45,8.74"><forename type="first">Alessandro</forename><surname>Vallin</surname></persName>
							<email>vallin@itc.it</email>
							<affiliation key="aff1">
								<orgName type="institution">ITC-Irst</orgName>
								<address>
									<settlement>Trento</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,271.42,101.13,71.94,8.74"><forename type="first">Christelle</forename><surname>Ayache</surname></persName>
							<email>ayache@elda.fr</email>
							<affiliation key="aff2">
								<orgName type="institution">ELDA/ELRA</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,351.54,101.13,59.17,8.74"><forename type="first">Gregor</forename><surname>Erbach</surname></persName>
							<email>erbach@dfki.de</email>
							<affiliation key="aff3">
								<orgName type="institution">DFKI</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,418.97,101.13,61.96,8.74"><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
							<email>anselmo@lsi.uned.es</email>
							<affiliation key="aff4">
								<orgName type="department">Departamento de Lenguajes y Sistemas Informáticos</orgName>
								<orgName type="institution">UNED</orgName>
								<address>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,158.34,112.59,69.51,8.74"><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Informatics Institute</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,236.04,112.59,50.85,8.74"><forename type="first">Paulo</forename><surname>Rocha</surname></persName>
							<email>paulo.rocha@alfa.di.uminho.pt</email>
							<affiliation key="aff6">
								<orgName type="department">Braga Node</orgName>
								<orgName type="laboratory">Linguateca</orgName>
								<orgName type="institution">Universidade do Minho</orgName>
								<address>
									<settlement>Potugal</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,295.13,112.59,47.46,8.74"><forename type="first">Kiril</forename><surname>Simov</surname></persName>
							<affiliation key="aff7">
								<orgName type="department">IPP</orgName>
								<orgName type="institution">Bulgarian Academy of Sciences</orgName>
								<address>
									<settlement>Sofia</settlement>
									<country key="BG">Bulgaria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,365.34,112.59,68.65,8.74"><forename type="first">Richard</forename><surname>Sutcliffe</surname></persName>
							<email>richard.sutcliffe@ul.ie</email>
							<affiliation key="aff8">
								<orgName type="department">DLTG</orgName>
								<orgName type="institution">University of Limerick</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,92.34,74.83,410.56,12.19">Overview of the CLEF 2004 Multilingual Question Answering Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0ECC21556C0C7FC762FEBC97870D554D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Following the pilot Question Answering Track at CLEF 2003, a new evaluation exercise for multilingual QA systems took place in 2004. This paper reports on the novelties introduced in the new campaign and on participants' results. Almost all the cross-language combinations between nine source languages and seven target languages were exploited to set up more than fifty different tasks, both monolingual and bilingual. New types of questions (How-questions and definition questions) were given as input to the participating systems, while just one exact answer per question was allowed as output. The evaluation exercise has highlighted some difficulties in assessing definition questions and can be improved in the future, but the overall analysis of submissions shows encouraging results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Question Answering (QA) systems have been evaluated for the last six years at the TREC campaigns. The TREC QA tracks have evolved over the years, so that increasingly difficult tasks have been proposed, addressing not only factoid but also list and definition questions, and requiring exact answers instead of longer text snippets as output <ref type="bibr" coords="1,99.23,347.19,10.65,8.74" target="#b7">[8]</ref>. Nevertheless, multilinguality has never been investigated at TREC's QA track, thus leaving room for challenging tasks in languages other than English or even across different languages, which is actually in the focus of the CLEF campaigns.</p><p>The first multilingual QA track at CLEF took place in 2003. Eight groups from Europe, the U.S. and Canada participated in nine tasks, submitting a total of seventeen runs. Three languages were addressed in the monolingual tasks (Dutch, Italian and Spanish), while in the bilingual tasks questions were formulated in five source languages (Dutch, French, German, Italian and Spanish) and searched for answers in an English document collection. It was a pilot evaluation exercise and 200 simple, fact-based questions were given as input in all tasks, and participants were allowed to return up to three responses per questions, either exact or 50 byteslong answer-strings <ref type="bibr" coords="1,152.07,450.69,10.64,8.74" target="#b5">[6]</ref>.</p><p>In 2004 the QA@CLEF track 10 attracted considerable attention within the CLEF framework; in fact three different tasks were devoted to it: the main QA track, a Spanish pilot task and iCLEF, the interactive track. The main track has included more European languages and all the cross-language combinations between them have been exploited in order to set up a number of different tasks. As a result, the CLEF QA community has grown and eighteen groups tested their systems, submitting forty-eight runs.</p><p>This paper provides an overview of the main QA track. The following sections report on the languages considered in the experiments, on the procedure that was adopted to build the test sets, and on the participants' results. Each target language will be treated separately, as different subtasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Test Set Preparation</head><p>Multilingual QA entails a number of subtasks, such as the development of tools (PoS-taggers, parsers and Named Entity recognisers) for languages other than English and the translation of questions and answers into other languages <ref type="bibr" coords="2,136.60,643.89,10.64,8.74">[1]</ref>. The construction of a reusable, multilingual collection of questions with the related [answerstrings, docid] pairs represents a useful resource, and the CLEF QA evaluation exercise offers the opportunity to create such a benchmark. As in the 2003 campaign, when two multilingual Gold Standard collections of questions and answers were built <ref type="bibr" coords="2,210.72,678.33,36.99,8.74">[5 and 6]</ref>, in 2004 the generation of the test sets was closely monitored and exploited in order to build similar test sets for all the tasks, and to translate all the questions proposed into the track in all the source languages. Because of the number of languages involved, there was no attempt to have exactly the same test set in all the tasks, as we managed to do in 2003.</p><p>Eight groups were involved in the generation, translation and manual verification of the questions: the IPP group at the Bulgarian Academy of Sciences translated the entire collection of questions and answers in Bulgarian, DFKI created the German test set, ELRA/ELDA took over the work on the French questions, ITC-Irst was in charge of the Italian and English test sets, Linguateca provided the Portuguese part of the benchmark, UNED prepared the Spanish part, the University of Amsterdam worked on Dutch and the University of Helsinki joined the activity translating 200 English questions into Finnish, in order to set up the Finnish-English task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Question Generation</head><p>The questions in the test sets addressed large (on average 230 Mb), open domain corpora. The document collections for all the target languages were comparable because they were made up of newspapers and news agencies articles that referred to the same time-span: NRC Handelsblad <ref type="bibr" coords="3,370.26,170.13,93.09,8.74">(years 1994 and 1995)</ref> and Algemeen <ref type="bibr" coords="3,70.92,181.59,80.48,8.74">Dagblad (1994 and</ref><ref type="bibr" coords="3,155.27,181.59,23.38,8.74">1995)</ref> for Dutch; Los Angeles Times (1994) and Glasgow Herald (1995) for English; Le Monde (1994) and SDA <ref type="bibr" coords="3,170.16,193.11,72.62,8.74">French (1994 and</ref><ref type="bibr" coords="3,245.75,193.11,23.32,8.74">1995)</ref> for French; Frankfurter Rundschau (1994), Der Spiegel <ref type="bibr" coords="3,501.06,193.11,23.37,8.74;3,70.92,204.57,40.82,8.74">(1994 and 1995)</ref> and SDA German <ref type="bibr" coords="3,189.42,204.57,67.21,8.74">(1994 and 1995)</ref> for German; La Stampa (1994) and SDA <ref type="bibr" coords="3,427.56,204.57,70.47,8.74">Italian (1994 and</ref><ref type="bibr" coords="3,501.06,204.57,23.38,8.74">1995)</ref> for Italian; <ref type="bibr" coords="3,115.92,216.09,85.06,8.74">PÚBLICO (1994 and</ref><ref type="bibr" coords="3,203.48,216.09,23.34,8.74">1995)</ref> for <ref type="bibr" coords="3,243.47,216.09,61.41,8.74">Portuguese and</ref><ref type="bibr" coords="3,307.32,216.09,61.10,8.74">EFE (1994 and</ref><ref type="bibr" coords="3,370.91,216.09,23.38,8.74">1995)</ref> for Spanish.</p><p>As a first step in the test sets preparation, each co-ordinating group generated 100 questions in its own target language, searched manually for at least one answer per question supported by a document and then translated into English, that was used as the interlingua between all the groups, both questions and answers. The questions had to be compliant with specific criteria that were previously established: list questions (e.g. What are the three most important export products of Italy?), embedded questions (e.g. When did the king who succeeded Queen Victoria die?), yes/no questions (e.g. Did Shakespeare have any sisters?) and Why-questions (e.g. Why did Nixon resign?) were not considered in the track <ref type="bibr" coords="3,278.92,296.61,10.64,8.74" target="#b1">[2]</ref>.</p><p>On the other hand, the test set included two question types that were avoided in 2003: How-questions and definition questions. These two categories, which can have longer answer-strings than the factoid questions, were approached basically in the same way, though assessors were less demanding in terms of exactness. How-questions (e.g. How did Hitler die?), may have several different responses (e.g. He committed suicide, or in mysterious circumstances or hit by a bullet, or even alone) that provide different kinds of information.</p><p>Similarly, definition questions (e.g. What is the atom? or Who are the Martians?) are considered very difficult because though their target is clear, they are posed in isolation, and different questioners might expect different answers depending on their previous assumptions. They were first introduced at TREC 2001 and then proposed again in 2003, when organisers tried to define a potential user of the QA system, who would be "an adult, a native speaker of English, and an 'average' reader of US newspapers" <ref type="bibr" coords="3,388.67,411.57,10.61,8.74" target="#b7">[8]</ref>. TREC assessors created a list of "information nuggets" (i.e. significant facts that were likely to appear in the desired response), some of which were necessary, and judged the content of each answer checking how many nuggets it contained. This way of assessing the definition questions was quite complex and far from being exhaustive, so the CLEF approach in this sense has been simplified: first of all only definition questions that referred to either a person or an organisation were chosen, in order to avoid more abstract "concept definition" questions such as What is religion?, that would be too complex to be judged. The restriction to persons (Who is Kofi Annan?) and organisations (What is Amnesty International?) aimed at generating simple definition questions, whose answer could be a single, well defined text snippet such as British spies listened in to UN Secretary General Kofi Annan's office or Amnesty International campaigns for human rights, without any previous expectations regarding the most relevant information that a system should return. Secondly, as they were introduced as a stepping stone in 2004, the most general answers were judged as correct, assuming that potential users did not know anything about the addressed person or organisation.</p><p>The track co-ordinators attempted to balance the test sets according to the different answer types of the questions. Eight answer types were considered: TIME (e.g. What year was Thomas Mann awarded the Nobel Prize?), MEASURE (e.g. How many years of imprisonment did Nelson Mandela serve?), PERSON (e.g. Who was Lisa Marie Presley's father?), ORGANISATION (e.g. What is the name of the Kurdish separatist party?), LOCATION (What is the capital of Japan?), OBJECT (e.g. Name an odourless and tasteless liquid.), MANNER (e.g. How did Pasolini die?) and OTHER (e.g. What animal coos?). It is difficult to determine the intrinsic difficulty of a question, but the distribution of several answer types in the test sets could differentiate the task and offer some insights in the systems performance with regard to particular categories of questions, as we will show in the results section below.</p><p>Each organising group (except IPP and the University of Helsinki) collected 100 questions that had at least one answer in their own target corpus. Those questions would be shared with the other groups, so they were translated into English and saved in a simple XML format. For instance, during this work phase ELRA/ELDA generated the factoid question Où se trouve Halifax ?, that had a LOCATION as answer type, translating it into Where is Halifax located?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Translation</head><p>Seven hundred questions were formulated in an original source language, manually verified against a document collection, translated into English and collected in a common XML format. In order to share them in a multilingual scenario, a second translation in all the nine source languages of the track was necessary. Native speakers of each source language with a good command of English were recruited, and they were asked to translate the questions trying to adhere as much as possible to the English version. In case of any discrepancies between the original and the English form, they were expected to follow the former, and to communicate the changes that the latter presented. Nevertheless, cultural differences made some cross-lingual obstacles unavoidable: so, for example, the English question What does a luthier make? became tautological in German (Was macht ein Geigen-und Gitarrenbauer?), while some other concepts, such as CEO, were ambiguous and were translated in different ways (chairman, managing director or president). Moreover, translators encountered difficulties in the transliteration of proper names: for instance, Vladimir Zhirinovsky is written Wladimir Schirinowski in German, Vladimir Zhirinovskij in Italian and Vladimir Jirinovski in French. Translators usually chose the most frequent form in which proper names appeared in their target corpus.</p><p>Finally, in carrying out the assessments it became clear that translation has a discernible effect on the integrity of the judgement process. For example is a Finance Minister the same as a Minister for Economic Affairs? These might be (and in fact are) different roles but they could equally be the same one translated differently. Similarly, when is a General Manager the same as a Secretary General? In English a General Manager is quite a junior managerial position so the answer is probably "never". However in another language they might be quite equivalent. It is hard therefore to know what to conclude from judgements relating to questions describing translated versions of ranks, titles and so on. &lt;q cnt="0504" category="F" answer_type="LOCATION"&gt; &lt;language val="BG" original="FALSE"&gt; &lt;question group="BTB"&gt;Къде се намира Халифакс?&lt;/question&gt; &lt;answer n="1" docid=""&gt;TRANSLATION[Канада]&lt;/answer&gt; &lt;/language&gt; &lt;language val="DE" original="FALSE"&gt; &lt;question group="DFKI"&gt;Wo liegt Halifax?&lt;/question&gt; &lt;answer n="1" docid=""&gt;TRANSLATION[Kanada]&lt;/answer&gt; &lt;/language&gt; &lt;language val="EN" original="FALSE"&gt; &lt;question group="ELDA"&gt;Where is Halifax located?&lt;/question&gt; &lt;answer n="1" docid=""&gt;TRANSLATION[Canada]&lt;/answer&gt; &lt;answer n="2" docid="LA112094-0062"&gt;Canada&lt;/answer&gt; &lt;/language&gt; &lt;language val="ES" original="FALSE"&gt; &lt;question group="UNED"&gt;¿Dónde se encuentra Halifax?&lt;/question&gt; &lt;answer n="1" docid=""&gt;TRANSLATION[Canadá]&lt;/answer&gt; &lt;answer n="2" docid="EFE19940927-15402"&gt;Canadá&lt;/answer&gt; &lt;/language&gt; &lt;language val="FR" original="TRUE"&gt; &lt;question group="ELDA"&gt;Où se trouve Halifax ?&lt;/question&gt; &lt;answer n="1" docid="ATS.950616.0005"&gt;Canada&lt;/answer&gt; &lt;/language&gt; &lt;language val="IT" original="FALSE"&gt; &lt;question group="IRST"&gt;Dove si trova Halifax?&lt;/question&gt; &lt;answer n="1" docid=""&gt;TRANSLATION[Canada]&lt;/answer&gt; &lt;/language&gt; &lt;language val="NL" original="FALSE"&gt; &lt;question group="UoA"&gt;Waar is Halifax?&lt;/question&gt; &lt;answer n="1" docid=""&gt;TRANSLATION[Canada]&lt;/answer&gt; &lt;/language&gt; &lt;language val="PT" original="FALSE"&gt; &lt;question group="LING"&gt;Onde fica Halifax?&lt;/question&gt; &lt;answer n="1" docid=""&gt;TRANSLATION[Canadá]&lt;/answer&gt; &lt;answer n="2" docid="LING-940526-150"&gt;West Yorkshire&lt;/answer&gt; &lt;answer n="3" docid="LING-941009-021"&gt;Nova Escócia, no Canadá&lt;/answer&gt; &lt;answer n="4" docid="LING-941201-050"&gt;Canadá&lt;/answer&gt; &lt;/language&gt; &lt;/q&gt; In order to reduce inconsistencies, questions were translated into the form in which a native speaker would naturally ask it. The fact that manual translation captured some of the cross-cultural as well as cross-language problems is good since QA systems are designed to work in the real world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Gold Standard</head><p>Once all the 700 questions were translated into eight languages (Finnish was added only shortly before the beginning of the experiments, and just for 200 questions), 100 additional questions for each target language were selected from the collection, in order to collect 200 questions per test set.</p><p>Around twenty of them did not have any answer in the document collections, and the right response to them was the string "NIL". The organisers decided not to include any NIL question among the definitions. The usual procedure to choose them was to select those containing proper nouns that did not occur in the document collection. Though it was easy to implement, this strategy probably made it too easy for participating systems to identify NIL questions, and should be reconsidered for future campaigns. Being aware of this drawback, some groups randomly selected the required NIL questions from those that seemed to have no answer in the document collections, and double checked them.</p><p>Additional questions were manually verified and new answers were added to those that were just the translation of the original one. Figure <ref type="figure" coords="5,225.29,271.29,5.01,8.74" target="#fig_0">1</ref> above shows a sample from the multilingual collection of questions and answers built by the organising groups, called Multieight-04 corpus. From this XML file the plain text test sets used for the evaluation exercise were extracted. Each question is described according to its category (either factoid or definition) and to its answer type. The information concerning the category was kept also in test sets released to participants, where the character F designated a factoid, and D a definition. Questions appear in eight languages, and in one or more of them at least one [answer-string, docid] pair is given. The Boolean attribute "original" keeps track of the language in which each question was first generated and verified.</p><p>The entire collection is made up of 608 factoid and 92 definition questions, and the eight answer types are rather balanced: it includes 173 PERSON, 118 LOCATION, 98 ORGANISATION, 88 OTHER, 84 MEASURE, 82 TIME, 31 OBJECT and 26 MANNER. Each question has at least one answer in one or more target document collections, but due to the variety of languages, just a few were manually verified in all the languages and consequently appeared in all the test sets.</p><p>Similar to the DISEQuA and the Multisix collections built for the CLEF 2003 QA track, Multieight-04 is a valuable and reusable benchmark resource that can be further enlarged and distributed. Unfortunately it does not contain all the responses to each question, but just those that were manually found for the test sets preparation. It could be enriched with automatically retrieved pattern sets of correct answers in all the languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Participants</head><p>The encouraging results of the 2003 campaign, which led to the consolidation of the CLEF QA community, and probably the variety of the proposed tasks, gave rise to an increase in the number of participating teams. At the CLEF 2003 QA track 8 groups (3 from the U.S. and 5 from Europe) submitted a total of 17 runs in 9 tasks, while in 2004 18 teams (all of them from Europe except one from Mexico) returned 48 runs distributed over 19 monolingual and bilingual tasks. These figures are similar to those of the TREC-8 pilot QA evaluation exercise, where 20 groups submitted 46 runs, and represent a promising starting point for future campaigns, in which participants from other parts of the world should be involved.  <ref type="table" coords="6,112.97,73.53,5.01,8.74" target="#tab_0">1</ref> shows, many of the 56 tasks that were set up did not attract any participants, but in all the six monolingual tasks, highlighted in the table with grey cells, two or more runs were returned. Black cells indicate the tasks that were not activated.</p><p>The bilingual tasks with English (EN) as target were chosen by six different groups. On the contrary, English as source language did not receive much attention. French (FR) as target registered the highest number of submissions, but they were returned by a single participating team. Five Spanish groups participated in the monolingual Spanish (ES) task, while in 2003 only the University of Alicante managed to run its system. New Dutch (NL) and Italian (IT) research groups registered in 2004 (only one Dutch group actually participated) in the corresponding monolingual tasks, which testifies the growing interest in QA for languages other than English. German (DE), that in 2003 was source language only, was chosen by two groups as target, like Portuguese (PT), at its first time at CLEF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Participants were allowed to submit just one response per question and up to two runs per task. Submissions were manually judged by human assessors, who considered both correctness and exactness of each answer.</p><p>A response was judged as correct when its form was clear and its content was responsive, while exactness is more related to the quantity than to the quality of the information retrieved by the systems. In the track guidelines <ref type="bibr" coords="6,115.38,294.27,10.64,8.74" target="#b1">[2]</ref>, articles and prepositions were tentatively indicated as acceptable parts of speech that would not penalise the exactness of an answer. Adjectives, verbs and adverbs could instead add irrelevant or unnecessary information, as in the answer Ex IMF Secretary General Dies (that was returned in response to the question Of what organisation was Pierre-Paul Schweitzer general manager?), where only IMF would have been the exact and required string. At any rate, exactness was never precisely defined, so a certain degree of subjectivity in the judgements could not be eliminated.</p><p>In 2003, in order to facilitate participation, both exact and 50 bytes-long answer-strings were accepted (though assessed separately), but most participants chose to return exact responses. So, in 2004 only exact answers were allowed, which made the tasks more difficult. Responses were judged either as Right, Wrong, ineXact or Unsupported (when the answer-string contained a correct answer but the returned docid did not support it).</p><p>Factoid questions with the answer type MANNER (i.e. How-questions) and definition questions, that were included in the test sets in 2004 for the first time, needed more heuristically oriented evaluation criteria because their answers could be also long circumlocution or even entire sentences. In particular, answers to definition questions were judged considering their usefulness for a potential user that was assumed to know nothing of the person or the organisation addressed by the question. For instance, a correct answer returned in response to the question Who is Jorge Amado? was the following sentence: American authors such as Stephen King and Sidney Sheldon are perennial best sellers in Latin American countries, while Brazilian Jorge Amado, Colombian Gabriel Garcia Marquez and Mexican Carlos Fuentes are renowned in U.S. literary circles. In fact, it is clear from the sentence that Jorge Amado is a Brazilian writer and, moreover, it would have been difficult to extract a shorter and responsive string from this snippet.</p><p>The assessors were basically less demanding in terms of exactness when they judged these types of questions. However, accepting such long answers might be seen as equivalent to considering passage extraction rather than QA, so some judges disagreed on this subject. Because of the unnecessary information included in the answer-string above, some assessors would judge the response as inexact. No specific assessment training was offered to all the groups, which should be taken into account in the future.</p><p>The organising group that had generated the questions in a particular language was in charge of the assessment of the runs with the same target language (except for the judgement of the English runs, that was taken over by the University of Limerick). As a common procedure, each run, containing 200 answers, was judged by more than one assessor. The DLTG group used a different approach, as described in section 5.2. The main measure was accuracy, that is the fraction of right answers. Answers had to be unranked (i.e. in the same order as in the test set), but a confidence value could be given for each response. Though it was not mandatory, this absolute value that could range between 0 and 1 was considered to calculate an additional Confidence-weighted Score (CWS), borrowed from the TREC-2002 track <ref type="bibr" coords="6,374.86,673.76,10.65,8.74" target="#b6">[7]</ref>. Both accuracy and CWS reward systems for recognising correct answers, and both penalise them for mistaking wrong responses for correct ones. However, only CWS rewards systems that can predict their own performance.</p><p>The restriction to a single exact answer per question made the task harder than that proposed in 2003, when three ranked responses were accepted and the Mean Reciprocal Rank was computed. At CLEF 2003 the average performance was 41% of correct answers in the monolingual tasks and 25% in the cross-language ones, but if we consider just the first response to each question, the results drop to 29% and 17% respectively.</p><p>In 2004 the average accuracy over the 20 runs submitted in the monolingual tasks was 23.7%, and 14.7% over the 28 bilingual runs. So, the average results of the two evaluation exercises are not so different, and the slight downgrade registered in 2004 is probably due to the introduction of the definition questions.</p><p>In the following seven sections the results of the runs for each target language are thoroughly discussed. For each target language two kinds of results are given: the overall performance (accuracy and confidenceweighted score) of all the submitted runs and an analysis of the systems' accuracy with respect to the answer types of the questions in test set. Answer types are designated by the following abbreviations: loc ≡ location, mea ≡ measure, org ≡ organisation, per ≡ person, man ≡ manner, obj ≡ object, oth ≡ other and tim ≡ time. Below each answer type, the number of posed questions of that type is shown in square brackets.</p><p>The last row of the tables that analyse questions according to their answer type shows a virtual run, called combination, in which the classification "right answer" is assigned to a question if any of the participating systems found it. The objective of this combination run is to show the potential achievement if one merged all answers and considered the set of answers right, provided one answer were right.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dutch as Target</head><p>Two research groups registered for tasks with Dutch as the target language, but only one team submitted runs: the University of Amsterdam, who had also participated in 2003. They submitted two monolingual runs, and one bilingual run (English to Dutch).</p><p>The Dutch test set contains 200 questions. Table <ref type="table" coords="7,297.89,294.27,5.01,8.74" target="#tab_1">2</ref> below details the results of the three submitted runs. Interestingly, on definition questions the bilingual English to Dutch run performed better than either of the two monolingual runs. The aim of the virtual run called combination is to provide an upperbound on the possible performance of a system that would merge the existing runs and somehow select the right answers from the combined pool of candidate answers. As an aside, this is actually how the University of Amsterdam's QA system works: separate streams each generate result files, and these are combined into a joint pool of candidate answers from which the final answers are selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">English as Target</head><p>The work of assessing questions with English answers was assigned to the Documents and Linguistic Technology Group at Limerick. The five tasks enacted involved questions in Bulgarian, Finnish, French, German, Italian with English answers being returned from the LA Times (American English) and Glasgow Herald (Scottish English) collections. The starting point in carrying out the assessment comprised the TREC Evaluation Software written by Ellen Voorhees and the Multieight-04 collection of manually retrieved answers. Having studied the TREC software it was decided that it should be used on a question-by-question basis rather than on a run-by-run basis. This means that a single assessor reviews and evaluates all candidate answers to a given question. before moving to the next question. Originally we had envisaged that a given evaluator would assess all answers to different questions comprising a complete run before moving on to the next run. The method used in carrying out the assessment was as follows. There were four primary assessors plus one secondary assessor. Each primary assessor -a native speaker of English -was assigned a set of questions, 1-50, 51-100, 101-150 and 151-200 respectively. The assessors, provided with a set of guidelines, then carried out their work, noting any doubtful cases. A series of meetings then took place at which these cases were considered in turn by all five assessors and a joint decision was made. To ensure consistency, the consequences of each decision were then cross-checked by each assessor against judgements of comparable cases. It should be noted therefore that while all responses to a particular question were judged by the same person, we did not use double-blind assessment where each judgement is made independently by two assessors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NIL Accuracy</head><p>We should point out that our reasoning and judgements were made with respect to the English versions of the questions. However, all the systems in this task group were using the 'same' questions in languages other than English. It is possible therefore that a question inadvertently asked something different in a particular language due to differences of translation. This could affect the results though perhaps not to a major degree. The main results of the task group can be seen in Table <ref type="table" coords="8,294.41,257.49,3.75,8.74" target="#tab_4">4</ref>. 11 Since some typos were found in the FI=&gt;EN test set, seven questions were not taken into consideration in the evaluation. None of them had received a right answer, so their exclusion did not affect the data in Table <ref type="table" coords="8,404.77,748.78,3.34,7.85" target="#tab_5">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NIL Accuracy</head><p>Some answer types (i.e. manner, measure and object) turned out to be difficult for systems, while the performance on location, factoid-person and time is quite good.</p><p>In making judgements concerning definitions we decided to err on the side of generosity and made no correction for the length of submissions although in practice these tended to be short. A response was considered correct if it provided salient information concerning the topic. Generally the task specification for such questions was considered somewhat vague and so the results while being interesting are not necessarily that informative. What seems to be necessary is a means of punishing answers which contain both relevant and irrelevant information. This has been attempted in TREC with mixed results.</p><p>While the level of participation in the English target task group was very encouraging, the numbers participating was still very small in statistical terms and also varied from language pair to language pair. Therefore we should be careful not to conclude too much from the results in terms for example of the relative difficulty of different language pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">French as Target</head><p>A single research group took part in evaluation tasks with French as a target language: Neuchatel University. It took part in both monolingual and bilingual tasks. This participating team submitted 16 runs, two runs per source language, taken from the 8 available source languages: Bulgarian, German, English, Spanish, French, Italian, Dutch and Portuguese. In particular, two runs were submitted for the monolingual task French-French. Table <ref type="table" coords="9,96.20,305.79,5.01,8.74" target="#tab_6">6</ref> shows the assessment of the sixteen submitted runs. The monolingual runs appear in italics. The best results were obtained for one of the monolingual runs (gine042frfr). This proves once again that it is a priori easier for the systems to answer correctly when the source language is the same as the target language. However, it is noticeable that the 2nd and 3rd best results are obtained by the two German-French runs (better than the other monolingual French run). It is important to notice that the number of unsupported answers is 0 for all runs. This is expectable as all 16 runs are versions of the same system, and indicates that this system always supports the answers it gives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NIL Accuracy</head><p>The correct answers given for all the runs are presented in Table <ref type="table" coords="9,352.10,666.51,3.76,8.74" target="#tab_7">7</ref>, clustered by answer type of questions. Neuchatel system's weaknesses obviously lie in definition-organisation (recall 0%) and in factoid-manner (max. recall 21%) questions, whereas it gives its better results for definition-person (max. recall 50%), measure (32%) and location (34.5%) questions.</p><p>The virtual run in the last row, called combination, aims at getting an idea of what could be the expected potential performance of a system giving all the correct answers. The best run (gine042frfr) is able to supply only 50.51% of the correct answers of "combination". This ratio could be enhanced if results for definitionorganisation and factoid-manner, in particular, would be bettered. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">German as Target</head><p>Two research groups took part in tasks with German as target language, and only in the monolingual German task: DFKI, which had participated at CLEF-2003, and Fernuniversität Hagen, at its first participation, submitted one run each. The German test set contained 200 questions. However, three questions contained spelling errors and were subsequently excluded from the evaluation, so that only 197 questions were taken into consideration. Table <ref type="table" coords="10,96.14,444.81,5.01,8.74" target="#tab_8">8</ref> shows the assessment of the two runs which were submitted. DFKI did not handle any definition questions. Both groups produced short and exact answers; no answer was longer than 6 words or 48 characters. The combination run in the last row shows that the best performing system (fuha041dede) is able to respond correctly to 78% of the questions that have been correctly answered by both teams in conjunction.</p><p>The DFKI group conducted an experiment to compare the QA system performance against human QA performance under time constraints <ref type="bibr" coords="10,218.33,746.07,10.63,8.74" target="#b2">[3]</ref>. Three subjects answered all 200 questions of the monolingual German test set with the help of a search engine. The time between the presentation of each question and the submission of the document ID was measured, and the answers were assessed. Only answers that were found within a given time limit were considered. Then the accuracy a human could achieve was calculated. It was found that a human who is allowed a maximum of 42 seconds per question achieves the same level of accuracy as the German "combination" run (DFKI run ≈ 30s, FUHA run ≈ 34s). In addition, the experiment revealed the difficulty of different answer types for humans, e.g., the average definition questions required 39 seconds and the average factoid questions 81 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Italian as Target</head><p>Two research groups took part in tasks with Italian as target language, and precisely only in the monolingual Italian task: ITC-Irst, that had participated also at CLEF-2003, and the Institute for Computational Linguistics in Pisa 12 , at its first participation.</p><p>In 2003 ITC-Irst submitted two runs, and the system answered correctly at the first rank to 37.5% and 41.5% of the questions respectively. The lower results achieved in 2004 with the same system demonstrate that the task was harder. Nevertheless, as Table <ref type="table" coords="11,248.94,248.31,9.99,8.74" target="#tab_11">10</ref> shows, the overall accuracy of the runs ILCP and irst041 is over the average performance of the participants in the monolingual tasks. The analysis of the results in Table <ref type="table" coords="11,238.73,392.25,10.05,8.74" target="#tab_12">11</ref> shows that location, person and time were the easiest answer types for the participating systems. How-questions constituted a problem for the Irst system, while ILCP answered four of them correctly, retrieving long text snippet that were judged as responsive. The accuracy over definition questions in all three submitted runs is relatively high. While the Irst system returned very short answers, trying to select the most relevant portion of text, ILCP system often gave long answer-strings, and many of them (14.5%) were judged as inexact, though they often contained the required information.</p><p>The runs ILCP and irst042 were the most precise in the whole track in identifying the questions with no response, though their recall is not so high. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Portuguese as Target</head><p>Two research groups took part in tasks with Portuguese as target language, both in the monolingual task; one of them submitted two runs. None provided a confidence score. Since there was a duplicated question, ("Who was the first President of the United States?"), only 199 questions were taken into account in the summary statistics. The table below shows the assessment of the three submitted runs. While the answers of the SFNX system were generally rather short, the PTUE system occasionally submitted longer answers (in one case, reaching 35 words).</p><p>12 Joint work with the Department of Information and Communication Technology of the University of Pisa. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Spanish as Target</head><p>Eight runs were submitted having Spanish as target language. The only source language was Spanish, too.  As the virtual combination run in the last row shows, the best performing system (aliv042eses) is able to respond correctly to only 57.5% of the questions that would have been correctly answered by all teams in conjunction. Systems show better behaviour when answering about locations, organisations, dates and persons. It is interesting to remark that, whereas individual systems show important differences among the number of correct answers depending on the type of question, the combination of systems shows a quite uniform distribution.</p><p>Though different questions and different text collections were used, the overall results obtained for monolingual Spanish in 2004 are better than those in the 2003 track. The best result obtained in last edition was 40% of questions with a correct answer. However, three answers per question were allowed in 2003: if we consider only the percentage of correct answers found at the first rank, that was 24.5% for the best system, it is outperformed by the run aliv042eses, submitted by the University of Alicante, that in 2004 reached an accuracy of 32.5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Remarks on Evaluation</head><p>The four judgements adopted by the assessors (right, wrong, inexact and unsupported) have been used at TREC for many years and seem to cover most of the possible answers of a real QA system. Even so, the evaluation of the runs submitted at CLEF shows that sometimes they are somehow simplistic, and that they do not enable assessors to grasp the responsiveness of all the answers.</p><p>In particular, as the disagreement between assessors has shown, exactness is really difficult to judge, considering also that it has never been defined with objective criteria. The tentative rules we tried to draft concerning the acceptable and the unacceptable parts of speech did not always match with the sensibility of the human assessors. Furthermore, some types of questions, such as How-questions and definitions, have relatively long strings as answers, and for the time being it would be too demanding to require essential and not redundant responses. Maybe we should consider going back to the retrieval of short, meaningful passages (similar to the optional justifications that could be attached to the answers at TREC 2002), possibly rewarding those systems that are able to return just the minimal piece of information. Alternatively, the judgement inexact could be kept, but differentiated so as to distinguish between an incomplete answer and one that is too long.</p><p>In addition, the judgement unsupported could be considered independently from right and wrong because assessors came across wrong answers that were completely unrelated to the document indicated in the docid.</p><p>Finally, an additional heuristic judgement that quantifies the usefulness of a response could be introduced; in fact an answer can be either wrong or inexact, but at the same time a potential user could draw some partial information from it.</p><p>As far as the NIL questions are concerned, they were usually generated using proper names or keywords that did not appear in the document collection. This procedure needs to be reconsidered, because a simple IR system could trivially identify them, though in 2004 the NIL accuracy was not very high. If NIL questions addressed entities that actually appear in the corpus, the task would be more challenging and significant.</p><p>Confidence-weighted score, that was used at TREC 2002 <ref type="bibr" coords="13,338.81,512.79,10.66,8.74" target="#b6">[7]</ref>, could not be calculated for all the runs because the confidence value was not mandatory. When computed, it seemed to reflect the overall accuracy, and it does not provide further insight in the systems' performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>Thanks to the high number of proposed tasks and to a growing interest in Question Answering by the European research community, the QA@CLEF-2004 attracted more participants than the previous edition. In addition, the benchmark resources built within the framework of these evaluation exercises contribute to the development and tuning of systems, and can be reused as training resources.</p><p>The results of the 2004 track are not fully comparable to those achieved in 2003, in fact the two tasks were designed differently: nonetheless, the accuracy in answering specific questions, such as those that had location and time as answer types, was encouragingly high in all the seven target languages. The introduction of definition and How-questions made the task harder, and the assessors encountered some difficulties in defining and judging objectively the responsiveness and exactness of the responses. It seems that in assessing these particular questions, it would be reasonable to accept short text passages instead of exact answer-strings. Besides, the evaluation process as it was designed, i.e. split over different sites with multiple assessors, lacked uniformity and would need stricter, common guidelines that cover as much as real output cases. This should as much as possible be reconsidered for future campaigns.</p><p>The evaluation measures adopted in 2004 followed closely the TREC-2002 QA track, but since the assessors sometimes found the four judgements (right, wrong, inexact and unsupported) inadequate, some changes might be introduced in the next exercises, aimed for instance at rewarding the usefulness of responses for a potential user. However, coming up with a user model that is useful, satisfactory, and realistic is highly non-trivial.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,149.34,744.93,296.60,8.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sample of the Multieight-04 collection of questions and answers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,104.52,589.65,386.30,158.86"><head>Table 1 :</head><label>1</label><figDesc>The tasks and the corresponding number of submitted runs at the CLEF 2004 QA track.</figDesc><table coords="5,147.99,589.65,293.34,145.90"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Target Languages</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>DE</cell><cell>EN</cell><cell>ES</cell><cell>FR</cell><cell>IT</cell><cell>NL</cell><cell>PT</cell></row><row><cell></cell><cell>BG</cell><cell></cell><cell>1</cell><cell></cell><cell>2</cell><cell></cell><cell></cell></row><row><cell>Source Languages</cell><cell>DE EN ES FI FR IT</cell><cell>2</cell><cell>3 1 6 2</cell><cell>8</cell><cell>2 2 2 2 2</cell><cell>3</cell><cell>1</cell></row><row><cell></cell><cell>NL</cell><cell></cell><cell></cell><cell></cell><cell>2</cell><cell></cell><cell>2</cell></row><row><cell></cell><cell>PT</cell><cell></cell><cell></cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell>3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,70.92,420.69,442.49,137.28"><head>Table 2 :</head><label>2</label><figDesc>Results of the monolingual and bilingual Dutch runs.In Table3the results are broken down by answer type.</figDesc><table coords="7,74.82,473.02,438.59,84.95"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">given correct answers</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Run</cell><cell cols="2">definition (#) org [11] per [12]</cell><cell>loc [32]</cell><cell>man [15]</cell><cell>mea [15]</cell><cell cols="2">Factoid (#) obj [10] org [22]</cell><cell>oth [17]</cell><cell>per [49]</cell><cell>tim [17]</cell><cell>total [200] # %</cell></row><row><cell>uams041nlnl</cell><cell>6</cell><cell>7</cell><cell>14</cell><cell>3</cell><cell>6</cell><cell>1</cell><cell>10</cell><cell>5</cell><cell>26</cell><cell>10</cell><cell>88 44.00</cell></row><row><cell>uams042nlnl</cell><cell>4</cell><cell>7</cell><cell>15</cell><cell>3</cell><cell>4</cell><cell>1</cell><cell>11</cell><cell>5</cell><cell>30</cell><cell>11</cell><cell>91 45.50</cell></row><row><cell>uams041ennl</cell><cell>6</cell><cell>9</cell><cell>11</cell><cell>0</cell><cell>4</cell><cell>1</cell><cell>8</cell><cell>1</cell><cell>21</cell><cell>9</cell><cell>70 35.00</cell></row><row><cell>combination</cell><cell>7</cell><cell>10</cell><cell>20</cell><cell>3</cell><cell>8</cell><cell>2</cell><cell>13</cell><cell>5</cell><cell>36</cell><cell>16</cell><cell>120 60.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,147.66,568.11,300.05,8.74"><head>Table 3 :</head><label>3</label><figDesc>Results of the Dutch runs, according to answer types of questions.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,70.92,285.52,448.31,420.28"><head>Table 4 :</head><label>4</label><figDesc>Results of the runs with English as target language.In Table5the results are sorted by category of questions.</figDesc><table coords="8,72.18,285.52,447.05,420.28"><row><cell></cell><cell>Right</cell><cell>Wrong</cell><cell>ineXact</cell><cell cols="2">Unsupported</cell><cell cols="2">Overall</cell><cell>Accuracy</cell><cell>Accuracy</cell><cell></cell><cell></cell><cell></cell><cell>Confidence</cell></row><row><cell>Run Name</cell><cell>answers</cell><cell>answers</cell><cell>answers</cell><cell>answers</cell><cell></cell><cell cols="2">Accuracy</cell><cell>over F</cell><cell>over D</cell><cell></cell><cell></cell><cell></cell><cell>weighted</cell></row><row><cell></cell><cell>(#)</cell><cell>(#)</cell><cell>(#)</cell><cell>(#)</cell><cell></cell><cell></cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell cols="3">Precision Recall</cell><cell>Score</cell></row><row><cell>bgas041bgen</cell><cell>26</cell><cell>168</cell><cell>5</cell><cell>1</cell><cell></cell><cell></cell><cell>13.00</cell><cell>11.67</cell><cell>25.00</cell><cell>0.13</cell><cell cols="2">0.40</cell><cell>0.056</cell></row><row><cell>dfki041deen</cell><cell>47</cell><cell>151</cell><cell>0</cell><cell>2</cell><cell></cell><cell></cell><cell>23.50</cell><cell>23.89</cell><cell>20.00</cell><cell>0.10</cell><cell cols="2">0.75</cell><cell>0.177</cell></row><row><cell>dltg041fren</cell><cell>38</cell><cell>155</cell><cell>7</cell><cell>0</cell><cell></cell><cell></cell><cell>19.00</cell><cell>17.78</cell><cell>30.00</cell><cell>0.17</cell><cell cols="2">0.55</cell><cell>-</cell></row><row><cell>dltg042fren</cell><cell>29</cell><cell>164</cell><cell>7</cell><cell>0</cell><cell></cell><cell></cell><cell>14.50</cell><cell>12.78</cell><cell>30.00</cell><cell>0.14</cell><cell cols="2">0.45</cell><cell>-</cell></row><row><cell>edin041deen</cell><cell>28</cell><cell>166</cell><cell>5</cell><cell>1</cell><cell></cell><cell></cell><cell>14.00</cell><cell>13.33</cell><cell>20.00</cell><cell>0.14</cell><cell cols="2">0.35</cell><cell>0.049</cell></row><row><cell>edin041fren</cell><cell>33</cell><cell>161</cell><cell>6</cell><cell>0</cell><cell></cell><cell></cell><cell>16.50</cell><cell>17.78</cell><cell>5.00</cell><cell>0.15</cell><cell cols="2">0.55</cell><cell>0.056</cell></row><row><cell>edin042deen</cell><cell>34</cell><cell>159</cell><cell>7</cell><cell>0</cell><cell></cell><cell></cell><cell>17.00</cell><cell>16.11</cell><cell>25.00</cell><cell>0.14</cell><cell cols="2">0.35</cell><cell>0.052</cell></row><row><cell>edin042fren</cell><cell>40</cell><cell>153</cell><cell>7</cell><cell>0</cell><cell></cell><cell></cell><cell>20.00</cell><cell>20.56</cell><cell>15.00</cell><cell>0.15</cell><cell cols="2">0.55</cell><cell>0.058</cell></row><row><cell>hels041fien</cell><cell>21</cell><cell>171</cell><cell>1</cell><cell>0</cell><cell></cell><cell></cell><cell>10.88</cell><cell>11.56</cell><cell>5.00</cell><cell>0.10</cell><cell cols="2">0.85</cell><cell>0.046</cell></row><row><cell>irst041iten</cell><cell>45</cell><cell>146</cell><cell>6</cell><cell>3</cell><cell></cell><cell></cell><cell>22.50</cell><cell>22.22</cell><cell>25.00</cell><cell>0.24</cell><cell cols="2">0.30</cell><cell>0.121</cell></row><row><cell>irst042iten</cell><cell>35</cell><cell>158</cell><cell>5</cell><cell>2</cell><cell></cell><cell></cell><cell>17.50</cell><cell>16.67</cell><cell>25.00</cell><cell>0.24</cell><cell cols="2">0.30</cell><cell>0.075</cell></row><row><cell>lire041fren</cell><cell>22</cell><cell>172</cell><cell>6</cell><cell>0</cell><cell></cell><cell></cell><cell>11.00</cell><cell>10.00</cell><cell>20.00</cell><cell>0.05</cell><cell cols="2">0.05</cell><cell>0.032</cell></row><row><cell>lire042fren</cell><cell>39</cell><cell>155</cell><cell>6</cell><cell>0</cell><cell></cell><cell></cell><cell>19.50</cell><cell>20.00</cell><cell>15.00</cell><cell>0.00</cell><cell cols="2">0.00</cell><cell>0.075</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">given correct answers</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>run</cell><cell cols="2">Definition (#) org [11] per [9]</cell><cell>loc [28]</cell><cell>man [15]</cell><cell cols="2">mea [20]</cell><cell cols="2">Factoid (#) obj [12] org [20]</cell><cell>oth [27]</cell><cell>per [28]</cell><cell>tim [30]</cell><cell cols="2">Total [200] # %</cell></row><row><cell>bgas041bgen</cell><cell>2</cell><cell>3</cell><cell>5</cell><cell>2</cell><cell>1</cell><cell></cell><cell>2</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>4</cell><cell>26</cell><cell>13.00</cell></row><row><cell>dfki041deen</cell><cell>4</cell><cell>0</cell><cell>10</cell><cell>2</cell><cell>2</cell><cell></cell><cell>1</cell><cell>5</cell><cell>5</cell><cell>6</cell><cell>12</cell><cell>47</cell><cell>23.50</cell></row><row><cell>dltg041fren</cell><cell>3</cell><cell>3</cell><cell>8</cell><cell>5</cell><cell>2</cell><cell></cell><cell>1</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>9</cell><cell>38</cell><cell>19.00</cell></row><row><cell>dltg042fren</cell><cell>3</cell><cell>3</cell><cell>4</cell><cell>3</cell><cell>1</cell><cell></cell><cell>1</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>8</cell><cell>29</cell><cell>14.50</cell></row><row><cell>edin041deen</cell><cell>1</cell><cell>3</cell><cell>6</cell><cell>2</cell><cell>0</cell><cell></cell><cell>0</cell><cell>2</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>28</cell><cell>14.00</cell></row><row><cell>edin041fren</cell><cell>0</cell><cell>1</cell><cell>7</cell><cell>3</cell><cell>1</cell><cell></cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>3</cell><cell>11</cell><cell>33</cell><cell>16.50</cell></row><row><cell>edin042deen</cell><cell>1</cell><cell>4</cell><cell>6</cell><cell>4</cell><cell>1</cell><cell></cell><cell>2</cell><cell>2</cell><cell>5</cell><cell>3</cell><cell>6</cell><cell>34</cell><cell>17.00</cell></row><row><cell>edin042fren</cell><cell>0</cell><cell>3</cell><cell>7</cell><cell>4</cell><cell>3</cell><cell></cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>4</cell><cell>12</cell><cell>40</cell><cell>20.00</cell></row><row><cell>hels041fien 11</cell><cell>0</cell><cell>0</cell><cell>3</cell><cell>0</cell><cell>2</cell><cell></cell><cell>0</cell><cell>5</cell><cell>4</cell><cell>5</cell><cell>2</cell><cell>21</cell><cell>10.88</cell></row><row><cell>irst041iten</cell><cell>0</cell><cell>5</cell><cell>11</cell><cell>0</cell><cell>1</cell><cell></cell><cell>0</cell><cell>6</cell><cell>3</cell><cell>8</cell><cell>11</cell><cell>45</cell><cell>22.50</cell></row><row><cell>irst042iten</cell><cell>0</cell><cell>5</cell><cell>5</cell><cell>0</cell><cell>1</cell><cell></cell><cell>0</cell><cell>2</cell><cell>5</cell><cell>6</cell><cell>11</cell><cell>35</cell><cell>17.50</cell></row><row><cell>lire041fren</cell><cell>3</cell><cell>1</cell><cell>9</cell><cell>0</cell><cell>1</cell><cell></cell><cell>0</cell><cell>3</cell><cell>0</cell><cell>1</cell><cell>4</cell><cell>22</cell><cell>11.00</cell></row><row><cell>lire042fren</cell><cell>2</cell><cell>1</cell><cell>13</cell><cell>0</cell><cell>1</cell><cell></cell><cell>0</cell><cell>4</cell><cell>1</cell><cell>6</cell><cell>11</cell><cell>39</cell><cell>19.50</cell></row><row><cell>combination</cell><cell>7</cell><cell>5</cell><cell>26</cell><cell>6</cell><cell>7</cell><cell></cell><cell>5</cell><cell>18</cell><cell>10</cell><cell>22</cell><cell>24</cell><cell cols="2">130 65.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,125.58,709.95,344.14,8.74"><head>Table 5 :</head><label>5</label><figDesc>Results of the bilingual English runs, according to answer types of questions.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,77.04,335.14,436.14,242.12"><head>Table 6 :</head><label>6</label><figDesc>Results of the monolingual and bilingual French runs.</figDesc><table coords="9,77.04,335.14,436.14,223.48"><row><cell></cell><cell>Right</cell><cell>Wrong</cell><cell>ineXact</cell><cell>Unsupported</cell><cell>Overall</cell><cell>Accuracy</cell><cell>Accuracy</cell><cell></cell><cell></cell><cell>Confidence</cell></row><row><cell>Run Name</cell><cell>answers</cell><cell>answers</cell><cell>answers</cell><cell>answers</cell><cell>Accuracy</cell><cell>over F</cell><cell>over D</cell><cell></cell><cell></cell><cell>weighted</cell></row><row><cell></cell><cell>(#)</cell><cell>(#)</cell><cell>(#)</cell><cell>(#)</cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell><cell cols="2">Precision Recall</cell><cell>Score</cell></row><row><cell>gine041bgfr</cell><cell>13</cell><cell>182</cell><cell>5</cell><cell>0</cell><cell>6.50</cell><cell>6.67</cell><cell>5.00</cell><cell>0.10</cell><cell>0.50</cell><cell>0.051</cell></row><row><cell>gine041defr</cell><cell>29</cell><cell>161</cell><cell>10</cell><cell>0</cell><cell>14.50</cell><cell>14.44</cell><cell>15.00</cell><cell>0.15</cell><cell>0.20</cell><cell>0.079</cell></row><row><cell>gine041enfr</cell><cell>18</cell><cell>170</cell><cell>12</cell><cell>0</cell><cell>9.00</cell><cell>8.89</cell><cell>10.00</cell><cell>0.05</cell><cell>0.10</cell><cell>0.033</cell></row><row><cell>gine041esfr</cell><cell>27</cell><cell>165</cell><cell>8</cell><cell>0</cell><cell>13.50</cell><cell>14.44</cell><cell>5.00</cell><cell>0.12</cell><cell>0.15</cell><cell>0.056</cell></row><row><cell>gine041frfr</cell><cell>27</cell><cell>160</cell><cell>13</cell><cell>0</cell><cell>13.50</cell><cell>13.89</cell><cell>10.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.048</cell></row><row><cell>gine041itfr</cell><cell>25</cell><cell>165</cell><cell>10</cell><cell>0</cell><cell>12.50</cell><cell>13.33</cell><cell>5.00</cell><cell>0.15</cell><cell>0.30</cell><cell>0.049</cell></row><row><cell>gine041nlfr</cell><cell>20</cell><cell>169</cell><cell>11</cell><cell>0</cell><cell>10.00</cell><cell>10.00</cell><cell>10.00</cell><cell>0.12</cell><cell>0.20</cell><cell>0.044</cell></row><row><cell>gine041ptfr</cell><cell>25</cell><cell>169</cell><cell>6</cell><cell>0</cell><cell>12.50</cell><cell>12.22</cell><cell>15.00</cell><cell>0.11</cell><cell>0.15</cell><cell>0.044</cell></row><row><cell>gine042bgfr</cell><cell>13</cell><cell>180</cell><cell>7</cell><cell>0</cell><cell>6.50</cell><cell>6.11</cell><cell>10.00</cell><cell>0.10</cell><cell>0.35</cell><cell>0.038</cell></row><row><cell>gine042defr</cell><cell>34</cell><cell>154</cell><cell>12</cell><cell>0</cell><cell>17.00</cell><cell>15.56</cell><cell>30.00</cell><cell>0.23</cell><cell>0.20</cell><cell>0.097</cell></row><row><cell>gine042enfr</cell><cell>27</cell><cell>164</cell><cell>9</cell><cell>0</cell><cell>13.50</cell><cell>12.22</cell><cell>25.00</cell><cell>0.06</cell><cell>0.10</cell><cell>0.051</cell></row><row><cell>gine042esfr</cell><cell>34</cell><cell>162</cell><cell>4</cell><cell>0</cell><cell>17.00</cell><cell>17.22</cell><cell>15.00</cell><cell>0.11</cell><cell>0.10</cell><cell>0.075</cell></row><row><cell>gine042frfr</cell><cell>49</cell><cell>145</cell><cell>6</cell><cell>0</cell><cell>24.50</cell><cell>23.89</cell><cell>30.00</cell><cell>0.09</cell><cell>0.05</cell><cell>0.114</cell></row><row><cell>gine042itfr</cell><cell>29</cell><cell>164</cell><cell>7</cell><cell>0</cell><cell>14.50</cell><cell>15.56</cell><cell>5.00</cell><cell>0.14</cell><cell>0.30</cell><cell>0.054</cell></row><row><cell>gine042nlfr</cell><cell>29</cell><cell>156</cell><cell>15</cell><cell>0</cell><cell>14.50</cell><cell>13.33</cell><cell>25.00</cell><cell>0.14</cell><cell>0.20</cell><cell>0.065</cell></row><row><cell>gine042ptfr</cell><cell>29</cell><cell>164</cell><cell>7</cell><cell>0</cell><cell>14.50</cell><cell>13.33</cell><cell>25.00</cell><cell>0.10</cell><cell>0.15</cell><cell>0.056</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="10,76.26,96.82,438.85,244.89"><head>Table 7 :</head><label>7</label><figDesc>Results of the monolingual and bilingual French runs, according to answer types of questions.</figDesc><table coords="10,76.26,96.82,438.85,226.01"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">given correct answers</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>run</cell><cell cols="2">Definition (#) org [8] per [12]</cell><cell>loc [29]</cell><cell>man [14]</cell><cell>mea [28]</cell><cell cols="2">Factoid (#) obj [15] org [20]</cell><cell>oth [21]</cell><cell>per [32]</cell><cell>tim [21]</cell><cell cols="2">Total [200] # %</cell></row><row><cell>gine041bgfr</cell><cell>0</cell><cell>1</cell><cell>1</cell><cell>3</cell><cell>2</cell><cell>2</cell><cell>1</cell><cell>1</cell><cell>2</cell><cell>0</cell><cell>13</cell><cell>6.50</cell></row><row><cell>gine041defr</cell><cell>0</cell><cell>3</cell><cell>6</cell><cell>0</cell><cell>5</cell><cell>3</cell><cell>4</cell><cell>2</cell><cell>4</cell><cell>2</cell><cell>29</cell><cell>14.50</cell></row><row><cell>gine041enfr</cell><cell>0</cell><cell>2</cell><cell>5</cell><cell>0</cell><cell>4</cell><cell>1</cell><cell>0</cell><cell>1</cell><cell>3</cell><cell>2</cell><cell>18</cell><cell>9.00</cell></row><row><cell>gine041esfr</cell><cell>0</cell><cell>1</cell><cell>7</cell><cell>0</cell><cell>4</cell><cell>3</cell><cell>3</cell><cell>2</cell><cell>4</cell><cell>3</cell><cell>27</cell><cell>13.50</cell></row><row><cell>gine041frfr</cell><cell>0</cell><cell>2</cell><cell>8</cell><cell>0</cell><cell>8</cell><cell>0</cell><cell>1</cell><cell>3</cell><cell>2</cell><cell>3</cell><cell>27</cell><cell>13.50</cell></row><row><cell>gine041itfr</cell><cell>0</cell><cell>1</cell><cell>3</cell><cell>1</cell><cell>5</cell><cell>3</cell><cell>4</cell><cell>2</cell><cell>3</cell><cell>3</cell><cell>25</cell><cell>12.50</cell></row><row><cell>gine041nlfr</cell><cell>0</cell><cell>2</cell><cell>6</cell><cell>1</cell><cell>5</cell><cell>1</cell><cell>1</cell><cell>2</cell><cell>1</cell><cell>1</cell><cell>20</cell><cell>10.00</cell></row><row><cell>gine041ptfr</cell><cell>0</cell><cell>3</cell><cell>5</cell><cell>0</cell><cell>5</cell><cell>2</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>25</cell><cell>12.50</cell></row><row><cell>gine042bgfr</cell><cell>0</cell><cell>2</cell><cell>2</cell><cell>1</cell><cell>2</cell><cell>2</cell><cell>0</cell><cell>2</cell><cell>2</cell><cell>0</cell><cell>13</cell><cell>6.50</cell></row><row><cell>gine042defr</cell><cell>0</cell><cell>6</cell><cell>7</cell><cell>0</cell><cell>5</cell><cell>3</cell><cell>3</cell><cell>2</cell><cell>6</cell><cell>2</cell><cell>34</cell><cell>17.00</cell></row><row><cell>gine042enfr</cell><cell>0</cell><cell>5</cell><cell>7</cell><cell>0</cell><cell>5</cell><cell>1</cell><cell>2</cell><cell>1</cell><cell>4</cell><cell>2</cell><cell>27</cell><cell>13.50</cell></row><row><cell>gine042esfr</cell><cell>0</cell><cell>3</cell><cell>8</cell><cell>0</cell><cell>4</cell><cell>2</cell><cell>5</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>34</cell><cell>17.00</cell></row><row><cell>gine042frfr</cell><cell>0</cell><cell>6</cell><cell>10</cell><cell>0</cell><cell>9</cell><cell>1</cell><cell>6</cell><cell>6</cell><cell>4</cell><cell>7</cell><cell>49</cell><cell>24.50</cell></row><row><cell>gine042itfr</cell><cell>0</cell><cell>1</cell><cell>5</cell><cell>1</cell><cell>4</cell><cell>3</cell><cell>4</cell><cell>3</cell><cell>4</cell><cell>4</cell><cell>29</cell><cell>14.50</cell></row><row><cell>gine042nlfr</cell><cell>0</cell><cell>5</cell><cell>5</cell><cell>0</cell><cell>7</cell><cell>2</cell><cell>2</cell><cell>4</cell><cell>3</cell><cell>1</cell><cell>29</cell><cell>14.50</cell></row><row><cell>gine042ptfr</cell><cell>0</cell><cell>5</cell><cell>5</cell><cell>0</cell><cell>5</cell><cell>2</cell><cell>2</cell><cell>3</cell><cell>3</cell><cell>4</cell><cell>29</cell><cell>14.50</cell></row><row><cell>combination</cell><cell>0</cell><cell>7</cell><cell>19</cell><cell>3</cell><cell>17</cell><cell>5</cell><cell>8</cell><cell>8</cell><cell>11</cell><cell>9</cell><cell>97</cell><cell>48.50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="10,70.92,559.77,445.97,124.14"><head>Table 8 :</head><label>8</label><figDesc>Results of the monolingual German runs.Table9shows the results for each answer type.</figDesc><table coords="10,80.88,609.76,436.01,74.15"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">given correct answers</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Run</cell><cell cols="2">Definition (#) org [11] per [9]</cell><cell cols="7">Factoid (#) loc per [23]</cell><cell>tim [23]</cell><cell cols="2">Total [197] #</cell><cell>%</cell></row><row><cell>FUHA041-DEDE</cell><cell>6</cell><cell>5</cell><cell>12</cell><cell>4</cell><cell>4</cell><cell>4</cell><cell>5</cell><cell>7</cell><cell>10</cell><cell>10</cell><cell>67</cell><cell cols="2">34.01</cell></row><row><cell>dfki041dede</cell><cell>0</cell><cell>0</cell><cell>8</cell><cell>2</cell><cell>4</cell><cell>2</cell><cell>8</cell><cell>4</cell><cell>9</cell><cell>13</cell><cell>50</cell><cell cols="2">25.38</cell></row><row><cell>combination</cell><cell>6</cell><cell>5</cell><cell>14</cell><cell>4</cell><cell>5</cell><cell>4</cell><cell>11</cell><cell>8</cell><cell>13</cell><cell>16</cell><cell>86</cell><cell cols="2">43.65</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="10,117.24,694.05,360.84,8.74"><head>Table 9 :</head><label>9</label><figDesc>Results of the monolingual German runs, according to answer types of questions.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="11,198.18,363.21,198.97,8.74"><head>Table 10 :</head><label>10</label><figDesc>Results of the monolingual Italian runs.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="11,75.06,496.06,434.81,102.21"><head>Table 11 :</head><label>11</label><figDesc>Results of the monolingual Italian runs, according to answer types of questions.</figDesc><table coords="11,75.06,496.06,434.81,84.11"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">given correct answers</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Run</cell><cell cols="2">Definition (#) org [11] per [9]</cell><cell>loc [25]</cell><cell>man [12]</cell><cell>mea [30]</cell><cell cols="2">Factoid (#) obj [10] org [17]</cell><cell>oth [33]</cell><cell>per [28]</cell><cell>tim [25]</cell><cell cols="2">Total [200] #</cell><cell>%</cell></row><row><cell>ILCP-QA-ITIT</cell><cell>5</cell><cell>5</cell><cell>9</cell><cell>4</cell><cell>3</cell><cell>2</cell><cell>2</cell><cell>4</cell><cell>5</cell><cell>12</cell><cell>51</cell><cell cols="2">25.50</cell></row><row><cell>irst041itit</cell><cell>5</cell><cell>3</cell><cell>8</cell><cell>1</cell><cell>6</cell><cell>3</cell><cell>5</cell><cell>3</cell><cell>8</cell><cell>14</cell><cell>56</cell><cell cols="2">28.00</cell></row><row><cell>irst042itit</cell><cell>5</cell><cell>3</cell><cell>7</cell><cell>0</cell><cell>3</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>8</cell><cell>12</cell><cell>44</cell><cell cols="2">22.00</cell></row><row><cell>combination</cell><cell>8</cell><cell>7</cell><cell>12</cell><cell>4</cell><cell>8</cell><cell>4</cell><cell>7</cell><cell>6</cell><cell>13</cell><cell>19</cell><cell>88</cell><cell cols="2">44.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="11,78.48,292.06,441.18,61.78"><head>Table 12 :</head><label>12</label><figDesc>Results of the monolingual Portuguese runs.Table13shows the results for each answer type.</figDesc><table coords="11,422.88,294.28,48.60,6.96"><row><cell>NIL Accuracy</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" coords="12,108.66,299.01,378.03,8.74"><head>Table 13 :</head><label>13</label><figDesc>Results of the monolingual Portuguese runs, according to answer types of questions.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16" coords="12,76.20,81.58,441.96,662.84"><head>Table 15 :</head><label>15</label><figDesc>Results of the monolingual Spanish runs, according to answer types of questions.</figDesc><table coords="12,414.30,84.82,48.60,6.96"><row><cell>NIL Accuracy</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The authors would like to thank <rs type="person">Donna Harman</rs> for her valuable feedback and suggestions in designing the track, and <rs type="person">Ellen Voorhees</rs> for providing the NIST software for the assessment of the submitted runs. <rs type="person">Gregor Erbach</rs> wishes to thank the <rs type="funder">German Federal Ministry of Education and Research (BMBF)</rs> through the project <rs type="projectName">COLLATE II</rs> (<rs type="grantNumber">01 IN C02</rs>), that supported the work. <rs type="person">Bernardo Magnini</rs> and <rs type="person">Alessandro Vallin</rs> have been partially supported by the <rs type="projectName">WEBFAQ</rs> project funded by the <rs type="funder">Autonomous Province of Trento</rs>. In addition, they would like to thank <rs type="person">Danilo Giampiccolo</rs> and <rs type="person">Oleksandr Vagin</rs> for their help in the track preparation. <rs type="person">Maarten de Rijke</rs> was supported by the <rs type="funder">Netherlands Organization for Scientific Research (NWO)</rs> under project numbers <rs type="grantNumber">612-13-001</rs>, <rs type="grantNumber">365-20-005</rs>, <rs type="grantNumber">612.069.006</rs>, <rs type="grantNumber">612.000.106</rs>, <rs type="grantNumber">220-80-001</rs>, <rs type="grantNumber">612.000.207</rs>, <rs type="grantNumber">612.066.302</rs>, and <rs type="grantNumber">264-70-050</rs>. <rs type="person">Paulo Rocha</rs> was supported by the <rs type="funder">Portuguese Fundação para a Ciência e Tecnologia</rs>, through grant <rs type="grantNumber">POSI/PLP/43931/2001</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_er8CkHJ">
					<idno type="grant-number">01 IN C02</idno>
					<orgName type="project" subtype="full">COLLATE II</orgName>
				</org>
				<org type="funded-project" xml:id="_U4j8YBQ">
					<orgName type="project" subtype="full">WEBFAQ</orgName>
				</org>
				<org type="funding" xml:id="_VR9EafP">
					<idno type="grant-number">612-13-001</idno>
				</org>
				<org type="funding" xml:id="_gPUEb4V">
					<idno type="grant-number">365-20-005</idno>
				</org>
				<org type="funding" xml:id="_BmJY8NY">
					<idno type="grant-number">612.069.006</idno>
				</org>
				<org type="funding" xml:id="_php7xSM">
					<idno type="grant-number">612.000.106</idno>
				</org>
				<org type="funding" xml:id="_A7jNbvf">
					<idno type="grant-number">220-80-001</idno>
				</org>
				<org type="funding" xml:id="_YWb2Bju">
					<idno type="grant-number">612.000.207</idno>
				</org>
				<org type="funding" xml:id="_h6cs3sd">
					<idno type="grant-number">612.066.302</idno>
				</org>
				<org type="funding" xml:id="_Zm5g9VJ">
					<idno type="grant-number">264-70-050</idno>
				</org>
				<org type="funding" xml:id="_n6aZruT">
					<idno type="grant-number">POSI/PLP/43931/2001</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NIL Accuracy</head><p>Since, as Table <ref type="table" coords="12,160.55,534.51,10.03,8.74">15</ref> shows, some systems performed better for certain types of questions, the following question arises: why do we not reward specialisation? This issue has been explored in the Pilot Question Answering Task <ref type="bibr" coords="12,141.68,557.55,10.63,8.74" target="#b3">[4]</ref>, in which the confidence score has been taken into account in the evaluation measure in order to reward systems' self-knowledge and answer validation when responding to different types of questions. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="14,91.88,354.09,432.59,8.74;14,91.92,365.61,432.55,8.74;14,91.92,377.07,432.50,8.74;14,91.92,388.59,205.56,8.74;14,92.22,400.11,321.14,8.74" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Chaudhri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Israel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Jacquemin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Maiorano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ogden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Shrihari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strzalkowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weishedel</surname></persName>
		</author>
		<ptr target="http://www-nlpir.nist.gov/projects/duc/papers/qa.Roadmap-paper_v2.doc" />
		<title level="m" coord="14,313.91,377.07,210.51,8.74;14,91.92,388.59,170.34,8.74">Issues Tasks and Program Structures to Roadmap Research in Question &amp; Answering (Q&amp;A)</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,91.86,423.09,240.05,8.74;14,91.92,434.55,184.96,8.74" xml:id="b1">
	<monogr>
		<ptr target="http://clef-qa.itc.it/2004/guidelines.html" />
		<title level="m" coord="14,97.65,423.09,200.87,8.74">CLEF 2004 Question Answering Track Guidelines</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,92.20,457.59,395.59,8.74;14,91.92,469.05,188.40,8.74" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="14,144.92,457.59,309.76,8.74">Evaluating Human Question Answering Performance under Time Constraints</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Erbach</surname></persName>
		</author>
		<ptr target="http://purl.org/net/gregor/pub/human-qa/" />
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,91.89,492.09,425.56,8.74;14,91.92,503.55,280.93,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="14,251.44,492.09,182.95,8.74">Question Answering Pilot Task at CLEF 2004</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,456.13,492.09,61.33,8.74;14,91.92,503.55,117.69,8.74">Working Notes for the CLEF 2004 Workshop</title>
		<meeting><address><addrLine>Bath, United Kingdom</addrLine></address></meeting>
		<imprint>
			<publisher>Press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,91.89,526.59,432.46,8.74;14,91.92,538.05,432.55,8.74;14,91.92,549.57,144.50,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="14,96.93,538.05,262.78,8.74">The Multiple Language Question Answering Track at CLEF 2003</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Romagnoli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vallin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Peinado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Verdejo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,464.58,538.05,59.89,8.74;14,91.92,549.57,44.92,8.74">Proceedings of CLEF 2003</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<meeting>CLEF 2003</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,91.89,572.55,432.46,8.74;14,91.92,584.07,432.51,8.74;14,91.92,595.59,365.98,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="14,96.24,584.07,320.91,8.74">Creating the DISEQuA Corpus: a Test Set for Multilingual Question Answering</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Romagnoli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vallin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Peinado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Verdejo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,251.19,595.59,107.15,8.74">Proceedings of CLEF 2003</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<meeting>CLEF 2003</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,91.85,618.57,396.25,8.74;14,91.92,630.09,371.96,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="14,167.76,618.57,224.30,8.74">Overview of the TREC 2002 Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,413.48,618.57,74.62,8.74;14,91.92,630.09,199.49,8.74">Proceedings of the Eleventh Text Retrieval Conference (TREC 2002)</title>
		<meeting>the Eleventh Text Retrieval Conference (TREC 2002)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="500" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,91.85,653.07,430.32,8.74;14,91.92,664.53,333.90,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="14,167.79,653.07,224.29,8.74">Overview of the TREC 2003 Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,413.49,653.07,108.68,8.74;14,91.92,664.53,161.54,8.74">Proceedings of the Twelfth Text Retrieval Conference (TREC 2003)</title>
		<meeting>the Twelfth Text Retrieval Conference (TREC 2003)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="500" to="255" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
