<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,106.78,148.86,389.46,15.15">Question Answering Pilot Task at CLEF 2004</title>
				<funder ref="#_bMurZz8">
					<orgName type="full">Spanish Ministry of Science and Technology</orgName>
				</funder>
				<funder ref="#_2crqPSr #_VnEjUVr">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,194.00,182.75,57.58,8.74"><forename type="first">Jesús</forename><surname>Herrera</surname></persName>
							<email>jesus.herrera@lsi.uned.es</email>
							<affiliation key="aff0">
								<orgName type="department">Sistemas Informáticos</orgName>
								<orgName type="institution">UNED</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,259.37,182.75,66.10,8.74"><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
							<email>anselmo@lsi.uned.es</email>
							<affiliation key="aff0">
								<orgName type="department">Sistemas Informáticos</orgName>
								<orgName type="institution">UNED</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,348.18,182.75,60.82,8.74"><forename type="first">Felisa</forename><surname>Verdejo</surname></persName>
							<email>felisa@lsi.uned.es</email>
							<affiliation key="aff0">
								<orgName type="department">Sistemas Informáticos</orgName>
								<orgName type="institution">UNED</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,193.72,196.70,72.83,8.74"><forename type="first">Dpto</forename><surname>Lenguajes</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sistemas Informáticos</orgName>
								<orgName type="institution">UNED</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,106.78,148.86,389.46,15.15">Question Answering Pilot Task at CLEF 2004</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DE596CDB4706F5F5300B4E063B755734</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A Pilot Question Answering Task has been activated in the Cross-Language Evaluation Forum 2004 with a twofold objective. In the first place, the evaluation of Question Answering systems when they have to answer conjunctive lists, disjunctive lists and questions with temporal restrictions. In the second place, the evaluation of systems' capability to give an accurate self-scoring about the confidence on their answers. In this way, two measures have been designed to be applied on all these different types of questions and to reward systems that give a confidence score with a high correlation with the human assessments. The forty eight runs submitted to the Question Answering Main Track have been taken as a case of study, confirming that some systems are able to give a very accurate score and showing how the measures reward this fact.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A Pilot Question Answering (QA) Task has been activated this year within the Main QA Track of the CLEF<ref type="foot" coords="1,145.73,436.32,3.97,6.12" target="#foot_0">1</ref> 2004 competition. The Pilot Task aims at investigating how QA systems are able to cope with another type of questions than the ones posed in the Main Track. To accomplish it, a set of questions has been prepared and new evaluation measures have been proposed.</p><p>Few questions were similar to those posed in the Main Track (factoid and definition questions) although they were selected with more than one correct and distinct answer. Questions whose answer is a list of items were also posed, following TREC<ref type="foot" coords="1,349.78,496.10,3.97,6.12" target="#foot_1">2</ref> and NTCIR<ref type="foot" coords="1,411.59,496.10,3.97,6.12" target="#foot_2">3</ref> previous experiences. Finally, more than half of the questions in the Pilot Task aim at dealing with temporal restrictions.</p><p>The evaluation measure proposed for this Pilot Task has been designed to take into consideration all these types of questions and, simultaneously, reward systems that, even focusing their attention in a few types of questions, are able to obtain very accurate results, with a good answer validation and a good confidence score.</p><p>In the present edition, the Pilot Task has been activated only for Spanish and has been carried out simultaneously with the Main QA Track. Participants in the Pilot Task have made a special effort to accomplish the extra work.</p><p>Section 2 describes the task and the different types of questions, including those with temporal restrictions. Section 3 presents some criteria to design the evaluation measure and presents the K and K1 measures. The results for the Main QA Track at CLEF <ref type="bibr" coords="1,368.75,629.18,10.52,8.74" target="#b5">[6]</ref> are taken as a case of study to discuss and compare these measures with the previous ones used at TREC, NTCIR and CLEF. Section 4 presents the results obtained by participants in the Pilot Task and, finally, Section 5 points out some conclusions and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Definition</head><p>The QA Pilot Task followed the rules stated in the QA Main Track guidelines except for the source and the target languages, the type and number of questions, and the evaluation measure.</p><p>One hundred of questions were posed in Spanish and the corpus used was the EFE Spanish press agency collection of news from 1994 and 1995. The questions of this Pilot Task were distributed throughout the following types: factoid (18), definition <ref type="bibr" coords="2,381.79,181.66,11.63,8.74" target="#b1">(2)</ref>, conjunctive list (20), temporally restricted by date (20), temporally restricted by period (20), and temporally restricted by event (20 nested questions). A little amount of questions had no answer in the document collection (2 NIL factoid questions). As usual, a question was assumed to have no answer when neither human assessors nor participating systems could find one.</p><p>Ideally, QA systems should tend to give a unique answer for each question but, however, there exist some questions whose answer depends on the context or evolves in time. In these cases, disjunctive lists are obtained, that is, lists of different and correct items representing a disjunction of concepts. The decision of which one of them is the most correct is strongly dependant on the user's information need, text errors, consistency between different texts (specially in the news domain), etcetera. Therefore, being able to obtain all the possible correct and distinct answers for a question seems to be a desirable feature for open domain QA systems.</p><p>For this reason, there was no limit for the number of answers at the Pilot Task, but one answer for each question must be given at least. If systems believed that it was no response to a question in the corpus, they had to answer NIL.</p><p>In the conjunctive list type of questions, a determined or undetermined quantity of items is required for conforming an only answer. A conjunctive list is a series of items representing a conjunction of concepts. For the Pilot Task, the goal was to obtain the largest amount of different items within each answer.</p><p>Three subtypes of temporally restricted questions have been proposed at the Pilot Task, and three moments with regard to the restriction (before, during or after the temporal restriction):</p><p>• Restriction by Date, where a precise date contextualises the question, which can refer either to a particular moment, before or after. A date could consist in a day, a month, a year, etcetera, depending on the question. Examples:</p><p>-T ES ES 0011 ¿Qué sistema de gobierno tenía Andorra hasta mayo de 1993?</p><p>-T ES ES 0014 ¿Quién visitó Toledo el 22 de febrero de 1994?</p><p>• Restriction by Period. In this case, questions are referred explicitly to a whole period or range of time. A period could be expressed by a pair of dates delimiting it, or by a name accepted as designation of some important periods as, for example, Cuaresma 4 . Examples:</p><p>-T ES ES 0086 ¿Quién reinó en España durante el Siglo de Oro de su literatura?</p><p>-T ES ES 0037 ¿Quién gobernó en Bolivia entre el 17 de julio de 1980 y el 4 de agosto de 1981?</p><p>• Event restriction, that implies an embedded or implicit extra question because it is necessary to answer the nested question to determine the temporal restriction. Then, the temporal restriction refers to the moment in which the second event occurred. For example:</p><p>-T ES ES 0098 ¿Quién fue el rey de Bélgica inmediatamente antes de la coronación de Alberto II?</p><p>-T ES ES 0079 ¿Qué revolución estudiantil surgió en Francia al año siguiente de la Guerra de los Seis Días?</p><p>The degree of inference necessary to solve the temporal restrictions was not the same for all the questions. In some questions a reference to the temporal restriction could be found in the same document, while in other questions it was necessary to accede to other documents to temporally locate the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation Measure</head><p>The evaluation measure has been designed in order to reward systems that return as many different and correct answers as possible to each question but, at the same time, punishing incorrect answers. Two reasons motivate the negative adding for the incorrect answers: First, it is assumed that a user of a QA system would prefer a void answer rather than an incorrect one. Systems must validate their answers and must give an accurate confidence score. Second, since there was no limit in the number of answers, systems must calibrate the risk of giving too much incorrect ones. The effect was that no more than three answers per question were given.</p><p>In order to evaluate systems' self-scoring, a mandatory confidence score given by means of a real number ranged between 0 and 1, was requested. 0 meant that the system had no evidence on the correctness of the answer, and 1 meant that the system was totally sure about its correctness.</p><p>The evaluation measure has been designed to reward systems that:</p><p>• answer as many questions as possible,</p><p>• give as many different right answers to each question as possible,</p><p>• give the smaller number of wrong answers to each question,</p><p>• assign higher values of the score to right answers,</p><p>• assign lower values of the score to wrong answers,</p><p>• give answer to questions that have less known answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The K -measure</head><p>According to the criteria above, the evaluation measure is defined as follows:</p><formula xml:id="formula_0" coords="3,90.00,521.41,423.44,35.92">K(sys) = 1 #questions • i∈questions r∈answers(sys,i) score(r) • eval(r) max {R(i), answered(sys, i)} ; K(sys) ∈ R∧K(sys) ∈ [-1, 1]</formula><p>where R (i) is the total number of known answers to the question i that are correct and distinct; answered(sys,i) is the number of answers given by the system sys for the question i ; score (r) is the confidence score assigned by the system to the answer r ; eval (r) depends on the judgement given by a human assessor.</p><formula xml:id="formula_1" coords="3,192.02,625.18,212.78,34.61">eval (r) =    1 if r is judged as correct 0 if r is a repeated answer -1 if r is judged as incorrect</formula><p>When K (sys) equals 0 it matches with a system without knowledge that assigns 0 to the confidence score of all their answers. Therefore, K (sys) = 0 is established as a baseline and K -measure gives an idea about the system's knowledge.</p><p>The answers finding process, accomplished by human assessors, is strongly determined by the evaluation measure. In the case of K -measure the parameter R(i) requires a knowledge of all the correct and distinct answers contained in the corpus for each question. This fact introduces a very high cost in the pre-assessment process because it is not easy to ensure that, even with a human search, all distinct answers for each question have been found in a very large corpus. One alternative is to relax the pre-assessment process and consider only the set of different answers found by humans or systems along the process. Another alternative is to request only one answer per question and ignore recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The K1 -measure</head><p>A second measure, derived from the K -measure, is proposed to evaluate exercises when just one answer per question is requested (number of questions equals number of answers) or when the achievement of all the possible answers by the system is not outstanding for the exercise. That measure has been called K1 -measure (K -measure for systems giving 1 answer per question) and it is defined as follows:</p><formula xml:id="formula_2" coords="4,131.70,265.40,339.60,31.58">K1(sys) = r∈answers(sys) score(r) • eval(r) #questions ; K1(sys) ∈ R ∧ K1(sys) ∈ [-1, 1]</formula><p>where score (r) is the confidence score assigned by the system to the answer r and eval (r) depends on the judgement given by a human assessor.</p><formula xml:id="formula_3" coords="4,196.87,337.89,203.10,20.69">eval (r) = 1 if r is judged as correct -1 in other case</formula><p>Again, K1 (sys) = 0 is established as a baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison with Precedent Measures</head><p>Comparing K and K1 measures with other measures used in precedent QA evaluation exercises, the following differences and similarities have been found:</p><p>• Accuracy measure, commonly used in all evaluations <ref type="bibr" coords="4,357.11,446.04,10.76,8.74" target="#b0">[1]</ref> <ref type="bibr" coords="4,436.04,446.04,14.35,8.74" target="#b10">[11]</ref>, measures the precision in giving correct answers. But it does not take into account the confidence score, as in K and K1 measures, nor the recall when more than one answer per question is given, as in F-measure or K -measure.</p><formula xml:id="formula_4" coords="4,367.88,446.04,68.17,8.74">[2][3][7][8][9][10]</formula><p>• Mean F-measure, used in the QA Track at TREC 2003 <ref type="bibr" coords="4,371.86,501.83,15.49,8.74" target="#b10">[11]</ref> and in the QA Challenge at NTCIR 2002 <ref type="bibr" coords="4,175.71,513.79,9.97,8.74" target="#b0">[1]</ref>, gives a combination between precision and recall, generally the mean of both. As the K -measure, it is designed for systems that must give all the correct answers existing in the corpus for every question. The K -measure takes into account a combination of precision and recall by means of the max{R(i), answered(sys, i)} denominator. In addition, K and K1 measures include the confidence score into their calculations.</p><p>• Mean Reciprocal Rank, used in the QA Track at TREC <ref type="bibr" coords="4,372.25,581.53,10.67,8.74" target="#b6">[7]</ref>[8][9] <ref type="bibr" coords="4,404.27,581.53,14.23,8.74" target="#b9">[10]</ref>, in the QA Challenge at NTCIR 2002 <ref type="bibr" coords="4,188.08,593.49,10.52,8.74" target="#b0">[1]</ref> and in the QA Track at CLEF 2003 <ref type="bibr" coords="4,366.93,593.49,24.34,8.74">[2] [3]</ref>. It is designed for systems that give one or more answers per question, in a decreasing order of confidence. It rewards systems assigning a higher confidence to the correct answers. However, Mean Reciprocal Rank cannot evaluate systems that find several different and correct answers for the same question, and the incorrect answers are not considered as a worse case than the absence of answers.</p><p>• Confident-Weighted Score (CWS), used in the QA Track at TREC 2002 <ref type="bibr" coords="4,449.73,673.19,15.50,8.74" target="#b9">[10]</ref> and in the QA Track at CLEF 2004 <ref type="bibr" coords="4,229.80,685.14,10.51,8.74" target="#b5">[6]</ref> as a secondary measure. It is designed for systems that give only one answer per question. Answers are in a decreasing order of confidence and CWS rewards systems that give correct answers at the top of the ranking. Hence, correct answers in the lower zone of the ranking make a very poor contribution to the global valuation, and this contribution is determined by the ranking position instead of the system's self-scoring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Correlation Between Self-Scoring and Correctness</head><p>Since the confidence score has been included in the K -measure, a high correlation between selfscoring and correctness is expected to produce higher values of K. However, it is interesting to know separately the quality of the scoring given by every system. Hence, it is proposed the use of the correlation coefficient (r ) between self-scoring value (in range [0,1]) and the value associated to the human assessment: 1 for the correct answers and 0 otherwise. That is:</p><formula xml:id="formula_5" coords="5,163.46,194.71,276.09,24.22">r(sys) = σ assess(sys)score(sys) σ assess(sys) • σ score(sys) ; r(sys) ∈ R ∧ r(sys) ∈ [-1, 1]</formula><p>where assess(sys) and score(sys) are the two multidimensional variables containing the values of the human assessment and the confidence score for the system sys; σ assess(sys) , σ score(sys) are the typical deviations for assess(sys) and score(sys); σ assess(sys)score(sys) is the covariance between the two variables.</p><p>When a system assigns a score = 1 to its correct answers and score = 0 to the rest, it obtains a correlation coefficient r = 1, meaning that such a system has a perfect knowledge about the correctness of its response. A correlation coefficient equal to 0 indicates that score and correctness have no correlation. A negative value indicates that there is a certain correlation but in the other direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">A Case of Study</head><p>In the QA 2004 Main Track <ref type="bibr" coords="5,221.39,368.88,9.96,8.74" target="#b5">[6]</ref>, the confidence score has been requested in order to calculate the CWS as a secondary evaluation measure. This confidence score, together with the human assessments of all the submitted runs, permitted to study the effect of the K1 -measure in the ranking of systems, and to compare the official measures with this one. No conclusions should be stated about the quality of systems because they should not be compared across different target languages, and also because they did not develop any strategy in order to obtain good values of K1.</p><p>Table <ref type="table" coords="5,132.61,452.57,4.98,8.74" target="#tab_0">1</ref> shows the number of given correct answers, CWS, K1 and the correlation coefficient for all the systems participating in the QA at CLEF 2004 Main Track.</p><p>A higher correlation coefficient (higher score for the correct answers) brings associated better values of K1 for the same or similar number of given correct answers. For example, ptue041ptpt (r &gt; 0.5) has the 12th position in the ranking of given correct answers and reaches the 1st position for K1.</p><p>On the contrary, there are some interesting examples, as fuha041dede or dfki041deen, that have a low or even negative correlation coefficient and experiment a huge drop in the ranking of K1.</p><p>However, these systems obtain a very good CWS value, showing that CWS does not reward a good correlation between self-scoring and correctness. Why do these systems obtain good values of CWS? The reason can be found when looking at their answers in detail: they tune their score to obtain a better CWS and, obviously, not a better K1. For example, when they have not enough confidence in the answer, they return NIL with a score 1, ensuring 20 correct answers (the 20 NIL questions) very high weighted in the CWS measure. All wrong NIL answers (149, with score 1) affect negatively the correlation coefficient and also the K1 -measure. Adopting a K1 oriented strategy, they would obtain very good results. For example, if all NIL answers of fuha041dede had a score equal to 0 then the correlation coefficient would have been very high (r = 0.7385) and the system would have obtained again the first place in the ranking with K1 = 0.218. These systems are an example of how, with the current state-of-the-art, systems can give a very accurate self-scoring.</p><p>Since K1 depends on the number of correct given answers, a good correlation coefficient is not enough to obtain good results: the more correct answers given, the more quantity of positive components conforming the global calculation of K1. For example, to beat fuha041dede using the mentioned K1 -oriented strategy (K1 = 0.218), a system with perfect scoring (r=1) would need to answer correctly more than 40 questions. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,90.00,167.80,423.00,533.46"><head>Table 1 :</head><label>1</label><figDesc>Values and rankings for accuracy, CWS, K1, and correlation coefficient r, for all runs submitted to the Main QA Track at CLEF 2004 †CWS and r are Not Available because 0 was given as confident score for every answer.</figDesc><table coords="6,95.98,201.10,409.84,500.16"><row><cell></cell><cell cols="3">given correct answers</cell><cell cols="2">CWS</cell><cell>K1</cell><cell></cell><cell></cell></row><row><cell>run</cell><cell>#</cell><cell>%</cell><cell>ranking</cell><cell>value</cell><cell>ranking</cell><cell>value</cell><cell>ranking</cell><cell>r</cell></row><row><cell>uams042nlnl</cell><cell>91</cell><cell>45.50</cell><cell>1</cell><cell>0.3262</cell><cell>2</cell><cell>0.0078</cell><cell>2</cell><cell>0.1148</cell></row><row><cell>uams041nlnl</cell><cell>88</cell><cell>44</cell><cell>2</cell><cell>0.2841</cell><cell>3</cell><cell>0.0063</cell><cell>3</cell><cell>0.0987</cell></row><row><cell>uams041ennl</cell><cell>70</cell><cell>35</cell><cell>3</cell><cell>0.2222</cell><cell>4</cell><cell>0.0055</cell><cell>4</cell><cell>0.1105</cell></row><row><cell>fuha041dede</cell><cell>67</cell><cell>33.50</cell><cell>4</cell><cell>0.3284</cell><cell>1</cell><cell>-0.3271</cell><cell>27</cell><cell>0.0094</cell></row><row><cell>aliv042eses</cell><cell>65</cell><cell>32.50</cell><cell>5</cell><cell>0.1449</cell><cell>8</cell><cell>-0.0416</cell><cell>15</cell><cell>0.1711</cell></row><row><cell>aliv041eses</cell><cell>63</cell><cell>31.50</cell><cell>6</cell><cell>0.1218</cell><cell>9</cell><cell>-0.0500</cell><cell>16</cell><cell>0.1099</cell></row><row><cell>irst041itit</cell><cell>56</cell><cell>28</cell><cell>7</cell><cell>0.1556</cell><cell>7</cell><cell>-0.1853</cell><cell>19</cell><cell>0.2128</cell></row><row><cell>talp042eses</cell><cell>52</cell><cell>26</cell><cell>8</cell><cell>0.1029</cell><cell>12</cell><cell>-0.2252</cell><cell>20</cell><cell>-0.0366</cell></row><row><cell>dfki041dede</cell><cell>51</cell><cell>25.50</cell><cell>9..10</cell><cell>N/A  †</cell><cell>N/A</cell><cell>0</cell><cell>5..14</cell><cell>N/A</cell></row><row><cell>ilcp041itit</cell><cell>51</cell><cell>25.50</cell><cell>9..10</cell><cell>N/A</cell><cell>N/A</cell><cell>0</cell><cell>5..14</cell><cell>N/A</cell></row><row><cell>talp041eses</cell><cell>48</cell><cell>24</cell><cell>11</cell><cell>0.0878</cell><cell>15</cell><cell>-0.2464</cell><cell>22</cell><cell>-0.0483</cell></row><row><cell>ptue041ptpt</cell><cell>47</cell><cell>23.62</cell><cell>12</cell><cell>0.2162</cell><cell>5</cell><cell>0.0201</cell><cell>1</cell><cell>0.5169</cell></row><row><cell>dfki041deen</cell><cell>47</cell><cell>23.50</cell><cell>13</cell><cell>0.1771</cell><cell>6</cell><cell>-0.5131</cell><cell>45</cell><cell>-0.0453</cell></row><row><cell>inao041eses</cell><cell>45</cell><cell>22.50</cell><cell>14..15</cell><cell>N/A</cell><cell>N/A</cell><cell>0</cell><cell>5..14</cell><cell>N/A</cell></row><row><cell>irst041iten</cell><cell>45</cell><cell>22.50</cell><cell>14..15</cell><cell>0.1215</cell><cell>10</cell><cell>-0.2310</cell><cell>21</cell><cell>0.1411</cell></row><row><cell>irst042itit</cell><cell>44</cell><cell>22</cell><cell>16</cell><cell>0.1075</cell><cell>11</cell><cell>-0.3248</cell><cell>26</cell><cell>-0.0188</cell></row><row><cell>gine042frfr</cell><cell>42</cell><cell>21</cell><cell>17</cell><cell>0.0954</cell><cell>13</cell><cell>-0.3152</cell><cell>24</cell><cell>0.1917</cell></row><row><cell>edin042fren</cell><cell>40</cell><cell>20</cell><cell>18</cell><cell>0.0589</cell><cell>21</cell><cell>-0.4066</cell><cell>38</cell><cell>0.0004</cell></row><row><cell>lire042fren</cell><cell>39</cell><cell>19.50</cell><cell>19</cell><cell>0.0754</cell><cell>16</cell><cell>-0.1738</cell><cell>18</cell><cell>0.3707</cell></row><row><cell>dltg041fren</cell><cell>38</cell><cell>19</cell><cell>20</cell><cell>N/A</cell><cell>N/A</cell><cell>0</cell><cell>5..14</cell><cell>N/A</cell></row><row><cell>inao042eses</cell><cell>37</cell><cell>18.50</cell><cell>21</cell><cell>N/A</cell><cell>N/A</cell><cell>0</cell><cell>5..14</cell><cell>N/A</cell></row><row><cell>irst042iten</cell><cell>35</cell><cell>17.50</cell><cell>22</cell><cell>0.0751</cell><cell>17</cell><cell>-0.3300</cell><cell>28</cell><cell>0.0566</cell></row><row><cell>edin042deen</cell><cell>34</cell><cell>17</cell><cell>23</cell><cell>0.0527</cell><cell>25</cell><cell>-0.3556</cell><cell>30</cell><cell>0.1124</cell></row><row><cell>edin041fren</cell><cell>33</cell><cell>16.50</cell><cell>24</cell><cell>0.0570</cell><cell>22</cell><cell>-0.5336</cell><cell>46</cell><cell>-0.0560</cell></row><row><cell>gine042defr</cell><cell>32</cell><cell>16</cell><cell>25</cell><cell>0.0878</cell><cell>14</cell><cell>-0.3009</cell><cell>23</cell><cell>0.3040</cell></row><row><cell>gine042esfr</cell><cell>30</cell><cell>15</cell><cell>26</cell><cell>0.0635</cell><cell>19</cell><cell>-0.3757</cell><cell>32</cell><cell>0.1568</cell></row><row><cell>dltg042fren</cell><cell>29</cell><cell>14.50</cell><cell>27</cell><cell>N/A</cell><cell>N/A</cell><cell>0</cell><cell>5..14</cell><cell>N/A</cell></row><row><cell>edin041deen</cell><cell>28</cell><cell>14</cell><cell>28</cell><cell>0.0492</cell><cell>29</cell><cell>-0.5515</cell><cell>47</cell><cell>-0.0077</cell></row><row><cell>gine041defr</cell><cell>27</cell><cell>13.50</cell><cell>29..30</cell><cell>0.0714</cell><cell>18</cell><cell>-0.3945</cell><cell>34</cell><cell>0.2039</cell></row><row><cell>gine042itfr</cell><cell>27</cell><cell>13.50</cell><cell>29..30</cell><cell>0.0525</cell><cell>26</cell><cell>-0.4035</cell><cell>37</cell><cell>0.1361</cell></row><row><cell>bgas041bgen</cell><cell>26</cell><cell>13</cell><cell>31..33</cell><cell>0.0564</cell><cell>23</cell><cell>-0.3618</cell><cell>31</cell><cell>0.2023</cell></row><row><cell>gine041frfr</cell><cell>26</cell><cell>13</cell><cell>31..33</cell><cell>0.0470</cell><cell>32</cell><cell>-0.4523</cell><cell>40</cell><cell>0.1447</cell></row><row><cell>gine042nlfr</cell><cell>26</cell><cell>13</cell><cell>31..33</cell><cell>0.0607</cell><cell>20</cell><cell>-0.3884</cell><cell>33</cell><cell>0.1958</cell></row><row><cell>gine041esfr</cell><cell>25</cell><cell>12.50</cell><cell>34..36</cell><cell>0.0541</cell><cell>24</cell><cell>-0.4585</cell><cell>41</cell><cell>0.1051</cell></row><row><cell>gine042enfr</cell><cell>25</cell><cell>12.50</cell><cell>34..36</cell><cell>0.0481</cell><cell>30</cell><cell>-0.3306</cell><cell>29</cell><cell>0.2462</cell></row><row><cell>gine042ptfr</cell><cell>25</cell><cell>12.50</cell><cell>34..36</cell><cell>0.0508</cell><cell>28</cell><cell>-0.4028</cell><cell>36</cell><cell>0.1646</cell></row><row><cell>gine041itfr</cell><cell>23</cell><cell>11.50</cell><cell>37</cell><cell>0.0475</cell><cell>31</cell><cell>-0.4013</cell><cell>35</cell><cell>0.1262</cell></row><row><cell>sfnx042ptpt</cell><cell>22</cell><cell>11.06</cell><cell>38</cell><cell>N/A</cell><cell>N/A</cell><cell>0</cell><cell>5..14</cell><cell>N/A</cell></row><row><cell>cole041eses</cell><cell>22</cell><cell>11</cell><cell>39..41</cell><cell>N/A</cell><cell>N/A</cell><cell>0</cell><cell>5..14</cell><cell>N/A</cell></row><row><cell>gine041ptfr</cell><cell>22</cell><cell>11</cell><cell>39..41</cell><cell>0.0413</cell><cell>35</cell><cell>-0.4596</cell><cell>42</cell><cell>0.0970</cell></row><row><cell>lire041fren</cell><cell>22</cell><cell>11</cell><cell>39..41</cell><cell>0.0330</cell><cell>37</cell><cell>-0.3200</cell><cell>25</cell><cell>0.2625</cell></row><row><cell>hels041fien</cell><cell>21</cell><cell>10.61</cell><cell>42</cell><cell>0.0443</cell><cell>33</cell><cell>-0.1136</cell><cell>17</cell><cell>0.0359</cell></row><row><cell>mira041eses</cell><cell>18</cell><cell>9</cell><cell>43</cell><cell>N/A</cell><cell>N/A</cell><cell>0</cell><cell>5..14</cell><cell>N/A</cell></row><row><cell>gine041nlfr</cell><cell>17</cell><cell>8.50</cell><cell>44</cell><cell>0.0416</cell><cell>34</cell><cell>-0.4640</cell><cell>43</cell><cell>0.1850</cell></row><row><cell>gine041enfr</cell><cell>16</cell><cell>8</cell><cell>45</cell><cell>0.0313</cell><cell>38</cell><cell>-0.4511</cell><cell>39</cell><cell>0.1444</cell></row><row><cell>sfnx041ptpt</cell><cell>14</cell><cell>7.04</cell><cell>46</cell><cell>N/A</cell><cell>N/A</cell><cell>0</cell><cell>5..14</cell><cell>N/A</cell></row><row><cell>gine041bgfr</cell><cell>13</cell><cell>6.50</cell><cell>47..48</cell><cell>0.0514</cell><cell>27</cell><cell>-0.5603</cell><cell>48</cell><cell>0.1067</cell></row><row><cell>gine042bgfr</cell><cell>13</cell><cell>6.50</cell><cell>47..48</cell><cell>0.0380</cell><cell>36</cell><cell>-0.4945</cell><cell>44</cell><cell>0.0928</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,105.24,683.97,245.33,6.99"><p>Cross-Language Evaluation Forum, http://www.clef-campaign.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,105.24,693.48,175.55,6.99"><p>Text REtrieval Conference, http://trec.nist.gov</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="1,105.24,702.98,332.89,6.99"><p>NII-NACSIS Test Collection for IR Systems, http://research.nii.ac.jp/ntcir/index-en.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>6 Acknowledgements This work has been partially supported by the <rs type="funder">Spanish Ministry of Science and Technology</rs> within the following projects: <rs type="grantNumber">TIC-2002-10597-E</rs> <rs type="projectName">Organization of a Competitive Task for QA Systems</rs>; <rs type="grantNumber">TIC-2003-07158-104-01</rs> Answer Retrieval from <rs type="projectName">Digital Documents</rs>, R2D2; and <rs type="grantNumber">TIC-2003-07158-C04-02</rs> <rs type="projectName">Multilingual Answer Retrieval Systems and Evaluation, SyEMBRA</rs>.</p><p>We are grateful to <rs type="person">Julio Gonzalo</rs>, from <rs type="affiliation">UNED-NLP Group</rs>, and <rs type="person">Alessandro Vallin</rs>, from <rs type="affiliation">ITC-Irst (Italy</rs>), for their contributions to this work. In addition, we would like to thank the <rs type="institution">University of Alicante team</rs> for their effort in participating in the Pilot Task.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_bMurZz8">
					<idno type="grant-number">TIC-2002-10597-E</idno>
					<orgName type="project" subtype="full">Organization of a Competitive Task for QA Systems</orgName>
				</org>
				<org type="funded-project" xml:id="_2crqPSr">
					<idno type="grant-number">TIC-2003-07158-104-01</idno>
					<orgName type="project" subtype="full">Digital Documents</orgName>
				</org>
				<org type="funded-project" xml:id="_VnEjUVr">
					<idno type="grant-number">TIC-2003-07158-C04-02</idno>
					<orgName type="project" subtype="full">Multilingual Answer Retrieval Systems and Evaluation, SyEMBRA</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results of the Pilot Task</head><p>The data from the assessment process for the Pilot Task are shown in Table <ref type="table" coords="7,438.38,133.84,3.88,8.74">2</ref>. Only one run from the University of Alicante (UA) <ref type="bibr" coords="7,257.17,145.80,10.52,8.74" target="#b4">[5]</ref> was submitted and, therefore, a comparison with other participants cannot be done. The UA system is based in the splitting of nested questions in order to answer questions with temporal restrictions. They have evaluated their system over the TERQAS corpus <ref type="bibr" coords="7,166.75,181.66,9.96,8.74" target="#b3">[4]</ref>, obtaining better results than in this Pilot Task at CLEF 2004. The UA system has correctly answered 15% of the questions. The best result corresponds to factoid questions with a 22.22% of questions with a correct answer. However, in the past edition of QA at CLEF, this team obtained better results (up to 40% of questions with a correct answer) <ref type="bibr" coords="7,90.00,375.49,9.97,8.74" target="#b1">[2]</ref>. This results show that the questions posed in the Pilot Task have been too difficult.</p><p>The UA system never gave more than three answers per question, independently of the type of formulated question. It seems an heuristically established limit for the system that has affected the achievement of good conjunctive and disjunctive list answers. 41 questions got NIL as an answer, with a confidence score of 0 for all them. Unfortunately, these 41 questions had at least one answer in the corpus. On the other hand, the UA system did not identify the 2 posed NIL questions.</p><p>Finally, it seems that the UA system did not play with the score value in the best way. The maximum value given for the confidence score was 0.5002 and several questions with only one correct answer in the corpus had associated several different answers with similar confidence score. The K -measure for the UA's exercise was K = -0.086 with a correlation coefficient of r = 0.246 between self-scoring and real assessment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>Questions whose answer is a conjunctive or a disjunctive list, and questions with temporal restrictions, still remain a challenge for most QA systems. However, these are only a few types of difficult questions which QA systems will have to manage in the near future. A specialization and further collaboration among teams could be expected in order to achieve QA systems with higher accuracy and coverage for different types of questions. In fact, the QA Main Track at CLEF shows that different participant systems answer correctly different subsets of questions.</p><p>Two measures have been proposed in order to reward systems that give a confidence score with a high correlation with human assessments and, at the same time, return more correct answers and less incorrect ones. The case of study shows that systems are able to give very accurate self-scoring, and that the K and K1 measures reward it. However, systems don't need to respond all the questions to obtain good results, but to find a good balance between the number of correct answers and the accuracy of their confidence score.</p><p>On the one hand, this seems a good way to promote the development of more accurate systems with better answer validation. On the other hand, it is a good way to permit some specialization, to open the possibility of posing new types of questions and, at the same time, to leave the door open for new teams starting to develop their own systems.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="8,110.48,259.52,402.52,8.74;8,110.48,271.47,402.52,8.74;8,110.48,283.43,402.52,8.74;8,110.48,295.38,402.52,8.74;8,110.48,307.34,89.47,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,281.06,259.52,231.94,8.74;8,110.48,271.47,246.46,8.74">Question Answering Challenge (QAC-1). An Evaluation of Question Answering Task at NTCIR Workshop 3</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fukumoto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Masui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,213.55,283.43,299.45,8.74;8,110.48,295.38,315.17,8.74">Proceedings of the Third NTCIR Workshop on Research in Information Retrieval, Automatic Text Summarization and Question Answering</title>
		<editor>
			<persName><forename type="first">Keizo</forename><surname>Oyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Emi</forename><surname>Ishida</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Noriko</forename><surname>Kando</surname></persName>
		</editor>
		<meeting>the Third NTCIR Workshop on Research in Information Retrieval, Automatic Text Summarization and Question Answering</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Informatics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.48,325.35,402.53,8.74;8,110.48,337.30,402.52,8.74;8,110.48,349.26,402.53,8.74;8,110.48,361.21,402.52,8.74;8,110.48,373.17,208.35,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,167.52,337.30,282.17,8.74">The Multiple Language Question Answering Track at CLEF 2003</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Romagnoli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vallin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Peinado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Verdejo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,337.97,349.26,175.04,8.74;8,110.48,361.21,338.99,8.74">Comparative Evaluation of Multilingual Information Access Systems. Results of the CLEF 2003 Evaluation Campaign</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3237</biblScope>
			<biblScope unit="page" from="479" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.48,391.18,402.51,8.74;8,110.48,403.14,402.51,8.74;8,110.48,415.09,231.59,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,271.29,391.18,170.45,8.74">Spanish Question Answering Evaluation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,143.86,403.14,305.01,8.74">Computational Linguistics and Intelligent Text Processing, CICLing</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2004">2004. 2945. 2004</date>
			<biblScope unit="page" from="472" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.48,433.11,108.40,8.74;8,242.89,433.11,117.23,8.74;8,384.13,433.11,128.87,8.74;8,110.48,445.06,341.83,8.74" xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pustejovsky</surname></persName>
		</author>
		<ptr target="http://www.cs.brandeis.edu/˜jamesp/arda/time/readings.html" />
		<imprint>
			<date type="published" when="2002-10">October 2002</date>
		</imprint>
		<respStmt>
			<orgName>MITRE</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="8,110.48,463.07,402.52,8.74;8,110.48,475.03,402.52,8.74;8,110.48,486.98,402.52,8.74;8,110.48,498.94,44.69,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,369.83,463.07,143.17,8.74;8,110.48,475.03,157.96,8.74">Splitting complex temporal questions for question answering systems</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Saquete</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Martínez-Barco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Muñoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Vicedo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,290.37,475.03,222.63,8.74;8,110.48,486.98,209.62,8.74">Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL&apos;04), Main</title>
		<meeting>the 42nd Meeting of the Association for Computational Linguistics (ACL&apos;04), Main<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-07">July 2004</date>
			<biblScope unit="page" from="566" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.48,516.95,402.53,8.74;8,110.48,528.91,363.27,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,186.15,516.95,305.36,8.74">Overview of the CLEF 2004 Multilingual Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vallin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,110.48,528.91,176.46,8.74">Proceedings of the CLEF 2004 Workshop</title>
		<meeting>the CLEF 2004 Workshop<address><addrLine>Bath, United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-09">September 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.48,546.92,402.53,8.74;8,110.48,558.88,402.52,8.74;8,110.48,570.83,278.30,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,188.41,546.92,211.44,8.74">The TREC-8 Question Answering Track Report</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,221.79,558.88,287.23,8.74">Proceedings of the Eigthh Text REtrieval Conference (TREC 8)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Eigthh Text REtrieval Conference (TREC 8)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="77" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.48,588.84,402.51,8.74;8,110.48,600.80,402.52,8.74;8,110.48,612.75,278.30,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,188.70,588.84,231.05,8.74">Overview of the TREC-9 Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,235.23,600.80,273.79,8.74">Proceedings of the Ninth Text REtrieval Conference (TREC 9)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Ninth Text REtrieval Conference (TREC 9)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="71" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.47,630.77,402.52,8.74;8,110.48,642.72,402.53,8.74;8,110.48,654.68,308.83,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,184.41,630.77,240.52,8.74">Overview of the TREC 2001 Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,242.23,642.72,270.77,8.74">Proceedings of the Tenth Text REtrieval Conference (TREC</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Tenth Text REtrieval Conference (TREC</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="42" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.48,672.69,402.52,8.74;8,110.48,684.65,402.52,8.74;8,110.48,696.60,250.66,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,184.41,672.69,240.52,8.74">Overview of the TREC 2002 Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,237.98,684.65,275.02,8.74;8,110.48,696.60,22.68,8.74">Proceedings of the Eleventh Text REtrieval Conference (TREC 2002)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<meeting>the Eleventh Text REtrieval Conference (TREC 2002)</meeting>
		<imprint>
			<publisher>NIST Special Publication</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="500" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.48,714.61,402.51,8.74;8,110.48,726.57,402.53,8.74;8,110.48,738.52,135.47,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,189.75,714.61,248.24,8.74">Overview of the TREC 2003 Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,463.34,714.61,49.66,8.74;8,110.48,726.57,249.31,8.74">Proceedings of the Twelfth Text REtrieval Conference (TREC 2003)</title>
		<meeting>the Twelfth Text REtrieval Conference (TREC 2003)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="54" to="68" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
