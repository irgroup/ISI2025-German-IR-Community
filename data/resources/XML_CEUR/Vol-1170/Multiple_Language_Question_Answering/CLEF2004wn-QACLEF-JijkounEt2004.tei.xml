<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,418.35,106.49,27.90,12.55">2004</title>
				<funder ref="#_AadhEWq">
					<orgName type="full">Netherlands Organization for Scientific Research (NWO)</orgName>
				</funder>
				<funder ref="#_KdyNubZ">
					<orgName type="full">NWO</orgName>
				</funder>
				<funder ref="#_JxXp3yT #_W8kpzSX #_KGtdtQU #_52USmZD #_KfWMFRe">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,159.94,136.74,77.92,10.76;1,261.77,136.74,26.56,10.76"><forename type="first">Valentin</forename><forename type="middle">Jijkoun</forename><surname>Gilad</surname></persName>
							<email>gilad@science.uva.nl</email>
						</author>
						<author>
							<persName coords="1,291.32,136.74,35.87,10.76;1,351.10,136.74,82.88,10.76"><forename type="first">Mishne</forename><surname>Maarten De Rijke</surname></persName>
						</author>
						<author>
							<persName coords="1,173.78,150.69,83.22,10.76"><forename type="first">Stefan</forename><surname>Schlobach</surname></persName>
							<email>schlobac@science.uva.nl</email>
						</author>
						<author>
							<persName coords="1,280.91,150.69,52.55,10.76"><forename type="first">David</forename><surname>Ahn</surname></persName>
							<email>ahn@science.uva.nl</email>
						</author>
						<author>
							<persName coords="1,357.38,150.69,62.75,10.76"><forename type="first">Karin</forename><surname>MÃ¼ller</surname></persName>
							<email>kmueller@science.uva.nl</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">CLEF</orgName>
								<orgName type="institution">The University of Amsterdam at QA</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Informatics Institute</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<addrLine>Kruislaan 403</addrLine>
									<postCode>1098 SJ</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,418.35,106.49,27.90,12.55">2004</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">03877561F18F9A6CDEE6FAB441911C29</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the official runs of our team for the CLEF 2004 question answering tasks. We took part in the monolingual Dutch task and in the bilingual English to Dutch task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>To address the question answering (QA) task one has to address a challenging recall problem. As with all language processing tasks, in QA we face a vocabulary gap -the phenomenon that the question and its answer(s) may be phrased in different vocabularies. For QA the vocabulary gap can be especially challenging as systems have to return highly relevant and focused text snippets as output, given very short questions as input. To address the recall problem that QA confronts us with, we advocate a multi-stream architecture which implements multiple ways of identifying candidate answers, complemented with elaborate filtering mechanisms to weed out incorrect candidate answers. In 2003 we completed a first version of this architecture, of which we made good use for the QA tracks both at CLEF [8] and at TREC <ref type="bibr" coords="1,195.16,403.21,10.58,8.97" target="#b8">[9]</ref>. For the 2004 edition of the QA@CLEF task we built on the same architecture.</p><p>This year, we took part in the monolingual Dutch task, and in the bilingual English to Dutch task. Our main aim with our monolingual work was to extend and improve our QA system following an error analysis after the 2003 edition of the task. The bilingual English to Dutch task was new for us. Our main aim here was to evaluate the applicability of our system in a cross-language setting, and to check whether correct results obtained by the bilingual run are a subset of the monolingual one -or whether something can be gained by combining them.</p><p>The paper is organized as follows. In Section 2 we describe the architecture of our QA system. Section 3 describes our official runs. In Section 4 we discuss the results we have obtained and give a preliminary analysis of the performance of different components of the system. We conclude in Section 5.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Description</head><p>Many QA systems share the following pipeline architecture. A question is first associated with a question type, out of a predefined set such as DATE-OF-BIRTH or CURRENCY. Then a query is formulated based on the question, and an information retrieval engine is used to identify a list of documents that are likely to contain the answer. Those documents are sent to an answer extraction module, which identifies candidate answers, ranks them, and selects the final answer. On top of this basic architecture, numerous add-ons have been devised, ranging from logic-based methods <ref type="bibr" coords="1,106.44,610.53,16.60,8.97" target="#b9">[10]</ref> to ones that rely heavily on the redundancy of information available on the World Wide Web <ref type="bibr" coords="1,496.26,610.53,10.58,8.97" target="#b3">[4]</ref>.</p><p>In essence, our system architecture implements multiple copies of the standard architecture, each of which is a complete standalone QA system that produces ranked answers, but not necessarily for all types of questions; the overall system's answer is then selected from the combined pool of candidates through a combination of merging and filtering techniques. For a reasonably detailed discussion of our QA system architecture we refer to <ref type="bibr" coords="1,488.55,658.35,10.79,8.97" target="#b7">[8,</ref><ref type="bibr" coords="1,502.04,658.35,7.19,8.97" target="#b8">9]</ref>. A general overview of the system is given in Figure <ref type="figure" coords="1,264.51,670.31,3.74,8.97" target="#fig_0">1</ref>. This year, we improved our question classifier by incorporating Dutch WordNet to deal with questions such as Which X. . . ?, where X represents a person, animal, agent etc. This year's system contains 8 streams, organized in four groups, depending on the main data source from which they try to answer questions. We now provide a brief description of these four groups.  capitals, abbreviations, and names of political leaders) tend to occur in the document collection in a small number of fixed patterns. When a question type indicates that the question might potentially have an answer in these tables, a lookup is performed in the appropriate knowledge base and answers which are found there are assigned high confidence. For a detailed overview of this stream, see <ref type="bibr" coords="2,287.59,312.60,10.58,8.97" target="#b6">[7]</ref>. In addition to the knowledge bases used in CLEF 2003, we built new ones (such as AWARDS and MEASUREMENTS, storing facts about winners of various prizes, and information about dimensions of objects, respectively). Furthermore, we enriched our previous knowledge bases with information extracted not with surface patterns but with syntactic patterns on top of a version of the Dutch CLEF collection that was parsed using the Alpino parser, a wide coverage dependency parser for Dutch <ref type="bibr" coords="2,479.55,360.42,10.58,8.97" target="#b0">[1]</ref>. Earlier experiments on the AQUAINT corpus had suggested that offline extraction using syntactic extraction patterns can substantially improve recall <ref type="bibr" coords="2,183.06,384.33,10.58,8.97" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Streams that</head><p>The Dutch Tequesta stream is a linguistically informed QA system for Dutch that implements the traditional architecture outlined above. Amongst others, it uses a Part-of-Speech tagger (TNT <ref type="bibr" coords="2,418.02,408.24,11.62,8.97" target="#b1">[2]</ref> trained on the Corpus Gesproken Nederlands <ref type="bibr" coords="2,164.10,420.20,14.94,8.97" target="#b12">[12]</ref>), a home-grown named entity tagger for Dutch, as well as proximity-based candidate answer selection <ref type="bibr" coords="2,138.80,432.15,15.27,8.97" target="#b10">[11]</ref>.</p><p>In the Pattern Match stream, zero or more regular expressions are generated for each question according to its type and structure. These patterns indicate strings which contain the answer with high probability, and are then matched against the entire document collection.</p><p>The Ngram stream, similar in spirit to <ref type="bibr" coords="2,247.58,479.97,10.58,8.97" target="#b2">[3]</ref>, constructs a weighted list of queries for each question using a shallow reformulation process, similar to the Pattern Match stream. These queries are fed to a retrieval engine (we used our home-grown FlexIR, with the Lnu.ltc weighting scheme), and the top retreived documents are used for harvesting word ngrams. The ngrams were ranked according to the weight of the query that generated them, their frequency, NE type, the proximity to the query keywords and more parameters; the top-ranking ngrams were considered candidate answers. The output of this stream is piped to the Justification module (see below).</p><p>As mentioned earlier, we aim at higher recall at the earlier stages, relying on various filtering mechanisms to "clean" the results and achieve also high precision. Therefore, for both the Ngram and the Pattern Match streams, we extended the generated regular expressions and queries, compared to our system at CLEF 2003 -sometimes creating ungrammatical ones, but we assumed that the few results they would produce would be filtered out later.</p><p>Streams that Consult the Web. Quartz-N has two streams that attempt to locate answers on the web: Ngram mining and Pattern Match. For Web searches, we used Google, and ngrams were harvested from the snippets provided by it. Pattern matching was done against full documents returned by Google.</p><p>Streams that Consult the English CLEF Corpus. One of the streams used by Quartz-N is the English language version of our QA system, which consults the English CLEF corpus instead of the Dutch version (but which is otherwise similar to the Dutch version). The answers found by Quartz-E are also piped to the Justification module.</p><p>Streams that Use Other Resources. One of the new streams this year was the Wikipedia stream. Similarly to the streams that consult the Web or the English document collection, this stream also uses an external corpus -the Dutch Wikipedia (http://nl.wikipedia.org), an open-content encyclopedia in Dutch (and other languages). However, since this corpus is much "cleaner" than news paper text, the stream operates in a different manner. First, the focus of the question is identified; this is usually the main named entity in the question. Then, this entity's encyclopedia entry is looked up; since Wikipedia is standardized to a large extent, this information has a template-like nature. Finally, using knowledge about the templates used in Wikipedia, information such as DATE-OF-DEATH and FIRST-NAME can easily be extracted.</p><p>While each of the above streams is a "small" QA system in itself, many components are shared between the streams, including an Answer Justification and a Filtering, Tiling and Type Checking module, both of which we will now describe.</p><p>Answer Justification. As some of our streams obtain candidate answers outside the Dutch CLEF corpus, and as answers need to be supported, or justified, by a document in the Dutch CLEF corpus, we need to find justification for externally found candidate answers. To this end we construct a query with keywords from a given question and candidate answer, and consider the top-ranking document for this query to be the justification, using an Okapi model as this tends to do well on early high precision in our experience. Additionally, we use some retrieval heuristics such as marking the answer as boolean terms in the query (requiring them to appear in retrieved documents).</p><p>Filtering, Tiling, and Type Checking. A detailed error analysis carried out after the 2003 edition of QA@CLEF revealed that the two most important sources of errors were answer selection and named entity recognition <ref type="bibr" coords="3,509.62,245.68,10.58,8.97" target="#b7">[8]</ref>. For this year's task we used a new final answer selection module (similar to that described in <ref type="bibr" coords="3,450.75,257.63,11.20,8.97" target="#b4">[5]</ref>) with heuristic candidate answer filtering and merging, and with stream voting. To compensate for named entity errors made during answer extraction, our type checking module (see <ref type="bibr" coords="3,306.08,281.54,16.60,8.97" target="#b13">[13]</ref> for details) uses several geographical knowledge bases to remove candidates of incorrect type for location questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Runs</head><p>We submitted two runs for the monolingual Dutch question answering task: uams041nlnl and uams042nlnl, and one run for the bilingual English to Dutch task: uams041ennl. All runs return exact answers, and combine answers from all streams. The uams042nlnl run is identical to uams041nlnl, with additional filtering and sanity checks on the candidate answers before selecting the final answer. These checks included zero-count filters (assuming that answers which do not appear as a phrase on the web are incorrect, and questions for which the focus does not appear in the local collection have no answer), and type-checking for location questions <ref type="bibr" coords="3,445.82,405.03,15.27,8.97" target="#b13">[13]</ref>. Our bilingual run included a simple translation of the questions from English to Dutch using a publicly-available interface of Systran (http://www.systranet.com), and then using Quartz-N for for the translated questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head><p>The following table shows the evaluation results of our CLEF 2004 submissions. Beside the standard Right, Wrong, Inexact, and Unsupported measures, we also list various accuracy figures. The run uams042nlnl scored slightly better than uams041nlnl. Interestingly, the gain is in the factoids only: the run uams042nlnl actually scored worse on definitions than uams041nlnl. Had we combined uams042nlnl's answers to factoids with uams041nlnl's answers to definition question, we would have obtained on overall accuracy of 46.5%. This suggests that factoids benefit from additional checks and filters (that work well on short candidate answers), while definition questions benefit from a more lenient approach. Additionally, our filters prove useful for detecting questions with no answers: 5 out of the 9 NIL answers returned (as part of the run uams042nlnl) were correctly identified using the filters, while none were identified without them. The overall accuracy of the bilingual run uams041ennl is less than that of the monolingual runs; this can largely be attributed to the imperfect machine translation used. However, it is interesting to note that the correct answers provided in this run are not a subset of the correct answers provided by the monolingual runs; while 44 questions (22%) were answered correctly by uams041nlnl and not by uams041ennl, there are 25 questions (12.5%) that were answered correctly by the bilingual run and not the monolingual one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overall</head><p>To analyze the contribution of different answering streams to the performance of the whole system, we carried out a number of experiments, disabling each stream individually and evaluating the resulting "smaller" systems using the assessors' judgements available for our official runs. The Lookup stream proved to be the most essential (the system answers 19 more questions with the Lookup on), followed by the Ngrams streams (11 and 4, for the Web and Collection Ngrams, respectively) and the Collection Pattern Match stream (3 more answers). Moreover, our final answer selection module makes use of the essential redundancy of the multi-stream architecture: 70% of the correct answers come from two or more answering streams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>We presented our multi-stream question answering system and the runs it produced for CLEF 2004. Running in parallel several subsystems that approach the QA task from different angles proved successful, as some approaches seem better fit to answer certain types of questions than others. Our ongoing work on the system is focused on additional filtering and type checking mechanisms, and on exploiting high-quality external resources such as the CIA world fact book, Wikipedia, and WordNet.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,121.56,244.69,350.79,8.97"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Quartz-N: the University of Amsterdam's Dutch Question Answering System.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="1,70.19,724.11,453.55,32.88"><head>Consult the Dutch CLEF Corpus.</head><label></label><figDesc>Four streams generate candidate answers from the Dutch CLEF corpus: Lookup, Pattern Match, Ngrams, and Tequesta. The Table Lookup stream uses specialized knowledge bases constructed by preprocessing the collection, exploiting the fact that certain information types (such as country</figDesc><table coords="2,98.11,83.36,395.90,133.17"><row><cell>lookup</cell><cell>Tequesta NL</cell><cell>pattern match</cell><cell>ngrams</cell><cell>Dutch CLEF corpus</cell><cell></cell><cell></cell><cell></cell></row><row><cell>question</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>pattern match</cell><cell>ngrams</cell><cell>Web</cell><cell></cell><cell>candidate answers</cell><cell>filtering</cell><cell>tiling</cell><cell>type checking</cell><cell>answer</cell></row><row><cell>question</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>classification</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Quartz-E</cell><cell>English CLEF</cell><cell></cell><cell></cell><cell>justification</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>corpus</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dutch</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Wikipedia</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We are grateful to <rs type="person">Gertjan van Noord</rs> for supplying us with a dependency parsed version of the Dutch CLEF corpus. This research was supported by the <rs type="funder">Netherlands Organization for Scientific Research (NWO)</rs> under project number <rs type="grantNumber">220-80-001</rs>. In addition, <rs type="person">Maarten de Rijke</rs> was also supported by grants from <rs type="funder">NWO</rs>, under project numbers <rs type="grantNumber">365-20-005</rs>, <rs type="grantNumber">612.069.006</rs>, <rs type="grantNumber">612.000.106</rs>, <rs type="grantNumber">612.000.207</rs>, <rs type="grantNumber">612.066.302</rs>, and <rs type="grantNumber">264-70-050</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_AadhEWq">
					<idno type="grant-number">220-80-001</idno>
				</org>
				<org type="funding" xml:id="_KdyNubZ">
					<idno type="grant-number">365-20-005</idno>
				</org>
				<org type="funding" xml:id="_JxXp3yT">
					<idno type="grant-number">612.069.006</idno>
				</org>
				<org type="funding" xml:id="_W8kpzSX">
					<idno type="grant-number">612.000.106</idno>
				</org>
				<org type="funding" xml:id="_KGtdtQU">
					<idno type="grant-number">612.000.207</idno>
				</org>
				<org type="funding" xml:id="_52USmZD">
					<idno type="grant-number">612.066.302</idno>
				</org>
				<org type="funding" xml:id="_KfWMFRe">
					<idno type="grant-number">264-70-050</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="4,91.77,347.45,431.97,8.97;4,91.77,359.41,234.70,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,273.90,347.45,229.98,8.97">Alpino: Wide-coverage computational analysis of Dutch</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Bouma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Van Noord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Malouf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,91.77,359.41,184.88,8.97">Computational Linguistics in The Netherlands</title>
		<imprint>
			<date type="published" when="2000">2000. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,91.77,373.92,431.96,8.97;4,91.77,385.88,75.00,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,138.51,373.92,162.34,8.97">TnT -a statistical part-of-speech tagger</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brants</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,323.38,373.92,195.99,8.97">Proceedings of the 6th Applied NLP Conference</title>
		<meeting>the 6th Applied NLP Conference</meeting>
		<imprint>
			<publisher>ANLP-2000</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,91.77,400.40,431.97,8.97;4,91.77,412.35,380.04,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,306.11,400.40,196.20,8.97">Web question answering: Is more always better</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,279.11,412.35,98.24,8.97">Proceedings of SIGIR&apos;02</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Bennett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Dumais</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</editor>
		<meeting>SIGIR&apos;02</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="291" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,91.77,426.87,431.96,8.97;4,91.77,438.82,22.42,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,161.33,426.87,229.12,8.97">AskMSR: Question answering using the Worldwide Web</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Banko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,412.91,426.87,106.33,8.97">Proceedings EMNLP 2002</title>
		<meeting>EMNLP 2002</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,91.77,453.34,431.97,8.97;4,91.77,465.29,431.96,8.97;4,91.77,477.25,265.06,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,217.55,453.34,306.18,8.97;4,91.77,465.29,13.08,8.97">Answer selection in a multi-stream open domain question answering system</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,261.18,465.29,262.54,8.97;4,91.77,477.25,38.48,8.97">Proceedings 26th European Conference on Information Retrieval (ECIR&apos;04)</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Mcdonald</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Tait</surname></persName>
		</editor>
		<meeting>26th European Conference on Information Retrieval (ECIR&apos;04)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2997. 2004</date>
			<biblScope unit="page" from="99" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,91.77,491.76,431.97,8.97;4,91.77,503.72,431.97,8.97;4,91.77,515.67,70.29,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="4,235.65,491.76,288.08,8.97;4,91.77,503.72,68.05,8.97">Information extraction for question answering: Improving recall through syntactic patterns</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,177.64,503.72,346.09,8.97;4,91.77,515.67,41.09,8.97">Proceedings of the 20th International Conference on Computational Linguistics (COL-ING 2004)</title>
		<meeting>the 20th International Conference on Computational Linguistics (COL-ING 2004)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,91.77,530.19,431.97,8.97;4,91.77,542.14,348.88,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="4,254.12,530.19,214.92,8.97">Preprocessing Documents to Answer Dutch Questions</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,487.67,530.19,36.07,8.97;4,91.77,542.14,319.34,8.97">Proceedings of the 15th Belgian-Dutch Conference on Artificial Intelligence (BNAIC&apos;03)</title>
		<meeting>the 15th Belgian-Dutch Conference on Artificial Intelligence (BNAIC&apos;03)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,91.77,556.66,431.96,8.97;4,91.77,568.61,61.31,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="4,252.07,556.66,122.90,8.97">How frogs built the Berlin Wall</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,392.82,556.66,96.16,8.97">Proceedings CLEF 2003</title>
		<meeting>CLEF 2003</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,91.77,583.13,431.96,8.97;4,91.77,595.08,380.50,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="4,393.00,583.13,130.73,8.97;4,91.77,595.08,170.85,8.97">The University of Amsterdam at the TREC 2003 Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Schlobach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Tsur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,281.22,595.08,96.63,8.97">Proceedings TREC 2003</title>
		<meeting>TREC 2003</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="586" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,91.77,609.60,431.97,8.97;4,91.77,621.55,269.34,8.97" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="4,91.77,621.55,140.41,8.97">LCC Tools for Question Answering</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Morarescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Lacatusu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Novischi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Badulescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bolohan</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
	<note>Voorhees and Harman</note>
</biblStruct>

<biblStruct coords="4,91.77,636.07,431.97,8.97;4,91.77,648.02,431.96,8.97;4,91.77,659.98,17.44,8.97" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="4,200.73,636.07,306.25,8.97">Tequesta: The University of Amsterdam&apos;s textual question answering system</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,265.86,648.02,179.86,8.97">The Tenth Text REtrieval Conference (TREC</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="519" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,111.69,659.98,360.87,8.97" xml:id="b11">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j" coord="4,307.63,659.98,101.84,8.97">NIST Special Publication</title>
		<imprint>
			<biblScope unit="page" from="500" to="250" />
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>National Institute for Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="4,91.77,674.49,431.96,8.97;4,91.77,686.45,62.27,8.97" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="4,145.88,674.49,230.49,8.97">The Spoken Dutch Corpus: Overview and first evaluation</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Oostdijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,396.25,674.49,97.37,8.97">Proceedings LREC 2000</title>
		<meeting>LREC 2000</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="887" to="894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,91.77,700.96,431.96,8.97;4,91.77,712.92,350.22,8.97" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="4,282.08,700.96,203.36,8.97">Type checking in open-domain question answering</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Schlobach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Olsthoorn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,505.92,700.96,17.81,8.97;4,91.77,712.92,321.03,8.97">Proceedings of the 16th European Conference on Artificial Intelligence (ECAI 2004)</title>
		<meeting>the 16th European Conference on Artificial Intelligence (ECAI 2004)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,91.77,727.43,431.97,8.97;4,91.77,739.39,324.08,8.97" xml:id="b14">
	<analytic>
	</analytic>
	<monogr>
		<title level="m" coord="4,273.31,727.43,208.05,8.97">The Tenth Text REtrieval Conference (TREC 2002)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="500" to="251" />
		</imprint>
		<respStmt>
			<orgName>National Institute for Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
