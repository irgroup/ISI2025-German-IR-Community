<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,111.18,86.47,357.07,12.19">Cross-Language Retrieval Using HAIRCUT for CLEF 2004</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,217.26,115.77,61.94,8.74"><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
							<email>mcnamee@jhuapl.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The Johns Hopkins University Applied Physics Laboratory</orgName>
								<address>
									<addrLine>11100 Johns Hopkins Road</addrLine>
									<postCode>20723-6099</postCode>
									<settlement>Laurel</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,298.74,115.77,63.55,8.74"><forename type="first">James</forename><surname>Mayfield</surname></persName>
							<email>mayfield@jhuapl.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The Johns Hopkins University Applied Physics Laboratory</orgName>
								<address>
									<addrLine>11100 Johns Hopkins Road</addrLine>
									<postCode>20723-6099</postCode>
									<settlement>Laurel</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,111.18,86.47,357.07,12.19">Cross-Language Retrieval Using HAIRCUT for CLEF 2004</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C2C8D14C1426EEB16328CFF5D42654ED</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>JHU/APL continued to explore the use of knowledge-light methods for scalable multilingual retrieval during the CLEF 2004 evaluation. We relied on the language-neutral techniques of character n-gram tokenization, pre-translation query expansion, statistical translation using aligned parallel corpora, fusion from disparate retrievals, and reliance on language similarity when resources are scarce. We participated in the monolingual and bilingual evaluations. Our results support the claims that n-gram based retrieval is highly effective; that fusion of multiple retrievals is helpful in bilingual retrieval; and, that reliance on language similarity in lieu of translation can outperform a high performing system using abundant translation resources and a less similar query language.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>As in the past JHU/APL's work with the HAIRCUT retrieval system for CLEF 2004 was based on languageneutral methods. In particular, we favor techniques that can be readily applied to any language or language pair. We believe that such methods are at least as effective as approaches that rely on language-specific processing, and perhaps more so. Our principal monolingual techniques include character n-gram tokenization, use of a statistical language model of retrieval, and fusion from multiple retrievals. For bilingual retrieval we focus on pre-translation query expansion using comparable collections, statistical translation from aligned parallel collections, and when translation resources are scarce, reliance on language similarity alone. We also rely on a technique that we first explored in the CLEF 2003 evaluation: direct ngram translation, a new method of translating queries that uses n-grams rather than words as the elements to be translated <ref type="bibr" coords="1,124.91,428.13,10.63,8.74" target="#b6">[7]</ref>. This method does not suffer from certain obstacles in dictionary-based translation, such as word lemmatization, matching of multiple word expressions, and inability to handle out-of-vocabulary words such as common surnames <ref type="bibr" coords="1,180.13,451.11,15.40,8.74" target="#b10">[11]</ref>.</p><p>We submitted official runs for the monolingual and bilingual tracks. For all of our runs we used the HAIRCUT system and a statistical language model similarity calculation. Some of our official runs were based solely on n-gram processing; however, we thought that by using a combination of n-grams and words or stemmed words better performance could be obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>HAIRCUT supports several ways of representing documents using a bag-of-terms assumption. (We emphasize that we frequently use character n-grams, not words as indexing terms.) Our general approach is to process the text of each document, reducing all terms to lower-case. Words were deemed to be whitespace delimited tokens in the text; however, we preserve only the first 4 digits of a number and we truncate any particularly long tokens (those greater than 35 characters in length). We make no attempt at compound splitting. Once words are identified we optionally perform transformations on the words to create indexing terms (e.g., stemming). Starting in 2003 we began removing diacritical marks, believing that they are of little importance. So-called stopwords are retained in our index and the dictionary is created from all words present in the corpus. At query time we ignore high frequency terms for reasons of run-time efficiency, and because such terms typically add little to query semantics. (By default, query terms occurring in greater than 20% of documents are ignored.) HAIRCUT applies gamma compression to reduce the size of the inverted file, but does not store withindocument positional information in the inverted index. A 'dual file', that is a document-indexed collection of term-ids and counts, is also created. Construction of this data structure doubles our on-disk space requirements, but facilitates examination of individual document representations, which is useful when generating expansion terms during pseudo relevance feedback). Our lexicon is stored as a B-tree with nodes compressed in memory to maximize the number of in-memory terms subject to physical memory limitations. For the indexes created for CLEF 2004 memory was not an issue as only O(10 6 ) distinct terms were found in each collection and the corresponding dictionaries were relatively small. We continue to use a statistical language model for retrieval akin to those presented by Miller et al. <ref type="bibr" coords="2,475.19,119.13,16.70,8.74" target="#b9">[10]</ref> and Hiemstra <ref type="bibr" coords="2,109.72,130.59,11.71,8.74" target="#b3">[4]</ref> with Jelinek-Mercer smoothing <ref type="bibr" coords="2,246.57,130.59,13.02,8.74" target="#b4">[5]</ref>. In this model, relevance is defined as</p><formula xml:id="formula_0" coords="2,189.78,142.17,202.02,30.59">P(D | Q) = αP(q | D) + (1-α)P(q | C) [ ] q ∈Q ∏ ,</formula><p>where Q is a query, D is a document, C is the collection as a whole, and α is a smoothing parameter. The probabilities on the right side of the equation are replaced by their maximum likelihood estimates when scoring a document. The language model has the advantage that term weights are mediated by the corpus.</p><p>Our experience has been that this type of probabilistic model outperforms a vector-based cosine model or a binary independence model with Okapi BM25 weighting.</p><p>For the monolingual task our submitted runs were based on a combination of several base runs using different options for tokenization. JHU/APL's official bilingual submissions were based solely on stemmed words, although we had hoped to submit composite runs. Our method for combination is to normalize scores by probability mass and to then merge documents by score. All of our submitted runs were automatic runs and used only the title and description topic fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monolingual Task</head><p>For our monolingual work we created several indexes for each language using the permissible document fields appropriate to each collection. We indexed the full language collection, making use of documents from 1994 and 1995, despite the fact that only half the collection was used in the evaluation. Prior to submission we discarded retrieved documents from the wrong time period. Our reasons for using the larger collection were to improve corpus statistics, pseudo relevance feedback, and for the bilingual task, pre-translation expansion. Our four basic methods for tokenization were unnormalized words, stemmed words obtained through the use of the Snowball stemmer, 4-grams, and 5-grams. We were unable to get the Snowball stemmer to work with Russian text, and we had some difficulty with it while processing Portuguese queriesmany query terms were discarded. Information about each index is shown in Table <ref type="table" coords="2,405.41,447.75,3.76,8.74" target="#tab_0">1</ref>. Our use of 4-grams and 5-grams as indexing terms represents a departure from earlier studies using 6-grams that we justify based on recent findings <ref type="bibr" coords="2,245.62,589.65,10.66,8.74" target="#b8">[9]</ref>. The 4-grams and 5-grams seem to work equally well for monolingual retrieval. Our language model requires a single smoothing constant; we used α=0.3 with both words and stems, and α=0.8 with 4-grams and 5-grams. Each of our base runs used blind relevance feedback (queries expanded to 60 terms; terms selected using 20 top-ranked and 75 low-ranked documents). Figure <ref type="figure" coords="2,504.05,625.65,5.01,8.74" target="#fig_0">1</ref> charts performance using our four different term indexing strategies, in isolation. The relative advantage we have previously observed n-grams to have over words is less apparent on the CLEF 2004 data.</p><p>Our official submissions were produced by fusing several base runs. We submitted three runs for each language and we report results on the English document set since the relevance judgments are available. Runs were labeled aplmoxxa, aplmoxxb, or aplmoxxc, where xx denotes the language of interest. Runs whose names end with a terminal 'a' were produced by combining a 4-gram base run with a stemmed word base run; a terminal 'b' indicates fusion of a 5-grams and stemmed words; terminal 'c' is used for runs that used both 4-grams and 5-grams. Monolingual performance based on mean average precision is reported in Table <ref type="table" coords="2,70.56,740.55,3.77,8.74" target="#tab_1">2</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bilingual Task</head><p>We spent a rather considerable amount of time this year in an effort to improve our translation resources. We have had consistent success using aligned parallel corpora to extract statistical translations. We have relied on this technique for single word translation; however, we recently demonstrated significant improvements in bilingual performance by translating character n-grams directly <ref type="bibr" coords="3,366.33,715.17,10.63,8.74" target="#b6">[7]</ref>. We call this 'direct n-gram translation'. Additionally we also translated stemmed words and words.</p><p>There is a consensus that lexical coverage is essential for good cross-language retrieval performance. Several studies have sought to understand the relationship between lexical coverage of translation resources and CLIR performance <ref type="bibr" coords="4,156.20,96.15,11.61,8.74" target="#b1">[2]</ref>[3][7] <ref type="bibr" coords="4,191.04,96.15,15.49,8.74" target="#b11">[12]</ref>. We believe that the relationship between translation coverage and performance is approximately linear. Accordingly, we sought to grow the size of our parallel collection. However, due to the nature of corpus statistics, doubling the size of a parallel collection will not necessarily double the coverage of a statistically produced translation.</p><p>For the 2002 and 2003 campaigns we relied on a single source for parallel texts, the Official Journal of the E.U.</p><p>[13], which is published in the official languages (20 languages as of May 2004). The Journal is available in each of the E.U. languages and consists mainly of governmental topics, for example, trade and foreign relations. For the CLEF 2003 evaluation we had obtained 33 GB of PDF files that we distilled into approximately 300 MB of alignable text, per language. In December 2003 we began the process of mining archival issues of the Journal, beginning with 1998. This process took nearly five months. We obtained data from January 1998 through April 2004 -over six years of data. This is nearly 80 GB of PDF files, or roughly 750 MB of plain text per language. We extracted text using the pdftotext program; however this software cannot extract the Greek data set; we were left with data in ten languages, from which 45 possible alignments are possible. Though focused on European topics, the time span is three to ten years after the CLEF-2004 document collection. Though aware of smaller, but aligned parallel data (e.g., Philip Koehn's Europarl corpus <ref type="bibr" coords="4,99.82,279.99,11.27,8.74" target="#b5">[6]</ref>) we did not utilize additional data for reasons of homogeneity and convenience.</p><p>To align data between two languages, we would: o convert the data from PDF format to plain text (this introduced some errors, especially when processing diacritical marks in the earlier years); o apply rules for splitting the text into sections (the data was page-aligned, we desired paragraph-sized chunks); o and, align files using char_align <ref type="bibr" coords="4,239.34,360.39,10.65,8.74" target="#b0">[1]</ref>.</p><p>To induce a translation for a given source language term, we proceed by: o identifying documents (i.e., approximately paragraphs) containing the source language term; o examining the set of corresponding documents from the target language portion of the aligned collection; o producing a score for each term that occurs in at least one of the target language paragraphs (more on this below); o and finally, selecting the single term with the largest translation score for the source language term. Our method for scoring candidate translations does not require translation model software such as GIZA++. Rather, we rely on information theoretic scores to rank terms. We adopt the same technique we rely on for pseudo relevance feedback -a method we have developed called affinity sets. Terms are weighted based on their inverse document frequency (IDF) and the difference between their relative frequency in the set of documents under consideration and the global set of documents. This measure is related to mutual information; however, we believe our technique is more general as it permits the set of documents to be identified through any means, including potentially, query-specific attempts at translation (though we do not attempt this in the experiments we report on here).</p><p>We performed pairwise alignments between languages pairs, for example, between Dutch and French. Once aligned, we indexed each pairwise-aligned collection using the technique described for the CLEF-2004 document collections. That is, we created four indexes per sub-collection, per language -one each of words, stems, 4-grams and 5-grams. This year, rather than create a translation dictionary for every term in a source language index, we translated terms on demand using the algorithm presented above. Of course, one could generate multiple translations rather than simply identifying a single one. We have not found this necessary as techniques such as pre-translation query expansion are capable of generating many terms related to a query; thus the harm introduced by a dubious translation is lessened. o and, German and French. We had envisioned using English as a source language for the multilingual task, but not produce a submission.</p><p>At this point we should mention that the 'proper' translation of an n-gram is decidedly elusive concept -there is typically no single, correct answer. Nonetheless, we simply relied on the large volume of n-grams to smooth topic translation. For example, the central 5-grams of the English phrase 'prime minister' include 'ime_m', 'me_mi', and 'e_min'. The derived 'translations' of these English 5-grams into French are 'er_mi', '_mini', and 'er_mi', respectively. This seems to work as expected for the French phrase 'premier ministre', although the method is not foolproof. Consider n-gram translations from the phrase 'communist party' (parti communiste): '_commu' (mmuna), 'commu' (munau), 'ommun' (munau), 'mmuni' (munau), 'munis' (munis), 'unist' (unist), 'nist_' (unist), 'ist_p' (ist_p), 'st_pa' (1_re_), 't_par' (rtie_), '_part' (_part), 'party' (rtie_), and 'arty_' (rtie_). The lexical coverage of translation resources is a critical factor for good CLIR performance, so the fact that almost any n-gram has a 'translation' should improve performance. The direct translation of n-grams may offer a solution to several key obstacles in dictionary-based translation. Word normalization is not essential since sub-word strings will be compared. Translation of multiword expressions can be approximated by translation of word-spanning n-grams. Out-of-vocabulary words, particularly proper nouns, can be partially translated by common n-gram fragments or left untranslated in close languages.</p><p>Our experience on the CLEF 2002 and 2003 bilingual tasks led us to believe that direct translation of 5grams would likely be the most effective single technique, but that combination using runs generated by translating multiple term types would yield an improvement (see Fig. <ref type="figure" coords="5,361.67,348.92,3.63,8.74" target="#fig_1">2</ref>). It was our intent to submit such composite runs for this year's evaluation; however, we could not complete the processing required prior to the submission deadline; it required eight indexes and runs per language pair (48 in total). Instead, we submitted runs for six language pairs using stemmed words as the sole type of token that was translated. We also submitted two runs that made no use of translation whatsoever for the language pairs Spanish to Portuguese and Bulgarian to Russian. We regret to report that we were not able to utilize the Amharic topics.  The performance of APL's official bilingual runs is summarized in Table <ref type="table" coords="6,381.66,208.05,3.76,8.74" target="#tab_3">3</ref>. A terminal 'a' in the run id indicates the use of translation; a 'b' indicates no translation was attempted. The first six rows report performance against the Finnish, French, and Portuguese sub collections, using two source languages each. For these runs pre-translation expansion was incorporated by using a monolingual run based on 4-grams and stems; from these monolingual runs (against the full source language collection) 60 words were extracted. To produce our bilingual submissions, these words were stemmed and then the stems were translated into corresponding stems using parallel data for the language pair. This expanded, translated query was run against the full target language collection and retrieved documents from the wrong period were omitted.</p><p>Generally, performance for the Portuguese collection was higher than for the French and Finnish collections. We observed that translation from a very closely related language resulted in exceptional performance; for the Spanish to Portuguese run, we obtained performance 102% of a monolingual Portuguese baseline. We attribute this to the additional query expansion step that occurred (i.e., pre-translation expansion). We also noted that our method of not translating queries between very closely related languages, but relying only on partial n-gram matches (i.e., using 4-grams), was highly effective. This technique was so effective, that Spanish to Portuguese retrieval using 4-grams and no translation (aplbiesptb) outperformed translation of English queries (aplbienpta). Run aplbiesptb did at or better than median on 34 of the 46 topics. Even for language pairs with significant translation resources, language similarity should not be ignored. We did not have adequate opportunity to develop translation resources for Russian. Thus, we used the Bulgarian topic statements which are also in Cyrillic and hoped 'no-translation' would be effective. We report bilingual retrieval performance 45% of that of a monolingual Russian baseline, which while not as effective as between Spanish and Portuguese, might be serviceable to an end-user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Improvement with Multiple Translation</head><p>Fusion of multiple bilingual runs using translation of different token types did, in fact, confer an improvement on this year's data, as it had in previous years. Relative performance increased from between 4% and 33%, depending on the language pair, when runs using words, stems, and 4-grams and 5-grams were combined (see Fig. <ref type="figure" coords="7,153.46,167.37,3.63,8.74" target="#fig_2">3</ref>). We observed that the improvement due to this additional fusion seemed inversely proportional to the baseline monolingual performance using our official submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>JHU/APL continued its language-neutral approach to multilingual retrieval for the CLEF 2004 evaluation.</p><p>For monolingual retrieval we compared words, a popular suffix stemmer, and n-grams of lengths four and five, all using the same retrieval engine and language model similarity metric. We found that n-grams continued to work well for monolingual retrieval; however, their relative efficacy compared to ordinary words appeared to be less for the CLEF 2004 data than that previously reported. We continued to combine runs produced through disparate retrievals, which we believe yields a modest improvement.</p><p>For bilingual retrieval we used direct translation of n-grams in addition to words and stems. We also found that not translating queries between closely related languages, when n-grams are used, can outperform retrieval with translation from a less similar language, even when large translation resources are available.</p><p>We will continue our work in exploring knowledge-light, language neutral approaches for retrieval. We have found the use of character n-grams, pre-translation query expansion, statistical translation using aligned parallel corpora, fusion from disparate retrievals, and reliance on language similarity when resources are scarce, all highly effective. In the future we hope to examine the identification and translation of multi-word phrases to see if such compounds can be used to improve retrieval quality.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,70.56,349.89,364.69,8.74"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Relative efficacy of different tokenization methods using the CLEF 2004 test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,70.56,722.18,438.41,8.74;5,70.56,733.64,434.88,8.74"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Relative performance of individual runs using direct translation of words, stems, and n-grams. Fusion of all four yielded the best performance in three of four cases using the CLEF 2002 bilingual test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,70.56,711.33,438.55,8.74;6,70.56,722.85,223.74,8.74"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Improvement observed through combining multiple term translations on the CLEF 2004 Bilingual Task. The improved runs were not official submissions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,70.56,470.73,368.01,92.68"><head>Table 1 .</head><label>1</label><figDesc>Summary information about the test collection and index data structures</figDesc><table coords="2,140.94,482.73,297.63,80.68"><row><cell cols="2">language #docs</cell><cell>#rel</cell><cell cols="3">index size (MB) / unique terms (1000s)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>words</cell><cell>stems</cell><cell>4-grams</cell><cell>5-grams</cell></row><row><cell>EN</cell><cell cols="5">166754 375 143 / 302 123 / 236 504 / 166 827 / 916</cell></row><row><cell>FI</cell><cell cols="3">55344 413 90 / 978</cell><cell cols="2">60 / 521 136 / 138 228 / 707</cell></row><row><cell>FR</cell><cell cols="5">177450 915 129 / 328 107 / 226 393 / 159 628 / 838</cell></row><row><cell>PT</cell><cell cols="5">106821 678 101 / 303 77 / 178 292 / 152 492 / 735</cell></row><row><cell>RU</cell><cell cols="3">16715 123 26 / 253</cell><cell>26 / 253</cell><cell>44 / 136</cell><cell>86 / 569</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,70.56,407.37,438.49,220.78"><head>Table 2 .</head><label>2</label><figDesc>Official results for monolingual task. The shaded rows are for unofficial English runs. The maximal performing run for each language is emboldened.</figDesc><table coords="3,95.28,430.83,389.08,197.32"><row><cell>run id</cell><cell cols="2">Fields Terms</cell><cell>MAP</cell><cell cols="5">=Best &gt;=Median Rel. Found Relevant # topics</cell></row><row><cell>aplmoena</cell><cell>TD</cell><cell cols="2">4+snow 0.5414</cell><cell></cell><cell></cell><cell>363</cell><cell>375</cell><cell>42</cell></row><row><cell>aplmoenb</cell><cell>TD</cell><cell cols="2">5+snow 0.5417</cell><cell></cell><cell></cell><cell>364</cell><cell>375</cell><cell>42</cell></row><row><cell>aplmoenc</cell><cell>TD</cell><cell>4+5</cell><cell>0.5070</cell><cell></cell><cell></cell><cell>295</cell><cell>375</cell><cell>42</cell></row><row><cell>aplmofia</cell><cell>TD</cell><cell cols="2">4+snow 0.5393</cell><cell>8</cell><cell>34</cell><cell>395</cell><cell>413</cell><cell>45</cell></row><row><cell>aplmofib</cell><cell>TD</cell><cell cols="2">5+snow 0.5443</cell><cell>6</cell><cell>29</cell><cell>394</cell><cell>413</cell><cell>45</cell></row><row><cell>aplmofic</cell><cell>TD</cell><cell>4+5</cell><cell>0.5336</cell><cell>8</cell><cell>33</cell><cell>392</cell><cell>413</cell><cell>45</cell></row><row><cell>aplmofra</cell><cell>TD</cell><cell cols="2">4+snow 0.4284</cell><cell>1</cell><cell>29</cell><cell>888</cell><cell>915</cell><cell>49</cell></row><row><cell>aplmofrb</cell><cell>TD</cell><cell cols="2">5+snow 0.4581</cell><cell>4</cell><cell>32</cell><cell>891</cell><cell>915</cell><cell>49</cell></row><row><cell>aplmofrc</cell><cell>TD</cell><cell>4+5</cell><cell>0.4249</cell><cell>2</cell><cell>25</cell><cell>810</cell><cell>915</cell><cell>49</cell></row><row><cell>aplmopta</cell><cell>TD</cell><cell cols="2">4+snow 0.4230</cell><cell>8</cell><cell>27</cell><cell>582</cell><cell>678</cell><cell>46</cell></row><row><cell>aplmoptb</cell><cell>TD</cell><cell cols="2">5+snow 0.4445</cell><cell>10</cell><cell>30</cell><cell>604</cell><cell>678</cell><cell>46</cell></row><row><cell>aplmoptc</cell><cell>TD</cell><cell>4+5</cell><cell>0.4690</cell><cell>11</cell><cell>34</cell><cell>589</cell><cell>678</cell><cell>46</cell></row><row><cell>aplmorua</cell><cell>TD</cell><cell cols="2">4+snow 0.2974</cell><cell>4</cell><cell>18</cell><cell>98</cell><cell>123</cell><cell>34</cell></row><row><cell>aplmorub</cell><cell>TD</cell><cell cols="2">5+snow 0.3076</cell><cell>6</cell><cell>19</cell><cell>100</cell><cell>123</cell><cell>34</cell></row><row><cell>aplmoruc</cell><cell>TD</cell><cell>4+5</cell><cell>0.2604</cell><cell>5</cell><cell>14</cell><cell>97</cell><cell>123</cell><cell>34</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,70.56,73.17,433.76,120.10"><head>Table 3 .</head><label>3</label><figDesc>JHU/APL's official results for bilingual task.</figDesc><table coords="6,75.24,85.11,429.08,108.16"><row><cell>run id</cell><cell cols="3">Fields Terms MAP</cell><cell cols="6">% mono =Best &gt;=Median Rel. Found Relevant # topics</cell></row><row><cell>aplbidefra</cell><cell>TD</cell><cell cols="2">4+s / s 0.3030</cell><cell>66.14</cell><cell>5</cell><cell>28</cell><cell>770</cell><cell>915</cell><cell>49</cell></row><row><cell>aplbienpta</cell><cell>TD</cell><cell cols="2">4+s / s 0.3414</cell><cell>76.91</cell><cell>10</cell><cell>23</cell><cell>423</cell><cell>678</cell><cell>46</cell></row><row><cell>aplbiesfia</cell><cell>TD</cell><cell cols="2">4+s / s 0.2982</cell><cell>54.79</cell><cell>17</cell><cell>36</cell><cell>310</cell><cell>413</cell><cell>45</cell></row><row><cell>aplbiespta</cell><cell>TD</cell><cell cols="3">4+s / s 0.4537 102.08</cell><cell>12</cell><cell>35</cell><cell>546</cell><cell>678</cell><cell>46</cell></row><row><cell>aplbifrfia</cell><cell>TD</cell><cell cols="2">4+s / s 0.2899</cell><cell>53.26</cell><cell>20</cell><cell>32</cell><cell>322</cell><cell>413</cell><cell>45</cell></row><row><cell>aplbinlfra</cell><cell>TD</cell><cell cols="2">4+s / s 0.3753</cell><cell>81.93</cell><cell>8</cell><cell>33</cell><cell>845</cell><cell>915</cell><cell>49</cell></row><row><cell>aplbibgrub</cell><cell>TD</cell><cell>4</cell><cell>0.1407</cell><cell>45.75</cell><cell>3</cell><cell>18</cell><cell>81</cell><cell>123</cell><cell>34</cell></row><row><cell>aplbiesptb</cell><cell>TD</cell><cell>4</cell><cell>0.3825</cell><cell>86.06</cell><cell>9</cell><cell>34</cell><cell>439</cell><cell>678</cell><cell>46</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,88.57,449.44,420.48,7.85;7,70.56,459.76,290.81,7.85" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,147.01,449.44,261.76,7.85">Char_align: A program for aligning parallel texts at the character level</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">W</forename><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,421.56,449.44,87.49,7.85;7,70.56,459.76,234.57,7.85">Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 31st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,88.55,470.14,420.51,7.85;7,70.56,480.46,390.06,7.85" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,240.21,470.14,268.84,7.85;7,70.56,480.46,73.76,7.85">The effect of bilingual term list size on dictionary-based cross-language information retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,153.06,480.46,281.28,7.85">Proceedings of the 36th Hawaii International Conference on System Sciences</title>
		<meeting>the 36th Hawaii International Conference on System Sciences</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,88.59,490.78,420.45,7.85;7,70.56,501.16,438.48,7.85;7,70.56,511.48,38.35,7.85" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,275.92,490.78,153.82,7.85">Quantifying the Utility of Parallel Corpora</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Mccarley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,440.28,490.78,68.76,7.85;7,70.56,501.16,401.87,7.85">Proceedings of the 24th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR-01)</title>
		<meeting>the 24th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR-01)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="398" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,88.57,521.86,420.47,7.85;7,70.56,532.18,177.86,7.85" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="7,142.68,521.86,194.44,7.85">Using Language Models for Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<pubPlace>The Netherlands</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Center for Telematics and Information Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph. D. Thesis</note>
</biblStruct>

<biblStruct coords="7,88.59,542.50,420.48,7.85;7,70.56,552.88,317.28,7.85" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,186.62,542.50,258.43,7.85">Interpolated Estimation of Markov Source Parameters from Sparse Data</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,142.80,552.88,113.74,7.85">Pattern Recognition in Practice</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Gelsema</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Kanal</surname></persName>
		</editor>
		<meeting><address><addrLine>North Holland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1980">1980</date>
			<biblScope unit="page" from="381" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,88.57,563.20,420.48,7.85;7,70.56,573.52,110.63,7.85" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="7,130.51,563.20,249.35,7.85">Europarl: A multilingual corpus for evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<ptr target="http://www.isi.edu/koehn/publications/europarl/" />
		<imprint/>
	</monogr>
	<note type="report_type">Unpublished</note>
</biblStruct>

<biblStruct coords="7,88.57,583.90,420.44,7.85;7,70.56,594.22,438.51,7.85;7,70.56,604.54,220.21,7.85" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,203.91,583.90,305.10,7.85;7,70.56,594.22,34.60,7.85">Comparing Cross-Language Query Expansion Techniques by Degrading Translation Resources</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mayfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,130.83,594.22,378.24,7.85;7,70.56,604.54,77.14,7.85">the Proceedings of the 25th Annual International Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>Tampere, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="159" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,88.53,614.92,420.50,7.85;7,70.56,625.24,167.68,7.85" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,205.04,614.92,240.41,7.85">JHU/APL Experiments in Tokenization and Non-Word Translation</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mayfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,455.04,614.92,53.99,7.85;7,70.56,625.24,101.63,7.85">Working Notes of the CLEF 2003 Workshop</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="19" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,88.58,635.56,420.46,7.85;7,70.56,645.94,151.59,7.85" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,216.59,635.56,273.48,7.85">Character N-gram Tokenization for European Language Text Retrieval</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mayfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,70.56,645.94,77.14,7.85">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="73" to="97" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,88.60,656.26,420.44,7.85;7,70.56,666.58,9.00,7.85;7,79.56,664.52,6.00,5.23;7,88.14,666.58,420.91,7.85;7,70.56,676.96,109.61,7.85" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,229.91,656.26,190.83,7.85">A hidden Markov model information retrieval system</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Leek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,440.88,656.26,68.16,7.85;7,70.56,666.58,9.00,7.85;7,79.56,664.52,6.00,5.23;7,88.14,666.58,379.06,7.85">Proceedings of the 22 nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 22 nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Berkeley, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="214" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,88.58,687.28,420.47,7.85;7,70.56,697.66,309.16,7.85" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,299.31,687.28,209.75,7.85;7,70.56,697.66,154.70,7.85">Dictionary-Based Cross-Language Information Retrieval: Problems, Methods, and Research Findings</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pirkola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hedlund</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Keskusalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Järvelin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,234.84,697.66,77.14,7.85">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="209" to="230" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,88.57,707.98,420.49,7.85;7,70.56,718.30,438.52,7.85;7,70.56,728.68,100.27,7.85" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,188.34,707.98,240.90,7.85">Cross-lingual Information Retrieval Using Hidden Markov Models</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,450.22,707.98,58.84,7.85;7,70.56,718.30,438.52,7.85;7,70.56,728.68,73.10,7.85">the Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-2000)</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
