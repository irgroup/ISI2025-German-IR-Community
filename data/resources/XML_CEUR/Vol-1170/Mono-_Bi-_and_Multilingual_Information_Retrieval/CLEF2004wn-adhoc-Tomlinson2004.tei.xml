<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,106.56,146.21,389.87,18.08">Finnish, Portuguese and Russian Retrieval with</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2004-08-16">August 16, 2004</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,117.34,168.13,225.07,18.08"><forename type="first">Hummingbird</forename><surname>Searchserver</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Ottawa</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,259.83,203.18,83.34,10.46;1,271.20,217.12,60.62,10.46"><forename type="first">Stephen</forename><surname>Tomlinson Hummingbird</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Ottawa</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,106.56,146.21,389.87,18.08">Finnish, Portuguese and Russian Retrieval with</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2004-08-16">August 16, 2004</date>
						</imprint>
					</monogr>
					<idno type="MD5">8A9E38BB3A14799AC87AC5E4A02E0C5E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hummingbird participated in the Finnish, Portuguese, Russian and French monolingual information retrieval tasks of the Cross-Language Evaluation Forum (CLEF) 2004: for the natural language queries, find all the relevant documents (with high precision) in the CLEF 2004 document sets. SearchServer's experimental lexical stemmers significantly increased mean average precision for each of the 4 languages. For Finnish, mean average precision was significantly higher with SearchServer's experimental decompounding option enabled. For each language, the submitted SearchServer run returned a relevant document in the first row for more than half of the short (Titleonly) queries. At least one relevant document was returned in the first ten rows for 75-90% of the short queries.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Hummingbird SearchServer<ref type="foot" coords="1,208.85,485.01,3.97,7.32" target="#foot_0">1</ref> is a toolkit for developing enterprise search and retrieval applications. The SearchServer kernel is also embedded in other Hummingbird products for the enterprise.</p><p>SearchServer works in Unicode internally <ref type="bibr" coords="1,285.64,510.00,10.51,10.46" target="#b2">[3]</ref> and supports most of the world's major character sets and languages. The major conferences in text retrieval evaluation (CLEF <ref type="bibr" coords="1,431.02,521.95,9.96,10.46" target="#b0">[1]</ref>, NTCIR <ref type="bibr" coords="1,483.33,521.95,10.51,10.46" target="#b3">[4]</ref> and TREC <ref type="bibr" coords="1,122.48,533.90,10.79,10.46" target="#b7">[8]</ref>) have provided opportunities to objectively evaluate SearchServer's support for more than a dozen languages. This (draft) paper describes experimental work with SearchServer for the task of finding relevant documents for natural language queries in 4 European languages (Finnish, Portuguese, Russian and French) using the CLEF 2004 test collections. Portuguese is new to CLEF this year, and the experimental SearchServer version has some enhancements which substantially affect Finnish and Russian, so we focus on these 3 languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data</head><p>The CLEF 2004 document sets consisted of tagged (SGML-formatted) news articles (mostly from 1995) in 4 different languages: Finnish, Portuguese, Russian and French. Table <ref type="table" coords="1,438.96,692.64,4.98,10.46" target="#tab_0">1</ref> gives the sizes.</p><p>The CLEF organizers created 50 natural language "topics" (numbered 201-250) and translated them into many languages. Each topic contained a "Title" (subject of the topic), "Description" (a one-sentence specification of the information need) and "Narrative" (more detailed guidelines for what a relevant document should or should not contain). The participants were asked to use the Title and Description fields for at least one automatic submission per task this year to facilitate comparison of results. Some topics were discarded for some languages because no relevant documents existed for them. Table <ref type="table" coords="2,245.41,276.06,4.98,10.46" target="#tab_0">1</ref> gives the final number of topics for each language and their average number of relevant documents. For more information on the CLEF test collections, see the CLEF web site <ref type="bibr" coords="2,175.68,299.97,9.96,10.46" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Indexing</head><p>Our indexing approach was the mostly the same as last year <ref type="bibr" coords="2,350.57,346.24,14.61,10.46" target="#b9">[10]</ref>. Accents were not indexed except for the combining breve in Russian. The apostrophe was treated as a word separator for the 4 investigated languages. Our custom text reader, cTREC, was updated to maintain support for the CLEF guidelines of only indexing specifically tagged fields (the new Portuguese collection necessitated a minor update). Some stop words were excluded from indexing (e.g. "the", "by" and "of" in English). For these experiments, our stop word lists for Portuguese and Russian were based on the Porter lists <ref type="bibr" coords="2,499.72,417.97,9.96,10.46" target="#b4">[5]</ref>, and this year we based on our Finnish list on Savoy's <ref type="bibr" coords="2,325.91,429.92,9.96,10.46" target="#b6">[7]</ref>. We used our own list for French.</p><p>By default, the SearchServer index supports both exact matching (after some Unicode-based normalizations, such as decompositions and conversion to upper-case) and morphological matching (e.g. inflections, derivations and compounds, depending on the linguistic component used).</p><p>For many languages (including the 4 European languages investigated in CLEF 2004), Search-Server includes the option of finding inflections based on lexical stemming (i.e. stemming based on a dictionary or lexicon for the language). For example, in English, "baby", "babied", "babies", "baby's" and "babying" all have "baby" as a stem. Specifying an inflected search for any of these terms will match all of the others. The lexical stemming of the post-5.x experimental development version of SearchServer used for the experiments in this paper was based on internal stemming component 3.6.3.4 for the submitted runs and 3.7.0.15 for the diagnostic runs. We treat each linguistic component as a black box in this paper.</p><p>SearchServer typically does "inflectional" stemming which generally retains the part of speech (e.g. a plural of a noun is typically stemmed to the singular form). It typically does not do "derivational" stemming which would often change the part of speech or the meaning more substantially (e.g. "performer" is not stemmed to "perform").</p><p>SearchServer's lexical stemming includes compound-splitting (decompounding) for compound words in Finnish (and also some other languages not investigated this year, such as German, Dutch and Swedish). For example, in German, "babykost" (baby food) has "baby" and "kost" as stems.</p><p>SearchServer's lexical stemming also supports some spelling variations. In English, British and American spellings have the same stems, e.g. "labour" stems to "labor", "hospitalisation" stems to "hospitalization" and "plough" stems to "plow".</p><p>Lexical stemmers can produce more than one stem, even for non-compound words. For example, in English, "axes" has both "axe" and "axis" as stems (different meanings), and in French, "important" has both "important" (adjective) and "importer" (verb) as stems (different parts of speech). SearchServer records all the stem mappings at index-time to support maximum recall and does so in a way to allow searching to weight some inflections higher than others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Searching</head><p>Unlike previous years, this year we experimented with SearchServer's CONTAINS predicate (instead of the IS_ABOUT predicate) though it should not make a difference to the ranking. Our test application specified SearchSQL to perform a boolean-OR of the query words. For example, for Russian topic 250 whose Title was "Бешенство у людей" (Rabies in Humans), a corresponding SearchSQL query would be:</p><formula xml:id="formula_0" coords="3,90.00,200.66,240.56,46.32">SELECT RELEVANCE('2:3') AS REL, DOCNO FROM CLEF04RU WHERE FT_TEXT CONTAINS 'Бешенство'|'у'|'людей' ORDER BY REL DESC;</formula><p>(Note that "у" is a stopword for Russian so its inclusion in the query won't actually add any matches.)</p><p>Most aspects of SearchServer's relevance value calculation are the same as described last year <ref type="bibr" coords="3,90.00,296.30,14.61,10.46" target="#b9">[10]</ref>. Briefly, SearchServer dampens the term frequency and adjusts for document length in a manner similar to Okapi <ref type="bibr" coords="3,198.04,308.25,10.51,10.46" target="#b5">[6]</ref> and dampens the inverse document frequency using an approximation of the logarithm. These calculations are based on the stems of the terms when doing morphological searching (i.e. when SET TERM_GENERATOR 'word!ftelp/inflect' was previously specified).</p><p>An experimental new default is that SearchServer only includes morphological matches from compound words if all of its stems (from a particular stemming interpretation) are in the same or consecutive words. For example, in German, a morphological search for the compound "babykost" (baby food) will no longer match "baby" or "kost" by themselves, but it will match "babykost" and "baby kost" (and if SET PHRASE_DISTANCE 1 is specified, it will also match the hyphenated "baby-kost"). Words (and compounds) still match inside compounds (and larger compounds), e.g. a search for "kost" still matches "babykost". To restore the old behaviour of matching if just one stem is in common, one can specify the /decompound option (e.g. SET TERM_GENERATOR 'word!ftelp/inflect/decompound'). See Section 3.3.1 for several more decompounding examples. This year's experimental SearchServer version contains an enhancement for handling multiple stemming interpretations. For each document, only the interpretation that produces the highest score for the document is used in the relevance calculation (but all interpretations are still used for matching and search term highlighting). Sometimes this enhancement causes the original query form of the word to get more weight than some of its inflections (and it never gets less weight). This approach overcomes the previous issue of terms with multiple stemming interpretations being over-weighted; it used to be better for CLEF experiments to workaround by using the /single or /noalt options, but Section 3.5 verifies that this is no longer the case.</p><p>SearchServer's RELEVANCE_METHOD setting can be used to optionally square the importance of the inverse document frequency (by choosing a RELEVANCE_METHOD of '2:4' instead of '2:3'). The importance of document length to the ranking is controlled by SearchServer's RELEVANCE_DLEN_IMP setting (scale of 0 to 1000). For all experiments in this paper, REL-EVANCE_METHOD was set to '2:3' and RELEVANCE_DLEN_IMP was set to 750.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Diagnostic Runs</head><p>For the diagnostic runs listed in Table <ref type="table" coords="3,253.60,640.85,3.87,10.46" target="#tab_1">2</ref>, the run names consist of a language code ("FI" for Finnish, "FR" for French, "PT" for Portuguese and "RU" for Russian) followed by one of the following labels:</p><p>• "lex": The run used SearchServer's lexical stemming with decompounding enabled, i.e. SET TERM_GENERATOR 'word!ftelp/inflect/decompound'. (Of the investigated languages, decompounding only makes a difference for Finnish.)</p><p>• "compound" (Finnish only): Same as "lex" except that /decompound was not specified.</p><p>• "single": Same as "lex" except that /single was additionally specified (so that just one stemming interpretation was used). • "alg": The run used a different index based on the coarser algorithmic Porter "Snowball" stemmer <ref type="bibr" coords="4,155.90,454.33,10.51,10.46" target="#b4">[5]</ref> for the language. Decompounding is not available with this stemmer and the /single option is redundant.</p><p>• "chain": The run used a different index based on applying the SearchServer stemmer (as "lex") and then the algorithmic stemmer.</p><p>• "none": The run disabled morphological searching, i.e. SET TERM_GENERATOR ''.</p><p>Note that all diagnostic runs just used the Title field of the topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Evaluation Measures</head><p>The primary evaluation measure in this paper is "mean average precision" based on the first 1000 retrieved documents for each topic (denoted "AvgP" in Tables <ref type="table" coords="4,362.59,598.24,4.98,10.46" target="#tab_1">2</ref> and<ref type="table" coords="4,390.27,598.24,3.87,10.46" target="#tab_7">9</ref>). "Average precision" for a topic is the average of the precision after each relevant document is retrieved (using zero as the precision for relevant documents which are not retrieved). The score ranges from 0.0 (no relevants found) to 1.0 (all relevants found at the top of the list). For a set of topics, all topics are weighted equally by the mean. Average precision takes into account both precision and recall, and it is very good for detecting retrieval differences because even small differences in the ranks of relevant documents affect the score. A more experimental measure is "robustness at 10 documents" (denoted "Robust@10") which is the percentage of topics for which at least one relevant document was returned in the first 10 rows (this was one of the measures investigated in the TREC Robust Retrieval track last year <ref type="bibr" coords="4,90.00,717.79,14.76,10.46" target="#b10">[11]</ref>). This measure hides a lot of retrieval differences (particularly in recall), but it may be an indicator of a user's impression of a method's robustness across topics. We also list the Robust@1 and Robust@5 variants. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Statistical Significance Tables</head><p>For tables comparing 2 diagnostic runs (such as Table <ref type="table" coords="5,328.99,247.13,3.87,10.46" target="#tab_2">3</ref>), the columns are as follows:</p><p>• "Expt" specifies the experiment. The language code is given, followed by the labels of the 2 runs being compared. The difference is the first run minus the second run. For example, "FI lex-none" specifies the difference of subtracting the scores of the Finnish 'none' run from the Finnish 'lex' run (of Table <ref type="table" coords="5,250.14,301.50,3.87,10.46" target="#tab_1">2</ref>).</p><p>• "AvgDiff" is the difference of the mean scores of the two runs being compared (the table heading says which evaluation measure is being compared).</p><p>• "95% Conf" is an approximate 95% confidence interval for the difference calculated using Efron's bootstrap percentile method<ref type="foot" coords="5,276.19,362.76,3.97,7.32" target="#foot_1">2</ref> [2] (using 100,000 iterations). If zero is not in the interval, the result is "statistically significant" (at the 5% level), i.e. the feature is unlikely to be of neutral impact, though if the average difference is small (e.g. &lt;0.020) it may still be too minor to be considered "significant" in the magnitude sense.</p><p>• "vs." is the number of topics on which the first run scored higher, lower and tied (respectively) compared to the second run. These numbers should always add to the number of topics (45 for Finnish, 49 for French, 46 for Portuguese, 34 for Russian).</p><p>• "3 Extreme Diffs (Topic)" lists 3 of the individual topic differences, each followed by the topic number in brackets (the topic numbers range from 201 to 250). The first difference is the largest one of any topic (based on the absolute value). The third difference is the largest difference in the other direction (so the first and third differences give the range of differences observed in this experiment). The middle difference is the largest of the remaining differences (based on the absolute value).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results of Morphological Experiments</head><p>This section looks at the differences between the runs of Table <ref type="table" coords="5,365.88,576.26,4.98,10.46" target="#tab_1">2</ref> in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Impact of Lexical Stemming</head><p>Table <ref type="table" coords="5,117.77,622.22,4.98,10.46" target="#tab_2">3</ref> isolates the impact of SearchServer's lexical stemming on the average precision measure (e.g. "FI lex-none" is the difference of the "FI-lex" and "FI-none" runs of Table <ref type="table" coords="5,432.07,634.17,3.87,10.46" target="#tab_1">2</ref>). For each of the 4 languages, the increase in mean average precision was statistically significant (i.e. zero is not in the approximate 95% confidence interval). Note that for some queries, it is still better to only match the original query form (not inflections); SearchServer allows this option to be controlled for each query term at search-time. Table <ref type="table" coords="5,133.37,693.95,4.98,10.46" target="#tab_3">4</ref> isolates the impact of SearchServer's lexical stemming on the Robust@10 measure. For each language, at least one relevant was found in the first 10 rows more often with inflections enabled than disabled; the increases were statistically significant for Finnish and Russian. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison with Algorithmic Stemming</head><p>Table <ref type="table" coords="6,116.90,360.23,4.98,10.46">5</ref> contains the results of a diagnostic experiment comparing average precision when the only difference is the stemmer used: the experimental SearchServer lexical stemmer or Porter's algorithmic stemmer. Positive differences indicate that the SearchServer stemmer led to a higher score and negative differences indicate that the algorithmic stemmer led to a higher score. Using Search-Server's stemmer scored higher on average for each language and this increase was statistically significant for Finnish.</p><p>In this section, we look at the Portuguese and Russian topics with the largest differences. Finnish is examined in more detail in the subsequent decompounding section. French was investigated in last year's paper <ref type="bibr" coords="6,210.51,455.87,14.61,10.46" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Portuguese Stemming</head><p>Topic PT-229: Table <ref type="table" coords="6,187.68,500.15,4.98,10.46">5</ref> shows that the largest difference between the stemming approaches for Portuguese was on topic 229 (Construção de Barragens (Dam Building)) in which average precision was 53 points higher with SearchServer's stemmer. The main reason was that, unlike the algorithmic stemmer, the SearchServer stemmer matched "Barragem", an inflection used in many relevant documents. SearchServer additionally matched "construções" which may also have been helpful.</p><p>Topic PT-217: The next largest difference for Portuguese was on topic 217 (Sida em África (AIDS in Africa)) for which Table <ref type="table" coords="6,251.09,583.84,4.98,10.46">5</ref> shows that average precision was 32 points higher with SearchServer's stemmer. The main reason was that, unlike SearchServer, the algorithmic stemmer matched "sido", a common word unrelated to AIDS, which decreased precision substantially. SearchServer additionally matched "africanos" which may also have been helpful.</p><p>Topic PT-204: The largest negative difference was on topic 204 (Vítimas de Avalanches (Victims of Avalanches)) for which using the algorithmic stemmer scored 27 points higher. Both stemmers matched "Avalanche" but the algorithmic stemmer additionally matched "avalancha" which was the only variant used in 3 of the relevant documents. We should investigate this case further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Russian Stemming</head><p>Topic RU-227: Table <ref type="table" coords="6,188.08,723.76,4.98,10.46">5</ref> shows that the largest difference between the stemming approaches for Russian was on topic 227 (Алтайская амазонка (Altai Ice Maiden)) for which average precision was 40 points higher with SearchServer's stemmer. SearchServer internally produced 2 stems for "Алтайская" (Altai), itself and "Алтайскай". The words which had "Алтайская" as a stem (such as "Алтайской", "Алтайские", "Алтайскую" and "алтайских") were less common in the documents than the words which shared the "Алтайскай" stem (the same words plus more such as "Алтайского", "Алтайском" and "Алтайскому"), so SearchServer's experimental new scoring scheme for alternative stems gives the former group a higher weight from inverse document frequency than the latter group. In this case, it turned out just 1 relevant document was matched by either stemmer and it just used the original word "Алтайская". The algorithmic stemmer produced just one stem for these words, so its weighting did not have a preference for the query form and some documents with the second group of terms ended up ranking higher. The algorithmic stemmer additionally matched "Алтайске" which was not helpful in this case. This topic illustrates a benefit from SearchServer's experimental new handling of multiple stemming interpretations.</p><p>Topic RU-202: The next largest difference was on topic 202 (Арест Ника Леесон (Nick Leeson's Arrest)) for which the score was 20 points higher with SearchServer's stemmer. The 3 relevant documents used different spellings for "Leeson" ("Лисон", "Лизона", "Лизон" and "Лисона") which did not match the query form of "Леесон" with either stemmer. And inflections of "Арест" (Arrest) did not appear in the relevant documents. So the matches just came from variants of "Nick". Both stemmers matched the forms used in the relevant documents ("Ника" and "Ник"). But the algorithmic stemmer additionally matched other terms such as "Никому" and "никого" which lowered precision substantially in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Impact of Decompounding (Finnish)</head><p>The first row of Table <ref type="table" coords="7,190.95,492.75,4.98,10.46" target="#tab_4">6</ref> ("FI lex-cmpd") isolates the impact of SearchServer's experimental new "/decompound" option for Finnish (decompounding is not new to SearchServer for Finnish, but an option to control its impact separately from inflectional stemming at search-time is). This option allows words to match if they share any stem of query compound words. Without the /decompound option, the (experimental new) default is to require all the stems of a compound word to be in the same or consecutive words to be considered a match. Table <ref type="table" coords="7,431.35,552.53,4.98,10.46" target="#tab_4">6</ref> shows that mean average precision was 9 points higher with /decompound set, and this difference was statistically significant.</p><p>The second row of Table <ref type="table" coords="7,211.63,588.39,4.98,10.46" target="#tab_4">6</ref> ("FI cmpd-none") shows that even without the /decompound option, use of SearchServer's stemming for Finnish scored 14 points higher than not using stemming. (Note that the first two rows of Table <ref type="table" coords="7,234.55,612.30,4.98,10.46" target="#tab_4">6</ref> add up to the 23 point gain from lexical stemming shown in Table <ref type="table" coords="7,117.39,624.26,3.87,10.46" target="#tab_2">3</ref>.)</p><p>The third row of Table <ref type="table" coords="7,216.28,636.21,4.98,10.46" target="#tab_4">6</ref> ("FI cmpd-alg") compares SearchServer's stemming without the /decompound option to algorithmic stemming (which does not even decompound at index-time) and shows that using SearchServer's stemmer scored 4.5 points higher, though this difference did not quite pass the statistical significance test. (SearchServer's stemming with the /decompound option is compared to algorithmic stemming in Table <ref type="table" coords="7,327.41,684.04,4.98,10.46">5</ref> in which the difference is the sum of the differences of rows 1 and 3 of Table <ref type="table" coords="7,247.59,695.99,3.87,10.46" target="#tab_4">6</ref>.)</p><p>We look at some Finnish topics in more detail to understand these results better. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Finnish Decompounding</head><p>Topic FI-210: Table <ref type="table" coords="8,185.18,248.56,4.98,10.46" target="#tab_4">6</ref> shows that the largest impact of Finnish decompounding was on topic 210 (Nobel rauhanpalkintoehdokkaat (Nobel Peace Prize Candidates)) for which using Search-Server's stemmer with the /decompound option scored 98 points higher than not using /decompound (and also 98 points higher than using the algorithmic stemmer according to Table <ref type="table" coords="8,477.62,284.42,3.87,10.46">5</ref>). This topic had just 1 relevant document, and the only match for the non-decompounding approaches was the word "Nobel" which occurred in lots of documents, so the relevant document did not stand out among them. With SearchServer's decompounding, many more words in the relevant document matched such as "rauhan", "rauhanpalkituksi", "rauhanpalkinnon", "rauhanvälittäjänä", "ehdokasta" and "ehdokkaina" because these words shared at least one (but not all) the stems of the query compound "rauhanpalkintoehdokkaat", and the relevant document was ranked first. Topic FI-226: Table <ref type="table" coords="8,196.81,368.12,4.98,10.46" target="#tab_4">6</ref> shows that the next largest impact of Finnish decompounding was on topic 226 (Sukupuolenvaihdosleikkaukset (Sex-change Operations)) for which using SearchServer's stemmer with the /decompound option scored 72 points higher than not using /decompound (and also 86 points higher than using the algorithmic stemmer according to Table <ref type="table" coords="8,461.98,403.98,3.87,10.46">5</ref>). The algorithmic stemmer just found the one of the 13 relevant documents which contained the query word "Sukupuolenvaihdosleikkaukset". SearchServer without /decompound matched that document plus 3 other relevants, two which contained "sukupuolen vaihdosleikkaukseen" (an example of a consecutive-word match) and one which contained "Sukupuolenvaihdosleikkausta". SearchServer with /decompound matched all 13 relevant documents; the key additional matches appeared to be "Sukupuolen-vaihdos", "sukupuolenvaihtoleikkaukset", "sukupuolenvaihdot", "Sukupuolenvaihdoshan", "sukupuolenkorjausleikkausten" and "sukupuolenvahvistusleikkaus", though other matching words may also have been helpful such as "leikkaussali", "sukupuoli" and "vaihdos".</p><p>Topic FI-219: Table <ref type="table" coords="8,202.00,511.58,4.98,10.46" target="#tab_4">6</ref> shows that the largest negative impact of Finnish decompounding was on topic 219 (EU:n komissaariehdokkaat (EU Commissioner Candidates)) for which using SearchServer's stemmer with the /decompound option scored 18 points lower than not using /decompound (and also 15 points lower than using the algorithmic stemmer according to Table <ref type="table" coords="8,501.37,547.45,3.87,10.46">5</ref>). Without the /decompound option, SearchServer found a lot of precise matches in relevant documents such as "komissaariehdokasta", "komissaariehdokkaalle", "komissaariehdokkaista", "komissaariehdokkaalta", "komissaariehdokkaiden" and "komissaariehdokkaan". Furthermore, in some relevant documents it found matches in larger compounds (which the algorithmic stemmer could not) such as "naiskomissaariehdokasta" and "tanskalaiseltakomissaariehdokkaalta". With /decompound set, SearchServer would also find all these matches, but precision was substantially hurt in this case by additionally matching terms in non-relevant documents such as "jäsenehdokkaiden", "jäsenehdokkaita", "ykkösehdokkaista", "tutkimuskomissaari" and "henkilöstökomissaari". This topic shows why a user may prefer to have /decompound not set; in cases where the user does not need the component words to occur together, the user can either manually separate the terms or set the /decompound option. But for automatic ad hoc searches for topics, it is better on average to use the /decompound option. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Adding Algorithmic to Lexical Stemming</head><p>Table <ref type="table" coords="9,118.48,248.56,4.98,10.46" target="#tab_5">7</ref> shows the impact of applying the algorithmic stemmer to the result of SearchServer's stemmer (this is possible because SearchServer's stemmer returns real words; the other order would not work because the algorithmic stemmer often truncates to a non-word). This approach would still produce all the matches of SearchServer's stemming and may sometimes produce additional matches from algorithmic stemming. However, there was a decrease in mean average precision for Russian which was borderline significant. The other differences were not statistically significant. While algorithmic stemming may occasionally add a helpful match, it can also add poor matches that hurt precision. In a future experiment, perhaps it would be better to treat algorithmic stems as alternative stemming interpretations (instead of replacing the lexical stem) so that lexical inflections are likely to get higher weight when the algorithmic stem is too common.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Impact of Using All Lexical Stems</head><p>Table <ref type="table" coords="9,117.43,402.44,4.98,10.46" target="#tab_6">8</ref> shows the impact of using all stemming interpretations from SearchServer's lexical stemming instead of arbitrarily just using the first one. The increase in mean average precision was statistically significant for Russian. On the individual topics, there were some large increases, but (reassuringly) no correspondingly large decreases. In past years, mean average precision was typically lower when including all the stems because of over-weighting issues, so this result suggests that the enhancement for handling multiple stemming interpretations has succeeded at addressing this issue. Topic RU-203: The largest difference for Russian was on topic 203 (Партизанская война в Восточном Тиморе (East Timor Guerrillas)) for which the score was 67 points higher when using all stemming interpretations. The query word "Восточном" (Eastern) had 2 stems, "Восточнома" and "восточный". The inflections in the relevant document (namely "Восточного", "Восточный", "восточных" and "восточной") only shared the latter stem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Submitted Runs</head><p>In the identifiers of the runs submitted for assessment in May 2004 (e.g. "humFI04tde"), the first 3 letters "hum" indicate a Hummingbird submission, the next 2 letters are the language code, and the number "04" indicates CLEF 2004. "t", "d" and "n" indicate that the Title, Description and Narrative field of the topic were used (respectively). "e" indicates that query expansion from blind feedback on the first 2 rows was used (see last year's paper <ref type="bibr" coords="9,350.46,636.52,15.49,10.46" target="#b9">[10]</ref> for more details). The submitted runs all used inflections from SearchServer's lexical stemming (including decompounding where applicable). The scores of the submitted runs are listed in Table <ref type="table" coords="9,374.22,660.43,3.87,10.46" target="#tab_7">9</ref>.</p><p>The submitted Title-only runs (e.g. "humFI04t" of Table <ref type="table" coords="9,354.46,672.38,4.43,10.46" target="#tab_7">9</ref>) correspond to the "lex" diagnostic runs (e.g. "FI-lex" of Table <ref type="table" coords="9,209.82,684.35,4.43,10.46" target="#tab_1">2</ref>) except that the submitted runs used an older experimental version of SearchServer (including an older version of the lexical stemming component) so the scores are not exactly the same. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,125.85,117.46,351.31,77.41"><head>Table 1 :</head><label>1</label><figDesc>Sizes of CLEF 2004 Test Collections</figDesc><table coords="2,125.85,130.20,351.31,64.67"><row><cell>Language</cell><cell>Text Size (uncompressed)</cell><cell cols="3">Documents Topics Rel/Topic</cell></row><row><cell>French</cell><cell>255,334,872 bytes (244 MB)</cell><cell>90,261</cell><cell>49</cell><cell>19</cell></row><row><cell>Portuguese</cell><cell>185,739,565 bytes (177 MB)</cell><cell>55,070</cell><cell>46</cell><cell>15</cell></row><row><cell>Finnish</cell><cell>143,902,109 bytes (137 MB)</cell><cell>55,344</cell><cell>45</cell><cell>9</cell></row><row><cell>Russian</cell><cell>68,802,653 bytes (66 MB)</cell><cell>16,716</cell><cell>34</cell><cell>4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,140.71,117.46,318.27,291.55"><head>Table 2 :</head><label>2</label><figDesc>Scores of Diagnostic Title-only runs</figDesc><table coords="4,140.71,132.14,318.27,276.87"><row><cell>Run</cell><cell>AvgP</cell><cell>Robust@1</cell><cell>Robust@5</cell><cell>Robust@10</cell></row><row><cell>FI-lex</cell><cell>0.561</cell><cell>32/45 (71%)</cell><cell>36/45 (80%)</cell><cell>38/45 (84%)</cell></row><row><cell>FI-chain</cell><cell>0.553</cell><cell>30/45 (67%)</cell><cell>36/45 (80%)</cell><cell>39/45 (87%)</cell></row><row><cell>FI-single</cell><cell>0.550</cell><cell>32/45 (71%)</cell><cell>35/45 (78%)</cell><cell>37/45 (82%)</cell></row><row><cell>FI-compound</cell><cell>0.469</cell><cell>28/45 (62%)</cell><cell>30/45 (67%)</cell><cell>33/45 (73%)</cell></row><row><cell>FI-alg</cell><cell>0.424</cell><cell>26/45 (58%)</cell><cell>30/45 (67%)</cell><cell>34/45 (76%)</cell></row><row><cell>FI-none</cell><cell>0.328</cell><cell>19/45 (42%)</cell><cell>26/45 (58%)</cell><cell>27/45 (60%)</cell></row><row><cell>RU-lex</cell><cell>0.430</cell><cell>19/34 (56%)</cell><cell>27/34 (79%)</cell><cell>27/34 (79%)</cell></row><row><cell>RU-chain</cell><cell>0.405</cell><cell>18/34 (53%)</cell><cell>26/34 (76%)</cell><cell>26/34 (76%)</cell></row><row><cell>RU-single</cell><cell>0.396</cell><cell>17/34 (50%)</cell><cell>26/34 (76%)</cell><cell>27/34 (79%)</cell></row><row><cell>RU-alg</cell><cell>0.410</cell><cell>18/34 (53%)</cell><cell>26/34 (76%)</cell><cell>26/34 (76%)</cell></row><row><cell>RU-none</cell><cell>0.220</cell><cell>9/34 (26%)</cell><cell>20/34 (59%)</cell><cell>22/34 (65%)</cell></row><row><cell>PT-lex</cell><cell>0.405</cell><cell>24/46 (52%)</cell><cell>33/46 (72%)</cell><cell>35/46 (76%)</cell></row><row><cell>PT-chain</cell><cell>0.411</cell><cell>24/46 (52%)</cell><cell>34/46 (74%)</cell><cell>35/46 (76%)</cell></row><row><cell>PT-single</cell><cell>0.388</cell><cell>22/46 (48%)</cell><cell>31/46 (67%)</cell><cell>36/46 (78%)</cell></row><row><cell>PT-alg</cell><cell>0.387</cell><cell>25/46 (54%)</cell><cell>33/46 (72%)</cell><cell>34/46 (74%)</cell></row><row><cell>PT-none</cell><cell>0.327</cell><cell>18/46 (39%)</cell><cell>26/46 (57%)</cell><cell>31/46 (67%)</cell></row><row><cell>FR-lex</cell><cell>0.422</cell><cell>25/49 (51%)</cell><cell>39/49 (80%)</cell><cell>44/49 (90%)</cell></row><row><cell>FR-chain</cell><cell>0.418</cell><cell>26/49 (53%)</cell><cell>38/49 (78%)</cell><cell>42/49 (86%)</cell></row><row><cell>FR-single</cell><cell>0.423</cell><cell>26/49 (53%)</cell><cell>39/49 (80%)</cell><cell>44/49 (90%)</cell></row><row><cell>FR-alg</cell><cell>0.417</cell><cell>26/49 (53%)</cell><cell>38/49 (78%)</cell><cell>43/49 (88%)</cell></row><row><cell>FR-none</cell><cell>0.361</cell><cell>22/49 (45%)</cell><cell>39/49 (80%)</cell><cell>42/49 (86%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,104.94,117.46,393.12,79.34"><head>Table 3 :</head><label>3</label><figDesc>Impact of Lexical Stemming on Average Precision</figDesc><table coords="5,104.94,132.14,393.12,64.66"><row><cell>Expt</cell><cell>AvgDiff</cell><cell>95% Conf</cell><cell>vs.</cell><cell>3 Extreme Diffs (Topic)</cell></row><row><cell>FI lex-none</cell><cell>0.233</cell><cell>( 0.146, 0.326)</cell><cell>31-9-5</cell><cell>1.00 (224), 0.96 (210), -0.24 (208)</cell></row><row><cell>RU lex-none</cell><cell>0.209</cell><cell>( 0.108, 0.325)</cell><cell>22-1-11</cell><cell>1.00 (250), 1.00 (203), -0.04 (228)</cell></row><row><cell>PT lex-none</cell><cell>0.078</cell><cell>( 0.037, 0.125)</cell><cell>25-8-13</cell><cell>0.61 (213), 0.53 (229), -0.08 (248)</cell></row><row><cell>FR lex-none</cell><cell>0.061</cell><cell>( 0.030, 0.096)</cell><cell>23-20-6</cell><cell>0.42 (229), 0.40 (235), -0.07 (216)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,102.73,117.46,397.54,191.01"><head>Table 4 :</head><label>4</label><figDesc>Impact of Lexical Stemming on Robustness at 10 Documents</figDesc><table coords="6,102.73,132.14,397.54,176.33"><row><cell>Expt</cell><cell>AvgDiff</cell><cell>95% Conf</cell><cell>vs.</cell><cell>3 Extreme Diffs (Topic)</cell></row><row><cell>FI lex-none</cell><cell>0.244</cell><cell>( 0.133, 0.378)</cell><cell>11-0-34</cell><cell>1.00 (224), 1.00 (215), 0.00 (250)</cell></row><row><cell>RU lex-none</cell><cell>0.147</cell><cell>( 0.029, 0.265)</cell><cell>5-0-29</cell><cell>1.00 (221), 1.00 (244), 0.00 (226)</cell></row><row><cell>PT lex-none</cell><cell>0.087</cell><cell>(-0.001, 0.196)</cell><cell>5-1-40</cell><cell>1.00 (243), 1.00 (234), -1.00 (235)</cell></row><row><cell>FR lex-none</cell><cell>0.041</cell><cell>(-0.041, 0.123)</cell><cell>3-1-45</cell><cell>1.00 (241), 1.00 (239), -1.00 (222)</cell></row><row><cell></cell><cell cols="4">Table 5: Lexical vs. Algorithmic Stemming on Average Precision</cell></row><row><cell>Expt</cell><cell>AvgDiff</cell><cell>95% Conf</cell><cell>vs.</cell><cell>3 Extreme Diffs (Topic)</cell></row><row><cell>FI lex-alg</cell><cell>0.137</cell><cell>( 0.064, 0.219)</cell><cell>26-12-7</cell><cell>0.98 (210), 0.86 (226), -0.15 (219)</cell></row><row><cell>RU lex-alg</cell><cell>0.019</cell><cell>(-0.003, 0.050)</cell><cell>7-8-19</cell><cell>0.40 (227), 0.20 (202), -0.07 (224)</cell></row><row><cell>PT lex-alg</cell><cell>0.018</cell><cell>(-0.014, 0.055)</cell><cell>16-14-16</cell><cell>0.53 (229), 0.32 (217), -0.27 (204)</cell></row><row><cell>FR lex-alg</cell><cell>0.005</cell><cell>(-0.003, 0.013)</cell><cell>18-14-17</cell><cell>-0.09 (203), 0.06 (209), 0.08 (231)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,96.64,118.01,409.73,67.95"><head>Table 6 :</head><label>6</label><figDesc>Decompounding Experiments (Finnish) on Average Precision</figDesc><table coords="7,96.64,133.25,409.73,52.70"><row><cell>Expt</cell><cell>AvgDiff</cell><cell>95% Conf</cell><cell>vs.</cell><cell>3 Extreme Diffs (Topic)</cell></row><row><cell>FI lex-cmpd</cell><cell>0.092</cell><cell>( 0.034, 0.162)</cell><cell>17-9-19</cell><cell>0.98 (210), 0.72 (226), -0.18 (219)</cell></row><row><cell>FI cmpd-none</cell><cell>0.141</cell><cell>( 0.075, 0.214)</cell><cell>27-10-8</cell><cell>1.00 (224), 0.81 (204), -0.22 (208)</cell></row><row><cell>FI cmpd-alg</cell><cell>0.045</cell><cell>(-0.001, 0.094)</cell><cell>19-14-12</cell><cell>0.57 (204), 0.50 (216), -0.40 (205)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,96.29,117.46,410.43,79.34"><head>Table 7 :</head><label>7</label><figDesc>Impact of Adding Algorithmic to Lexical Stemming on Average Precision</figDesc><table coords="8,96.29,132.14,410.43,64.66"><row><cell>Expt</cell><cell>AvgDiff</cell><cell>95% Conf</cell><cell>vs.</cell><cell>3 Extreme Diffs (Topic)</cell></row><row><cell>FI chain-lex</cell><cell>-0.008</cell><cell>(-0.037, 0.019)</cell><cell>13-16-16</cell><cell>-0.38 (203), -0.30 (215), 0.31 (205)</cell></row><row><cell>RU chain-lex</cell><cell>-0.025</cell><cell>(-0.062, 0.000)</cell><cell>5-8-21</cell><cell>-0.50 (226), -0.20 (202), 0.02 (232)</cell></row><row><cell>PT chain-lex</cell><cell>0.006</cell><cell>(-0.007, 0.023)</cell><cell>9-14-23</cell><cell>0.27 (204), 0.08 (205), -0.15 (232)</cell></row><row><cell>FR chain-lex</cell><cell>-0.004</cell><cell>(-0.010, 0.003)</cell><cell>8-18-23</cell><cell>0.09 (203), -0.06 (220), -0.06 (209)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,98.34,117.46,406.33,79.34"><head>Table 8 :</head><label>8</label><figDesc>Impact of Using All Lexical Stems on Average Precision</figDesc><table coords="9,98.34,132.14,406.33,64.66"><row><cell>Expt</cell><cell>AvgDiff</cell><cell>95% Conf</cell><cell>vs.</cell><cell>3 Extreme Diffs (Topic)</cell></row><row><cell>FI lex-sing</cell><cell>0.010</cell><cell>(-0.002, 0.032)</cell><cell>7-10-28</cell><cell>0.44 (215), 0.05 (236), -0.02 (233)</cell></row><row><cell>RU lex-sing</cell><cell>0.033</cell><cell>( 0.002, 0.082)</cell><cell>9-3-22</cell><cell>0.67 (203), 0.31 (210), -0.01 (233)</cell></row><row><cell>PT lex-sing</cell><cell>0.017</cell><cell>(-0.004, 0.049)</cell><cell>6-11-29</cell><cell>0.60 (213), 0.16 (236), -0.12 (248)</cell></row><row><cell>FR lex-sing</cell><cell>-0.001</cell><cell>(-0.004, 0.003)</cell><cell>2-9-38</cell><cell>0.06 (235), -0.03 (248), -0.03 (215)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="10,141.26,117.46,317.17,182.02"><head>Table 9 :</head><label>9</label><figDesc>Scores of Submitted Runs</figDesc><table coords="10,141.26,130.20,317.17,169.27"><row><cell>Run</cell><cell>AvgP</cell><cell>Robust@1</cell><cell>Robust@5</cell><cell>Robust@10</cell></row><row><cell>humFI04t</cell><cell>0.556</cell><cell>34/45 (76%)</cell><cell>35/45 (78%)</cell><cell>37/45 (82%)</cell></row><row><cell>humFI04td</cell><cell>0.593</cell><cell>31/45 (69%)</cell><cell>37/45 (82%)</cell><cell>38/45 (84%)</cell></row><row><cell>humFI04tde</cell><cell>0.637</cell><cell>32/45 (71%)</cell><cell>40/45 (89%)</cell><cell>42/45 (93%)</cell></row><row><cell>humRU04t</cell><cell>0.430</cell><cell>19/34 (56%)</cell><cell>27/34 (79%)</cell><cell>27/34 (79%)</cell></row><row><cell>humRU04td</cell><cell>0.409</cell><cell>17/34 (50%)</cell><cell>26/34 (76%)</cell><cell>27/34 (79%)</cell></row><row><cell>humRU04tde</cell><cell>0.443</cell><cell>17/34 (50%)</cell><cell>26/34 (76%)</cell><cell>27/34 (79%)</cell></row><row><cell>humPT04t</cell><cell>0.405</cell><cell>24/46 (52%)</cell><cell>33/46 (72%)</cell><cell>35/46 (76%)</cell></row><row><cell>humPT04td</cell><cell>0.453</cell><cell>23/46 (50%)</cell><cell>32/46 (70%)</cell><cell>34/46 (74%)</cell></row><row><cell>humPT04tde</cell><cell>0.475</cell><cell>23/46 (50%)</cell><cell>32/46 (70%)</cell><cell>35/46 (76%)</cell></row><row><cell>humFR04t</cell><cell>0.421</cell><cell>25/49 (51%)</cell><cell>39/49 (80%)</cell><cell>44/49 (90%)</cell></row><row><cell>humFR04td</cell><cell>0.458</cell><cell>26/49 (53%)</cell><cell>43/49 (88%)</cell><cell>44/49 (90%)</cell></row><row><cell>humFR04tde</cell><cell>0.493</cell><cell>26/49 (53%)</cell><cell>43/49 (88%)</cell><cell>43/49 (88%)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,105.24,735.27,407.81,9.49;1,90.00,745.85,309.41,8.37"><p>SearchServer TM , SearchSQL TM and Intuitive Searching TM are trademarks of Hummingbird Ltd. All other copyrights, trademarks and tradenames are the property of their respective owners.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="5,105.24,736.39,407.87,8.37;5,90.00,745.85,256.29,8.37"><p>See<ref type="bibr" coords="5,120.53,736.39,8.94,8.37" target="#b8">[9]</ref> for some comparisons of confidence intervals from the bootstrap percentile, Wilcoxon signed rank and standard error methods for both average precision and Precision@10.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,105.49,354.66,119.12,10.46;10,259.63,354.66,177.99,10.46" xml:id="b0">
	<monogr>
		<ptr target="http://www.clef-campaign.org/" />
		<title level="m" coord="10,105.49,354.66,119.12,10.46;10,259.63,354.66,34.40,10.46">Cross-Language Evaluation web site</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.49,374.59,407.51,10.46;10,100.52,386.54,47.17,10.46" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="10,283.37,374.59,143.63,10.46">An Introduction to the Bootstrap</title>
		<author>
			<persName coords=""><forename type="first">Bradley</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><forename type="middle">J</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Chapman &amp; Hall/CRC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.49,406.47,407.51,10.46;10,100.52,418.42,310.36,10.46" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,188.11,406.47,222.98,10.46">Converting the Fulcrum Search Engine to Unicode</title>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Hodgson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,433.10,406.47,79.90,10.46;10,100.52,418.42,112.99,10.46">Sixteenth International Unicode Conference</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-03">March 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.49,438.35,32.78,10.46;10,156.14,438.35,58.66,10.46;10,232.65,438.35,18.60,10.46;10,269.11,438.35,43.71,10.46;10,330.70,438.35,11.93,10.46;10,360.50,438.35,10.93,10.46;10,389.29,438.35,39.12,10.46;10,446.28,438.35,25.18,10.46;10,489.33,438.35,23.67,10.46;10,100.52,450.31,213.71,10.46" xml:id="b3">
	<monogr>
		<ptr target="http://research.nii.ac.jp/∼ntcadm/index-en.html" />
		<title level="m" coord="10,105.49,438.35,32.78,10.46;10,156.14,438.35,58.66,10.46;10,232.65,438.35,18.60,10.46;10,269.11,438.35,43.71,10.46;10,330.70,438.35,11.93,10.46;10,360.50,438.35,10.93,10.46;10,389.29,438.35,39.12,10.46;10,446.28,438.35,25.18,10.46;10,489.33,438.35,18.94,10.46">NTCIR (NII-NACSIS Test Collection for IR Systems) Home Page</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.49,470.24,116.65,10.46;10,241.61,470.24,271.39,10.46;10,100.52,482.19,231.03,10.46" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="10,180.36,470.24,41.78,10.46;10,241.61,470.24,187.41,10.46">Snowball: A language for stemming algorithms</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
		<ptr target="http://snowball.tartarus.org/texts/introduction.html" />
		<imprint>
			<date type="published" when="2001-10">October 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.49,502.11,407.51,10.46;10,100.52,514.07,412.48,10.46;10,100.52,526.02,412.47,10.46;10,100.52,537.97,231.83,10.46" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,192.15,514.07,87.01,10.46">Okapi at TREC-3</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/pubs/trec3/t3_proceedings.html" />
	</analytic>
	<monogr>
		<title level="m" coord="10,431.92,514.07,81.07,10.46;10,100.52,526.02,221.56,10.46">Overview of the Third Text REtrieval Conference (TREC-3)</title>
		<title level="s" coord="10,338.65,526.02,65.80,10.46">NIST Special</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="500" to="226" />
		</imprint>
		<respStmt>
			<orgName>City University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.49,557.90,407.13,10.46" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="10,172.92,557.90,194.75,10.46">CLEF and Multilingual information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Jacques</forename><surname>Savoy</surname></persName>
		</author>
		<ptr target="http://www.unine.ch/info/clef/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.49,577.83,303.79,10.46" xml:id="b7">
	<monogr>
		<ptr target="http://trec.nist.gov/" />
		<title level="m" coord="10,105.49,577.83,206.29,10.46">Text REtrieval Conference (TREC) Home Page</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,105.49,597.75,394.16,10.46;10,499.65,596.68,12.85,7.32;10,100.52,609.70,412.49,10.46;10,100.52,621.67,239.15,10.46" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,192.76,597.75,306.90,10.46;10,499.65,596.68,12.85,7.32;10,100.52,609.70,41.82,10.46">Experiments in 8 European Languages with Hummingbird SearchServer TM at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<ptr target="http://clef.iei.pi.cnr.it:2002/workshop2002/WN/26.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="10,295.18,609.70,212.68,10.46">Working Notes for the CLEF 2002 Workshop</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,641.59,402.52,10.46;10,100.52,653.55,142.47,10.46;10,242.99,652.47,12.85,7.32;10,259.72,653.55,253.27,10.46;10,100.52,665.50,320.07,10.46" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,198.87,641.59,314.12,10.46;10,100.52,653.55,142.47,10.46;10,242.99,652.47,12.85,7.32;10,259.72,653.55,38.95,10.46">Lexical and Algorithmic Stemming Compared for 9 European Languages with Hummingbird SearchServer TM at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<ptr target="http://clef.iei.pi.cnr.it/2003/WN_web/19.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="10,432.78,653.55,80.22,10.46;10,100.52,665.50,111.56,10.46">Working Notes for the CLEF 2003 Workshop</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,685.43,402.54,10.46;10,100.52,697.38,412.47,10.46;10,100.52,709.33,241.79,10.46" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,221.82,685.43,286.61,10.46">Overview of the TREC 2003 Robust Retrieval Track</title>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/pubs/trec12/t12_proceedings.html" />
	</analytic>
	<monogr>
		<title level="m" coord="10,123.90,697.38,348.28,10.46">Proceedings of the Twelfth Text REtrieval Conference (TREC</title>
		<meeting>the Twelfth Text REtrieval Conference (TREC</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
