<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,188.16,70.04,235.68,11.84;1,178.26,92.72,255.48,11.84">Searching a Russian Document Collection Using English, Chinese and Japanese Queries</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,275.76,127.48,60.41,8.48"><forename type="first">Fredric</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
							<email>gey@ucdata.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UC Data Archive</orgName>
								<orgName type="department" key="dep2">Technical Assistance</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>94720</postCode>
									<settlement>Berkeley</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,188.16,70.04,235.68,11.84;1,178.26,92.72,255.48,11.84">Searching a Russian Document Collection Using English, Chinese and Japanese Queries</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">633824527305D97FBE84078AB2FE0A01</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As in CLEF 2003, Berkeley experimented with the CLEF Russian Izvestia document collection with monolingual and bilingual runs for the Russian collection. For CLEF 2004 we also experimented with Chinese and Japanese as topic languages, using English as the 'pivot' language. For bilingual retrieval our approaches were query translation (for English as a topic language) and 'fast' document translation from Russian to English (for Chinese and Japanese translated to English as the topic language). Chinese and Japanese topic retrieval significantly under-performed English Russian retrieval because of the 'double translation' loss of effectiveness.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>CLEF 2003 was the first time a Russian language document collection was available in CLEF. We had worked for several years with Russian topics in both the GIRT task and the CLEF main tasks, so extension of our techniques to Russian was straightforward No unusual methodology was applied to the Russian collection, however encoding remained an issue and we ended up using the KOI-8 encoding scheme for both Russian documents and topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Document ranking</head><p>Berkeley has used a monolingual document ranking algorithm which uses statistical clues found in documents and queries to predict a dichotomous variable (relevance) based upon logistic regression fitting of prior relevance judgments. The exact formula is: </p><formula xml:id="formula_0" coords="1,215.58,483.89,83.31,25.96">) , |<label>( 1 )</label></formula><formula xml:id="formula_1" coords="1,92.58,483.59,199.95,119.54">, | ( log ) , | ( log x x x x Q D R P Q D R P Q D R P Q D R P Q D R O * + * - * + * + - = = - = where ) , | ( ), , | ( Q D R P Q D R O</formula><p>mean, respectively, odds and probability of relevance of a document with respect to a query, and</p><formula xml:id="formula_2" coords="1,94.80,619.01,96.75,28.21">∑ = + + = n i i ql qtf n x 1 1 35 1 1 ∑ = + + = n i i dl dtf n x 1 2 80 log 1 1 ∑ = + = n i i cl ctf n x 1 3 log 1 1 n x = 4</formula><p>where n is the number of matching terms between a document and a query, and ql : query length dl: document length cl: collection length qtf_i: the within-query frequency of the ith matching term dtf_i: the within-document frequency of the ith matching term ctf_i: the occurrence frequency of the ith matching term in the collection. This formula has been used since the second TREC conference and for all NTCIR and CLEF cross-language evaluations <ref type="bibr" coords="2,137.81,278.93,9.96,8.22" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3</head><p>Russian Retrieval for the CLEF main task CLEF 2003 marked the first time a document collection was available and evaluated in the Russian language.</p><p>The CLEF Russian collection consists of 16,716 articles from Izvestia newspaper for 1995. This is a small number of documents by most CLEF measures (the smallest other collection of CLEF 2003, Finnish, has 55,344 documents; the Spanish collection has 454,045 documents). We used the Russian and English indexes generated for CLEF 2003 for all our CLEF 2004 Russian runs. The collection is also rich in metadata, including specification of geography for news articles; this can be exploited for mapping and geotemporal querying of documents relating to place and time <ref type="bibr" coords="2,233.63,393.65,10.04,8.22" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Encoding Issues</head><p>The Russian document collection was supplied in the UTF-8 unicode encoding, as were the Russian version of the topics. However, since the stemmer we employ is in KOI8 format, the entire collection was converted into KOI8 encoding, as with CLEF 2003 <ref type="bibr" coords="2,231.30,462.84,9.98,8.22" target="#b2">[3]</ref>. In indexing the collection, we converted upper-case letters to lowercase and applied Snowball's Russian stemmer (http://snowball.tartarus.org/russian/stemmer.html) together with Russian stopword list created by merging the Snowball list with a translation of the English stopword list. In addition the PROMPT translation system would also only work on KOI8 encoding which meant that our translations from English also would come in that encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Russian Monolingual Retrieval</head><p>We submitted two Russian monolingual runs, the results of which are summarized below. As in CLEF 2003, both runs utilized blind feedback, choosing the top 30 terms from the top ranked 20 documents of an initial retrieval run For BKRUMLRR1 and BKRUMLRR2 runs we used TITLE and DESCRIPTION document fields for indexing. The results of our retrieval are summarized in Adding the Narrative section to the query did not significantly improve results because the Narrative section did not contribute additional content terms beyond those found in the Title and Description fields of the topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Bilingual Retrieval from English to Russian</head><p>We submitted eight bilingual runs against the Russian document collection, four with English as topic language and two each with Chinese and Japanese as topic languages. These runs used an index in which only the TITLE and TEXT fields of each Russian document was indexed, so are directly comparable to the monolingual runs BKMLRURR1 and BKMLRURR2 above. The four English Russian runs utilized query translation from English topics into Russian.</p><p>We compared two web-available translation systems, SYSTRAN at http://babelfish.altavista.com/ for the first two runs (BKRUBLER1, BKRUBLER2) and the PROMT system (runs BKRUBLER3, BKRUBLER4) developed in Russia and found at http://www.translate.ru. The results demonstrate clearly the superiority of the PROMT system for this topic set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Bilingual Retrieval from Chinese and Japanese to Russian</head><p>Because Chinese and Japanese were available as topic languages, we experimented with these languages by translating the topics to English (i.e. used English as a pivot language). Our approach to translation from Chinese or Japanese topics to English was to utilize a widely available software package, the SYSTRAN CJK Personal system available for less than $US100. from www.systransoft.com. However, instead of query translation a second time, we utilized a technique (also used for Russian in CLEF 2003) developed by Aitao Chen, called 'Fast Document Translation' <ref type="bibr" coords="4,253.01,168.72,9.94,8.22" target="#b3">[4]</ref>. Instead of doing complete document translation using MT software, the MT system is used to translate the entire vocabulary of the document collection on a word-by-word basis without the contextualization of position in sentence with respect to other words. Monolingual retrieval was performed by matching the English versions of the Chinese or Japanese topics against the translated English document collection. More details can be found in our CLEF-2003 final paper <ref type="bibr" coords="4,391.35,211.92,9.97,8.22" target="#b2">[3]</ref>.</p><p>The results, displayed below in Table <ref type="table" coords="4,236.07,233.64,3.54,8.22" target="#tab_3">3</ref>, show that there is considerable loss of performance when using English as a pivot language for these Asian language (we have re-displayed the best English Russian runs for comparison). It may be that this performance was hampered by the reduced utility of the English documents translated from Russian, as was the case for our CLEF 2003 bilingual performance which used this method. We did not try merging of runs from the two methods to see if it would improvement performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Brief Analysis of Retrieval Performance</head><p>Our monolingual Russian performance was acceptable but certainly not outstanding. For many topics, Title-Description runs out-performed Title-Description-Narrative runs, because the Narrative section added no new information and might sometimes add noise terms. For all our runs our bilingual retrieval results were worse than monolingual (Russian-Russian) retrieval in terms of overall precision. However the translation of English to Russian by the PROMT system achieved 82% of monolingual for the TD runs. One puzzling and interesting topic was number 202 ("Nick Leeson's Arrest") where our bilingual retrieval out-performed our monolingual runs -it seems that the PROMT translation and transliteration "Арест Ника Лизона" came up with a better spelling of the last name than the Russian topic creator who used "Арест Ника Леесон", which did not seem to match any relevant documents. According to the summary results for Russian monolingual, at least one run achieved 1.00 precision for this topic; it would be most interesting to see how they modified the topic to match to the three relevant documents.</p><p>A cautionary note must be made about the CLEF-2004 Russian topic set. The total number of relevant documents was only 123 for the entire topic set, with a mean of 3.6 relevant documents per topic. Because of the nature of the retrieval results by query from the Russian collection (22 of the 34 topics have 2 or fewer relevant documents) one has to be careful about drawing conclusions from any submitted results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,92.58,584.03,396.22,96.00"><head>Table 1 .</head><label>1</label><figDesc>Results were reported by the CLEF organizers for 34 topics which had one or more relevant documents.</figDesc><table coords="2,217.44,629.69,177.19,50.34"><row><cell>Run Name</cell><cell cols="2">BKRUMLRR1 BKRUMLRR2</cell></row><row><cell>Index</cell><cell>Koi</cell><cell>Koi</cell></row><row><cell>Topic fields</cell><cell>TD</cell><cell>TDN</cell></row><row><cell>Retrieved</cell><cell>34000</cell><cell>34000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,183.48,267.16,244.92,8.48"><head>Table 1 : Berkeley Monolingual Russian runs for CLEF 2004</head><label>1</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,92.58,423.36,353.02,259.99"><head>Table 2 . Bilingual English Russian runs. .</head><label>2</label><figDesc></figDesc><table coords="3,166.44,423.36,279.17,237.78"><row><cell cols="5">Name BKRUBLER1 BKRUBLER2 BKRUBLER3 BKRUBLER4</cell></row><row><cell>Translation</cell><cell>Babelfish</cell><cell>Babelfish</cell><cell>PROMT</cell><cell>PROMT</cell></row><row><cell>Topic fields</cell><cell>TD</cell><cell>TDN</cell><cell>TD</cell><cell>TDN</cell></row><row><cell>Retrieved</cell><cell>34000</cell><cell>34000</cell><cell>34000</cell><cell>34000</cell></row><row><cell>Relevant Rel Ret</cell><cell>69</cell><cell>85</cell><cell>98</cell><cell>93</cell></row><row><cell>Precision</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>at 0.00</cell><cell>0.2444</cell><cell>0.2965</cell><cell>0.5158</cell><cell>0.4575</cell></row><row><cell>at 0.10</cell><cell>0.2430</cell><cell>0.2965</cell><cell>0.5147</cell><cell>0.4575</cell></row><row><cell>at 0.20</cell><cell>0.2423</cell><cell>0.2806</cell><cell>0.4951</cell><cell>0.4493</cell></row><row><cell>at 0.30</cell><cell>0.1809</cell><cell>0.2269</cell><cell>0.4328</cell><cell>0.4281</cell></row><row><cell>at 0.40</cell><cell>0.1563</cell><cell>0.2205</cell><cell>0.3617</cell><cell>0.3239</cell></row><row><cell>at 0.50</cell><cell>0.1445</cell><cell>0.1976</cell><cell>0.3470</cell><cell>0.2932</cell></row><row><cell>at 0.60</cell><cell>0.0896</cell><cell>0.0940</cell><cell>0.2648</cell><cell>0.1990</cell></row><row><cell>at 0.70</cell><cell>0.0796</cell><cell>0.0813</cell><cell>0.2268</cell><cell>0.1907</cell></row><row><cell>at 0.80</cell><cell>0.0771</cell><cell>0.0806</cell><cell>0.2145</cell><cell>0.1782</cell></row><row><cell>at 0.90</cell><cell>0.0764</cell><cell>0.0802</cell><cell>0.1997</cell><cell>0.1629</cell></row><row><cell>at 1.00</cell><cell>0.0764</cell><cell>0.0797</cell><cell>0.1997</cell><cell>0.1629</cell></row><row><cell>Avg. Prec.</cell><cell>0.1361</cell><cell>0.1638</cell><cell>0.3291</cell><cell>0.2850</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,113.88,300.90,384.64,261.85"><head>Table 3 . Bilingual Chinese/Japanese Russian runs</head><label>3</label><figDesc></figDesc><table coords="4,113.88,300.90,384.64,250.44"><row><cell cols="7">Run Name BKRUBLER3 BKRUBLER4 BKRUMLZE1 BKRUMLZE2BKRUMLJR1 BKRUMLJR2</cell></row><row><cell>Language</cell><cell>English</cell><cell>English</cell><cell>Chinese</cell><cell>Chinese</cell><cell>Japanese</cell><cell>Japanese</cell></row><row><cell>Translation</cell><cell>PROMT</cell><cell>PROMT</cell><cell cols="4">Systran CJK Systran CJK Systran CJK Systran CJK</cell></row><row><cell>Topic fields</cell><cell>TD</cell><cell>TDN</cell><cell>TD</cell><cell>TDN</cell><cell>TD</cell><cell>TDN</cell></row><row><cell>Retrieved</cell><cell>34000</cell><cell>34000</cell><cell>34000</cell><cell>34000</cell><cell>34000</cell><cell>34000</cell></row><row><cell>Relevant Rel Ret</cell><cell>98</cell><cell>93</cell><cell>57</cell><cell>68</cell><cell>64</cell><cell>67</cell></row><row><cell>Precision</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>at 0.00</cell><cell>0.5158</cell><cell>0.4575</cell><cell>0.1659</cell><cell>0.1924</cell><cell>0.2036</cell><cell>0.1709</cell></row><row><cell>at 0.10</cell><cell>0.5147</cell><cell>0.4575</cell><cell>0.1659</cell><cell>0.1924</cell><cell>0.2036</cell><cell>0.1709</cell></row><row><cell>at 0.20</cell><cell>0.4951</cell><cell>0.4493</cell><cell>0.1559</cell><cell>0.1822</cell><cell>0.1888</cell><cell>0.1699</cell></row><row><cell>at 0.30</cell><cell>0.4328</cell><cell>0.4281</cell><cell>0.1167</cell><cell>0.1417</cell><cell>0.1689</cell><cell>0.1249</cell></row><row><cell>at 0.40</cell><cell>0.3617</cell><cell>0.3239</cell><cell>0.1137</cell><cell>0.1414</cell><cell>0.1607</cell><cell>0.1185</cell></row><row><cell>at 0.50</cell><cell>0.3470</cell><cell>0.2932</cell><cell>0.1051</cell><cell>0.1215</cell><cell>0.1288</cell><cell>0.1130</cell></row><row><cell>at 0.60</cell><cell>0.2648</cell><cell>0.1990</cell><cell>0.0844</cell><cell>0.1077</cell><cell>0.0858</cell><cell>0.0898</cell></row><row><cell>at 0.70</cell><cell>0.2268</cell><cell>0.1907</cell><cell>0.0704</cell><cell>0.0921</cell><cell>0.0808</cell><cell>0.0846</cell></row><row><cell>at 0.80</cell><cell>0.2145</cell><cell>0.1782</cell><cell>0.0551</cell><cell>0.0782</cell><cell>0.0653</cell><cell>0.0726</cell></row><row><cell>at 0.90</cell><cell>0.1997</cell><cell>0.1629</cell><cell>0.0540</cell><cell>0.0776</cell><cell>0.0611</cell><cell>0.0701</cell></row><row><cell>at 1.00</cell><cell>0.1997</cell><cell>0.1629</cell><cell>0.0540</cell><cell>0.0776</cell><cell>0.0611</cell><cell>0.0701</cell></row><row><cell>Avg. Prec.</cell><cell>0.3291</cell><cell>0.2850</cell><cell>0.0956</cell><cell>0.1197</cell><cell>0.1166</cell><cell>0.1050</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="4">Summary and Acknowledgments</head><p>For CLEF 2004, we experimented with the CLEF Russian document collection with both monolingual Russian and bilingual to Russian from English, Chinese and Japanese topics</p><p>In addition to query translation methodology for bilingual retrieval, we tried a fast document translation method of the Russian collection to English and performed English-English monolingual retrieval with the translated topics from Chinese and English to Japanese. Chinese Russian and Japanese Russian bilingual performance results were significantly worse than query translation from English to Russian.</p><p>We would like to thank <rs type="person">Aitao Chen</rs> for supplying writing the logistic regression ranking software and for performing the fast document translation from Russian to English.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="5,106.63,356.94,412.87,8.22;5,92.58,367.48,426.90,8.48;5,92.58,378.59,45.75,8.22" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,237.26,356.94,282.23,8.22;5,92.58,367.74,81.27,8.22">Full text retrieval based on probabilistic equations with coefficients fitted by logistic regression</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Gey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,275.04,367.48,187.45,8.48">The Second Text Retrieval Conference (TREC-2)</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1994-03">March 1994</date>
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,109.79,400.19,409.63,8.22;5,92.58,411.05,375.38,8.22" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,189.78,400.19,192.58,8.22">Geotemporal Querying of Multilingual Documents</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Gey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Carl</surname></persName>
		</author>
		<ptr target="http://www.geo.unizh.ch/~rsp/gir/abstracts/gey.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="5,393.07,400.19,126.36,8.22;5,92.58,411.05,126.76,8.22">Proceedings of the Workshop on Geographic Information Retrieval</title>
		<meeting>the Workshop on Geographic Information Retrieval</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.90,432.66,385.14,8.22;5,92.58,443.26,395.37,8.48;5,92.58,454.26,56.93,8.22" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,240.70,432.66,250.34,8.22;5,92.58,443.52,99.84,8.22">UC Berkeley at CLEF-2003 -Russian Language Experiments and Domain-Specific Retrieval</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Petras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Gey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,253.32,443.26,155.41,8.48">Proceedings of the CLEF 2003 Workshop</title>
		<meeting>the CLEF 2003 Workshop</meeting>
		<imprint>
			<publisher>Springer Computer Science Series</publisher>
		</imprint>
	</monogr>
	<note>To appear in</note>
</biblStruct>

<biblStruct coords="5,106.92,475.98,412.51,8.22;5,92.58,486.46,426.81,8.48;5,92.58,497.58,18.81,8.22" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,191.57,475.98,327.86,8.22;5,92.58,486.72,77.17,8.22">Multilingual Information Retrieval Using Machine Translation, Relevance Feedback, and Decompounding</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Gey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,177.42,486.46,206.99,8.48">Information Retrieval Journal: Special Issue on CLEF</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="149" to="182" />
			<date type="published" when="2004-04">Jan-Apr 2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
