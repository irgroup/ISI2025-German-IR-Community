<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,79.00,82.53,438.04,15.50">Selection and Merging Strategies for Multilingual Information Retrieval</title>
				<funder ref="#_ExYT7CP">
					<orgName type="full">Swiss National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,235.00,107.09,55.76,11.07"><forename type="first">Jacques</forename><surname>Savoy</surname></persName>
							<email>jacques.savoy@unine.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Institut interfacultaire d&apos;informatique</orgName>
								<orgName type="institution">Université de Neuchâtel</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,297.66,107.09,76.45,11.07"><forename type="first">Pierre-Yves</forename><surname>Berger</surname></persName>
							<email>pierre-yves.berger@unine.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Institut interfacultaire d&apos;informatique</orgName>
								<orgName type="institution">Université de Neuchâtel</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,79.00,82.53,438.04,15.50">Selection and Merging Strategies for Multilingual Information Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">65E02218AA5EE03AD26C3AB76B347221</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For our fourth participation in the CLEF evaluation campaigns, our objective was to verify whether our combined query translation approach would work well with new requests and new languages (Russian and Portuguese in this case). As a second objective, we suggested a selecting procedure that could extract a smaller number of documents from collections that for the current request seem to contain no or only few relevant items. We also applied different merging strategies in order to obtain more evidence on the respective relative merits.</p><p>1.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Based on our bilingual and multilingual experiments of last years <ref type="bibr" coords="1,370.68,294.09,56.37,11.07" target="#b10">(Savoy 2003;</ref><ref type="bibr" coords="1,432.00,294.09,26.63,11.07">2004a)</ref>, we conducted different experiments involving various bilingual and multilingual test-collections. In the latter case, we retrieved documents written in the English, French, Finnish and Russian languages, based on a request written in English. As of last years, we adopted a combined query translation strategy that is able to produce queries in different European languages based on an original request written in English. Once the translation phase was completed, we searched in the corresponding document collection using our retrieval scheme (bilingual retrieval). In Section 2, we carried out multilingual information retrieval, investigating various merging strategies based on the results obtained during our bilingual searches.</p><p>Table <ref type="table" coords="1,112.07,631.09,5.00,11.07" target="#tab_1">1</ref> shows the resulting mean average precision using the various translation tools and the Okapi probabilistic model (see <ref type="bibr" coords="1,173.75,642.09,60.29,11.07" target="#b13">Savoy (2004c)</ref> for implementation details). Of course, not all tools can be used for each language, and thus as shown in Table <ref type="table" coords="1,245.22,653.09,5.00,11.07" target="#tab_1">1</ref> various entries are missing (indicated with the label "N/A"). From this data, we can see that the results from the FreeTranslation MT system usually obtain satisfactory retrieval performances (around 82% of the MAP of the corresponding monolingual search). As another good translation system, we may mention Reverso or BabelFish for the French, Prompt for the Russian or Online for both the Russian and Portuguese languages. For the Finnish language we found only two translation tools, but unfortunately their overall performance levels were not very good (a similar low level performance was also found when translating English topics into various Asian languages <ref type="bibr" coords="1,361.00,719.09,59.03,11.07" target="#b12">(Savoy 2004b)</ref>). Not surprisingly, we found there was a relationship between the various translation tools. For example, the Systran, BabelFish, and WorldLingo MT systems appeared to be nearly identical MT systems.  It is known that while a given translation tool may produce acceptable translations for a given set of requests, it may perform poorly for other queries <ref type="bibr" coords="2,269.43,299.09,53.77,11.07" target="#b10">(Savoy 2003;</ref><ref type="bibr" coords="2,325.79,299.09,25.99,11.07">2004a)</ref>. To date we have not been able to detect very precisely when a given translation will produce satisfactory retrieval performance and when it will fail. In this vein, <ref type="bibr" coords="2,112.10,322.50,85.70,10.00" target="#b5">Kishida et al. (2004)</ref> suggest using a linear regression model to predict the average precision of the current query, based on both manual evaluations of translation quality for the current query and the underlying topic difficulty. In this study, before carrying out the retrievals, we chose to concatenate two or more translations before submitting a query for translation. Table <ref type="table" coords="2,110.55,582.09,5.00,11.07" target="#tab_4">2</ref> shows the retrieval effectiveness for such combinations, using the Okapi probabilistic model. The top part of the table indicates the exact query translation combination used while the bottom part shows the mean average precision achieved by our combined query translation approach. When selecting the query translations to be combined, a priori we considered the best translation tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean average precision</head><p>The resulting retrieval performances shown in Table <ref type="table" coords="2,301.40,632.09,5.00,11.07" target="#tab_4">2</ref> are sometimes better than the best single translation scheme indicated in the row labeled "Best single" (e.g., the strategies "Comb 4" or "Comb 5" for French, or "Comb 1" for Russian, and "Comb 3" for the Portuguese language). Of course, the main difficulty in this bilingual search was the translation of English topics into Finnish, due to limited number of free translation tools. When handling those languages less-often speaking around the world, it seems it would be worthwhile considering other translation alternatives, such as probabilistic translation based on parallel corpora <ref type="bibr" coords="2,482.00,687.09,42.82,11.07;2,71.00,698.09,17.18,11.07" target="#b9">(Nie et al. 1999</ref><ref type="bibr" coords="2,88.18,698.09,133.61,11.07" target="#b8">), (MacNamee &amp; Mayfield 2003)</ref>.</p><p>For monolingual searches, as described in <ref type="bibr" coords="2,272.00,715.09,58.48,11.07" target="#b13">Savoy (2004c)</ref>, we used a data fusion search strategy that combined the Okapi and Prosit probabilistic models (see details in Section 2). The data shown in Table <ref type="table" coords="2,520.01,726.09,5.00,11.07" target="#tab_5">3</ref> indicates that our data fusion approaches may result in better retrieval effectiveness (except for the Finnish 4-gram indexing scheme or the Russian corpus). Of course before combining the result lists we could also automatically expand the translated queries, using a pseudo-relevance feedback method (Rocchio's approach in the present case). The resulting mean average precision as shown in Table <ref type="table" coords="3,393.00,93.09,5.00,11.07" target="#tab_6">4</ref> did not improve the retrieval effectiveness when compared to the best single approach. In Tables <ref type="table" coords="3,347.08,105.50,5.00,10.00" target="#tab_5">3</ref> and<ref type="table" coords="3,374.00,105.50,3.79,10.00" target="#tab_6">4</ref>, under the heading "Z-scoreW", we attached a weight of 1.5 to the Prosit model, and 1 to the Okapi model. Finally, Table <ref type="table" coords="3,426.40,115.09,5.00,11.07" target="#tab_7">5</ref> depicts the parameters used for our official bilingual runs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean average precision</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Multilingual Information Retrieval</head><p>Our multilingual information retrieval system is based on the use of a query translation strategy instead of either translating all documents into a common language (e.g., English), combining both query and document translations <ref type="bibr" coords="3,122.88,680.09,86.28,11.07" target="#b2">(Chen &amp; Gey 2003)</ref> or ignoring the translation phase <ref type="bibr" coords="3,360.00,680.09,94.23,11.07" target="#b1">(Buckley et al. 1998)</ref>, <ref type="bibr" coords="3,464.00,680.09,60.78,11.07;3,71.00,691.09,62.48,11.07" target="#b8">(MacNamee &amp; Mayfield 2003)</ref>; for a general overview of these questions, see <ref type="bibr" coords="3,337.63,691.09,106.54,11.07" target="#b0">(Braschler &amp; Peters 2004)</ref>. In our approach, when a request was received (in English in this study), we automatically translated it into the desired target languages and then searched for pertinent items within each of the four corpora (English, French, Finnish and Russian). After receiving a result list from each search engine, we needed to introduce a merging procedure to provide a unique ranked result list. As a first approach to this problem, we considered the round-robin approach whereby we took one document in turn from each individual list <ref type="bibr" coords="3,332.88,746.09,88.82,11.07" target="#b14">(Voorhees et al. 1995)</ref>.</p><p>To account for the document score computed for each retrieved item (denoted RSVk for document Dk), we might formulate the hypothesis that each collection is searched by the same or a very similar search engine and that the similarity values are therefore directly comparable <ref type="bibr" coords="4,313.00,93.09,77.56,11.07" target="#b6">(Kwok et al. 1995)</ref>. Such a strategy is called rawscore merging and produces a final list sorted by the document score computed by each collection. When using the same IR model (with the same or very similar parameter settings) to search into all collections, such a merging strategy may produce good retrieval performance (e.g., with a logistic regression IR model in <ref type="bibr" coords="4,501.00,126.09,24.01,11.07;4,71.00,137.09,20.73,11.07" target="#b3">(Chen 2003)</ref>).</p><p>Unfortunately the document scores cannot always be directly compared, thus as a third merging strategy we normalized the document scores within each collection by dividing them by the maximum score (i.e. the document score of the retrieved record in the first position) and denoted them "Norm Max". As a variant of this normalized score merging scheme (denoted "Norm RSV"), we could normalize the document RSVk scores within the ith result list, according to the following formula:</p><formula xml:id="formula_0" coords="4,127.00,213.65,397.02,12.51">Norm RSVk = ((RSVk -MinRSV i ) / (MaxRSV i -MinRSV i ))<label>(1)</label></formula><p>As a fifth merging strategy, we might use logistic regression to predict the probability of a binary outcome variable, according to a set of explanatory variables (Le <ref type="bibr" coords="4,297.33,243.09,85.42,11.07" target="#b7">Calvé &amp; Savoy 2000)</ref>. In our current case, we predicted the probability of relevance of document Dk given both the logarithm of its rank (indicated by ln(rankk)) and the original document score RSVk as indicated in Equation <ref type="formula" coords="4,305.07,265.09,3.79,11.07">2</ref>. Based on these estimated relevance probabilities (computed independently for each language using the S+ software), we sorted the records retrieved from separate collections in order to obtain a single ranked list. However, in order to estimate the underlying parameters, this approach requires that a training set is available. To achieve this, we used the CLEF-2003 topics and their relevance assessments in our evaluations. † Finally, we suggest merging the retrieved documents according to the Z-score, taken from their document scores <ref type="bibr" coords="4,98.90,632.09,52.87,11.07" target="#b10">(Savoy 2003)</ref>. Within this scheme, we need to compute, for the ith result list, the average of the RSVk (denoted MeanRSV i ) and the standard deviation (denoted StdevRSV i ). Based on these values, we can normalize the retrieval status value of each document Dk provided by the ith result list, by computing the following formula:</p><formula xml:id="formula_1" coords="4,129.00,336.90,153.49,12.85">Pr ob D k is rel | rank k , rsv k [ ] =</formula><formula xml:id="formula_2" coords="4,80.00,680.65,444.02,13.34">Z-Score RSVk = ai . [((RSVk-MeanRSV i ) / StdevRSV i ) + d i ] with d i = ((MeanRSV i -MinRSV i )/StdevRSV i ) (3)</formula><p>within which the value of d i is used to generate only positive values, and ai (usually fixed at 1) is used to reflect the retrieval performance of the underlying retrieval model and to account for the fact that pertinent items are not uniformly distributed across all collections.</p><p>Table <ref type="table" coords="5,112.04,71.09,5.00,11.07" target="#tab_8">6</ref> depicts the exact parameters used to search in the four different collections. For the Russian collection, we only considered the word-based indexing strategy while for the Finnish language we only used the 4-gram indexing scheme. In the top part of Table <ref type="table" coords="5,310.89,93.09,3.82,11.07" target="#tab_8">6</ref>, it can be seen that we used a combined query translation strategy for French, Finnish and Russian languages. As described in our monolingual experiments <ref type="bibr" coords="5,71.00,115.09,57.27,11.07" target="#b13">(Savoy 2004c)</ref>, we might also apply a data fusion phase before merging the result lists. Thus when searching into the English or French corpus, we combined the Okapi and Prosit result lists (both with blind query expansion). In a second multilingual experiment (denoted Condition B), we have applied a data fusion approach for all bilingual searches (descriptions given in the middle part of Table <ref type="table" coords="5,380.04,148.09,3.65,11.07" target="#tab_8">6</ref>). Finally, we decided to search through all corpora using the same retrieval model, Prosit in this case, as shown in the bottom part of Table <ref type="table" coords="5,520.01,159.09,5.00,11.07" target="#tab_8">6</ref> (and corresponding to Condition C).</p><p>Table <ref type="table" coords="5,110.06,187.09,5.00,11.07" target="#tab_10">7</ref> depicts the retrieval effectiveness of various merging strategies using three different bilingual search parameter settings. In this table, the round-robin scheme will be used as a baseline. On the one hand, when different search engines are merged (Condition A and Condition B), the raw-score merging strategy results in very poor mean average precision. On the other hand, when the same search engine is used (Condition C), the resulting performance is better, but this is not the best one we should be able to achieve. The normalized score merging based on Equation <ref type="formula" coords="5,186.97,242.09,5.00,11.07" target="#formula_0">1</ref>shows degradation over the simple round-robin approach when using parameter setting Condition B (0.1042 vs. 0.2340, or -4.9% in relative performance). Applying our logistic model using both the rank and the document score as explanatory variables, the resulting mean average precision is clearly better than the round-robin merging strategy and than other merging approaches (under Condition A or C). Under Condition B, the difference between our logistic model and the Z-score merging strategy is rather small (0.3111 vs. 0.3019, or 3.1% in relative performance).</p><p>As a simple alternative, we also suggest a biased round-robin approach which extracts not one document per collection per round but one document for the Russian corpus and two from the English, French and Finnish collection (because the last three represent larger corpora). This merging strategy results in good retrieval performance, better that the simple round-robin approach. Finally, the Z-score merging approach seems to provide generally satisfactory performance. Moreover, we may multiply the Z-score by an a value (performance under the label "ai = 1.5" with the ai values set as follows: EN: 1.5, FR: 1.5, FI: 1.0, and RU: 1.0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean average precision (% change) Parameters setting</head><p>Condition  It cannot be expected however that each result list would always contain pertinent items in response to a given request. In fact, a given corpus may contain no relevant information regarding the submitted request or the pertinent articles cannot be found by the search engine. In a cross-lingual environment we have found an additional problem: important facets of the original request were translated with inappropriate words or expressions. In all these cases, it is not useful to include items provided by such collections (or such search engines) in the final result list. In addition, the number of pertinent documents is usually not uniformly distributed across all four collections. For a given request (e.g., related to a regional or a national event), only one or two collections may contain relevant documents describing this particular event.</p><p>To take into account these phenomena, we have designed a selection procedure which works as follows. First, for each result list we normalize the document score according to our logistic regression method (given in Equation <ref type="formula" coords="5,111.98,720.09,3.70,11.07">2</ref>). After this step, each document score represents the probability that the underlying article is relevant (with respect to the submitted query and the collection). In the second step, for each result list (or language) we sum the document scores of the first 15 top-ranked documents. If this sum exceeds a given threshold (depending on the collection or search engine), we can thus consider that the corresponding collection contains many pertinent documents. Otherwise, we might only include the m best ranking retrieved items from the corpus (with a relatively small m value). We may thus limit the number of items extracted from a given corpus while also taking account of the fact that each collection usually contains few pertinent items. Table <ref type="table" coords="6,520.01,104.09,5.00,11.07" target="#tab_10">7</ref> lists the mean average precision achieved using this selection strategy under the label "Logistic reg. &amp; Selection (m)," where the value m indicates that we always include the m best retrieved items from each corpus in our final result list. Of course, when we set m = 0, the system will not extract any documents from a collection having a poor overall score. Finally under the label "Logistic reg. &amp; OptimalSelect", we have computed the mean average precision that can be achieved when the selection is done without any error (with m = 0). When using such an ideal selection system, the mean average precision is clearly better than all other merging strategies (e.g, under Condition C, the MAP is 0.3558 vs. 0.3393 with the logistic regression without selection).</p><p>Table <ref type="table" coords="6,112.24,198.09,5.00,11.07" target="#tab_11">8</ref> contains the descriptions of our official runs for the multilingual tracks. In the row entitled UniNEmulti3, all searches were done based on the Prosit retrieval model in order to obtain more comparable document score across the various collections. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this fifth CLEF evaluation campaign, we evaluated various query translation tools (see Table <ref type="table" coords="6,478.49,373.09,3.62,11.07" target="#tab_1">1</ref>), together with a combined translation strategy (see Table <ref type="table" coords="6,262.85,384.09,3.61,11.07" target="#tab_4">2</ref>), resulting in a retrieval performance that is worth considering. However, while a bilingual search can be viewed as easier for some language pairs (e.g., from an English query into a French document collection, or English to Portuguese), this task is clearly more complex for other language pairs (e.g., English to Finnish). Combining different result lists (see Table <ref type="table" coords="6,417.40,417.09,5.00,11.07" target="#tab_5">3</ref> or 4), we cannot always obtain a better retrieval effectiveness compared to isolated runs.</p><p>In multilingual tasks, searching documents written in different languages represents a real challenge. In this case we propose a new simple selecting strategy which will avoid extracting a relatively large number of documents from collections when these documents are of little interest with respect to the current request (see Table <ref type="table" coords="6,96.93,478.09,3.64,11.07" target="#tab_10">7</ref>). In this multilingual task, it is also interesting to mention that combining the result lists provided by same search engine (Condition C in Table <ref type="table" coords="6,246.20,489.09,4.16,11.07" target="#tab_10">7</ref>) may sometimes produce good retrieval effectiveness compared to combining different search models (Condition A in Table <ref type="table" coords="6,305.15,500.09,3.62,11.07" target="#tab_10">7</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,101.00,271.09,392.96,11.07"><head>Table 1 :</head><label>1</label><figDesc>Mean average precision of various single translation devices (TD queries, Okapi model)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="2,74.17,385.50,448.87,149.00"><head>. 2 5 2 9 0 . 3 0 4 2 0 . 3 8 8 8</head><label></label><figDesc></figDesc><table coords="2,74.17,385.50,448.87,149.00"><row><cell>Language</cell><cell>French</cell><cell>Finnish</cell><cell>Finnish</cell><cell>Russian</cell><cell>Portuguese</cell></row><row><cell></cell><cell>Okapi</cell><cell>Okapi</cell><cell>Okapi</cell><cell>Okapi</cell><cell>Okapi</cell></row><row><cell>Combination</cell><cell>49 queries</cell><cell>45 queries</cell><cell>45 queries</cell><cell>34 queries</cell><cell>46 queries</cell></row><row><cell>Comb 1</cell><cell>Bab2+Free</cell><cell>Bab1+Inter</cell><cell>Bab1+Inter</cell><cell>Bab1+Free</cell><cell>Free+Online</cell></row><row><cell>Comb 2</cell><cell>Bab2+Reverso</cell><cell></cell><cell></cell><cell>Free+Prompt</cell><cell>Bab1+Systran</cell></row><row><cell>Comb 3</cell><cell>Reverso+Systran</cell><cell></cell><cell></cell><cell>Prompt+Online</cell><cell>Bab1+Free+Onl</cell></row><row><cell>Comb 4</cell><cell>Free+Rev</cell><cell></cell><cell></cell><cell>Free+Online</cell><cell>Bab1+Free+Sys</cell></row><row><cell>Comb 5</cell><cell>Bab2+Free+</cell><cell></cell><cell></cell><cell>Bab1+Free+</cell><cell>Bab1+Free+</cell></row><row><cell></cell><cell>Reverso</cell><cell></cell><cell></cell><cell>Online</cell><cell>Online+Systran</cell></row><row><cell>Best single</cell><cell>0.3845</cell><cell>0.2290</cell><cell>0.2653</cell><cell>0.3067</cell><cell>0.4057</cell></row><row><cell>Comb 1</cell><cell>0.3784</cell><cell cols="4">0 0.4072</cell></row><row><cell>Comb 2</cell><cell>0.3857</cell><cell></cell><cell></cell><cell>0.3032</cell><cell>0.3713</cell></row><row><cell>Comb 3</cell><cell>0.3858</cell><cell></cell><cell></cell><cell>0.2964</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="2,77.00,524.50,431.00,32.00"><head>. 4 2 0 4 Comb 4 0 . 4 0 6 6</head><label></label><figDesc></figDesc><table coords="2,77.00,535.50,428.00,21.00"><row><cell></cell><cell></cell><cell>0.3043</cell><cell>0.3996</cell></row><row><cell>Comb 5</cell><cell>0.3962</cell><cell>0.3324</cell><cell>0.4070</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="2,94.00,565.09,408.01,11.07"><head>Table 2 :</head><label>2</label><figDesc>Mean average precision of various combined translation devices (TD queries, Okapi model)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="3,73.32,159.50,443.70,316.00"><head>Table 3 :</head><label>3</label><figDesc>Mean average precision of automatically translated queries (without automatic query expansion)</figDesc><table coords="3,73.32,159.50,443.70,316.00"><row><cell>Language</cell><cell>French</cell><cell>Finnish</cell><cell>Russian</cell><cell>Portuguese</cell></row><row><cell></cell><cell>word</cell><cell>4-gram</cell><cell>word</cell><cell>word</cell></row><row><cell></cell><cell>49 queries</cell><cell>45 queries</cell><cell>34 queries</cell><cell>46 queries</cell></row><row><cell>Combination \ Translation</cell><cell>Comb 5</cell><cell>Comb 1</cell><cell>Comb 4</cell><cell>Comb 3</cell></row><row><cell>Okapi</cell><cell>0.3962</cell><cell>0 . 3 0 4 2</cell><cell>0 . 3 0 4 3</cell><cell>0 . 4 2 0 4</cell></row><row><cell>Prosit</cell><cell>0.3937</cell><cell>0.2853</cell><cell>0.2928</cell><cell>0.4085</cell></row><row><cell>Round-robin</cell><cell>0.3950</cell><cell>0.2969</cell><cell>0.2943</cell><cell>0.4129</cell></row><row><cell>Sum RSV</cell><cell>0 . 3 9 8 0</cell><cell>0.2965</cell><cell>0.3036</cell><cell>0.4134</cell></row><row><cell>Norm Max</cell><cell>0.3977</cell><cell>0.2935</cell><cell>0.3010</cell><cell>0.4152</cell></row><row><cell>Norm RSV (Eq.1)</cell><cell>0.3978</cell><cell>0.2937</cell><cell>0.3010</cell><cell>0.4152</cell></row><row><cell>Z-score</cell><cell>0 . 3 9 8 0</cell><cell>0.2937</cell><cell>0.3014</cell><cell>0.4152</cell></row><row><cell>Z-scoreW</cell><cell>0.3973</cell><cell>0.2965</cell><cell>0.3009</cell><cell>0.4043</cell></row><row><cell></cell><cell></cell><cell cols="2">Mean average precision</cell><cell></cell></row><row><cell>Language</cell><cell>French</cell><cell>Finnish</cell><cell>Russian</cell><cell>Portuguese</cell></row><row><cell></cell><cell>word</cell><cell>4-gram</cell><cell>word</cell><cell>word</cell></row><row><cell></cell><cell>49 queries</cell><cell>45 queries</cell><cell>34 queries</cell><cell>46 queries</cell></row><row><cell>Combination \ Translation</cell><cell>Comb 5</cell><cell>Comb 1</cell><cell>Comb 4</cell><cell>Comb 3</cell></row><row><cell>Okapi (#docs/#terms)</cell><cell>0.4071 (5/15)</cell><cell>0.2956 (5/30)</cell><cell>0.3110 (3/15)</cell><cell>0.4315 (5/15)</cell></row><row><cell>Prosit (#docs/#terms)</cell><cell>0.4055 (5/10)</cell><cell>0.2909 (10/30)</cell><cell>0.2914 (3/15)</cell><cell>0.4724 (10/20)</cell></row><row><cell>Round-robin</cell><cell>0 . 4 1 5 3</cell><cell>0 . 2 9 9 9</cell><cell>0.3007</cell><cell>0.4637</cell></row><row><cell>Sum RSV</cell><cell>0.4096</cell><cell>0.2928</cell><cell>0.3000</cell><cell>0.4611</cell></row><row><cell>Norm Max</cell><cell>0.4091</cell><cell>0.2964</cell><cell>0.3070</cell><cell>0.4711</cell></row><row><cell>Norm RSV (Eq.1)</cell><cell>0.4126</cell><cell>0.2967</cell><cell>0.3086</cell><cell>0.4704</cell></row><row><cell>Z-score</cell><cell>0.4118</cell><cell>0.2955</cell><cell>0.3073</cell><cell>0.4699</cell></row><row><cell>Z-scoreW</cell><cell>0.4098</cell><cell>0.2948</cell><cell>0.3024</cell><cell>0.4722</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="3,80.00,484.09,442.08,110.07"><head>Table 4 :</head><label>4</label><figDesc>Mean average precision of automatically translated queries (after blind query expansion)</figDesc><table coords="3,80.00,503.09,442.08,91.07"><row><cell></cell><cell>Russian 34 queries</cell><cell>Russian 34 queries</cell><cell>Portuguese 46 queries</cell><cell>Portuguese 46 queries</cell></row><row><cell>IR model 1 (#docs/#terms) IR model 2 (#docs/#terms) Data fusion operator Translation tools Mean average precision Run name</cell><cell>Prosit (3/15) Okapi (3/15) Round-robin Free-Reverso 0.3007 UniNEBru1</cell><cell>Prosit (3/15) Okapi (3/10) Round-robin Pro-Free-Reverso 0.2962 UniNEBru2</cell><cell cols="2">Prosit (10/20) Okapi (5/15) Norm RSV Onl-Free-Bab1 Onl-Free-Sys-Bab1 Okapi (0/0) Prosit (0/0) Norm RSV 0.4704 0.4491 UniNEBpt1 UniNEBpt2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="3,124.00,603.09,348.02,11.07"><head>Table 5 :</head><label>5</label><figDesc>Description and mean average precision (MAP) of our official bilingual runs</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="4,80.00,327.50,444.02,287.66"><head>Table 6 :</head><label>6</label><figDesc>e a+b 1 ⋅ln(rank k )+b 2 ⋅rsv k 1 + e a+b 1 ⋅ln(rank k )+b 2 ⋅rsv k Description of the various runs done separately on each corpus (top descriptions form the Condition A, middle descriptions form Condition B, and bottom descriptions form Condition C)</figDesc><table coords="4,513.00,336.09,11.02,11.07"><row><cell>(2)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="5,143.00,587.09,310.24,11.07"><head>Table 7 :</head><label>7</label><figDesc>Mean average precision of various merging strategies (TD queries)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="6,71.00,240.50,437.00,88.66"><head>Table 8 :</head><label>8</label><figDesc>Description and mean average precision (MAP) of our official multilingual runs</figDesc><table coords="6,71.00,240.50,437.00,68.00"><row><cell>Run name</cell><cell cols="2">Query lang. Query type</cell><cell>Type</cell><cell>Merging</cell><cell>Parameters</cell><cell>MAP</cell></row><row><cell cols="2">UniNEmulti1 English</cell><cell>TD</cell><cell>automatic</cell><cell>logistic</cell><cell>Condition A</cell><cell>0 . 3 0 9 0</cell></row><row><cell cols="2">UniNEmulti2 English UniNEmulti3 English UniNEmulti4 English UniNEmulti5 English</cell><cell>TD TD TD TD</cell><cell cols="3">automatic automatic automatic logistic &amp; select Cond. A, m = 20 Z-scoreW Cond. A, a i = 1.5 raw-score Condition C automatic Z-scoreW Condition B</cell><cell>0.2969 0.3067 0.3010 0.3019</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The authors would like to thank the CLEF-2004 task organizers for their efforts in developing various European languages test-collections. The authors would also like to thank <rs type="person">C. Buckley</rs> from <rs type="affiliation">SabIR</rs> for giving us the opportunity to use the SMART system, together with <rs type="person">Samir Abdou</rs> for his help in translating the English topics. This research was supported by the <rs type="funder">Swiss National Science Foundation</rs> under Grant #<rs type="grantNumber">21-66 742.01</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ExYT7CP">
					<idno type="grant-number">21-66 742.01</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="6,71.00,618.09,454.01,11.07;6,85.00,629.09,88.01,11.07" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,218.41,618.09,286.81,11.07">Cross-language evaluation forum: Objectives, results and achievements</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Braschler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,516.00,618.09,9.01,11.07;6,85.00,629.09,29.74,11.07">IR Journal</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="7" to="31" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,71.00,642.09,454.03,11.07;6,85.00,653.09,373.01,11.07" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,309.00,642.09,209.69,11.07">Using clustering and superconcepts within SMART</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Waltz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,95.94,653.09,93.92,11.07">Proceedings of TREC-6</title>
		<meeting>TREC-6<address><addrLine>Gaithersburg</addrLine></address></meeting>
		<imprint>
			<publisher>NIST Special Publication</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="500" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,71.00,666.09,454.03,11.07;6,85.00,677.09,218.02,11.07" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,189.46,666.09,332.07,11.07">Combining query translation and document translation in cross-language retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Gey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,95.95,677.09,98.86,11.07">Proceedings CLEF-2003</title>
		<meeting>CLEF-2003<address><addrLine>Trondheim</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,71.00,690.09,287.43,11.07" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="6,144.93,690.09,188.12,11.07">Cross-language retrieval experiments at CLEF</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2003. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,364.00,690.09,161.02,11.07;6,85.00,701.09,440.06,11.07;6,85.00,712.09,88.00,11.07" xml:id="b4">
	<monogr>
		<title level="m" coord="6,171.73,701.09,222.37,11.07">Advances in Cross-Language Information Retrieval</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2785">2785</date>
			<biblScope unit="page" from="28" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,71.00,726.50,454.04,10.00;6,85.00,736.09,395.98,11.41" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,344.00,726.50,181.04,10.00;6,85.00,737.50,169.44,10.00">Prediction of performance on cross-lingual information retrieval by regression models</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kishida</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kuriyama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kando</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Eguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,275.16,736.09,87.36,11.07">Proceedings NTCIR-4</title>
		<meeting>NTCIR-4<address><addrLine>Tokyo</addrLine></address></meeting>
		<imprint>
			<publisher>NII</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="219" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,71.00,71.09,454.02,11.07;7,85.00,82.09,440.01,11.07;7,85.00,93.09,18.00,11.07" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,306.00,71.09,219.02,11.07;7,85.00,82.09,99.64,11.07">TREC-3 ad-hoc, routing retrieval and thresholding experiments using PIRCS</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">L</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Grunfeld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<idno>#500- 225</idno>
	</analytic>
	<monogr>
		<title level="m" coord="7,205.32,82.09,94.13,11.07">Proceedings of TREC&apos;3</title>
		<meeting>TREC&apos;3<address><addrLine>Gaithersburg</addrLine></address></meeting>
		<imprint>
			<publisher>NIST Publication</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="247" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,71.00,106.09,454.03,11.07;7,85.00,117.09,178.01,11.07" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,225.30,106.09,237.19,11.07">Database merging strategy based on logistic regression</title>
		<author>
			<persName coords=""><forename type="first">Le</forename><surname>Calvé</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Savoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,477.00,106.09,48.03,11.07;7,85.00,117.09,107.61,11.07">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="341" to="359" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,71.00,130.09,454.01,11.07;7,85.00,141.09,207.02,11.07" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,236.62,130.09,267.21,11.07">JHU/APL experiments in tokenization and non-word translation</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Macnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mayfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,85.00,141.09,98.84,11.07">Proceedings CLEF-2003</title>
		<meeting>CLEF-2003<address><addrLine>Trondheim</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="19" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,71.00,154.09,454.00,11.07;7,85.00,165.09,440.04,11.07;7,85.00,176.09,169.04,11.07" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,333.00,154.09,192.00,11.07;7,85.00,165.09,271.05,11.07">Cross-language information retrieval based on parallel texts and automatic mining of parallel texts from the Web</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Isabelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,381.00,165.09,139.19,11.07">Proceedings of the ACM-SIGIR&apos;99</title>
		<meeting>the ACM-SIGIR&apos;99<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>The ACM Press</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,71.00,189.09,454.02,11.07;7,85.00,200.09,47.02,11.07" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,152.11,189.09,180.73,11.07">Report on CLEF-2003 multilingual tracks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,359.12,189.09,109.68,11.07">Proceedings CLEF-2003</title>
		<meeting>CLEF-2003<address><addrLine>Trondheim</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,71.00,213.09,454.01,11.07;7,85.00,224.09,103.01,11.07" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,155.33,213.09,348.20,11.07">Combining multiple strategies for effective monolingual and cross-lingual retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,516.00,213.09,9.01,11.07;7,85.00,224.09,29.69,11.07">IR Journal</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="121" to="148" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,71.00,237.09,454.01,11.07;7,85.00,248.09,93.03,11.07" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,150.24,237.09,239.40,11.07">Report on CLIR task for the NTCIR-4 evaluation campaign</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,410.80,237.09,89.50,11.07">Proceedings NTCIR-4</title>
		<meeting>NTCIR-4<address><addrLine>Tokyo</addrLine></address></meeting>
		<imprint>
			<publisher>NII</publisher>
			<date type="published" when="2004">2004b</date>
			<biblScope unit="page" from="178" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,71.00,261.09,453.00,11.07" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="7,147.32,261.09,170.27,11.07">Report on CLEF-2004 monolingual tracks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,338.17,261.09,101.92,11.07">Proceedings CLEF-2004</title>
		<meeting>CLEF-2004<address><addrLine>Bath</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004c</date>
		</imprint>
	</monogr>
	<note>this volume</note>
</biblStruct>

<biblStruct coords="7,71.00,274.09,454.00,11.07;7,85.00,285.09,270.01,11.07" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="7,317.00,274.09,122.59,11.07">The collection fusion problem</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Johnson-Laird</surname></persName>
		</author>
		<idno>#500-225</idno>
	</analytic>
	<monogr>
		<title level="m" coord="7,464.00,274.09,61.00,11.07;7,85.00,285.09,30.61,11.07">Proceedings of TREC&apos;3</title>
		<meeting>TREC&apos;3<address><addrLine>Gaithersburg</addrLine></address></meeting>
		<imprint>
			<publisher>NIST Publication</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="95" to="104" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
