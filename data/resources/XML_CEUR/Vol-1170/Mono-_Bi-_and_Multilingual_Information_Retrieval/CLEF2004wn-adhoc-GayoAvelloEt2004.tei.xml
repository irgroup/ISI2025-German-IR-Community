<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,93.84,71.36,407.32,15.41;1,202.80,90.71,189.64,12.14">Application of Variable Length N-gram Vectors to Monolingual and Bilingual Information Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,167.04,123.47,79.11,8.79"><forename type="first">Daniel</forename><surname>Gayo-Avello</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">University of Oviedo</orgName>
								<address>
									<addrLine>Calvo Sotelo s/n</addrLine>
									<postCode>33007</postCode>
									<settlement>Oviedo</settlement>
									<country key="ES">SPAIN</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,253.44,123.47,96.28,8.79"><forename type="first">Darío</forename><surname>Álvarez-Gutiérrez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">University of Oviedo</orgName>
								<address>
									<addrLine>Calvo Sotelo s/n</addrLine>
									<postCode>33007</postCode>
									<settlement>Oviedo</settlement>
									<country key="ES">SPAIN</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,356.65,123.47,71.52,8.79"><forename type="first">José</forename><surname>Gayo-Avello</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">University of Oviedo</orgName>
								<address>
									<addrLine>Calvo Sotelo s/n</addrLine>
									<postCode>33007</postCode>
									<settlement>Oviedo</settlement>
									<country key="ES">SPAIN</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,93.84,71.36,407.32,15.41;1,202.80,90.71,189.64,12.14">Application of Variable Length N-gram Vectors to Monolingual and Bilingual Information Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">67B979A9DFE25BA4D16A1651A45C87D8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our group in the Department of Informatics at the University of Oviedo has participated, for the first time, in two tasks from CLEF: monolingual (Russian) and bilingual (Spanish-to-English) information retrieval. Our main goal was to test the application to IR of a modified version of n-gram vector space model (codenamed blindLight). This new approach has been successfully applied to other NLP tasks such as language identification or text summarization and the results achieved at CLEF'04, although not exceptional, are encouraging. Major differences between the blindLight approach and classical techniques are two: (1) relative frequencies are no more used as vector weights but replaced by n-gram significances, and (2) cosine distance is abandoned in favor of a new metric inspired by sequence alignment techniques although not so computationally expensive. In order to perform cross-language IR we have developed a naive n-gram pseudo-translator similar to those described by McNamee and Mayfield or Pirkola et al.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1.</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Vector model is a classic approach for text retrieval <ref type="bibr" coords="1,287.07,322.67,10.53,8.79" target="#b0">[1]</ref>. Within such a model any document (or query) can be represented as a vector of terms and, thus, similarity between text objects can be determined by a distance in the vector space (often, the cosine of the angle between the vectors). This model does not specify how to set vector weights although there are common elements to any term weighting approach: (1) term weight within a particular document, (2) term weight within the document corpus and, (3) document length normalization. Index terms are usually words or word stems, although n-grams have been also successfully used (e.g., D'amore and Mah <ref type="bibr" coords="1,91.92,391.55,11.52,8.79" target="#b1">[2]</ref> or Kimbrell <ref type="bibr" coords="1,155.53,391.55,10.44,8.79" target="#b2">[3]</ref>).</p><p>Although this model is widely used it shows two major drawbacks. First, since documents are represented by D dimensional vectors of weights, where D is the total amount of different terms in the whole document set, such vectors are not document representations by themselves but representations according to a bigger, potentially growing, "contextual" corpus. Secondly, cosine similarities (the metric most often used) between high dimensional vectors tend to be zero <ref type="foot" coords="1,212.88,452.95,3.24,5.65" target="#foot_0">1</ref> , so, to avoid this "curse of dimensionality" problem it is necessary to reduce the number of features (i.e. terms), which is usually done, when using n-grams, by setting arbitrary weight thresholds.</p><p>blindLight is a new approach differing in two aspects from the classical vector space model: (1) every document is assigned to a unique document vector with no regards to any corpus (so, in fact, there is no vector space!) and, <ref type="bibr" coords="1,70.80,518.75,10.71,8.79" target="#b1">(2)</ref>, another measure, suitable to compare different length vectors is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Foundations of the blindLight approach blindLight, as other n-gram vector space solutions, maps every document to a vector of weights; however, such document vectors are rather different from classical ones. On one hand, any two document vectors obtained through this technique are not necessarily of equal dimensions, thus, there is no actual "vector space" in this proposal. On the other hand, weights used in these vectors are not relative frequencies but the significance of each n-gram within the document.</p><p>Computing a measure of the relation between elements inside n-grams, and thus the importance of the whole ngram, is a problem with a long history of research, however, we will focus in just a few references. In 1993 Dunning described a method based on likelihood ratio tests to detect keywords and domain-specific terms <ref type="bibr" coords="1,510.29,654.35,10.53,8.79" target="#b3">[4]</ref>. However, his technique worked only for word bigrams and Ferreira da Silva and Pereira Lopes <ref type="bibr" coords="1,454.83,665.87,11.76,8.79" target="#b4">[5]</ref> were the ones who presented a generalization of different statistical measures so these could be applied to arbitrary length word n-grams. In addition to this, they also introduced a new measure, Symmetrical Conditional Probability <ref type="bibr" coords="1,512.25,688.91,12.00,8.79" target="#b5">[6]</ref> (equations 1 and 2 where (w 1 …w n ) is an n-gram), which overcomes other statistics-based measures. According to Pereira Lopes, their approach obtains better results than those achieved by Dunning.</p><p>blindLight implements the technique described by da Silva and Lopes although applied to character n-grams rather than word n-grams. Thus, it measures the relation among characters inside each n-gram and, so, the significance of every n-gram, or what is the same, the weight for the components in a document vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>∑</head><formula xml:id="formula_0" coords="2,216.00,113.78,154.49,30.54">- = = + - = 1 1 1 1 ) ... ( )• ... (<label>1</label></formula><formula xml:id="formula_1" coords="2,194.40,114.44,320.16,69.84">1 n i i n i i w w p w w p n Avp (1) Avp w w p w w f SCP n n 2 1 1 ) ... ( )) ... (( _ =<label>(2)</label></formula><p>With regards to comparisons between vectors, a simple similarity measure such as the cosine distance cannot be straightforward applied when using vectors of different dimension. Of course, it could be considered a temporary vector space of dimension d 1 +d 2 ; being d 1 and d 2 the respective dimensions of the document vectors to be compared, assigning a null weight to one vector's n-grams not present in the other and vice versa. However, we consider the absence of a particular n-gram within a document rather distinct from its presence with null significance.</p><p>Eventually, comparing two vectors with different dimensions can be seen as a pairwise alignment problem.</p><p>There are two sequences with different lengths and some (or none) elements in common that must be aligned, that is, the highest number of columns of identical pairs must be obtained by only inserting gaps, changing or deleting elements in both sequences.</p><p>One of the simplest models of distance for pairwise alignment is the so-called Levenshtein or edit distance <ref type="bibr" coords="2,512.69,321.71,11.76,8.79" target="#b6">[7]</ref> which can be defined as the smallest number of insertions, deletions, and substitutions required to change one string into another (e.g. the distance between "accommodate" and "aconmodate" is 2).</p><p>However, there are two noticeable differences between pairwise-aligning text strings and comparing different length vectors, no matter the previous ones can be seen as vectors of characters. First difference is rather important, namely, the order of components is central in pairwise alignment (e.g., DNA analysis or spell checking) while unsuitable within a vector-space model. Second one is also highly significant: although not taking into account the order of the components, "weights" in pairwise alignment are integer values while in vector-space models they are real.</p><p>Thus, distance functions for pairwise alignment, although inspiring, cannot be applied to the concerned problem. Instead, a new distance measure is needed and, in fact, two are provided. Classical vector-space based approaches assume that the distance, and so the similarity, between two document vectors is commutative (e.g., cosine distance). blindLight, however, proposes two similarity measures when comparing document vectors. For the sake of clarity, we will called those two documents query (Q) and target (T) although these similarity functions can be equally applied to any pair of documents, not only for information retrieval purposes.</p><p>Let Q and T be two blindLight document vectors with dimensions m and n:</p><formula xml:id="formula_2" coords="2,189.60,525.12,325.68,40.08">(3) ( ) ( ) ( ) { } ( ) ( ) ( ) { } nT nT T T T T mQ mQ Q Q Q Q w k w k w k T w k w k w k Q , , , , , , 2 2 1 1 2 2 1 1 K K = = (4)</formula><p>k ij is the i-th n-gram in document j while w ij is the significance (computed using SCP <ref type="bibr" coords="2,433.46,572.75,11.16,8.79" target="#b5">[6]</ref>) for the n-gram k ij within the same document j. We define the total significance for document vectors Q and T, S Q and S T respectively, as:</p><p>(5)</p><formula xml:id="formula_3" coords="2,265.68,614.13,249.60,65.38">∑ ∑ = = = = n i iT T m i iQ Q w S w S 1 1 (6)</formula><p>Then, the pseudo-alignment operator, Ω, is defined as follows:</p><formula xml:id="formula_4" coords="3,156.00,88.81,354.95,61.18">( ) ( ) ( )           &lt; ≤ ∈ &lt; ≤ ∈ = ∧ = = = Ω n j T w k m i Q w k w w w k k k w k T Q jT jT iQ iQ jT iQ x jT iQ x x x 0 , ) , (<label>, 0 , ) , ( , ) , min( , (7)</label></formula><p>Similarly to equations 5 and 6 we can define the total significance for QΩT:</p><formula xml:id="formula_5" coords="3,256.08,173.26,254.88,21.94">∑ Ω Ω = T iQ T Q w S ( 8)</formula><p>Finally, we can define two similarity measures, one to compare Q vs. T, Π (uppercase Pi), and a second one to compare T vs. Q, Ρ (uppercase Rho), which can be seen analogous to precision and recall measures:</p><formula xml:id="formula_6" coords="3,261.60,234.62,253.92,33.29">(9) T T Q Q T Q S S S S / / Ω Ω = Ρ = Π (10)</formula><p>To clarify these concepts we will show a simple example based on (one of) the shortest stories ever written. We will compare original version of Monterroso's Dinosaur with a Portuguese translation; the first one will play the query role and the second one the target, the n-grams will be quad-grams.  So, the blindLight technique, although vector-based, does not need a predefined document collection and thus, it can perform IR over ever-growing document sets. Relative frequencies are abandoned as vector weights in favor of a measure of the importance of each n-gram. In addition to this, similarity measures are analogous to those used in pairwise-alignment although computationally inexpensive and, also, non commutative which allows us to "tune" both measures, Π and Ρ, into any linear combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>Information Retrieval using blindLight blindLight has been used to extract keyphrases and summaries from single documents <ref type="bibr" coords="3,444.27,631.07,11.52,8.79" target="#b7">[8]</ref> and to perform language identification and classification of natural languages <ref type="bibr" coords="3,334.60,642.59,10.71,8.79" target="#b8">[9]</ref>. At this moment we are interested in the evaluation of this technique applied to information retrieval, reason why we developed a "quick and dirty" prototype to take part in CLEF 2004.</p><p>As with any other application of blindLight, a similarity measure to compare queries and documents is needed.</p><p>At this moment just two have been tested: Π and a more complex one (see equation 11) which provides rather satisfactory results.</p><p>( )</p><formula xml:id="formula_7" coords="4,258.48,79.33,257.04,28.38">2 ΠΡ + Π norm (11)</formula><p>The goal of the norm function shown in previous equation is just translate the range of Π•Ρ values into the range of Π values, making thus possible a comprehensive combination of both (otherwise, Ρ, and thus Π•Ρ values, are negligible when compared to Π).</p><p>The operation of the blindLight IR system is really simple:</p><p>-For each document in the collection an n-gram vector (specifically quad-gram) is obtained and stored.</p><p>-When a query is submitted to the system this computes an n-gram vector and compares it with every document from the collection obtaining Π and Ρ values.</p><p>-From these values a ranking measure is worked out, and a reverse ordered list of documents is returned as a response to the query.</p><p>This way of operation supposes both advantages and disadvantages: documents may be added to the collection at any moment because there is no indexing process; however, comparing a query with every document can be rather time consuming and not feasible with very large datasets. In order to reduce the number of document-toquery comparisons a clustering phase should be done in advance. Of course, by doing this the processing over ever-growing datasets would be no more possible because the system should be shut down periodically to perform indexing. Thorough performance analysis is needed to determine what database size requires this previous clustering.</p><p>Previously to perform tasks from CLEF, we tested the blindLight IR prototype on two very small standard collections with encouraging results. These collections were CACM (3204 documents and 64 queries) and CISI (1460 documents and 112 queries). Figure <ref type="figure" coords="4,246.98,346.43,5.04,8.79">3</ref> shows the interpolated precision-recall graphs for both collections and ranking measures (namely, pi and piro). Figure <ref type="figure" coords="4,101.28,633.47,3.78,8.79">3</ref>. Interpolated precision-recall graphs for the blindLight IR system applied to CACM and CISI test collections. Top-10 average precision for CACM and CISI was 19.8% and 19.6% respectively, in both cases using piro ranking.</p><p>Such results are similar to those reached by several systems but not as good as those achieved by other ones; for instance, 11-pt. average precision was 16.73% and 13.41% for CACM and CISI, respectively, while the SMART IR system achieves 37.78% and 19.45% for the same collections. However, it must be said that these experiments were performed over the documents and the queries just as they are, that is, common techniques such as stop-word removal, stemming, or query term weighting were not applied to the document set and the queries were provided to the system in a literal fashion 2 , as if they were actually submitted by the original users. By avoiding such techniques, the system is totally language independent, at least for non ideographic languages, although performance must be improved. One obvious area for future work are the similarity measures, we are planning to use genetic programming in order to test new measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CLEF 2004 Tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Information Retrieval Method</head><p>We applied our prototype to two tasks from CLEF 2004: monolingual and bilingual IR, specifically we query the Russian collection with the Russian topic's version and the English collection with the Spanish topics. All the queries were automatically built from the topics using both title and description fields.</p><p>The method employed to obtain the results was the following one:</p><p>1. Every SGML file from a collection was parsed to extract individual pieces of news. 2. For each piece of news a quad-gram vector was computed, as described above, from the allowed fields (typically, TEXT and TITLE or HEADLINE) and stored. 3. Once the entire collection was processed the topics to query it were also parsed, computing for every topic another quad-gram vector from title and description fields. 4. After parsing the topics file, queries (i.e., their corresponding vectors) were submitted to the prototype in batch mode obtaining ranked lists of one thousand documents. The similarity measure employed to rank the results was the so-called piro since this was the one that performed the best when applied to CACM and CISI collections; however, as it was explained before this measure is far from being good and this area needs thorough research and work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pseudo-translation of Queries</head><p>With regards to bilingual information retrieval, previous method needs minor changes in the way in which query vectors are obtained. This was done without performing actual machine translation by taking advantage of a sentence aligned corpus of languages source (S) and target (T).</p><p>A query written in the source language, QS, is splitted in word chunks (from one word to the whole query). The S corpus is gathered looking for sentences containing any of these chunks. Every sentence (up to ten) found in S is replaced by its counterpart in the T corpus. For every sentence found in T an n-gram vector is computed and then all these vectors are Ω-intersected. Since such T sentences contain, allegedly, the translation of some words from language S into language T, it can be supposed that the Ω-intersection of their vectors would contain a kind of "translated" n-grams (see Figure <ref type="figure" coords="5,218.64,470.03,3.56,8.79">4</ref>). Those word chunks that do not appear in the S corpus are incorporated without "translation". Thus, it is obtained a vector similar, in theory, to that which could be compute from a real translation from the original query.</p><p>The European Parliament Proceedings Parallel Corpus 1996-2003 <ref type="bibr" coords="5,338.90,510.35,16.79,8.79" target="#b9">[10]</ref> has been used as sentence aligned corpus and the results achieved have been really interesting. In average terms, 38.59% of the n-grams from pseudotranslated query vectors are present within the vectors from actual translated queries and, in turn, 28.31% of the n-grams from the actual translated query vectors correspond to n-grams within the pseudo-translated ones. In order to check this we have compared vectors obtained through pseudo-translation of Spanish queries into English with the vectors computed from actual English topics. This constitutes another area for future work employing different parallel corpora (e.g., OPUS, http://logos.uio.no/opus) and improving the "translation" method.</p><p>This technique is related to those described by Pirkola et al <ref type="bibr" coords="5,324.03,608.75,16.56,8.79" target="#b10">[11]</ref> to find cross-lingual spelling variants or by McNamee and Mayfield <ref type="bibr" coords="5,171.15,620.27,16.80,8.79" target="#b11">[12]</ref> to "translate" individual n-grams. The difference between such techniques and ours is that we do not attempt to obtain word translations nor individual n-gram translations but a pseudo-translation for a whole n-gram vector containing n-grams from the target language that would likely appear in actual query translations, such a vector can then be straightforward submitted to the IR system. 2 Just an example query from the CACM collection: #64 List all articles on EL1 and ECL (EL1 may be given as EL/1; I don't remember how they did it. The blindLight IR prototype processes queries like this one in an "as is" manner.</p><p>Topic 206 written in language S (Spanish) Encontrar documentos en los que se habla de las discusiones sobre la reforma de instituciones financieras y, en particular, del Banco Mundial y del FMI durante la cumbre de los G7 que se celebró en Halifax en 1995. Some sentences from corpus S (Europarl Spanish) (1315) …mantiene excelentes relaciones con las instituciones financieras internacionales. (5865) …el fortalecimiento de las instituciones financieras internacionales... (6145) La Comisión deberá estudiar un mecanismo transparente para que las instituciones financieras europeas...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Counterpart sentences from corpus T (Europarl English)</head><p>(1315) …has excellent relationships with the international financial institutions.. (5865) …strengthening international financial institutions... (5116) The Commission will have to look at a transparent mechanism so that the European financial institutions... Pseudo-translated query vector (Ω-intersection of previous T sentences) (al_i, anci, atio, cial, _fin, fina, ial_, inan, _ins, inst, ions, itut, l_in, nanc, ncia, nsti, stit, tion, titu, tuti, utio) Figure <ref type="figure" coords="6,100.56,224.03,3.78,8.79">4</ref>. Procedure to pseudo-translate a query written originally in a source language (in this case Spanish) onto a vector containing appropriate n-grams from the target language (English in this example). Blanks have been replaced by underscores, just one chunk from the query has been pseudo-translated (shown underlined).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results obtained by blindLight IR</head><p>As we said before our group submitted results for just two tasks: monolingual retrieval within Russian collection and bilingual retrieval querying the English collection using Spanish as query language. For the Russian task our prototype returned 72 from 123 relevant documents with an average precision of 0.  <ref type="table" coords="6,98.17,528.83,3.78,8.79">1</ref>. Top-5 and bottom-5 performing topics for monolingual and bilingual tasks. Top-5 are those with highest precision at 5 documents. Bottom-5 topics are those which do not provide any relevant result; the most relevant documents available within the collection the worst the query. As it can be seen, focused topics related to people, places and/or particular events are the best performers within blindLight IR prototype while broad queries are poorly managed by our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions and Future Work</head><p>blindLight is a new technique related to classical n-gram vector space models to perform several natural language processing tasks. We showed that it is well-suited to extract keyphrases and automatic summaries from single documents <ref type="bibr" coords="6,144.97,646.91,11.76,8.79" target="#b7">[8]</ref> in addition to perform language identification and classification of natural languages <ref type="bibr" coords="6,510.31,646.91,10.53,8.79" target="#b8">[9]</ref>. At this moment we are testing its applicability to information retrieval since we totally agree with McNamee and Mayfield when they say that "knowledge-light methods can be quite effective" <ref type="bibr" coords="6,391.56,669.95,15.46,8.79" target="#b11">[12]</ref>. With regards to this goal, it must be said that partial results are not outstanding but we feel optimistic about this issue since poor performance is mostly constrained to broad topics and focused queries usually achieved reasonable precision.</p><p>Three areas require thorough work: (1) Similarity measures between queries and documents must be improved, perhaps with genetic programming. (2) Different parallel corpora should be used to enhance the n-gram pseudotranslator employed to perform bilingual IR. And (3) thorough research must be done in order to improve precision when broad topics are submitted to the system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,70.80,493.79,453.72,8.79;3,70.80,505.32,453.77,10.08;3,70.80,517.55,148.96,8.79"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. blindLight document vectors for both documents in Figure 1 (truncated to show ten elements, blanks have been replaced by underscores). QΩT intersection vector is shown plus Π and Ρ values indicating the similarities between both documents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,103.20,318.31,384.62,170.15"><head></head><label></label><figDesc>Cuando despertó, el dinosaurio todavía estaba allí. (Query) Quando acordou, o dinossauro ainda estava lá. (Target) Figure 1. "El dinosaurio" by Augusto Monterroso, Spanish original and Portuguese translation.</figDesc><table coords="3,103.20,360.31,377.75,128.15"><row><cell cols="2">Q vector (45 elements)</cell><cell cols="2">T vector (39 elements)</cell><cell cols="2">QΩT (10 elements)</cell></row><row><cell cols="2">Cuan 2.489</cell><cell>va_l</cell><cell>2.545</cell><cell>saur</cell><cell>2.244</cell></row><row><cell>l_di</cell><cell>2.392</cell><cell>rdou</cell><cell>2.323</cell><cell>inos</cell><cell>2.177</cell></row><row><cell>stab</cell><cell>2.392</cell><cell>stav</cell><cell>2.323</cell><cell cols="2">uand 2.119</cell></row><row><cell>...</cell><cell></cell><cell>...</cell><cell></cell><cell>_est</cell><cell>2.091</cell></row><row><cell>saur</cell><cell>2.313</cell><cell>saur</cell><cell>2.244</cell><cell>dino</cell><cell>2.022</cell></row><row><cell cols="2">desp 2.313</cell><cell cols="2">noss 2.177</cell><cell>_din</cell><cell>2.022</cell></row><row><cell>...</cell><cell></cell><cell>...</cell><cell></cell><cell>esta</cell><cell>2.012</cell></row><row><cell cols="2">ndo_ 2.137</cell><cell>a_lá</cell><cell>2.022</cell><cell cols="2">ndo_ 1.981</cell></row><row><cell cols="2">nosa 2.137</cell><cell cols="2">o_ac 2.022</cell><cell cols="2">a_es 1.943</cell></row><row><cell>...</cell><cell></cell><cell>...</cell><cell></cell><cell cols="2">ando 1.876</cell></row><row><cell cols="2">ando 2.012</cell><cell>auro</cell><cell>1.908</cell><cell></cell><cell></cell></row><row><cell>avía</cell><cell>1.945</cell><cell cols="2">ando 1.876</cell><cell></cell><cell></cell></row><row><cell>_all</cell><cell>1.915</cell><cell cols="2">do_a 1.767</cell><cell cols="2">Π: 0.209 Ρ: 0.253</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,70.80,319.31,453.79,218.31"><head></head><label></label><figDesc>1433. With regards to the bilingual task we obtained 145 from 375 relevant documents showing an average precision of 0.0644. Such results are far from being good but we found them kind of encouraging. Firstly, it is our first participation in CLEF. Secondly, although average results are rather poor we can clearly separate classes of topics that obtain good results from other types which perform poorly (e.g., broad queries) showing us a future line of work.</figDesc><table coords="6,70.80,391.51,399.36,146.10"><row><cell></cell><cell>Top-5 performing topics (ES-EN)</cell><cell></cell><cell>Top-5 performing topics (RU)</cell></row><row><cell>218</cell><cell>Andreotti and the Mafia</cell><cell>230</cell><cell>Atlantis-Mir Docking</cell></row><row><cell>248</cell><cell>Macedonia Name Dispute</cell><cell>209</cell><cell>Tour de France Winner</cell></row><row><cell>202</cell><cell>Nick Leeson's Arrest</cell><cell>210</cell><cell>Nobel Peace Prize Candidates</cell></row><row><cell>224</cell><cell>Woman solos Everest</cell><cell>211</cell><cell>Peru-Ecuador Border Conflict</cell></row><row><cell>205</cell><cell>Tamil Suicide Attacks</cell><cell>202</cell><cell>Nick Leeson's Arrest</cell></row><row><cell></cell><cell>Bottom-5 performing topics (ES-EN)</cell><cell></cell><cell>Bottom-5 performing topics (RU)</cell></row><row><cell>212</cell><cell>Sportswomen and Doping</cell><cell>227</cell><cell>Altai Ice Maiden</cell></row><row><cell>235</cell><cell>Seal-hunting</cell><cell>203</cell><cell>East Timor Guerrillas</cell></row><row><cell>241</cell><cell>New political parties</cell><cell>207</cell><cell>Fireworks Injuries</cell></row><row><cell>214</cell><cell>Multi-billionaires</cell><cell>228</cell><cell>Prehistorical Art</cell></row><row><cell>216</cell><cell>Glue-sniffing Youngsters</cell><cell>250</cell><cell>Rabies in Humans</cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,76.08,748.58,323.45,7.95"><p>That is, two random documents have a high probability of being orthogonal to each other.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,106.80,92.90,417.62,7.95;7,106.80,103.22,144.49,7.95" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,261.21,92.90,171.06,7.95">A vector space model for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gerard Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,438.50,92.90,85.92,7.95;7,106.80,103.22,17.01,7.95">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="613" to="620" />
			<date type="published" when="1975-11">November 1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,106.80,113.54,417.80,7.95;7,106.80,123.86,40.00,7.95" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,201.11,113.54,206.65,7.95">One-time complete indexing of text: Theory and practice</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>D'amore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">P</forename><surname>Mah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,413.70,113.54,73.60,7.95">Proc. of SIGIR 1985</title>
		<meeting>of SIGIR 1985</meeting>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="page" from="155" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,106.80,134.18,305.96,7.95" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,164.65,134.18,130.72,7.95">Searching for text? Send an n-gram!</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">E</forename><surname>Kimbrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,297.37,134.18,15.46,7.95">Byte</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="297" to="312" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,106.80,144.74,417.78,7.95;7,106.80,155.06,47.44,7.95" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,155.80,144.74,226.19,7.95">Accurate methods for the statistics of surprise and coincidence</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Dunning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,388.30,144.74,94.57,7.95">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="74" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,106.80,165.38,417.30,7.95;7,106.80,175.70,243.76,7.95" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,265.90,165.38,258.20,7.95;7,106.80,175.70,146.67,7.95">A Local Maxima method and a Fair Dispersion Normalization for extracting multi-word units from corpora</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ferreira Da Silva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Pereira Lopes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,269.06,175.70,55.19,7.95">Proc. of MOL</title>
		<meeting>of MOL</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,106.82,186.02,417.53,7.95;7,106.80,196.34,111.30,7.95" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,267.17,186.02,217.81,7.95">Extracting Multiword Terms from Document Collections</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ferreira Da Silva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Pereira Lopes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,493.49,186.02,30.86,7.95;7,106.80,196.34,30.97,7.95">Proc. of VExTAL</title>
		<meeting>of VExTAL<address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,106.80,206.66,417.52,7.95;7,106.80,216.98,222.63,7.95" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,174.52,206.66,252.75,7.95">Binary codes capable of correcting deletions, insertions, and reversals</title>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">I</forename><surname>Levenshtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,143.04,216.98,83.86,7.95">Soviet Physics Doklady</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="707" to="710" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
	<note>English translation from Russian</note>
</biblStruct>

<biblStruct coords="7,106.82,227.30,417.51,7.95;7,106.80,237.86,417.77,7.95;7,106.80,248.18,82.21,7.95" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,322.79,227.30,201.54,7.95;7,106.80,237.86,316.85,7.95">Naive Algorithms for Key phrase Extraction and Text Summarization from a Single Document inspired by the Protein Biosynthesis Process</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Gayo-Avello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Álvarez-Gutiérrez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gayo-Avello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,430.84,237.86,89.63,7.95">Proc. of Bio-ADIT 2004</title>
		<meeting>of Bio-ADIT 2004</meeting>
		<imprint>
			<publisher>LNCS</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>In press</note>
</biblStruct>

<biblStruct coords="7,106.82,258.50,417.52,7.95;7,106.80,268.82,417.77,7.95;7,106.80,279.14,127.81,7.95" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,326.65,258.50,197.68,7.95;7,106.80,268.82,69.16,7.95">One Size Fits All? A Simple Technique to Perform Several NLP Tasks</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Gayo-Avello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Álvarez-Gutiérrez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gayo-Avello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,182.86,268.82,228.01,7.95">EsTAL -España for NATURAL LANGUAGE PROCESSING</title>
		<imprint>
			<date type="published" when="2004">October 20-22, 2004</date>
			<pubPlace>Alicante, Spain</pubPlace>
		</imprint>
	</monogr>
	<note>Accepted for publication</note>
</biblStruct>

<biblStruct coords="7,106.82,289.46,417.73,7.95;7,106.80,299.78,185.71,7.95" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="7,154.10,289.46,285.73,7.95">Europarl: A Multilingual Corpus for Evaluation of Machine Translation</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<ptr target="http://www.isi.edu/~koehn/publications/europarl.ps" />
		<imprint/>
	</monogr>
	<note>Draft, Unpublished</note>
</biblStruct>

<biblStruct coords="7,106.82,310.10,417.62,7.95;7,106.80,320.66,417.65,7.95;7,106.80,330.98,52.72,7.95" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,492.56,310.10,31.88,7.95;7,106.80,320.66,368.51,7.95">Targeted s-gram matching: a novel n-gram matching technique for cross-and monolingual word form variants</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pirkola</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Keskustalo</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Heikki</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Leppänen</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Erkka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antti-Pekka And</forename><surname>Känsälä</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kalervo</forename><surname>Järvelin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,481.50,320.66,42.95,7.95;7,106.80,330.98,31.45,7.95">Information Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,106.80,341.30,417.34,7.95;7,106.80,351.62,227.30,7.95" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,208.79,341.30,241.59,7.95">JHU/APL Experiments in Tokenization and Non-Word Translation</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mayfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,456.28,341.30,67.86,7.95;7,106.80,351.62,93.20,7.95">Working Notes for the CLEF 2003 Workshop</title>
		<meeting><address><addrLine>Trondheim, Norway</addrLine></address></meeting>
		<imprint>
			<date>21-22 August</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
