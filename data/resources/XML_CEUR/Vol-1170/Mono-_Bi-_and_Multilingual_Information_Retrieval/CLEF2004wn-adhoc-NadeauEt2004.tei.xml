<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,114.78,81.65,383.36,11.88">Using COTS Search Engines and Custom Query Strategies at CLEF</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,140.16,111.37,52.12,8.53"><forename type="first">David</forename><surname>Nadeau</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Language Technologies Research Centre Interactive Language Technologies Group</orgName>
								<orgName type="institution">National Research Council of Canada Gatineau</orgName>
								<address>
									<postCode>K1A 0R6</postCode>
									<region>Québec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,198.84,111.37,53.91,8.53"><forename type="first">Mario</forename><surname>Jarmasz</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Language Technologies Research Centre Interactive Language Technologies Group</orgName>
								<orgName type="institution">National Research Council of Canada Gatineau</orgName>
								<address>
									<postCode>K1A 0R6</postCode>
									<region>Québec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,259.32,111.37,64.19,8.53"><forename type="first">Caroline</forename><surname>Barrière</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Language Technologies Research Centre Interactive Language Technologies Group</orgName>
								<orgName type="institution">National Research Council of Canada Gatineau</orgName>
								<address>
									<postCode>K1A 0R6</postCode>
									<region>Québec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,329.70,111.37,52.12,8.53"><forename type="first">George</forename><surname>Foster</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Language Technologies Research Centre Interactive Language Technologies Group</orgName>
								<orgName type="institution">National Research Council of Canada Gatineau</orgName>
								<address>
									<postCode>K1A 0R6</postCode>
									<region>Québec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,403.68,111.37,69.21,8.53"><forename type="first">Claude</forename><surname>St-Jacques</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Language Technologies Research Centre Interactive Language Technologies Group</orgName>
								<orgName type="institution">National Research Council of Canada Gatineau</orgName>
								<address>
									<postCode>K1A 0R6</postCode>
									<region>Québec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,154.13,165.44,65.98,6.65"><forename type="first">Mario</forename><surname>Nadeau</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Language Technologies Research Centre Interactive Language Technologies Group</orgName>
								<orgName type="institution">National Research Council of Canada Gatineau</orgName>
								<address>
									<postCode>K1A 0R6</postCode>
									<region>Québec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,225.19,165.44,86.31,6.65"><forename type="first">Caroline</forename><surname>Jarmasz</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Language Technologies Research Centre Interactive Language Technologies Group</orgName>
								<orgName type="institution">National Research Council of Canada Gatineau</orgName>
								<address>
									<postCode>K1A 0R6</postCode>
									<region>Québec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,316.58,165.44,81.23,6.65"><forename type="first">George</forename><surname>Barriere</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Language Technologies Research Centre Interactive Language Technologies Group</orgName>
								<orgName type="institution">National Research Council of Canada Gatineau</orgName>
								<address>
									<postCode>K1A 0R6</postCode>
									<region>Québec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,402.89,165.44,71.05,6.65"><forename type="first">Claude</forename><surname>Foster</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Language Technologies Research Centre Interactive Language Technologies Group</orgName>
								<orgName type="institution">National Research Council of Canada Gatineau</orgName>
								<address>
									<postCode>K1A 0R6</postCode>
									<region>Québec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,479.02,165.44,10.16,6.65"><surname>St</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Language Technologies Research Centre Interactive Language Technologies Group</orgName>
								<orgName type="institution">National Research Council of Canada Gatineau</orgName>
								<address>
									<postCode>K1A 0R6</postCode>
									<region>Québec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,114.78,81.65,383.36,11.88">Using COTS Search Engines and Custom Query Strategies at CLEF</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3E2F7FFB5324D40C6119362361169EC5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a system for bilingual information retrieval using commercial offthe-shelf search engines (COTS). Several custom query construction, expansion and translation strategies are compared. We present the experiments and the corresponding results for the CLEF 2004 event.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In our first participation in the Cross-Language Evaluation Forum (CLEF) we entered the French monolingual task as well as the newcomer French to English bilingual tasks. This report is mainly for the latter task although some experimental results are discussed using data from the former. Our research consists in the use of two commercial off-the-shelf (COTS) search engines which we use to perform boolean queries. These search engines do not allow us to perform weighted queries; we attempt to overcome this weakness by developing innovative query strategies. We test our query construction techniques which vary the ways in which the terms are extracted from the topics. We then experiment with various approaches for querying the search engines by combining the terms using the boolean operators. We briefly explore a query expansion approach based on fuzzy logic. Finally, we investigate three different word-for-word translation methods.</p><p>We begin by presenting Copernic Enterprise Search (CES) and AltaVista Enterprise Search (AVES), the two COTS search engines used for the 2004 event. Section 3 describes the query term selection process and section 4 describes the steps for constructing the query, i.e. the manner in which the terms and operators are combined. The subsequent sections discuss the query expansion and translation approaches. We end our discussion by stating our conclusions and future work items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Commercial Off-The-Shelf Search Engines</head><p>Two commercial search engines were used for our participation at CLEF. Both offer boolean query syntax rather than weighted queries. We realize that this may be a handicap in CLEF-like competitions. Researchers have found strict binary queries to be limiting <ref type="bibr" coords="1,251.94,534.73,75.68,8.53" target="#b4">(Cöster et al., 2003)</ref>, and most of the best results from previous years rely on systems where each term in a query can be assigned a weight. UC Berkeley performed very well at CLEF 2003 using such a search engine <ref type="bibr" coords="1,223.68,556.51,44.84,8.53" target="#b3">(Chen, 2002</ref>). Yet the availability and quality of commercial search engines make them interesting resources which we feel merit proper investigation.</p><p>The first search engine that we use is Copernic Enterprise Search (CES), a system which ranked third in the topic distillation task of the Text Retrieval Conference (TREC) held in 2003 <ref type="bibr" coords="1,387.66,599.53,84.11,8.53" target="#b5">(Craswell et al., 2003)</ref>. Copernic's ranking is based on term frequency, term adjacency and inclusion of terms in automatically generated document summaries and keywords. It performs stemming using a multilingual algorithm akin to <ref type="bibr" coords="1,422.94,621.31,56.03,8.53" target="#b11">Porter's (1980)</ref>. Copernic also has the ability to handle meta-data and to take it into consideration when performing its ranking calculations. In our experiments we provided CES with the title meta-data which is found in the TITLE, TI or HEADLINE tags depending on the corpus.</p><p>The second search engine used is AltaVista Enterprise Search (AVES) which implements algorithms from the renowned AltaVista company. AVES ranking is based on term frequencies and term adjacency. It performs stemming but the exact algorithm is not documented. Meta-data was not taken into consideration for the searches performed with AVES.</p><p>Copernic retrieves more relevant documents than AltaVista for the majority of the configurations which we tested on the 2003 data. This observation holds for the CLEF 2004 data. Figure <ref type="figure" coords="2,414.84,112.27,4.74,8.53" target="#fig_0">1</ref> plots the precision-recall curves for both search engines using the 2004 Monolingual French data. The queries consist of a disjunction of the terms in the topic title. This simple strategy serves as our baseline. Our query strategies are explained in detail in the following sections. An analysis of the 2003 data allows us to observe that the use of the title meta-data, meaning that the search engine assigns a better score to documents in which query terms are found in the title, accounts for about 20% of the difference between the two systems. It is a reasonable assumption that the remaining 80% difference is due to the different ranking algorithms. Since CES and AVES are commercial products, we use them as black boxes and cannot explain the difference in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Precision-Recall Values for Baseline Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Query Term Selection</head><p>The query term selection step consists in extracting important keywords from the topic. Each topic consists of a title, a description and a narrative field. Here is an example of a French topic: We investigated various methods for exploiting these fields. Our research focused on the following strategies:</p><p>S1. the use of the title in isolation (this is our baseline); S2. the use of the description in isolation; S3. the use of the combination of the title and the description; S4. the use of Extractor <ref type="bibr" coords="3,217.92,123.07,56.98,8.53" target="#b15">(Turney, 2000)</ref> keyphrases extracted from all fields; S5. the use of the title plus the best Extractor keyphrases.</p><p>In all cases we removed the words trouvez, documents, pertinents and informations from the French topics. These words are not stop words but are commonly used in CLEF topics. Stop words are later discarded as explained in the Query Construction section. Comparison of methods can be found in Figure <ref type="figure" coords="3,443.64,177.25,3.61,8.53">2</ref>.</p><p>We have established that it is not efficient to use the narrative in isolation due to the presence of many unrelated words and because the narrative often contains a sentence explaining what not to find, for example "Les plans de réformes futures ne sont pas pertinents". More sophisticated natural language processing techniques are required to take advantage of these explanations.</p><p>Using the information contained in the topics, queries can consist of as little as two words, when using the title in isolation, or tens of words, when Extractor is used to select salient terms from the entire topic.</p><p>Exhaustive results are given in the next section but it is worth noting some interesting observations. First, the title in isolation performs well, even if it only contains a few words. Titles are indeed made of highly relevant words. All our best runs are obtained using the words in the title. Furthermore, Extractor is useful for selecting pertinent words from the description and narrative parts. The best term selection strategy we found is the use of title words combined with a number of Extractor keyphrases.</p><p>Extractor can select noun phrases from a text. In our experiments, a noun phrase containing n words is considered as n independent words instead of one lexical unit. It would be worthwhile to investigate if any gains can be obtained by searching for exact matches of these multi-word-units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Query Construction</head><p>We perform three major tasks when building our queries. (1) First, we remove stop words from the list of terms based on their frequency in the corresponding CLEF corpus. (2) Then, the terms are again sorted based on their frequency in order to create a query where the rarest word comes first. (3) Finally, we combine words using the boolean AND and OR operators. Some of our search strategies require several variants of the queries to be sent to the search engines. In this scenario, the first query usually returns a small number of documents. Then, a larger number of documents is obtained by appending the results of a second query, and so on.</p><p>Let's study the term filtering step in greater detail. First, the words that do not appear in the corpus are removed. Then, we remove terms that occur above a specified threshold. We determined this threshold, as a percentage of the total number of documents. For example, the very frequent French stop word "le" appears in about 95% of documents, while the less frequent stop word "avec" appears in about 47% of the documents. We trained our system using the 2003 CLEF data and tested it using the 2002 data. Using these corpora we set our threshold for the exclusion of terms in a query at about 25%.</p><p>The second step involves sorting the terms according to their frequencies in the corpora, from least frequent to most frequent. This decision is based on the TF-IDF idea <ref type="bibr" coords="3,322.14,595.39,100.90,8.53" target="#b12">(Salton &amp; Buckley, 1988)</ref> which states that a rare, infrequent term is more informative that a common, frequent term. These informative terms allow obtaining precise results. Sorting is useful with the strategy described next.</p><p>The last step is to issue the query to the search engine. Here we experimented with two variants. The first, which we use as baseline, is a simple disjunction of all terms. The second, which we call Successive Constraint Relaxation (SCR) consists in sending successive queries to the search engine starting with a conjunction of all terms and ending with a disjunction of all terms. The constraints, which are represented by the conjunctions, are replaced with disjunctions term by term, starting by the last term, meaning the least informative, in the query.</p><p>When necessary, a query containing the previously removed terms is issued to obtain a list of 1000 documents for our results. Here's a sample query for which the constraints are successively relaxed:</p><p>Given the following words with their frequency in the corpus: incendies (394), domestiques <ref type="bibr" coords="4,468.12,101.65,19.07,8.53">(194)</ref>, causes (1694) and maison (4651), SCR issues: Query 1: domestiques AND incendies AND causes AND maison Query 2: domestiques AND incendies AND (causes OR maison) Query 3: domestiques AND (incendies OR causes OR maison) Query 4: domestiques OR incendies OR causes OR maison On Clef 2004 data, SCR produces 4% more relevant documents than a simple disjunction. Figure <ref type="figure" coords="4,473.70,188.05,4.74,8.53">2</ref> shows the results of our query construction strategies using a disjunction of four to eight terms. Figure <ref type="figure" coords="4,451.20,198.85,4.74,8.53">3</ref> shows the same experiments but using SCR. The results are plotted using the CLEF 2003 monolingual-French data. Precision is not plotted here, since experiment is conducted using a fixed (1000) number of documents. We developed our strategies and trained our system using the CLEF 2003 data. We tested all combinations of preceding approaches on the 2002 data. We identified the following methods as being the best query term selection and query construction strategy:</p><p>• Use terms from the title plus the three best Extractor keyphrases from the entire topic.</p><p>• Remove any words that appear in more than 25% of documents.</p><p>• Sort low-frequency first.</p><p>• Keep at most 8 terms.</p><p>• Issue queries using successive constraint relaxation.</p><p>In the bilingual track, all our runs use this combination of strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Query Expansion</head><p>It has been reported that query expansion using pseudo-relevance feedback generally improves results for Information Retrieval (Buckey and <ref type="bibr" coords="5,233.94,252.67,52.12,8.53" target="#b2">Salton, 1995)</ref> and is very effective in CLEF-like settings <ref type="bibr" coords="5,462.90,252.67,56.73,8.53;5,93.18,263.65,20.42,8.53" target="#b8">(Lam-Adesina, 2002)</ref>. In our experiments, our query expansion strategy relies on a Pseudo-Thesaurus construction approach <ref type="bibr" coords="5,93.18,274.45,65.49,8.53" target="#b10">(Miyamoto, 1990</ref>) making use of the fuzzy logic operator of max-min composition <ref type="bibr" coords="5,414.48,274.45,77.48,8.53" target="#b7">(Klir &amp; Yuan, 1995)</ref>. The approach is to take the N-best search engine results (hereafter N-best corpus), to extend our initial query with other pertinent words from that corpus as determined by evaluating their fuzzy similarity to the query words. Texts from N-best corpus are segmented into sentences and a term set (W) of single words is extracted after filtering prepositions, conjunctions and adverbs from the vocabulary of the DAFLES dictionary <ref type="bibr" coords="5,459.84,317.65,59.97,8.53;5,93.18,328.45,20.42,8.53" target="#b16">(Verlinde et al., 2003)</ref>. The number of occurrences per sentence for all words is determined. The association between every word pairs is then calculated using the following fuzzy similarity measure:</p><p>Let ( ) ik w f be the frequency of the word</p><formula xml:id="formula_0" coords="5,221.88,358.99,220.89,76.00">W w i ∈ in the sentence k from the N-best corpus. ( ) ( ) = k jk ik k jk ik j i w f w f w f w f w w sim ) ( ), ( max ) ( ), ( min )</formula><p>, ( Among all words, the closest ones to the original query terms were added to our query. We tried adding 1 to 10 terms when building the N-best corpus with 5, 10, 25 and 50 documents. This did not improve results, when tested on CLEF 2002 data. The same conclusion holds for 2004. A possible explanation of the lack of improvement with our query expansion algorithm may be that search engines using only boolean queries may not be able to take advantage of these expanded terms. The extra words added to the queries can be unrelated to the topic, and should have a smaller weight than the initial query terms. The Pseudo-Thesaurus gives confidence levels for its expanded list of terms, but we were not able to incorporate this information into our final queries. More investigation is needed to understand why our query expansion attempt failed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Query Translation</head><p>A critical part of bilingual information retrieval is the translation of queries, or conversely the translation of the target documents. In our experiments we decided to translate the queries using three different methods. As a baseline we use the free Babel Fish translation service <ref type="bibr" coords="5,303.42,628.51,69.38,8.53" target="#b0">(Babel Fish, 2004)</ref>. We compare this to (1) an automatic translation method which relies on TERMIUM Plus ® <ref type="bibr" coords="5,335.10,639.31,67.40,8.53" target="#b13">(Termium, 2004)</ref>, an English-French-Spanish terminological knowledge base which contains more than 3 500 000 terms recommended by the Translation Bureau of Canada and (2) a statistical machine translation technique inspired by IBM Model 1 <ref type="bibr" coords="5,465.42,661.09,54.21,8.53;5,93.18,671.89,20.42,8.53" target="#b1">(Brown et al., 1993)</ref>, which we call BagTrans. BagTrans has been trained on part of the Europarl Corpus and the Canadian Hansard. The following sections present Termium, BagTrans and, finally, the results obtained by all systems.</p><p>The terms stored in Termium are arranged in records, each record containing all the information in the database pertaining to one concept, and each record dealing with only one concept alone <ref type="bibr" coords="6,447.60,80.05,68.12,8.53" target="#b9">(Leonhardt, 2004)</ref>. Thus the translation task becomes one of word sense disambiguation, where a term must be matched to its most relevant record; this record in turn offers us standardized and alternative translations. A record contains a list of subject fields and entries, all of which are in English and French, and some of which are in Spanish as well. Entries include the main term, synonyms and abbreviations. The translation procedure attempts to find an overlap between the subject fields of the terms in the query so as to select the correct record of the word which is being translated. If none is found, then the most general term is selected. Generality is determined by the number of times a term appears across all records for a given word, or, if the records themselves do not provide adequate information, generality is determined by the term frequency in a terabyte-sized corpus of unlabeled text <ref type="bibr" coords="6,496.20,166.45,23.47,8.53;6,93.18,177.25,67.07,8.53" target="#b14">(Terra and Clarke, 2003)</ref>. When a word is not contained in Termium, then its translation is obtained using Babel Fish. More details about the translation procedure using Termium can be found in <ref type="bibr" coords="6,382.44,188.05,107.39,8.53" target="#b6">(Jarmasz and Barrière, 2004)</ref>.</p><p>Given a French word, BagTrans assigns probabilities to individual English words that reflect their likelihood of being the translation of that word and then uses the most probable word in the English query. The probability of an English word e is then calculated as the average over all French tokens f in the query of the probability p(e|f) that e is the translation of f. Translation probabilities p(e|f) are derived from the standard bag-of-words translation IBM Model 1, and estimated from parallel corpora using the EM algorithm. Two different parallel corpora were used in our experiments: the Europarl corpus, containing approximately 1M sentence pairs and 60M words; and a segment of the Hansard corpus, containing approximately 150,000 sentence pairs and 6.5M words.</p><p>Figure <ref type="figure" coords="6,132.42,306.85,4.74,8.53" target="#fig_2">4</ref> shows the precision and recall curves for the three translation techniques as measured using the CLEF 2004 data. The automatic translations strategies with Babel Fish and BagTrans do not perform any word sense disambiguation, whereas the ones using Termium attempt to disambiguate the senses by determining the context from the other terms in the query. Note that Termium found more relevant documents than Babel Fish but its precision-recall curve is lower. There are many ways in which our Termium and BagTrans translation systems can be improved. None have been customized or trained in particular for this CLEF competition. Since our search engines use boolean operators, an incorrect translation can have a big impact on the results. As we do not take context into consideration when using Babel Fish or BagTrans, it is not surprising that the translations are often incorrect. Termium, on the other hand, is a governmental terminological database and it may contain only specific senses of a word, which might be more correct in some official sense, yet less popular. Termium suffers from being normative. We will continue to pursue automatic machine translation methods which can be trained on specific corpora like BagTrans and which take into account the correct word senses for our future participations at CLEF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Precision-Recall Values for Three Translation Strategies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion and Future Work</head><p>In our first participation in the Cross-Language Evaluation Forum (CLEF) we participated in the French monolingual and French to English bilingual tasks. We use two COTS search engines and implement various query strategies. The best setup we found consists in creating a query using all words of the topic title plus the 3 best keyphrases of Extractor. We filter stop words based on their frequency in the corpus. Then we sort terms from the rarest to the most frequent. We retrieved documents by issuing successive queries to the search engine, starting with a conjunction of all terms and gradually relaxing constraint by adding disjunction of terms. For the bilingual aspect, BagTrans, a statistical model based on IBM Model 1, yields the best results.</p><p>Two main points need more investigation. The first one is our unsuccessful use of the pseudo-relevance feedback. We believe that a strict boolean search engine may be problematic for this kind of algorithm. Indeed, the insertion of only one irrelevant term may lead to irrelevant documents. A weighted query may be the key to smoothen the impact of those terms, especially when our pseudo-relevance feedback algorithm has the ability to output confidence values.</p><p>Another pending question is why Termium found many more documents than Babel Fish while the latter present a higher precision-recall curve. We believe it means that Termium did not rank the relevant documents as well as the other strategies did. The explanation, though, remains unclear.</p><p>For our next participation, we plan to use a search engine which can perform weighted queries. We'll concentrate on pseudo-relevance feedback, known to be useful at CLEF. We should also add a third language to our translation models to participate in another bilingual track.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,138.18,424.57,336.27,8.53"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Precision and recall values for the two search engines using our baseline strategy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,173.10,436.81,266.43,8.53"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig. 2. Results of various term selection strategies using a disjunction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,204.96,667.93,203.07,8.53"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Comparison of the three translation strategies.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8.">Acknowledgements</head><p>Thanks to <rs type="person">Roland Kuhn</rs> and <rs type="person">Peter Turney</rs> for thorough reading and helpful comments.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="7,93.18,523.39,399.81,8.53" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Babel</forename><surname>Fish</surname></persName>
		</author>
		<ptr target="http://babelfish.altavista.com/" />
		<title level="m" coord="7,168.42,523.39,85.63,8.53">Babel Fish Translation</title>
		<imprint>
			<date type="published" when="2004-08">2004. August 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,93.18,545.17,426.53,8.53;7,103.98,555.97,330.33,8.53" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,402.06,545.17,117.65,8.53;7,103.98,555.97,162.62,8.53">The Mathematics of Statistical Machine Translation: Parameter Estimation</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">F</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">J</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,272.82,555.97,98.82,8.53">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="311" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,93.18,577.57,426.53,8.53;7,103.98,588.37,383.43,8.53" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,228.18,577.57,165.65,8.53">Optimization of relevance feedback weights</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,400.44,577.57,119.27,8.53;7,103.98,588.37,343.95,8.53">Proceedings of the 18th annual international ACM SIGIR conference on research and development in information retrieval</title>
		<meeting>the 18th annual international ACM SIGIR conference on research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="351" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,93.18,609.97,386.53,8.53;7,103.98,620.77,71.67,8.53" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,159.24,609.97,202.38,8.53">Cross-Language Retrieval Experiments at CLEF 2002</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,368.22,609.97,42.21,8.53">CLEF 2002</title>
		<title level="s" coord="7,417.18,609.97,62.53,8.53;7,103.98,620.77,67.01,8.53">Cross-Language Evaluation Forum</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,93.18,642.37,426.48,8.53;7,103.98,653.17,311.61,8.53" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,284.52,642.37,235.14,8.53;7,103.98,653.17,119.96,8.53">Selective compound splitting of Swedish queries for Boolean combinations of truncated terms</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Cöster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sahlgren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Karlgren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,230.16,653.17,42.21,8.53">CLEF 2003</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,93.18,674.95,404.17,8.53;7,103.98,685.75,253.65,8.53" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Wilkinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<title level="m" coord="7,325.56,674.95,171.79,8.53;7,103.98,685.75,176.39,8.53">Overview of the TREC 2003 Web Track, The Twelfth Text Retrieval Conference, TREC-2003</title>
		<meeting><address><addrLine>Washington, D. C.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,93.18,69.07,426.49,8.53;8,103.98,80.05,346.35,8.53" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="8,240.42,69.07,279.25,8.53;8,103.98,80.05,126.15,8.53">A Terminological Resource and a Terabyte-Sized Corpus for Automatic Keyphrase in Context Translation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jarmasz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Barrière</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>National Research Council of Canada</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="8,93.18,101.65,374.43,8.53" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="8,212.52,101.65,102.26,8.53">Fuzzy Sets and Fuzzy Logic</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J</forename><surname>Klir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Prentice Hall</publisher>
			<pubPlace>Upper Saddle River, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,93.18,123.07,426.34,8.53;8,103.98,133.87,415.83,8.53;8,103.98,144.85,74.37,8.53" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,262.74,123.07,256.78,8.53;8,103.98,133.87,67.58,8.53">Exeter at CLEF 2001: Experiments with Machine Translation for bilingual retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Lam-Adesina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J H</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,178.86,133.87,43.83,8.53">CLEF 2001</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<meeting><address><addrLine>Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2406</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,93.18,166.45,426.53,8.53;8,103.98,177.25,24.33,8.53" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Leonhardt</surname></persName>
		</author>
		<ptr target="http://www.termium.gc.ca/site/histo_e.html" />
		<title level="m" coord="8,179.22,166.45,73.78,8.53">Termium ® History</title>
		<imprint>
			<date type="published" when="2004-08">2004. August 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,93.18,198.85,426.34,8.53;8,103.98,209.65,81.93,8.53" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="8,177.60,198.85,213.85,8.53">Fuzzy Sets in Information Retrieval and Cluster Analysis</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Miyamoto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Dordrecht, Netherlands</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,93.18,231.25,312.33,8.53" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,173.82,231.25,128.20,8.53">An Algorithm for Suffix Stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,307.92,231.25,31.84,8.53">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="130" to="127" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,93.18,252.85,387.60,8.53;8,103.98,263.65,271.86,8.53" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,226.02,252.85,204.18,8.53">Term-weighting approaches in automatic text retrieval</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,435.72,252.85,45.06,8.53;8,103.98,263.65,206.64,8.53">Information Processing and Management: an International Journal</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="513" to="523" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,93.18,285.43,326.37,8.53;8,103.98,296.05,219.27,8.53" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="8,160.50,285.43,254.77,8.53">The Government of Canada&apos;s Terminology and Linguistic Database</title>
		<author>
			<persName coords=""><surname>Termium</surname></persName>
		</author>
		<ptr target="http://www.termium.com/" />
		<imprint>
			<date type="published" when="2004-08">2004. August 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,93.18,317.65,426.30,8.53;8,103.98,328.45,415.73,8.53;8,103.98,339.43,371.37,8.53" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="8,256.44,317.65,245.33,8.53">Frequency estimates for statistical word similarity measures</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Terra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,103.98,328.45,415.73,8.53;8,103.98,339.43,248.12,8.53">Proceedings of the Human Language Technology and North American Chapter of Association of Computational Linguistics Conference 2003 (HLT/NAACL 2003)</title>
		<meeting>the Human Language Technology and North American Chapter of Association of Computational Linguistics Conference 2003 (HLT/NAACL 2003)<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="244" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,93.18,361.03,405.21,8.53" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="8,174.18,361.03,174.99,8.53">Learning Algorithms for Keyphrase Extraction</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">D</forename><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,355.26,361.03,80.41,8.53">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="303" to="336" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,93.18,382.63,426.63,8.53;8,103.98,393.43,50.11,8.53;8,176.88,393.43,58.93,8.53;8,258.60,393.43,9.42,8.53;8,290.82,393.43,29.97,8.53;8,343.56,393.43,24.91,8.53;8,391.26,393.43,34.99,8.53;8,449.04,393.43,9.60,8.53;8,481.44,393.43,38.27,8.53;8,103.98,404.23,308.73,8.53" xml:id="b16">
	<analytic>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Verlinde</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Selva</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Grelep</surname></persName>
		</author>
		<ptr target="http://www.kuleuven.ac.be/dafles/acces.php?id=/" />
	</analytic>
	<monogr>
		<title level="m" coord="8,243.98,382.63,212.54,8.53;8,492.78,382.63,23.17,8.53">Groupe de Recherche en Lexicographie Pédagogique)</title>
		<title level="s" coord="8,107.83,393.43,46.25,8.53;8,176.88,393.43,58.93,8.53;8,258.60,393.43,9.42,8.53;8,290.82,393.43,29.97,8.53;8,343.56,393.43,24.91,8.53;8,391.26,393.43,34.99,8.53;8,449.04,393.43,9.60,8.53;8,481.44,393.43,29.22,8.53">Dictionnaire d&apos;apprentissage du français langue étrangère ou seconde</title>
		<imprint>
			<date type="published" when="2003-08">2003. August 2004</date>
		</imprint>
	</monogr>
	<note>Dafles</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
