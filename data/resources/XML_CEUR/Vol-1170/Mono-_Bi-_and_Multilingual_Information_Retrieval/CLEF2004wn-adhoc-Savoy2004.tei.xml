<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,81.00,82.53,434.04,15.50">Data Fusion for Effective European Monolingual Information Retrieval</title>
				<funder ref="#_CG3AMDe">
					<orgName type="full">Swiss National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,276.00,107.09,58.03,11.07"><forename type="first">Jacques</forename><surname>Savoy</surname></persName>
							<email>jacques.savoy@unine.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Institut interfacultaire d&apos;informatique</orgName>
								<orgName type="institution">Université de Neuchâtel</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,81.00,82.53,434.04,15.50">Data Fusion for Effective European Monolingual Information Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">703D6CC903249B349BECCCC3792E5779</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For our fourth participation in the CLEF evaluation campaigns, our first objective was to propose an effective and general stopword list and a light stemming procedure for the Portuguese language. Our second objective was to obtain a better picture of the relative merit of various search engines when processing documents in the Finnish and Russian languages. Finally, based on the Z-score method we suggested a data fusion strategy intended to improve monolingual searches in various European languages.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Based on our experiments of the previous years <ref type="bibr" coords="1,285.83,294.09,54.95,11.07" target="#b10">(Savoy 2003;</ref><ref type="bibr" coords="1,344.04,294.09,26.28,11.07">2004a)</ref>, we are participating in French, Finnish, Russian and Portuguese monolingual tasks without relying on a dictionary and using fully automated approaches. This paper describes the information retrieval models we used in the monolingual tracks and is organized as follows: Section 1 contains an overview of the test-collections built during this evaluation campaign while Section 2 describes our general approach to building stopword lists and stemmers for use with languages other than English. Section 3 evaluates two probabilistic models and nine vector-space schemes using five different languages. Finally, Section 4 describes and evaluates various data fusion operators, together with our official runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Overview of the Test-Collections</head><p>The corpora used in our experiments included newspaper and news agency articles, for example the Glasgow Herald <ref type="bibr" coords="1,103.79,437.09,63.36,11.07">(1995, English)</ref>, Le <ref type="bibr" coords="1,191.00,437.09,92.73,11.07">Monde (1995, French)</ref>, SDA <ref type="bibr" coords="1,315.00,437.09,205.90,11.07">(Schweizerische Depeschenagentur, 1995, French)</ref>, <ref type="bibr" coords="1,71.00,448.09,118.35,11.07">Aamulehti (1994/95, Finnish)</ref>, <ref type="bibr" coords="1,196.26,448.09,117.64,11.07">Izvestia (1995, Russian), and</ref><ref type="bibr" coords="1,316.71,448.09,109.59,11.07">Público (1995, Portuguese)</ref>. As shown in Table <ref type="table" coords="1,517.49,448.09,3.76,11.07" target="#tab_1">1</ref>, these corpora are of various sizes, with the French collection being the biggest (244 MB) and the Portuguese, English and Finnish collections ranking second (around 150 MB). Finally the Russian collection ranks as the smallest, both in size (68 MB) and in number of documents <ref type="bibr" coords="1,316.46,481.09,16.41,11.07">(16,</ref><ref type="bibr" coords="1,332.86,481.09,16.41,11.07">716)</ref>. Across all the corpora the mean number of distinct indexing terms per document is relatively similar (around 130), but this number is a little bit larger for the Portuguese collection (180.94) and smaller for the Russian corpus <ref type="bibr" coords="1,368.94,503.09,32.70,11.07">(124.53)</ref>. As for the mean number of indexing terms per article (listed in third part of Table <ref type="table" coords="1,291.49,514.09,3.62,11.07" target="#tab_1">1</ref>), the Portuguese documents have the largest mean size (254.96), the English corpus ranks second (mean value: 200.72), and the Russian collection has the smallest mean document size <ref type="bibr" coords="1,155.92,536.09,32.72,11.07">(163.24)</ref>. However this last corpus exhibits also the largest variability (standard deviation: 252.41) in terms of document length.</p><p>Table <ref type="table" coords="1,110.66,565.50,5.00,10.00" target="#tab_1">1</ref> (bottom part) also compares the number of relevant documents per request, with the mean always being greater than the median (e.g., for the English collection, the average number of relevant documents per query is 8.93 with the corresponding median being 4). These findings indicate that each collection contains numerous queries, yet only a rather small number of relevant items are found. For each collection, 50 queries were created. Relevant documents cannot however be found for each request and each language. For the French collection, Query #227 does not have any relevant items; for the English collection, these requests are <ref type="bibr" coords="1,502.00,620.50,23.00,10.00">#203,</ref><ref type="bibr" coords="1,71.00,631.50,22.63,10.00">#220,</ref><ref type="bibr" coords="1,96.53,631.50,22.63,10.00">#225,</ref><ref type="bibr" coords="1,122.06,631.50,22.63,10.00">#227,</ref><ref type="bibr" coords="1,147.59,631.50,22.63,10.00">#234,</ref><ref type="bibr" coords="1,173.12,631.50,22.63,10.00">#243,</ref><ref type="bibr" coords="1,198.65,631.50,63.31,10.00">#244 and #250;</ref><ref type="bibr" coords="1,264.86,631.50,152.27,10.00">for the Finnish corpus: Queries #206,</ref><ref type="bibr" coords="1,421.00,631.50,22.77,10.00">#227,</ref><ref type="bibr" coords="1,448.00,631.50,22.77,10.00">#231,</ref><ref type="bibr" coords="1,475.00,631.50,22.77,10.00">#240,</ref><ref type="bibr" coords="1,502.00,631.50,23.00,10.00">#247;</ref><ref type="bibr" coords="1,71.00,642.50,160.25,10.00">for the Russian corpus: Queries #204,</ref><ref type="bibr" coords="1,235.03,642.50,22.93,10.00">#205,</ref><ref type="bibr" coords="1,261.73,642.50,22.93,10.00">#206,</ref><ref type="bibr" coords="1,288.44,642.50,22.93,10.00">#208,</ref><ref type="bibr" coords="1,315.14,642.50,22.93,10.00">#217,</ref><ref type="bibr" coords="1,341.85,642.50,22.93,10.00">#219,</ref><ref type="bibr" coords="1,368.55,642.50,22.93,10.00">#222,</ref><ref type="bibr" coords="1,395.26,642.50,22.93,10.00">#223,</ref><ref type="bibr" coords="1,421.96,642.50,22.93,10.00">#229,</ref><ref type="bibr" coords="1,448.66,642.50,22.93,10.00">#236,</ref><ref type="bibr" coords="1,475.37,642.50,22.93,10.00">#240,</ref><ref type="bibr" coords="1,502.07,642.50,22.93,10.00">#243,</ref><ref type="bibr" coords="1,71.00,653.50,22.62,10.00">#246,</ref><ref type="bibr" coords="1,96.46,653.50,22.62,10.00">#247,</ref><ref type="bibr" coords="1,121.92,653.50,22.62,10.00">#248,</ref><ref type="bibr" coords="1,147.39,653.50,22.62,10.00">#249,</ref><ref type="bibr" coords="1,172.85,653.50,14.50,10.00">and</ref> for the Portuguese corpus: Queries #216, #220, #227, #240.</p><p>During the indexing process of our automatic runs, we retained only the following logical sections from the original documents: &lt;TITLE&gt;, &lt;HEADLINE&gt;, &lt;TEXT&gt;, &lt;LEAD1&gt;, &lt;TX&gt;, &lt;LD&gt;, &lt;TI&gt; and &lt;ST&gt;. From the topic descriptions we automatically removed certain phrases such as "Relevant document report …", "Find documents …" or "Trouver des documents qui parlent …".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Stopword Lists and Stemming Procedures</head><p>In order to define general stopword lists, we first created a list of the top 200 most frequent words found in the various languages, from which some words were removed (e.g., <ref type="bibr" coords="2,348.00,108.09,173.02,11.07">Roma, police, minister, president, Chirac)</ref>. From this list of very frequent words, we added articles, pronouns, prepositions, conjunctions or very frequently occurring verb forms (e.g., to be, is, has, etc.). We created a new one for the Portuguese language, adding it to last year's stopword lists <ref type="bibr" coords="2,175.66,141.09,50.87,11.07" target="#b10">(Savoy 2003</ref>) (these lists are available at www.unine.ch/info/clef/). For English we used the list provided by the SMART system (571 words), while for the other European languages, our stopword list contained 463 words for the French language, 747 for Finnish, 420 for Russian and 356 for Portuguese. To this last list, we recently added a few forms to obtain a Portuguese stopword list containing 392 words.  Once high-frequency words were removed, an indexing procedure generally applied a stemming algorithm in an attempt to conflate word variants into the same stem or root. In developing this procedure for various European languages <ref type="bibr" coords="2,134.81,504.09,53.45,11.07" target="#b14">(Sproat 1992)</ref>, we first wanted to remove only inflectional suffixes such as singular and plural word forms, and also feminine and masculine forms, such that they conflate to the same root. Our suggested stemmers also tried to remove various case markings (e.g., accusative or genitive case) used in the Finnish and Russian languages. The Finnish language however raised more morphological difficulties, because this language frequently uses 12 cases and also the stem is often modified when suffixes are added. For example, "matto" (carpet in nominative singular form) becomes "maton" (in genitive singular form, with "-n" as suffix) or "mattoja" (in partitive plural form, with "-a" as suffix). When we simply removed the corresponding suffix, we were faced with three distinct stems, namely "matto", "mato", and "matoj". Of course such irregularities also occur in other languages, usually introduced to make the spoken language flow better, such as "submit" and "submission". In Finnish however, these irregularities are more common, thus rendering the conflation of various word forms into the same stem more problematic. For indexing Finnish documents, some authors therefore suggested using a morphological analyzer (using a dictionary) as well as word form normalization procedures <ref type="bibr" coords="2,71.00,636.09,81.35,11.07" target="#b6">(Hedlund et al. 2004</ref>).</p><formula xml:id="formula_0" coords="2,82.00,434.50,420.03,21.00">(Q#212) 20 (Q#241) 189 (Q#229) Minimum 1 (Q#210) 1 (Q#225) 1 (Q#209) 1 (Q#203) 1 (Q#215)</formula><p>More sophisticated schemes were already proposed for the removal of derivational suffixes (e.g., "-ize", "ably", "-ship" in the English language), as for example the stemmer developed by <ref type="bibr" coords="2,402.04,664.09,57.11,11.07" target="#b7">Lovins (1968)</ref> (based on a list of over 260 suffixes), or that of <ref type="bibr" coords="2,206.96,675.09,55.36,11.07" target="#b8">Porter (1980)</ref> (which looks for about 60 suffixes). For the French language only, we developed a stemming approach to remove some derivational suffixes (e.g., "communicateur" -&gt; "communiquer", "faiblesse" -&gt; "faible"). Our various stemming procedures can be found at www.unine.ch/info/clef/. Currently, it is not clear whether a stemming procedure removing only inflectional suffixes from nouns and adjectives would result in better retrieval effectiveness than would other stemming approaches that also consider verbs or remove both inflectional and derivational suffixes (e.g., the Snowball stemmers available at http://snowball.tartarus.org/). Diacritic characters are usually not present in English collections (with certain exceptions, such as "résumé" or "cliché"). For the Finnish, Portuguese and Russian languages, these characters were replaced by their corresponding non-accentuated letter. For the Russian language, we converted and normalized the Cyrillic Unicode characters into the Latin alphabet (the Perl script is available at www.unine.ch/clef/). Finally, most European languages manifest other morphological characteristics, with compound word constructions being just one example (e.g., handgun, worldwide). In Finnish, we encounter similar constructions as such as "rakkauskirje" ("rakkaus" + "kirje" for love &amp; letter) or "työviikko" ("työ" + "viikko" for work &amp; week). Recently, <ref type="bibr" coords="3,181.28,154.09,126.51,11.07" target="#b1">Braschler &amp; Ripplinger (2004)</ref> showed that decompounding German words would significantly improve retrieval performance. In our experiments, for the Finnish language we used our decompounding algorithm <ref type="bibr" coords="3,155.45,176.09,50.95,11.07" target="#b10">(Savoy 2003</ref>) (see also <ref type="bibr" coords="3,253.82,176.09,49.23,11.07" target="#b3">(Chen 2003)</ref>), where both the compound words and their components were left in documents and queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Indexing and Searching Strategies</head><p>In order to obtain a broader view of the relative merit of various retrieval models, we first adopted a binary indexing scheme in which each document (or request) was represented by a set of keywords, without any weight. To measure the similarity between documents and requests, we computed the inner product (retrieval model denoted "doc=bnn, query=bnn" or "bnn-bnn"). In order to weight the presence of each indexing term in a document surrogate (or in a query), we would account for the term occurrence frequency (denoted tfij for indexing term t j in document D i , and the corresponding retrieval model is denoted: "doc=nnn, query=nnn" or "nnn-nnn") or we might also account for their frequency in the collection (or more precisely the inverse document frequency, denoted by idfj). Moreover, we found that cosine normalization could prove beneficial, and in this case, each indexing weight could vary within the range of 0 to 1 (retrieval model notation: "ntc-ntc"). In Table <ref type="table" coords="3,479.40,330.09,5.00,11.07">3</ref> w ij represents the indexing weight assigned to term t j in document D i , n to indicate the number of documents in the collection and nt i the number of distinct indexing terms included in the representation of D i .</p><p>Other variants might also be created. For example, the tf component could be computed as 0.5 + 0.5 • [tf / max tf in a document] (retrieval model denoted "doc=atn"). We might also consider that a term's presence in a shorter document provides stronger evidence than it does in a longer document, leading to more complex IR models; for example, the IR model denoted by "doc=Lnu" <ref type="bibr" coords="3,330.00,404.09,91.87,11.07" target="#b2">(Buckley et al. 1996)</ref>, "doc=dtu" <ref type="bibr" coords="3,478.00,404.09,47.00,11.07;3,71.00,415.09,35.65,11.07" target="#b13">(Singhal et al. 1999)</ref>.</p><p>In addition to the previous models based on the vector-space approach, we also considered probabilistic models. In this vein, we used the Okapi probabilistic model <ref type="bibr" coords="3,317.97,444.50,91.90,10.00" target="#b9">(Robertson et al. 2000)</ref>. As a second probabilistic approach, we implemented the Prosit (or deviation from randomness) approach <ref type="bibr" coords="3,392.00,455.50,133.03,10.00" target="#b0">(Amati &amp; van Rijsbergen 2002)</ref> which is based on the combination of two information measures as follows:</p><formula xml:id="formula_1" coords="3,127.00,480.65,296.02,47.34">wij = Inf 1 ij • Inf 2 ij = (1 -Prob 1 ij) • -log2[Prob 2 ij] Prob 1 ij = tfnij / (tfnij + 1) with tfnij = tfij • log2[1 + ((C • mean dl) / l i )] Prob 2 ij = [1 / (1+lj)] • [lj / (1+lj)] tfn ij with lj = tcj / n</formula><p>where wij indicates the indexing weight attached to term tj in document Di, l i the number of indexing terms included in the representation of D i , tcj represents the number of occurrences of term tj in the collection and n the number of documents in the corpus. In our experiments, the constants b, k1, avdl, pivot, slope, C and mean dl were fixed according to values listed in Table <ref type="table" coords="3,255.60,569.50,3.75,10.00" target="#tab_2">2</ref> </p><formula xml:id="formula_2" coords="4,86.00,103.09,429.47,209.85">ij = tf ij . ln[(n-df j ) / df j ] Okapi w ij = (k 1 + 1) ⋅ tf ij ( ) K + tf ij ( ) Lnu w ij = 1 + ln(tf ij ) ln(mean tf) + 1 Ê Ë Á ˆ ¯ ˜ (1 -slope) ⋅ pivot + slope ⋅ nt i lnc w ij = ln(tf ij ) + 1 ln( tf i k ) +1 ( ) 2 k =1 t Â ntc w ij = tf i j ⋅ idf j tf i k ⋅idf k ( ) 2 k =1 t Â ltc w ij = ln(tf ij ) + 1 ( )⋅idf j ln(tf ik ) + 1 ( ) ⋅ idf k ( ) 2 k=1 t Â dtu w ij = ln ln(tf ij ) + 1 ( ) + 1 ( ) ⋅idf j (1 -slope) ⋅ pivot + slope ⋅ nt i</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3: Weighting schemes</head><p>To evaluate our approaches, we used the SMART system as a test bed running on an Intel Pentium III/600 (memory: 1 GB, swap: 2 GB, disk: 6 x 35 GB). To measure the retrieval performance, we adopted the noninterpolated mean average precision (computed on the basis of 1,000 retrieved items per request by the TREC-EVAL program). We indexed the English, French, and Portuguese collections using words as indexing units. The evaluation of our two probabilistic models and nine vector-space schemes are listed in Table <ref type="table" coords="4,488.00,383.09,5.00,11.07" target="#tab_3">4</ref> for the French and Portuguese corpus, and in Table <ref type="table" coords="4,250.75,394.09,5.00,11.07" target="#tab_4">5</ref> for the English collection.</p><p>In order to represent Finnish and Russian documents and queries, we considered the n-gram, and word-based indexing schemes. The resulting mean average precision for these various indexing approaches is shown in Table <ref type="table" coords="4,96.56,433.09,5.00,11.07" target="#tab_4">5</ref> (Finnish word-based indexing with decompounding), in Table <ref type="table" coords="4,357.78,433.09,5.00,11.07" target="#tab_5">6</ref> (Finnish based on the 5-gram or the 4gram indexing scheme) and in Table <ref type="table" coords="4,220.15,444.09,5.00,11.07" target="#tab_6">7</ref> (Russian corpus both word-based and 4-gram indexing). In these tables, we depicted in bold the best performance under given conditions (with the same indexing scheme and the same collection). From an analysis of these results, it can be seen that when the number of search terms increases (from T, TD to TDN), so usually does retrieval effectiveness (except for "bnn-bnn" or "nnn-nnn" IR models). When considering the five best retrieval schemes (namely, Prosit, Okapi, "Lnu-ltc", "dtu-dtn" and "atn-ntc"), Tables <ref type="table" coords="4,493.37,700.09,5.00,11.07" target="#tab_3">4</ref> and<ref type="table" coords="4,520.00,700.09,5.00,11.07" target="#tab_4">5</ref> show that the improvement is around 29% when comparing title-only (or T) with TDN queries for the Portuguese collection, or of 22.1% with the English corpus or 16.6% for the French collection. When considering the Finnish language (Table <ref type="table" coords="4,186.25,733.09,5.00,11.07" target="#tab_5">6</ref> and right part of Table <ref type="table" coords="4,291.49,733.09,3.62,11.07" target="#tab_4">5</ref>), we can see that 4-gram indexing scheme usually performs better than both 5-gram indexing (e.g., with the TD queries, 4-gram: mean MAP of the five best IR models is 0.5278 vs. 0.4729 with 5-gram indexing approach, a performance difference of 11.6% in favor of the 4-gram model) or better than the word-based indexing model (mean of 5 best IR models of 0.4692, with a performance difference of 12.5% in favor of the 4-gram indexing approach). There are of course exceptions to this rule (e.g., for TD queries and "ntc-ntc" model, the 5-gram indexing scheme results in slightly better performance than the 4-gram strategy, 0.4472 vs. 0.4466). As illustrated in Table <ref type="table" coords="5,364.52,115.09,3.80,11.07" target="#tab_6">7</ref>, for the Russian language the wordbased indexing scheme provides better retrieval performance than do the 4-gram schemes (based on the five best search models, for TD queries the mean MAP of the five best retrieval is 0.3646 vs. 0.2774 for the 4-gram indexing scheme, a difference of 31.4%). For the Finnish language, we also indexed documents and the queries using words and "words" composed only of consonants. With this indexing scheme, the term "rakkaus" is indexed under both "rakkaus" and "rkks". In this experiment, before removing all vowels, we applied our Finnish stemming stemmer. The mean average precision achieved by this indexing strategy was always lower than the corresponding word-based approach (see second column of Table <ref type="table" coords="5,233.38,599.09,5.00,11.07" target="#tab_5">6</ref> under the label "word &amp; CC"). We must recognize that the Finnish language, with its rich inflectional morphology and its frequent irregularities, resulted in many difficulties for our simple stemming approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean average precision</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean average precision</head><p>It was observed that pseudo-relevance feedback (blind-query expansion) seemed to be a useful technique for enhancing retrieval effectiveness. In this study, we adopted Rocchio's approach <ref type="bibr" coords="5,412.00,649.09,86.04,11.07" target="#b2">(Buckley et al. 1996</ref>) with a = 0.75, b = 0.75 whereby the system was allowed to add m terms extracted from the k best ranked documents from the original query. To evaluate this proposition, we used the Okapi and the Prosit probabilistic models and enlarged the query by the 10 to 40 terms provided by the 3 or 10 best-retrieved articles.</p><p>The results depicted in Table <ref type="table" coords="5,209.10,699.09,5.00,11.07" target="#tab_7">8</ref> (depicting our best results for the Okapi model) indicate that the optimal parameter setting seemed to be collection-dependant. Moreover, performance improvement also seemed to be collection dependant (or language dependant), with the Portuguese corpus showing an increase of 6% (from a mean average precision of 0.4835 to 0.5127), 5.2% for the English collection (from 0.5422 to 0.5704), 3.8% for the Russian collection (from 0.3800 to 0.3945), and 3.5% for the French corpus (from 0.4685 to 0.4851). For the Finnish corpus and 4-gram indexing scheme, the query expansion approach did not improve the mean average precision, while with word-based indexing scheme, the best improvement was of 4.4% (0.4773 vs. 0.4984). Using the Prosit model (see Table <ref type="table" coords="6,211.06,93.09,3.63,11.07" target="#tab_8">9</ref>), similar conclusions can be drawn. In this case however, the blind query expansion improves the mean average precision for all collections. Using the same query expansion technique (Rocchio in this case), various IR models have resulted in varying degrees of evolution when increasing the number of terms to be included in the expanded query. To illustrate this phenomenon, Figure <ref type="figure" coords="6,194.17,654.09,5.00,11.07" target="#fig_0">1</ref> depicts the evolution of the mean average precision of four different IR models (French corpus, and using the 3 best ranked documents). When we increased the number of terms to be included in the expanded query, the "dtu-dtn" model showed a small but constant improvement. With this IR model, each parameter setting produced a retrieval performance not that far from the best one. A similar evolution can be seen from the "Lnu-ltc" model, with a greater improvement however. When compared to the Okapi or Prosit models however, performance levels achieved were lower. For the Prosit model as well as for the Okapi scheme, the mean average precision increased, reached a maximum point and then subsequently fell slowly (with a greater variability for the Prosit model however). When a few terms were added to the original query however, the Prosit model usually performed at lower levels than did the Okapi. When this number of additional terms was increased however, the Prosit model tended to result in better mean average precision than did the Okapi scheme. However, when more than 100 terms are added, the Okapi model produced a better retrieval effectiveness than the Prosit model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean average precision</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Data Fusion</head><p>For the each language, we may assume that different indexing and search models would retrieve different and non-relevant items and that combining different search models should improve retrieval effectiveness. More precisely, when combining different indexing schemes we would expect to improve recall due to the fact that different document representations may retrieve different pertinent items <ref type="bibr" coords="7,405.00,436.09,95.71,11.07" target="#b15">(Vogt &amp; Cottrell 1999)</ref>. On the other hand, when combining different search schemes, we would suppose that these various IR strategies are more likely to rank the same relevant items higher on the list than they would the same non-relevant documents (that can be viewed as outliers). Thus combining them could improve retrieval effectiveness by ranking pertinent documents higher and ranking non-relevant items lower. In this study, we hope to enhance retrieval performance by making use of this second characteristic, while for the Finnish language our assumption would be that word-based and n-gram indexing schemes are distinct and independent sources of evidence regarding the content of documents. For this language only, we expect to improve recall due to the first effect described above.</p><p>In order to combine two or more indexing schemes, we evaluated various fusion operators, and their precise descriptions are listed in Table <ref type="table" coords="7,198.74,552.09,8.37,11.07" target="#tab_1">10</ref>. For example, the Sum RSV operator indicates that the combined document score (or the final retrieval status value) is simply the sum of the retrieval status value (RSVk) of the corresponding document Dk computed by each single indexing scheme <ref type="bibr" coords="7,331.00,574.09,85.97,11.07" target="#b5">(Fox &amp; Shaw 1994)</ref>. We can thus see from Table <ref type="table" coords="7,96.59,585.09,10.02,11.07" target="#tab_1">10</ref> that both the Norm Max and Norm RSV apply a normalization procedure when combining document scores. When combining the retrieval status value (RSVk) for various indexing schemes, we may multiply the document score by a constant ai (usually equal to 1) in order to favor the ith more efficient retrieval scheme.</p><p>In addition to using these data fusion operators, we also considered the round-robin approach, whereby in turn we take one document from all individual lists and remove duplicates, keeping the most highly ranked instance. Finally we suggested merging the retrieved documents according to the Z-score, computed for each result list. Within this scheme, for the ith result list, we needed to compute the average of the RSV k (denoted Mean i ) and the standard deviation (denoted Stdev i ). Based on these values, we would then normalize the retrieval status value for each document D k provided by the ith result list by computing the deviation of RSV k with respect to the mean (Mean i ). In Table 10, Min i (Max i ) denotes the minimal (maximal) RSV value in the ith result list.</p><p>Finally, in Table <ref type="table" coords="9,157.53,71.09,10.08,11.07" target="#tab_2">12</ref> we show the exact specifications of our 12 official monolingual runs. These experiments were based on different data fusion operators (mainly the Z-score and the round-robin schemes). Although we expected that combining the Okapi and the Prosit probabilistic models would provide good retrieval effectiveness, for some languages (e.g., French or Russian), we also considered other IR models (e.g., "dtu-dtn" or "Lnu-ltc"). We also sent some runs with longer queries formulations (TDN) in order to increase the number of relevant documents to be found per language. In the "UniNEfi1" run, we removed all documents appearing in the year 1994 (in order to search all newspaper articles that described events occurring in the year 1995. However, 66 (over 413) relevant items have been published in year 1994).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this fifth CLEF evaluation campaign, we proposed a general stopword list and stemming procedure for the Portuguese language. Currently it is not clear if a stemming procedure, such as the one we suggested whereby only inflectional suffixes were removed from nouns and adjectives, could result in better retrieval effectiveness than a stemming approach that takes both inflectional and derivational suffixes into account. In order to achieve better retrieval results, we used a data fusion approach based on the Z-score, where it was required that document (and query) representation be based on two or three indexing schemes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,104.00,348.09,387.19,11.07"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Mean average precision using blind-query expansion within different retrieval models</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,204.00,465.09,187.04,11.07"><head>Table 1 :</head><label>1</label><figDesc>CLEF 2004 test-collection statistics</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,105.00,569.50,378.01,165.66"><head>Table 2 :</head><label>2</label><figDesc>. Parameter setting for the various test-collections</figDesc><table coords="3,286.00,588.50,166.03,10.00"><row><cell>Okapi</cell><cell>Prosit</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,77.00,496.09,441.03,176.07"><head>Table 4 :</head><label>4</label><figDesc>Mean average precision of various single searching strategies (French &amp; Portuguese language)</figDesc><table coords="4,77.00,496.09,441.03,156.07"><row><cell>Query Model \ # of queries</cell><cell cols="2">French T 49 queries 49 queries French TD</cell><cell>French TDN 49 queries</cell><cell cols="3">Portuguese Portuguese Portuguese T TD TDN 46 queries 46 queries 46 queries</cell></row><row><cell>Prosit doc=Okapi, query=npn</cell><cell>0.4111 0.4263</cell><cell>0.4568 0.4685</cell><cell>0.4857 0.4852</cell><cell>0.3824 0.3997</cell><cell>0.4695 0.4835</cell><cell>0.4995 0.4968</cell></row><row><cell>doc=Lnu, query=ltc doc=dtu, query=dtn doc=atn, query=ntc doc=ltn, query=ntc doc=ntc, query=ntc doc=ltc, query=ltc doc=lnc, query=ltc doc=bnn, query=bnn doc=nnn, query=nnn</cell><cell>0.3952 0.3873 0.3768 0.3718 0.3056 0.2822 0.3023 0.2262 0.2073</cell><cell>0.4349 0.4143 0.4210 0.4035 0.3309 0.3184 0.3463 0.2017 0.2104</cell><cell>0.4666 0.4504 0.4397 0.4238 0.3468 0.3433 0.3811 0.1460 0.2008</cell><cell>0.3633 0.3620 0.3559 0.3737 0.2981 0.2820 0.2911 0.1793 0.1714</cell><cell>0.4579 0.4600 0.4454 0.4319 0.3708 0.3571 0.3658 0.1834 0.1681</cell><cell>0.4765 0.4735 0.4579 0.4401 0.3751 0.3831 0.3977 0.1332 0.1578</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,77.00,178.09,445.03,351.07"><head>Table 5 :</head><label>5</label><figDesc>Mean average precision of various single searching strategies (English &amp; Finnish language)</figDesc><table coords="5,77.00,178.09,445.03,351.07"><row><cell>Query Model \ # of queries</cell><cell cols="2">English T 42 queries 42 queries English TD</cell><cell>English TDN 42 queries</cell><cell cols="3">Finnish (wd) Finnish (wd) Finnish (wd) T TD TDN 43 queries 45 queries 45 queries</cell></row><row><cell>Prosit doc=Okapi, query=npn</cell><cell>0.4638 0.4763</cell><cell>0.5313 0.5422</cell><cell>0.5652 0.5707</cell><cell>0.3237 0.4190</cell><cell>0.4620 0.4773</cell><cell>0.4697 0.4820</cell></row><row><cell>doc=Lnu, query=ltc doc=dtu, query=dtn doc=atn, query=ntc doc=ltn, query=ntc doc=ntc, query=ntc doc=ltc, query=ltc doc=lnc, query=ltc doc=bnn, query=bnn doc=nnn, query=nnn</cell><cell>0.4435 0.4444 0.4203 0.3876 0.3109 0.3072 0.3342 0.3177 0.1937</cell><cell>0.4979 0.5319 0.4764 0.4602 0.3706 0.3915 0.4108 0.3005 0.1846</cell><cell>0.5470 0.5372 0.5245 0.5072 0.4006 0.4028 0.4326 0.2090 0.1570</cell><cell>0.4187 0.4152 0.4019 0.4054 0.3485 0.3511 0.3451 0.2226 0.1817</cell><cell>0.4643 0.4746 0.4629 0.4580 0.3862 0.3964 0.4176 0.1859 0.1318</cell><cell>0.4961 0.4989 0.4819 0.4801 0.3960 0.4172 0.4354 0.1394 0.1200</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Mean average precision</cell><cell></cell><cell></cell></row><row><cell>Finnish Query Model \ # of queries</cell><cell cols="2">word &amp; CC TD 45 queries 45 queries 5-gram TD</cell><cell>5-gram TDN 45 queries</cell><cell>4-gram T 45 queries</cell><cell cols="2">4-gram TD 45 queries 45 queries 4-gram TDN</cell></row><row><cell>Prosit doc=Okapi, query=npn</cell><cell>0.4445 0.4564</cell><cell>0.4707 0.4805</cell><cell>0.4666 0.4855</cell><cell>0.4953 0.4987</cell><cell>0.5357 0.5386</cell><cell>0.5166 0.5151</cell></row><row><cell>doc=Lnu, query=ltc doc=dtu, query=dtn doc=atn, query=ntc doc=ltn, query=ntc doc=ntc, query=ntc doc=ltc, query=ltc doc=lnc, query=ltc doc=bnn, query=bnn doc=nnn, query=nnn</cell><cell>0.4466 0.4565 0.4187 0.4466 0.3747 0.3897 0.4005 0.2373 0.1694</cell><cell>0.4767 0.4629 0.4735 0.4824 0.4472 0.4290 0.4177 0.2616 0.2038</cell><cell>0.4805 0.4615 0.5104 0.4907 0.4709 0.4398 0.4592 0.1631 0.1668</cell><cell>0.4731 0.4806 0.4900 0.4553 0.4000 0.3766 0.3989 0.3146 0.2028</cell><cell>0.5022 0.5200 0.5427 0.4880 0.4466 0.4284 0.4345 0.2387 0.1781</cell><cell>0.5138 0.5143 0.5465 0.4688 0.4472 0.4693 0.4893 0.1185 0.1354</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="5,114.00,538.09,366.98,11.07"><head>Table 6 :</head><label>6</label><figDesc>Mean average precision of various single searching strategies (Finnish collection)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="6,71.00,134.09,448.00,313.41"><head>Table 7 :</head><label>7</label><figDesc>Mean average precision of various single searching strategies (Russian corpus)</figDesc><table coords="6,71.00,134.09,448.00,313.41"><row><cell cols="2">Russian Query Model \ # of queries</cell><cell cols="2">word T 34 queries 34 queries word TD</cell><cell cols="2">word TDN 34 queries</cell><cell cols="2">4-gram T 34 queries</cell><cell>4-gram TD 34 queries 34 queries 4-gram TDN</cell></row><row><cell cols="2">Prosit doc=Okapi, query=npn</cell><cell>0.3130 0.3566</cell><cell>0.3448 0.3800</cell><cell cols="2">0.3598 0.3944</cell><cell>0.2268 0.2367</cell><cell>0.2879 0.2890</cell><cell>0.2734 0.2800</cell></row><row><cell cols="2">doc=Lnu, query=ltc doc=dtu, query=dtn doc=atn, query=ntc doc=ltn, query=ntc doc=ntc, query=ntc doc=ltc, query=ltc doc=lnc, query=ltc doc=bnn, query=bnn doc=nnn, query=nnn</cell><cell>0.3409 0.3802 0.3264 0.3272 0.2541 0.2341 0.1850 0.1680 0.1130</cell><cell>0.3794 0.3768 0.3422 0.3579 0.2716 0.2362 0.1598 0.1512 0.1023</cell><cell cols="2">0.3900 0.3894 0.3650 0.3241 0.2581 0.2451 0.2014 0.1055 0.0967</cell><cell>0.2425 0.1851 0.2325 0.2014 0.1690 0.1134 0.1032 0.1437 0.0537</cell><cell>0.2852 0.2705 0.2543 0.2137 0.1916 0.1430 0.1303 0.0373 0.0408</cell><cell>0.3109 0.2923 0.2173 0.1697 0.1862 0.1290 0.1167 0.0061 0.0229</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Mean average precision</cell></row><row><cell>Query TD Model</cell><cell>English word 42 queries</cell><cell>French word 49 queries</cell><cell cols="2">Finnish 4-gram 45 queries</cell><cell cols="2">Finnish word 45 queries</cell><cell>Russian word 34 queries</cell><cell>Portuguese word 46 queries</cell></row><row><cell>Okapi</cell><cell>0.5422</cell><cell>0.4685</cell><cell>0.5386</cell><cell></cell><cell cols="2">0.4773</cell><cell>0.3800</cell><cell>0.4835</cell></row><row><cell>k doc.</cell><cell>0.5582</cell><cell>0 . 4 8 5 1</cell><cell cols="2">0.5308</cell><cell></cell><cell>0.4687</cell><cell>0.3925</cell><cell>0.5005</cell></row><row><cell>/ m terms</cell><cell>0.5581</cell><cell>0.4748</cell><cell cols="2">0.5296</cell><cell></cell><cell>0.4628</cell><cell>0.3678</cell><cell>0 . 5 1 2 7</cell></row><row><cell></cell><cell>0 . 5 7 0 4</cell><cell>0.4738</cell><cell cols="2">0.5277</cell><cell></cell><cell>0.4799</cell><cell>0.3896</cell><cell>0.5098</cell></row><row><cell></cell><cell>0.5587</cell><cell>0.4628</cell><cell cols="2">0.5213</cell><cell></cell><cell>0 . 4 9 8 4</cell><cell>0 . 3 9 4 5</cell><cell>0.5005</cell></row><row><cell></cell><cell>0.5596</cell><cell>0.4671</cell><cell cols="2">0.5291</cell><cell></cell><cell>0.4758</cell><cell>0.3796</cell><cell>0.5077</cell></row><row><cell></cell><cell>0.5596</cell><cell>0.4547</cell><cell cols="2">0.5297</cell><cell></cell><cell>0.4461</cell><cell>0.3913</cell><cell>0.4806</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="6,71.00,457.09,448.00,148.41"><head>Table 8 :</head><label>8</label><figDesc>Mean average precision using blind-query expansion (Okapi model)</figDesc><table coords="6,274.00,476.09,93.05,11.07"><row><cell>Mean average precision</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="6,141.00,615.09,313.02,11.07"><head>Table 9 :</head><label>9</label><figDesc>Mean average precision using blind-query expansion (Prosit model)</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The author would like to also thank the CLEF-2004 task organizers for their efforts in developing various European language test-collections. The author would also like to thank <rs type="person">C. Buckley</rs> from <rs type="affiliation">SabIR</rs> for giving us the opportunity to use the SMART system. This research was supported by the <rs type="funder">Swiss National Science Foundation</rs> under Grant #<rs type="grantNumber">21-66 742.01</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_CG3AMDe">
					<idno type="grant-number">21-66 742.01</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="table" coords="8,111.24,152.09,10.06,11.07">11</ref> <p>depicts the evaluation of various data fusion operators, comparing them to the single approach using the Okapi and the Prosit probabilistic models. From this data, we could see that combining two IR models might sometimes improve retrieval effectiveness (for the French or Russian corpora however, no improvement can be found). When combining two retrieval models, the Z-score scheme tended to produce the best, or at least, a good performance. In Table <ref type="table" coords="8,262.71,197.50,8.36,10.00">11</ref>, under the heading "Z-scoreW", we attached a weight of 2 to the Prosit model, and 1.5 to the Okapi model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean average precision</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="9,71.00,377.50,454.00,10.00;9,92.00,388.50,238.01,10.00" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,244.48,377.50,280.52,10.00;9,92.00,388.50,114.50,10.00">Probabilistic models of information retrieval based on measuring the divergence from randomness</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,216.30,388.50,42.95,10.00">ACM-TOIS</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="357" to="389" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,71.00,400.09,454.00,11.41;9,92.00,412.50,156.04,10.00" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,245.01,401.50,279.99,10.00;9,92.00,412.50,33.89,10.00">How effective is stemming and decompounding for German text retrieval</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Braschler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ripplinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,134.58,412.50,41.19,10.00">IR Journal</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="291" to="316" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,71.00,424.09,454.01,11.07;9,92.00,435.09,322.01,11.07" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,329.52,424.09,171.83,11.07">New retrieval approaches using SMART</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<idno>#500-236</idno>
	</analytic>
	<monogr>
		<title level="m" coord="9,92.00,435.09,93.76,11.07">Proceedings of TREC-4</title>
		<meeting>TREC-4<address><addrLine>Gaithersburg</addrLine></address></meeting>
		<imprint>
			<publisher>NIST Publication</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="25" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,71.00,448.09,287.43,11.07" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="9,144.93,448.09,188.12,11.07">Cross-language retrieval experiments at CLEF</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2003. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,364.00,448.09,161.02,11.07;9,92.00,459.09,433.06,11.07;9,92.00,470.09,88.00,11.07" xml:id="b4">
	<monogr>
		<title level="m" coord="9,180.00,459.09,216.30,11.07">Advances in Cross-Language Information Retrieval</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2785">2785</date>
			<biblScope unit="page" from="28" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,71.00,483.09,454.02,11.07;9,92.00,494.09,172.01,11.07" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,211.00,483.09,138.67,11.07">Combination of multiple searches</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
		<idno>#500-215</idno>
	</analytic>
	<monogr>
		<title level="m" coord="9,373.00,483.09,84.15,11.07">Proceedings TREC-2</title>
		<meeting>TREC-2<address><addrLine>Gaithersburg</addrLine></address></meeting>
		<imprint>
			<publisher>NIST Publication</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="243" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,71.00,508.50,454.05,10.00;9,92.00,519.50,433.01,10.00;9,92.00,530.50,18.00,10.00" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,459.00,508.50,66.05,10.00;9,92.00,519.50,330.52,10.00">Dictionary-based cross-language information retrieval: Learning experiences from CLEF 2000-2002</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hedlund</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Airio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Keskustalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Lehtokangas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pirkola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Järvelin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,432.07,519.50,41.50,10.00">IR Journal</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="99" to="119" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,71.00,542.09,454.01,11.07;9,92.00,553.09,104.01,11.07" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,163.90,542.09,162.78,11.07">Development of a stemming algorithm</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">B</forename><surname>Lovins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,334.95,542.09,190.07,11.07;9,92.00,553.09,43.57,11.07">Mechanical Translation and Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="31" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,71.00,566.09,322.01,11.07" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,158.90,566.09,133.09,11.07">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,298.76,566.09,34.58,11.07">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,71.00,579.09,454.05,11.07;9,92.00,590.09,224.01,11.07" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,305.51,579.09,214.53,11.07">Experimentation as a way of life: Okapi at TREC</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beaulieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,92.00,590.09,158.51,11.07">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="95" to="108" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,71.00,603.09,454.03,11.41;9,92.00,615.50,320.02,10.00" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,150.00,604.50,375.03,10.00;9,92.00,615.50,85.40,10.00">Report on CLEF-2003 monolingual tracks : Fusion of probabilistic models for effective monolingual retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,196.90,615.50,97.92,10.00">Proceedings CLEF-2003</title>
		<meeting>CLEF-2003<address><addrLine>Trondheim</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="179" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,71.00,628.50,454.01,10.00;9,92.00,639.50,101.04,10.00" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,155.33,628.50,347.20,10.00">Combining multiple strategies for effective monolingual and cross-lingual retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,515.00,628.50,10.01,10.00;9,92.00,639.50,29.16,10.00">IR Journal</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="121" to="148" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,71.00,652.50,454.01,10.00;9,92.00,663.50,90.03,10.00" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,150.66,652.50,241.17,10.00">Report on CLIR task for the NTCIR-4 evaluation campaign</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,416.00,652.50,87.07,10.00">Proceedings NTCIR-4</title>
		<meeting>NTCIR-4<address><addrLine>Tokyo</addrLine></address></meeting>
		<imprint>
			<publisher>NII</publisher>
			<date type="published" when="2004">2004b</date>
			<biblScope unit="page" from="178" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,71.00,675.09,454.18,11.07;9,92.00,686.09,244.01,11.07" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,354.17,675.09,69.59,11.07">AT&amp;T at TREC-7</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hindle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<idno>#500-242</idno>
	</analytic>
	<monogr>
		<title level="m" coord="9,448.00,675.09,77.18,11.07;9,92.00,686.09,3.78,11.07">Proceedings TREC-7</title>
		<meeting>TREC-7<address><addrLine>Gaithersburg</addrLine></address></meeting>
		<imprint>
			<publisher>NIST Publication</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="239" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,71.00,700.50,345.04,10.00" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="9,149.20,700.50,121.56,10.00">Morphology and Computation</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sproat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,71.00,713.50,438.01,10.00" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="9,224.48,713.50,166.83,10.00">Fusion via a linear combination of scores</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Vogt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,400.99,713.50,42.31,10.00">IR Journal</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="151" to="173" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
