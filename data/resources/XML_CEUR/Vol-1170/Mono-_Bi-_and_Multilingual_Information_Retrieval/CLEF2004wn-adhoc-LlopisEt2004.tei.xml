<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,147.24,146.03,308.50,18.08">IR-n r2 : Using normalized passages</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,137.51,181.09,70.09,10.46"><forename type="first">Fernando</forename><surname>Llopis</surname></persName>
							<email>llopis@dlsi.ua.es</email>
							<affiliation key="aff0">
								<orgName type="department">Grupo de investigación en Procesamiento del Lenguaje y Sistemas de Información</orgName>
								<orgName type="institution">University of Alicante</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,230.30,181.09,60.48,10.46"><forename type="first">Rafael</forename><surname>Muñoz</surname></persName>
							<email>rafael@dlsi.ua.es</email>
							<affiliation key="aff0">
								<orgName type="department">Grupo de investigación en Procesamiento del Lenguaje y Sistemas de Información</orgName>
								<orgName type="institution">University of Alicante</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,313.46,181.09,68.53,10.46"><forename type="first">Rafael</forename><forename type="middle">M</forename><surname>Terol</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Grupo de investigación en Procesamiento del Lenguaje y Sistemas de Información</orgName>
								<orgName type="institution">University of Alicante</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,404.68,181.09,60.83,10.46"><forename type="first">Elisa</forename><surname>Noguera</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Grupo de investigación en Procesamiento del Lenguaje y Sistemas de Información</orgName>
								<orgName type="institution">University of Alicante</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,147.24,146.03,308.50,18.08">IR-n r2 : Using normalized passages</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">79065772ACB6050856E03466BD2304BE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the fourth participation of IR-n system (Alicante University) at CLEF conferences. At present conference, we have modified the similarity measure and the query expansion model. Concerning the similarity measure, we use the normalization based on the number of words for each one of the passages. Finally, we test two different approaches for query expansion: the first one is based on documents and the second one is based on passages.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the line showed at previous conferences, IR-n system will participate in several tasks of CLEF'2004. We exactly will participate in monolingual, bilingual and multilingual tasks. IR-n system has considerably changed. On the one hand, the IR-n system was re-programmed in order to improve the answer speed. Moreover, the new version of IR-n system can use different similarity measures by means of a parameter. On the other hand, several changes were made in order to improve the similarity measures and the query expansion.</p><p>This paper is organized as follows: next section describes IR-n system and its new changes. Following, we describe the task developed at CLEF 2004 by our system. And finally, we present the achieved results and the conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">IR-n system</head><p>Information Retrieval (IR) systems have to find the relevant documents to an user query from a document collection. We can find different kinds of IR systems at the literature. On the one hand, if the document collection and the user query are written in the same language then the IR system can be defined like a monolingual IR system. On the other hand, if the document collection and the user query are written in different languages then the IR system can be defined like a bilingual (two different languages) or multilingual (more than two languages) IR system. Obviously, the document collection for multilingual systems is written in two different languages at least. IR-n system is a monolingual, bilingual and multilingual IR system based on passages.</p><p>Passage Retrieval (PR) systems are information retrieval systems that determine the similarity of a document with regard to a user query according to the similarity of fragments of the document (passages) with regard to the same query.</p><p>There are a lot of proposals <ref type="bibr" coords="1,228.13,655.39,10.52,10.46" target="#b0">[1,</ref><ref type="bibr" coords="1,241.81,655.39,7.75,10.46" target="#b6">7]</ref> in order to define the best way of obtaining the passages for achieving the better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">IR-n system r1 (2000-2003)</head><p>IR-n system was originally developed using the C++ language programming and running in linux without excessive requirements. IR-n system is a PR system that uses the sentences as atoms with the aim to define the passages. Thus each passage is composed by a specific number of sentences. This number depends in a great measure of the collection used. For this reason, the system requires a training phase to improve its results. IR-n system uses overlapping passages in order to avoid that some documents can be considered not relevant if it appears words of the question in adjacent passages.</p><p>From the beginning, IR-n system used the traditional cosine measure <ref type="bibr" coords="2,415.15,158.18,14.61,10.46" target="#b12">[14]</ref>. However, further experiments were performed using other similarity measures which results were better than the previous ones. The similarity measures used by IR-n system differ from traditional IR systems. For example, IR-n does not use normalization factors related to the passage or document size. This is due to the fact that passage size is the same for all documents. So, IR-n system calculates the similarity between a passage P and the user query q in the following way:</p><formula xml:id="formula_0" coords="2,233.18,241.86,279.82,21.77">sim(Q, P ) = t∈Q∧P (w Q,t • w P,t )<label>(1)</label></formula><p>where:</p><formula xml:id="formula_1" coords="2,230.89,289.29,282.10,24.94">w Q,t = f req q,t • log e ( N -f req t f req t )<label>(2)</label></formula><p>w P,t = 1 + log e (1 + log e (f req p,t + 1))</p><p>where f req Y,t is the number of appearances or the frequency of term t in the passage or in the question Y . N is the total number of documents in the collection, and f req t is the number of different documents that contain term t.</p><p>Once the system caculates this score for each one of the passages, it is necessary to determine the similarity of the document that contains these passages. All PR systems calculate the similarity measure of the document according to the similarity measure of their passages using the sum of similarity measures for each passage or using the best passage similarity measures for each one of the documents. The experiments performed in <ref type="bibr" coords="2,308.44,426.37,10.52,10.46" target="#b5">[6]</ref> have been re-run by IR-n system, obtaining better results when the use of the best passage similarity measures was performed as the similarity measure of the document.</p><p>Our approach is based on the fact that if a passage is relevant then the document is also relevant. In fact, if a PR system uses the sum of every passage similarity measure then the system has the same behaviour as a document-based IR system adding proximity concepts.</p><p>Moreover, the use of the best passage similarity measure offers the possibility to retrieve the best passage, thus further improving the search process. IR-n system calculates the similarity measure of the document based on the best passage similarity measure in the following way:</p><formula xml:id="formula_3" coords="2,234.21,557.88,278.79,18.58">sim(Q, D) = max ∀ i:P i ∈D sim(Q, P i )<label>(4)</label></formula><p>According to most of IR systems, IR-n system uses also techniques of query expansion. Originally, first release of IR-n system <ref type="bibr" coords="2,235.40,593.25,10.52,10.46" target="#b8">[9]</ref> incorporated synonyms to the original query obtaining worse scores that the model without query expansion. After that, we incorporated the model proposed in <ref type="bibr" coords="2,101.41,617.16,9.96,10.46" target="#b2">[3]</ref>, but the terms that were added to the original question were the most frequent terms of the most relevant passages instead of the most frequent terms of the most relevant documents. The use of these techniques permitted us to improve our results practically in all the performed tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">IR-n system r2 (2004)</head><p>A set of changes have been developed in our system in order to improve it. These changes are the following:</p><p>1. Firstly, we have modified the similarity measure in order to take in account the size of the passages in addition to the number of sentences used in the first release (IR-n r1). For each word, IR-n r1 stored the document and the sentences in which it was found, but did not store the size of each one of the sentences. In this way, it was not possible to compare the similarities between passages using the size of passages. This fact has supposed an important change in the index task and in the search process. We did some experiments with pivoted cosine and okapi measures. We obtained better results with okapi measure.</p><p>2. Moreover, the system was updated in order to consider different similarity measures such as Okapi system. In this way, we can test the best setup for each document collection.</p><p>3. This new release applies techniques of query expansion based on documents. The first release of IR-n system used the most frequent terms in the passages to add them to the original question. The new IR-n release presents a new approach based on adding the most frequent terms in the documents instead of passages.</p><p>4. One of the most important factor for an information retrieval system is the speed. The first release of IR-n system had a low answer time, nevertheless the system has a delayed time in writing the most relevant passages. This fact caused that if the system used query expansion then the answer time will increase.</p><p>In order to obtain these objectives, we have affronted the decision to develop IR-n system r2 practically from the beginning. We decided to use a object oriented approach using C++ as language of implementation. Moreover in parallel way, a IR-n r3 has been developed using the ".net" technology <ref type="bibr" coords="3,169.26,347.47,9.96,10.46" target="#b4">[5]</ref>. Nowadays this release is found in an early phase of development, but it has just achieved better results for XML documents.</p><p>Also, this year we developed the web searcher of University of Alicante using IR-n system. You can access it from the web of University of Alicante (www.ua.es) or directly (www.tabarca.com).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">IR-n r2 at Clef-2004</head><p>This year our system will participate in the following tasks:</p><p>• monolingual tasks:</p><formula xml:id="formula_4" coords="3,104.94,477.95,117.13,145.96">-French -Portuguese -Finnish -Russian • bilingual tasks: -Spanish-Russian -English-Russian -Spanish-Portuguese -English-Portuguese</formula><p>• multilingual tasks:</p><p>-English-French-Finnish-Russian</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Monolingual Tasks</head><p>We used the main resources available in the web address http://www.unine.ch/info/clef/. We take from this website stemmers and stop-word list for each language. Moreover, we used the program to convert Cyrillic characters into ASCII characters in order to process Russian documents. Nevertheless at the moment to perform the Portuguese task, it was not available a Portuguese stemmer. For this reason, we decide to develop one. We changed the Spanish terminations using the adequate Portuguese ones.</p><p>Finnish language presents an additional feature, compound noun. A compound noun usually is composed by the combination of two or more free elements, which are morphemes that can stand on their own and that have their own meaning but together form another word with a modified meaning. We develop an algorithm for splitting compound noun into several words. This fact permitted us to improve the results in the training phase. According to <ref type="bibr" coords="4,421.86,284.39,9.96,10.46" target="#b7">[8]</ref>, the split process consists on splitting words over 6 letters into known words. Obviously, we can split a word in different ways. For this reason, we use a frequency list extracted from the same corpus. We choose the known words combination that provide the highest frequency with a minimum number of words using the following formula:</p><formula xml:id="formula_5" coords="4,240.41,354.55,272.59,23.94">argmin S ( p i S count(p i ))) 1/n<label>(5)</label></formula><p>The similarity measure used for all languages in monolingual task was okapi measure obtaining the best scores. We developed several test with and without normalization using the passage size. Different scores were achieved according to the language as shows Table <ref type="table" coords="4,407.39,408.19,3.87,10.46" target="#tab_0">1</ref>.</p><p>Similar scores were achieved for Finnish and Russian languages with the possibility of using normalization. However, for English language the system achieved best scores using a normalized measures and for French language the best scores were achieved without normalization. This is due to the fact that we have not chosen the same parameters for okapi system or well that in this case is preferably to use other similarity measure.</p><p>Different tests were performed in order to add the best approach to query expansion. Moreover, for each test we checked the adequate number of passages and documents should be considered using 5 or 10 words and 5 and 10 documents/passages. The results obtained in the experimentation phase were similar to them obtained in the final tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bilingual Task</head><p>The participation of the system IR-n r2 in the bilingual task this year has been focused on the following language pairs:</p><formula xml:id="formula_6" coords="4,104.94,595.93,113.06,70.24">• English-Russian • Spanish-Russian • English-Portuguese and • Spanish-Portuguese.</formula><p>According the strategy used last year by IR-n r1, the bilingual task has been performed merging several translation built by an on-line translator. This strategy is based on the idea that the words that appears in different translations have more relevancy that those that only appear in one translation. Two translators were used for all languages: Freetanslation<ref type="foot" coords="5,365.21,360.63,3.97,7.32" target="#foot_0">1</ref> and Babel Fish<ref type="foot" coords="5,440.25,360.63,3.97,7.32" target="#foot_1">2</ref> . An additional on-line translator was used for Russian language. This translator was IMTranslator<ref type="foot" coords="5,456.79,372.59,3.97,7.32" target="#foot_2">3</ref> . Freetranslator and Babel Fish have not a direct translation for Spanish to Russian for this reason we used English language as intermediate language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multilingual task</head><p>We use the formula described in <ref type="bibr" coords="5,233.76,443.85,10.52,10.46" target="#b1">[2]</ref> ir order to merge the different lists of relevant documents for each language.</p><formula xml:id="formula_7" coords="5,209.06,479.71,303.94,12.33">rsv j = (rsv j -rsv min )/(rsv max -rsv min )<label>(6)</label></formula><p>We can not test the merging procedure in the training phase due to we conclude its implementation before to send the test results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Monolingual tasks</head><p>The results achieved in the monolingue task are at least peculiar. In general, all results excluding Russian results are down of the average. We considered that the results are acceptable due to our test only uses the title and the description. However, according to the training phase the Russian scores are impressive very over the average. We do not know all about the Russian language, for this reason we do not know why the results using the same release of IR are also superior to the average.</p><p>Table <ref type="table" coords="5,133.11,656.48,4.98,10.46" target="#tab_1">2</ref> shows the results for each language using the model without expansion (nexp), the model with expansion based on documents (dexp) and the model with expansion based on passages (pexp). The results of the best model in each case are compared with the CLEF average. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Bilingual results</head><p>Obviously, the results achieved in bilingual tests were affected by the results of monolingual track. In this way, the English-Russian and the Spanish-Russian scores are over the average whereas the results on the English-Portuguese and Spanish-Portuguese are worse than the average. Table <ref type="table" coords="6,508.02,468.85,4.98,10.46" target="#tab_2">3</ref> shows the scores achieved for each pair of languages with and without query expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Multilingual results</head><p>Table <ref type="table" coords="6,117.27,527.08,4.98,10.46" target="#tab_3">4</ref> shows the scores achieved in multilingual task without using the query expansion (nexp). Moreover, Table <ref type="table" coords="6,164.29,539.03,4.98,10.46" target="#tab_3">4</ref> presents the scores achieved using two different types of query expansion: the first one uses the model based on passages (pexp) and the second one uses the model based on documents (dexp). Best scores achieved for each pair of language was compared against the CLEF average as shows the Dif. column.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and future works</head><p>This year our sensations are some contradictory. We did not have enough time to develop a new architecture system and to training it. We needed two weeks more to be able of tuning our system in order to increase the scores. First of all, it is a surprise for us the excellent scores achieved in the bilingual task using the Russian as target language. We did not have any previous experience in this language and the system was the same for every task. For this reason, we can explain why the scores are better in Russian language than in other language.</p><p>An additional aspect to be considered it is the use of normalization. We could not demonstrate that the normalization improved the scores in all cases. Due to in the performed experiments this fact has not been able to contrast. We want to run additional experiments in order to study this fact.</p><p>We are presented a comparison between two different query expansion models. Both models achieve similar scores but the model based on passages instead of documents is faster than the other one. Moreover, we want to check the efficiency of this model using larger documents than the CLEF collection.</p><p>Concerning to bilingual task and according our experience of last year, we follow achieving the best scores using a merging of different translator than using only all the translations. Another conclusions is that English is the best language origin to use (bilingual tests on Russian) at least if the both languages are very different. Quite the opposite occurs if both language have the same root: Romanic, Slavian, etc. (bilingual tests Spanish -Portuguese).</p><p>Concerning the multilingual task, the achieved results have been worse than the last year although they are slightly on the average of the CLEF systems. We already have known that the used model to merge document list is very dependent of the number of questions that have answer in each language. For the next edition we will hope to count on a new merging model which we have not been able to finish in this edition. Due to the results obtained int the first experiments we think this model will improve our results.</p><p>Finally, we want remark that the spent time on developing the IR-n r2 system has been allow us to do updates easily. In addition we want to emphasize the process speed in order to show the relevant passages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowlegments</head><p>This work has been partially supported by the Spanish Government (CICYT) with grant TIC2003-07158-C04-01.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,180.57,117.28,241.84,60.62"><head>Table 1 :</head><label>1</label><figDesc>AvgP without query expansion</figDesc><table coords="4,180.57,128.73,241.84,49.16"><row><cell></cell><cell cols="2">Passage size using number of sentences</cell></row><row><cell>Similarity measure</cell><cell>Normalize okapi</cell><cell>no normalized okapi</cell></row><row><cell>Finnish</cell><cell>0.4968</cell><cell>0.5011</cell></row><row><cell>Russian</cell><cell>0.4179</cell><cell>0.4180</cell></row><row><cell>French</cell><cell>0.4263</cell><cell>0.4939</cell></row><row><cell>English</cell><cell>0.5233</cell><cell>0.4827</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,180.51,117.28,241.99,221.01"><head>Table 2 :</head><label>2</label><figDesc>CLEF 2004 official results: Monolingual tasks.</figDesc><table coords="5,190.80,128.98,221.39,209.31"><row><cell>Language</cell><cell>Run</cell><cell>AvgP</cell><cell>Dif.</cell></row><row><cell></cell><cell cols="2">CLEF Average 0.3700</cell><cell></cell></row><row><cell>Russian</cell><cell>nexp</cell><cell cols="2">0.4809 +29.97%</cell></row><row><cell></cell><cell>pexp</cell><cell>0.4733</cell><cell></cell></row><row><cell></cell><cell>dexp</cell><cell>0.4796</cell><cell></cell></row><row><cell></cell><cell cols="2">CLEF Average 0.4370</cell><cell></cell></row><row><cell>French</cell><cell>nexp</cell><cell>0.4086</cell><cell></cell></row><row><cell></cell><cell>pexp</cell><cell>0.4251</cell><cell>-2.72%</cell></row><row><cell></cell><cell>dexp</cell><cell>0.4217</cell><cell></cell></row><row><cell></cell><cell cols="2">CLEF Average 0.5096</cell><cell></cell></row><row><cell>Finish</cell><cell>nexp</cell><cell>0.4533</cell><cell></cell></row><row><cell></cell><cell>pexp</cell><cell>0.4908</cell><cell></cell></row><row><cell></cell><cell>dexp</cell><cell>0.4914</cell><cell>-3.57%</cell></row><row><cell></cell><cell cols="2">CLEF Average 0.4024</cell><cell></cell></row><row><cell>Portuguese</cell><cell>nexp</cell><cell>0.3532</cell><cell></cell></row><row><cell></cell><cell>pexp</cell><cell>0.3750</cell><cell></cell></row><row><cell></cell><cell>dexp</cell><cell>0.3909</cell><cell>-2.85%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,132.27,117.28,338.47,193.12"><head>Table 3 :</head><label>3</label><figDesc>CLEF 2004 official results: Monolingual tasks.</figDesc><table coords="6,132.27,128.98,338.47,181.42"><row><cell>Language</cell><cell>Run</cell><cell>AvgP</cell><cell>Dif.</cell></row><row><cell></cell><cell>CLEF Average</cell><cell>0.1741</cell><cell></cell></row><row><cell>Spanish-Rusian</cell><cell>nexp</cell><cell>0.3044</cell><cell></cell></row><row><cell></cell><cell>pexp</cell><cell cols="2">0.3087 +77.31%</cell></row><row><cell>English-Rusian</cell><cell>nexp</cell><cell>0.3296</cell><cell></cell></row><row><cell></cell><cell>exp</cell><cell cols="2">0.3357 +92.82%</cell></row><row><cell></cell><cell>CLEF Average</cell><cell>0.3316</cell><cell></cell></row><row><cell></cell><cell>Free-translator</cell><cell></cell><cell></cell></row><row><cell>English-Portuguese</cell><cell>nexp</cell><cell cols="2">0.2379 -28.25%</cell></row><row><cell></cell><cell>pexp</cell><cell>0.2173</cell><cell></cell></row><row><cell>Spanish-Portuguese</cell><cell>nexp</cell><cell>0.2977</cell><cell></cell></row><row><cell></cell><cell>exp</cell><cell>0.3243</cell><cell>-2.2%</cell></row><row><cell></cell><cell>Free-translator-Google-BabelFish</cell><cell></cell><cell></cell></row><row><cell>English-Portuguese</cell><cell>nexp</cell><cell>0.2975</cell><cell></cell></row><row><cell></cell><cell>pexp</cell><cell>0.3123</cell><cell>-5.83%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,180.79,332.76,241.42,70.38"><head>Table 4 :</head><label>4</label><figDesc>CLEF 2004 official results: Multilingual tasks.</figDesc><table coords="6,199.76,344.46,203.49,58.67"><row><cell>Language</cell><cell>Run</cell><cell>AvgP</cell><cell>Dif.</cell></row><row><cell></cell><cell cols="2">CLEF Average 0.2339</cell><cell></cell></row><row><cell>English</cell><cell>nexp</cell><cell>0.2204</cell><cell></cell></row><row><cell></cell><cell>pexp</cell><cell cols="2">0.2353 +0.6%</cell></row><row><cell></cell><cell>dexp</cell><cell>0.2330</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,105.24,700.20,91.59,8.37"><p>www.freetranslation.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="5,105.24,709.71,102.65,8.37"><p>http://world.altavista.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="5,105.24,719.22,175.70,8.37"><p>http://translation.paralink.com/translation.asp</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,110.48,458.99,402.53,10.46;7,110.48,470.95,402.54,10.46;7,110.48,482.91,199.76,10.46" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,177.60,458.99,206.03,10.46">Passage-Level Evidence in Document Retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,407.62,458.99,105.39,10.46;7,110.48,470.95,398.34,10.46">Proceedings of the 17th Annual International Conference on Research and Development in Information Retrieval</title>
		<meeting>the 17th Annual International Conference on Research and Development in Information Retrieval<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="302" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,502.83,397.02,10.46" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="7,157.79,502.83,232.43,10.46">Cross-Language Retrieval Experiments at CLEF-2002</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<editor>Peters et al.</editor>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="5" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,522.75,402.52,10.46;7,110.48,534.71,402.53,10.46;7,110.48,546.67,402.53,10.46;7,110.48,558.62,70.01,10.46" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,110.48,534.71,325.83,10.46">Question Answering: CNLP at the TREC-10 Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Diekema</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Taffet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Mccracken</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ozgencil</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Yilmazel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Liddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,462.98,534.71,50.03,10.46;7,110.48,546.67,144.94,10.46">Tenth Text REtrieval Conference (Notebook)</title>
		<meeting><address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<publisher>NIST Special Publication</publisher>
			<date type="published" when="2001-11">Nov 2001</date>
			<biblScope unit="page" from="500" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,578.55,402.52,10.46;7,110.48,590.50,402.52,10.46;7,110.48,602.45,53.71,10.46" xml:id="b3">
	<analytic>
	</analytic>
	<monogr>
		<title level="m" coord="7,359.90,578.55,153.11,10.46;7,110.48,590.50,136.19,10.46">Proceedings of the Cross-Language Evaluation Forum (CLEF 2002)</title>
		<title level="s" coord="7,258.21,590.50,185.95,10.46">Lecture Notes in Computer Science, LNCS</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<meeting>the Cross-Language Evaluation Forum (CLEF 2002)</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2785</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,622.38,402.54,10.46;7,110.48,634.34,402.53,10.46" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,409.49,622.38,103.53,10.46;7,110.48,634.34,94.48,10.46">IR-n system, a Passage Retrieval Architecture</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>García-Puigcerver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Llopis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Toral</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Espí</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,212.60,634.34,197.75,10.46">Proceedings of Text Speech and Dialogue 2004</title>
		<meeting>Text Speech and Dialogue 2004<address><addrLine>Brno</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-09">September 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,654.27,402.53,10.46;7,110.48,666.22,402.54,10.46;7,110.48,678.17,149.93,10.46" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,220.12,654.27,229.16,10.46">Subtopic structuring for full-length document access</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hearst</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Plaunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,472.05,654.27,40.97,10.46;7,110.48,666.22,398.34,10.46">Sixteenth International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>Pittsburgh, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993-06">June 1993</date>
			<biblScope unit="page" from="59" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,698.10,402.53,10.46;7,110.48,710.05,350.83,10.46" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,216.14,698.10,182.89,10.46">Effective Ranking with Arbitrary Passages</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kaszkiel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,407.44,698.10,105.57,10.46;7,110.48,710.05,251.18,10.46">Journal of the American Society for Information Science and Technology (JASIST)</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="344" to="364" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.48,729.98,402.53,10.46" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,213.22,729.98,181.89,10.46">Empirical Methods for Compond Splitting</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,402.78,729.98,84.71,10.46">Proceding of EACL</title>
		<meeting>eding of EACL</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.48,110.35,356.53,10.46" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="8,219.01,110.35,95.03,10.46">IR-n system at CLEF</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Llopis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Vicedo</surname></persName>
		</author>
		<editor>Peters et al.</editor>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.48,130.28,402.52,10.46;8,110.48,142.23,138.18,10.46" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Llopis</surname></persName>
		</author>
		<title level="m" coord="8,160.39,130.28,293.40,10.46">IR-n un sistema de Recuperación de Información basado en pasajes</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>Universidad de Alicante</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="8,110.48,162.16,402.52,10.46;8,110.48,174.11,402.53,10.46;8,110.48,186.08,99.65,10.46" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,245.73,162.16,222.96,10.46">IR-n system, a passage retrieval system at CLEF</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Llopis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Vicedo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,110.48,174.11,297.57,10.46">Proceedings of the Cross-Language Evaluation Forum (CLEF 2001)</title>
		<meeting>the Cross-Language Evaluation Forum (CLEF 2001)</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2001">2001. 2002</date>
			<biblScope unit="volume">2406</biblScope>
			<biblScope unit="page" from="244" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.48,206.00,402.53,10.46;8,110.48,217.95,402.53,10.46;8,110.48,229.91,166.18,10.46" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,246.12,206.00,266.89,10.46;8,110.48,217.95,251.46,10.46">Sentence Boundary and Named Entity Recognition in EXIT System: Information Extraction System of Notarial Texts</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Muñoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Palomar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,382.48,217.95,130.52,10.46;8,110.48,229.91,94.23,10.46">Emerging Technologies in Accounting and Finance</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="129" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.48,269.76,402.52,10.46;8,110.48,281.71,242.97,10.46" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,219.25,269.76,247.34,10.46">Term-weighting Approaches in Automatic Text Retrieval</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,475.34,269.76,37.67,10.46;8,110.48,281.71,145.02,10.46">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="513" to="123" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
