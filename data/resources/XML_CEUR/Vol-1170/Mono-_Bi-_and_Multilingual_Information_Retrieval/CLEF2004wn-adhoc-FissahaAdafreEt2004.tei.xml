<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
				<funder ref="#_mDvr8dD #_6txrgbc #_JSPDd86">
					<orgName type="full">NWO</orgName>
				</funder>
				<funder ref="#_FdgrKVS">
					<orgName type="full">Netherlands Organization for Scientific Research (NWO)</orgName>
				</funder>
				<funder ref="#_5PdwG8z #_bpACEHV #_kVBb4Wz #_sTjqqfd #_KpPEAGz #_kM9rUSB">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,131.00,136.74,64.76,10.76"><forename type="first">Sisay</forename><surname>Fissaha</surname></persName>
							<email>sfissaha@science.uva.nl</email>
						</author>
						<author>
							<persName coords="1,198.75,136.74,33.19,10.76;1,258.83,136.74,35.39,10.76"><forename type="first">Adafre</forename><surname>Willem</surname></persName>
						</author>
						<author>
							<persName coords="1,297.21,136.74,80.71,10.76"><forename type="first">Robert</forename><surname>Van Hage</surname></persName>
						</author>
						<author>
							<persName coords="1,404.81,136.74,58.10,10.76;1,462.92,134.44,1.49,7.86"><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
							<email>kamps@science.uva.nl</email>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Currently at Archives and Information Studies</orgName>
								<orgName type="department" key="dep2">Faculty of Humanities</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,180.39,150.69,123.35,10.76"><forename type="first">Gustavo</forename><surname>Lacerda De Melo</surname></persName>
						</author>
						<author>
							<persName coords="1,330.64,150.69,82.88,10.76"><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">The University of Amsterdam at CLEF</orgName>
								<address>
									<postCode>2004</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Informatics Institute</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<addrLine>Kruislaan 403</addrLine>
									<postCode>1098 SJ</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">08BD960AACF061572C054776A175A0AF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe the official runs of our team for the CLEF 2004 ad hoc tasks. We took part in the monolingual task (for Finnish, French, Portuguese, and Russian), in the bilingual task (for Amharic to English, and English to Portuguese), and, finally, in the multilingual task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the CLEF 2004 evaluation exercise we participated in all three ad hoc retrieval tasks. We took part in the monolingual tasks for four non-English languages, Finnish, French, Portuguese, and Russian. The Portuguese language was new for CLEF 2004. Our participation in the monolingual task was a further continuation of our earlier efforts to monolingual retrieval <ref type="bibr" coords="1,225.49,377.31,15.77,8.97" target="#b10">[11,</ref><ref type="bibr" coords="1,243.91,377.31,7.47,8.97" target="#b4">5,</ref><ref type="bibr" coords="1,254.04,377.31,7.19,8.97" target="#b5">6]</ref>. Our first aim was to continue our experiments with a number of language-dependent techniques, in particular stemming algorithms for all European languages <ref type="bibr" coords="1,443.94,389.26,15.27,8.97" target="#b13">[14]</ref>, and compound splitting for the compound rich Finnish language. A second aim was to continue our experiments with languageindependent techniques, in particular the use of character n-grams, where we may also index leading and ending character sequences, and retain the original words. Our third aim was to experiment with combinations of runs.</p><p>We took part in the bilingual task, this year focusing on Amharic into English, and on English to Portuguese. Our bilingual runs were motivated by the following aims. Our first aim was to experiment with a language for which resources are few and far between, Amharic, and to see how far we could get by combining the scarcely available resources. Our second aim was to experiment with the relative effectiveness of a number of translation resources: machine translation <ref type="bibr" coords="1,193.85,484.90,16.60,8.97" target="#b14">[16]</ref> versus a parallel corpus <ref type="bibr" coords="1,307.82,484.90,10.58,8.97" target="#b6">[7]</ref>, and query translation versus collection translation. Our third aim was to evaluate the effectiveness of our monolingual retrieval approaches for imperfectly translated queries, shedding light on the robustness of these approaches.</p><p>Finally, we continued our participation for the multilingual task, where we experimented with straightforward ways of query translation, using machine translation whenever available, and a translation dictionary otherwise. We also experimented with combination methods using runs made on varying types of indexes.</p><p>In Section 2 we describe the FlexIR system as well as the approaches used for each of the tasks in which we participated. Section 3 describes our official retrieval runs for CLEF 2004. In Section 4 we discuss the results we have obtained. Finally, in Section 5, we offer some conclusions regarding our document retrieval efforts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Retrieval Approach</head><p>All retrieval runs used FlexIR, an information retrieval system developed at the University of Amsterdam. FlexIR supports many types of preprocessing, scoring, indexing, and retrieval tools. It also supports several retrieval models, including the standard vector space model, and language models. Our default retrieval model is a vector space model using the Lnu.ltc weighting scheme <ref type="bibr" coords="1,269.63,688.99,11.62,8.97" target="#b0">[1]</ref> to compute the similarity between a query and a document. For the experiments on which we report in this note, we fixed slope at 0.2; the pivot was set to the average number of unique words per document. We also experimented with language models <ref type="bibr" coords="1,381.14,712.90,10.58,8.97" target="#b2">[3]</ref>. Here, we used a uniform query term importance weight of 0.15.</p><p>Text normalization. We do some limited text normalization by removing punctuation, applying case-folding, and mapping diacritics to the unmarked characters. The Cyrillic characters used in Russian can appear in a variety of font encodings. The collection and topics are encoded using the UTF-8 or Unicode character encoding. We converted the UTF-8 encoding into KOI8 (Kod Obmena Informatsii), a 1-byte per character encoding. We did all our processing, such as lower-casing, stopping, stemming, and n-gramming, on documents and queries in this KOI8 encoding. Finally, to ensure proper indexing of the documents using our standard architecture, we converted the resulting documents into the Latin alphabet using the Volapuk transliteration. We processed the Russian queries similar to the documents. Morphological Normalization. We carried out extensive experiments with different forms of morphological normalizations for monolingual retrieval <ref type="bibr" coords="2,234.05,186.92,10.58,8.97" target="#b3">[4]</ref>. These include the following:</p><p>Stemming -For all languages we used a stemming algorithm to map word forms to their underlying stems. Stemming is a language-dependent approach to morphological normalization. We used the family of Snowball stemming algorithms, available for all the languages of the CLEF collections. Snowball is a small string processing language designed for creating stemming algorithms for use in information retrieval <ref type="bibr" coords="2,406.98,234.74,15.27,8.97" target="#b13">[14]</ref>.</p><p>Decompounding -For the compound-rich Finnish language, we also apply a decompounding algorithm. We treat all the words occurring in the Finnish collection as potential base words for decompounding, and also use their associated collection frequencies. We ignore words of length less than four as potential compound parts, thus a compound must have at least length eight. As a safeguard against oversplitting, we only regard compound parts that have a higher collection frequency than the compound itself. We retain the original compound words, and add their parts to the documents; the queries are processed similarly.</p><p>n-Gramming -For all languages, we used character n-gramming to index all character-sequences of a given length that occur in a word. n-Gramming is a language-independent approach to morphological normalization. We used three different ways of forming n-grams of length 4. First, we index pure 4-grams. For example, the word Information will be indexed as 4-grams info nfor form orma rmat mati atio tion. Second, we index 4grams with leading and ending 3-grams. For the example this will give inf info nfor form orma rmat mati atio tion ion . Third, we index 4-grams plus original words. For the example this gives info nfor form orma rmat mati atio tion information.</p><p>Stopwords. Both topics and documents were stopped using the stopword lists from the Snowball stemming algorithms <ref type="bibr" coords="2,115.44,420.04,15.49,8.97" target="#b13">[14]</ref>; for Finnish we used the Neuchâtel-stoplist <ref type="bibr" coords="2,309.34,420.04,15.27,8.97" target="#b9">[10]</ref>. Additionally, we removed topic specific phrases such as 'Find documents that discuss . . . ' from the queries. We did not use a stop stem or stop n-gram list, but we first used a stop word list, and then stemmed/n-grammed the topics and documents.</p><p>Blind Feedback. Blind feedback was applied to expand the original query with related terms. We experimented with different schemes and settings, depending on the various indexing methods and retrieval models used. For our Lnu.ltc runs term weights were recomputed by using the standard Rocchio method <ref type="bibr" coords="2,403.01,485.80,15.27,8.97" target="#b12">[13]</ref>, where we considered the top 10 documents to be relevant and the bottom 500 documents to be non-relevant. We allowed at most 20 terms to be added to the original query.</p><p>Combined Runs. We combined various 'base' runs using either a weighted or unweighted combination methods. The weighted interpolation was produced as follows. First, we normalized the retrieval status values (RSVs), since different runs may have radically different RSVs. For each run we reranked these values in [0, 1] using RSV i = (RSV i -min i )/(max i -min i ); this is the Min Max Norm considered in <ref type="bibr" coords="2,382.93,563.51,10.57,8.97" target="#b7">[8]</ref>. Next, we assigned new weights to the documents using a linear interpolation factor λ representing the relative weight of a run:</p><formula xml:id="formula_0" coords="2,70.19,572.20,453.55,25.11">RSV new = λ • RSV 1 + (1-λ)•RSV 2 .</formula><p>For λ = 0.5 this is similar to the simple (but effective) combSUM function used by Fox and Shaw <ref type="bibr" coords="2,512.12,587.42,11.62,8.97" target="#b1">[2]</ref> The interpolation factors λ were loosely based on experiments on earlier CLEF data sets. When we combine more than two runs, we give all runs the same relative weight, effectively resulting in the familiar combSUM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Runs</head><p>We submitted a total of 24 retrieval runs: 12 for the monolingual task, 7 for the bilingual task, and 5 for the multi-lingual task. Below we discuss these runs in some detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Monolingual Runs</head><p>All our monolingual runs used the title and description fields of the topics. We constructed five different indexes for each of the languages using Words, Stems, 4-Grams, 4-Grams+start/end, and 4-Grams+Words:</p><p>• Words -no morphological normalization is applied, although for Finnish Split indicates that words are decompounded.</p><p>• Stems -topic and document words are stemmed using the morphological tools described in Section 2. For Finnish, Split+stem indicates that compounds are split, where we stem the words and compound parts.</p><p>• n-Grams -both topic and document words are n-grammed, using the settings discussed in Section 2.</p><p>We have three different indexes: 4-Grams; 4-Grams+words where also the words are retained; and 4-Grams+start/end with beginning and ending 3-grams.</p><p>On all these indexes we made runs using the Lnu.ltc retrieval model; on the Words and on the Stems index we also made runs with a language model, resulting in 7 base runs for French, Portuguese, and Russian. In addition, for the compound rich Finnish language we also applied a decompounding algorithm <ref type="bibr" coords="3,412.88,201.78,10.58,8.97" target="#b3">[4]</ref>, on words and on stems, from which we produced base runs with both the Lnu.ltc retrieval model and a language model, leading to a total of 11 base runs for Finnish. All our official submissions were combinations of the base runs just described. For each of the four languages we constructed two combinations of stemmed and n-grammed base runs, as well as a "grand" combination of all base runs. Table <ref type="table" coords="3,137.57,261.56,4.98,8.97">1</ref> provides an overview of the runs that we submitted for the monolingual task. The third column in Table <ref type="table" coords="3,105.56,273.51,4.98,8.97">1</ref> indicates the type of run, and for two-way combinations the interpolation factor λ used is given in the fourth column.  <ref type="table" coords="3,94.68,468.35,3.88,8.97">1</ref>: Overview of the monolingual runs submitted. For combined runs column 3 gives the base runs that were combined, and column 4 gives the interpolation factor λ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bilingual Runs</head><p>For the bilingual task, we focused on Amharic to English, and English to Portugues. We submitted a total of 7 runs; all of them used the title and description fields of the topics. For our bilingual runs, we experimented with the WorldLingo machine translation <ref type="bibr" coords="3,219.31,564.29,16.60,8.97" target="#b14">[16]</ref> for translations into Portuguese, with a parallel corpus for translations into Portuguese, and with a variety of techniques for the Amharic topics, as we will now explain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">English to Portuguese</head><p>Machine Translation. We used the WorldLingo machine translation <ref type="bibr" coords="3,355.38,621.11,16.60,8.97" target="#b14">[16]</ref> for translating the English topics into Portuguese. The translation is actually in Brazilian Portuguese, but the linguistic differences between Portuguese and Brazilian are fairly limited.</p><p>Parallel Corpus. We used the sentence-aligned parallel corpus <ref type="bibr" coords="3,329.23,671.20,10.58,8.97" target="#b6">[7]</ref>, based on the Official Journal of the European Union <ref type="bibr" coords="3,98.48,683.15,15.27,8.97">[15]</ref>. We built a Portuguese to English translation dictionary, based on a word alignment in the parallel corpus. Since the word order in English and Portuguese are not very different, we only considered potential alignments with words in the same position, or one or two positions off. We ranked potential translations with a score based on:</p><p>• Cognate matching -Rewarding similarity in word forms, by looking at the number of leading characters that agree in both languages.</p><p>• Length matching -Rewarding similarity in word lengths in both languages.</p><p>• Frequency matching -Rewarding similarity in word frequency in both languages.</p><p>To further aid the alignment, we constructed a list of 100 most frequent Portuguese words in the corpus, and manually translated these to English. The alignments of these highly frequent words were resolved before the word alignment phase. We built a Portuguese to English translation dictionary by choosing the most likely translation, where we only include words that score above a threshold. The length of the translation dictionary is 19,554 words. We use the translation dictionary resulting from the parallel corpus for two different purposes. Firstly, we translate the English topics into Portuguese. Secondly, we translate the Portuguese collection into English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Amharic to English</head><p>Amharic, which belongs to the Semitic family of languages, is one of the most widely spoken languages in Ethiopia. In Amharic, word formation involves affixation, reduplication, Semitic stem interdigitation, among others. The most characteristic feature of Amharic morphology is root-pattern phenomena. This is especially true of Amharic verbs, which rely heavily on the arrangement of consonants and vowels in order to code different morphosyntactic properties (such as perfect, imperfect, jussive etc.). Consonants, which mostly carry the semantic core of the word, form the root of the verb. Consonants and vowel patterns together constitute the stems, and stems take different types of affixes (prefixes and suffixes) to form the fully inflected words; see <ref type="bibr" coords="4,428.75,289.91,15.27,8.97" target="#b11">[12]</ref>.</p><p>For our bilingual Amharic to English runs, we attempted to show how the (minimal) available resources for Amharic can be used in (Amharic-English) bilingual information retrieval settings. Since English is used on the document side, it is interesting to see how the existing retrieval techniques can be optimized in order to make the best use of the output of the error-prone translation component.</p><p>Resources and Query Translation. Our Amharic to English query translation is based mainly on dictionary look up. We used an Amharic-English bilingual dictionary which consists of 15,000 fully inflected words. Due to the morphological complexity of the language, we expected the dictionary to have limited coverage. In order to improve on the coverage, two further dictionaries, root-based and stem-based, were derived from the original dictionary. We also tried to augment the dictionary with a bilingual lexicon extracted from aligned Amharic-English Bible text. However, most of the words are old English words and are also found in the dictionary. The word dictionary also contains commonly used Amharic collocations. Multiword collocations were identified and marked in the topics. For this purpose, we used a list of multiword collocations extracted from an Amharic text corpus. The dictionaries were searched for a translation of Amharic words in the following order: word-dictionary, stem dictionary, root dictionary. Total no. of words Word dictionary Root dictionary English spell checker 1,893 813 178 57</p><p>Table <ref type="table" coords="4,131.77,526.03,3.88,8.97">2</ref>: Coverage of the respective techniques over the words occurring in the Amharic topics.</p><p>Leaving aside the ungrammaticality of the output of the above translation, there are a number of problems. One is the problem of unknown words. The words may be Amharic words not included in the dictionary or foreign words. Some foreign words and their transliteration have the same spelling or are nearly identical. To take advantage of this fact, a word is checked using an English spellchecker (Aspell); if found, it is returned as a translation. In some cases, there may be typographical variations between the English word and its transliteration; to address this, the first word among the suggestions will be checked for string similarity. If it falls above some threshold, it is taken as translation. Other unknown words are simply passed over to the English translation. Another problem relates to the selection of the appropriate translation from among the possible translations found in the dictionary. In the absence of frequency information, which allows selecting the right translation, the most frequently used English word is selected as a translation of the corresponding Amharic word. This is achieved by querying the web. The coverage of the translation is 55%. The number of correct translations is still lower. Table <ref type="table" coords="4,428.22,669.49,4.98,8.97">2</ref> gives some idea of the performance of the translation strategy.</p><p>For both English and Portuguese we used a similar set of indexes as for the monolingual runs described earlier (Words, Stems, 4-Grams, 4-Grams+start/end, 4-Grams+words); for all of these, Lnu.ltc runs were produced, and for the Word and Stems indexes we also produced a language model run, leading to 7 base runs for the Amharic to English task. Additionally, for the English to Portuguese task we used three types of translation: query translation using machine translation (WorldLingo), query translation using a parallel corpus (query EU), and collection translation using a parallel corpus (collection EU). This gave rise to a total of 21 base runs for the English to Portuguese task. Table <ref type="table" coords="5,109.28,109.21,4.98,8.97">3</ref> provides an overview of the runs that we submitted for the bilingual task. The fourth column in  <ref type="table" coords="5,95.38,247.08,3.88,8.97">3</ref>: Overview of the bilingual runs submitted. For combined runs column 4 gives the base runs that were combined, and column 5 gives the interpolation factor λ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multilingual Runs</head><p>We submitted a total of 4 multilingual runs, all using the title and description of the English topic set. The multilingual runs were based on the following mono-and bilingual runs:</p><p>• English to English -This is just a monolingual run, similarly processed as the other monolingual runs above.</p><p>• English to Finnish -We translated the English topics into Finnish using the Mediascape on-line dictionary <ref type="bibr" coords="5,115.47,381.95,10.58,8.97" target="#b8">[9]</ref>. For words present in the dictionary, we included all possible translations available. For words not present in the dictionary, we simply retained the original English words.</p><p>• English to French -We translated the English topics into French using the WorldLingo machine translation <ref type="bibr" coords="5,113.09,425.78,15.27,8.97" target="#b14">[16]</ref>.</p><p>• English to Russian -Again, we translated the English topics into Russian using the WorldLingo machine translation <ref type="bibr" coords="5,139.65,457.66,15.27,8.97" target="#b14">[16]</ref>.</p><p>Results of the mono-and bilingual runs just described were combined using unweighted combSUM. We also translated topics using another Russian on-line translator. However, the resulting translations were identical those provided by WorldLingo. We submitted a fifth multilingual run, UAmsC04EnMuAll2, including English to Russian results using both translations. This run scored inferior due to the overweighting of the Russian documents. Table <ref type="table" coords="5,110.22,525.41,4.98,8.97" target="#tab_2">4</ref> provides an overview of the runs that we submitted for the multilingual task. The fourth column in Table <ref type="table" coords="5,94.57,537.36,4.98,8.97" target="#tab_2">4</ref> indicates the document sets used. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>This section summarizes the results of our CLEF 2004 submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Monolingual Results</head><p>Table <ref type="table" coords="6,95.35,92.02,4.98,8.97" target="#tab_3">5</ref> contains the mean average precision (MAP) scores for all the monolingual 'base' runs described in the previous section. The language model experiment clearly indicate the effectiveness of the stemming algorithm.</p><p>For the vector space model, there is a small loss for Portuguese, but also a gain in performance for the other three languages. The outcome for the n-gram runs is less clear: there is a substantial gain in effectiveness for Finnish, but no or only a moderate gain for the other three languages. When comparing 4-gram with 4-gram+start/end, we see that including leading and ending 3-grams is always effective. Similarly, including words is effective for three of the four languages. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Finnish</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Bilingual Results</head><p>Table <ref type="table" coords="6,94.32,617.86,4.98,8.97" target="#tab_6">8</ref> shows the mean average precision scores for our base runs. For the resource-poor Amharic to English task, we expected a fairly low performance, somewhere in the 0.12-0.20 range. However, the vector space model run on the Words index is surprisingly effective. Furthermore, n-gramming leads to a loss of performance.</p><p>If we compare the different translation methods for the English to Portuguese tasks, and for the plain Words index, we see that, for query translation, the machine translation is more effective than the parallel corpus. This is as expected, since a word by word translation dictionary was derived from the parallel corpus. However, if the parallel corpus is used to translate the collection, we obtain a higher score for the Words index than both query translation methods. Applying a stemming algorithm is helpful for the MAP score for both ways of query translation, although it hurts the score of the collection translation. The use of n-gramming is only effective for query translation with the parallel corpus, where it leads to substantial improvements in the MAP score. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Multilingual Results</head><p>Table <ref type="table" coords="7,94.67,312.58,4.98,8.97" target="#tab_8">9</ref> shows our mean average precision scores for all base runs used in the multilingual task. We did not apply decompounding to the Finnish topics. As an aside, we see that for monolingual English, the language model is particularly effective. The results for Finnish, French, and Russian are generally in line with the monolingual results discussed above, be it that the n-gramming approaches are generally more effective on the translated topics. Table <ref type="table" coords="7,95.35,570.40,4.98,8.97" target="#tab_8">9</ref> also includes the run combinations submitted as official runs; recall that all these combinations are unweighted. The additional multilingual run having two English to Russian runs for each of the indexes scored lower with an MAP of 0.2520. On the whole, the performance increases with the number of runs included in the combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>English to</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper we documented the University of Amsterdam's participation in the CLEF 2004 ad hoc retrieval tasks: monolingual, bilingual, and multilingual retrieval. For the monolingual task, we conducted experiments on the effectiveness of morphological normalization approaches and combination methods. Our results shed further light on the effectiveness of language-dependent and language-independent approached to morphological normalization. As to the bilingual task, we experimented with bilingual retrieval in a resource-poor language, Amharic, and examined the relative effectiveness of different translation resources and of query versus collection translation. Our results indicate interesting differences between the bilingual approaches. The effectiveness of combining different translation methods was highlighted by the fact that the best bilingual score outperformed the best monolingual score. Finally, for the multilingual task, we experimented with straightforward query translations and combination methods, and showed the effectiveness of combining a wide range of base runs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,70.19,305.14,385.05,172.17"><head>Table</head><label></label><figDesc></figDesc><table coords="3,138.45,305.14,316.78,152.83"><row><cell></cell><cell>Language</cell><cell>Type</cell><cell>Factor</cell></row><row><cell>UAmsC04FiFi4GiSb</cell><cell>FI</cell><cell>4-Grams+words;Split+stem</cell><cell>0.4</cell></row><row><cell>UAmsC04FiFi4GiWd</cell><cell>FI</cell><cell>4-Grams+start/end;Split</cell><cell>0.4</cell></row><row><cell>UAmsC04FiFiAll</cell><cell>FI</cell><cell cols="2">Grand combination of 11 runs -</cell></row><row><cell>UAmsC04FrFr4GiSb</cell><cell>FR</cell><cell>4-Grams+words;Stems</cell><cell>0.6</cell></row><row><cell>UAmsC04FrFr4GiWd</cell><cell>FR</cell><cell>4-Grams+start/end;Words</cell><cell>0.6</cell></row><row><cell>UAmsC04FrFrAll</cell><cell>FR</cell><cell>Grand combination of 7 runs</cell><cell>-</cell></row><row><cell>UAmsC04PoPo4GiSb</cell><cell>PT</cell><cell>4-Grams+words;Stems</cell><cell>0.4</cell></row><row><cell cols="2">UAmsC04PoPo4GiWd PT</cell><cell>4-Grams+start/end;Words</cell><cell>0.4</cell></row><row><cell>UAmsC04PoPoAll</cell><cell>PT</cell><cell>Grand combination of 7 runs</cell><cell>-</cell></row><row><cell>UAmsC04RuRu4GiSb</cell><cell>RU</cell><cell>4-Grams+words;Stems</cell><cell>0.5</cell></row><row><cell cols="2">UAmsC04RuRu4GiWd RU</cell><cell>4-Grams+start/end;Words</cell><cell>0.5</cell></row><row><cell>UAmsC04RuRuAll</cell><cell>RU</cell><cell>Grand combination of 7 runs</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,70.19,109.21,453.54,146.83"><head>Table</head><label></label><figDesc>Table 3 indicates the type of run.</figDesc><table coords="5,99.64,143.65,394.41,93.05"><row><cell>Run</cell><cell cols="2">Topics Documents</cell><cell>Type</cell><cell>Factor</cell></row><row><cell>UAmsC04EnPo4GiSb</cell><cell>EN</cell><cell>PT</cell><cell cols="2">4-Grams+words;Stems (collection EU) 0.7</cell></row><row><cell>UAmsC04EnPo4iSPC</cell><cell>EN</cell><cell>PT</cell><cell>4-Grams+words;Stems (query EU)</cell><cell>0.7</cell></row><row><cell cols="2">UAmsC04EnPo4iSWL EN</cell><cell>PT</cell><cell>4-Grams+words;Stems (WorldLingo)</cell><cell>0.7</cell></row><row><cell>UAmsC04EnPoAll</cell><cell>EN</cell><cell>PT</cell><cell>Grand combination of 21 runs</cell><cell>-</cell></row><row><cell>UAmsC04AmEnWrd</cell><cell>AM</cell><cell>EN</cell><cell>Words</cell><cell>-</cell></row><row><cell cols="2">UAmsC04AmEn4GiSb AM</cell><cell>EN</cell><cell>4-Grams+words;Stems</cell><cell>0.7</cell></row><row><cell>UAmsC04AmEnAll</cell><cell>AM</cell><cell>EN</cell><cell>Grand combination of 7 runs</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,70.19,557.80,453.54,88.49"><head>Table 4 :</head><label>4</label><figDesc>Overview of the multilingual runs submitted. Column 4 indicates the base runs used to generate the multilingual run.</figDesc><table coords="5,101.95,557.80,390.02,57.19"><row><cell></cell><cell>Topics</cell><cell>Documents</cell><cell>Type</cell></row><row><cell>UAmsC04EnMu4Gr</cell><cell>EN</cell><cell cols="2">EN, FI, FR, RU 4 × 4-Grams+words</cell></row><row><cell cols="2">UAmsC04EnMuWSLM EN</cell><cell cols="2">EN, FI, FR, RU 8 × Words LM, Stems LM</cell></row><row><cell>UAmsC04EnMu3Way</cell><cell>EN</cell><cell cols="2">EN, FI, FR, RU 12 × Words, Stems, 4-Grams+start/end</cell></row><row><cell>UAmsC04EnMuAll</cell><cell>EN</cell><cell cols="2">EN, FI, FR, RU Grand combination of 7 runs per language</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,70.19,184.89,453.55,216.27"><head>Table 5 :</head><label>5</label><figDesc>Overview of MAP scores for monolingual base runs. Best scores are in boldface.Table6contains the MAP scores for the Finnish decompounding experiments. Decompouding leads to improvements for both retrieval models; decompounding and stemming only leads to improvements for the language model run. All Finnish n-gram runs in Table5outperform all decompounded runs.</figDesc><table coords="6,167.29,184.89,259.34,216.27"><row><cell></cell><cell></cell><cell cols="3">French Portuguese Russian</cell></row><row><cell>Words</cell><cell cols="2">0.3776 0.4084</cell><cell cols="2">0.4032</cell><cell>0.3186</cell></row><row><cell>Stems</cell><cell cols="2">0.4549 0.4312</cell><cell cols="2">0.4023</cell><cell>0.3611</cell></row><row><cell>4-Grams</cell><cell cols="2">0.4949 0.3673</cell><cell cols="2">0.3439</cell><cell>0.2783</cell></row><row><cell cols="3">4-Grams+start/end 0.5264 0.3794</cell><cell cols="2">0.3653</cell><cell>0.3212</cell></row><row><cell>4-Grams+words</cell><cell cols="2">0.4930 0.4133</cell><cell cols="2">0.3723</cell><cell>0.3357</cell></row><row><cell>Words LM</cell><cell cols="2">0.3825 0.4059</cell><cell cols="2">0.4040</cell><cell>0.2958</cell></row><row><cell>Stems LM</cell><cell cols="2">0.4530 0.4463</cell><cell cols="2">0.4269</cell><cell>0.3847</cell></row><row><cell></cell><cell cols="2">Finnish Words</cell><cell cols="2">Finnish Stems</cell></row><row><cell></cell><cell>Words</cell><cell>Split</cell><cell>Stems</cell><cell>Split</cell></row><row><cell cols="5">Lnu.ltc 0.3776 0.4329 0.4549 0.4414</cell></row><row><cell>LM</cell><cell cols="4">0.3825 0.4021 0.4530 0.4617</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,70.19,413.53,453.55,134.70"><head>Table 6 :</head><label>6</label><figDesc>Overview of MAP scores for Finnish decompounding runs.Finally, Table7lists the MAP scores for our official runs. For these, the grand combination of all base runs always outperforms the combination of a single (non)stemmed run and a single n-grammed run. When comparing with the best scoring base runs in Tables 5, we see that there is only a substantial improvement for Russian. There is a moderate improvement for French and Portuguese. The best Finnish n-gram run even outperforms the grand combination.</figDesc><table coords="6,286.26,503.01,169.70,8.97"><row><cell>Finnish French Portuguese Russian</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,79.80,558.62,434.32,8.97"><head>Table 7 :</head><label>7</label><figDesc>Overview of MAP scores for our officially submitted runs. Best scores per language are in boldface.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="6,70.19,737.41,453.55,20.92"><head>Table 8</head><label>8</label><figDesc>also contains the run combinations that were submitted as official runs. The combination of a stemmed and a n-grammed run does generally not lead to improvement. The combination of all base runs leads to the best</figDesc><table coords="7,110.65,71.75,372.61,129.31"><row><cell></cell><cell>Amharic to English</cell><cell></cell><cell>English to Portuguese</cell><cell></cell></row><row><cell></cell><cell>(query)</cell><cell cols="3">(query EU) (Wordlingo) (collection EU)</cell></row><row><cell>Words</cell><cell>0.2071  *</cell><cell>0.2641</cell><cell>0.3220</cell><cell>0.3830</cell></row><row><cell>Stems</cell><cell>0.1961</cell><cell>0.3201</cell><cell>0.3901</cell><cell>0.3281</cell></row><row><cell>4-Grams</cell><cell>0.1224</cell><cell>0.3704</cell><cell>0.2134</cell><cell>0.2954</cell></row><row><cell>4-Grams+start/end</cell><cell>0.1300</cell><cell>0.3826</cell><cell>0.2296</cell><cell>0.2856</cell></row><row><cell>4-Grams+words</cell><cell>0.1467</cell><cell>0.3678</cell><cell>0.2355</cell><cell>0.3203</cell></row><row><cell>Words LM</cell><cell>0.1694</cell><cell>0.2511</cell><cell>0.3167</cell><cell>0.3471</cell></row><row><cell>Stems LM</cell><cell>0.1703</cell><cell>0.2993</cell><cell>0.3835</cell><cell>0.3257</cell></row><row><cell>4-Grams+words;Stems</cell><cell>0.1915  *</cell><cell>0.2755  *</cell><cell>0.3207  *</cell><cell>0.3850  *</cell></row><row><cell>All base runs</cell><cell>0.2138  *</cell><cell></cell><cell>0.4366  *</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="7,70.19,211.45,453.55,62.90"><head>Table 8 :</head><label>8</label><figDesc>Overview of MAP scores for all bilingual runs. Best scores are in boldface. Officially submitted runs are marked with an asterisk. performance for Amharic to English, as well as for English to Portuguese. The score for English to Portuguese is particularly impressive, even outperforming our best monolingual score for Portuguese.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="7,70.19,382.88,453.55,172.57"><head>Table 9 :</head><label>9</label><figDesc>Overview of MAP scores for all multilingual runs (bottom half) and of the mono-and bilingual runs used to produce them (top half). Best scores are in boldface. Officially submitted runs are marked with an asterisk.</figDesc><table coords="7,147.05,382.88,299.81,141.27"><row><cell></cell><cell>English Finnish French Russian</cell></row><row><cell>Words</cell><cell>0.4488 0.2057 0.3351 0.2012</cell></row><row><cell>Stems</cell><cell>0.4885 0.2719 0.3677 0.1478</cell></row><row><cell>4-Grams</cell><cell>0.3986 0.2376 0.3585 0.2140</cell></row><row><cell>4-Grams+start/end</cell><cell>0.4369 0.2578 0.3810 0.2623</cell></row><row><cell>4-Grams+words</cell><cell>0.4387 0.2270 0.3596 0.2595</cell></row><row><cell>Words LM</cell><cell>0.4909 0.1913 0.3489 0.1935</cell></row><row><cell>Stems LM</cell><cell>0.5156 0.2303 0.3676 0.1978</cell></row><row><cell>4-Grams+words</cell><cell>0.2333  *</cell></row><row><cell>Words LM;Stems LM</cell><cell>0.3040  *</cell></row><row><cell>Words;Stems;4-Grams+start/end</cell><cell>0.3258  *</cell></row><row><cell>All</cell><cell>0.3427  *</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We want to thank <rs type="person">Valentin Jijkoun</rs> for help with the Russian collection. <rs type="person">Sisay Fissaha Adafre</rs> was supported by the <rs type="funder">Netherlands Organization for Scientific Research (NWO)</rs> under project number <rs type="grantNumber">220-80-001</rs>. <rs type="person">Jaap Kamps</rs> was supported by a grant from <rs type="funder">NWO</rs> under project number <rs type="grantNumber">612.066.302</rs>. Maarten de Rijke was supported by grants from <rs type="funder">NWO</rs>, under project numbers <rs type="grantNumber">612-13-001</rs>, <rs type="grantNumber">365-20-005</rs>, <rs type="grantNumber">612.069.006</rs>, <rs type="grantNumber">612.000.106</rs>, <rs type="grantNumber">220-80-001</rs>, <rs type="grantNumber">612.000.207</rs>, <rs type="grantNumber">612.066.302</rs>, and <rs type="grantNumber">264-70-050</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_FdgrKVS">
					<idno type="grant-number">220-80-001</idno>
				</org>
				<org type="funding" xml:id="_mDvr8dD">
					<idno type="grant-number">612.066.302</idno>
				</org>
				<org type="funding" xml:id="_6txrgbc">
					<idno type="grant-number">612-13-001</idno>
				</org>
				<org type="funding" xml:id="_JSPDd86">
					<idno type="grant-number">365-20-005</idno>
				</org>
				<org type="funding" xml:id="_5PdwG8z">
					<idno type="grant-number">612.069.006</idno>
				</org>
				<org type="funding" xml:id="_bpACEHV">
					<idno type="grant-number">612.000.106</idno>
				</org>
				<org type="funding" xml:id="_kVBb4Wz">
					<idno type="grant-number">220-80-001</idno>
				</org>
				<org type="funding" xml:id="_sTjqqfd">
					<idno type="grant-number">612.000.207</idno>
				</org>
				<org type="funding" xml:id="_KpPEAGz">
					<idno type="grant-number">612.066.302</idno>
				</org>
				<org type="funding" xml:id="_kM9rUSB">
					<idno type="grant-number">264-70-050</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,91.77,236.93,175.77,8.97;8,305.55,236.93,218.18,8.97;8,91.77,248.89,431.97,8.97;8,91.77,260.84,216.16,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,248.97,236.93,18.56,8.97;8,305.55,236.93,142.74,8.97">New approaches using SMART: TREC 4</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,120.42,248.89,194.10,8.97">The Fourth Text REtrieval Conference (TREC-4)</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<publisher>NIST Special Publication</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="500" to="236" />
		</imprint>
		<respStmt>
			<orgName>National Institute for Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.77,276.78,431.96,8.97;8,91.77,288.74,431.97,8.97;8,91.77,300.69,140.30,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,203.11,276.78,137.36,8.97">Combination of multiple searches</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,456.24,276.78,67.49,8.97;8,91.77,288.74,128.56,8.97">The Second Text REtrieval Conference (TREC-2)</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<publisher>NIST Special Publication</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="500" to="215" />
		</imprint>
		<respStmt>
			<orgName>National Institute for Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.77,316.63,431.97,8.97;8,91.77,328.59,212.73,8.97" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="8,151.14,316.63,204.92,8.97">Using Language Models for Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>Center for Telematics and Information Technology, University of Twente</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="8,91.77,344.53,431.97,8.97;8,91.77,356.48,163.12,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,294.15,344.53,225.34,8.97">Monolingual document retrieval for European languages</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Hollink</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,91.77,356.48,85.07,8.97">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="52" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.77,372.42,431.97,8.97;8,91.77,384.38,431.96,8.97;8,91.77,396.33,207.24,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,241.18,372.42,241.01,8.97">Combining evidence for cross-language information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,313.32,384.38,210.41,8.97;8,91.77,396.33,24.36,8.97">Advances in Cross-Language Information Retrieval, CLEF</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2002. 2003</date>
			<biblScope unit="volume">2785</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.77,412.27,431.97,8.97;8,91.77,424.23,431.96,8.97;8,91.77,436.18,418.51,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,331.23,412.27,192.51,8.97;8,91.77,424.23,164.59,8.97">Language-dependent and language-independent approaches to cross-lingual text retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Sigurbjörnsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,91.77,436.18,184.07,8.97">Cross-Language Information Retrieval, CLEF</title>
		<title level="s" coord="8,303.23,436.18,139.13,8.97">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.77,452.12,431.97,8.97;8,91.77,465.33,200.65,7.04" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<ptr target="http://people.csail.mit.edu/people/koehn/publications/europarl/" />
		<title level="m" coord="8,132.43,452.12,237.27,8.97">European parliament proceedings parallel corpus 1996-2003</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.77,480.02,431.97,8.97;8,91.77,491.97,431.96,8.97;8,91.77,503.93,431.96,8.97;8,91.77,515.88,22.42,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,132.41,480.02,308.35,8.97">Combining multiple evidence from different properties of weighting schemes</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,222.17,491.97,301.56,8.97;8,91.77,503.93,213.97,8.97">Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Ingwersen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fidel</surname></persName>
		</editor>
		<meeting>the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="180" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.77,531.82,364.21,8.97" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="8,145.43,531.82,169.03,8.97">English-Finnish-English on-line dictionary</title>
		<ptr target="http://efe.scape.net/" />
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Mediascape</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.77,547.77,413.92,8.97" xml:id="b9">
	<monogr>
		<ptr target="http://www.unine.ch/info/clef" />
		<title level="m" coord="8,138.23,547.77,184.84,8.97">CLEF resources at the University of Neuchâtel</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>Neuchâtel</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.77,563.71,431.97,8.97;8,91.77,575.66,431.97,8.97;8,91.77,587.62,431.97,8.97;8,91.77,599.57,22.42,8.97" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,202.12,563.71,321.62,8.97;8,91.77,575.66,77.79,8.97">Shallow morphological analysis in monolingual information retrieval for Dutch, German and Italian</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,439.27,575.66,84.47,8.97;8,91.77,587.62,164.95,8.97">Evaluation of Cross-Language Information Retrieval Systems</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<meeting><address><addrLine>CLEF</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001. 2002</date>
			<biblScope unit="volume">2406</biblScope>
			<biblScope unit="page" from="262" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.77,615.51,431.96,8.97;8,91.77,627.47,63.12,8.97" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="8,138.18,615.51,266.48,8.97">Development of Stemming Algorithm for Amharic Text Retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nega</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
		<respStmt>
			<orgName>University of Sheffield</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="8,91.77,643.41,431.96,8.97;8,91.77,655.36,431.96,8.97;8,91.77,667.32,109.29,8.97" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,162.68,643.41,176.89,8.97">Relevance feedback in information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Rocchio</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,434.68,643.41,89.05,8.97;8,91.77,655.36,229.72,8.97">The SMART Retrieval System: Experiments in Automatic Document Processing</title>
		<title level="s" coord="8,329.08,655.36,190.10,8.97">Prentice-Hall Series in Automatic Computation</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="313" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.77,683.26,431.96,8.97;8,91.77,696.47,22.81,7.04" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="8,135.09,683.26,209.49,8.97">Stemming algorithms for use in information retrieval</title>
		<author>
			<persName coords=""><surname>Snowball</surname></persName>
		</author>
		<ptr target="http://www.snowball.tartarus.org/" />
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,91.77,727.09,283.63,8.97" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="8,142.99,727.09,65.71,8.97">Online translator</title>
		<author>
			<persName coords=""><surname>Worldlingo</surname></persName>
		</author>
		<ptr target="http://www.worldlingo.com/" />
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
