<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,93.39,149.13,416.04,15.12;1,250.59,171.09,101.73,15.12;1,121.59,192.93,359.89,15.12;1,147.51,214.89,307.75,15.12">Using surface-syntactic parser and Derivation from Randomness X-IOTA IR system used for CLIPS Mono &amp; Bilingual Experiments for CLEF 2004</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,181.83,248.76,63.03,8.78"><forename type="first">Gilles</forename><surname>Sérasset</surname></persName>
							<email>gilles.serasset@imag.fr</email>
						</author>
						<author>
							<persName coords="1,328.95,248.75,93.65,8.78"><forename type="first">Jean-Pierre</forename><surname>Chevallet</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratoire CLIPS-IMAG</orgName>
								<address>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">IPAL-CNRS</orgName>
								<orgName type="institution" key="instit2">I2R</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">STAR National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,93.39,149.13,416.04,15.12;1,250.59,171.09,101.73,15.12;1,121.59,192.93,359.89,15.12;1,147.51,214.89,307.75,15.12">Using surface-syntactic parser and Derivation from Randomness X-IOTA IR system used for CLIPS Mono &amp; Bilingual Experiments for CLEF 2004</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E1557F881235E776F940E83585DAF7AC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This document present work we have done for the CLEF 2004 participation. We promote the use of surface-syntactic parsing to extract indexing terms. We also promote the Derivation From Randomness weighting. For the bilingual part, we have tested reinforcement query weighting using an association thesaurus. * This work is part of the PRISM-IMAG project devoted to high level indexing representation using interlingual graph formalism.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In our previous participation at CLEF in 2003 <ref type="bibr" coords="1,309.08,440.04,9.91,8.78" target="#b3">[3]</ref>, we have tested the use of an association thesaurus to enhance query translation. We have only use the association thesaurus to add some new terms to the proposed translation terms. In our current participation, we have tried another use of such a thesaurus: we do not enlarge the query, but rather we use it to modify the weighing of a given translated query. Our basic idea is the selection of the best term translation using query context and association thesaurus from the corpus. Last year, we have neglected the study of the matching function and the influence of the weighting scheme. For this participation we will also focus on this aspect: we have test the Derivation From Randomness (DFR) against Okapi measure and some other classical IR weighting. We also promote the use of a surface-syntactic parser. All documents are first transform by the parser. The stemming is then proposed by the parser. Finnish is an agglutinative language, and using such an NL parsing enable to correctly split the glued words into separated correct indexing terms.</p><p>The paper present first the training experiments performed on 2003 collection in part 2. In part 3 we discuss the monolingual results. Then, in part 4, we present the technique used for bilingual results and present hypothesis based on the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Training on monolingual run</head><p>In this part, we present some training we have achieved using monolingual corpus of CLEF 2003. We have mainly used the Finnish and French corpus. The purpose of this training is to select the best weighting scheme for the given CLEF document collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The underlying IR model</head><p>All experiments are grounded on the classic vector space model. Goal of experiment is to compare the probabilistic model of Okapi with Derivation From Randomness model, versus more classical weightings. This comparison will be done on two different languages.</p><p>Basically, the final matching process is achieved by a product between query vector and document matrix, which computes the Relevant Status Value (RSV) of all document against the query. For a query vector Q = (q i ) with a dimension of t term i ∈ [1..t], and a index document matrix of</p><formula xml:id="formula_0" coords="2,90.03,201.59,138.45,10.65">n documents D j = (d ij ), j ∈ [1.</formula><p>.n], the RSV is computed by:</p><formula xml:id="formula_1" coords="2,239.79,225.83,122.58,21.69">RSV (Q, D j ) = i∈[1..t] q i * d ij</formula><p>We keep this matching process for all tests, the changes are in the documents and query processing to select indexing terms, and in the weighting scheme. We recall here the scheme that is inspired by the SMART system. We suppose the previous processing steps have produced a matrix D = (d i,j ). Usually, the value d i,j is only the result of term t i counting in the document D j , called term frequency tf ij . Each weighting scheme can be decomposed in three steps: a local, a global and a normalization step. The local is related to only one vector. All these transformations are listed in table <ref type="table" coords="2,115.03,331.32,3.91,8.78">1</ref>. For all measure we use the following symbols: weight of term i of query q</p><formula xml:id="formula_2" coords="2,110.91,342.60,223.28,59.01">f i total number of term i in the corpus d * ij is a normalization of d ij λ i is the fraction fi T T is the corpus size : T = i f i df i the document</formula><formula xml:id="formula_3" coords="2,193.11,505.19,216.90,84.02">n w ij = d ij none, no change b w ij = 1 binary a w ij = 0.5+0.5 * dij maxi(dij ) local max l w ij = ln(d ij + 1) natural log d w ij = ln(ln(d ij + 1) + 1) double natural log Table 1: Local weighting</formula><p>The global weighting is related to the matrix, and then it is a weighing which takes into account the relative importance of a term regarding the whole document collection. The most famous is the Inverse Document Frequency : Idf. The table 2 lists the global weighting we have tested. Okapi and DFR are not global weighting per se but rather complete weighting scheme themselves. In our X-IOTA system, they are computed at the same time than global weighting, and it is technically feasible to use them with a local and a final normalization. DFR is presented in the next part.</p><p>The Okapi measure described in <ref type="bibr" coords="2,254.32,696.00,10.58,8.78" target="#b5">[5,</ref><ref type="bibr" coords="2,269.33,696.00,6.97,8.78" target="#b4">4]</ref>, uses the length of the document, the function L(), and also a normalization by the average length of all documents in the corpus, the function A(). This length is related to the number of indexing terms in a document. The Okapi measure uses 2 constants values called k 1 and b. Finally, the last treatment is the normalization of the final vector.</p><p>n</p><formula xml:id="formula_4" coords="3,177.51,110.27,248.24,169.58">w ij = d ij none, no global change t w ij = d ij * log n dfi Idf p w ij = d ij * log n-dfi dfi Idf variant for Okapi O w ij = (k1+1) * dij k1 * [(1-b)+b * L(d j ) A(d j ) ]+dij Okapi R (see below) DFR Table 2: Global weighting n w ij = d ij none, no normalization c w ij = dij i d 2 ij cosine Table 3: Final normalization</formula><p>A weighting scheme is composed by the combination of the local, global and final weighting. We represent a weighting scheme by 3 letters. For example, nnn is only the raw term frequency. The scheme bnn for both documents and queries leads to a sort of Boolean model where every term in the query is considered connected by a conjunction. In that case the RVS counts the terms intersection between documents and queries. The c normalization applied to both document and query vector leads to the computation of the cosine between these two vectors. This is the classical vector space model if we use the ltc scheme for document and queries. The scheme nOn for the documents, and npn with the queries, is the Okapi model, and the use of nRn for document and nnn for the queries is the DFR model. For these two models, constants have to be defined.</p><p>Notice that the c normalization of the queries, leads to divide the RSV for this query by i q 2 i . For each query this is a constant value which does not influence the relative order of answered document list. It follows that this normalization is useless for queries and should not be used. In the next section we briefly present the derivation from Randomness weighting that seems to give best results, and that we have used for all CLEF 2004 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">derivation from randomness (DFR)</head><p>This weighting scheme has been proposed by Gianni Amati in <ref type="bibr" coords="3,379.33,512.64,10.45,8.78" target="#b1">[1]</ref> (with a small error in the definition of the value f * t,d ). Theoretical discussions about this approach can be found in <ref type="bibr" coords="3,499.62,524.64,10.00,8.78" target="#b2">[2]</ref>. Figure <ref type="figure" coords="3,121.45,536.52,4.98,8.78">4</ref> sum up the results we obtain using this weighting scheme, on the CLEF2003 queries for training using the formula described in <ref type="bibr" coords="3,263.16,548.52,9.92,8.78" target="#b1">[1]</ref>. The formula is given by:</p><formula xml:id="formula_5" coords="3,183.87,567.71,329.20,25.53">w ij = (log 2 (1 + λ i ) + d * ij * log 2 1 + λ i λ i ) * f i + 1 n i * (f * ij + 1)<label>(1)</label></formula><p>The value d * ij is a normalization by the length L(D j ) of the document D j regarding the average size of all document in the corpus : awr(D j ). A constant value c adjusts the effect of the document length in the weight.</p><formula xml:id="formula_6" coords="3,221.55,644.15,291.52,24.21">d * i,j = d ij * log 2 (1 + c * awr(L(D j )) L(D j ) )<label>(2)</label></formula><p>For this participation of CLEF, we have test this weighting scheme against another set of other computation. We present these results on Finnish and French collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Finnish IR</head><p>In these experiments, we have first tested the influence of stop words (SW) and stemming. We have not tested the influence of the surface syntactic parser, because the parsing was not available the time we have made these tests. The test performed here is done on the Finnish collection with 2003 queries. As the best results is of course, obtained with stop word and stemming, we have then tested the influence of the c constant in order to find out when we reach the optimum. The treatment we apply to both documents and queries is given by:</p><formula xml:id="formula_7" coords="4,90.03,168.65,246.47,44.23">xmlFilterTag | xml2Latin1 | xmldeldia | xmlcase | xmlAntiDico -dico common_word.fi | xmlcase -noAcc | xmlStemFi</formula><p>The first step is filtering the relevant tags from documents or queries. Then we transform XML special characters to their ISO counterpart. We delete all diacritic characters, and change to lower case. At this stage we still have special Finnish characters and accents. We eliminate common words using a list provided by Savoy<ref type="foot" coords="4,250.11,269.47,3.97,6.97" target="#foot_0">1</ref> and then suppress all accents from characters. We apply a Finnish stemmer also proposed by Savoy and modifies to accept XML input/output to produce the final vector. For the queries, we have used the following fields: FI-title FI-desc FI-narr. For documents only the text field has been used. When we examine DFR formula, one can see that when a term does not appear in document d, then only d i,j is null. Then d * i,j is also equal to zero. If we strictly apply the formula in that case, the weight of the term is still not null and is equal to the formula (4). For practical reason, we have replaced this residual value by zero. This approximation reduces the size of the inverse file, because we do not store null values in file. In fact we have applied the following weighting:</p><formula xml:id="formula_8" coords="4,239.79,514.43,273.28,22.65">w i,j = w i,j if d i,j = 0 0 if d i,j = 0<label>(3)</label></formula><p>Table <ref type="table" coords="4,117.45,544.68,4.98,8.78" target="#tab_2">5</ref> show results for some variation in the constant c. We can notice that optimum value is about c = 0.84. This optimization gain 1.21 points referring neutral value c = 1. One can also notice that we obtain more documents in the first 1000 answer for c = 2, but the average precision is lower, which means that they are not well sorted.</p><formula xml:id="formula_9" coords="4,245.19,599.63,267.88,24.21">w t,d = log 2 (1 + λ t ). f t + 1 n t (4)</formula><p>The conclusion of the use of this weighting is that a good constant c value seems to be 0.83. In the rest of the test, we will use the approximated value c = 0.8.</p><p>For the Okapi weighting, we have use the same value as in <ref type="bibr" coords="4,364.46,653.27,9.91,8.78" target="#b1">[1]</ref>, that is k 1 = 1.2, and b = 0.75. In table 9, we have also tested some other value for the French collection: it seems these values are on average good ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Testing query weighting</head><p>We have tested all combination of the following weight: bnn: This is the binary model. Terms presents are associated to the value 1, and 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>lnc:</head><p>The cosine is the finale normalization. When both used in document and queries, it ensure true vector space model matching, ie. only angle between query et document vector is used. This weighting suppose a log distribution of frequency.</p><p>ntc: This is the classical tf*idf measure. We used with queries, the idf is taken from the document collection, not the query collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ltc:</head><p>The same classical measure using log on term frequency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ltn:</head><p>The log tf*idf without the cosine normalization.</p><p>atn: Normalization with the local maximum term frequency is used with idf.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>dtn:</head><p>The double natural log is used in place of the simple one in ltn.</p><p>npn: It is the idf variant use for the Okapi system.</p><p>nRn: This is the name for Derivation from randomness.</p><p>nOn: This is the name for the Okapi probabilistic weighting.</p><p>Results are sum up in the table 6. We notice that the derivation from randomness model is very stable again the query weighting and that it has the best results in the majority of query weighting. We have the decided to use it for <ref type="bibr" coords="5,105.27,708.24,50.08,8.78">CLEF 2004</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">French IR</head><p>For training, we have used the French corpus of CLEF 2003. We have used our own stemmer, and our own list for removal of common French terms. In this collection, there are 3 sets of documents. For each collection we have selected the following fields: lemonde94 TITLE TEXT, and TI KW LD TX ST for sda 94 and 95. For the queries, we have selected the fields FR-title FR-desc FR-narr. We have tested the same combination of weighting schemes as the one tested in the Finnish collection. The results are in the Finally, we have taken the best weighting query scheme for the Okapi model (nOn) and we have computed some variation of the two constant k 1 and b. The results are in the table 9. The best values are obtained with the couple (1, 0.75) which confirm the choice usually taken for this measure.</p><p>In this language, we also demonstrate the stability of the DFR measure (nRn) which performs better than other query weightings, except with binary queries (bnn). We obtain the best average precision with the inverse document frequency (ntn).</p><p>We have not performed any special treatments for the queries, like removing terms that are not related to the theme (ex: document, retrieved, etc). The results show that a natural language analysis of the query to remove these empty words should improve the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Monolingual results</head><p>In this part, we comment the results we have obtained at CLEF 2004. We have participated to the monolingual track on French, Finnish and Russian. As we promote the use of syntactic parsing, we  <ref type="table" coords="7,242.19,379.44,3.91,8.78">9</ref>: k 1 and b variation for nOn ntn have submitted mono lingual run all using surface syntactic parsing. Because of time constrains, we have not trained the system with the parsed collection. So we can only compare with CLEF 2003 without natural parsing.</p><p>After each parsing, we have transformed the output into a common XML simplified format. One of the main interests in using a natural language parser is the correct normalization of words, the correct detection of compound nouns and correct filtering using lexical categories. For all run, we have choose the derivation from randomness weighting with the constant value fixed to c = 0.8, according to the training experiments. No special treatments are done on queries.</p><p>These characteristics are important for language which has a morphological derivation like French and Finnish. For all language, we have filtered only nouns, proper noun, verb and adjective. For French, we have used the XIP system from XEROX. After this filtering, we still remove some terms using a stop list, and used also a French stemming. The French queries are weighted using ntn. Hence we only modify the weight according to the inverse document frequency. This computation is of course performed using document corpus. The average precision is 44%, which is not an absolute good result. This value is a little lower than our training.</p><p>When we examine more closely the results, we discover a big discrepancy between queries. Figure <ref type="figure" coords="7,121.34,612.12,3.90,8.78">1</ref>, shows the histogram repartition of the 29 queries (from 201 to 250 without 227). There are a lot of query that are either very low precision level (18 queries under 20%) of very high (13 upper 80%).</p><p>For the Finnish monolingual run, we obtain an average prevision of 53%, witch is better than the results obtained on CLEF 2003. The histogram in figure <ref type="figure" coords="7,367.08,659.88,4.98,8.78">2</ref> shows that 10 query are above 90%, in fact exactly 5 queries reach 100% of precision.</p><p>We have use also a surface syntactic parser for the Russian collection, but we cannot compare yet with a more simple raw term indexing because we do not have a Russian stemmer and stop list. The average result of 35% is the lowest for all three languages. Query precision repartition in figure <ref type="figure" coords="7,129.39,719.64,4.98,8.78">3</ref> shows that a lot of query (12) have very low precision (under 10%).</p><p>The conclusion that we draw is the good behavior of DFR weighting, and probably the benefit of using a surface syntactic parsing on Finnish. In this language, the parser is able to "unglue" terms and so could achieve better results. We cannot investigate more our results, because we should compare on the same collection the use of the syntactic surface parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Topic translation</head><p>Bilingual results are obtained by translating the topics using general dictionaries built by compiling several bilingual dictionaries available online (see section 4.1). Then, we experimented 2 methods of translation (see sections 4.2). Both methods take the topic vectors as input and outputs a new translated topic vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Construction of the dictionaries</head><p>We compiled 6 bilingual translation dictionaries (see figure <ref type="figure" coords="9,349.99,260.04,4.46,8.78">4</ref>) using several resources available in house or from Internet. Each resulting dictionary associates a word form to a set of translations and is stored as an XML file (see figure <ref type="figure" coords="9,265.37,284.04,3.88,8.78" target="#fig_1">5</ref>). These dictionaries where compiled from the following sources:</p><p>• the Bilingual French-English dictionary from the university of Rennes 1, freely available at http://sun-recomgen.med.univ-rennes1.fr/Dico/,</p><p>• the FeM dictionary (French English Malay), freely accessible at http://www-clips.imag. fr/cgi-bin/geta/fem/fem.pl?lang=fr,</p><p>• the French English dictionary available for the participants on the CLEF web site,</p><p>• dictionary entries from the Logos website<ref type="foot" coords="9,295.23,551.95,3.97,6.97" target="#foot_1">2</ref> ,</p><p>• the "engrus" English Russian dictionary available on many web sites<ref type="foot" coords="9,412.47,570.79,3.97,6.97" target="#foot_2">3</ref> .</p><p>As for the French-Russian, French-Finnish and English-Finnish dictionaries, the only available online resource we used is the Logos web site. As it is the only online service we used (other data was available off-line), we chose to only extract entries that were present in the topics to be translated in order to avoid high loads on a public web site. This explains the very small size of these dictionaries.</p><p>As French and English were our topic languages of choices, we also reverted the merged French-English dictionaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Topic translation</head><p>For each bilingual task we participated in, we propose 2 methods of translation. Both methods take the topic vectors as input and outputs a new translated topic vector.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>English to</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Simple topic translation</head><p>The first method substitutes each term by all of its available translations. The weight associated to each translation is equal to the weight of the original term divided by the number of available translation (see figure <ref type="figure" coords="10,188.22,476.88,3.88,8.78" target="#fig_2">6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Filtering by way of an association thesaurus</head><p>As one may see in figure <ref type="figure" coords="10,203.16,521.16,3.90,8.78" target="#fig_1">5</ref>, many different translations may be found for a single term. Hence, we tried to develop a strategy to give more importance to the "correct" translation(s). For this, we tried to take some context into account, without changing anything to the available lexical resource.</p><p>For this, we needed contextual information in each language. Hence, we automatically built an association thesaurus (as exposed in <ref type="bibr" coords="10,272.63,580.92,10.83,8.78" target="#b3">[3]</ref>) for each language from the available monolingual documents (see figure <ref type="figure" coords="10,187.60,592.92,3.88,8.78">7</ref>).</p><p>Each association thesaurus is as a graph linking terms. Each arc in the graph links 2 terms that "regularly"<ref type="foot" coords="10,158.19,614.71,3.97,6.97" target="#foot_3">4</ref> appear in the same context. For this experiment, 2 terms are said to be in the same context when they appear in the same document.</p><p>For our experiment, we assume that terms that are close to each others share some common semantic. We also assume that their "correct" translations should also share the same semantic. Hence, we used these association thesaurus to know if terms and translations share some semantics. Hence, we chose to associate each translations t i,j of a term c j with a weight w ti,j depending on its distance (d ti,j ) with the translated context. The distance of a translation to the translated context is given by formula 5. </p><formula xml:id="formula_10" coords="11,195.87,487.07,211.24,10.65">d ti,j = M in(d(t i,j , t k,l ); ∀l, k | l = j, 1 ≤ k ≤ [T l |)</formula><p>where t k,l ∈ T l and T l is the set of translation of the term c l and d(t i,j , t k,l ) is the minimal distance in the target thesaurus between t i,j and t k,l <ref type="bibr" coords="11,500.31,505.80,12.76,8.78" target="#b5">(5)</ref> w ti,j = w j /d i,j if d i,j = 0 w j /|T j | if d i,j = 0 where w j is the weight of the source term c j in the source vector (6)</p><p>Figure <ref type="figure" coords="11,136.32,588.96,4.98,8.78">8</ref> shows a sample resulting translated vector. One may notice the higher weight of the selected translations (e.g. interest → ИНТЕРЕС).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discussion</head><p>CLIPS results on the bilingual tasks are rather disappointing, with interpolated recall-precision averages at 0.00 dropping from 57.68% (Monolingual Russian) to 17.1% with simple topic translation (Bilingual English-Russian: CLIPSENRU1) and even to 8.59% with filtered topic translation (CLIPSENRU2).</p><p>The main reason for this drop is certainly due to the lack of wide coverage bilingual lexical resources. The dictionaries we used were very small and did not provide translations for many terms of the topics. This is especially true for French to Russian and Finnish lexical resources where 60% to 70% of the source terms are not translated. However, English to Russian lexicon was a little better, and about 18% of the terms remain without translation. Figure <ref type="figure" coords="12,242.40,204.96,3.90,8.78">7</ref>: Size of the association thesaurii However, this does not explain the drop in interpolated recall-precision averages when filtering the translations through the association thesaurii, as it does not change the set of translations, but only the weight of those translations. Moreover, when manually evaluating the weighted translation, one usually agree with the translation that are chosen.</p><p>We think that 2 factors explains theses drops:</p><p>• First, in the simple topic translation method, the weight of each translation is divided by the number of translations for the source term. This lowers the relative importance of terms that bear many translations, (which is usually the case of general nouns or support verbs).</p><p>• Second, when raising the weight of "correct" translations by way of the association thesaurii, we also raise the weight of such general terms. Hence, we give more importance to terms that do not bear any thematic closeness with the requested documents (and this is especially the case with CLEF topics that are instructions usually containing "find documents reporting on. . . " or "find information on. . . ").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>All run are performed on the collection parsed using a syntactic surface parsing. Best monolingual results are obtained for the Finnish collection, probably because of the correct word splitting. We have to redo the tests with no analyzer to have a strong conclusion on its use in an IR context/ Bilingual results are disappointing but they are partly explained by the difficulty in finding wide coverage lexical resources for languages in which we previously had no experience whatsoever.</p><p>The filtering of translations through association thesaurii is rather interesting, even if we did not have enough time to use it appropriately. This technique may also be interesting in translation selection tasks or, with adaptation, on lexical disambiguation tasks. It's main interest in such tasks comes from the fact that it does not require any special training data (like parallel documents or manually disambiguated corpora) as association thesaurii may be computed automatically from the corpus. Hence such technique may easily bring some result in those tasks in any language, provided that monolingual data is available as well as an automatic process to lemmatize such corpora.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="8,190.71,278.52,221.76,8.78;8,202.83,140.04,197.14,113.30"><head>Figure 1 :Figure 2 :Figure 3 :</head><label>123</label><figDesc>Figure 1: Mono lingual French precision histogram</figDesc><graphic coords="8,202.83,140.04,197.14,113.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="10,194.91,392.76,213.32,8.78"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Resulting compiled dictionaries sample</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="11,227.43,426.72,148.45,8.78"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Simple topic translation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,90.03,319.44,423.03,102.02"><head>Table 4 :</head><label>4</label><figDesc>Results of DFR test with nnn query weighting scheme is in the table 5. When c is zero, then the equation becomes<ref type="bibr" coords="4,187.50,331.44,11.61,8.78" target="#b4">(4)</ref>, where term weight are all equal for all documents. Test weighting nRn nnn</figDesc><table coords="4,211.83,354.60,179.58,45.02"><row><cell>run</cell><cell cols="2">nRn nnn c=2 ret_rel (483)</cell></row><row><cell>raw</cell><cell>29.89</cell><cell>388</cell></row><row><cell>SW</cell><cell>35.39</cell><cell>429</cell></row><row><cell>stem SW</cell><cell>39.26</cell><cell>452</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,90.03,111.00,314.27,313.10"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table coords="5,233.07,111.00,137.10,248.30"><row><cell>c</cell><cell cols="2">precision ret_rel (483)</cell></row><row><cell>0.00</cell><cell>4.89</cell><cell>286</cell></row><row><cell>0.10</cell><cell>30.24</cell><cell>436</cell></row><row><cell>0.50</cell><cell>39.63</cell><cell>448</cell></row><row><cell>0.70</cell><cell>40.40</cell><cell>448</cell></row><row><cell>0.75</cell><cell>40.90</cell><cell>449</cell></row><row><cell>0.80</cell><cell>40.97</cell><cell>449</cell></row><row><cell>0.81</cell><cell>41.04</cell><cell>449</cell></row><row><cell>0.82</cell><cell>41.06</cell><cell>449</cell></row><row><cell>0.83</cell><cell>41.07</cell><cell>449</cell></row><row><cell>0.84</cell><cell>41.07</cell><cell>449</cell></row><row><cell>0.85</cell><cell>41.02</cell><cell>449</cell></row><row><cell>0.86</cell><cell>41.01</cell><cell>449</cell></row><row><cell>0.87</cell><cell>41.02</cell><cell>450</cell></row><row><cell>0.90</cell><cell>40.16</cell><cell>450</cell></row><row><cell>0.95</cell><cell>39.98</cell><cell>450</cell></row><row><cell>1.00</cell><cell>39.86</cell><cell>450</cell></row><row><cell>1.50</cell><cell>39.41</cell><cell>451</cell></row><row><cell>2.00</cell><cell>39.26</cell><cell>452</cell></row><row><cell>5.00</cell><cell>39.03</cell><cell>449</cell></row><row><cell>10.0</cell><cell>37.96</cell><cell>447</cell></row></table><note coords="5,238.23,372.96,166.07,8.78;5,90.03,415.32,170.11,8.78"><p><p>Variation of c for nRn nnn (stem AD)</p>nnn: Only the term frequency is used.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,158.53,708.24,48.27,8.78"><head>Table 6 :</head><label>6</label><figDesc>in all tests. Query weighting (stem SW c= 0.83)</figDesc><table coords="6,282.75,111.00,69.33,8.78"><row><cell>query weighting</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,215.43,373.56,172.40,171.14"><head>Table 7 :</head><label>7</label><figDesc>table 7 and 8. French average precision 1</figDesc><table coords="6,215.43,394.20,172.40,128.66"><row><cell></cell><cell></cell><cell cols="2">Query weighting</cell><cell></cell></row><row><cell>Doc.</cell><cell>nnn</cell><cell>bnn</cell><cell>lnn</cell><cell>ntn</cell></row><row><cell>nnn</cell><cell>7.72</cell><cell>2.78</cell><cell>5.71</cell><cell>16.71</cell></row><row><cell>bnn</cell><cell>16.01</cell><cell>4.25</cell><cell>13.19</cell><cell>29.73</cell></row><row><cell>atn</cell><cell>31.02</cell><cell>27.03</cell><cell>31.16</cell><cell>29.91</cell></row><row><cell>ntc</cell><cell>33.53</cell><cell>34.68</cell><cell>35.86</cell><cell>32.09</cell></row><row><cell>lnc</cell><cell>36.20</cell><cell>32.22</cell><cell>36.74</cell><cell>39.06</cell></row><row><cell>ltc</cell><cell>35.39</cell><cell>35.37</cell><cell>37.40</cell><cell>34.38</cell></row><row><cell>ltn</cell><cell>35.65</cell><cell>22.36</cell><cell>32.68</cell><cell>37.87</cell></row><row><cell cols="5">nRn 46.98 38.15 45.01 49.06</cell></row><row><cell cols="2">nOn 42.25</cell><cell>33.02</cell><cell>40.39</cell><cell>49.01</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,105.27,740.31,112.61,7.01"><p>http://www.unine.ch/info/clef</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="9,105.27,737.67,77.76,7.01"><p>http://www.logos.it/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="9,105.27,747.15,233.07,7.01"><p>see list of mirrors at http://sinyagin.pp.ru/engrus-mirrors.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="10,105.27,719.43,385.03,7.01"><p>In this experiment, we filtered out arcs that had a confidence score lower than 20% or higher than 90%.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,96.03,141.06,131.58,7.50;11,96.03,151.98,98.69,7.50;11,96.03,163.02,98.69,7.50;11,96.03,173.94,84.53,7.50;11,96.03,184.86,122.22,7.50;11,96.03,195.90,112.86,7.50;11,96.03,206.82,94.01,7.50;11,96.03,217.74,98.69,7.50;11,96.03,228.78,112.86,7.50;11,96.03,239.70,112.86,7.50;11,96.03,250.62,98.69,7.50;11,96.03,261.66,103.37,7.50;11,96.03,272.58,122.22,7.50;11,96.03,283.50,103.37,7.50;11,96.03,294.54,103.37,7.50;11,96.03,305.47,112.86,7.50;11,96.03,316.39,117.54,7.50;11,96.03,327.43,89.21,7.50;11,96.03,339.31,42.16,7.50;11,277.11,141.06,131.58,7.50;11,277.11,151.98,14.07,7.50;11,277.11,163.02,193.03,7.50;11,277.11,173.94,197.24,7.50;11,277.11,184.86,188.23,7.50;11,277.11,195.90,141.06,7.50;11,277.11,206.82,117.54,7.50;11,277.11,217.74,202.40,7.50;11,277.11,228.78,188.24,7.50;11,277.11,239.70,112.86,7.50;11,277.11,250.62,136.38,7.50;11,277.11,261.66,188.23,7.50;11,277.11,272.58,192.56,7.50;11,277.11,283.50,211.88,7.50;11,277.11,294.54,216.08,7.50;11,277.11,305.47,202.40,7.50;11,277.11,316.39,122.22,7.50;11,277.11,327.43,117.54,7.50;11,277.11,338.35,145.74,7.50;11,277.11,349.27,164.59,7.50;11,277.11,360.31,193.03,7.50;11,277.11,371.23,117.54,7.50;11,277.11,382.15,14.07,7.50;11,277.11,394.15,42.16,7.50;12,90.03,634.33,73.65,12.58" xml:id="b0">
	<monogr>
		<title level="m" coord="11,152.46,141.06,65.79,7.50;11,128.81,151.98,65.91,7.50;11,96.03,163.02,98.69,7.50;11,96.03,173.94,84.53,7.50;11,96.03,184.86,122.22,7.50;11,96.03,195.90,112.86,7.50;11,96.03,206.82,94.01,7.50;11,96.03,217.74,98.69,7.50;11,96.03,228.78,112.86,7.50;11,96.03,239.70,112.86,7.50;11,96.03,250.62,98.69,7.50;11,96.03,261.66,103.37,7.50;11,96.03,272.58,122.22,7.50;11,96.03,283.50,103.37,7.50;11,96.03,294.54,103.37,7.50;11,96.03,305.47,112.86,7.50;11,96.03,316.39,117.54,7.50;11,96.03,327.43,89.21,7.50;11,96.03,339.31,42.16,7.50;11,277.11,141.06,131.58,7.50;11,277.11,151.98,14.07,7.50;11,277.11,163.02,193.03,7.50;11,277.11,173.94,197.24,7.50;11,277.11,184.86,188.23,7.50;11,277.11,195.90,141.06,7.50;11,277.11,206.82,117.54,7.50;11,277.11,217.74,202.40,7.50;11,277.11,228.78,188.24,7.50;11,277.11,239.70,112.86,7.50;11,277.11,250.62,136.38,7.50;11,277.11,261.66,188.23,7.50;11,277.11,272.58,192.56,7.50;11,277.11,283.50,211.88,7.50;11,277.11,294.54,216.08,7.50;11,277.11,305.47,202.40,7.50;11,277.11,316.39,28.21,7.50;11,352.49,316.39,32.79,7.50;11,310.01,327.43,84.64,7.50;11,277.11,338.35,145.74,7.50;11,277.11,349.27,164.59,7.50;11,277.11,360.31,193.03,7.50;11,277.11,371.23,28.21,7.50">Rabie&quot; w=&quot;1&quot;/&gt; &lt;c id=&quot;allow&quot; w=&quot;1&quot;/&gt; &lt;c id=&quot;be&quot; w=&quot;1&quot;/&gt; &lt;c id=&quot;discussing&quot; w=&quot;1&quot;/&gt; &lt;c id=&quot;document&quot; w=&quot;3&quot;/&gt; &lt;c id=&quot;find&quot; w=&quot;1&quot;/&gt; &lt;c id=&quot;human&quot; w=&quot;5&quot;/&gt; &lt;c id=&quot;incident&quot; w=&quot;2&quot;/&gt; &lt;c id=&quot;interest&quot; w=&quot;1&quot;/&gt; &lt;c id=&quot;learn&quot; w=&quot;1&quot;/&gt; &lt;c id=&quot;method&quot; w=&quot;2&quot;/&gt; &lt;c id=&quot;prevention&quot; w=&quot;2&quot;/&gt; &lt;c id=&quot;rabies&quot; w=&quot;4&quot;/&gt; &lt;c id=&quot;reader&quot; w=&quot;1&quot;/&gt; &lt;c id=&quot;relevant&quot; w=&quot;1&quot;/&gt; &lt;c id=&quot;reporting&quot; w=&quot;2&quot;/&gt; &lt;c id=&quot;use&quot; w=&quot;1&quot;/&gt; &lt;/vector&gt; &lt;vector id=&quot;C250&quot; size=&quot;57&quot;&gt; ... &lt;!--Translation of id=&quot;reader&quot; w=&quot;1&quot; --&gt; &lt;c id=&quot;reader&quot; w=&quot;1&quot; untranslated=&quot;true&quot;/&gt; &lt;!--Translation of id=&quot;human&quot; w=&quot;5&quot; --&gt; &lt;c id=&quot;ЧЕЛОВЕЧЕСКИЙ&quot; w=&quot;2.5&quot;/&gt; &lt;c id=&quot;ЧЕЛОВЕК&quot; w=&quot;2.5&quot;/&gt; &lt;!--Translation of id=&quot;document&quot; w=&quot;3&quot; --&gt; &lt;c id=&quot;ПОДТВЕРЖДАТЬ_ДОКУМЕНТАМИ&quot; w=&quot;1&quot;/&gt; &lt;c id=&quot;ДОКУМЕНТ&quot; w=&quot;1&quot;/&gt; &lt;c id=&quot;СВИДЕТЕЛЬСТВО&quot; w=&quot;1&quot;/&gt; &lt;!--Translation of id=&quot;Rabie&quot; w=&quot;1&quot; --&gt; &lt;c id=&quot;Rabie&quot; w=&quot;1&quot; untranslated=&quot;true&quot;/&gt; &lt;!--Translation of id=&quot;prevention&quot; w=&quot;2&quot; --&gt; &lt;c id=&quot;prevention&quot; w=&quot;2&quot; untranslated=&quot;true&quot;/&gt; &lt;!--Translation of id=&quot;interest&quot; w=&quot;1&quot; --&gt; &lt;c id=</title>
		<imprint/>
	</monogr>
	<note>ВЫГОДА&quot; w=&quot;0.25&quot;/&gt; &lt;c id=&quot;ИНТЕРЕСОВАТЬ&quot; w=&quot;0.25&quot;/&gt; &lt;c id=&quot;ЗАИНТЕРЕСОВЫВАТЬ&quot; w=&quot;0.25&quot;/&gt; &lt;!--Translation of id=&quot;rabies&quot; w=&quot;4&quot; --&gt; &lt;c id=</note>
</biblStruct>

<biblStruct coords="12,105.51,659.16,407.58,8.78;12,105.51,671.16,337.37,8.78" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,369.31,659.16,143.78,8.78;12,105.51,671.16,145.19,8.78">Comparing weighting models for monolingual information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Gianni</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Claudio</forename><surname>Carpineto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Giovanni</forename><surname>Romano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,271.35,671.16,48.46,8.77">CLEF 2003</title>
		<meeting><address><addrLine>Trondheim, Norway</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,105.52,691.08,407.67,8.78;12,105.51,703.08,407.49,8.78;12,105.51,714.96,154.00,8.78;13,96.03,112.10,99.63,8.71;13,277.11,112.10,113.58,8.71;13,96.03,141.06,131.58,7.50;13,96.03,151.98,98.69,7.50;13,96.03,163.02,98.69,7.50;13,96.03,173.94,84.53,7.50;13,96.03,184.86,122.22,7.50;13,96.03,195.90,112.86,7.50;13,96.03,206.82,94.01,7.50;13,96.03,217.74,98.69,7.50;13,96.03,228.78,112.86,7.50;13,96.03,239.70,112.86,7.50;13,96.03,250.62,98.69,7.50;13,96.03,261.66,103.37,7.50;13,96.03,272.58,122.22,7.50;13,96.03,283.50,103.37,7.50;13,96.03,294.54,103.37,7.50;13,96.03,305.47,112.86,7.50;13,96.03,316.39,117.54,7.50;13,96.03,327.43,89.21,7.50;13,96.03,339.31,42.16,7.50;13,277.11,141.06,131.58,7.50;13,277.11,151.98,14.07,7.50;13,277.11,163.02,193.03,7.50;13,277.11,173.94,197.24,7.50;13,277.11,184.86,188.23,7.50;13,277.11,195.90,141.06,7.50;13,277.11,206.82,108.17,7.50;13,277.11,217.74,202.40,7.50;13,277.11,228.78,188.24,7.50;13,277.11,239.70,112.86,7.50;13,277.11,250.62,136.38,7.50;13,277.11,261.66,188.23,7.50;13,277.11,272.58,192.56,7.50;13,277.11,283.50,211.88,7.50;13,277.11,294.54,216.08,7.50;13,277.11,305.47,202.40,7.50;13,277.11,316.39,117.54,7.50;13,277.11,327.43,117.54,7.50;13,277.11,338.35,145.74,7.50;13,277.11,349.27,164.59,7.50;13,277.11,360.31,193.03,7.50;13,277.11,371.23,117.54,7.50;13,277.11,382.15,14.07,7.50;13,277.11,393.19,42.16,7.50;13,212.55,425.64,177.89,8.78" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,321.95,691.08,191.24,8.78;12,105.51,703.08,228.08,8.78">Probabilistic models of information retrieval based on measuring the divergence from randomness</title>
		<author>
			<persName coords=""><forename type="first">Gianni</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cornelis</forename><surname>Joost Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,128.81,151.98,65.91,7.50;13,96.03,163.02,98.69,7.50;13,96.03,173.94,84.53,7.50;13,96.03,184.86,122.22,7.50;13,96.03,195.90,112.86,7.50;13,96.03,206.82,94.01,7.50;13,96.03,217.74,98.69,7.50;13,96.03,228.78,112.86,7.50;13,96.03,239.70,112.86,7.50;13,96.03,250.62,98.69,7.50;13,96.03,261.66,103.37,7.50;13,96.03,272.58,122.22,7.50;13,96.03,283.50,103.37,7.50;13,96.03,294.54,103.37,7.50;13,96.03,305.47,112.86,7.50;13,96.03,316.39,117.54,7.50;13,96.03,327.43,89.21,7.50;13,96.03,339.31,42.16,7.50;13,277.11,141.06,122.21,7.50;13,277.11,163.02,193.03,7.50;13,277.11,173.94,197.24,7.50;13,277.11,184.86,188.23,7.50;13,277.11,195.90,141.06,7.50;13,277.11,206.82,108.17,7.50;13,277.11,217.74,202.40,7.50;13,277.11,228.78,188.24,7.50;13,277.11,239.70,112.86,7.50;13,277.11,250.62,136.38,7.50;13,277.11,261.66,188.23,7.50;13,277.11,272.58,192.56,7.50;13,277.11,283.50,211.88,7.50;13,277.11,294.54,216.08,7.50;13,277.11,305.47,202.40,7.50;13,277.11,316.39,28.21,7.50;13,352.49,316.39,28.11,7.50;13,310.01,327.43,70.59,7.50">&lt;!--Translation of id=&quot;reader&quot; w=&quot;1&quot; --&gt; &lt;c id=&quot;reader&quot; w=&quot;1&quot; untranslated=&quot;true&quot;/&gt; &lt;!--Translation of id=&quot;human&quot; w=&quot;5&quot; --&gt; &lt;c id=&quot;ЧЕЛОВЕЧЕСКИЙ&quot; w=&quot;2.5&quot;/&gt; &lt;c id=&quot;ЧЕЛОВЕК&quot; w=&quot;5&quot;/&gt; &lt;!--Translation of id=&quot;document&quot; w=&quot;3&quot; --&gt; &lt;c id=&quot;ПОДТВЕРЖДАТЬ_ДОКУМЕНТАМИ&quot; w=&quot;1&quot;/&gt; &lt;c id=&quot;ДОКУМЕНТ&quot; w=&quot;1&quot;/&gt; &lt;c id=&quot;СВИДЕТЕЛЬСТВО&quot; w=&quot;1&quot;/&gt; &lt;!--Translation of id=&quot;Rabie&quot; w=&quot;1&quot; --&gt; &lt;c id=&quot;Rabie&quot; w=&quot;1&quot; untranslated=&quot;true&quot;/&gt; &lt;!--Translation of id=&quot;prevention&quot; w=&quot;2&quot; --&gt; &lt;c id=&quot;prevention&quot; w=&quot;2&quot; untranslated=&quot;true&quot;/&gt; &lt;!--Translation of id=&quot;interest&quot; w=&quot;1&quot; --&gt; &lt;c id=</title>
		<imprint>
			<date type="published" when="2002-10">October 2002</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="357" to="389" />
		</imprint>
	</monogr>
	<note>ВЫГОДА&quot; w=&quot;0.25</note>
</biblStruct>

<biblStruct coords="13,105.52,467.52,407.51,8.78;13,105.51,479.40,407.47,8.78;13,105.51,491.40,92.06,8.78" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,304.83,467.52,208.20,8.78;13,105.51,479.40,402.88,8.78">Simple translations of monolingual queries expanded through an association thesaurus. x-iota ir system used for clips bilingual experiments</title>
		<author>
			<persName coords=""><forename type="first">Jean-Pierre</forename><surname>Chevallet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gilles</forename><surname>Serrasset</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,117.99,491.40,48.58,8.77">CLEF 2003</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,105.52,511.32,406.83,8.78" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,181.94,511.32,132.18,8.78">Overview of the okapi projects</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,322.59,511.32,112.11,8.77">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="7" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,105.52,531.24,407.50,8.78;13,105.51,543.24,296.18,8.78" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,354.71,531.24,158.31,8.78;13,105.51,543.24,144.90,8.78">Okapi at trec-7: Automatic ad hoc, filtering, vlc and interactive track</title>
		<author>
			<persName coords=""><forename type="first">Steve</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Micheline</forename><surname>Baulieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,271.35,543.24,98.39,8.77">Preceedings of TREC-7</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
