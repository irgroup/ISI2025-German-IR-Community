<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,134.93,116.95,345.48,12.62;1,193.18,134.89,229.00,12.62;1,209.13,152.82,197.09,12.62">Unsupervised Language Model Adaptation using Utterance-based Web Search for Clinical Speech Recognition</title>
				<funder ref="#_ebqMfmy #_Vy8Q7MH">
					<orgName type="full">German Federal Ministry of Education and Research</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,156.33,190.49,61.68,8.74"><forename type="first">Robert</forename><surname>Herms</surname></persName>
							<email>robert.herms@cs.tu-chemnitz.de</email>
							<affiliation key="aff0">
								<orgName type="department">Chair Media Informatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,228.57,190.49,63.13,8.74"><forename type="first">Daniel</forename><surname>Richter</surname></persName>
							<email>daniel.richter@cs.tu-chemnitz.de</email>
							<affiliation key="aff1">
								<orgName type="department">Junior Professorship Media Computing</orgName>
								<orgName type="institution">Technische Universität Chemnitz</orgName>
								<address>
									<postCode>09107</postCode>
									<settlement>Chemnitz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,302.25,190.49,70.43,8.74"><forename type="first">Maximilian</forename><surname>Eibl</surname></persName>
							<email>maximilian.eibl@cs.tu-chemnitz.de</email>
							<affiliation key="aff0">
								<orgName type="department">Chair Media Informatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,402.62,190.49,51.94,8.74"><forename type="first">Marc</forename><surname>Ritter</surname></persName>
							<email>marc.ritter@cs.tu-chemnitz.de</email>
							<affiliation key="aff1">
								<orgName type="department">Junior Professorship Media Computing</orgName>
								<orgName type="institution">Technische Universität Chemnitz</orgName>
								<address>
									<postCode>09107</postCode>
									<settlement>Chemnitz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,134.93,116.95,345.48,12.62;1,193.18,134.89,229.00,12.62;1,209.13,152.82,197.09,12.62">Unsupervised Language Model Adaptation using Utterance-based Web Search for Clinical Speech Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9616F2BCBC89C8F554BB26513A873380</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Speech recognition</term>
					<term>Language modeling</term>
					<term>Unsupervised adaptation</term>
					<term>Information retrieval</term>
					<term>Clinical texts</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this working notes paper we present our methodology in clinical speech recognition for the Task 1.a.1 of the CLEF eHealth Evaluation Lab 2015. The goal of this task is to minimize the worddetection errors. Our approach is based on the assumption that each spoken clinical document has its own context. Hence, the recognition system is adapted for each document separately. The proposed method performs two-pass decoding whereas the first transcript is processed to queries which are used for retrieving web resources as adaptation data to build a document-specific dictionary and language model. The second pass decodes the same document using the adapted dictionary and language model. The experimental results show a reduction of the insertion errors in comparison to the baseline system, but no improvement of the overall incorrectness percentage across all spoken documents.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In general, the creation of acceptable transcripts of spoken language requires high human intervention and remains time-as well as cost-intensive. Since manual generated transcriptions are a challenging task, especially for large and heterogeneous datasets, it is more appropriate to apply automatic speech recognition (ASR). In the medical domain, ASR supports a typical handover workflow as a first step by transforming verbal clinical information into electronic structured records. The CLEF eHealth Evaluation Lab 2015 <ref type="bibr" coords="1,354.77,573.43,10.52,8.74" target="#b0">[1]</ref> aims to ease patients and nurses in understanding and accessing eHealth information. The goal of Task 1.a <ref type="bibr" coords="1,134.77,597.34,10.52,8.74" target="#b1">[2]</ref> is to convert verbal nursing handover to free-form text documents, whereas the challenge of Task 1.a.1 is to minimize word-detection errors by addressing the correctness of the speech recognition engine itself.</p><p>In this connection, out-of-vocabulary (OOV) has a serious impact on ASR results. It necessarily requires the utilization of domain-specific language models (LMs) in order to cope with the huge amount of data and different topics. For this purpose, the adaptation of a generic LM with a more specific LM using weighted linear interpolation is a common way. Supervised LM adaptation is very costly for huge amount of data and different topics, since the generation of specific corpora takes a lot of time. A conclusive way is an unsupervised method, which takes the context of a situation into account. As described in <ref type="bibr" coords="2,430.25,167.81,9.96,8.74" target="#b2">[3]</ref>, it is not suitable for unsupervised adaptation to use the hypothesis of an ASR system as adaptation data. This is due to the fact, that automatic generated transcripts contain recognition errors and do not counteract the OOV problem. However, transcripts can be processed to queries and used in an information retrieval system, e.g., <ref type="bibr" coords="2,177.69,227.59,10.52,8.74" target="#b2">[3,</ref><ref type="bibr" coords="2,189.86,227.59,7.01,8.74" target="#b3">4]</ref>. Resources such as specific corpora or the web with HTML pages (e.g., <ref type="bibr" coords="2,159.55,239.54,7.75,8.74" target="#b3">[4]</ref><ref type="bibr" coords="2,167.30,239.54,3.87,8.74" target="#b4">[5]</ref><ref type="bibr" coords="2,171.17,239.54,7.75,8.74" target="#b5">[6]</ref>), RSS Feeds and Twitter (e.g., <ref type="bibr" coords="2,319.12,239.54,10.79,8.74" target="#b6">[7]</ref>) are very useful in order to obtain further textual data for the LM adaptation. Moreover, this enables to get new specific vocabulary for covering the OOV (names, brands, technical terms, etc.). Additional data especially from out-of-domain does not always lead to improvements (e.g., <ref type="bibr" coords="2,189.54,287.36,10.30,8.74" target="#b7">[8]</ref>). In contrast, domain-specific data is helpful to address certain topics. Hence, the work <ref type="bibr" coords="2,242.09,299.32,10.52,8.74" target="#b4">[5]</ref> proposed a complete unsupervised technique based on information retrieval methods to build a thematically coherent adaptation corpus using the web. However, in <ref type="bibr" coords="2,286.98,323.23,10.52,8.74" target="#b3">[4]</ref> was clarified that the application of topic specific LMs is not easy to handle for an out-of-the-box ASR system, especially, if the topic is very heterogeneous or the contents change dynamically.</p><p>In this working notes paper we present our methodology and the results we obtained in Task 1.a.1 of the CLEF eHealth Evaluation Lab 2015. Our approach is based on the assumption, that each spoken clinical document has its own context. Therefore, we suggest adapting ASR for each document separately. The proposed method uses a two-pass decoding strategy. First, the transcript of a document is generated by an ASR system. Keywords of the utterances are extracted and used as queries in order to retrieve web resources as adaptation data to build a document-specific dictionary and LM. Finally, re-decoding of the same document is performed using the adapted dictionary and LM. The developed system was already applied in the previous works <ref type="bibr" coords="2,399.96,466.95,10.52,8.74" target="#b8">[9]</ref> and <ref type="bibr" coords="2,433.17,466.95,14.61,8.74" target="#b9">[10]</ref>.</p><p>This Paper is organized as follows: In the next section we present our method for unsupervised language model adaptation in clinical speech recognition. In Section 3 we describe the applied dataset, the experimental setup, and the evaluation results. Finally, we conclude this paper in Section 4 and give some future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Adaptation Method</head><p>Our method works out-of-the-box with a two-pass decoding strategy. First, a transcript of utterances in the spoken document is generated by ASR. The segmentation of the transcript into several units is performed by the recognizer itself using long silences. Each segment ranges from a short statement to a whole sentence. The segments are processed and used as queries for retrieving adaptation data to build a document-specific dictionary and LM. The second pass of the recognizer decodes the same document using the adapted dictionary and LM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recognizer Web Search Keyword Extraction</head><p>Web Docs Text Normalizer</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adaptation Corpus</head><p>Fig. <ref type="figure" coords="3,154.40,169.77,4.13,7.89">1</ref>. Process chain for the retrieval of web-based adaptation data using the transcript of recognized speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Retrieval of Adaptation Data</head><p>As shown in Fig. <ref type="figure" coords="3,210.54,233.89,3.87,8.74">1</ref>, a transcribed segment generated by the ASR system is used for building a query in order to perform a web search. Since a segment often contain more words than useful for a web search query, especially for retrieving documents in a close context, the following steps are performed to limit their number:</p><p>1. Nouns, plural nouns and the corresponding adjectives are extracted to obtain the most meaningful words. 2. A pre-defined stop-word list is applied which is derived from the training data and contains unnecessary as well as recurring vocabulary (e.g., date and time specification) 3. If 2. yields more keywords than a predefined threshold, the sequence of words is split into several parts with almost the same number of words fulfilling the requirements and each of these parts is considered to be a separate query.</p><p>Otherwise there is only one query.</p><p>For each resulting query a web search is conducted. The amount of the retrieved web documents is combined and normalized before adding to the adaptation corpus. In detail, the pure articles of the retrieved web documents are extracted and special characters, acronyms and numbers are converted in order to be conform to the conventions of the pending adaptation process and the ASR system. These steps are performed for all transcribed segments of one spoken document and their normalized texts are accumulated to one corresponding adaptation corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dictionary and LM Adaptation</head><p>The accumulated adaptation corpus is used for modifying the base dictionary and the base LM as illustrated in Fig. <ref type="figure" coords="3,310.64,561.47,3.87,8.74" target="#fig_0">2</ref>. The pronunciation dictionary adaptation aims to enrich a base dictionary by new vocabulary coming from the adaptation corpus. For this purpose, the vocabulary of the corpus is extracted and compared to the base dictionary. The additional vocabulary is phonetically transcribed by a grapheme-to-phoneme (G2P) decoder and combined into a temporary dictionary. Finally, the temporary and the base dictionary are merged to an adapted dictionary.</p><p>The LM adaptation is performed by a weighted linear interpolation of the temporary and the base LM. The temporary LM is trained by means of the adaptation corpus, whereas the base LM is a more general model trained on topic-independent data collections. Finally, the vocabulary of the resulting model is a superset of the vocabulary of both, the temporary and the base LM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>Before describing the details of the experimental setup, the used dataset is introduced and some observations on conducted preliminary experiments are stated. Afterwards we discuss our experimental intermediate as well as final results of the evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>In this work the NICTA Synthetic Nursing Handover Data dataset <ref type="bibr" coords="4,431.92,501.51,15.50,8.74" target="#b10">[11]</ref> is used which was created at NICTA in 2012-2014 for clinical speech recognition and information extraction related to nursing shift-change handover. The training set as well as the test set consist of 100 written, free-form text documents and the corresponding recorded audio files spoken by an Australian registered nurse with over twelve years of working experience. The text documents of the test set were not released for evaluation purposes. Furthermore, this dataset includes recordings lasting about half an hour of her reading an excerpt of "The Final Odyssey" as initialization data for speech recognition engines. In a preliminary experiment the ASR output of the training set was compared to the written, free-form text documents. This exposed some common errors, which are partially already mentioned in the task description. Further investigation revealed, that there are some abbreviations used instead of the correctly spelled words. However, these words are verbalised correct by the nurse in the provided audio files. Therefore, a list of usual misspellings was created, which should be used instead of the correct words generated by the standard configuration of the ASR system. For instance, only 26 appearances of the correctly spelled word "years" were counted, but 53 appearances of the abbreviation "yrs". Using the abbreviation should lead to less substitutions counted for the final evaluation metric. More detected substitutions are shown in Table <ref type="table" coords="5,442.11,366.39,3.87,8.74" target="#tab_0">1</ref>. Beside these misspellings in the text documents, there are some misspeaks and following corrections by the nurse as well as some filler words like "ehm". Another observation is the usage of numerals and numbers in the written, free-form text documents. Only 22 numerals from "one" to "eight" are used but 266 numbers up to three digits (more than half of this with only one digit) were found. Hence, always using numbers instead of numerals seems promising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Setup</head><p>ASR was performed using the engine of the open-source framework sphinx-4 <ref type="bibr" coords="5,134.77,501.70,14.61,8.74" target="#b11">[12]</ref>. As a basic configuration, we used already existing components. We applied the acoustic model "HUB4" (http://www.speech.cs.cmu.edu/sphinx/models/), which has been trained using 140 hours of 1996 and 1997 hub4 training data. It includes 3-state within-word and cross-word triphone Hidden-Markov-Models with 8 Gaussian mixture models. We performed maximum a posteriori (MAP) adaptation by using the initialization data of the training set to update the parameters of the acoustic model in order to better match the observed data. Next, we used the pronunciation dictionary "cmudict.0.7a SPHINX 40" (https://svn.code.sf.net/p/cmusphinx/code/trunk/cmudict/sphinxdict/), which comprises 133k words and the corresponding phonetic transcription. We modified the dictionary concerning the notation of the clinical reports, for instance, adding new vocabulary or replacing words with abbreviations. Moreover, we assigned some vocabulary to filler words to avoid misinterpretation caused by fillers (e.g., "ah" or "ehm"). As a generic LM we used the "US English Generic</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>The intermediate results of our adaptation method are relevant for further processing steps and consequently for the final results. Table <ref type="table" coords="6,385.91,501.70,4.98,8.74" target="#tab_1">2</ref> gives an overview of the mean values concerning the retrieval process across all spoken documents. Comparing the numbers of segments per spoken documents leads to the conclusion, that the speech recognizer was able to detect much more pauses in the training set than in the test set. Fewer segments in the test set also lead to fewer but longer queries built by the system, as seen in line two and three of Table <ref type="table" coords="6,162.62,573.43,3.87,8.74" target="#tab_1">2</ref>. As the number of web documents retrieved per query is almost equal for both datasets, the number of web documents per spoken document is also much higher for the training set than for the test set.</p><p>The number of tokens per web document (after normalization) are quite similar. Hence, the differences in the resulting adaptation corpora for the training and the independent test set, as shown in Table <ref type="table" coords="6,350.75,633.20,3.87,8.74" target="#tab_2">3</ref>, can only be traced back to the differences in the number of segments per spoken document. The statistics in Table <ref type="table" coords="6,172.55,657.11,4.98,8.74" target="#tab_2">3</ref> indicate that more tokens in the adaptation corpus lead to more types and also to more new types, which were used to extend the dictionary of the ASR system for the second pass. A series of speech recognition experiments was carried out using the two different interpolation weights λ=0.8 and λ=0.9 for the proposed adaptation method on the training set and the independent test set. These weights imply a higher preference of the base LM which was designed for the clinical freeform text documents. The results are illustrated in Table <ref type="table" coords="7,384.04,476.40,3.87,8.74" target="#tab_3">4</ref>. The baseline results were achieved by Dragon Medical 11.0 which was trained on the initialization data and applied with the vocabulary of nursing. In general, it can be seen that for both datasets our system had more substitution errors and deleted some more words than the baseline system. We considered the specific notation of the written, free-form text documents such as abbreviations or fillers, which leads to less inserted words than the baseline system. Compared to the baseline, the mean value of the incorrectness percentage across all documents in the training set was improved by our system with 2.1% (λ=0.9). The reason for that is the consideration of the specific notation of the written, free-form text documents and the adaptation of the generic LM using the free-form text documents of the training set which increases the probability of recognizing the correct words.</p><p>The performance measurements of our system on the test set in comparison to the training set concerning the mean of the incorrectness percentage across all documents show similar results with a decrease of 3.6% (λ=0.9). Our system generated many substitution errors on the test set with 36.6% and a difference of 14.0% to the baseline that is crucial for the overall incorrectness percentage. We achieved a mean value of 52.1% and consequently 12.6% over the baseline. However, we could achieve a reduction of the insertion errors with 5.1% (λ=0.8) and 5.0% (λ=0.9). All in all, the evaluation on the test set shows that our system did not improve the baseline mean incorrectness percentage. The configuration with the interpolation weight λ=0.9 was just slightly better than the lower one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>We presented a method for unsupervised language model adaptation in automatic speech recognition for the Task 1.a.1 of the CLEF eHealth Evaluation Lab 2015. Our approach is based on the assumption, that each spoken clinical document has its own context. Hence, the recognition system is adapted for each document separately. The method uses a two-pass decoding strategy, whereas the first transcript is processed to queries, which are used for retrieving web resources as adaptation data to build a document-specific dictionary and language model. The second pass of the speech recognizer decodes the same document using the adapted dictionary and language model. The experimental results on the test set showed a reduction of the insertion errors in comparison to the baseline system. We achieved a mean value of 52.1% incorrectness across all documents. All in all, we did not improve the baseline incorrectness percentage, since our system produces more substitution errors and deleted some more words. The configuration of our method with the interpolation weight λ=0.9 was just slightly better than λ=0.8. However, further improvements could be achieved by a more sophisticated selection of the retrieved adaptation data. For instance, a model for disease classification in text corpora could be helpful to obtain only specific adaptation data for the corresponding spoken document. Moreover, it would be interesting to use further resources from web, like Twitter and RSS Feeds. For future work, the investigation of phonetics for accented speech and consequently the application of pronunciation modeling should enhance the performance of the recognition system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,134.77,287.34,345.83,7.89;4,134.77,298.32,128.83,7.86"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. Procedure of the pronunciation dictionary and LM adaptation based on the accumulated adaptation corpus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,134.77,116.91,345.83,151.60"><head>Table 1 .</head><label>1</label><figDesc>List of typical abbreviations in free-form text documents of the training set which were predominantly used instead of the correct spelling.</figDesc><table coords="5,226.91,148.67,161.53,119.84"><row><cell>Correct spelling</cell><cell>Used abbreviation</cell></row><row><cell>years</cell><cell>yrs</cell></row><row><cell>hours</cell><cell>hrs</cell></row><row><cell>doctor</cell><cell>dr</cell></row><row><cell>antibiotics</cell><cell>abs</cell></row><row><cell cols="2">arteriovenous fistula avf</cell></row><row><cell>blood pressure</cell><cell>bp</cell></row><row><cell>blood pressures</cell><cell>bps</cell></row><row><cell>hypertension</cell><cell>hpn</cell></row><row><cell cols="2">level of consciousness loc</cell></row><row><cell>prednisolone</cell><cell>pred</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,134.77,116.91,345.82,109.76"><head>Table 2 .</head><label>2</label><figDesc>Mean values concerning the retrieval process of the adaptation corpus across all spoken documents in the training set and the independent test set.</figDesc><table coords="7,355.78,148.67,74.19,7.86"><row><cell>Train set Test set</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,134.77,253.39,345.83,109.36"><head>Table 3 .</head><label>3</label><figDesc>Statistics of the resulting adaptation corpus across all spoken documents in the training set and the independent test set.</figDesc><table coords="7,141.35,285.15,332.66,77.60"><row><cell></cell><cell></cell><cell>Train set</cell><cell></cell><cell></cell><cell>Test set</cell><cell></cell></row><row><cell></cell><cell>Tokens</cell><cell cols="2">Types New Types</cell><cell>Tokens</cell><cell cols="2">Types New Types</cell></row><row><cell>Min</cell><cell>18.0k</cell><cell>3.9k</cell><cell>0.3k</cell><cell>31.6k</cell><cell>5.1k</cell><cell>1.0k</cell></row><row><cell>Max</cell><cell>1,074.7k</cell><cell>34.7k</cell><cell>14.9k</cell><cell>629.6k</cell><cell>25.0k</cell><cell>9.8k</cell></row><row><cell>Mean</cell><cell>441.9k</cell><cell>20.4k</cell><cell>7.5k</cell><cell>315.9k</cell><cell>17.4k</cell><cell>5.9k</cell></row><row><cell>Median</cell><cell>394.1k</cell><cell>20.0k</cell><cell>7.4k</cell><cell>299.3k</cell><cell>17.3k</cell><cell>5.8k</cell></row><row><cell>SD</cell><cell>248.7k</cell><cell>6.1k</cell><cell>3.1k</cell><cell>155.8k</cell><cell>4.9k</cell><cell>2.3k</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,134.77,116.91,345.82,192.45"><head>Table 4 .</head><label>4</label><figDesc>Performance measurement on the training set and the independent test set using different interpolation weights (λ) of the proposed method for unsupervised language model adaptation.</figDesc><table coords="8,160.24,159.62,294.88,149.73"><row><cell></cell><cell cols="2">Train set</cell><cell></cell><cell cols="2">Test set</cell><cell></cell></row><row><cell></cell><cell cols="6">Baseline λ=0.8 λ=0.9 Baseline λ=0.8 λ=0.9</cell></row><row><cell>% Correct words</cell><cell>72.3</cell><cell>64.6</cell><cell>65.6</cell><cell>73.1</cell><cell>53.7</cell><cell>54.3</cell></row><row><cell>% Substituted words</cell><cell>24.1</cell><cell>28.6</cell><cell>27.9</cell><cell>22.6</cell><cell>36.7</cell><cell>36.6</cell></row><row><cell>% Deleted words</cell><cell>3.6</cell><cell>6.8</cell><cell>6.6</cell><cell>4.3</cell><cell>9.6</cell><cell>9.1</cell></row><row><cell>% Inserted words</cell><cell>28.2</cell><cell>19.5</cell><cell>19.6</cell><cell>11.6</cell><cell>6.5</cell><cell>6.6</cell></row><row><cell>% Incorrect words</cell><cell>55.9</cell><cell>54.9</cell><cell>54.1</cell><cell>38.5</cell><cell>52.8</cell><cell>52.3</cell></row><row><cell cols="5">Incorrectness percentage across all documents</cell><cell></cell><cell></cell></row><row><cell>Min</cell><cell>30.2</cell><cell>22.0</cell><cell>22.0</cell><cell>20.7</cell><cell>26.2</cell><cell>26.2</cell></row><row><cell>Max</cell><cell cols="3">137.5 142.5 142.5</cell><cell>59.1</cell><cell>92.0</cell><cell>92.0</cell></row><row><cell>Mean</cell><cell>57.8</cell><cell>56.5</cell><cell>55.7</cell><cell>39.5</cell><cell>52.6</cell><cell>52.1</cell></row><row><cell>Median</cell><cell>55.5</cell><cell>52.9</cell><cell>53.0</cell><cell>39.1</cell><cell>51.7</cell><cell>51.6</cell></row><row><cell>SD</cell><cell>17.0</cell><cell>18.6</cell><cell>18.4</cell><cell>9.8</cell><cell>13.1</cell><cell>12.9</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This work was partially funded by the <rs type="funder">German Federal Ministry of Education and Research</rs> within the project <rs type="projectName">MACeLot</rs> (funding code <rs type="grantNumber">16SV7260</rs>) and the program of <rs type="programName">Entrepreneurial Regions InnoProfile-Transfer in the project group localizeIT</rs> (funding code <rs type="grantNumber">03IP608X</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_ebqMfmy">
					<idno type="grant-number">16SV7260</idno>
					<orgName type="project" subtype="full">MACeLot</orgName>
					<orgName type="program" subtype="full">Entrepreneurial Regions InnoProfile-Transfer in the project group localizeIT</orgName>
				</org>
				<org type="funding" xml:id="_Vy8Q7MH">
					<idno type="grant-number">03IP608X</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,138.35,291.66,342.24,7.86;9,146.91,302.62,333.68,7.86;9,146.91,313.57,333.68,7.86;9,146.91,324.53,132.01,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,211.27,302.62,194.28,7.86">Overview of the CLEF eHealth Evaluation Lab</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Hanlen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Névéol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Grouin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Palotti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,433.82,302.62,46.77,7.86;9,146.91,313.57,212.45,7.86">CLEF 2015 -6th Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="9,367.93,313.57,112.66,7.86;9,146.91,324.53,56.67,7.86">Lecture Notes in Computer Science (LNCS</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,334.77,342.24,7.86;9,146.91,345.73,333.68,7.86;9,146.91,356.69,305.50,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,420.46,334.77,60.13,7.86;9,146.91,345.73,256.48,7.86">Task 1a of the CLEF eHealth Evaluation Lab 2015: Clinical speech recognition</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Hanlen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,410.31,345.73,70.28,7.86;9,146.91,356.69,272.02,7.86">Working Notes of the CLEF 2015 -6th Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,366.93,342.24,7.86;9,146.91,377.89,333.68,7.86;9,146.91,388.85,234.04,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,348.54,366.93,132.05,7.86;9,146.91,377.89,59.92,7.86">Dynamic language modeling for broadcast news</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lamel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-L</forename><surname>Gauvain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Adda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,228.81,377.89,247.69,7.86">8th International Conference on Spoken Language Processing</title>
		<meeting><address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>INTERSPEECH</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="997" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,399.09,342.24,7.86;9,146.91,410.05,333.68,7.86;9,146.91,421.01,333.68,7.86;9,146.91,431.97,25.60,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,435.01,399.09,45.58,7.86;9,146.91,410.05,234.59,7.86">Vocabulary and language model adaptation using just one speech file</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Thambiratnam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,402.60,410.05,77.99,7.86;9,146.91,421.01,226.34,7.86">IEEE International Conference on Acoustics Speech and Signal Processing</title>
		<imprint>
			<publisher>ICASSP</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="5410" to="5413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,442.21,342.24,7.86;9,146.91,453.17,333.68,7.86;9,146.91,464.13,218.73,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,304.42,442.21,176.18,7.86;9,146.91,453.17,101.69,7.86">An unsupervised web-based topic language model adaptation method</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Lecorvé</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sebillot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,269.86,453.17,210.74,7.86;9,146.91,464.13,86.52,7.86">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>ICASSP</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="5081" to="5084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,474.37,342.24,7.86;9,146.91,485.33,333.68,7.86;9,146.91,496.28,303.57,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,320.07,474.37,160.52,7.86;9,146.91,485.33,189.02,7.86">Language model adaptation using www documents obtained by utterance-based queries</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tsiartas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Georgiou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,356.33,485.33,124.27,7.86;9,146.91,496.28,171.36,7.86">IEEE International Conference on Acoustics Speech and Signal Processing</title>
		<imprint>
			<publisher>ICASSP</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="5406" to="5409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,506.52,342.25,7.86;9,146.91,517.48,333.68,7.86;9,146.91,528.44,333.68,7.86;9,146.91,539.40,219.78,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,330.29,506.52,150.31,7.86;9,146.91,517.48,298.32,7.86">Unsupervised Language Model Adaptation for Automatic Speech Recognition of Broadcast News Using Web 2.0</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Schlippe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,464.71,517.48,15.88,7.86;9,146.91,528.44,329.62,7.86">The 14th Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>Lyon, France</addrLine></address></meeting>
		<imprint>
			<publisher>INTERSPEECH</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2698" to="2702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,549.64,342.24,7.86;9,146.91,560.60,333.68,7.86;9,146.91,571.56,25.60,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,242.38,549.64,238.21,7.86;9,146.91,560.60,95.50,7.86">Relevance weighting for combining multi-domain data for ngram language modeling</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ostendorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,249.19,560.60,119.10,7.86">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="267" to="282" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,581.80,342.24,7.86;9,146.91,592.76,333.68,7.86;9,146.91,603.72,333.67,7.86;9,146.91,614.68,256.23,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,361.71,581.80,118.88,7.86;9,146.91,592.76,333.68,7.86;9,146.91,603.72,24.81,7.86">Improving Spoken Document Retrieval by Unsupervised Language Model Adaptation Using Utterance-Based Web Search</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Herms</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wilhelm-Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Eibl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,195.41,603.72,285.18,7.86;9,146.91,614.68,44.67,7.86">15th Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>INTERSPEECH</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1430" to="1433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,624.92,337.97,7.86;9,146.91,635.88,333.68,7.86;9,146.91,646.84,333.68,7.86;9,146.91,657.79,62.50,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,366.74,624.92,113.85,7.86;9,146.91,635.88,273.61,7.86">Improving Transcript-Based Video Retrieval Using Unsupervised Language Model Adaptation</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wilhelm-Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Herms</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Eibl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,445.76,635.88,34.83,7.86;9,146.91,646.84,280.08,7.86">Information Access Evaluation. Multilinguality, Multimodality, and Interaction</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="110" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,120.67,337.98,7.86;10,146.91,131.63,333.68,7.86;10,146.91,142.59,111.43,7.86" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="10,359.93,120.67,120.66,7.86;10,146.91,131.63,303.79,7.86">Benchmarking clinical speech recognition and information extraction: New data, methods, and evaluations</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Hanlen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ferraro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
		<respStmt>
			<orgName>JMIR Medical Informatics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,153.55,337.98,7.86;10,146.91,164.51,333.68,7.86;10,146.91,175.46,270.13,7.86" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="10,159.46,164.51,276.36,7.86">Sphinx-4: A Flexible Open Source Framework for Speech Recognition</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Lamere</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Gouvea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Woelfel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Sun Microsystems, Inc</publisher>
			<pubPlace>Mountain View, CA, USA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="10,142.62,186.42,337.98,7.86;10,146.91,197.38,333.68,7.86;10,146.91,208.34,89.41,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,350.37,186.42,130.23,7.86;10,146.91,197.38,28.67,7.86">SRILM at sixteen: Update and outlook</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Abrash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,195.15,197.38,285.45,7.86;10,146.91,208.34,38.06,7.86">Proceedings of IEEE Automatic Speech Recognition and Understanding Workshop</title>
		<meeting>IEEE Automatic Speech Recognition and Understanding Workshop</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,219.30,337.98,7.86;10,146.91,230.26,202.29,7.86" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="10,206.72,219.30,167.06,7.86">Phonetisaurus: A wfst-driven phoneticizer</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Novak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Tokyo Institute of Technology</publisher>
			<biblScope unit="page" from="221" to="222" />
		</imprint>
		<respStmt>
			<orgName>The University of Tokyo</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,241.22,337.97,7.86;10,146.91,252.18,333.68,7.86;10,146.91,263.14,211.97,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,259.51,241.22,221.08,7.86;10,146.91,252.18,69.32,7.86">Fast Exact Inference with a Factored Model for Natural Language Parsing</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,235.68,252.18,244.91,7.86;10,146.91,263.14,20.48,7.86">Advances in Neural Information Processing Systems 15 (NIPS 2002)</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
