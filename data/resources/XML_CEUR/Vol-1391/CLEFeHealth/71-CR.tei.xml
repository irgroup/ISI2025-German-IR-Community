<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,133.82,152.67,327.60,12.64;1,242.57,170.67,110.09,12.64">Automatic Clinical Speech Recognition for CLEF 2015 eHealth Challenge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,179.45,210.18,60.46,8.96"><forename type="first">Thoai</forename><forename type="middle">Man</forename><surname>Luu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Canberra</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,247.05,210.18,47.65,8.96"><forename type="first">Robert</forename><surname>Phan</surname></persName>
							<email>robertphan10s62@yahoo.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Canberra</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,301.83,210.18,53.97,8.96"><forename type="first">Rachel</forename><surname>Davey</surname></persName>
							<email>rachel.davey@canberra.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Canberra</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,363.10,210.18,52.46,8.96"><forename type="first">Girija</forename><surname>Chetty</surname></persName>
							<email>girija.chetty@canberra.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Canberra</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,133.82,152.67,327.60,12.64;1,242.57,170.67,110.09,12.64">Automatic Clinical Speech Recognition for CLEF 2015 eHealth Challenge</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D7C816D203138D7829BFC3DB1720AB41</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this working notes report/paper, we describe the details of two submissions for CLEF 2015 eHealth challenge for Task 1a, with details of methods and tools developed for automatic speech recognition of NICTA synthetic nursing handover dataset. The first method involves a novel zero-resource approach based on unsupervised acoustic only modeling of speech involving word discovery, and the second method is based on combination of acoustic, language, grammar and dictionary models, using well known open source speech recognition toolkit from CMU, the CMU Sphinx <ref type="bibr" coords="1,147.70,372.02,11.50,8.10" target="#b6">[7]</ref>. The experimental evaluation of the two methods was done on Challenge dataset (NICTA synthetic nursing handover dataset).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Fluent information flow is important in any information-intensive area of decision making, but critical in healthcare. Clinicians are responsible for making decisions with even life-and-death impact on their patients' lives. The flow is defined as links, channels, contact, or communication to a pertinent person or people in the organisation <ref type="bibr" coords="1,143.39,487.32,10.91,8.96" target="#b0">[1,</ref><ref type="bibr" coords="1,157.58,487.32,7.53,8.96" target="#b1">2,</ref><ref type="bibr" coords="1,168.23,487.32,7.29,8.96" target="#b2">3]</ref>. In Australian healthcare, failures in this flow are associated with over one tenth of preventable adverse events <ref type="bibr" coords="1,288.19,499.32,10.98,8.96" target="#b0">[1,</ref><ref type="bibr" coords="1,302.21,499.32,7.52,8.96" target="#b1">2,</ref><ref type="bibr" coords="1,312.73,499.32,7.27,8.96" target="#b2">3]</ref>. Failures in the flow are tangible in clinical handover, that is, when a clinician is transferring professional responsibility and accountability, for example, at shift change <ref type="bibr" coords="1,326.15,523.32,10.79,8.96" target="#b2">[3]</ref>. Regardless of verbal handover being accurate and comprehensive, anything from two-thirds to all of this information is lost after three to five shifts if no notes are taken or they are taken by hand <ref type="bibr" coords="1,433.95,547.33,10.87,8.96" target="#b0">[1,</ref><ref type="bibr" coords="1,447.35,547.33,7.52,8.96" target="#b1">2,</ref><ref type="bibr" coords="1,457.29,547.33,7.40,8.96" target="#b2">3]</ref>. Nursing 'handover' in the clinical context involves the transfer of information, professional responsibility and accountability for patient quality care and safety from one clinical team to another either temporarily or permanently <ref type="bibr" coords="1,359.54,583.33,10.78,8.96" target="#b3">[4]</ref>. With changes in working hours and shifts of clinical teams (doctors, nurses and registrars in health care system), and an increasing demand for flexible work practices, the need for mechanisms to support effective and efficient handover processes for transferring information, responsibility, accountability and patient safety has become recognised as increasingly important for the delivery of high quality health care <ref type="bibr" coords="1,391.41,643.34,10.83,8.96" target="#b4">[5]</ref>. Clinical handover has been identified as a high risk scenario for patient safety with dangers of discontinuity of care, medical errors, adverse events and the potential for legal claims of malpractice <ref type="bibr" coords="1,169.12,679.35,12.11,8.96" target="#b4">[5]</ref>.</p><p>In general, implementation of ICTs in health care, to improve quality and safety has achieved mixed results. While some studies have demonstrated significant benefits and improvements in patient care, others have either met with mixed success or failed to generate their forecasted benefits <ref type="bibr" coords="2,275.33,186.18,10.68,8.96" target="#b5">[6]</ref>. Given strong advocacy through guidelines <ref type="bibr" coords="2,124.70,198.18,11.72,8.96" target="#b6">[7]</ref> and the vast amount of resources and funding which have been allocated for implementation of electronic solutions to health care, there is an urgent need to generate a better understanding of the effect of the implementation of ICTs in health care. One of the reason for such mixed and suboptimal outcomes could be due to complexity of clinical handover processes, characterized with highly unstructured information flows(free text from nursing handover notes or those transcribed from speech recognisers, for example), and inability of existing technologies and tools in making sense of such ill structured or unstructured data. This could be due to limitations of existing speech recognition technologies for instance, which act as front ends in automatic transcription of bed side clinical notes to text, and their vulnerability to noisy clinical environments and sensitivity to accent and dialect variations, leading to errors getting cascaded in subsequent stages of information extraction. CLEF eHealth Challenge Task 1a focused on addressing the short comings of existing clinical speech recognition systems by providing an open source challenge data set developed by authors in <ref type="bibr" coords="2,124.70,366.21,10.66,8.96" target="#b2">[3]</ref>, and provided an opportunity for researchers and practitioners by soliciting submissions on suitable approaches and methods to this challenge task.</p><p>In this paper, we present two methods we have developed and submitted to this challenge (CLEF eHealth 2015 evaluation challenge task (Task 1a)). The details of each method used and outcomes from the experimental trials are described in detail in next few Sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>Method I : Zero Resource Unsupervised Acoustic Modelling (Team UC_submission 1)</p><p>For this method (Team UC_submission 1), we used a novel approach based on zero resource unsupervised acoustic modelling technique. Zero resource speech technologies operate without the expert provided linguistic knowledge that standard recognition systems rely on-transcribed speech, language models, and pronunciation dictionaries. They are motivated by biologically inspired infant learning modes, and are suitable for less resourced contexts. As the challenge dataset comprised of non-native English language speaker recordings, with abbreviations and terms from clinical settings, traditional resources for training in terms of phonetic transcriptions, dictionaries and grammars for this context are scarce, and speech recognizer cannot perform well in this scenario. A robust zero-resource system must instead discover this linguistic knowledge from speech audio automatically.</p><p>The system for this approach consists of acoustic feature extraction and segmentation module, clustering module and word discovery module. Here, the acoustic similarities between multiple acoustic tokens of the same words or word like segments are exploited to perform recognition. Although, the performance of this method currently falls short of capabilities of the performance benchmarks provided by the challenge, the value of this algorithm is its potential to serve as a computational model in two research directions. First, this method may lead to a speech recognition approach that is fundamentally liberated from the extensive resources needed to perform automatic speech recognition, in terms of language models and pronunciation dictionaries. Second, it can lead to an approach for computational modelling of language acquisition that takes actual speech signal and is able to discover words as "evolving" properties from raw input. The motivation behind using this approach for discovering words from the raw speech signal is drawn from evolving speech recognition capabilities of babies and young infants, who can detect words from continuous speech. Psycholinguistic research <ref type="bibr" coords="3,454.55,268.65,15.90,8.96" target="#b8">[10,</ref><ref type="bibr" coords="3,124.70,280.17,12.55,8.96" target="#b9">11,</ref><ref type="bibr" coords="3,140.50,280.17,12.55,8.96" target="#b10">12,</ref><ref type="bibr" coords="3,156.17,280.17,12.55,8.96" target="#b11">13,</ref><ref type="bibr" coords="3,171.97,280.17,13.27,8.96" target="#b12">14]</ref> shows that babies can use the statistical correspondence of sound sequences as a cue for word segmentation. Also, the techniques that learn to decode speech without an upfront specified lexicon and phone models are interesting for recognizing speech outside of the vocabulary (OOV), such as in clinical domain, where there are several words with clinical meanings and abbreviations. For these scenarios -use of existing resources such as language models and pronunciation dictionaries will be a mismatch, and might radically reduce the speech recognizer performance. Hence, it is of considerable interest to investigate recognition approaches that circumvent the need for a priori defined lexicon. The focus for this method hence was to discover the words and word-like speech fragment by combining raw speech signals, and additional abstract representations of this speech signal that can model statistical co-occurrence information, by extracting repetitive structure within the speech fragment. For this we exploit two types of evolving patterns in the speech, the statistical properties of repetitive structure within the speech modality to hypothesize speech fragments or segments and their labelling, and cross-modal associations between the speech segments to hypothesize words, which can evolve when more and more input has been processed to represent the word correctly. The method consists of three modules, and instead of employing any phonetic recognizers to transcribe speech fragments in terms of phone sequences, we do a bootstrap aggregation with abstract representations to improve the speech transcriber performance.</p><p>As the speech signal gets transformed from one module to the next, it gets more symbolic in nature. The first module consists of automatic feature extraction, followed by a data driven boundary segmentation of speech segments at sentence level. The output of this module is a set of feature vectors and hypothesized speech segment boundaries. Module 2 is a clustering module, which reads the sentence feature vector segments and performs a k-means clustering, and gives label sequence for each segment. The third and final module implements the word discovery algorithm, using the sentence and hypothesized labels, and abstract tags representing presence of a word in that utterance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Module 1: Feature Extraction and Boundary Segmentation Module</head><p>In this module, audio file is down sampled to 16 kHz, and each speech frame is obtained by windowing the speech signal with 32 milliseconds windows (e.g. 512 sam-ple points for 16 kHz files) with 25 % percent overlap between consecutive frames. Cepstral mean substraction is then performed to normalize the frames, different acoustic features are extracted, including mel frequency cepstral coefficients (MFFCs), log energy, delta and delta-delta features. A total of 39 features are extracted from each frame comprising 12 MFCCs, 1 log-energy, 12 delta, and 12 delta-delta features <ref type="bibr" coords="4,158.73,210.18,15.92,8.96" target="#b8">[10,</ref><ref type="bibr" coords="4,177.18,210.18,11.84,8.96" target="#b9">11]</ref>. The distance between two frames, f 1 and f 2 was obtained by</p><formula xml:id="formula_0" coords="4,160.10,242.80,259.57,26.40">ùëë(ùëì 1 , ùëì 2 ) = cos -1 ( ùëì 1 ùë° ùëì 2 ‚àöùëì 1 ùë° ùëì 1 ùëì 2 ùë° ùëì 2 ‚ÅÑ )<label>(1)</label></formula><p>where | ùë° indicates transpose of the feature vector.</p><p>A high similarity or correlation corresponds to a small distance 'd' and vice versa.</p><p>Next, by using sliding window, we search the segment boundaries, where the boundary is hypothesized if the distance function that measures the difference between the average of feature vectors before the boundary and after the boundary attains a local maximum above a certain threshold. We use a window of 2 frames to either side of the boundary. And, with log(E) as the weighing factor, the criterion for detecting the boundary is:</p><formula xml:id="formula_1" coords="4,160.10,421.02,259.57,17.88">log(ùê∏) . ùëë( ùëì ùëñ-2 +ùëì ùëñ-1 2 , ùëì ùëñ+2 +ùëì ùëñ+1 2 ) &gt; ùõø (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Module 2: Clustering Module</head><p>This module takes as input, the segments from module 1, fits a Gaussian model to each segment, and clusters different segment models using k-means clustering algorithm . In this clustering module, the distance between two segment models S 1 and S 2 is defined similar to equation (1). For a better tractability, clustering of segment models is not applied to complete set, but first 10 utterances were first processed and in subsequent steps, more segments, in increments of 10 utterances were added and clusters updated until all sentences/segments in the data set were included. The output of this module is a set of unique labels assigned to each cluster. This module is for word discovery, and works by taking as input the wave files, in combination with the sequence of labels from the clustering module, and abstract tags, shown in Table <ref type="table" coords="5,215.70,434.27,3.71,8.96" target="#tab_0">1</ref>. The word discovery algorithm for this module involves a DTW (Dynamic Time Warping) algorithm, where the likelihood of two utterance sharing a common word is estimated using a DTW on two label sequences, with the assumption that the audio segment plus abstract tags are available as a list (Table <ref type="table" coords="5,459.51,470.27,3.59,8.96" target="#tab_0">1</ref>).</p><p>The word discovery algorithm works as follows.</p><p>1. New utterance is selected a. Two empty sets A match and A no_match are initialized. b. The new utterance is compared with all previously observed utterances using DTW algorithm on all corresponding label sequences. c. On the best path found by DTW, best-matching sub sequence is found. d. If both utterances share the same abstract tag, then this bestmatching sub sequence is put in A match, otherwise in A no_match. 2. All items in these sets are sorted according to their occurrence, 3. From the A match, N-best utterances are selected, that do not occur in A no_match. <ref type="bibr" coords="5,142.70,614.30,3.77,8.96" target="#b3">4</ref>. Repeat from Step 1 again. The advantage of this simple word discovery algorithm is, that it is able to bootstrap from the speech signal itself without using any predefined lexical knowledge or phone models. As the word discovery module consists of a cascade of intertwined stages, the evolution of correct word discovery improves incrementally with more data, better label sequence information from clustering module and availability of abstract tags indicating the presence of a word. The same distance measure is used in both module 2 and module 3, and the same DTW principle is used to define distances between segments and to represent the symbol hypothesis of shared word like speech segments.</p><p>Some interesting points that should be noted for this method are that the number of clusters in the k-means module turns out to be approximately equal to the number of phones that can be identified in the speech material, and acquisition of phones precedes the acquisition of words. The phone-like units are hypothesized in a data-driven way, whereas words are hypothesized in an hierarchical manner. With additional abstract information provided in the word discovery module, including some paralinguistic cues, such as prosody, accent, gender and culture information, word detection accuracy can be considerably improved.</p><p>Algorithms for different modules for this method (method I) were implemented in Matlab, ported to C++ using mex compiler, and a GUI tool was built to test different utterances from the challenge data set. A software prototype for this method was built, and is shown in Figure <ref type="figure" coords="6,260.51,330.21,3.86,8.96" target="#fig_0">1</ref>. The experimental evaluation of this method using challenge dataset, consisting of 100 audio files for training and another 100 files for testing provided by CLEF challenge task1 is discussed in Section 4. For this method we used well known existing system based on CMU Sphinx Speech Recognition toolkit [8], which is an open source repository of tools jointly designed by Carnegie Mellon University, Sun Microsystems Laboratories and Mitsubishi Electric Research Laboratories. It is designed differently from earlier versions of Sphinx systems in terms of modularity, flexibility and algorithmic aspects. Some of the im-provements from the earlier versions include newer search strategies, wide range of grammar and language models, and different types of acoustic models and feature streams. Due to several algorithmic innovations included in the system design it is possible to incorporate multiple sources in an elegant manner. Further, the system is modular, and is available in different versions, such as Sphinx4, Sphinx5, Pocket Sphinx and Pocket Sphinx for Android. While Sphinx4 version is entirely developed on the Java‚Ñ¢ platform and is highly portable, flexible, and easier to use with multithreading, the Pocket Sphinx is migrated from legacy C code with appropriate wrappers.</p><p>The speech recognition is performed in Sphinx 4 using a combination of HMM-based acoustic models and appropriate language and grammar models. Due to modularity of Sphinx architecture, it is possible to change the language model from a statistical Ngram language model to a context free grammar (CFG) or a stochastic CFG by modifying only one component of the system, namely the linguist. Likewise, it is possible to run the system using continuous, semi-continuous or discrete state output distributions by appropriate modification of the acoustic scorer. Further, information from multiple information streams can be incorporated and combined at any level, i.e., state, phoneme, word or grammar, and search module can also be switched between depth-first and breadth-first search strategies [8]. Figure <ref type="figure" coords="7,353.91,366.21,4.98,8.96">2</ref> shows the overall architecture of the CMU Sphinx decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2. CMU Sphinx Decoder Architecture [8]</head><p>As shown in Figure <ref type="figure" coords="8,212.93,150.18,3.83,8.96">2</ref>, the front-end module parameterizes the speech signal, and sends the extracted features to the decoder block. The decoder block consists of search manager module, linguist module and acoustic scorer module, and decoding is performed by co-ordination of these three blocks. The details of each module is described briefly here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Front End Module</head><p>Figure <ref type="figure" coords="8,153.58,260.25,4.98,8.96">3</ref> shows the detailed representation of the front-end module, which consists of several communicating blocks, each with an input and an output. The input of each block is linked to the output of its predecessor, and probes it to find out if the incoming information is speech data or control signal. The purpose of control signal here is to indicate the beginning or end of speech, or data dropped or some other problem. If the incoming data is speech, it is processed and the output is buffered, waiting for the successor block to request it. This design has several advantages, as it allows the output of any of the blocks to be tapped, actual input to the system to be any of the intermediate blocks, not just the first block. Due to this arrangement, it is possible to plugin not only speech signals, but also spectra, cepstra or other kinds of auditory representations for running the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 3. CMU Sphinx Front End Module [8]</head><p>The system is capable of running in different modes, including continuously from a stream of speech, and fully end pointed, where the system performs explicit end pointing, determining both beginning and ending end points of a speech segment automatically. The algorithm for the endpoint detection is based on comparison of energy level to three threshold levels, where two out of these three are used to determine start of speech, and one for the end of speech. Also, the starting and/or ending of speech from the incoming audio is detected by end pointer, and the end pointer ensures that the decoder does not waste any time by processing non-speech segments, by sending only speech segments to the decoder, and discarding any non-speech segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Decoder Block</head><p>There are three modules in the decoder block: search manager, linguist, and acoustic scorer, as described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Search Manager</head><p>The search manager constructs and searches a tree of possibilities for the best hypothesis, by using the information from the linguist. Also, the communication with the acoustic scorer to obtain the acoustic scores for incoming data is done by the search manager. A token tree is used by the search manager <ref type="bibr" coords="9,341.08,268.29,10.66,8.96" target="#b7">[9]</ref>, to represent the information about the search and complete history of all active paths a given point. Each token in the token tree contains the overall acoustic and language scores of the path, the reference to SentenceHMM reference, an identifier to the input feature frame, and the previous token reference, facilitating backtracing. The search manager is able to fully categorize a token to its senone, context-dependent phonetic unit, pronunciation, word and grammar state with the Sentence HMM reference. A set of active tokens is maintained in the active list in the search algorithm, to represent the tips of active search branches. During search phase, each input feature frame is scored against the acoustic models associated with each token in the active list, and pruning of low scoring branches is done. After pruning, the active list is updated by the search manager, using the successive SentenceHMM states of the tokens. New implementations that can provide alternate methods of storing and pruning of the active list can be easily created. The active list available as part of the final recognition results, can then be used by applications to inspect the highest scoring paths, and construct N-Best lists.</p><p>The next important mechanism in the search manager is searching through the token tree and the sentenceHMM, which is performed in two different ways: depth-first or breadth-first. Depth-first search is analogous to conventional stack decoding, where there is a time-sequential expansion of most promising tokens, and hence the paths from the root of the token tree to currently active tokens can be of varying lengths. However, for the breadth-first search, there is a synchronous expansion of all active tokens, resulting in equally long paths from the root of the tree to the currently active tokens. Further, breadth-first search is performed using the standard Viterbi algorithm, in which during search process, competing units (phoneme, word, grammar etch) are each represented by a directed acyclic graph (DAG). As can be seen in Figure <ref type="figure" coords="9,140.04,580.34,3.78,8.96" target="#fig_1">4</ref>, each DAG has a source and a sink, with Figure <ref type="figure" coords="9,341.32,580.34,9.62,8.96" target="#fig_1">4a</ref> showing the two-node DAGs for two competing phonemes AX and AXR, and a more complicated association represented by DAGs for the competing word units CAT and RAT, as in Figure <ref type="figure" coords="9,441.04,604.34,8.49,8.96" target="#fig_1">4b</ref>. For Viterbi decoding mechanism, the winner is decided by scoring each competing unit using the probability of the single best path, and the unit with the best-path score wins. For instance, if the phonemes AX and AXR have probabilities on the edges as (0.9, 0.02, 0.01) and (0.2, 0.7, 0.6) respectively, then the scores would be 0.9 and 0.7 and the AX would be the winner. However, if sum of the probabilities instead of the maximum is used for scoring, then the phoneme AXR would be the winner.</p><p>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Linguist</head><p>The purpose of linguist is to translate the linguistic constraints provided to the system into an internal data construct, called the grammar, which search manager uses it for search. Typical linguistic constraints are provided in the form of context free grammars, N-gram language models, finite state machines etc. The directed acyclic graph ( DAG ) representation is also used for grammar, with each node representing a set of words, that may be spoken at a particular time Linguistic constraints are typically provided in the form of context free grammars, N-gram language models, finite state machines etc. The grammar is also represented with directed graph, with each node representing a set of words that may be spoken at a particular time. The associated language and acoustic probabilities are shown by arcs for connecting nodes, which predict the likelihood of transmitting from one node to another.</p><p>Due to pluggable nature of CMU Sphinx, it is possible to load new grammars with several grammar loaders which can load different external grammar formats and generate internal grammar structure. This grammar is then compiled into a Sen-tenceHMM, which is basically a directed state graph, with each state in the graph represented a unit of speech. Then, a series of word states are extracted by decomposition of grammar nodes, with each node representing a word state. Next, a series of pronunciation states are obtained by decomposition of word states, with pronunciations extracted from a dictionary maintained by the linguist. And then, each pronunciation state is decomposed into a set of unit states, where these units may represent phonemes, diphones, and these could be specific to contexts of arbitrary length. Finally, each phoneme/diphone unit is then further decomposed into a sequence of HMM states. Each unit is then further decomposed to its sequence of HMM states. The Sentence-HMM construct thus comprises all of these states which are connected by arcs that have language, acoustic and insertion probabilities associated with them.</p><p>The linguist module as such, defines the contents of the SenthenceHMM construct very well. However, it is possible to improve the search results by altering the topology of the SentenceHMM, the memory footprint, the perplexity, speed and the recognition accuracy. Due to pluggable nature of CMU Sphinx, it is possible to use different SentenceHMM compilations without changing other aspects of the search.</p><p>Although the contents of a SentenceHMM are well defined by the linguist, there are a number of strategies that can be used in constructing the SentenceHMM that affect the search. By altering the topology of the SentenceHMM, the memory footprint, perplexity, speed and recognition accuracy can be affected. The pluggable nature of CMU Sphinx allows different SentenceHMM compilation methods to be used without changing other aspects of the search. For large grammars, since Sen-tenceHMM can grow to be quite large, we use a mechanism that allows dynamic contruction of SentenceHMM, where it is possible to discard the SentenceHMM when no longer needed. These features allow support for very large grammars as is normally required for general dictation recognition tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Acoustic Scorer Module</head><p>The next module is the acoustic scorer module which computes the state output probability or density values for the various states, for any given input vector using Gaussian scoring procedures. The search module obtains these scores from the acoustic scorer whenever it needs, and hence the acoustic scorer also communicates with the front-end module to obtain the features for which the scores need to be computed. All the information pertaining to the state output densities is retained by the scorer, and hence the search manager module is ignorant of whether scoring is done with continuous, semi-continuous or discrete HMMs. The speeding up of the scoring procedure is performed by heuristic algorithms locally within the search module, where such heuristics can benefit from additional information derived from the search module. The details of experimental evaluation for method I, is described in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Evaluation and Discussion</head><p>In this section we report results on CLEF 2015 challenge dataset, which is the NICTA synthetic nursing handover dataset provided by challenge organizers. We segmented each audio file to sentence level and down sampled it to 16kHz before feeding it to both the methods (Method I and Method II). Same approach involving sentence level segmentation and down sampling was done for both train and test subsets, where the test subset comprised 100 different audio recordings from the same speaker. As per the requirements of the challenge for CLEF EHealth Task 1a, the evaluation of performance has to be done with NIST scoring toolkit <ref type="bibr" coords="12,335.40,303.21,10.66,8.96" target="#b7">[9]</ref>, the submissions involved the scoring toolkit results, in terms of different performance measures including detection of correct words, insertions, deletions, substitutions and incorrect words for both training subset and test subset.</p><p>For method 2, we could not finish the evaluation before deadline for submission, and we submitted partial and incomplete results. However, we completed the experiments by the due date for working notes submission and Table <ref type="table" coords="12,357.15,387.23,4.98,8.96">2</ref> shows the performance of system in for both submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2: Evaluation of Method I /Method 2 against the benchmark performance results</head><p>Since this is still work in progress, we envisage the performance of the system, particularly method I, can be improved by appropriate choice of models, model parameters, acoustic features and abstract labels, which is currently being pursued. For method 2 (UC_2_test/UC_2_train), we could not include the results in the original submission. However, as can be seen in Table <ref type="table" coords="12,284.40,677.30,3.86,8.96">2</ref>, the average word detection accuracy on test on the test set was 77.7 %, and on training set, it was 74.03%, comparable to benchmark results provided by the challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion:</head><p>In this working notes/paper, we present the details of methods used for our two submissions to CLEF eHealth Challenge Task 1a, on clinical speech recognition. First method involve the proposal of novel zero resource word discovery algorithm, whereas the 2 nd method uses well known open source CMU Sphinx speech recognition toolkit. Further investigations are in progress to improve the performance of each of these approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,193.61,558.07,207.95,8.10;6,124.70,375.40,345.78,161.75"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Clinical Speech to Text Recognizer Software Tool</figDesc><graphic coords="6,124.70,375.40,345.78,161.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="10,223.37,498.04,148.46,8.10;10,124.70,195.40,345.75,293.45"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Search Manager DAG module [8]</figDesc><graphic coords="10,124.70,195.40,345.75,293.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="7,125.15,405.40,345.00,255.94"><head></head><label></label><figDesc></figDesc><graphic coords="7,125.15,405.40,345.00,255.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="12,124.70,449.40,345.85,164.80"><head></head><label></label><figDesc></figDesc><graphic coords="12,124.70,449.40,345.85,164.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,126.02,150.23,342.97,52.13"><head>Table 1 : Abstract Tags indicating the presence of a word in the utterance Each utterance is associated with abstract information that indicates the presence of a word, but not its acoustical representation or its position in the utterance. As an illustration, this table shows eight abstract tags, related to the occurrence of 'forty', 'eight', 'years', 'old', 'bed', 'investigation', 'monitoring', 'stable'</head><label>1</label><figDesc></figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div><p>yes yes yes No no no no under <rs type="person">Dr Johnson</rs>, bed 3 no no no no Yes no no no came in for investigation no no no no</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="13,132.67,348.02,337.77,8.10;13,141.74,359.06,328.58,8.10;13,141.74,370.10,329.00,8.10;13,141.74,381.04,181.09,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="13,275.98,359.06,171.67,8.10">Overview of the CLEF eHealth Evaluation Lab</title>
		<author>
			<persName coords=""><forename type="first">Lorraine</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liadh</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hanna</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Leif</forename><surname>Hanlen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aur√©lie</forename><surname>N√©v√©ol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cyril</forename><surname>Grouin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joao</forename><surname>Palotti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guido</forename><surname>Zuccon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,141.74,370.10,236.21,8.10">CLEF 2015 -6th Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="13,385.44,370.10,85.30,8.10;13,141.74,381.04,73.33,8.10">Lecture Notes in Computer Science (LNCS</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015-09">2015. September 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.67,392.08,337.70,8.10;13,141.74,403.12,328.63,8.10;13,141.74,414.04,309.41,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="13,441.91,392.08,28.46,8.10;13,141.74,403.12,257.02,8.10">Task 1a of the CLEF eHealth Evaluation Lab 2015: Clinical speech recognition</title>
		<author>
			<persName coords=""><forename type="first">Hanna</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Leif</forename><surname>Hanlen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lorraine</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liadh</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J F</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,404.89,403.12,65.47,8.10;13,141.74,414.04,241.39,8.10">Working Notes of the CLEF 2015 -6th Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2015-09">September 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.67,425.08,337.62,8.10;13,141.74,436.12,328.62,8.10;13,141.74,447.04,130.40,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,386.82,425.08,83.47,8.10;13,141.74,436.12,301.98,8.10">Benchmarking clinical speech recognition and information extraction: New data, methods, and evaluations</title>
		<author>
			<persName coords=""><forename type="first">Hanna</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Leif</forename><surname>Hanlen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gabriela</forename><surname>Ferraro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,449.95,436.12,20.40,8.10;13,141.74,447.04,73.04,8.10">JMIR Medical Informatics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.67,458.08,337.83,8.10;13,141.74,469.12,325.08,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,335.17,458.08,135.32,8.10;13,141.74,469.12,199.57,8.10">An analysis of the causes of adverse events from the quality of Australian Health Care Study</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gibberd</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,347.44,469.12,42.42,8.10">Med J Aust</title>
		<imprint>
			<biblScope unit="volume">170</biblScope>
			<biblScope unit="page" from="411" to="415" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.67,480.04,334.87,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,194.71,480.04,99.90,8.10">How safe are our hospitals?</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Armstrong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,296.84,480.04,95.02,8.10">Australian Nursing Journal</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="18" to="21" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.67,491.08,337.62,8.10;13,141.74,502.12,208.91,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,340.49,491.08,84.55,8.10">The future health care</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Charlesworth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Davey</surname></persName>
		</author>
		<idno type="DOI">10.1071/AH14243</idno>
		<ptr target="http://dx.doi.org/10.1071/AH14243" />
	</analytic>
	<monogr>
		<title level="j" coord="13,432.91,491.08,37.37,8.10;13,141.74,502.12,51.63,8.10">Australian Health Review</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.67,513.04,337.67,8.10;13,141.74,524.08,328.80,8.10;13,141.74,535.15,135.72,8.10" xml:id="b6">
	<monogr>
		<title level="m" coord="13,168.95,524.08,171.77,8.10">OSSIE guide to clinical handover improvement</title>
		<meeting><address><addrLine>Sydney</addrLine></address></meeting>
		<imprint>
			<publisher>Australian Commission on Safety and Quality in Health Care</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.67,557.02,337.69,8.19;13,141.74,568.06,329.00,8.19;13,141.74,579.07,75.13,8.10" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="13,336.58,557.02,133.78,8.18;13,141.74,568.06,174.56,8.19">Token passing: A simple conceptual model for connected speech recognition systems</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">H</forename><surname>Russel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H S</forename><surname>Russel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
		<respStmt>
			<orgName>Cambridge University Engineering Dept</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="13,132.40,590.02,338.16,8.19;13,141.74,601.15,224.86,8.10" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,251.50,590.02,187.29,8.19">Liveness&apos; verification in audio-video authentication</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chetty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,451.77,590.11,18.79,8.10;13,141.74,601.15,141.25,8.10">Proc. Int. Conf. Spoken Language Processing</title>
		<meeting>Int. Conf. Spoken Language essing</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="2509" to="2512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.40,612.07,337.88,8.10;13,141.74,623.11,328.45,8.10;13,141.74,634.15,85.31,8.10" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,252.37,612.07,184.53,8.10">The Big Australian Speech Corpus (The Big ASC)</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chetty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,454.27,612.07,16.01,8.10;13,141.74,623.11,267.90,8.10">13th Australasian International Conference on Speech Science and Technology</title>
		<meeting><address><addrLine>ASSTA; Melbourne</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="166" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.40,644.98,338.32,8.19;13,141.74,656.02,109.91,8.18" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,241.35,644.98,229.38,8.19;13,141.74,656.02,24.13,8.18">Towards unsupervised training of speaker independent acoustic models</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,185.49,656.02,40.10,8.18">Interspeech</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.40,667.06,338.24,8.19;13,141.74,678.07,105.79,8.10" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="13,237.43,667.06,150.72,8.18">Unsupervised pattern discovery in speech</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,398.67,667.06,50.64,8.18">IEEE T-ASLP</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="186" to="197" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,132.40,149.90,337.89,8.19;14,141.74,160.94,135.69,8.19" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="14,304.39,149.90,165.89,8.18;14,141.74,160.94,50.64,8.19">Towards spoken term discovery at scale with zero resources</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,211.28,160.94,40.00,8.18">Interspeech</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,132.40,171.98,337.98,8.19;14,141.74,182.90,328.57,8.18;14,141.74,194.03,78.79,8.10" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="14,321.50,171.98,148.88,8.18;14,141.74,182.90,32.43,8.18">Learning phonetic categories by learning a lexicon</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">H</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,184.29,182.90,282.38,8.18">Proceedings of the 31st Annual Conference of the Cognitive Science Society</title>
		<meeting>the 31st Annual Conference of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2208" to="2213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,132.40,204.98,338.35,8.19;14,141.74,215.99,90.47,8.10" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="14,311.07,204.98,152.32,8.18">Learning phonemes with a proto-lexicon</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Peperkamp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,141.74,215.99,64.18,8.10">Cognitive Science</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,132.40,226.97,338.34,8.19;14,141.74,238.01,251.13,8.19" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="14,245.49,226.97,225.25,8.18;14,141.74,238.01,9.45,8.18">Influences on infant speech processing: Toward a new synthesis</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Werker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">C</forename><surname>Tees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,159.73,238.01,104.74,8.18">Annual review of psychology</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="509" to="535" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,132.40,248.93,338.22,8.19;14,141.74,259.97,305.53,8.19" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="14,319.89,248.93,150.73,8.18;14,141.74,259.97,138.07,8.18">A Bayesian framework for word segmentation: Exploring the effects of context</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,289.32,259.97,34.50,8.18">Cognition</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="54" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,132.40,271.01,338.46,8.19;14,141.74,281.93,217.72,8.19" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="14,196.30,271.01,274.55,8.18;14,141.74,281.93,92.75,8.19">Using adaptor grammars to identify synergies in the unsupervised acquisition of linguistic structure</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,252.80,281.93,15.01,8.18">ACL</title>
		<meeting><address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,132.40,292.97,338.34,8.19;14,141.74,304.01,328.64,8.18;14,141.74,315.02,58.23,8.10" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="14,268.56,292.97,202.18,8.18;14,141.74,304.01,240.42,8.18">Improving nonparametric Bayesian inference: Experiments on unsupervised word segmentation with adaptor grammars</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Goldwater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,403.15,304.01,27.57,8.18">NAACL</title>
		<meeting><address><addrLine>Boulder, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
