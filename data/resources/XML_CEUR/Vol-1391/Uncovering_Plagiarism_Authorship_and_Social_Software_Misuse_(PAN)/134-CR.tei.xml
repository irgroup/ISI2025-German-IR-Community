<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,150.14,152.67,294.93,12.64;1,251.09,170.67,93.09,12.64;1,212.45,190.54,170.20,10.80">Authorship Verification -An Approach based on Random Forest Notebook for PAN at CLEF 2015</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,196.73,228.18,59.03,8.96"><forename type="first">Promita</forename><surname>Maitra</surname></persName>
							<email>promita.maitra@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">Jadavpur University</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,262.44,228.18,61.44,8.96"><forename type="first">Souvick</forename><surname>Ghosh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">Jadavpur University</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,343.39,228.18,55.17,8.96"><forename type="first">Dipankar</forename><surname>Das</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">Jadavpur University</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,150.14,152.67,294.93,12.64;1,251.09,170.67,93.09,12.64;1,212.45,190.54,170.20,10.80">Authorship Verification -An Approach based on Random Forest Notebook for PAN at CLEF 2015</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">932D6AE62739B622F79F2EF8940DE4C1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Authorship attribution, being an important problem in many areas including information retrieval, computational linguistics, law and journalism etc., has been identified as a subject of increasingly research interest in the recent years. In case of Author Identification task in PAN at CLEF 2015, the main focus was given on cross-genre and cross-topic author verification tasks. We have used several word-based and style-based features to identify the differences between the known and unknown problems of one given set and label the unknown ones accordingly using a Random Forest based classifier.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Author identification has a long history that includes some famous disputed authorship cases as well as various forensic applications. The task is used in order to determine the author who wrote a chapter or passage of a book (e.g., the holy Bible, being the most famous example). In general, the researches on author identification make use of the structure of the text and the words used to describe the contexts. In stylometry research, it is generally accepted that the authors have unconscious writing habits that can be evident from their use of words and grammar or punctuation etc. as such indicators could be reliable to identify an author <ref type="bibr" coords="1,356.83,519.23,11.37,8.96" target="#b0">[1]</ref>[2][3] <ref type="bibr" coords="1,390.95,519.23,11.37,8.96" target="#b3">[4]</ref>. The individual differences in use of language are referred to as idiolect (our idiolect includes the vocabulary appropriate to our various interests and activities, pronunciations reflective of the region in which we live or have lived, and variable styles of speaking that shift subtly depending on whom we are addressing.). The same feature, i.e. unconscious use of syntax gives rise to the opportunity to perform automatic author identification based on words and style based features.</p><p>In the paper, we have presented our approach of automatic authorship verification from a dataset containing cross-genre and cross-topic text snippets available in four different languages-Dutch, English, Greek and Spanish. In each of the problem sets, a group of known documents and one unknown document were provided. The goal of the present task is to predict if the author of an unknown document is same as that of the known document set by analyzing the similarity among the known documents and their differences (or similarities) to the unknown one. We have used 17 types of word based and style based features in total to find out the underlying similarities and differences of a set of documents. The Random Forest classifier that use a decision tree based approach and available with Weka 1 tool was employed in our system to choose the important features as we were not sure about the importance of a specific feature out of the main features viz. punctuation, sentence length, vocabulary, N-gram, Parts-of-Speech (POS).</p><p>The rest of the paper is as follows. Section 2 consists of a brief discussion on the related work available till date. Section 3 describes the feature selection for implementing the Random Forest algorithm whereas Section 4 gives the details of the system architecture. In Section 5, the experiments with detailed analysis of results are presented. Finally, the conclusions and future scopes of the experiments are presented in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Studies</head><p>It is observed that the two main factors that characterize a text are its content and style, and both can be used as means of authorship categorization. Generally, there are two kinds of problems in author identification, the first one being comparatively easier than the second one. In the first kind, a finite set of documents with known authors is given and the task is to comment on the authorship of the unknown documents. In the second kind of task, a set of documents of a particular author is given along with an unknown document and the task is to predict whether the unknown document is written by the same author or not.</p><p>In author identification research, different aspects influence the performance of the author classification task. Some of the aspects such as the language of texts used, length of the text snippets, number of authors and texts, types of features etc. are considered as important for the classification task. It is observed that the number of features is most often varied to determine the influence of certain types of features. <ref type="bibr" coords="2,124.70,495.23,53.54,8.96">Corney et al.</ref> indicates that the most successful features are the function words and character n-grams whereas McCombe performed the tests using word unigrams as classification feature, for which the results were promising <ref type="bibr" coords="2,362.23,519.23,11.15,8.96" target="#b1">[2]</ref> <ref type="bibr" coords="2,373.38,519.23,11.15,8.96" target="#b3">[4]</ref>. But, no method was successful in classification based on word bigrams, which seems contradictory because word bigrams capture more information about the sentence structure used by the author. On the other hand, Hirst and Feiguina used tag bigrams to discriminate between the works of two authors with three experiments <ref type="bibr" coords="2,356.59,567.23,10.69,8.96" target="#b4">[5]</ref>.</p><p>So far, all the researches we described used only the English data sets. In contrast, author identification tasks were also performed on messages of other languages. In order to identify authors of Greek texts published in a newspaper, Stamatatos et al. used a combination of lexical measures and style markers and achieved an accuracy of 87% in texts classification <ref type="bibr" coords="2,282.05,627.26,10.69,8.96" target="#b5">[6]</ref>. It has to be mentioned that all of the previous researches discussed have one thing in common: they all use less than 10 authors.</p><p>Houvardas and Stamatatos used a data set consisting of a training set containing 2500 texts from 50 authors, with features like the most frequently occurring character ngrams of variable length (3-grams, 4-grams and 5-grams) and achieved an accuracy of 73.08% <ref type="bibr" coords="3,158.18,186.18,10.60,8.96" target="#b6">[7]</ref>.</p><p>In literary authorship, the stylometric features are commonly used, for example-Letter frequencies, N-gram frequencies, Function word usage, Vocabulary richness, Lexical richness, Distribution of syllables per word, Word frequencies, Hapax legomena, Hapax dislegomena, Word length distribution, Word collocations, Sentence length, Preferred word positions, Prepositional phrase structure, Distribution parts of speech, Phrasal composition grammar etc.</p><formula xml:id="formula_0" coords="3,333.43,258.21,68.75,8.96">[1][2][3][4][8][9]</formula><p>. The fact is that there is no such consensus on which stylometric features are applied to achieve the best results for authorship identification. In case of previous year's PAN and especially in the author identification task, Khonji and Iraqi used lots of features with parameter tuning in a GI framework to obtain a final score of 0.490 as product of AUC and c@1 <ref type="bibr" coords="3,321.41,318.21,15.45,8.96" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Feature and Classifier Selection</head><p>It is found that several possible features from different categories can be used for author identification task. Thus, the number and types of features are often varied in author identification research to determine the influence of certain types of features.</p><p>As this year's task includes cross-genre and cross-topic datasets, the features based on word-sense won't be much helpful compared to word structure based features. Therefore, we have used a combination of word and style based features in our present system.</p><p> Total Punctuation Count: This feature counts the number of total punctuation symbols used in a text, normalized by the word count in that text. For example: "We are participating in PAN, a part of CLEF 2015. Our main focus will be on Authorship Verification task-a subset of Author Identification." Here, this particular feature value is: (number of punctuation/number of words) = 4/24 = 0.16666 We will calculate differences between unknown text and each known text of a problem set and take the average of those differences.</p><p> Specific Punctuation Ratio: This is the ratio of the total number of specific punctuation symbols like comma (,), semicolon (;), question-mark (?), exclamation-mark (!), stop (.), slash (/), dash (-), colon (:) etc. to the total punctuation count.   POS Sequence Frequency: The feature is similar to the previous feature, but, here we try to find out the similarities based on a particular sequence (in a span of two consecutive POS tags) that the author uses for a considerable number of times.</p><p>In the above example, we have sequence overlaps: "AuxVerb Verb", "Verb Preposition", "Det Noun", and "Noun Preposition". So, feature value is: 4</p><p> Starting POS Frequency: We try to list the POS tags that the author uses in the beginning of sentences according to their frequency and then compare them among the known and unknown documents to find a lexical pattern. For example, a particular author might have the tendency to start sentences with auxiliary verbs (example) or prepositions (in, for) unknowingly for a considerable number of sentences in the corpus. The feature also indicates the writing style of the author.</p><p>In the above example, both known and unknown text starts with the POS tag Pronoun. So, feature value is 1.</p><p>As it is impossible to decide manually which of these features are most important or relevant to our problem structure, we decided to go for Decision Tree based classifier. Such classifiers are fast to train and easy to evaluate and interrupt and moreover non-parametric and for the very reason, we don't have to worry about the outliers or whether the data is linearly separable or not. The main disadvantage is that they easily overfit, but that's where ensemble methods like Random Forest come in. The main advantages of such approaches are:</p><p>-Almost always have lower classification error and better F-scores than decision trees.</p><p>-Almost always perform as well as or better than SVMs, but are easier to understand.</p><p>-Deal really well with uneven data sets that have missing variables.</p><p>-Give a good idea of which features in the data set are most important.</p><p>-Generally train faster than SVMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">System Architecture</head><p>Figure <ref type="figure" coords="6,153.50,303.21,4.98,8.96" target="#fig_1">1</ref> and 2 below illustrates the basic step-by-step architecture of our training and testing software, respectively. We were given datasets in four different languages-Dutch, Spanish, English and Greek, which contain numbers of known and unknown text samples by several authors. In a single author subset, we had one or more documents that are known to be written by that author and an unknown one for which the authorship is not known. The task is to predict whether that particular unknown document is written by the same author of that author subset or not. A .json file contains the language and the problem titles of a particular language dataset. For training sets, we were given a .txt file with the answers to those subset problems, i.e. whether or not those unknown files of a subset problem are written by the same author of other files in that subset.</p><p>At first, we read the contents from the text files and convert them all to the lowercase for an efficient parsing. Using the Stanford CoreNLP<ref type="foot" coords="6,341.47,445.04,3.24,5.83" target="#foot_0">2</ref> parser, we tagged the contents of files to obtain the root words and POS tag for each word. Next, we counted the frequencies of each root word, all POS tag, POS tag-sequence and the starting POS tags from the tagged output and calculated the bigram and trigram frequencies using root-words as well. Also, the vocabulary strength of an author, i.e. the ratio of number of different types of root words to number of total words occurred is another feature that we considered in our approach. The long and short sentence ratio and punctuation counts are calculated from the plain text, i.e. raw file contents. We used the API of Weka 3.7.7.5 tool to employ a Random Forest classifier with the .arff file written using the features that we extracted from the training dataset.</p><p>Weka is an open source data mining tool. It presents a collection of machine learning algorithms for data mining tasks.  In case of testing the dataset, after extracting, calculating features and writing in .arff file, we used the same classifier to predict the answer class for unknown documents in test dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment and Analysis</head><p>Table <ref type="table" coords="7,151.19,585.23,4.98,8.96">1</ref> shows the detailed results of our system in PAN 15 authorship verification task as provided by PAN organizing committee. Our present approach achieved the third best result in Dutch language with an AUC score of 0.75874 and C@1 score of 0.68283. We observe that our system didn't perform well in other languages like English, Greek and Spanish and obtained AUC scores 0.60174, 0.6126, 0.6096 and C@1 scores 0.57732, 0.5824 and 0.5768, respectively. One possible reason behind this fluctuation in result might be because of the variable number and size of the 'known'documents. Cross-topic and cross-genre texts appear to be more common in English and Spanish, which produced a visible decline in the performance. To deal with multi-Text Processing and Feature File Construction Module genre, we trained our system to analyze multiple corpuses/genre-specific training data and build the trainer as a whole. However, we had to modify our code as PAN committee decided to append time stamps automatically to the output folder and therefore the merging of the training sets was not possible. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Scope</head><p>In this paper, we have presented our approach to build software for automatic authorship verification using methods of Text Analytics. It uses same features for all the language datasets and a Random Forest classifier to classify the unknown documents based on the features extracted. In the recent years, the practical applications for authorship attribution have grown in areas such as intelligence (linking intercepted messages to each other and to known terrorists), criminal law, civil law and computer security (tracking authors of computer virus source code). This activity is part of computer science for identifying technologies, including biometrics, cryptographic signatures, intrusion detection systems and others.</p><p>In our future work, the accuracy of system can be improved by including some language specific features. . On the other hand, the features like the average paragraph length, average word length along with a Deep Learning classifier might produce interesting hike in our system score.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,160.70,651.74,54.15,8.96;3,160.70,663.63,309.67,9.07;3,160.70,675.63,269.16,9.07;4,160.70,150.18,128.87,8.96;4,160.70,162.18,281.75,8.96;4,160.70,174.18,300.23,8.96;4,160.70,186.18,290.03,8.96;4,160.70,198.18,288.47,8.96;4,160.70,210.18,309.84,8.96;4,160.70,222.18,12.52,8.96;4,160.70,234.18,299.77,8.96;4,160.70,246.18,285.24,8.96;4,160.70,258.21,297.02,8.96"><head></head><label></label><figDesc>For example: "We are participating in PAN, a part of CLEF 2015. Our main focus will be on Authorship Verification task-a subset of Author Identification." Here, the feature values will be: StopCount= (number of stop/total number of punctuation) = 2/4 = 0.5 CommaCount= (number of stop/total number of punctuation) = 1/4 = 0.25 DashCount= (number of stop/total number of punctuation) = 1/4 = 0.25 ColonCount= (number of stop/total number of punctuation) = 0/4 = 0.0 ExclamationCount= (number of stop/total number of punctuation) = 0/4 = 0.0 QuestionCount= (number of stop/total number of punctuation) = 0/4 = 0.0 SlashCount= (number of stop/total number of punctuation) = 0/4 = 0.0 SemicolonCount= (number of stop/total number of punctuation) = 0/4 = 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,293.33,405.56,18.53,7.24;7,169.70,412.90,246.05,64.50"><head>Fig. 1</head><label>1</label><figDesc>Fig. 1</figDesc><graphic coords="7,169.70,412.90,246.05,64.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,292.85,482.02,18.64,7.24"><head>Fig. 2</head><label>2</label><figDesc>Fig. 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,142.70,258.21,328.11,429.73"><head>Long-sentence/ Short-sentence Ratio:</head><label></label><figDesc>.0 We will calculate differences between unknown text and each known text of a problem set and take the average of those differences in a similar manner like the previous one.  Ratio of the long (length&gt;12) or short (length&lt;6) sentences to the total number of sentences is represented by this feature. In this feature, we try to capture the tendency of an author to use one or two particular types of POS that appear more frequently than the others, if there is any. So, we calculate the frequencies of each POS tag from texts and compare the known and unknown texts based on that.</figDesc><table coords="4,142.70,342.45,328.00,345.49"><row><cell>subset of Author Identification."</cell></row><row><cell>Here, some of the bigrams are-"We are", "are participating", "participating</cell></row><row><cell>in" etc. and some trigrams are-"We are participating", "are participating in"</cell></row><row><cell>etc. (without stemming and stopword removal)</cell></row><row><cell>BigramOverlap=2 ["We are", "are participating"]</cell></row><row><cell>TrigramOverlap=1 ["We are participating"], for this particular example.</cell></row><row><cell> POS Frequency: For example:</cell></row><row><cell>Known-"We are participating in PAN, a part of CLEF 2015."</cell></row><row><cell>Unknown-"We are participating mainly in Authorship Verification task-a</cell></row><row><cell>subset of Author Identification."</cell></row><row><cell>For example:</cell></row><row><cell>LongSentenceRatio=1/2=0.5</cell></row><row><cell>ShortSentenceRatio=0/2=0.0</cell></row><row><cell>Then again, we take the differences in a similar manner.</cell></row><row><cell>For example:</cell></row><row><cell>Known-"We are participating in PAN, a part of CLEF 2015."</cell></row><row><cell>Unknown-"We are participating mainly in an Authorship Verification task-</cell></row><row><cell>a subset of Author Identification task."</cell></row><row><cell>Here, in Known, total number of words= 10 and unique words= 10.</cell></row><row><cell>In Unknown, total number of words= 15 and unique words= 13 (without</cell></row><row><cell>stemming and stopword removal)</cell></row><row><cell>So, VocabularyStrength in Known=10/10=1.0.</cell></row><row><cell>VocabularyStrength in Unknown=13/15=0.8666.</cell></row><row><cell>So, the feature value will be: 0.1333</cell></row><row><cell> N-gram Difference: This particular feature is very common for the task of</cell></row><row><cell>authorship verification, where we tried to capture the n-gram (bigram and</cell></row><row><cell>trigram for our system) overlaps among the known and unknown texts.</cell></row><row><cell>For example:</cell></row><row><cell>Known-"We are participating in PAN, a part of CLEF 2015."</cell></row><row><cell>Unknown-"We are participating mainly in Authorship Verification task-a</cell></row></table><note coords="4,160.70,354.34,309.67,9.07;4,160.70,366.34,269.16,9.07;4,160.70,378.45,309.75,8.96;4,160.70,390.45,201.14,8.96;4,142.70,450.49,328.00,9.06;4,160.70,462.71,309.90,8.96;4,160.70,474.71,113.81,8.96;5,160.70,330.45,310.06,8.96;5,160.70,342.45,309.93,8.96;5,160.70,354.45,309.93,8.96;5,160.70,366.45,309.93,8.96;5,160.70,378.45,310.12,8.96;5,160.70,390.45,42.60,8.96;5,160.70,402.45,66.69,8.96"><p>"We are participating in PAN, a part of CLEF 2015. Our main focus will be on Authorship Verification task-a subset of Author Identification." Here, first sentence has length of 10 and second sentence has length of 14. So we have 0 short sentences and 1 long sentence.  Vocabulary Strength: We tried to capture the vocabulary strength of an author by calculating the ratio of the unique words used to the total number of words used in a text snippet. Here, the POS tags are: We/Pronoun are/AuxVerb participating/Verb in/Preposition PAN/ProperNoun, a/Det part/Noun of/Preposition CLEF/ProperNoun 2015. Our/Pronoun main/Adjective focus/Noun will/AuxVerb be/Verb on/Preposition Authorship/Noun Verification/Noun task/Noun-a/Det subset/Noun of/Preposition Author/Noun Identification/Noun. Feature value: 6</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="6,136.10,686.35,187.54,8.10"><p>http://www-nlp.stanford.edu/software/corenlp.shtml</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,132.67,531.04,338.11,8.10;8,141.74,542.08,301.78,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,370.36,531.04,100.41,8.10;8,141.74,542.08,101.96,8.10">Automatically profiling the author of an anonymous text</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Argamon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Pennebaker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,249.14,542.08,103.67,8.10">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="119" to="123" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.67,564.04,338.12,8.10;8,141.74,575.08,329.06,8.10;8,141.74,586.12,26.34,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,292.03,564.04,175.41,8.10">Computational methods in authorship attribution</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Argamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,141.74,575.08,278.46,8.10">Journal of the American Society for information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="26" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.67,608.02,337.98,8.19;8,141.74,619.15,234.00,8.10" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="8,192.86,608.02,210.16,8.19">Analysing e-mail text authorship for forensic purposes</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Corney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<pubPlace>Brisbane Australia</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Queensland University of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct coords="8,132.67,641.02,337.85,8.19;8,141.74,652.15,50.33,8.10" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Mccombe</surname></persName>
		</author>
		<title level="m" coord="8,199.49,641.02,117.96,8.18">Methods of author identification</title>
		<meeting><address><addrLine>Dublin Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>Trinity College</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct coords="8,132.67,674.11,338.08,8.10;8,141.74,685.15,216.08,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,246.77,674.11,223.97,8.10;8,141.74,685.15,36.35,8.10">Bigrams of syntactic labels for authorship discrimination of short texts</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hirst</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Feiguina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,183.57,685.15,123.50,8.10">Literary and Linguistic Computing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.67,161.03,338.03,8.10;9,141.74,172.07,276.22,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,329.47,161.03,141.22,8.10;9,141.74,172.07,87.64,8.10">Computer-based authorship attribution without lexical measures</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Fakotakis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kokkinakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,245.43,172.07,109.79,8.10">Computers and the Humanities</title>
		<imprint>
			<biblScope unit="page" from="193" to="214" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.67,194.03,338.09,8.10;9,141.74,205.07,103.64,8.10" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,265.49,194.03,192.11,8.10">N-gram feature selection for authorship identification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Houvardas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,141.74,205.07,25.97,8.10">AIMSA</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="77" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.67,227.03,338.07,8.10;9,141.74,238.07,328.75,8.10;9,141.74,248.99,295.69,8.10" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,358.39,227.03,112.35,8.10;9,141.74,238.07,242.72,8.10">Feeling may separate Two Authors: Incorporating Sentiment in Authorship Identification Task</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">G</forename><surname>Patra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bandyopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,404.32,238.07,66.17,8.10;9,141.74,248.99,209.72,8.10">10th International Conference on Natural Language Processing (ICON 2013)</title>
		<meeting><address><addrLine>Delhi, India</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="121" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.67,271.10,338.12,8.10;9,141.74,282.02,328.90,8.10;9,141.74,293.06,328.82,8.10;9,141.74,304.10,86.86,8.10" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,404.23,271.10,66.55,8.10;9,141.74,282.02,295.49,8.10">Automatic Author Profiling Based on Linguistic and Stylistic Features-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">G</forename><surname>Patra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Saikh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bandyopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,463.26,282.02,7.38,8.10;9,141.74,293.06,328.82,8.10;9,141.74,304.10,59.70,8.10">In 9th evaluation lab on uncovering plagiarism, authorship, and social software misuse (PAN 2013) with CLEF</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.40,327.02,338.46,8.10;9,141.74,338.06,206.42,8.10" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,237.02,327.02,233.84,8.10;9,141.74,338.06,58.14,8.10">A Slightly-modified GI-based Author-verifier with Lots of Features (ASGALF)</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Khonji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Iraqi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,216.68,338.06,103.93,8.10">proceedings of PAN at CLEF</title>
		<meeting>PAN at CLEF</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
