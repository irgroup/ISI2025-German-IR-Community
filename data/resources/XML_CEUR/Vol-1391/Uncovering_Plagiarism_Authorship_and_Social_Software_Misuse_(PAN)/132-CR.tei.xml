<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,281.56,115.90,179.18,12.90;1,149.32,133.83,316.72,12.90;1,279.90,151.77,55.56,12.90;1,223.43,172.15,168.50,10.75">Basic Stacked Generalization Applied To Predictions from a Set of Heterogeneous Learners Notebook for PAN at CLEF 2015</title>
				<funder ref="#_wt2PrNM">
					<orgName type="full">Science Foundation Ireland</orgName>
				</funder>
				<funder>
					<orgName type="full">ADAPT Centre</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,177.84,209.08,59.38,8.64"><forename type="first">Erwan</forename><surname>Moreau</surname></persName>
							<email>moreaue@cs.tcd.ie</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Computing and Language Studies School of Computer Science</orgName>
								<orgName type="laboratory">CNGL and Computational Linguistics Group</orgName>
								<orgName type="institution">Statistics Trinity College Dublin</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,246.67,209.08,52.84,8.64"><forename type="first">Arun</forename><surname>Jayapal</surname></persName>
							<email>jayapala@cs.tcd.ie</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Computing and Language Studies School of Computer Science</orgName>
								<orgName type="laboratory">CNGL and Computational Linguistics Group</orgName>
								<orgName type="institution">Statistics Trinity College Dublin</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,308.96,209.08,55.05,8.64"><forename type="first">Gerard</forename><surname>Lynch</surname></persName>
							<email>gerard.lynch@ucd.ie</email>
							<affiliation key="aff1">
								<orgName type="department">Centre for Applied Data Analytics Research</orgName>
								<orgName type="institution">University College Dublin</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,390.34,209.08,42.71,8.64"><forename type="first">Carl</forename><surname>Vogel</surname></persName>
							<email>vogel@cs.tcd.ie</email>
							<affiliation key="aff2">
								<orgName type="department">Centre for Computing and Language Studies School of Computer Science</orgName>
								<orgName type="laboratory">Computational Linguistics Group</orgName>
								<orgName type="institution">Statistics Trinity College Dublin</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,281.56,115.90,179.18,12.90;1,149.32,133.83,316.72,12.90;1,279.90,151.77,55.56,12.90;1,223.43,172.15,168.50,10.75">Basic Stacked Generalization Applied To Predictions from a Set of Heterogeneous Learners Notebook for PAN at CLEF 2015</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">57FC2C37876740E053D99D57CE2DF6CB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we present the system we submitted to the PAN 2015 competition for the author verification task. We consider the task as a supervised classification problem, where each case in a dataset is an instance. Our approach combines the output from multiple learners using basic stacked generalization. The individual learners are obtained using five distinct approaches, each trained using a generic genetic algorithm. Our system performed well on the test set: the macro-average score was 0.61 (2nd best).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the PAN 2015 author verification task <ref type="bibr" coords="1,299.22,512.97,15.27,8.64" target="#b10">[11]</ref>, a training set is provided for each of the 4 datasets: Dutch, English, Greek and Spanish. Each training dataset consists of a set of 100 problems, and each problem consists of a small set of "known" documents written by a single person and a "questioned" document: the task is to determine whether the questioned document was written by the same person. More precisely, the system must provide its prediction as a value in the interval [0, 1], which represents the probability that the answer is positive (same author). The intended interpretation is for 0 to mean "different author" with maximum certainty, and for 1 to mean "same author" with maximum certainty, and any intermediate value describes the likeliness of a positive answer, with 0.5 equivalent to the system saying "I don't know". The predictions are evaluated using the product of the area under the ROC curve (AUC) and the modified accuracy measure c@1 <ref type="bibr" coords="1,192.07,644.48,10.58,8.64" target="#b7">[8]</ref>, which treats 0.5 answers as a special case, in order to penalize such cases less than a wrong answer.</p><p>We consider the task as a supervised classification/regression problem <ref type="foot" coords="2,423.99,117.64,3.49,6.05" target="#foot_0">4</ref> , at the dataset level; in other words, each problem is an instance and we expect to find some regularities among the instances which belong to the same dataset. It is worth noticing that this view is compatible with, for instance, unsupervised learning at the problem level: the latter can be seen as an initial answer, which is then used as input in the dataset-level supervised system.</p><p>We propose a relatively complex solution <ref type="foot" coords="2,315.54,189.51,3.49,6.05" target="#foot_1">5</ref> , which consists in training a set of multiple (mostly) heterogeneous learners, later combined using a simple stacked generalization method. A genetic algorithm is used in two stages:</p><p>1. to learn an optimal combination of parameters for each of the five approaches from which the individual learners are obtained; 2. to learn the optimal way to combine the learners together.</p><p>The main difficulty in this approach is to avoid overfitting, especially with such a small number of instances. The genetic algorithm always tries to maximize the performance in any available way, because this is the criterion used to select the breeders for the next generation. But by doing so it can easily end selecting parameters which are optimal only for the set of instances seen by the algorithm, even if the performance of every individual is evaluated using cross-validation. This is why we try to minimize the risk of overfitting by using various additional techniques (see §3.2).</p><p>This paper is organized as follows: in §2 we detail the reasons why we chose this approach. In §3 we describe the architecture of our system, then in §4 we give a brief overview of each of the five approaches we take as individual learners. In §5 we present some of the techincal issues we faced with this approach. Finally in §6 we analyze the main observations that we made based on the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Motivations</head><p>We had participated in the previous edition of the author identification task of the PAN competition <ref type="bibr" coords="2,185.39,485.00,10.58,8.64" target="#b6">[7]</ref>. The system that we implemented then was using a genetic algorithm in order to find an optimal configuration of parameters for each dataset. The genetic algorithm gave good results, but was naturally prone to overfitting. This is why we had also prepared two distinct approaches: the fined-grained one, which contains a lot of parameters in order to maximize the performance, and the robust one, which was on the contrary a very simple method, intended to minimize the risk of overfitting. For each dataset, the final choice between the two approaches was made manually, based on the observations and statistics available to us.</p><p>Our system was ranked 4th overall, with 2nd or 3rd rank in 4 out of 6 datasets <ref type="bibr" coords="2,461.50,580.92,15.27,8.64" target="#b11">[12]</ref>. Given these promising results, we tried to improve the approach in three ways:</p><p>-Since the robust and fine-grained approaches in our previous system are naturally complementary, it seems relevant to combine their answers (instead of only choosing one or the other). -Furthermore, as part of their analysis of the participants systems, the organizers of PAN 2014 evaluated a meta-model based on all the individual systems answers. They found that " this meta-classifier was better than each individual submitted method while its ROC curve clearly outperformed the convex hull of all submitted approaches. This demonstrates the great potential of heterogeneous models in author verification, a practically unexplored area." <ref type="bibr" coords="3,345.41,215.22,15.27,8.64" target="#b11">[12]</ref>. This observation encouraged us to upgrade our system into a meta-learning system which combines the output of heterogeneous learners. -The General Impostor approach <ref type="bibr" coords="3,283.83,251.34,11.45,8.64" target="#b4">[5,</ref><ref type="bibr" coords="3,295.28,251.34,7.64,8.64" target="#b3">4]</ref> was a natural first candidate to use as individual learner, since it was the approach taken by the best system in PAN 2014 <ref type="bibr" coords="3,466.49,263.30,10.58,8.64" target="#b2">[3]</ref>.</p><p>We also implemented two additional strategies (see §4). -Finally, because of the genetic algorithm, the previous version of our system contained most of the basic components needed to generate multiple independent learners, which made it technically easier to take this quite complex approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">General Architecture</head><p>Our approach consists in training a set of 5 × N individual learners, with N learners obtained from each of the five distinct methods (we call them strategies) that we have implemented (see §4). Every model obtained at the end of this stage is a full "authorship verification system", in the sense that, given a set of problems as input, it provides a set of scores in [0, 1] as answers. Then the training of the meta-learner takes place, which consists in selecting the optimal subset of individual learners and the optimal way to combine their outputs together. Both training stages are carried out with a generic genetic algorithm, which evaluates models according to their performance obtained with cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Genetic Algorithm</head><p>We call configuration a set of parameter-value pairs:</p><formula xml:id="formula_0" coords="3,349.44,517.53,131.15,9.65">C = {p 1 → v 1 , . . . , p n → v n }.</formula><p>For each strategy, we define a set of parameters in such a way that a given configuration defines exactly how a model is learned (or applied, when associated with a corresponding model). In particular the configuration describes which features are used and how they are combined (possibly using a ML model). In training mode, a configuration C and a set of instances (problems) S define a model M in a unique way <ref type="foot" coords="3,473.84,575.96,3.49,6.05" target="#foot_2">6</ref> : f train (C, S) = M . In testing mode, a configuration C, a model M and an instance s define a unique prediction: f test (C, M, s) = p. The space of all possible configurations can be very large, depending on the strategy. This is why we use a genetic algorithm, in order to learn the optimal configurations.</p><p>Our genetic algorithm works with configurations as "individuals": each configuration describes the meta-parameters of a strategy. A multi-configuration associates multiple values to one parameter: 1  1 , . . . , v 1 m1 }, . . . , p n → {v n 1 , . . . , v n mn } Thus, a set of multi-configurations can be used to describe the set of meaningful combinations of parameters, in a way similar to a disjunctive normal form. <ref type="foot" coords="4,431.38,194.05,3.49,6.05" target="#foot_3">7</ref> This union of multi-configurations is the input of the genetic algorithm:</p><formula xml:id="formula_1" coords="4,190.39,166.81,72.47,9.65">M C = p 1 → {v</formula><p>-The first generation of configurations is initialized randomly: N configurations are selected among the possibilities described by the union of multi-configurations. -Then every generation is obtained based on the individual performance of the configurations from the previous generation:</p><p>• The "breeders" are selected in a way such that the probability of a configuration being selected is proportional to its rank by performance. • For every new configuration, two parents are selected randomly among the breeders and every parameter is assigned the value of either one of the parents value, with the possibility of mutation.</p><p>Additionally, the algorithm allows for a proportion of the new generation to be selected fully randomly, and for a proportion of the best previous configurations to be cloned to the next one (elitism). We had observed in <ref type="bibr" coords="4,357.07,364.11,11.62,8.64" target="#b6">[7]</ref> that the meta-parameters of the genetic algorithm do not have a major impact on its success or its speed. Thus we used the following fixed values:</p><p>-Proportion of selected breeders: 10%.</p><p>-Probability of mutation (by gene/parameter): 0.02.</p><p>-Proportion of cloned "elite" configurations: 10%.</p><p>-Proportion of new fully random configurations: 5%.</p><p>The convergence of the algorithm is tested by looking at the n latest windows of m generations (length), starting from the (n × m) th generation: the mean of the performance is computed for every window, and the stop criterion is met if the first window obtains the maximum performance (that is, the performance didn't improve on the last n -1 windows). This simple method offers some useful flexibility: depending on the characteristics of the process (e.g., size of the hypothesis space, time needed to compute a generation), the process can be set to favor speed or a more exhaustive exploration of the search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">ML Setting</head><p>The disadvantage of using a genetic algorithm, especially with a vast set of possible configurations, is the risk of overfitting. We use cross-validation inside the genetic algorithm: in other words, each configuration generated is evaluated using k-fold crossvalidation, and the resulting performance is used as the fitness function by the genetic algorithm. It is worth noticing that k-fold cross-validation outside the genetic process is not a good option for two reasons: first, the genetic process is very long, hence hardly practicable for any decent value of k; but it is also very unlikely that two distinct genetic processes would find the same optimal configurations, given the size of the search space <ref type="foot" coords="5,156.89,165.46,3.49,6.05" target="#foot_4">8</ref> and the fact that a genetic algorithm can only return a local optimum.</p><p>We use various techniques to keep overfitting to a minimum:</p><p>-The partitioning for the k-fold cross-validation is randomly (re-)generated at every generation.</p><p>-The system allows to chain multiple stages of genetic learning with different parameters, in particular different values of k and different values for the size and number of windows in the stop criterion. At every new stage, the previously selected set of configuration is used as first generation, and re-evaluated under the new parameters. This allows the process to check and progressively refine the optimal configurations and/or adjusting the trade-off between the precision of the process and the required computing power. -At the end of the last stage, the N best configurations are re-evaluated using a 10x2 cross-validation setting, in order to control the influence of the cross-validation partitioning on the performance variance.</p><p>At the end of the first stage, for every dataset we obtain N strategy configurations and their corresponding models for each of the five strategies. Then the second stage of meta-learning is carried out, using the the predicted scores of the strategy configurations as features and the same genetic algorithm. A special meta-configuration is built, which contains only:</p><p>-For each of the selected strategy configurations, a binary parameter which indicates whether its prediction is used or not (feature selection). -A parameter which indicates how the strategy predictions are combined. In order to avoid another potential source of overfitting (see discussion below), we restricted this combination to the safest methods: the algorithm can select only the arithmetic mean, geometric mean or the median. Of course, using a regression algorithm might provide better results. -A parameter which indicates whether to apply the C@1 optimization again on the output score (see §4).</p><p>We were hesitant as to what would be the most reliable approach regarding the use of instances:</p><p>-The most simple option consists in using a first subset of the instances (e.g. 50%)</p><p>for the strategy training stage, then the second one for the meta training stage. Optionally, the second set of instances can be split further in order to control for overfitting in the meta training stage with a new subset of fresh instances (e.g. 25% and 25%). -A proper nested cross-validation setting seems a methodologically more solid approach. There are, however, two major issues with that:</p><p>• Training the genetic algorithm for every strategy and every dataset (5 × 4 = 20 cases) requires a lot of time; multiplying that by k in the case of k-fold crossvalidation is likely to be prohibitive. • In the case of an inner cross-validation process with respect to the genetic algorithm, overfitting arises because, even if a given model at generation t is tested on instances that it did not see, the instances have been seen in the previous generations and influence greatly <ref type="foot" coords="6,316.39,189.50,3.49,6.05" target="#foot_5">9</ref> the configuration from which the model was generated. The case of outer cross-validation is discarded for the reasons explained above.</p><p>This is why we opted for an hybrid (maybe somewhat unorthodox) setting which consists in doing the three stages of the first option using 2-fold cross-validation. That is, in both runs:</p><p>1. The strategy genetic training uses inner cross-validation on 50 instances. As a result we obtain the 10 best models by strategy (50 models); 2. The meta training uses 25 fresh instances, in order to avoid the overfitting issue caused by inner cross-validation. We use only unsupervised combinations methods in order to minimize overfitting, which is very likely on such a small number of instances. 3. The best meta-models are evaluated on the fresh last 25 instances (as well as the best strategy models, mostly for comparison purposes). This part of the training data is called the meta test set.</p><p>The last stage of evaluation uses bagging (bootstrap aggregation): the models are evaluated 20 times against a different random 50% subset of the instances, so that we can control for performance variance. But there are still two problems left:</p><p>-The last evaluation stage is done on a small set of instances, and therefore not very reliable.</p><p>-The resulting models from the two cross-validation runs are not comparable together (see above). Moreover, it is likely that one of the runs will perform better in absolute value because of the different subsets they use for training and testing.</p><p>This is why we carry out an additional stage of evaluation of all the resulting models on the whole dataset, using bagging again. The rationales for this are: <ref type="bibr" coords="6,440.93,531.34,11.62,8.64" target="#b0">(1)</ref> this is a pragmatic workaround for the two issues above, and (2) the meta-models have not seen directly the first 50 instances used for the strategy training stage; testing proved that the performance on these instances was, in general, not overevaluated compared to the actual fresh instances of the meta test set. Of course, being aware of the potential biases, we do not consider the results of this final evaluation stage as totally reliable. We rather consider these as a useful indication of the models behaviour, that we study together with the results on the meta test set. This is why the very last stage of model selection is not currently automatic: we make the decision based on several statistics, the comparison to the other models and what we know about the possible biases.</p><p>There are probably other relevant options that we did not consider, due to lack of time or by ignorance. This naive and pragmatic approach is for us a rather reasonable first attempt at using multiple learners for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Individual Strategies</head><p>Five distinct strategies are used to generate the individual learners. Two of them, namely the robust and the fine-grained strategies, come from the system that we submitted at the previous PAN task (2014). A brief summary of these approaches is given below; for more details, see <ref type="bibr" coords="7,203.94,254.03,10.58,8.64" target="#b6">[7]</ref>.</p><p>-The fine-grained strategy contains many possible parameters. It is intended to try as many configurations as possible, in order to maximize the performance. -The robust strategy is a simpler method which uses only a small set of parameters.</p><p>It is intended to be safer (in particular less prone to overfitting), but probably not to perform as well as the fined-grained strategy.</p><p>We also implement three new strategies: GI, Universum Inference and Topic Detection, which are described below.</p><p>Technically, every strategy takes as input a set of problems with their answers (training mode) or a set of problems and a model (testing mode), and optionally some additional resources (e.g. impostors documents). As output, the strategy returns a set of features for every problem <ref type="foot" coords="7,240.46,396.64,6.97,6.05" target="#foot_6">10</ref> ; these features are fed to a ML algorithm <ref type="foot" coords="7,413.76,396.64,6.97,6.05" target="#foot_7">11</ref> , which is used to train a regression model (or to apply a previously trained model), using each problem as an instance ("Yes" answers are converted to 1, "No" to 0).</p><p>An additional optional step can be carried out, which consists in optimizing the answers in order to maximize the C@1 score, i.e. assign 0.5 scores to ambiguous cases. Two options are provided: the most simple option consists in finding the optimal accuracy threshold, then testing if assigning 0.5 to the instances with close scores improves the C@1 score. The second option consists in training an additional ML classification model, which tries to determine which cases are ambiguous based on the features and the predicted score.</p><p>Each application of a strategy is governed by a configuration (see §3.1) which defines not only the specific parameters for this strategy, but also some parameters which are common to all strategies. These include:</p><p>some general-purpose parameters, like the kind of observations (words or characters n-grams, POS n-grams, ...), the minimum frequency, etc. (see <ref type="bibr" coords="7,429.49,572.08,11.62,8.64" target="#b6">[7]</ref> for more details).</p><p>the ML algorithm to apply to the features and its hyper-parameters <ref type="foot" coords="8,418.82,117.64,6.97,6.05" target="#foot_8">12</ref> , and whether the optional C@1 optimization stage should be done, and if yes how.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">General Impostor (GI)</head><p>This method is described fully in <ref type="bibr" coords="8,270.51,194.21,10.58,8.64" target="#b4">[5]</ref>, used in previous PAN workshops by <ref type="bibr" coords="8,436.27,194.21,16.60,8.64" target="#b9">[10]</ref> and in a modified form by <ref type="bibr" coords="8,214.59,206.16,11.62,8.64" target="#b5">[6]</ref> and <ref type="bibr" coords="8,245.91,206.16,10.58,8.64" target="#b2">[3]</ref>. Similarly to these authors, we use the result of Google queries formed by words from the training set documents as impostors. We define the Python implementation of the method used in the system as follows.</p><p>Given a feature set of size N, a source text X, a target text Y and a set of impostors I = {Z 1 ....Z i } , we select a random percentage of features a of N and repeat this selection n times.</p><p>We then obtain for each system run a matrix P with dimensions (i + 1) × n where each cell is the value<ref type="foot" coords="8,250.28,290.80,6.97,6.05" target="#foot_9">13</ref> for a particular distance metric for a comparison (i + 1) (columns) between the source X and either the target Y or one of the set of impostors for each of n(rows), each row a randomly selected subset of N.</p><p>This distance metric employed in each run is either the cosine distance, Kullback-Leibler divergence or the Average Jaccard Index between two texts using a metric subset a of the full metric set N. The distance metric is randomly selected at the beginning of each run. The metric representation N used in a run of the system is also defined in a previous step. Finally the matrix values are combined into a small set of features based on various options, including the variants used in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Topic Modelling</head><p>The topic modelling is used to exhibit topics from the underlying text in a corpus. The intuition is that the Latent Dirichlet Allocation (LDA) <ref type="bibr" coords="8,383.30,464.32,11.62,8.64" target="#b0">[1]</ref> topic model will be able to exhibit the styles from the underlying text. Every author's text was split into different character n-grams, which were considered to represent the style of the author and LDA model was constructed using <ref type="bibr" coords="8,320.16,500.19,11.62,8.64" target="#b8">[9]</ref> to find 5 topics from each author's text; say A = {T A1 , T A2 , T A3 , T A4 , T A5 }, with each topic containing top 20 n-grams. Similarly, 5 topics were constructed from the document which is to be verified; say</p><formula xml:id="formula_2" coords="8,134.77,535.70,133.81,9.68">V = {T V 1 , T V 2 , T V 3 , T V 4 , T V 5 }</formula><p>with each topic containing the top 20 n-grams. Then, we used a overlap metric between each of the topics of A and V to find how close the topics of the author and the document to be verified are, in terms of the style. This got a matrix of overlap metric values for A × V . From this we obtained the overall mean of the overlap metrics as a feature for the genetic algorithm.</p><p>The use of topic modelling might seem counter-intuitive in this task. However, our idea is to let the genetic algorithm select the features used by the algorithm, hoping that it will select features which are relevant for style distinctiveness rather than topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Universum Inference</head><p>This strategy follows the idea described in <ref type="bibr" coords="9,305.15,139.75,15.49,8.64" target="#b12">[13]</ref>: in this paper, a large corpus containing several "categories" (known authors in our task) was split into small chunks. A chunk of category A was compared against many chunks from other categories and from category A as well, picked randomly. It was shown that a reliable measure of the category homogeneity can be derived from examining how different the level of similarity is between comparing A to A and comparing A to some distinct category X.</p><p>We adapted the approach to the case of smaller documents in the following way. Let A and B be two documents, the following process is repeated N times with different random subsets:</p><p>1. The two documents are split into three parts randomly: A 1 , A 2 , A 3 and B 1 , 2 , B 3 ; one of the thirds is split again into two parts: X 3 = X 3 , X 3 . 2. The pieces of text are re-organized under three categories, each containing two parts and all the parts being of similar size:</p><formula xml:id="formula_3" coords="9,139.25,293.57,341.34,32.97">C A = {A 1 , A 2 }; C B = {B 1 , B 2 }; C mixed = {A 3 ∪ B 3 , A 3 ∪ B 3 }. 3.</formula><p>The three categories are compared in the same way as <ref type="bibr" coords="9,369.78,317.90,15.27,8.64" target="#b12">[13]</ref>, that is, both against itself (using the two parts belonging to this category) and against each other category (picking one of the two parts randomly). 4. The general idea consists in measuring how much confusion there is between the categories: assuming that the goal is to correctly classify each category to its own category, more classification errors means more similar documents. This can be achieved numerically with different methods (a particular method is selected by the configuration parameters).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Practical Issues</head><p>During the development, training and testing of our system, we faced a number of practical issues. These issues probably had an impact on the quality of the results, but it is impossible to properly measure to what extent. Of course, the most important constraint was the time schedule of the competition. We started the development quite late. For the most part, the system was coded during the time that the competition was taking place. This left relatively little time to train the 40 runs of the genetic algorithm to their best (4 datasets × 5 strategies × 2-fold crossvalidation). Thus, it is possible that for some complex strategies we did not reach the optimal models<ref type="foot" coords="9,196.67,552.58,6.97,6.05" target="#foot_10">14</ref> ; more importantly, we did not have enough time to make the models run through enough different stages of cross-validation under restrictive convergence conditions (see §3.1), which could have provided more stability in the models.</p><p>It is difficult to evaluate precisely the amount of computation that we gave to the whole training process, because we used two distinct machines and there were interruptions during the process. We evaluate it roughly between 150 and 300 CPU hours for each of the 40 genetic processes.</p><p>6 Results on the Training Set and Observations</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Strategy Observations</head><p>Comparing the performance of the five strategies (see 4) shows little surprise; it confirms relatively well the differences between those strategies and their complementarity:</p><p>-The GI strategy performs very well in general, and its low variance makes it especially robust. -The Universum Inference strategy performs well and is even the best individual strategy on two (Dutch and English); it is however quite unstable. -The robust strategy keeps its promises: albeit low performance in general, it is practically insensitive to overfitting, showing very close performance on training instances and fresh instances. -The fine-grained strategy is vey unstable, but occasionally performs very well.</p><p>-The Topic Modeling strategy does not seem to work very well in general. <ref type="foot" coords="10,442.30,291.80,6.97,6.05" target="#foot_11">15</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Global Results</head><p>Table <ref type="table" coords="10,160.01,343.71,4.98,8.64" target="#tab_0">1</ref> shows some statistics about the performance of the selected meta-models on both the meta test set and the whole training set set<ref type="foot" coords="10,341.05,353.99,6.97,6.05" target="#foot_12">16</ref> ; the final column shows the performance on the actual test set.</p><p>The selected models were not necessarily the best performing on the meta-test set (or on all the instances). Our choice of the final model was mostly based on finding the most stable model among the most performant ones. We took standard deviation into account, as well as the difference in performance between the two sets (interpreting a large difference as an indication that the model is not stable). It is worth noticing that the relative performance demonstrates fairly well the most visible difficult characterstics of the dataset. For example, the English dataset contains only one known document for every problem and has the smallest documents (in average), and it is also the most difficult to predict for the system. On the contrary, there are always four known documents by problem and the documents are the largest for the Spanish dataset, and the system performs very well with it.</p><p>The system had the worst difficulties in the case of the English dataset. It might be because, as a strongly supervised system, it does not cope well with scarcity of information. It is also possible that a more complete training would have helped in finding the relevant clues in this more difficult case.</p><p>Finally, in most cases the results on the test set are close (within the standard deviation) to the ones obtained on the traning set. This tends to indicate that the approach is good at preventing overfitting. Even in the case of the visibly overfit Spanish model, the results do not fall drastically compared to the other participating systems; this moderation of the effect of overfitting is probably explained by the use of multiple learners, the overfit ones being partly compensated by the others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Future work</head><p>There is a lot of room for improvement in our current system: improving the current strategies, adding new strategies, solving efficiency issues, etc. In our opinion, the most interesting question to study is certainly how best to combine the predictions of the individual learners, in the context of a relatively small training set which is expectable in authorship verification. In the current version, the ML setting is not entirely satisfying (see §3.2). Various options could be explored in the future:</p><p>-Generating artificial instances from the existing ones, for instance by splitting original documents into multiple sub-documents. This option might not be completely safe, since artificial cases might be less useful in assessing the reliability of a model, and maybe even misleading. -Finding a way to combine the best models provided by multiple runs of the genetic algorithm. In theory, these models share some similarities, and these similarities correspond to the features that matter, those which allow the model to perform well in a reliable way (the other features can be seen as the result of statistical noise). It is however not necessarily possible to safely "merge" two distinct configurations. 17 Some of the calculations were performed on the Lonsdale cluster maintained by the Trinity Centre for High Performance Computing. This cluster was funded through grants from Science Foundation Ireland.</p><p>Fitting our complex and poorly efficient system to the tira.io framework proved challenging, and we are very grateful to the organizers for their patience and availability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="10,142.74,458.83,329.88,95.64"><head>Table 1 .</head><label>1</label><figDesc>Performance observed on the traning set and test set</figDesc><table coords="10,142.74,480.15,329.88,74.32"><row><cell>Dataset</cell><cell></cell><cell cols="2">Meta test set</cell><cell></cell><cell cols="2">Full training set</cell><cell cols="2">Test set</cell></row><row><cell></cell><cell>mean</cell><cell>median</cell><cell>std. dev.</cell><cell>mean</cell><cell>median</cell><cell>std. dev.</cell><cell>perf.</cell><cell>rank</cell></row><row><cell>Dutch</cell><cell>0.710</cell><cell>0.716</cell><cell>0.107</cell><cell>0.722</cell><cell>0.727</cell><cell>0.087</cell><cell>0.635</cell><cell>1st</cell></row><row><cell>English</cell><cell>0.405</cell><cell>0.400</cell><cell>0.117</cell><cell>0.421</cell><cell>0.420</cell><cell>0.064</cell><cell>0.453</cell><cell>6th</cell></row><row><cell>Greek</cell><cell>0.656</cell><cell>0.671</cell><cell>0.110</cell><cell>0.761</cell><cell>0.765</cell><cell>0.042</cell><cell>0.693</cell><cell>2nd</cell></row><row><cell>Spanish</cell><cell>0.950</cell><cell>0.917</cell><cell>0.042</cell><cell>0.952</cell><cell>0.946</cell><cell>0.024</cell><cell>0.661</cell><cell>4th</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Macro-average</cell><cell>0.610</cell><cell>2nd</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="2,144.73,613.06,335.86,7.77;2,144.73,623.73,335.87,8.06;2,144.73,634.98,111.42,7.77"><p>The general task definition is closer to a classification problem, the output being either "yes" or "no". However, since the system has to provide a score in the range [0, 1] instead, it is treated as a regression task in practice.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="2,144.73,646.13,335.86,7.77;2,144.73,657.08,270.99,7.77"><p>It is important to notice that this approach is computationally expensive. We address the task in the perspective of accuracy only, mostly ignoring the efficiency criterion.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2" coords="3,144.73,646.13,335.86,7.77;3,144.73,657.08,211.48,7.77"><p>This is not entirely true because of the randomness in some strategies. It is however theoretically true in the limit, that is with sufficiently random runs.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3" coords="4,144.73,646.13,335.86,7.77;4,144.73,657.08,194.80,7.77"><p>In practice, however, we have used only one multi-configuration by strategy, leaving the selection of relevant combinations to the genetic algorithm.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_4" coords="5,144.73,656.80,135.13,8.06;5,279.86,655.03,7.31,5.24;5,289.90,657.08,109.79,7.77"><p>From 124, 000 (robust strategy) to 10 21 (fine-grained strategy); see §4.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_5" coords="6,144.73,635.17,335.86,7.77;6,144.73,646.13,335.86,7.77;6,144.73,657.08,335.86,7.77"><p>We have tested this option and the results of the meta training stage were very bad: the resulting meta learner would expect too much reliability in the scores ("too good" scores) it uses as features, hence it performed badly on regular ("mediocre") scores, as obtained on unseen data.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_6" coords="7,144.73,602.10,335.86,7.77;7,144.73,613.06,335.86,7.77;7,144.73,624.02,162.78,7.77"><p>Currently we use only numerical features. The number of features depends on the strategy and its parameters, but is generally around 5 and 15, since more features would be likely to overfit the model given the low number of instances.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_7" coords="7,144.73,635.17,335.86,7.77;7,144.73,646.13,335.86,7.77;7,144.73,657.08,145.30,7.77"><p>It is also possible for a strategy to output a single value by problem, to be used directly as the score; we do not use this possibility in the current version. As regression algorithms we use only SVM and decision trees regression.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_8" coords="8,144.73,645.94,214.56,7.77"><p>We actually restrict these to a very small set of possibilities.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_9" coords="8,144.73,657.08,82.10,7.77"><p>To five decimal places.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_10" coords="9,144.73,646.13,335.86,7.77;9,144.73,657.08,191.75,7.77"><p>Additionally, we discovered after the competition that a bug caused a disruption in the genetic process in the case of the Dutch and English datasets.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_11" coords="10,144.73,624.02,335.86,7.77;10,144.73,634.98,335.86,7.77;10,144.73,645.94,335.86,7.77"><p>It has however been selected as individual learner in several cases in the meta-learning stage; thus, such models might actually bring a different kind of information which turns out to be useful to the meta-model (but we did not have time to investigate this question in more detail).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_12" coords="10,144.73,657.08,327.60,7.77"><p>See §3.2 for explanations about the setting in which these performance values are obtained.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17" xml:id="foot_13" coords="11,144.73,613.25,335.86,7.77;11,144.73,624.21,335.86,7.77;11,144.73,635.17,335.86,7.77;11,144.73,646.13,335.86,7.77;11,144.73,657.08,261.01,7.77"><p>This option could also be considered in the perspective of simply measuring how similar two configurations are: based on such a similarity measure, instead of "merging" heterogeneous configurations, the system could select configurations based on their similarity across different runs. The same idea could be used to maximize the diversity of the selected strategy models, which is likely to be a decisive feature in the success of the meta-learner.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research is supported by <rs type="funder">Science Foundation Ireland</rs> through the <rs type="programName">CNGL Programme</rs> (Grant <rs type="grantNumber">12/CE/I2267</rs>) in the <rs type="funder">ADAPT Centre</rs> (www.adaptcentre.ie) at <rs type="institution">Trinity College, University of Dublin</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_wt2PrNM">
					<idno type="grant-number">12/CE/I2267</idno>
					<orgName type="program" subtype="full">CNGL Programme</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,142.61,228.55,314.99,7.77;12,150.95,239.51,197.79,7.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,278.55,228.55,91.46,7.77">Latent dirichlet allocation</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,387.88,228.55,69.72,7.77;12,150.95,239.51,65.81,7.77">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-03">March 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,250.47,320.47,7.77;12,150.95,261.43,315.15,7.77;12,150.95,272.39,206.79,7.77" xml:id="b1">
	<analytic>
		<ptr target="http://ceur-ws.org/Vol-1180" />
	</analytic>
	<monogr>
		<title level="m" coord="12,351.72,250.47,111.35,7.77;12,150.95,261.43,39.61,7.77">Working Notes for CLEF 2014 Conference</title>
		<title level="s" coord="12,339.24,261.43,107.26,7.77">CEUR Workshop Proceedings</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Halvey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</editor>
		<meeting><address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">September 15-18, 2014. 2014</date>
			<biblScope unit="volume">1180</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,283.35,316.68,7.77;12,150.95,294.31,281.91,7.77" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="12,228.74,283.35,230.54,7.77;12,150.95,294.31,38.30,7.77">A slightly-modified gi-based author-verifier with lots of features (ASGALF)</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Khonji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Iraqi</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-1180" />
		<editor>Cappellato et al.</editor>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="977" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,305.27,334.89,7.77;12,150.95,316.22,318.76,7.77;12,150.95,327.18,320.54,7.77;12,150.95,338.14,271.45,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,244.04,305.27,172.02,7.77">Automatically identifying pseudepigraphic texts</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Seidman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,433.68,305.27,43.83,7.77;12,150.95,316.22,318.76,7.77;12,150.95,327.18,16.14,7.77">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013<address><addrLine>Grand Hyatt Seattle, Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2013-10-21">18-21 October 2013. 2013</date>
			<biblScope unit="page" from="1449" to="1454" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct coords="12,142.61,349.10,317.11,7.77;12,150.95,360.06,325.09,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,237.17,349.10,219.02,7.77">Determining if two documents are written by the same author</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Winter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,150.95,360.06,241.42,7.77">Journal of the Association for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="178" to="187" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,371.02,323.53,7.77;12,150.95,381.98,329.12,7.77;12,150.95,392.94,181.60,7.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,459.66,371.02,6.47,7.77;12,150.95,381.98,228.37,7.77">A single author style representation for the author verification task</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Mayor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gutierrez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Toledo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ledesma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Fuentes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Meza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,397.11,381.98,82.97,7.77;12,150.95,392.94,155.45,7.77">CLEF 2014 Evaluation Labs and Workshop-Online Working Notes</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,403.90,337.98,7.77;12,150.95,414.85,304.26,7.77;12,150.95,425.81,329.64,7.77;12,150.95,436.77,274.76,7.77" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,277.90,403.90,202.69,7.77;12,150.95,414.85,221.52,7.77">Author Verification: Exploring a Large set of Parameters using a Genetic Algorithm -Notebook for PAN at CLEF 2014</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jayapal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Vogel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,285.57,425.81,152.98,7.77">Working Notes for CLEF 2014 Conference</title>
		<title level="s" coord="12,173.37,436.77,107.26,7.77">CEUR Workshop Proceedings</title>
		<editor>
			<persName><forename type="first">Linda</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Kraaij</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename></persName>
		</editor>
		<meeting><address><addrLine>Sheffiled, United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-09">Sep 2014</date>
			<biblScope unit="volume">1180</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,447.73,325.86,7.77;12,150.95,458.69,298.61,7.77;12,150.95,469.65,288.27,7.77;12,150.95,480.61,157.21,7.77" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,236.87,447.73,146.56,7.77">A simple measure to assess non-response</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rodrigo</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P11-1142" />
	</analytic>
	<monogr>
		<title level="m" coord="12,401.73,447.73,66.74,7.77;12,150.95,458.69,236.49,7.77">Proceedings of the 49th Annual Meeting of the ACL: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the ACL: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011-06">June 2011</date>
			<biblScope unit="page" from="1415" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,491.57,307.09,7.77;12,150.95,502.53,197.07,7.77" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">X</forename><forename type="middle">H</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<ptr target="http://gibbslda.sourceforge.net/" />
		<title level="m" coord="12,252.65,491.57,197.05,7.77;12,150.95,502.53,35.36,7.77">gibbslda++: A c/c++ implementation of latent dirichlet allocation</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,513.49,300.25,7.77;12,150.95,524.44,222.46,7.77" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,198.78,513.49,183.00,7.77">Authorship verification using the impostors method</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Seidman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,400.39,513.49,42.10,7.77;12,150.95,524.44,196.32,7.77">CLEF 2013 Evaluation Labs and Workshop-Online Working Notes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,535.40,323.31,7.77;12,150.95,546.36,309.68,7.77;12,150.95,557.32,306.50,7.77;12,150.95,568.28,298.91,7.77" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,186.83,546.36,182.63,7.77">Overview of the Author Identification Task at PAN</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Daelemans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Verhoeven</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Juola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lopez Lopez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
	</analytic>
	<monogr>
		<title level="m" coord="12,406.32,546.36,54.32,7.77;12,150.95,557.32,149.87,7.77">Working Notes Papers of the CLEF 2015 Evaluation Labs</title>
		<title level="s" coord="12,307.00,557.32,150.46,7.77;12,150.95,568.28,19.77,7.77">CEUR Workshop Proceedings, CLEF and CEUR</title>
		<imprint>
			<date type="published" when="2015-09">2015. Sep 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,579.24,294.19,7.77;12,150.95,590.20,311.90,7.77;12,150.95,601.16,278.35,7.77" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,304.84,590.20,158.02,7.77;12,150.95,601.16,17.11,7.77">Overview of the author identification task at PAN</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Daelemans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Verhoeven</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Juola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Sánchez-Pérez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-1180" />
	</analytic>
	<monogr>
		<title level="m" coord="12,204.92,601.16,58.52,7.77">Cappellato et al</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="877" to="897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,612.12,338.35,7.77;12,150.95,623.07,325.76,7.77;12,150.95,634.03,136.48,7.77" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,270.79,612.12,162.74,7.77">Universum inference and corpus homogeneity</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Janssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,275.26,623.07,196.32,7.77">Research and Development in Intelligent Systems XXV</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Bramer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Petridis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Coenen</surname></persName>
		</editor>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="367" to="372" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
