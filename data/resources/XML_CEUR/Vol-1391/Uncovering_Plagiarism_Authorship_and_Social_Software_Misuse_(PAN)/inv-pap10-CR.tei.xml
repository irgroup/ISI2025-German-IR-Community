<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,180.28,115.90,254.80,12.90;1,165.49,133.83,284.37,12.90">Source Retrieval for Plagiarism Detection from Large Web Corpora: Recent Approaches</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,207.23,171.88,61.11,8.64"><forename type="first">Matthias</forename><surname>Hagen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,275.57,171.88,60.37,8.64"><forename type="first">Martin</forename><surname>Potthast</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,359.15,171.88,48.99,8.64"><forename type="first">Benno</forename><surname>Stein</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,180.28,115.90,254.80,12.90;1,165.49,133.83,284.37,12.90">Source Retrieval for Plagiarism Detection from Large Web Corpora: Recent Approaches</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">59D19810E797D275CC6564AF5AA129D3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper overviews the five source retrieval approaches that have been submitted to the seventh international competition on plagiarism detection at PAN 2015. We compare the performances of these five approaches to the 14 methods submitted in the two previous years (eight from PAN 2013 and six from PAN 2014). For the third year in a row, we invited software submissions instead of run submissions, such that cross-year evaluations are possible. This year's stand-alone source retrieval overview can thus to some extent also be used as a reference to the different ideas presented in the last three years-the text alignment subtask will be depicted in another individual overview.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The retrieval and extraction of text reuse from large document collections is central to applications such as plagiarism detection, copyright protection, and information flow analysis. Appropriate algorithms have to be able to deal with all kinds of text reuse ranging from verbatim copies and quotations to paraphrases and translations to summaries <ref type="bibr" coords="1,164.64,448.44,15.27,8.64" target="#b10">[12]</ref>. Particularly the latter kinds of text reuse still present a real challenge to both engineering and evaluation of retrieval algorithms. Until recently, one of the primary obstacles to the development of new algorithms has been a lack of evaluation resources. To rectify this lack and to enable the source retrieval subtask, we have worked on a high-quality, large-scale evaluation framework <ref type="bibr" coords="1,344.03,496.26,15.77,8.64" target="#b15">[17,</ref><ref type="bibr" coords="1,362.63,496.26,11.83,8.64" target="#b16">18]</ref>, that has been used in the last four years. <ref type="foot" coords="1,193.69,506.54,3.49,6.05" target="#foot_0">1</ref>The source retrieval subtask has been running for four years in a row now and we can observe the standard multi-year life cycle of repeated shared tasks. Basically, there are three phases: an innovation phase, a consolidation phase, and a production phase. In the innovation phase, new evaluation resources are being developed and introduced for the first time, such as new corpora, new performance measures, and new technologies. The introduction of such new resources typically stirs up a lot of dust and is prone to errors and inconsistencies that may spoil evaluation results to some extent. This cannot be avoided, since only the use of new evaluation resources by many different parties will . Generic retrieval process to detect plagiarism <ref type="bibr" coords="2,399.14,252.32,13.74,7.77" target="#b23">[25]</ref>.</p><p>reveal their shortcomings. Therefore, the evaluation resources are released only sparingly so they last for the remainder of a cycle. This phase spanned the first and also to some extent the second year of the source retrieval subtask. In the consolidation phase, based on the feedback and results obtained from the first phase, the new evaluation resources are developed to maturity by making adjustments and fixing errors. This phase spanned the second and to some extent the third year of the source retrieval subtask. In the production phase, the task is repeated with little changes to allow participants to build upon and to optimize against what has been accomplished, and, to make the most of the prior investment in developing the new evaluation resources. Meanwhile, new ideas are being developed to introduce further innovation.</p><p>This third production phase in part could be observed in last year's third edition of the source retrieval subtask and was the motivation behind organizing the subtask for a fourth time. However, as will be described in more detail later, no real progress in the probably most important directions for a source retrieval method (i.e., improving the recall of reused sources and minimizing the effort until the first evidence for reuse is detected) can be observed in this year's submission. New querying strategies might be the key but no participant developed new ideas in that direction. Before further elaborating the different submitted approaches of this year's edition and the respective evaluation results, we describe the task setting and test environment in a self-contained way (note again that a lot of the respective passages have already been contained in the previous years' overview papers).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Setting this Overview's Scene</head><p>Terminology. Figure <ref type="figure" coords="2,219.19,567.82,4.98,8.64" target="#fig_0">1</ref> shows a generic retrieval process to detect plagiarism in a given suspicious document d plg , when also given a (very large) document collection D of potential source documents. This process is also referred to as external plagiarism detection since plagiarism in d plg is detected by searching for text passages in D that are highly similar to text passages in d plg . <ref type="foot" coords="2,291.12,613.97,3.49,6.05" target="#foot_1">2</ref> The process is divided into three basic steps, which are typically implemented in most plagiarism detectors. First, source retrieval, which identifies a feasible set of candidate source documents D src ⊆ D that are likely sources for plagiarism regarding d plg -this is the problem tackled in the source retrieval subtask. Second, text alignment, where each candidate source document d src ∈ D src is compared to d plg , extracting all passages of text that are highly similar-this is the problem tackled in the text alignment subtask described in another overview paper. Third, knowledge-based post-processing, where the extracted passage pairs are cleaned, filtered, and possibly visualized for later inspection-this is not really reflected in the PAN plagiarism subtasks so far.</p><p>Shared Tasks on Plagiarism Detection. We have organized shared tasks on plagiarism detection annually since 2009. In the innovation phase of our shared task at PAN 2009 <ref type="bibr" coords="3,179.88,256.37,15.27,8.64" target="#b19">[21]</ref>, we developed the first standardized evaluation framework for plagiarism detection <ref type="bibr" coords="3,195.41,268.32,15.27,8.64" target="#b18">[20]</ref>. This framework was consolidated in the second and third task at PAN 2010 and 2011 <ref type="bibr" coords="3,220.05,280.28,15.77,8.64" target="#b11">[13,</ref><ref type="bibr" coords="3,238.83,280.28,11.83,8.64" target="#b12">14]</ref>, and it has since entered the production phase while being adopted by the community. Our initial goal with this framework was to evaluate the process of plagiarism detection depicted in Figure <ref type="figure" coords="3,354.99,304.19,4.98,8.64" target="#fig_0">1</ref> as a whole. We expected that participants would implement source retrieval algorithms as well as text alignment algorithms and use them as modules in their plagiarism detectors. However, the results of the innovation phase proved otherwise, since participants implemented only text alignment algorithms, whereas they resorted to exhaustively comparing all pairs of documents within our evaluation corpora, even when the corpora were tens of thousands of documents large. To establish source retrieval as a shared task of its own, we introduced it at PAN 2012 next to the text alignment task <ref type="bibr" coords="3,321.83,387.87,15.27,8.64" target="#b13">[15]</ref>, thus entering a new task life cycle for this task. We developed a new, large-scale evaluation corpus of essay-length plagiarism cases that have been written manually, and whose sources have been retrieved manually from the ClueWeb09 corpus <ref type="bibr" coords="3,289.85,423.74,15.27,8.64" target="#b16">[18]</ref>. Given our above observation from the text alignment task, the ClueWeb09 was deemed too large to be exhaustively compared to a given suspicious document in a reasonable time. Furthermore, we developed a new search engine for the ClueWeb09 called ChatNoir <ref type="bibr" coords="3,337.66,459.61,15.27,8.64" target="#b15">[17]</ref>, which serves participants who do not wish to develop their own ClueWeb09 search engine as a means of participation. We then offered source retrieval as an individual task based on the new evaluation resources <ref type="bibr" coords="3,166.79,495.47,15.77,8.64" target="#b13">[15,</ref><ref type="bibr" coords="3,184.73,495.47,11.83,8.64" target="#b14">16]</ref>, whereas this year marks the fourth time we do so, and the continuation of the source retrieval task's production phase.</p><p>Contributions. Since the source retrieval subtask probably now is in the production phase of its life cycle, we refrain from changing the existing evaluation resources too much, whereas we continue to maintain them. Therefore, our contributions this year consist of (1) a survey of this year's submitted approaches, which reveals that there are hardly any new trends among participants in the source retrieval subtask, and (2) an analysis of this years participants' retrieval performances in direct comparison to participants from previous years, which reveals that no real progress in the important direction of increased recall can be observed.</p><p>In this connection, our goal with both shared tasks is to further automate them. Hence, we continue to develop the TIRA evaluation platform <ref type="bibr" coords="3,379.88,632.53,10.79,8.64" target="#b4">[5,</ref><ref type="bibr" coords="3,393.08,632.53,7.19,8.64" target="#b5">6]</ref>, which gives rise to software submissions with minimal organizational overhead and secures the execution of untrusted software while making the release of the test corpora unnecessary <ref type="bibr" coords="3,446.12,656.44,10.58,8.64" target="#b3">[4]</ref>. Like last year, the fully-fledged web service as a user interface enables the participants to remote control their evaluations on the test corpus under our supervision. Within this framework, we will probably enable further evaluations of new approaches but probably we will for now refrain from organizing a fifth edition of a source retrieval subtask at PAN 2016.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Source Retrieval Evaluation Framework</head><p>In source retrieval, given a suspicious document and a web search engine, the task is to retrieve all source documents from which text has been reused whilst minimizing retrieval costs. The cost-effectiveness of plagiarism detectors in this task is important since using existing search engines is perhaps the only feasible way for researchers as well as small and medium-sized businesses to implement plagiarism detection against the web, whereas search companies charge considerable fees for automatic querying their APIs.</p><p>In what follows, we briefly describe the building blocks of our evaluation setup, provide brief details about the evaluation corpus, and discuss the performance measures (see the task overview from 2013 for more details on these three points <ref type="bibr" coords="4,422.62,330.52,14.94,8.64" target="#b14">[16]</ref>). We then survey the submitted softwares in Section 4, and finally in Section 5, report on their achieved results in this year's setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Evaluation Setup</head><p>For the evaluation of source retrieval from the web, we consider the real-world scenario of an author who uses a web search engine to retrieve documents in order to reuse text from them in their to-be-written text. A plagiarism detector typically uses a search engine, too, to find reused sources of a given document. Over the past years, we assembled the necessary building blocks to allow for a meaningful evaluation of source retrieval algorithms; Figure <ref type="figure" coords="4,211.69,464.02,4.98,8.64" target="#fig_1">2</ref> shows how they are connected. The setup was described in much more detail in the task overview of 2013 <ref type="bibr" coords="4,298.02,475.98,15.27,8.64" target="#b14">[16]</ref>.</p><p>Two main components are the TIRA experimentation platform and the ClueWeb09 with two associated search engines. TIRA <ref type="bibr" coords="4,309.80,499.89,11.62,8.64" target="#b4">[5]</ref> itself consists of a number of building blocks; one of them, depicted in Figure <ref type="figure" coords="4,292.27,511.84,4.98,8.64" target="#fig_1">2</ref> bottom left, facilitates both platform independent software development and software submissions at the same time by its capability to create and remote control virtual machines on which our lab's participants deploy their source retrieval systems.</p><p>The ClueWeb corpus 2009 (ClueWeb09) <ref type="foot" coords="4,314.79,557.99,3.49,6.05" target="#foot_2">3</ref> is one of the most widely adopted web crawls which is regularly used for large-scale web search-related evaluations. It consists of about one billion web pages, half of which are English ones. Although an updated version of the corpus has been released, <ref type="foot" coords="4,289.64,593.86,3.49,6.05" target="#foot_3">4</ref> our evaluation is still based on the 2009 version since our corpus of suspicious documents was built on top of ClueWeb09. Indri ChatNoir <ref type="bibr" coords="5,175.26,430.85,16.60,8.64" target="#b15">[17]</ref> are currently the only publicly available search engines that index the ClueWeb09 corpus; their retrieval models are based on language modeling and BM25F, respectively. For developer convenience, we also provide a proxy server which unifies the APIs of the search engines. At the same time, the proxy server logs all accesses to the search engines for later performance analyses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation Corpus</head><p>The evaluation corpus employed for source retrieval is based on the Webis text reuse corpus 2012 (Webis-TRC-2012) <ref type="bibr" coords="5,268.19,540.44,15.77,8.64" target="#b17">[19,</ref><ref type="bibr" coords="5,287.20,540.44,11.83,8.64" target="#b16">18]</ref>. The corpus consists of 297 documents that have been written by 27 writers who worked with our setup as shown in the first row of Figure <ref type="figure" coords="5,174.49,564.35,3.88,8.64" target="#fig_1">2</ref>: given a topic, a writer used ChatNoir to search for source material on that topic while preparing a document of 5700 words length on average, reusing text from the found sources.</p><p>As in the last year, we use the same 98 documents from the Webis-TRC-2012 as training documents and another 99 documents are sampled as test documents-also the same as in 2014. The remainder of the corpus will be used within future source retrieval evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Performance Measures</head><p>Given a suspicious document d plg that contains passages of text that have been reused from a set of source documents D src , we measure the retrieval performance of a source retrieval algorithm in terms of precision and recall of the retrieved documents D ret taking into account the effect of near-duplicate web documents as follows (cf. the 2013 task overview <ref type="bibr" coords="6,173.78,187.06,16.60,8.64" target="#b14">[16]</ref> for more details).</p><p>For any d ret ∈ D ret , we employ a near-duplicate detector to judge whether it is a true positive detection; i.e., whether there is a d src ∈ D src of d plg that is a nearduplicate of d ret . We say that d ret is a true positive detection for a given pair of d src and d plg iff (1) d ret = d src (equality), or (2) the Jaccard similarity of the word n-grams in d ret and d src is above 0.8 for n = 3, above 0.5 for n = 5, and above 0 for n = 8 (similarity), or (3) the passages in d plg known to be reused from d src are contained in d ret (containment). Here, containment is measured as asymmetrical set overlap of the passages' set of word n-grams regarding that of d ret , so that the overlap is above 0.8 for n = 3, above 0.5 for n = 5, and above 0 for n = 8. This three-way approach of determining true positive detections inherently entails inaccuracies. While there is no straightforward way to solve this problem, this error source affects all detectors, still allowing for relative comparisons.</p><p>Let d dup denote a near-duplicate of a given d src that would be considered a true positive detection according to the above conditions. Note that every d src may have more than one such near-duplicate and every d dup may be a near-duplicate of more than one source document. Based on these sets, we define precision and recall of D ret regarding D src and d plg as follows:</p><formula xml:id="formula_0" coords="6,207.93,481.25,199.49,23.23">prec = |D ret ∩ D src | |D ret | , r ec = |D ret ∩ D src | |D src | .</formula><p>Rationale for this definition is that retrieving more than one near-duplicate of a source document does not decrease precision, but it does not increase recall, either, since no additional source is obtained. A further graphical explanation of how we take nearduplicates into account for precision and recall is given in Figure <ref type="figure" coords="6,400.38,547.23,3.74,8.64" target="#fig_4">3</ref>. Note that D ret as defined above does not actually contain all duplicates of the retrieved documents, but only those that are already part of D src . Finally, to measure the cost-effectiveness of a source retrieval algorithm in retrieving D ret , we count the numbers of queries and downloads made and compute the workload in terms of queries and downloads until the first true positive detection is made (i.e., until the first real evidence for text reuse is found). The last measure highlights the probable end user needs that some evidence should be found fast in order to quickly flag such a suspicious document for a further detailed analysis.  The Source Oracle To allow for participation in the source retrieval task without the need of having a text alignment component at hand, we provide a source oracle that automatically enriches a downloaded document with information about whether or not it is considered a true positive source for the given suspicious document. Note that the oracle employs the aforementioned conditions to determine whether a document is a true positive detection. However, the oracle does not, yet, tell for which part of a suspicious document a downloaded document is a true positive detection. Hence, applying a custom text alignment strategy can still be beneficial to derive such a mapping and potentially to adjust the query strategy accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Survey of Retrieval Approaches Submitted for PAN 2015</head><p>Five teams submitted softwares for the source retrieval task, all of whom also submitted a notebook describing their approach-the approaches of Han <ref type="bibr" coords="7,403.82,445.28,11.62,8.64" target="#b8">[9]</ref> and Kong et al.</p><p>[10] are described in the same notebook. An analysis of the individual descriptions reveals the same building blocks that were commonly used in last years' source retrieval algorithms: (1) chunking, (2) keyphrase extraction, (3) query formulation, (4) search control, and (5) download filtering. Some participants only slightly changed their approach from the previous year or adopted ideas from other approaches; in what follows, we describe the employed ideas in a little more detail.</p><p>Chunking Given a suspicious document, it is divided into (possibly overlapping) passages of text. Each chunk of text is then processed individually. Rationale for chunking the suspicious document is to evenly distribute "attention" over a suspicious document so that algorithms employed in subsequent steps are less susceptible to unexpected characteristics of the suspicious document.</p><p>The chunking strategies employed by this year's participants are no chunking (i.e., the whole document as one chunk) <ref type="bibr" coords="7,288.93,607.42,15.27,8.64" target="#b24">[26]</ref>, 500-word chunks <ref type="bibr" coords="7,388.87,607.42,16.60,8.64" target="#b20">[22]</ref> (overlap size not detailed), paragraphs as chunks <ref type="bibr" coords="7,269.74,619.37,16.60,8.64" target="#b21">[23]</ref> (not detailed how paragraphs are split), paragraphs split at empty lines as chunks <ref type="bibr" coords="7,293.30,631.33,15.27,8.64" target="#b24">[26]</ref>, or individual sentences and headings as chunks <ref type="bibr" coords="7,165.48,643.28,15.77,8.64">[10,</ref><ref type="bibr" coords="7,183.74,643.28,7.19,8.64" target="#b8">9]</ref>.</p><p>Note that chunks typically seem to be implemented as non-overlapping by the participating approaches. The potentially interesting question of whether overlapping chunks might help was not really tackled by any approach so far-except that Suchomel and Brandejs <ref type="bibr" coords="8,190.44,155.18,16.60,8.64" target="#b24">[26]</ref> use different types of chunks in combination. A problem with nonoverlapping longer chunks might be that typical plagiarism cases have no fixed length and overlapping chunks might reduce the risk of, for instance, having more than one source in one chunk of 500 words. Furthermore, relying on the given document structure (e.g., chunking by lines or paragraphs) bears the risk of failing for some unseen documents that are not as well-formatted as the ones in our evaluation corpus (e.g., all the text in a single line). Maybe mixed chunking strategies as seen in Suchomel and Brandejs <ref type="bibr" coords="8,172.63,238.86,15.93,8.64" target="#b24">[26]</ref>' approach is the most promising direction. Notably, their document level queries seem to also guarantee an early recall measured in queries (cf. Section 5) and mixed chunking probably at least will not decrease total recall. Keyphrase Extraction Given a chunk, "keyphrases" are extracted from it in order to formulate queries with them. Rationale for keyphrase extraction is to select only those phrases (or words) which maximize the chance of retrieving source documents matching the suspicious document. Keyphrase extraction may also serve as a means to limit the amount of queries formulated, thus reducing the overall costs of using a search engine. This step is perhaps the most important one of a source retrieval algorithm since the decisions made here directly affect the overall performance: the fewer keywords are extracted, the better the choice must be or recall is irrevocably lost.</p><p>Some participants use single words while others extract whole phrases. Most of the participants preprocessed the suspicious document by removing stop words before the actual keyphrase extraction. Phrasal search was provided by the Indri search engine. All participants did use Indri when submitting phrasal queries; some of which also combine phrases with non-phrasal ChatNoir queries, the search engine that the original essay authors had used. In particular, Han <ref type="bibr" coords="8,308.47,436.87,11.62,8.64" target="#b8">[9]</ref> and <ref type="bibr" coords="8,340.90,436.87,64.91,8.64">Kong et al. [10]</ref> extract nouns and verbs according to the Stanford POS-tagger as their keywords. Rafiei et al. <ref type="bibr" coords="8,433.65,448.83,16.60,8.64" target="#b20">[22]</ref> use the words with highest tf •idf scores from a chunk. The idf is derived from the PAN 2011 corpus although it is not really clear why this would be a good choice since it contains rather different documents than a web corpus and most of the documents are artificially created with some more or less "random" word distributions. Ravi N and Gupta <ref type="bibr" coords="8,463.99,496.65,16.60,8.64" target="#b21">[23]</ref> also use single words as keywords (verbs, nouns, and adjectives according to the NLTK Python package) and also score by tf •idf (without further details on how idf is computed). Suchomel and Brandejs <ref type="bibr" coords="8,262.61,532.51,16.60,8.64" target="#b24">[26]</ref> apply a similar strategy as in their previous years' approaches: they also use the highest scoring tf •idf terms where idf is computed from a 4 billion word collection they also used in the last year.</p><p>Altogether, the participants' approaches to "keyphrase extraction" are more or less simplistic and do not really rely on established keyphrase extraction techniques from the NLP community. Giving several such established methods a try at least on document level might be an interesting direction for some more general keyphrases. Queries from such phrases in combination with queries from "extracted keyphrases" or just single words from shorter chunks might be a promising direction. Some first steps in this direction are shown in the method of Suchomel and Brandejs <ref type="bibr" coords="8,390.15,640.11,16.60,8.64" target="#b24">[26]</ref> but might still be enhanced. Not relying on just one strategy alone but combining different keyphrase extraction ideas might ensure a higher recall. This way, just as with chunking, the risk of algorithm error is further diminished and it becomes possible to exploit potentially different sources of information that complement each other.</p><p>Query Formulation Interestingly, most of the participants hardly put effort in finding good keyphrase combinations as queries. Instead, typically the top-k tf •idf -ranked terms form the first query, then the next k terms, etc. This way, mostly non-overlapping queries are generated for the individual chunks. This non-overlap-approach is in line with many query-by-document strategies <ref type="bibr" coords="9,303.95,209.72,10.79,8.64" target="#b0">[1,</ref><ref type="bibr" coords="9,318.10,209.72,8.30,8.64" target="#b2">3]</ref> but in contrast to previous source retrieval strategies that were shown to better identify highly related documents using overlapping queries from several keyphrase combinations <ref type="bibr" coords="9,374.80,233.63,10.58,8.64" target="#b7">[8]</ref>. Also note that hardly any of the participants made use of advanced search operators offered by Indri or Chat-Noir, such as the facet to search for web pages of at least 300 words of text, and the facet to filter search results by readability.</p><p>In particular, Han <ref type="bibr" coords="9,223.17,281.45,11.62,8.64" target="#b8">[9]</ref> and Kong et al.</p><p>[10] formulate one query from the keywords extracted per sentence (at most 10 words in a query as a threshold). Rafiei et al. <ref type="bibr" coords="9,463.99,293.41,16.60,8.64" target="#b20">[22]</ref> employ a rather involved scheme of first identifying the 10 words with highest tf • idf scores for each chunk, then selecting three sentences with these keywords of the respective chunk and then formulating queries from the keywords in these sentences until some not-detailed maximum is reached. Then they also seem to formulate individual queries for every sentence. Ravi N and Gupta <ref type="bibr" coords="9,317.22,353.19,16.60,8.64" target="#b21">[23]</ref> formulate two queries per paragraph from the respective top-n tf • idf terms (without further details on how n is chosen). Suchomel and Brandejs <ref type="bibr" coords="9,235.62,377.10,16.60,8.64" target="#b24">[26]</ref> apply a similar strategy as in their previous years' approaches: queries with 6 terms on the document level, phrase queries from their collocations, and then individual queries with the 10 highest scoring tf •idf terms per chunk.</p><p>Search Control Given sets of keywords or keyphrases extracted from chunks, queries are formulated which are tailored to the API of the search engine used. Rationale for this is to adhere to restrictions imposed by the search engine and to exploit search features that go beyond basic keyword search (e.g., Indri's phrasal search). The maximum number of search terms enforced by ChatNoir is 10 keywords per query while Indri allows for longer queries.</p><p>Given a set of queries, the search controller schedules their submission to the search engine and directs the download of search results. Rationale for this is to dynamically adjust the search based on the results of each query, which may include dropping queries, reformulating existing ones, or formulating new ones based on the relevance feedback obtained from the search results. Han <ref type="bibr" coords="9,347.35,539.24,11.62,8.64" target="#b8">[9]</ref> and <ref type="bibr" coords="9,377.66,539.24,61.73,8.64">Kong et al. [10]</ref> download 100 results per query but the downloads seem to start only after all queries were submitted. Rafiei et al. <ref type="bibr" coords="9,212.63,563.15,16.60,8.64" target="#b20">[22]</ref> drop a query when more than 60% of its terms are contained in an already downloaded document. Ravi N and Gupta <ref type="bibr" coords="9,349.02,575.10,16.60,8.64" target="#b21">[23]</ref> omit duplicate queries without giving further details on what constitutes a duplicate. Suchomel and Brandejs <ref type="bibr" coords="9,463.99,587.06,16.60,8.64" target="#b24">[26]</ref> apply a similar strategy as in their previous years' approaches: they schedule queries dependent on the keyphrase extractor which extracted the words. The order of precedence corresponds to the order in which they have been explained above. Whenever later queries were formulated for chunks of the suspicious document that were already mapped to a source, these queries are not submitted and discarded from the list of open queries. Depending on how many plagiarism sources are contained in a paragraph-long chunk, this might potentially miss some further sources when for instance two sources were used and one was already found.</p><p>Note that still (just as in the last years) none of the teams did try to reformulate existing queries or formulating new ones based on the available number of search results, the search snippets, or the downloaded documents, which probably leaves room for substantial improvement. Another interesting aspect might be the scheduling of the queries themselves. The experimental results (cf. Section 5) seem to suggest that some document-level queries in the first submission positions guarantee an early recall with respect to the number of submitted queries (e.g., Suchomel and Brandejs <ref type="bibr" coords="10,435.54,226.91,14.94,8.64" target="#b24">[26]</ref>). Simply scheduling queries in the order of chunks in the documents instead, might run into problems with early recall as maybe there is not that much reused text at the beginning of a document. This might also be an interesting point for future research that none of the approaches has investigated so far. Download Filtering Given a set of search engine results, a download filter removes all documents that are probably not worthwhile being compared in detail with the suspicious document. Rationale for this is to further reduce the set of candidates and to save invocations of the subsequent detailed comparison step. Many of the participants in the last years and also this year seem to focus a little too much on that part of a source retrieval system. This can be seen in the not improved overall recall and also in the number of submitted queries till the first evidence of text reuse is found compared to the approaches from 2013. Another evidence is that some approaches hardly download ten documents per suspicious document. In this case, the download filtering needs to be almost optimal which puts a higher burden on the search strategy. As downloads probably are not the most important bottleneck and probably queries are more costly in a general environment, reducing downloads too much does not seem to be the most promising strategy. A text alignment system is probably easily able to compare a suspicious document against even several thousands of potential sources in a couple of minutes. Maybe downloading more documents and putting more effort on the formulation of good queries is a promising future area.</p><p>In this year, Han <ref type="bibr" coords="10,221.17,484.69,11.62,8.64" target="#b8">[9]</ref> and <ref type="bibr" coords="10,253.64,484.69,64.96,8.64">Kong et al. [10]</ref> focus on the top-100 results of a query and download them depending on the number of queries for which they appear. This basically means that all queries are submitted before any download and that downloads have no influence on potential query scheduling etc. This strategy obviously has a high number of queries submitted until the first evidence is found since basically all queries are submitted before any evidence can be found. Han <ref type="bibr" coords="10,360.81,544.47,11.62,8.64" target="#b8">[9]</ref> additionally seems to use some learning approach similar to Williams et al. <ref type="bibr" coords="10,336.89,556.42,16.60,8.64" target="#b25">[27]</ref> employing snippet features but do not provide many details. Note that this learning approach seems to be the only difference between the two approaches of <ref type="bibr" coords="10,301.25,580.33,61.78,8.64">Kong et al. [10]</ref> and Han <ref type="bibr" coords="10,400.53,580.33,10.58,8.64" target="#b8">[9]</ref>. Rafiei et al. <ref type="bibr" coords="10,463.99,580.33,16.60,8.64" target="#b20">[22]</ref> download at most the top-14 results of a query when the individual snippets (no length information) contain at least 50% of the query terms. Ravi N and Gupta <ref type="bibr" coords="10,422.49,604.24,16.60,8.64" target="#b21">[23]</ref> download a document when the snippet (no length information) has a high cosine similarity to the document chunk for which the query was generated. However, there are no details on the similarity threshold or on how many documents are checked per query. Suchomel and Brandejs <ref type="bibr" coords="10,190.19,652.07,16.60,8.64" target="#b24">[26]</ref> apply a similar strategy as in their previous years' approaches: they obtain snippets for at most 100 results per query and download documents when more than 20% of the word 2-grams in the concatenated snippets also appear in the suspicious document. Some participants download very few documents based on their filtering while others download more documents per query than most participants did in the last years. As described above, the second option seems to be more promising with respect to recall since downloads typically are not that costly. An further interesting option for future research might be based on the User-over-Ranking hypothesis <ref type="bibr" coords="11,375.58,463.85,15.77,8.64" target="#b22">[24,</ref><ref type="bibr" coords="11,393.73,463.85,8.30,8.64" target="#b6">7]</ref> taking into account some goal number of candidate documents against which a detailed text alignment can be performed after the source retrieval phase of plagiarism detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation Results</head><p>Table <ref type="table" coords="11,159.84,543.55,4.98,8.64" target="#tab_0">1</ref> shows the performances of the five plagiarism detectors that took part in this year's source retrieval subtask as well as those of the last years' participants whose approaches were also evaluated on this year's test corpus using the TIRA experimentation platform. Since there is currently no single formula to organize retrieval performance and cost-effectiveness into just one absolute score or order, the detectors are ordered alphabetically, whereas the best performance value for each metric is highlighted-note that these individual "best" performances should be compared to the other metrics as for instance the fastest approach also has the highest number of no detections etc. As can be seen, there is no single detector that performs best on all accounts. Rather, different detectors have different characteristics. from the ensemble of the 2013 and 2014 approaches also indicating that this year's methods do not contain real innovations with respect to recall-oriented source retrieval.</p><p>A per-participant analysis also reveals some interesting observations when comparing the approaches from different years. For instance, after doubling the recall in the last year, Suchomel and Brandejs <ref type="bibr" coords="13,273.70,167.13,16.60,8.64" target="#b24">[26]</ref> were only able to slightly increase their recall this year with way more query and downloading effort. Rafiei et al. <ref type="bibr" coords="13,414.02,179.09,16.60,8.64" target="#b20">[22]</ref> and Ravi N and Gupta <ref type="bibr" coords="13,178.48,191.04,16.60,8.64" target="#b21">[23]</ref> manage to enter the competition with medium recalls while Kong et al.</p><p>[10] and Han <ref type="bibr" coords="13,189.64,203.00,11.62,8.64" target="#b8">[9]</ref> could not reach their still state-of-the-art recall from the 2013 edition.</p><p>As some further not-just-recall-oriented observations, for no metric, any of this year's participants achieved the best performance. This is not too surprising given the fact that no real "innovations" are contained in this year's submissions. They are rather very similar to methods that participated in the last year such that no "radical" change could be expected. Still, some notable achievements can be observed for the five participants of this year's competition. Rafiei et al. <ref type="bibr" coords="13,329.95,274.73,16.60,8.64" target="#b20">[22]</ref> in their first year manage to find sources for all but one of the suspicious documents. Also Ravi N and Gupta <ref type="bibr" coords="13,441.41,286.69,16.60,8.64" target="#b21">[23]</ref> enter the competition with a very good result: their number of downloads till the first detection is almost the top-performing one and is a little better than the also very good one of Han <ref type="bibr" coords="13,165.58,322.55,10.79,8.64" target="#b8">[9]</ref>; however, both probably download too few documents overall to achieve a good recall. The number of submitted queries of Suchomel and Brandejs <ref type="bibr" coords="13,435.80,334.51,16.60,8.64" target="#b24">[26]</ref> is still very good taking into account the achieved recall. Still, a recall of "only" 0.42 suggests that they might still not make the most of their interesting idea of combining more general with more focused queries.</p><p>Altogether, the current strategies might still be a little too focused on saving downloads (and queries) compared to for instance increase the recall. Also runtime should probably not be the key metric to optimize (e.g., using threads instead of sequential processing does not decrease the actual costs for using the search engines). A reasonable assumption probably is that recall is most important to the end user of a source retrieval system. Investing a couple of queries and a couple of downloads (maybe even hundreds to thousands) to achieve a recall above 0.8 might be a very important research direction since still none of the participating approaches can reach such levels. In the end, whatever source the source retrieval step misses, cannot be found by a later text alignment step. This probably is a key argument for a recall-oriented source retrieval strategy that also takes into account basic considerations on total workload of query submissions and downloads. It would be interesting to see efforts in that direction of substantially improved recall at a moderate cost increase in future approaches.</p><p>Another interesting direction from a practical standpoint is to find at least one source for a document that contains text reuse and to report a first detection as early as possible. This way the real end user of a detection system could focus on the really important suspicious documents very quickly and could even deepen the search depth or increase the allowed number of queries without the fear of missing text reuse in too many suspicious documents. However, this probably should be future work when better overall recall levels are reached.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Outlook</head><p>Altogether, even though new participants entered the source retrieval subtask this year, the ideas underlying the approaches did not contain "real" innovations compared to the approaches from the previous years since mostly the same ideas are just reused. Even though the source retrieval subtask now is expected to be in the production phase of its shared task life cycle-it is well-defined and all evaluation resources are set up and provide for a challenging testbed-most of the approaches still struggle to retrieve at least 50% of the text reuse sources with no real progress on the recall side this year. The combined ensemble of all approaches would result in a really good recall but at a rather high cost in the number of queries and downloads. None of this year's approaches really tried to work towards that recall at reduced costs resulting in the described "lack" of innovation.</p><p>Without really new ideas, the task does not seem to be interesting enough for another edition. Instead, we are planning to continue to pursue automating source retrieval evaluations without the requirement of an organized shared task. The key is the development of the TIRA experimentation platform <ref type="bibr" coords="14,320.91,310.60,11.62,8.64" target="#b5">[6]</ref> that facilitates software submissions, where participants submit their plagiarism detection software to be evaluated at our site <ref type="bibr" coords="14,151.09,334.51,10.58,8.64" target="#b3">[4]</ref>. The web front end of TIRA allows any researcher to conduct self-service evaluations on the test data of the source retrieval task under our supervision and guidance, whereas the test data remains hidden from direct access from participants. <ref type="foot" coords="14,438.76,356.75,3.49,6.05" target="#foot_5">6</ref> This has enabled us to put the participants back in charge of executing their software while the software itself remains in a running state within virtual machines managed by TIRA. Based on this technology, we conduct cross-year evaluations of all source retrieval systems that have been submitted since 2013. This platform will be further available for comparison against the state of the art in source retrieval but a new edition of the source retrieval subtask at PAN might probably not happen next year.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,199.04,251.97,217.27,8.12"><head>Figure 1</head><label>1</label><figDesc>Figure1. Generic retrieval process to detect plagiarism<ref type="bibr" coords="2,399.14,252.32,13.74,7.77" target="#b23">[25]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,134.77,372.20,345.83,8.12;5,134.77,383.51,345.82,7.77;5,134.77,394.47,345.82,7.77;5,134.77,405.43,255.09,7.77"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Overview of the building blocks used in the evaluation of the source retrieval subtask. The components are organized by the two activities corpus construction and evaluation runs (top two rows). Both activities are based on a static evaluation infrastructure (bottom row) consisting of an experimentation platform, web search engines, and a web corpus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,246.98,378.02,56.07,8.96;6,303.06,382.53,9.81,6.12;6,316.59,378.34,164.00,8.64;6,134.77,389.98,213.63,9.65;6,348.39,394.48,9.77,6.12;6,361.41,389.98,119.18,9.65;6,134.77,401.93,249.92,9.65;6,134.77,423.85,8.25,8.74;6,143.01,428.35,9.81,6.12;6,156.11,423.85,323.27,9.65;6,134.81,438.79,8.25,8.74;6,143.06,443.30,9.77,6.12;6,156.11,438.79,323.01,9.65"><head></head><label></label><figDesc>Further, let D src denote the set of all near-duplicates of a given set of source documents D src of d plg and let D ret denote the subset of D src that have at least one corresponding true positive detection in D ret : D src = {d dup | d dup ∈ D and ∃d src ∈ D src : d dup is a true positive detection of d src }, D ret = {d src | d src ∈ D src and ∃d ret ∈ D ret : d ret is a true positive detection of d src }.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,261.46,116.94,31.37,4.88;7,261.46,125.66,42.30,5.56;7,135.48,117.16,41.66,5.56;7,274.40,134.37,29.36,4.88;7,135.84,134.60,31.67,4.88;7,200.99,173.37,23.54,5.56;7,437.83,116.94,31.37,4.88;7,437.83,125.66,42.30,5.56;7,311.85,117.16,41.66,5.56;7,450.77,134.37,29.36,4.88;7,312.21,134.60,31.67,4.88;7,392.04,173.37,23.54,5.56"><head></head><label></label><figDesc>Documents D Duplicate Hull D' src Duplicate Hull D' ret Sources Dsrc Retrieved Dret Dret Ç D' src Documents D Duplicate Hull D' src Duplicate Hull D' ret Sources Dsrc Retrieved Dret D' ret Ç Dsrc</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,134.77,234.19,345.82,8.12;7,134.77,245.50,345.82,7.77;7,134.77,256.46,90.16,7.77"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Effect of near-duplicates on computing precision (left) and recall (right) of retrieved source documents. Without taking near-duplicates into account, a lot of potentially correct sources might be missed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="11,134.77,115.83,345.86,242.67"><head>Table 1 .</head><label>1</label><figDesc>Source retrieval results with respect to retrieval performance and cost-effectiveness.</figDesc><table coords="11,134.77,133.64,345.86,224.86"><row><cell cols="3">Software Submission Downloaded</cell><cell>Total</cell><cell></cell><cell cols="3">Workload to No Runtime</cell></row><row><cell>Team</cell><cell>Year</cell><cell>Sources</cell><cell cols="2">Workload</cell><cell cols="3">1st Detection Detect.</cell></row><row><cell cols="2">(alphabetical order)</cell><cell cols="5">F1 Prec. Rec. Queries Dwlds Queries Dwlds</cell><cell></cell></row><row><cell>Elizalde</cell><cell>2013</cell><cell>0.16 0.12 0.37</cell><cell>41.6</cell><cell>83.9</cell><cell>18.0</cell><cell>18.2</cell><cell>4 11:18:50</cell></row><row><cell>Elizalde</cell><cell>2014</cell><cell>0.34 0.40 0.39</cell><cell>54.5</cell><cell>33.2</cell><cell>16.4</cell><cell>3.9</cell><cell>7 04:02:00</cell></row><row><cell>Foltynek</cell><cell>2013</cell><cell>0.11 0.08 0.26</cell><cell>166.8</cell><cell>72.7</cell><cell>180.4</cell><cell cols="2">4.3 32 152:26:23</cell></row><row><cell>Gillam</cell><cell>2013</cell><cell>0.06 0.04 0.15</cell><cell>15.7</cell><cell>86.8</cell><cell>16.1</cell><cell cols="2">28.6 34 02:24:59</cell></row><row><cell>Haggag</cell><cell>2013</cell><cell>0.38 0.67 0.31</cell><cell>41.7</cell><cell>5.2</cell><cell>13.9</cell><cell cols="2">1.4 12 46:09:21</cell></row><row><cell>Han</cell><cell>2015</cell><cell>0.36 0.55 0.32</cell><cell>194.5</cell><cell>11.8</cell><cell>202.0</cell><cell cols="2">1.7 12 20:43:02</cell></row><row><cell>Kong</cell><cell>2013</cell><cell>0.01 0.01 0.59</cell><cell cols="2">47.9 5185.3</cell><cell cols="2">2.5 210.2</cell><cell>0 106:13:46</cell></row><row><cell>Kong</cell><cell>2014</cell><cell>0.12 0.08 0.48</cell><cell cols="2">83.5 207.1</cell><cell>85.7</cell><cell>24.9</cell><cell>6 24:03:31</cell></row><row><cell>Kong</cell><cell>2015</cell><cell>0.38 0.45 0.42</cell><cell>195.1</cell><cell>38.3</cell><cell>197.5</cell><cell>3.5</cell><cell>3 17:56:55</cell></row><row><cell>Lee</cell><cell>2013</cell><cell>0.40 0.58 0.37</cell><cell>48.4</cell><cell>10.9</cell><cell>6.5</cell><cell>2.0</cell><cell>9 09:17:10</cell></row><row><cell>Prakash</cell><cell>2014</cell><cell>0.39 0.38 0.51</cell><cell>60.0</cell><cell>38.8</cell><cell>8.1</cell><cell>3.8</cell><cell>7 19:47:45</cell></row><row><cell>Rafiei</cell><cell>2015</cell><cell>0.12 0.08 0.41</cell><cell cols="2">43.5 183.3</cell><cell>5.6</cell><cell>24.9</cell><cell>1 08:32:37</cell></row><row><cell>Ravi N</cell><cell>2015</cell><cell>0.43 0.61 0.39</cell><cell>90.3</cell><cell>8.5</cell><cell>17.5</cell><cell>1.6</cell><cell>8 09:17:20</cell></row><row><cell>Suchomel</cell><cell>2013</cell><cell>0.05 0.04 0.23</cell><cell cols="2">17.8 283.0</cell><cell>3.4</cell><cell cols="2">64.9 18 75:12:56</cell></row><row><cell>Suchomel</cell><cell>2014</cell><cell>0.11 0.08 0.40</cell><cell cols="2">19.5 237.3</cell><cell>3.1</cell><cell>38.6</cell><cell>2 45:42:06</cell></row><row><cell>Suchomel</cell><cell>2015</cell><cell>0.09 0.06 0.43</cell><cell cols="2">42.4 359.3</cell><cell>3.3</cell><cell>39.8</cell><cell>4 161:51:26</cell></row><row><cell>Williams</cell><cell>2013</cell><cell>0.47 0.60 0.47</cell><cell>117.1</cell><cell>12.4</cell><cell>23.3</cell><cell>2.2</cell><cell>7 76:58:22</cell></row><row><cell>Williams</cell><cell>2014</cell><cell>0.47 0.57 0.48</cell><cell>117.1</cell><cell>14.4</cell><cell>18.8</cell><cell>2.3</cell><cell>4 39:44:11</cell></row><row><cell>Zubarev</cell><cell>2014</cell><cell>0.45 0.54 0.45</cell><cell>37.0</cell><cell>18.6</cell><cell>5.4</cell><cell>2.3</cell><cell>3 40:42:18</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,623.49,335.85,7.77;1,144.73,634.45,335.85,7.77;1,144.73,645.41,162.73,7.77"><p>Some of the concepts found in this paper have been described earlier, so that, because of the inherently incremental nature of shared tasks, and in order for this paper to be self-contained, we reuse text from previous overview papers.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,144.73,635.01,335.87,7.93;2,144.73,646.13,335.86,7.77;2,144.73,657.08,206.96,7.77"><p>Another approach to detect plagiarism is called intrinsic plagiarism detection, where detectors are given only one suspicious document and are supposed to identify text passages in it which deviate in their style from the remainder of the document.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,144.73,627.12,122.39,7.77"><p>http://lemurproject.org/clueweb09</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,144.73,638.26,122.39,7.77"><p>http://lemurproject.org/clueweb12</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="4,144.73,649.41,195.23,7.77"><p>http://lemurproject.org/clueweb09/index.php#Services</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="14,144.73,642.84,42.25,7.77"><p>www.tira.io</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank the participating teams of the editions of the source retrieval subtask for their shared ideas and for their devoted work towards making their softwares run on TIRA.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Arguably, highest possible recall at a reasonable workload (queries and downloads) is the goal of source retrieval while, at the same time, it would be nice to quickly detect some first evidence for text reuse if there is one. Since downloads in most environments would be much cheaper than query submissions (that would accompanied by costs at most major web search engine's APIs) the most interesting metrics probably are: recall and number of no detections, number of submitted queries, and number of queries and downloads until the first detection.</p><p>Focusing on recall first, interestingly the top-6 approaches are from previous years with the best one by far still being the approach of Kong et al. <ref type="bibr" coords="12,403.10,466.81,16.60,8.64" target="#b9">[11]</ref> (at some high costs in number of queries and downloads and a poor precision, though). Thus, after last year's progress in the overall recall of most systems, this year the participating systems seem not to be able to actually increase their recall substantially. To further shed some light on the recall of the different approaches, Figure <ref type="figure" coords="12,405.46,514.63,4.98,8.64">4</ref> shows the recall against the number of downloaded documents. It can be seen that recall is typically gained over the whole process of downloading documents and not with the very first downloads (the plateau effect at the upper right end of each plot is due to the averaging). Unsurprisingly, some of the low-workload approaches achieve higher recall levels with fewer downloads while approaches with more downloads typically achieve their better final recall levels only at a much higher number of downloads-which still can be good depending on probably rather low practical costs for downloads.</p><p>The ensemble of all submitted approaches of the last three years would achieve an average recall of 0.85 retrieving all sources for 48 topics. Only for 14 topics the recall is below 0.6 (which is the best individual average recall). These numbers did not change</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="14,142.61,547.36,336.88,7.77;14,150.95,558.32,307.56,7.77;14,150.95,569.28,299.62,7.77;14,150.95,580.23,173.47,7.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="14,245.79,547.36,110.46,7.77">Finding Text Reuse on the Web</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,315.08,558.32,143.44,7.77;14,150.95,569.28,229.84,7.77">Proceedings of the Second International Conference on Web Search and Web Data Mining, WSDM 2009</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Baeza-Yates</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Boldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Ribeiro-Neto</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Cambazoglu</surname></persName>
		</editor>
		<meeting>the Second International Conference on Web Search and Web Data Mining, WSDM 2009<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">February 9-11, 2009. 2009</date>
			<biblScope unit="page" from="262" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.61,591.19,327.88,7.77;14,150.95,602.15,320.38,7.77;14,150.95,613.11,131.08,7.77;14,150.95,624.07,198.21,7.77" xml:id="b1">
	<analytic>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
	</analytic>
	<monogr>
		<title level="m" coord="14,352.67,591.19,117.83,7.77;14,150.95,602.15,124.46,7.77">CLEF 2015 Evaluation Labs and Workshop -Working Notes Papers</title>
		<title level="s" coord="14,408.54,602.15,62.79,7.77;14,150.95,613.11,68.09,7.77">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>San Juan</surname></persName>
		</editor>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09">September. 2015</date>
			<biblScope unit="page" from="8" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.61,119.96,319.53,7.77;15,150.95,130.92,318.00,7.77;15,150.95,141.88,318.66,7.77;15,150.95,152.84,319.58,7.77;15,150.95,163.80,23.90,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="15,327.12,119.96,135.02,7.77;15,150.95,130.92,125.78,7.77">Automatic retrieval of similar content using search engine query interface</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dasdan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>D'alberto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kolay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Drome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,204.00,141.88,265.62,7.77;15,150.95,152.84,93.09,7.77">Proceedings of the 18th ACM Conference on Information and Knowledge Management, CIKM 2009</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">W L</forename><surname>Cheung</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><forename type="middle">Y</forename><surname>Song</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Chu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</editor>
		<meeting>the 18th ACM Conference on Information and Knowledge Management, CIKM 2009<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">November 2-6, 2009. 2009</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.61,174.76,331.26,7.77;15,150.95,185.71,317.98,7.77;15,150.95,196.67,329.64,7.77;15,150.95,207.63,307.09,7.77;15,150.95,218.59,272.70,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="15,163.91,185.71,207.78,7.77">Recent Trends in Digital Text Forensics and its Evaluation</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Busse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,288.22,196.67,192.37,7.77;15,150.95,207.63,307.09,7.77;15,150.95,218.59,37.80,7.77">Information Access Evaluation meets Multilinguality, Multimodality, and Visualization. 4th International Conference of the CLEF Initiative (CLEF 13)</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Paredes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Rosso</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Stein</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013-09">Sep 2013</date>
			<biblScope unit="page" from="282" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.61,229.55,301.43,7.77;15,150.95,240.51,319.88,7.77;15,150.95,251.47,327.49,7.77;15,150.95,262.43,246.99,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="15,274.60,229.55,169.44,7.77;15,150.95,240.51,182.43,7.77">Ousting Ivory Tower Research: Towards a Web Framework for Providing Experiments as a Service</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Burrows</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,245.84,251.47,232.61,7.77;15,150.95,262.43,119.02,7.77">International ACM Conference on Research and Development in Information Retrieval (SIGIR 12)</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Hersh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Callan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Maarek</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012-08">Aug 2012</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1125" to="1126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.61,273.39,300.72,7.77;15,150.95,284.34,310.62,7.77;15,150.95,295.30,329.02,7.77;15,150.95,306.26,256.76,7.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="15,316.19,273.39,127.14,7.77;15,150.95,284.34,177.39,7.77">TIRA: Configuring, Executing, and Disseminating Information Retrieval Experiments</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Burrows</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hoppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,229.64,295.30,250.33,7.77;15,150.95,306.26,44.62,7.77">9th International Workshop on Text-based Information Retrieval (TIR 12) at DEXA</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Tjoa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Liddle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Schewe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</editor>
		<meeting><address><addrLine>Los Alamitos, California</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012-09">Sep 2012</date>
			<biblScope unit="page" from="151" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.61,317.22,329.85,7.77;15,150.95,328.18,324.45,7.77;15,150.95,339.14,316.39,7.77;15,150.95,350.10,199.98,7.77" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="15,229.41,317.22,239.18,7.77">Applying the User-over-Ranking Hypothesis to Query Formulation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,163.16,328.18,312.25,7.77;15,150.95,339.14,128.73,7.77">Advances in Information Retrieval Theory. 3rd International Conference on the Theory of Information Retrieval (ICTIR 11)</title>
		<title level="s" coord="15,285.48,339.14,126.47,7.77">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">6931</biblScope>
			<biblScope unit="page" from="225" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.61,361.06,332.38,7.77;15,150.95,372.02,322.33,7.77;15,150.95,382.97,293.83,7.77;15,150.95,393.93,104.45,7.77" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="15,229.41,361.06,241.88,7.77">Candidate Document Retrieval for Web-Scale Text Reuse Detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,163.16,372.02,310.13,7.77;15,150.95,382.97,10.65,7.77">18th International Symposium on String Processing and Information Retrieval (SPIRE 11)</title>
		<title level="s" coord="15,167.39,382.97,126.47,7.77">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">7024</biblScope>
			<biblScope unit="page" from="356" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.61,404.89,304.32,7.77;15,150.95,415.85,227.18,7.77;15,150.95,426.81,325.73,7.77;15,150.95,437.77,42.20,7.77;15,134.77,448.73,336.28,7.77;15,150.95,459.69,323.67,7.77;15,150.95,470.65,70.22,7.77" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
		<title level="m" coord="15,182.67,404.89,260.55,7.77;15,411.49,448.73,59.56,7.77;15,150.95,459.69,323.67,7.77;15,150.95,470.65,21.92,7.77">Source Retrieval and Text Alignment Corpus Construction for Plagiarism Detection-Notebook for PAN at CLEF</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note>Submission to the 7th International Competition on Plagiarism Detection</note>
</biblStruct>

<biblStruct coords="15,142.24,481.60,321.98,7.77;15,150.95,492.56,309.24,7.77;15,150.95,503.52,319.78,7.77;15,150.95,514.48,191.50,7.77" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="15,313.79,481.60,150.43,7.77;15,150.95,492.56,253.41,7.77">Approaches for Source Retrieval and Text Alignment of Plagiarism Detection-Notebook for PAN at CLEF 2013</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,250.74,503.52,219.99,7.77;15,150.95,514.48,22.30,7.77">CLEF 2013 Evaluation Labs and Workshop -Working Notes Papers</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Navigli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Tufis</surname></persName>
		</editor>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-09">23-26 September. Sep 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.24,525.44,269.34,7.77;15,150.95,536.40,148.82,7.77;15,150.95,547.36,282.16,7.77" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="15,199.28,525.44,158.88,7.77">Technologies for Reusing Text from the Web</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<ptr target="http://nbn-resolving.de/urn/resolver.pl?urn:nbn:de:gbv:wim2-20120217-15663" />
		<imprint>
			<date type="published" when="2011-12">Dec 2011</date>
		</imprint>
		<respStmt>
			<orgName>Bauhaus-Universität Weimar</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Dissertation</note>
</biblStruct>

<biblStruct coords="15,142.24,558.32,314.45,7.77;15,150.95,569.28,323.88,7.77;15,150.95,580.23,284.57,7.77;15,150.95,591.19,198.21,7.77" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="15,383.09,558.32,73.60,7.77;15,150.95,569.28,180.34,7.77">Overview of the 2nd International Competition on Plagiarism Detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Eiselt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
	</analytic>
	<monogr>
		<title level="m" coord="15,183.32,580.23,208.12,7.77">Working Notes Papers of the CLEF 2010 Evaluation Labs</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Harman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Pianta</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2010-09">Sep 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.24,602.15,312.95,7.77;15,150.95,613.11,313.65,7.77;15,150.95,624.07,274.60,7.77;15,150.95,635.03,198.21,7.77" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="15,383.09,602.15,72.10,7.77;15,150.95,613.11,180.34,7.77">Overview of the 3rd International Competition on Plagiarism Detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Eiselt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
	</analytic>
	<monogr>
		<title level="m" coord="15,173.36,624.07,208.12,7.77">Working Notes Papers of the CLEF 2011 Evaluation Labs</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Petras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2011-09">Sep 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.24,645.99,329.94,7.77;15,150.95,656.95,310.40,7.77;16,150.95,119.96,282.41,7.77;16,150.95,130.92,322.05,7.77;16,150.95,141.88,223.61,7.77" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="15,389.75,656.95,71.61,7.77;16,150.95,119.96,180.34,7.77">Overview of the 4th International Competition on Plagiarism Detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Graßegger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Oberländer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tippmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
	</analytic>
	<monogr>
		<title level="m" coord="16,246.21,130.92,208.12,7.77">Working Notes Papers of the CLEF 2012 Evaluation Labs</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Karlgren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Womser-Hacker</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2012-09">Sep 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.24,152.84,324.55,7.77;16,150.95,163.80,306.96,7.77;16,150.95,174.76,326.84,7.77;16,150.95,185.71,261.96,7.77" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="16,186.83,163.80,254.19,7.77">Overview of the 5th International Competition on Plagiarism Detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tippmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
	</analytic>
	<monogr>
		<title level="m" coord="16,289.35,174.76,188.45,7.77;16,150.95,185.71,17.43,7.77">Working Notes Papers of the CLEF 2013 Evaluation Labs</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Navigli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Tufis</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2013-09">Sep 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.24,196.67,328.71,7.77;16,150.95,207.63,321.27,7.77;16,150.95,218.59,329.64,7.77;16,150.95,229.55,229.31,7.77" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="16,150.95,207.63,195.48,7.77">ChatNoir: A Search Engine for the ClueWeb09 Corpus</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Graßegger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tippmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Welsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,257.57,218.59,223.02,7.77;16,150.95,229.55,128.24,7.77">International ACM Conference on Research and Development in Information Retrieval (SIGIR 12)</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Hersh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Callan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Maarek</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012-08">Aug 2012</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">1004</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.24,240.51,305.28,7.77;16,150.95,251.47,316.03,7.77;16,150.95,262.43,306.23,7.77;16,150.95,273.39,243.58,7.77;16,150.95,284.34,157.21,7.77" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="16,321.97,240.51,125.54,7.77;16,150.95,251.47,132.36,7.77">Crowdsourcing Interaction Logs to Understand Text Reuse from the Web</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Völske</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P13-1119" />
	</analytic>
	<monogr>
		<title level="m" coord="16,400.25,251.47,66.74,7.77;16,150.95,262.43,289.23,7.77">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 13)</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Fung</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Poesio</surname></persName>
		</editor>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics (ACL 13)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-08">Aug 2013</date>
			<biblScope unit="page" from="1212" to="1221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.24,295.30,321.69,7.77;16,150.95,306.26,318.52,7.77;16,150.95,317.22,287.95,7.77;16,150.95,328.18,210.28,7.77;16,150.95,339.14,252.85,7.77" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="16,321.97,295.30,141.96,7.77;16,150.95,306.26,22.23,7.77">Exploratory Search Missions for TREC Topics</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Völske</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="http://www.cs.nott.ac.uk/mlw/euroHCIR2013/proceedings/paper3.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="16,457.52,306.26,11.95,7.77;16,150.95,317.22,287.95,7.77">3rd European Workshop on Human-Computer Interaction and Information Retrieval</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Wilson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Russell-Rose</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Larsen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Hansen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Norling</surname></persName>
		</editor>
		<meeting><address><addrLine>EuroHCIR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08">2013. Aug 2013</date>
			<biblScope unit="page" from="11" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.24,350.10,310.53,7.77;16,150.95,361.06,329.23,7.77;16,150.95,372.02,317.55,7.77;16,150.95,382.97,183.74,7.77" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="16,344.98,350.10,107.78,7.77;16,150.95,361.06,73.99,7.77">An Evaluation Framework for Plagiarism Detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,379.59,361.06,100.59,7.77;16,150.95,372.02,147.38,7.77">International Conference on Computational Linguistics (COLING 10)</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</editor>
		<meeting><address><addrLine>Stroudsburg, Pennsylvania</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010-08">Aug 2010</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="997" to="1005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.24,393.93,311.46,7.77;16,150.95,404.89,326.03,7.77;16,150.95,415.85,326.19,7.77;16,150.95,426.81,271.13,7.77;16,150.95,437.77,94.88,7.77" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="16,383.09,393.93,70.61,7.77;16,150.95,404.89,180.34,7.77">Overview of the 1st International Competition on Plagiarism Detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Eiselt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-502" />
	</analytic>
	<monogr>
		<title level="m" coord="16,256.47,415.85,220.67,7.77;16,150.95,426.81,135.49,7.77">SEPLN 09 Workshop on Uncovering Plagiarism, Authorship, and Social Software Misuse (PAN 09)</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Stein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Rosso</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Koppel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Agirre</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2009-09">Sep 2009</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.24,448.73,336.08,7.77;16,150.95,459.69,329.64,7.77" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="16,318.40,448.73,159.93,7.77;16,150.95,459.69,281.71,7.77">Source Retrieval Plagiarism Detection based on Noun Phrase and Keyword Phrase Extraction-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rafiei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mohtaj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Zarrabi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Asghari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.24,470.65,320.99,7.77;16,150.95,481.60,257.72,7.77" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="16,233.46,470.65,229.77,7.77;16,150.95,481.60,209.42,7.77">Efficient Paragraph based Chunking and Download Filtering for Plagiarism Source Retrieval-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.24,492.56,317.65,7.77;16,150.95,503.52,327.00,7.77;16,150.95,514.48,324.32,7.77;16,150.95,525.44,20.92,7.77" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="16,229.41,492.56,166.83,7.77">Introducing the User-over-Ranking Hypothesis</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,415.54,492.56,44.35,7.77;16,150.95,503.52,270.66,7.77">Advances in Information Retrieval. 33rd European Conference on IR Resarch (ECIR 11)</title>
		<title level="s" coord="16,427.41,503.52,50.54,7.77;16,150.95,514.48,73.69,7.77">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011-04">Apr 2011</date>
			<biblScope unit="volume">6611</biblScope>
			<biblScope unit="page" from="503" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.24,536.40,296.37,7.77;16,150.95,547.36,300.54,7.77;16,150.95,558.32,312.34,7.77;16,150.95,569.28,194.18,7.77" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="16,308.23,536.40,130.38,7.77;16,150.95,547.36,38.78,7.77">Strategies for Retrieving Plagiarized Documents</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Meyer Zu Eißen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,435.55,547.36,15.94,7.77;16,150.95,558.32,312.34,7.77;16,150.95,569.28,39.28,7.77">30th International ACM Conference on Research and Development in Information Retrieval (SIGIR 07)</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Clarke</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Fuhr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Kando</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>De Vries</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007-07">Jul 2007</date>
			<biblScope unit="page" from="825" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.24,580.23,273.82,7.77;16,150.95,591.19,189.98,7.77" xml:id="b24">
	<monogr>
		<title level="m" type="main" coord="16,272.75,580.23,143.31,7.77;16,150.95,591.19,141.68,7.77">Improving Synoptic Quering for Source Retrieval-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Šimon</forename><surname>Suchomel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brandejs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.24,602.15,293.52,7.77;16,150.95,613.11,314.15,7.77;16,150.95,624.07,309.66,7.77;16,150.95,635.03,329.26,7.77" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="16,282.85,602.15,152.91,7.77;16,150.95,613.11,160.06,7.77">Supervised Ranking for Plagiarism Source Retrieval-Notebook for PAN at CLEF 2014</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Giles</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="16,212.37,624.07,244.53,7.77">CLEF 2014 Evaluation Labs and Workshop -Working Notes Papers</title>
		<title level="s" coord="16,270.66,635.03,133.12,7.77">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Halvey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</editor>
		<meeting><address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-09">15-18 September. Sep 2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
