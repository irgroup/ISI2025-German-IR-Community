<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,138.64,115.90,338.08,12.90;1,281.30,133.83,52.77,12.90;1,223.43,153.68,168.50,10.75">An Author Verification Approach Based on Differential Features Notebook for PAN at CLEF 2015</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,209.40,190.08,59.32,8.64"><forename type="first">Alberto</forename><surname>Bartoli</surname></persName>
							<email>bartoli.alberto@univ.trieste.it</email>
							<affiliation key="aff0">
								<orgName type="institution">DIA -University of Trieste</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,274.98,190.08,42.69,8.64"><forename type="first">Alex</forename><surname>Dagri</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DIA -University of Trieste</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,324.36,190.08,77.14,8.64"><forename type="first">Andrea</forename><surname>De Lorenzo</surname></persName>
							<email>andrea.delorenzo@units.it</email>
							<affiliation key="aff0">
								<orgName type="institution">DIA -University of Trieste</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,242.24,202.03,47.18,8.64"><forename type="first">Eric</forename><surname>Medvet</surname></persName>
							<email>emedvet@units.it</email>
							<affiliation key="aff0">
								<orgName type="institution">DIA -University of Trieste</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,313.47,202.03,59.65,8.64"><forename type="first">Fabiano</forename><surname>Tarlao</surname></persName>
							<email>fabiano.tarlao@phd.units.it</email>
							<affiliation key="aff0">
								<orgName type="institution">DIA -University of Trieste</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,138.64,115.90,338.08,12.90;1,281.30,133.83,52.77,12.90;1,223.43,153.68,168.50,10.75">An Author Verification Approach Based on Differential Features Notebook for PAN at CLEF 2015</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DCE9EA40181C2C85DA33C4AA0DC45A0C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe the approach that we submitted to the 2015 PAN competition <ref type="bibr" coords="1,184.12,296.26,10.45,7.77" target="#b6">[7]</ref> for the author identification task 1 . The task consists in determining if an unknown document was authored by the same author of a set of documents with the same author. We propose a machine learning approach based on a number of different features that characterize documents from widely different points of view. We construct non-overlapping groups of homogeneous features, use a random forest regressor for each features group, and combine the output of all regressors by their arithmetic mean. We train a different regressor for each language. Our approach achieved the first position in the final rank for the Spanish language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Problem statement</head><p>A problem instance is a tuple K, u, L where K is a set of documents {k 1 , . . . , k n } authored by the same author (called known documents), u is a document whose authorship must be ascertained (called unknown document), L is an enumerated value specifying the language of the documents: English, Dutch, Greek or Spanish. All documents in a problem instance are in the same language.</p><p>The author verification procedure consists in generating an answer in the form of a real number in [0, 1] which quantifies the degree of confidence of being u authored by the same author of the documents in K: 0 indicates absolute certainty that u was not authored by the same author of documents in K, while 1 indicates absolute certainty that all documents were authored by the same author.</p><p>A set of solved problem instances (the training set) is available in which, for each problem instance K, u, L , the solution consisting in one between 0 and 1 is provided.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The effectiveness of a method for author identification is assessed using a testing set of solved problem instances, as follows. The answers generated by the method for the problem instances in the testing set are compared against the actual values and the comparison outcome is expressed in terms of two indexes: area under the ROC curve (AUC) and c@1. AUC is computed basing on the ROC curve plotted by comparing the generated answers against a threshold moving between 0 and 1, hence obtaining a binary classification task. The latter index is computed as c@1 = nc n + nunc n 2 , where n is the size of the testing set, n u is the number of unanswered problem instances (i.e., those for which the generated answer was exactly 0.5), n c is the number of correct answers (i.e., those for which the generated answer &gt; 0.5 and the actual answer is 1 and those for which the generated answer &lt; 0.5 and the actual answer is 0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our approach</head><p>We propose a machine learning approach based on a number of different features that characterize documents from widely different points of view: character, word, part-ofspeech, sentence length, punctuation. We construct non-overlapping groups of homogeneous features and use a random forest regressor for each features group. The output of the resulting ensemble of regressors is the arithmetic mean of the output generated by each random forest.</p><p>We train a different regressor for each language. Based on extensive experimention on the training set, we decided to use the same features for problem instances in Dutch, Greek, Spanish but a different set of features for problem instances in English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Features</head><p>We extract a number of different features from each document. For ease of presentation, we group homogeneous features together, as described below.</p><p>Word ngrams (WG) We convert all characters to lowercase and then we transform the document to a sequence of words. We consider white spaces, punctuation characters and digits as word separators. We count all word ngrams, with n ≤ 3, and we obtain a feature for each different word ngram which occurs in the training set documents of a given language. Character ngrams (CG) We replace punctuation characters and digits with blank spaces and then sequences of blank spaces with a single blank space. We count all character ngrams, with n ≤ 3, and we obtain a feature for each different character ngram which occurs in the training set documents of a given language. POS (part-of-speech) tag ngrams (PG) We apply a part of speech (POS) tagger on each document, which assigns words with similar syntactic properties to the same POS tag. For English and Dutch we use the Apache OpenNLP Tools 2 , for Greek we use the tagger developed by the Department of Informatics at Athens University of Economics and Business<ref type="foot" coords="3,262.52,117.64,3.49,6.05" target="#foot_2">3</ref> while for Spanish we use TreeTagger<ref type="foot" coords="3,418.90,117.64,3.49,6.05" target="#foot_3">4</ref>  <ref type="bibr" coords="3,425.69,119.31,10.58,8.64" target="#b5">[6]</ref>. We count all POS ngrams, with n ≤ 3, and we obtain a feature for each different POS ngram which occurs in the training set documents of a given language. Word lengths (WL) We convert all characters to lowercase and then we transform the document to a sequence of words. We consider white spaces, punctuation characters and digits as word separators. We count the number of words whose length in characters is n, with n ∈ {1, . . . , 16}: we obtain a feature for each value of n. Sentence lengths (SL) We transform the document to a sequence of tokens, a token being a sequence of characters separated by one or more blank spaces. Next, we transform the sequence of tokens to a sequence of sentences, a sentence being a sequence of tokens separated by any of the following characters: .,;,:,!,?. We count the number of sentences whose length in tokens is n, with n ∈ {1, . . . , 40}: we obtain a feature for each value of n. Sentence length ngrams (SG) We transform each document to a sequence of labels, where each label represents a full sentence and is chosen based on the sentence length (as described in the following). Next, we compute the ngrams of the resulting labels, with n ≤ 2. In detail, we execute a preliminary analysis of all documents of a given language in the training set, as follows. For each document, we transform the document to a sequence of sentences as illustrated for the SL features group. Next, we compute the distribution of sentence length across all sentences in the training set and determine the length values associated with the following percentile values: 10%, 25%, 75%, and 90%. In other words, we divide the range of sentence lengths observed in the training set in 5 intervals, with boundaries between intervals determined by the specified percentiles. The label we assign to each sentence corresponds to one of the 5 lenght intervals, i.e., ]0%, 10%], ]10%, 25%], and so on: we obtain a feature for each label ngrams which occurs in the training set documents of a given language. Word richness (WR) We transform the document to a sequence of words as for the WG features group. Then we compute the ratio between the number of distinct words and the number of total words in the document-this features group contains only one feature. Punctuation ngrams (MG) We transform the document by removing all characters not included in the following set: {,, ., ;, :, !, ?, "}-the resulting document thus consists of a (possibly empty) sequence of characters in that set. We then count all character ngrams of the resulting document, with n ≤ 3, and we obtain a feature for each different punctuation ngram which occurs in the training set documents of a given language. Text shape ngrams (TG) We transform the document as follows: sequences of digits are replaced by the single character n; sequences of alphabetic characters are replaced by a single character: l if all the characters in the sequence are lowercase, u if only the first character is uppercase, w if at least two characters are uppercase; sequences of blank spaces are replaced by a single blank space; other characters are left unchanged. We then count all character ngrams of the resulting document, with n ≤ 3, and we obtain a feature for each different character ngram which occurs in the training set documents of a given language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Feature selection, normalization, and aggregation</head><p>We perform a simple feature selection for features in groups WG, CG, PG, and TG. To this end, we apply the following procedure to each of the 4 partitions of the training set for which the language of the documents was the same-in other words, we select different features for each language. We compute the feature values for all the documents in the training set partition. Next, among each group, we sort the features according to their average values on the documents of the partition-greater values coming first. Finally, for each group, we keep the n sel top features. We set n sel = 500 for WG, CG and PG and n sel = 100 for TG.</p><p>After the feature selection, we perform a normalization of the features values, as follows. Let f i (d) be the value of the ith feature for the document d and let G be the group of features (as defined in Section 2.1) which includes the feature f i , we set d) . We execute this procedure for all the groups of features, except for WR, which consists of a single feature.</p><formula xml:id="formula_0" coords="4,134.77,323.35,77.96,17.84">f i (d) := fi(d) f j ∈G fj (</formula><p>Finally, for the purpose of obtaining a single feature vector for each problem instance, rather than one feature vector for each document, we build a new feature f i whose value is obtained from the values of the corresponding f i for the documents in K and the document u, as follows:</p><formula xml:id="formula_1" coords="4,215.49,413.27,265.10,22.97">f i ( K, u, L ) = abs f i (u) -k∈K f i (k) |K|<label>(1)</label></formula><p>In other words, we consider the absolute difference between the feature value for the unknown document u and the average of the feature values for the known documents in K. We also consider a variant of our approach in which the difference is divided by the feature value for u:</p><formula xml:id="formula_2" coords="4,244.49,498.04,236.10,23.22">f i ( K, u, L ) = f i ( K, u, L ) f i (u)<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Regressor</head><p>We explored three different regressor algorithms: trees (Tree), random forests (RF), and support vector machines (SVM). In particular, we use the algorithm proposed in <ref type="bibr" coords="4,454.97,583.56,11.62,8.64" target="#b4">[5]</ref> for Tree, we use the gaussian kernel and C = 1 for SVM <ref type="bibr" coords="4,349.78,595.51,10.58,8.64" target="#b3">[4]</ref>, and we use the algorithm for regression proposed in <ref type="bibr" coords="4,227.31,607.47,11.62,8.64" target="#b2">[3]</ref> with n tree = 500 for RF. We apply each regressor, both in training and actual regression phase, only to the feature values of the same group. For obtaining an answer in [0, 1] for a problem instance, we average the predictions obtained by the trained regressors on the features groups. In other words, we built an ensemble of group regressors.</p><p>As described in the previous section, we considered two set of features (f and f ) and 3 regressors. We systematically assessed the effectiveness of all the 6 resulting combinations by means of a leave-one-out procedure applied on the training set, separately for each language. That is, for each language, type of feature, and regressor, (i) we built the subset T of the problem instances of the training with that language, (ii) we removed one element t 0 from T , (iii) we computed the feature values for the problem instances in T and trained the regressor, (iv) we applied the trained regressor to the problem instance t 0 and compared the generated answer against the known one. We repeated all but first steps |T | = 100 times, i.e., by removing each time a different element, and computed the performance of the method in terms of the indexes defined in Section 1: c@1 and AUC.</p><p>The results are in Table <ref type="table" coords="5,247.77,276.39,3.88,8.64">1</ref>: the table shows c@1 and AUC values for each method, the method name being composed by the regressor acronym and one among abs or rel indicating the use of f or f features, respectively. It can be seen that RF provides c@1 AUC Method EN DU GR SP EN DU GR SP RF-abs 0.67 0.74 0.77 0.94 0.718 0.707 0.808 0.992 RF-rel 0.58 0.66 0.77 0.95 0.584 0.776 0.796 0.989 SVM-abs 0.48 0.67 0.69 0.92 0.513 0.707 0.754 0.978 SVM-rel 0.45 0.62 0.66 0.86 0.584 0.645 0.732 0.936 Tree-abs 0.69 0.70 0.53 0.94 0.725 0.708 0.557 0.951 Tree-rel 0.56 0.62 0.69 0.97 0.526 0.595 0.699 0.992 Table <ref type="table" coords="5,261.84,422.08,3.36,8.06">1</ref>. c@1 and AUC for 6 methods.</p><p>in general better results than the other regressors; moreover, RF-abs appears to be the best performing method. In order to further validate the latter finding, we performed a Wilcoxon signed-rank test <ref type="bibr" coords="5,243.99,488.79,11.62,8.64" target="#b0">[1]</ref> with a significance level of 5% and Bonferroni correction <ref type="bibr" coords="5,153.44,500.74,10.79,8.64" target="#b1">[2]</ref>: the outcome is that RF-abs is significantly better than all the other methods, except Tree-rel, for a little gap, and RF-rel; RF-rel is not better than the other methods except SVM-rel; Tree-rel is not better than all the other methods.</p><p>In order to gain insights about which features group appeared to be more suitable for accomplishing the considered task, we applied the RF-abs method (with the leaveone-out procedure described above) 9 times, each time removing one of the 9 features groups-i.e., we performed a features group ablation analysis. The results (in terms of c@1) are reported in Table <ref type="table" coords="5,246.10,584.71,3.74,8.64" target="#tab_0">2</ref>. It can be seen, by comparing results of method RF-abs with those of Table <ref type="table" coords="5,218.20,596.66,3.74,8.64">1</ref>, that the largest decrease of c@1 occurs by removing features group MG, while the smallest one occurs by removing features group WR-on the average around 3% and 1%, respectively. It can also be observed that feature ablation may actually lead to some improvements: for English, we obtain 0.69, rather than 0.67, by removing WG; for Spanish, we obtain 0.96, rather than 0.94, by removing either WG or WR. Then, we analyzed the performance of RF-abs in terms of feature addition. We considered RF-abs using only features group MG (which showed to be the most relevant, according to the ablation analysis) and then using only MG and each of the 8 other features groups in isolation. The results are reported in Table <ref type="table" coords="6,412.46,312.91,3.74,8.64" target="#tab_1">3</ref> that, for English, there are combinations that improve c@1 with respect to the baseline value 0.67: MG+CG, MG+SL, and MG+SG reach 0.73. Since such improvement is not negligible, we inspected the mutual effect of these features groups more closely by analyzing the c@1 values resulting from all their combinations. The results are: 0.78 with MG+CG+SL, 0.65 with CG+SG+SL, 0.71 with MG+SL+SG, and 0.73 with MG+CG+SL+SG. Based on these results, which improved the 0.67 baseline (all feature groups), we chose to use RF-abs with only 3 features groups (MG+CG+SL), only for the English language. On the other hand, we did not notice significant improvements for specific sets of features groups for the other languages: hence, for Dutch, Greek, and Spanish, we chose to use RF-abs with all the features groups.</p><p>We observed that the results for the Spanish language tend to be much better than for the other languages. We believe that such good results depend more on the peculiarity of this dataset rather than to the quality of our method: indeed the training set for Spanish contained 100 problem instances with 5 documents each, but the number of distinct documents, though, was only 42.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Final results</head><p>Table <ref type="table" coords="7,160.02,181.08,4.98,8.64" target="#tab_2">4</ref> reports the final results obtained in the competition, as released by the organizers <ref type="foot" coords="7,158.56,191.37,3.49,6.05" target="#foot_4">5</ref> . The table shows the performance indexes computed on a separated testing set which was not available during the method design phase. Besides c@1 and AUC, the table also reports a score, according to which a ranking for each language has been compiled: the score is the product of c@1 and AUC. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,187.09,118.07,241.18,117.71"><head>Table 2 .</head><label>2</label><figDesc>c@1 with RF-abs by removing one features group at once.</figDesc><table coords="6,240.62,118.07,134.11,106.80"><row><cell cols="2">Features groups EN DU GR SP</cell></row><row><cell>All-WG</cell><cell>0.69 0.68 0.75 0.96</cell></row><row><cell>All-CG</cell><cell>0.66 0.71 0.75 0.95</cell></row><row><cell>All-PG</cell><cell>0.68 0.70 0.75 0.94</cell></row><row><cell>All-WL</cell><cell>0.67 0.71 0.75 0.95</cell></row><row><cell>All-SL</cell><cell>0.65 0.70 0.73 0.95</cell></row><row><cell>All-SG</cell><cell>0.66 0.69 0.75 0.95</cell></row><row><cell>All-WR</cell><cell>0.67 0.71 0.75 0.96</cell></row><row><cell>All-MG</cell><cell>0.62 0.71 0.74 0.94</cell></row><row><cell>All-TG</cell><cell>0.63 0.72 0.75 0.93</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,153.16,312.91,327.43,156.40"><head>Table 3 .</head><label>3</label><figDesc>. It can be seen c@1 with RF-abs by using MG features and zero or one other features group.</figDesc><table coords="6,240.62,351.60,134.11,106.80"><row><cell cols="2">Features groups EN DU GR SP</cell></row><row><cell>MG</cell><cell>0.71 0.63 0.66 0.89</cell></row><row><cell>MG+WG</cell><cell>0.67 0.71 0.75 0.94</cell></row><row><cell>MG+CG</cell><cell>0.73 0.63 0.68 0.93</cell></row><row><cell>MG+PG</cell><cell>0.71 0.67 0.68 0.94</cell></row><row><cell>MG+WL</cell><cell>0.72 0.65 0.66 0.91</cell></row><row><cell>MG+SL</cell><cell>0.73 0.65 0.72 0.90</cell></row><row><cell>MG+SG</cell><cell>0.73 0.58 0.71 0.87</cell></row><row><cell>MG+WR</cell><cell>0.59 0.56 0.60 0.74</cell></row><row><cell>MG+TG</cell><cell>0.72 0.64 0.68 0.91</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,194.64,262.17,226.08,63.15"><head>Table 4 .</head><label>4</label><figDesc>Final results.</figDesc><table coords="7,194.64,262.17,226.08,52.29"><row><cell>Method</cell><cell cols="2">Language c@1 AUC Score Ranking</cell></row><row><cell cols="2">RF-abs on MG+CG+SL EN</cell><cell>0.56 0.578 0.323 10/18</cell></row><row><cell>RF-abs on all</cell><cell>DU</cell><cell>0.69 0.751 0.518 4/17</cell></row><row><cell>RF-abs on all</cell><cell>GR</cell><cell>0.66 0.698 0.459 7/14</cell></row><row><cell>RF-abs on all</cell><cell>SP</cell><cell>0.83 0.932 0.773 1/17</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,591.33,335.86,7.77;1,144.73,602.29,335.86,7.77;1,144.73,613.25,335.86,7.77;1,144.73,624.21,335.86,7.77;1,144.73,635.17,335.86,7.77;1,144.73,646.13,335.86,7.77;1,144.73,657.08,40.78,7.77"><p>During the competition we discovered several opportunities for fraudulently boosting the accuracy of our method during the evaluation phase. We will describe these opportunities in a future report. We notified the organizers which promptly acknowledged the high relevance of our concerns and took measures to mitigate the corresponding vulnerabilities. The organizers acknowledged our contribution publicly. We submitted for evaluation an honestly developed method-the one described in this document-that did not exploit such unethical procedures in any way.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,144.73,657.93,134.50,6.31"><p>http://opennlp.apache.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,144.73,646.78,188.29,6.31"><p>http://nlp.cs.aueb.gr/software.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="3,144.73,657.93,296.38,6.31"><p>http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="7,144.73,657.93,258.73,6.31"><p>http://www.tira.io/task/authorship-verification/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,138.13,402.93,321.30,7.77;7,146.47,413.89,172.59,7.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,191.96,402.93,175.31,7.77">Constructing confidence sets using rank statistics</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">F</forename><surname>Bauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,372.53,402.93,86.90,7.77;7,146.47,413.89,79.95,7.77">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">339</biblScope>
			<biblScope unit="page" from="687" to="690" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.13,424.85,331.75,7.77;7,146.47,435.81,278.52,7.77;7,146.47,446.77,135.98,7.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,252.32,424.85,217.55,7.77;7,146.47,435.81,99.34,7.77">Controlling the false discovery rate: a practical and powerful approach to multiple testing</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Benjamini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Hochberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,251.32,435.81,173.67,7.77;7,146.47,446.77,58.83,7.77">Journal of the Royal Statistical Society. Series B (Methodological</title>
		<imprint>
			<biblScope unit="page" from="289" to="300" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.13,457.73,250.39,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,194.28,457.73,55.51,7.77">Random forests</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,255.30,457.73,63.00,7.77">Machine learning</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.13,468.69,341.01,7.77;7,146.47,479.65,214.79,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,230.91,468.69,162.95,7.77">Libsvm: a library for support vector machines</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,400.12,468.69,79.02,7.77;7,146.47,479.65,155.77,7.77">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.13,490.60,341.98,7.77;7,146.47,501.56,165.35,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,187.58,490.60,121.18,7.77">Classification and regression trees</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Y</forename><surname>Loh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,314.19,490.60,165.91,7.77;7,146.47,501.56,95.12,7.77">Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="14" to="23" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.13,512.52,338.56,7.77;7,146.47,523.48,303.91,7.77;7,146.47,534.44,55.53,7.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,191.80,512.52,200.51,7.77">Probabilistic part-of-speech tagging using decision trees</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,409.95,512.52,66.74,7.77;7,146.47,523.48,230.65,7.77">Proceedings of the international conference on new methods in language processing</title>
		<meeting>the international conference on new methods in language processing</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="44" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.13,545.40,322.93,7.77;7,146.47,556.36,309.68,7.77;7,146.47,567.32,306.50,7.77;7,146.47,578.28,98.46,7.77;7,146.47,590.08,296.89,6.31" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,182.35,556.36,182.63,7.77">Overview of the Author Identification Task at PAN</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Daelemans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Verhoeven</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Juola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lopez Lopez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
	</analytic>
	<monogr>
		<title level="m" coord="7,401.83,556.36,54.32,7.77;7,146.47,567.32,149.87,7.77">Working Notes Papers of the CLEF 2015 Evaluation Labs</title>
		<title level="s" coord="7,302.51,567.32,150.46,7.77;7,146.47,578.28,19.77,7.77">CEUR Workshop Proceedings, CLEF and CEUR</title>
		<imprint>
			<date type="published" when="2015-09">2015. Sep 2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
