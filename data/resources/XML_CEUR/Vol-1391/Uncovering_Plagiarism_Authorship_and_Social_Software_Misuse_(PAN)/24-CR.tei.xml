<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,159.53,115.90,296.31,12.90;1,146.96,133.83,321.44,12.90;1,223.43,153.68,168.50,10.75">A Machine Learning-based Intrinsic Method for Cross-topic and Cross-genre Authorship Verification Notebook for PAN at CLEF 2015</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,243.41,190.08,44.55,8.64"><forename type="first">Yunita</forename><surname>Sari</surname></persName>
							<email>y.sari@sheffield.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Sheffield Regent Court</orgName>
								<address>
									<addrLine>211 Portobello</addrLine>
									<postCode>S1 4DP</postCode>
									<settlement>Sheffield</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,307.33,190.08,64.63,8.64"><forename type="first">Mark</forename><surname>Stevenson</surname></persName>
							<email>mark.stevenson@sheffield.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Sheffield Regent Court</orgName>
								<address>
									<addrLine>211 Portobello</addrLine>
									<postCode>S1 4DP</postCode>
									<settlement>Sheffield</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,159.53,115.90,296.31,12.90;1,146.96,133.83,321.44,12.90;1,223.43,153.68,168.50,10.75">A Machine Learning-based Intrinsic Method for Cross-topic and Cross-genre Authorship Verification Notebook for PAN at CLEF 2015</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1A1DB983B792F26A6EB1D7E82D5B0253</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Machine Learning</term>
					<term>Intrinsic Method</term>
					<term>Authorship Verification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents our approach for the Author Identification task in the PAN CLEF Challenge 2015. We identified the challenges of this year's are the limited amount of training data and the problems in the sub-corpora are independent in terms of topic and genre. We adopted a machine learning based intrinsic method to verify whether a pair of documents have been written by same or different authors. Several content-independent features, such as function words and stylometric features, were used to capture the difference between documents. Evaluation results on the test corpora show our approach works best on the Spanish data set with 0.7238 and 0.67 for the AUC and C@1 scores respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1 Cross-topic and Cross-genre Authorship Verification Given a pair of documents (X,Y), the task of author verification is to identify whether the documents have been written by same or different authors. Compared to authorship attribution, the authorship verification task is significantly more difficult. Verification does not learn about the specific character of each author, but rather about the differences between a pair of documents. The problem is complicated by the fact that an author may consciously or unconsciously vary his/her writing style from text to text <ref type="bibr" coords="1,466.48,493.88,10.58,8.64" target="#b4">[5]</ref>. This year's PAN lab Author Identification task focuses on cross-genre and crosstopic authorship verification. In this case, the genre and/or topic may differ significantly between the known and unknown documents. This task is more representative of real world applications where we could not control the genre/topic of the documents.</p><p>The PAN Author idenfitication task is defined as follows: "Given a small set (no more than 5, possibly as few as one) of known documents by a single person and a questioned document, the task is to determine whether the questioned document was written by the same person who wrote the known document set. The genre and/or topic may differ significantly between the known and unknown docu-ments" [1]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Data set</head><p>The data set consists of author verification problems in four different languages. In each problem, there are some known documents written by single person and only one un-known document. The genre and/or topic between documents may differ significantly. The document length varies from a few hundred to a few thousand words. Table <ref type="table" coords="2,448.93,131.27,4.98,8.64" target="#tab_0">1</ref> shows the sub-corpora including their language and type (cross-genre or cross-topic) The author verification systems are tested on a set of problems. The system must provide a probability score for each unknown document. The performance of the system will be evaluated using area under the ROC curve (AUC). In addition, the output will also be measured based on c@1 score <ref type="bibr" coords="2,276.74,356.23,10.58,8.64" target="#b6">[7]</ref>. Probability score which is greater than 0.5 is considered as positive answer, while a score lower than 0.5 is considered as negative. If the score is 0.5, then it will be considered as an i don't know answer. The c@1 measure can be define as follows:</p><formula xml:id="formula_0" coords="2,238.16,411.52,242.44,22.31">c@1 = 1 n * n c + n u * n c n<label>(1)</label></formula><p>where:</p><p>n = number of problems n c = number of correct answer n u = number of unanswered problems</p><p>The overall performance will be evaluated on the product of AUC and c@1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodological Approach</head><p>We adopted a machine learning-based intrinsic method to address this verification problem. Intrinsic methods use only the provided documents (in this case known and unknown documents) to determine whether they are written by same author or not. A machine learning algorithm then will be trained on the labeled document pairs to construct a model which can be used to classify the unlabeled pairs. Note that in the verification problems, the machine learning does not learn about the specific character of each author, but rather about the differences between a pair of documents <ref type="bibr" coords="2,409.27,632.53,10.58,8.64" target="#b5">[6]</ref>. Texts are represented by various types of features such as function word, character n-grams, word n-grams and several stylometric features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Textual Representation</head><p>As the genre and/or topic may differ significantly between the known and unknown documents, we can not rely on the content based features to capture the differences between documents. We therefore focused more on content-independent features such as function words and stylometric features. In addition, those features can be applied to any of the language used in the task. We used six types of features in total including: stylometric features (10), function words, character 8-grams, character 3-grams, word bigrams, and word unigrams.</p><p>Given collection of problems P = {P i : ∀ i ∈ I} where I = {1, 2, 3, .., n} is the index of P . P i contains exactly one unknown document U and a set of known documents K = {K j : ∀ j ∈ J} where J is the index of K and 1 ≤ J ≤ 5. Our approach represented each problem P i as vector P i = {R 1 , R 2 , .., R n } where n is the maximum number of feature types (in our case are six). R i is the distance of two similar feature vector representation of a set of known documents K and unknown document U . If K contains more than one document, then the generated feature vector is an average vector of J documents. Table <ref type="table" coords="3,255.51,304.98,4.98,8.64" target="#tab_1">2</ref> shows details of the features vector representation and comparison measures used. Stylometric Features There are ten sytlometric features used in our experiment. Some features were adapted from Guthrie's work <ref type="bibr" coords="3,312.12,516.21,11.62,8.64" target="#b2">[3]</ref> on anomalous text detection and were among the most effective features to separate anomalous segments from normal segments of the text. The complete list of stylometric features are:</p><p>1. Average number of non standard word <ref type="foot" coords="3,304.82,556.33,3.49,6.05" target="#foot_0">1</ref>2. Average number of words per sentence 3. Percentage of short sentences (less than 8 words) 4. Percentage of long sentences (greater than 15 words) 5. Percentage word with three syllables 6. Lexical diversity (ratio of total number of unique words to total number of words in a document)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Total number of punctuations</head><p>We also implemented three readibility measures: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Distance Measures</head><p>We experimented with several different comparison measures for computing similarity between a pair of vectors. We noticed particular comparison metric performs better in certain type of features, thus we applied different measure for each features type. Three different distance measures were used:</p><p>Cosine similarity measure</p><formula xml:id="formula_1" coords="4,221.83,462.58,258.76,55.25">d(x, y) = x.y ||x||||y|| = p i=1 x i y i p i=1 x i 2 p i=1 y i 2<label>(5)</label></formula><p>Minimum maximum similarity measure</p><formula xml:id="formula_2" coords="4,236.58,551.24,244.01,51.30">minmax(x, y) = p i=1 min(x i , y i ) p i=1 max(x i , y i )<label>(6)</label></formula><p>City block distance (also called Manhattan distance or L 1 distance)</p><formula xml:id="formula_3" coords="4,260.46,632.48,220.13,30.79">d(x, y) = p i=1 |x i -y i |<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Feature selection and classifier</head><p>Our authorship identification software was written in Python. We applied feature selection using Extratreeclassifier and the SVM classifier. The classifier hyperparameters were optimized using the GridSearchCV. Scikit-learn library<ref type="foot" coords="5,374.71,165.25,3.49,6.05" target="#foot_1">2</ref> was used for both feature selection and classification.</p><p>3 Evaluation and Result</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training corpora</head><p>We evaluated the approach on the training data using 10-fold cross validation. Since there are some incompatibility issues, we did not perform the verification task on Greek data. Table <ref type="table" coords="5,179.87,289.83,4.98,8.64" target="#tab_3">3</ref> shows the result of our approach on three of the sub-language corpora. The best result was achieved on the Spanish data set with 0.846 and 0.807 for the AUC and C@1 scores respectively. Compared to other sub-language corpora, the Spanish data set contains more known documents; which may explain why the results on this data set are better than the results on the other sub-languages data. We also observed that the use of some NLP libraries which are mainly trained on English data did not perform well on the non-English language data sets. Thus, feature selection was applied to remove unhelpful features. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Testing corpora</head><p>Table <ref type="table" coords="5,159.81,550.75,4.98,8.64" target="#tab_4">4</ref> shows the official result of our approach on test data released by PAN 15 organizer. As predicted, our approach performed well on the data set which has more known documents. The best results were achieved on the Spanish data with final score of 0.48495. Our approach applied supervised learning where the performance depends strongly on the amount of training data. Thus, as can be seen in Table <ref type="table" coords="5,410.17,598.57,3.74,8.64" target="#tab_4">4</ref>, our verification task did not obtain good results on English data which has only one known document per problem. However, in term of runtime, our approach generally more efficient since all necessary processing were performed in the training phase. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This year's author verification problem is considerably harder than last year's since the number of known documents is very limited and the genre/topic between known and unknown documents differ significantly. In addition, for English, the data set was derived from Project Gutenberg's opera play scripts which are an unusual type of text.</p><p>We identified that the most challenging part of this task was to find suitable features which could capture the differences between documents. In addition, for certain data set, not all features were helpful. Thus applying feature selection were beneficial and greatly improved the accuracy of the classifier.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,134.77,176.56,284.55,131.99"><head>Table 1 :</head><label>1</label><figDesc>The authorship verification problems training data set</figDesc><table coords="2,134.77,202.06,242.07,106.49"><row><cell>Language</cell><cell>Type</cell><cell>Total_problems</cell></row><row><cell>Dutch</cell><cell>cross-genre</cell><cell>100</cell></row><row><cell cols="2">English cross-topic</cell><cell>100</cell></row><row><cell>Greek</cell><cell>cross-topic</cell><cell>100</cell></row><row><cell cols="2">Spanish cross-genre</cell><cell>100</cell></row><row><cell>1.2 Performance measure</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,149.27,349.27,316.82,125.95"><head>Table 2 :</head><label>2</label><figDesc>List of features and comparison measures</figDesc><table coords="3,149.27,374.42,316.82,100.79"><row><cell>Feature</cell><cell>Model</cell><cell>Comparison</cell><cell></cell></row><row><cell></cell><cell></cell><cell>method</cell><cell></cell></row><row><cell>(R1) Stylometric features</cell><cell>average feature's presence</cell><cell cols="2">min-max similarity</cell></row><row><cell>(R2) Function words</cell><cell>ratio function word to total number of</cell><cell>Manhattan</cell><cell>dis-</cell></row><row><cell></cell><cell>words in the document</cell><cell>tance</cell><cell></cell></row><row><cell>(R3) Character 8-grams</cell><cell>tf-idf</cell><cell cols="2">cosine similarity</cell></row><row><cell>(R4) Character 3-grams</cell><cell>tf-idf</cell><cell cols="2">cosine similarity</cell></row><row><cell>(R5) Word bigrams</cell><cell>tf-idf</cell><cell cols="2">cosine similarity</cell></row><row><cell>(R6) Word unigrams</cell><cell>tf-idf</cell><cell cols="2">cosine similarity</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,207.54,408.18,200.27,66.55"><head>Table 3</head><label>3</label><figDesc></figDesc><table coords="5,232.97,408.18,174.85,66.55"><row><cell>: 10-fold cross validation on the training corpora</cell></row><row><cell>Data set AUC C@1 finalScore</cell></row><row><cell>English 0.662 0.606 0.401</cell></row><row><cell>Dutch 0.618 0.553 0.342</cell></row><row><cell>Spanish 0.846 0.807 0.683</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,224.27,117.67,166.81,66.55"><head>Table 4 :</head><label>4</label><figDesc>Result on test data set</figDesc><table coords="6,224.27,143.17,166.81,41.05"><row><cell cols="3">Data set AUC C@1 finalScore Runtime</cell></row><row><cell>English 0.4011</cell><cell>0.5</cell><cell>0.20055 00:05:46</cell></row><row><cell cols="3">Dutch 0.61306 0.62075 0.38056 00:02:03</cell></row><row><cell cols="3">Spanish 0.7238 0.67 0.48495 00:03:47</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,144.73,646.13,335.86,7.77;3,144.73,657.08,130.66,7.77"><p>Enchant spell checking library (http://www.abisource.com/projects/enchant/) was used to identify non-standard English words</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="5,144.73,657.08,75.80,7.77"><p>http://scikit-learn.org</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,138.13,377.28,25.46,7.77;6,189.86,377.28,40.35,7.77;6,256.52,377.28,47.82,7.77;6,330.65,377.28,16.71,7.77;6,373.67,377.28,20.17,7.77;6,420.15,377.28,60.44,7.77;6,146.47,388.24,278.39,7.77" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="6,146.48,377.28,17.11,7.77;6,189.86,377.28,40.35,7.77;6,256.52,377.28,47.82,7.77;6,330.65,377.28,16.71,7.77">PAN Authorship Identification Task</title>
		<ptr target="https://www.uni-weimar.de/medien/webis/events/pan-15/pan15-web/author-identification.html" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.13,399.20,251.37,7.77" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gunning</surname></persName>
		</author>
		<title level="m" coord="6,195.29,399.20,113.39,7.77">The Technique of Clear Writing</title>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1952">1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.13,410.15,285.01,7.77" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="6,191.79,410.15,156.28,7.77">Unsupervised Detection of Anomalous Text</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Guthrie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct coords="6,138.13,421.11,342.46,7.77;6,146.47,432.07,334.12,7.77;6,146.47,443.03,192.19,7.77" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="6,367.95,421.11,112.65,7.77;6,146.47,432.07,334.12,7.77;6,146.47,443.03,86.96,7.77">Derivation of New Readability Formulas (Automated Readability Index, Fog Count and Flesch Reading Ease Formula) for Navy Enlisted Personnel</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Kincaid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">P</forename><surname>Fishburne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">S</forename><surname>Chissom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975-02">February. 1975</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct coords="6,138.13,453.99,342.46,7.77;6,146.47,464.95,269.93,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,229.67,453.99,214.90,7.77">Authorship verification as a one-class classification problem</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,450.93,453.99,29.66,7.77;6,146.47,464.95,223.61,7.77">Twentyfirst international conference on Machine learning -ICML &apos;04</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">62</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.13,475.91,342.46,7.77;6,146.47,486.87,334.12,7.77;6,146.47,497.83,140.76,7.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,234.09,475.91,222.14,7.77">Determining if two documents are written by the same author</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Winter</surname></persName>
		</author>
		<idno type="DOI">10.1002/asi.22954</idno>
		<ptr target="http://doi.wiley.com/10.1002/asi.22954" />
	</analytic>
	<monogr>
		<title level="j" coord="6,462.34,475.91,18.25,7.77;6,146.47,486.87,231.33,7.77">Journal of the Association for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="178" to="187" />
			<date type="published" when="2014-01">Jan 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.13,508.78,342.46,7.77;6,146.47,519.74,334.12,7.77;6,146.47,530.70,132.11,7.77" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,237.50,508.78,152.95,7.77">A simple measure to assess non-response</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rodrigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,411.30,508.78,69.29,7.77;6,146.47,519.74,334.12,7.77;6,146.47,530.70,46.08,7.77">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1415" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
