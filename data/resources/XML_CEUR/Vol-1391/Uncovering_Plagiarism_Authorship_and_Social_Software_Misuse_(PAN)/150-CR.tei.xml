<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,149.10,114.39,317.16,14.35;1,255.93,132.32,103.51,14.35;1,223.65,152.43,168.05,11.96">Author Identification using multi-headed Recurrent Neural Networks Notebook for PAN at CLEF 2015</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,274.33,188.76,66.69,9.96"><forename type="first">Douglas</forename><surname>Bagnall</surname></persName>
							<email>douglas@halo.gen.nz</email>
						</author>
						<title level="a" type="main" coord="1,149.10,114.39,317.16,14.35;1,255.93,132.32,103.51,14.35;1,223.65,152.43,168.05,11.96">Author Identification using multi-headed Recurrent Neural Networks Notebook for PAN at CLEF 2015</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C42CDD60651CECCCAC98085D23D7401E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recurrent neural networks (RNNs) are very good at modelling the flow of text, but typically need to be trained on a far larger corpus than is available for the PAN 2015 Author Identification task. This paper describes a novel approach where the output layer of a character-level RNN language model is split into several independent predictive sub-models, each representing an author, while the recurrent layer is shared by all. This allows the recurrent layer to model the language as a whole without over-fitting, while the outputs select aspects of the underlying model that reflect their author's style. The method proves competitive, ranking first in two of the four languages.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A recurrent neural network (RNN) is a feed forward neural network that shares parameters across time. At each time step t, a simple RNN has a hidden vector h t derived from the input vector x t and the previous hidden state h t-1 . The hidden vector is usually obtained via an affine transform offset by a bias vector b h , followed by a non-linear "activation" function, f h , which operates in a point-wise manner, giving the formula h t = f h (W hh h t-1 + W xh x t + b h ). The output vector y t is similarly obtained from the hidden state, with y t = f y (W hy h t + b y ), though the non-linear function f y is often not point-wise. When a discrete probability distribution is desired, the "softmax" function σ(z) j = e zj ∑ k e z k is common because it gives a correct distribution (all values positive and summing to 1) and has useful properties when it comes to training the network. See figure <ref type="figure" coords="1,435.16,535.57,3.74,9.96" target="#fig_0">1</ref>.</p><p>The self-referential hidden state allows the network to model complex time series processes. Training it to do so involves iteratively adjusting the weights and biases, usually using some form of gradient descent and back-propagation through time (BPTT). For the sake of brevity, the details of these algorithms are elided. Tomáš Mikolov's PhD thesis <ref type="bibr" coords="1,159.88,595.35,11.62,9.96" target="#b0">[1]</ref> offers a good introduction to these algorithms and the use of simple recurrent neural networks for language modelling, a task he shows they excel at.</p><p>A language model predicts the flow of text, one symbol at a time, estimating a probability distribution for the i-th symbol x i given its predecessors, or p(x i |x i-1 , x i-2 , . . . , x 1 ), where the symbols belong to a predetermined vocabulary. Language models for alphabetic languages are usually word-based or character-based, with the former having a large vocabulary with inevitable gaps, while the latter have a small vocabulary with few omissions. Word-based models achieve better results that in general, but require a lot of text and time to train. Character-based models perform better with tiny amounts of text and cope better with idiosyncratic usages. Figure <ref type="figure" coords="2,336.61,421.86,4.98,9.96" target="#fig_1">2</ref> shows a character-based language model trying to navigate its way through the character sequence cat.</p><p>The accuracy of a language model for a document can be measured as bits of cross entropy which is the mean negative binary log probability the model assigns to the symbols that actually occur. For a document of N characters, this is<ref type="foot" coords="2,403.55,468.97,3.97,6.12" target="#foot_0">1</ref> </p><formula xml:id="formula_0" coords="2,134.76,462.71,345.83,29.76">N ∑ N i -log 2 p(x i | x i-1 , x i-2 , . . . , x 1</formula><p>). Cross-entropy can be thought of a measure of the information the model fails to model. A language model that predicts every character in a document with probability 1 will result in a zero cross-entropy; on the other hand assigning probability 0 to an occurring character is very costly.</p><p>Supposing training is effective, a language model will better predict the flow of documents similar to the ones it was trained on-if that similarity is capturable by the model-and this will show up as reduced cross entropy relative to unrelated documents. The PAN author identification problem consists of 100 mini-corpora for each language. <ref type="bibr" coords="2,172.14,577.63,12.46,9.96" target="#b2">[3]</ref> 1 Each mini-corpus contains 1 to 5 documents known to be by a single author. These documents amount to a few thousand characters-generally fewer than in this paper-and the task is to decide whether another short document is by the same author. The unknown documents differ from the known ones in topic or genre. The cross entropy of this single step islog 2 (0.55) ≈ 0.86. On seeing the a it then gives the t about a 20% chance, and -log2(0.2) ≈ 2.32. The cross entropy over these two steps is the mean, 1.59.</p><p>It is plausible that a character-level RNN language model trained on an author's known output will match that author's unknown output more closely than it matches text written by others. Conventional language models are trained on millions of characters, and will severely over-fit on a corpus of a few thousand. To combat this, a multi-headed character-level language model is introduced, which shares a common recurrent state but multiple independent softmax output groups. Each softmax group is trained predominantly on one author's corpus, causing the recurrent layer to model a combination of all the author's texts, approximating the language as a whole. The output groups learn to weight the recurrent values in a way that best reflects their authors tendencies. Figure <ref type="figure" coords="3,475.61,496.75,4.98,9.96" target="#fig_2">3</ref> attempts to illustrate this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Text preprocessing</head><p>The known and unknown texts are mapped to a smaller character set to reduce computational complexity and remove the overwhelming self-importance of extremely rare characters. A separate mapping is used for each language.</p><p>The text is first converted into the NFKD unicode normal form, which decomposes accented letters into the letter followed by the a combining accent (for example, the code point &lt;U+00E0&gt; ("à") becomes &lt;U+0061&gt;&lt;U+0300&gt; ("a" followed by a combining grave marker). Capital letters are further decomposed into an uppercase marker followed Various rare characters that seemed largely equivalent are mapped together; for example the en-dash ("-") and em-dash ("-") are rare and appear to be used interchangeably in practice so these are mapped together. Some punctuation marks-such as the various forms of apostrophes and single quotation marks-are collapsed into canonical forms. It is likely that this discards some usable information-indications of finger habits or choice of software-but the avoidance of extremely rare characters is important. If only one author in the corpus uses a character, it may be assigned an excessive weight.</p><p>For the Greek text, all Latin characters are mapped to the letter s, arbitrarily chosen because it doesn't resemble a Greek character. The rationale is that foreign quotations and references appear too rarely for their content to be valuable and an attempt to model them would be wasteful, but the tendency to use them might be a useful signal. Following similar logic, all digits in all languages are mapped to 7. Runs of whitespace are collapsed into a single space.</p><p>At the end of this processing, any character with a frequency lower than 1 in 10,000 is discarded. Any characters occurring in a text but not in the resultant alphabet are ignored-there is no "unknown" token. Alphabet sizes are 39 for English, 47 for Dutch and Greek, and 51 for Spanish.</p><p>Finally, runs of more than 5 identical characters are truncated at 5. This is mainly aimed at the latin stretches in Greek text; there is little value in having the model try to guess the exact word length in a language it can't directly see. As an example, the following Greek text (the last paragraph of GR094/known02.txt in the training set):</p><p>«Τhis», μουρμουρίζει χαμογελώντας μελαγχολικά ο Αμερικάνος, ακούγοντας το Σαλονικιό «is the beginning of a beautiful friendship». Παίξτο ξανά.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>maps to this:</head><p>«¹τsss», μουρμουρι²ζει χαμογελω²ντας μελαγχολικα² ο ¹αμερικα²νος, ακου²γοντας το ¹σαλονικιο² «ss sss sssss ss s sssss sssss». ¹παι²ξτο ξανα².</p><p>where the superscript ¹ represents a capital marker attaching to the next character, while the superscript ² is an acute belonging to the previous character. In this case the transformation reveals that the author has spelt "This" with an uppercase τ (tau) instead of the visually identical uppercase t.</p><p>The character mappings were settled before training started and no attempts were made to test their efficacy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-headed recurrent neural network language model</head><p>This concept was briefly introduced in section 1. A single recurrent neural network is trained to predict the flow of text by many authors while sharing a collective model of the complete language. The output layer has a softmax group for each author, and sometimes another for a "control corpus"-a large body of text intended to help prevent over-fitting in the recurrent layer. For convenience the PAN 2014 training set is used for the control corpora. It turns out that there is some overlap between the 2014 and 2015 training sets (and possibly test sets), so the control corpora are not completely independent. Empirically they seem to have a very small positive effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">ReSQRT activation</head><p>The activation function used for the recurrent layer is a rectified shifted square root, or ReSQRT, defined as</p><formula xml:id="formula_1" coords="5,240.08,465.52,124.04,34.23">f (x) = { √ x + 1 -1 if x ≥ 0 0 otherwise.</formula><p>The derivative is 1 2 √ x+1 for x &gt; 0, and otherwise 0. In terms of y = f (x) (which is of practical use in training) the non-zero part is 1  2(y+1) . This follows the model of the widely used rectified linear unit (ReLU, <ref type="bibr" coords="5,309.23,535.57,13.66,9.96" target="#b1">[2]</ref> defined as f (x) = max(x, 0)) in that the output and derivative is zero for all non-positive numbers, which offers performance and propagative benefits and allows neurons to opt out of opining on areas outside their speciality. ReLU can be difficult to use in recurrent neural networks as the lack of inherent scale means it can slip into an explosive cycle of amplification more easily than conventional squashing activations like tanh. ReSQRT steers a middle course, and empirically works well for character based language models. The ReSQRT activation may not have been described before.</p><p>The training uses a form of adagrad learning, where the rate at which a weight can change is inversely proportional to the L 2 distance that it has already changed. This amounts to a monotonically decreasing per-weight learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Training</head><p>Training is structured as a number of "sub-epochs". In a sub-epoch each author provides a training text. Where there is one text for every author, a sub-epoch is the same as a conventional epoch; otherwise the author's texts are used in a cyclical way. In some runs all the texts of each author are concatenated-in these cases the sub-epoch is also a true epoch. In another mode, each sub-epoch is "balanced", with documents being drawn from each author until each has been trained around the same amount.</p><p>At each training step there is a chance of the training example "leaking" and affecting other authors, as if they had also made that particular choice of character. The initial leakage rate is in the order of 1/N where N is the number of authors, and it decays exponentially with each sub-epoch. Towards the end of training, the leakage rate is very low and each author's sub-model is being trained almost entirely on its own text. The parameters are thus roughly set during early training with high leakage and high learning rate, then refined to specialise on the author's style.</p><p>A mini-batch size of 40 is used, meaning the weights are modified by the accumulated deltas after every 40 characters. The training gradient for first 10 characters of each text is ignored. BPTT is truncated at a depth of 70, or sooner when the gradient diminishes below an adaptive threshold. The hidden layer is quite small; experiments with larger sizes showed no benefit.</p><p>Where there is a single "known" text, and it is significantly shorter than the "unknown" text, the two are swapped around and the RNN is trained on the unknown text.</p><p>Insufficient training text, which causes over-fitting, is worse than insufficient test text which only increases uncertainty.</p><p>The recurrent neural network code is written in C while the PAN specific parts are Python. <ref type="foot" coords="6,165.49,419.11,3.49,6.97" target="#foot_1">2</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Ensemble of variations</head><p>The final results combine several runs together, using a variety of configurations of metaparameters. Approximate ranges are shown in table 1. Each run consists of a training phase lasting several minutes or hours followed by the calculation of cross entropies for the unknown documents, which takes less than a second.</p><p>Sets of seemingly reasonable meta-parameters are chosen per-language via a haphazard search, and random selections of these configurations are used in the the evaluation ensembles. <ref type="foot" coords="6,203.83,549.82,3.49,6.97" target="#foot_2">3</ref> The evaluations are run with a time-out; after a set number of hours all the test scores obtained are averaged together. The length of time dedicated to each ensemble was determined by the looming submission deadline.</p><p>In tests with the training set (and in line with received wisdom), ensembles performed slightly better than the majority of their constituents. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Interpretation</head><p>With N predictive output clusters representing the N known authors in the corpus, the system produces N cross entropy scores for each document. The scores are not directly comparable to each other as there is both random variation in the various sub-models' performance and inherent variation in the texts' predictability. The scores were normalised on both axes. For each author's sub-model, the ranking of the score for the unknown text determines the probability that the author wrote the text. For example, if a text ranks first (i.e. has the lowest score) it is likely to be that author's work; if it ranks last, it is probably not. This system is likely not the best, but it worked well enough and got forgotten about. The final score must take the form of a probability between 0 and 1, with 0.5 having special significance for the C@1 calculation. The task is designed so that in exactly half the cases the unknown documents are by the known author, so (supposing confidence in the model) the correct way to align the 0.5 point is to take the median. Documents ranking better than the median receive a score above 0.5 linearly related to their ranking, while those ranking worse than the median receive a lower score. For the Spanish, Greek, and English tasks a small radius around the median was also assigned an 0.5 score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>Results are shown in table <ref type="table" coords="7,241.25,618.79,3.74,9.96" target="#tab_1">2</ref>.</p><p>The evaluation score for the Greek task is higher than expected, based on experiments with the training set, while that for Spanish is lower. The English and Dutch results are no surprise. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>While this method obviously has some merit, there are weaknesses too. The consistently bad results for the Dutch task seem to reflect a drastic genre difference between the known and unknown texts. Many competitors did a lot better in this task, presumably by not concentrating on the local flow of characters and instead using linguistic knowledge or otherwise deriving longer range features.</p><p>One the other hand, where the genre and topic seem closer, this model performs very well despite using no specialist linguistic or natural language processing techniques. It ought to extrapolate well to other languages. It also ought to perform very well in concert with unrelated methods.</p><p>Without an active recurrent layer the model falls to predicting unigram character probabilities per author via the output bias vector. The cross entropy in this case amounts to weighting the frequencies of characters in the text. When (accidentally) run this way on the training set, the AUC score was 0.85 for English and 0.91 for Spanish; for English this was one of the best training results. While this is humiliating for the RNN, it confirms the validity of the training and scoring methods. Leakage between authors allows each sub-model to learn background probabilities for the language as a whole, which is presumably better than the common technique of giving unseen symbols a fixed low chance. By being entirely additive, cross entropy sidesteps the curse of dimensionality. It also eschews (perhaps too much) the possibility of an aha moment involving strong positive evidence-positive evidence is typically accumulated a fraction of a bit at a time over the course of the entire document. This is in contrast to a typical human approach of identifying idiosyncratic usages and ignoring the boring bits in between; this contrast means the RNN might do well in ensembles with people. An RNN that directly reported an authorship probability distribution is conceivable but is unlikely to be easy to train. The character level language model learns to concentrate on the text; a more direct approach is likely to be more distracted. Tirelessly trying to understand the text is the strength of this method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,134.76,318.83,345.81,9.07;2,134.76,329.90,345.81,8.97;2,134.76,340.86,345.82,8.97;2,134.76,351.82,284.21,8.97;2,134.76,115.83,345.84,189.21"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The diagram on the left tries to show the output of the hidden nodes from the previous time step flowing round back into the hidden node inputs, but you are better off looking at the one on the right where the network has been unfolded through time. All layers are fully connected. At each time point the hidden state summarises the entire history of the input data.</figDesc><graphic coords="2,134.76,115.83,345.84,189.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,134.76,325.55,345.84,9.16;3,134.76,336.62,345.83,9.05;3,134.76,347.57,160.58,9.26;3,295.34,352.97,3.65,5.24;3,299.49,347.57,181.10,9.26;3,134.76,358.53,345.82,9.26;3,134.76,370.39,18.63,7.86;3,169.35,115.84,276.68,195.91"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. An example of a character based language model at work. When the model sees a c it (taking into account previous unseen characters) predicts the an a as having about a 55% chance.The cross entropy of this single step islog 2 (0.55) ≈ 0.86. On seeing the a it then gives the t about a 20% chance, and -log2(0.2) ≈ 2.32. The cross entropy over these two steps is the mean, 1.59.</figDesc><graphic coords="3,169.35,115.84,276.68,195.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,134.76,315.50,345.82,9.07;4,134.76,326.57,345.81,8.97;4,134.76,337.53,345.81,8.97;4,134.76,348.48,74.25,8.97;4,169.35,115.83,276.67,185.86"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. The multi-headed RNN language model has multiple sets of output probabilities-one for each author-each making slightly different predictions. The cross entropies with thus differ, with a relatively low cross entropy supporting the hypothesis that the corresponding author wrote the text being examined.</figDesc><graphic coords="4,169.35,115.83,276.67,185.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,134.76,118.79,345.81,193.69"><head>Table 1 .</head><label>1</label><figDesc>Meta-parameters used in training. Due the use of randomness, and the hidden nature of the evaluation, it is not precisely known which combinations of meta-parameters were used in the final ensembles.</figDesc><table coords="7,181.10,118.79,253.14,151.83"><row><cell>meta-parameter</cell><cell>typical values</cell></row><row><cell>initial adagrad learning scale</cell><cell>0.1, 0.14, 0.2, 0.3</cell></row><row><cell>initial leakage between classes</cell><cell>1 4N to 5 N</cell></row><row><cell>leakage decay (per sub-epoch)</cell><cell>0.67 to 0.9</cell></row><row><cell>hidden neurons</cell><cell>79, 99, 119, 139</cell></row><row><cell>presynaptic noise σ</cell><cell>0, 0.1, 0.2, 0.3, 0.5</cell></row><row><cell>sub-epochs</cell><cell>6 to 36</cell></row><row><cell>text direction</cell><cell>forward or backward</cell></row><row><cell>text handling</cell><cell>sequential, concatenated, balanced</cell></row><row><cell>initialisation</cell><cell>gaussian, zero</cell></row><row><cell>control corpus</cell><cell>PAN14 or none</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,134.76,118.79,345.82,130.13"><head>Table 2 .</head><label>2</label><figDesc>Results from the PAN Author Identification evaluation. The score is the product of the area under the ROC curve (AUC) and the C@1 score using 0.5 to indicate a non-decision. Rank indicates where the software placed in the competition. Where the software ranked first, the positive margin indicates how far its score was ahead of the nearest competitor; when negative the margin shows how far it was behind the leader.</figDesc><table coords="8,167.27,118.79,280.81,66.35"><row><cell>language</cell><cell>AUC</cell><cell>C@1</cell><cell>score</cell><cell>margin</cell><cell>rank</cell><cell>runtime</cell></row><row><cell>Dutch</cell><cell>0.70</cell><cell>0.64</cell><cell>0.45</cell><cell>-0.18</cell><cell>7</cell><cell>12:00:43</cell></row><row><cell>English</cell><cell>0.81</cell><cell>0.76</cell><cell>0.61</cell><cell>0.09</cell><cell>1</cell><cell>21:44:03</cell></row><row><cell>Greek</cell><cell>0.88</cell><cell>0.85</cell><cell>0.75</cell><cell>0.06</cell><cell>1</cell><cell>10:07:49</cell></row><row><cell>Spanish</cell><cell>0.89</cell><cell>0.81</cell><cell>0.72</cell><cell>-0.05</cell><cell>2</cell><cell>11:21:41</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.73,633.98,334.12,8.97;2,144.73,644.94,335.84,8.97;2,144.73,655.90,108.81,8.97"><p>http://www.uni-weimar.de/medien/webis/events/pan-15/pan15-web/author-identification.html has more information on the task. This paper was written in the context of that challenge and assumes some awareness of it.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="6,144.73,622.85,335.85,8.97;6,144.73,633.81,58.53,8.97"><p>See https://github.com/douglasbagnall/recur and https://github.com/pan-webis-de/caravel for software details.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="6,144.73,644.94,335.84,8.97;6,144.73,655.90,318.01,8.97"><p>The configuration pools from which the ensembles were chosen can be found at https://github.com/pan-webis-de/caravel/tree/master/config. Some decoding is necessary.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,138.12,141.69,329.05,8.97;9,146.47,152.65,162.56,8.97" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="9,192.37,141.69,198.26,8.97">Statistical Language Models Based on Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
		<respStmt>
			<orgName>Brno University of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph. D. thesis</note>
</biblStruct>

<biblStruct coords="9,138.12,163.61,323.10,8.97;9,146.47,174.57,314.37,8.97;9,146.47,185.53,57.53,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,229.07,163.61,215.96,8.97">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,146.47,174.57,296.65,8.97">Proceedings of the 27th International Conference on Machine Learning (ICML-10)</title>
		<meeting>the 27th International Conference on Machine Learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.12,196.49,322.52,8.97;9,146.47,207.45,334.11,8.97;9,146.47,218.40,280.52,8.97;9,146.47,229.36,297.37,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,182.02,207.45,181.90,8.97">Overview of the Author Identification Task at PAN</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Daelemans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Verhoeven</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Juola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lopez Lopez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
	</analytic>
	<monogr>
		<title level="m" coord="9,400.57,207.45,80.01,8.97;9,146.47,218.40,123.85,8.97">Working Notes Papers of the CLEF 2015 Evaluation Labs</title>
		<title level="s" coord="9,276.49,218.40,150.49,8.97;9,146.47,229.36,19.30,8.97">CEUR Workshop Proceedings, CLEF and CEUR</title>
		<imprint>
			<date type="published" when="2015-09">2015. Sep 2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
