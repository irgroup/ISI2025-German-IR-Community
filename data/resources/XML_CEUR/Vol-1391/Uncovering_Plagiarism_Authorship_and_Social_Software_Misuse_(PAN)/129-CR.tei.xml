<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,156.31,115.90,302.73,12.90;1,158.30,133.83,298.76,12.90;1,223.43,153.68,168.50,10.75">Authorship Verification by combining SVMs with kernels optimized for different feature categories Notebook for PAN at CLEF 2015</title>
				<funder ref="#_XJybcX4">
					<orgName type="full">CONACYT</orgName>
				</funder>
				<funder ref="#_wkuQVs7">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,169.95,190.08,66.69,8.64"><forename type="first">Julián</forename><surname>Solórzano</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Grupo de Ingeniería Lingüística</orgName>
								<orgName type="department" key="dep2">Instituto de Ingeniería</orgName>
								<orgName type="institution">UNAM</orgName>
								<address>
									<settlement>Mexico City</settlement>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,246.09,190.08,65.58,8.64"><forename type="first">Víctor</forename><surname>Mijangos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Grupo de Ingeniería Lingüística</orgName>
								<orgName type="department" key="dep2">Instituto de Ingeniería</orgName>
								<orgName type="institution">UNAM</orgName>
								<address>
									<settlement>Mexico City</settlement>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,321.13,190.08,77.76,8.64"><forename type="first">Alejandro</forename><surname>Pimentel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Grupo de Ingeniería Lingüística</orgName>
								<orgName type="department" key="dep2">Instituto de Ingeniería</orgName>
								<orgName type="institution">UNAM</orgName>
								<address>
									<settlement>Mexico City</settlement>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,408.33,190.08,37.07,8.64;1,191.01,202.03,66.95,8.64"><forename type="first">Fernanda</forename><surname>López-Escobedo</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Licenciatura de Ciencia Forense</orgName>
								<orgName type="department" key="dep2">Facultad de Medicina</orgName>
								<orgName type="institution">UNAM</orgName>
								<address>
									<settlement>Mexico City</settlement>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,267.41,202.03,67.23,8.64"><forename type="first">Azucena</forename><surname>Montes</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Grupo de Ingeniería Lingüística</orgName>
								<orgName type="department" key="dep2">Instituto de Ingeniería</orgName>
								<orgName type="institution">UNAM</orgName>
								<address>
									<settlement>Mexico City</settlement>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,360.96,202.03,58.92,8.64"><forename type="first">Gerardo</forename><surname>Sierra</surname></persName>
							<email>gsierram@iingen.unam.mx</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Grupo de Ingeniería Lingüística</orgName>
								<orgName type="department" key="dep2">Instituto de Ingeniería</orgName>
								<orgName type="institution">UNAM</orgName>
								<address>
									<settlement>Mexico City</settlement>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,156.31,115.90,302.73,12.90;1,158.30,133.83,298.76,12.90;1,223.43,153.68,168.50,10.75">Authorship Verification by combining SVMs with kernels optimized for different feature categories Notebook for PAN at CLEF 2015</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">58D1E9E6EC17F92EB8A47D73BC299490</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>authorship verification</term>
					<term>one-class SVM</term>
					<term>kernel selection</term>
					<term>logistic regression</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present our approach to the PAN-2015 authorship verification task. We combine one-class SVM classifiers under the hypothesis that different categories of features a) are better suited for different authors and b) have different underlying topologies. Thus, we have each classifier operate in a different feature subset with a different kernel function, and the output is used to train a logistic regression model which assigns a different weight to each category of features. Results show that further improvement of the method is needed, and we discuss its shortcomings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper presents our approach to the Author Verification task in PAN at CLEF 2015. Author Verification is one several of authorship analysis tasks, in which it must be determined whether a given text was written or not by a certain author <ref type="bibr" coords="1,416.61,494.63,10.58,8.64" target="#b7">[8]</ref>.</p><p>In the present task, a single problem consists of a set of documents, one of which is labeled as unknown and the rest are labeled as known. There can be a total of up to 6 documents in a single problem. The task consists in determining whether the document labeled as unknown is written by the same author as the rest of the documents. There are four different sets of problems, one for each of the following languages: Spanish, English, Greek and Dutch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>The main idea behind the methodology we present is to train classifiers that are to become experts at analyzing features from a specific category of features, and then combine the knowledge of all these classifiers. According to <ref type="bibr" coords="1,379.46,644.48,10.58,8.64" target="#b0">[1]</ref>, this is one of the two approaches to multi feature-set techniques that have been used in stylistic analysis. We hypothesize that an ensemble classifier will automatically assign more weight to each author's distinctive feature subsets without having to try various feature combinations.</p><p>Finally, there is also the observation that not all feature spaces are necessarily equal and that different feature subspaces may have different underlying topologies. Learning the best distance metric leads to improvements in distance-based classification schemes <ref type="bibr" coords="2,134.77,179.09,10.58,8.64" target="#b8">[9]</ref>. In this work we experiment with different kernel functions instead of distance functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Document Representation</head><p>Tagging Documents are tokenized and the Part of Speech (POS) tag of each token is obtained. Table <ref type="table" coords="2,200.36,251.80,4.98,8.64" target="#tab_0">1</ref> indicates the softwares we used to do this processing according to each language. Style Features For each document we obtain features in the following categories:</p><p>-Punctuation marks -tokens recognized as punctuation marks by the corresponding tagger -Multi word terms -tokens made up of more than one word (as determined by the tagger, in this case only obtained for Spanish and English) -Lexical features -Class intervals of word and sentence lengths -Start of sentence word profile -the grammatical category of the word at the beginning of sentences -End of sentence word profile -the grammatical category of the word at the end of sentences -Function words n-grams -1-grams, 2-grams and 3-grams -Function words skip-grams -2-grams and 3-grams, with up to 2 gaps -POS full tags n-grams -1-grams, 2-grams and 3-grams -POS abbreviated tags n-grams -1-grams, 2-grams and 3-grams. This only applies for languages in which the tagger outputs detailed POS tags in the first place (Spanish and English in this case). The abbreviated POS tag contains the grammatical category without further details such as number, gender, tense etc. -Character ngrams 2-grams and 3-grams All these features are used to create the vector representation of the document, i.e. a vector whose elements are the relative frequencies of each feature in that document (the frequency is relative to the total number of features in the category). A maximum of 200 features of each category was taken into account for a single document.</p><p>Distance-to-the-average An additional representation for each document is created as follows. First we compute a vector v avg which contains the average frequency of all the features among all documents in the dataset. This vector is broken down into n subvectors such that each one contains the features of one of the n feature categories. Then for each document we similarly break down its frequency vector into n subvectors and obtain their distance to the corresponding subvectors of v avg .</p><p>The resulting matrix encodes information about how a document deviates from the mean in each feature category, similar to Burrow's Delta <ref type="bibr" coords="3,365.60,271.58,11.62,8.64" target="#b1">[2]</ref> (only he used z-scores to normalize).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Classification</head><p>The classification is done by an ensemble that has its votes combined by means of a logistic regression model. The ensemble is comprised by n one-class SVM classifiers, each one using the features from one of the n feature categories, as well as an additional classifier that works with the distance-to-average representation of the documents. So, there is a total of n + 1 classifiers.</p><p>We separate each problem matrix into n feature category matrices (plus the distanceto-average matrix). In these matrices, each document represents a point on a space X. So, d 1 , ..., d n ∈ X, where d i , i ∈ {1, ..., n} are row vectors of each feature matrix representing a document. We assume points in this space are close to each other when written by the same author. Given a new point in this space, we want to determine if it is part of the cluster or it lies outside.</p><p>Dimensionality Reduction For the case of the feature frequency matrices, their highdimensionality makes them difficult to process. We perform dimensionality reduction by taking their first two eigenvectors, which correspond to the two highest eigenvalues; i. e. the eigenvectors with highest variance. To create the new matrices we use these eigenvectors as columns; so our new data set then is in R 2 .</p><p>This not only reduced data dimensionality, but also filtered noise. We empirically noted that the performance of the experiments increased by considering only these eigenvectors.</p><p>Novelty Detection through One-Class SVM To apply novelty detection using a one class SVM, we first think of a map φ : X → H, where H is a dot product space such that we can evaluate the dot product in the image of φ by a kernel function:</p><formula xml:id="formula_0" coords="3,256.01,656.12,224.58,9.65">k(x i , x j ) = φ(x i ) • φ(x j )<label>(1)</label></formula><p>We need to detect a neighborhood of the data points such that given a new document of the same author it lies inside this neighborhood. To do this, the one-class SVM finds a function f that returns +1 if the point lies into the neighborhood and returns -1 otherwise.</p><p>The value of the function evaluated at a new point y is obtained considering in which part of the hyperplane it falls on. So, we need to separate the data set from the origin, solving:</p><formula xml:id="formula_1" coords="4,230.06,214.50,250.53,26.65">min w∈H,ζ∈R n ,c∈R 1 2 ||w|| 2 + 1 v n i ζ i -c<label>(2)</label></formula><p>Where v ∈ (0, 1) and w</p><formula xml:id="formula_2" coords="4,250.93,251.95,102.76,9.65">• φ(x i ) ≤ c -ζ i , ζ i ≤ 0.</formula><p>This way, we can turn it into a decision function:</p><formula xml:id="formula_3" coords="4,250.48,291.34,230.11,8.96">f (x) = sgn((w • φ(x)) -c)<label>(3)</label></formula><p>such that it will be positive for the points in the data set; here the term ||w|| is a support vector type regularization. Deriving the dual problem with the kernel function showed in (1), the solution for a the new point y has a support vector expansion:</p><formula xml:id="formula_4" coords="4,242.73,364.13,237.86,19.91">f (y) = sgn( i α i k(x i , y) -c)<label>(4)</label></formula><p>Where x i with α i = 0 are support vectors. We calculate the one-class SVM for each feature matrix, obtaining a total of n outputs or judges. For each feature matrix we use different kernel functions for equation 4. One of the simplest is the linear kernel:</p><formula xml:id="formula_5" coords="4,263.57,443.56,217.02,12.69">k(x i , x j ) = x T i x j + c<label>(5)</label></formula><p>The gaussian kernel is defined as:</p><formula xml:id="formula_6" coords="4,242.88,490.09,237.71,23.89">k(x i , x j ) = exp(- ||x i -x j || 2 2σ 2 )<label>(6)</label></formula><p>A sigmoid kernel is defined as:</p><formula xml:id="formula_7" coords="4,246.53,547.85,234.06,12.69">k(x i , x j ) = tanh(αx T i x j + c)<label>(7)</label></formula><p>Finally, the polynomial kernel:</p><formula xml:id="formula_8" coords="4,252.71,596.73,227.88,12.69">k(x i , x j ) = (αx T i x j + c) m<label>(8)</label></formula><p>These different kernels functions correspond to each data distribution and were selected in the training process. We select one kernel for each feature matrix by evaluating every kernel over the training set and selecting the one with highest evaluation for each feature matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Logistic Regression</head><p>The final classification is done by means of a logistic regression. We take the n classifier outputs as judges voting for Yes or No (written by the same author or not). For each problem, we create a vector with these votes (1 if the judge thinks the document is written by the same author and 0 if not) and use the resulting matrix to train a logistic regression model to obtain the weight of each feature category. Intuitively these weights describe the relevance of each feature category in the author's style.</p><p>To do this, we use the training data set points x i , i = {1, ..., n} and the set Y of the training data labels (the output of the classifiers), such that Y = {y : y = {0, 1}}. We want to obtain weights w i for each point x i solving the equation ( <ref type="formula" coords="5,396.81,227.10,3.60,8.64" target="#formula_9">9</ref>):</p><formula xml:id="formula_9" coords="5,277.06,246.54,203.53,30.32">y = n i=1 w i • x i<label>(9)</label></formula><p>With these weights for each judge we then calculate the probability of a class in the evaluation set by the equation:</p><formula xml:id="formula_10" coords="5,280.21,315.17,200.38,23.89">p(z) e w•z 1 + e w•z<label>(10)</label></formula><p>Where z is the vector of judges for the unknown author and w is the weights vector obtained by solving <ref type="bibr" coords="5,216.36,357.92,11.62,8.64" target="#b8">(9)</ref> for {w i }. So we take this probability to determinate if the new point is part of the given set or if it is not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Results</head><p>Results of the evaluation are shown in Table <ref type="table" coords="5,313.08,448.47,3.74,8.64" target="#tab_1">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Discussion</head><p>Various problems exist in the methodology. First, the model that the logistic regression learns is a single one-fit-all model for all problems. This is not ideal because one of the hypotheses is that the effectiveness of each feature category is dependent on the author we want to identify. Thus a different linear regression model should be generated for each author.</p><p>Second, the only training data we provide to each SVM classifier is the unknown texts of a single problem. Evidently, this is less than ideal, specially in cases where there is only one unkown document. In these cases we split the unknown documents into three shorter documents, which does allow the program to run the SVM algorithm but it is not sufficient information to create a general model. A better use of the distance-to-average matrix was needed in order to account for the little information each problem presented, since we opted to not use the traditional Impostors approach.</p><p>Finally, linear regression is not necessarily the best approach for combining the classifiers. There is much literature on ensemble classifiers and the possible ways on generating the weights or scores of each one. Some examples of factors that the combining function can take into account include assigning more weight to classifiers that correctly classified hard instances, where hard refers to the fact that none or almost none of the other classifiers correctly classified them <ref type="bibr" coords="6,324.86,286.69,10.58,8.64" target="#b3">[4]</ref>.</p><p>Also, we consider that for our hypothesis to be true the combining function should always find the best features of the author no matter which feature categories were used in the first place. Yet we observe that the method performs differently depending on which feature categories were included in the experiment. For example, preliminar runs where not all feature categories had been added (specifically "Multi word terms", "End of sentence word profile" and "Character n-grams"), tended to performed better (reaching a training c@1 score of 0.8 in the Spanish dataset). This is most likely due to the logistic regression not being able to handle a large number of features at least without some selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and Future Work</head><p>Improvement of the method is needed, especially on the way the classifiers are combined. Ideally it could use a relatively large feature set without losing performance, as the Writeprints <ref type="bibr" coords="6,196.89,471.00,11.62,8.64" target="#b0">[1]</ref> method suggests, or as previous algorithms in this same task such as <ref type="bibr" coords="6,134.77,482.95,11.62,8.64" target="#b2">[3]</ref> have successfully shown.</p><p>Also, regarding the way instances are to be compared against control examples, either the Impostors approach must be adopted or else further experimentation must be done with the distance-to-average matrix in order to truly take advantage of the information it tells us about the corpus.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,251.85,294.82,111.15,76.71"><head>Table 1 .</head><label>1</label><figDesc>Taggers</figDesc><table coords="2,251.85,315.79,111.15,55.74"><row><cell>Language</cell><cell>Software</cell></row><row><cell>Spanish</cell><cell>Freeling [5]</cell></row><row><cell>English</cell><cell>Freeling</cell></row><row><cell>Dutch</cell><cell>TreeTagger [6][7]</cell></row><row><cell>Greek</cell><cell>Greek POS Tagger 1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,244.46,478.11,126.44,74.68"><head>Table 2 .</head><label>2</label><figDesc>Results</figDesc><table coords="5,244.46,497.25,126.44,55.55"><row><cell cols="2">Language AUC c@1 Combined</cell></row><row><cell>Dutch</cell><cell>0.396 0.384 0.152</cell></row><row><cell cols="2">English 0.517 0.5 0.258</cell></row><row><cell>Greek</cell><cell>0.589 0.56 0.33</cell></row><row><cell cols="2">Spanish 0.454 0.48 0.217</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.73,657.93,301.26,6.31"><p>http://php-nlp-tools.com/blog/category/greek-pos-tagger/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments This work is funded by the project <rs type="grantNumber">PAPIIT-UNAM IN400312</rs> "<rs type="projectName">Análisis estilométrico para la detección de similitud textual</rs>", as well as <rs type="funder">CONACYT</rs> <rs type="grantNumber">CB2012/178248</rs> "Detección y medición automática de similitud textual"</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_XJybcX4">
					<idno type="grant-number">PAPIIT-UNAM IN400312</idno>
					<orgName type="project" subtype="full">Análisis estilométrico para la detección de similitud textual</orgName>
				</org>
				<org type="funding" xml:id="_wkuQVs7">
					<idno type="grant-number">CB2012/178248</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,138.13,635.17,341.68,7.77;6,146.47,646.13,333.22,7.77;6,146.47,657.09,30.63,7.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,226.41,635.17,253.41,7.77;6,146.47,646.13,119.07,7.77">Writeprints: A stylometric approach to identity-level identification and similarity detection in cyberspace</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Abbasi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,271.59,646.13,184.20,7.77">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.13,119.96,342.45,7.77;7,146.47,130.92,177.82,7.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,191.89,119.96,254.01,7.77">Delta: A measure of stylistic difference and a guide to likely authorship</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Burrows</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,451.70,119.96,28.88,7.77;7,146.47,130.92,94.15,7.77">Literary and Linguistic Computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="267" to="287" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.13,141.88,316.66,7.77;7,146.47,152.84,164.48,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,224.25,141.88,230.53,7.77;7,146.47,152.84,26.29,7.77">A slightly-modified gi-based author-verifier with lots of features (asgalf)</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Khonji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Iraqi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,190.49,152.84,100.28,7.77">Notebook for PAN at CLEF</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.13,163.80,338.13,7.77;7,146.47,174.76,267.89,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,286.18,163.80,190.08,7.77;7,146.47,174.76,34.47,7.77">A weight-adjusted voting algorithm for ensembles of classifiers</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,186.32,174.76,144.37,7.77">Journal of the Korean Statistical Society</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="437" to="449" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.13,185.72,330.50,7.77;7,146.47,196.67,329.40,7.77;7,146.47,207.63,42.58,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,242.37,185.72,154.84,7.77">Freeling 3.0: Towards wider multilinguality</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Padró</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stanilovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,415.09,185.72,53.54,7.77;7,146.47,196.67,236.95,7.77">Proceedings of the Language Resources and Evaluation Conference (LREC 2012)</title>
		<meeting>the Language Resources and Evaluation Conference (LREC 2012)<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<publisher>ELRA</publisher>
			<date type="published" when="2012-05">May 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.13,218.59,338.55,7.77;7,146.47,229.55,303.89,7.77;7,146.47,240.51,55.53,7.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,191.80,218.59,200.50,7.77">Probabilistic part-of-speech tagging using decision trees</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,409.94,218.59,66.73,7.77;7,146.47,229.55,230.65,7.77">Proceedings of the international conference on new methods in language processing</title>
		<meeting>the international conference on new methods in language processing</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="44" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.13,251.47,321.34,7.77;7,146.47,262.43,219.11,7.77" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,191.80,251.47,251.31,7.77">Improvements in part-of-speech tagging with an application to german</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,146.47,262.43,156.81,7.77">Proceedings of the ACL SIGDAT-Workshop</title>
		<meeting>the ACL SIGDAT-Workshop</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.13,273.39,339.85,7.77;7,146.47,284.35,257.60,7.77" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,202.75,273.39,182.00,7.77">A survey of modern authorship attribution methods</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,391.08,273.39,86.90,7.77;7,146.47,284.35,173.93,7.77">Journal of the American Society for information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="538" to="556" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.13,295.30,332.11,7.77;7,146.47,306.26,298.11,7.77;7,146.47,317.22,66.49,7.77" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,294.28,295.30,175.96,7.77;7,146.47,306.26,79.91,7.77">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,244.09,306.26,183.21,7.77">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page" from="1473" to="1480" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
