<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,169.29,115.90,276.78,12.90;1,155.65,133.83,297.84,12.90">Towards Data Submissions for Shared Tasks: First Experiences for the Task of Text Alignment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,176.55,171.88,60.37,8.64"><forename type="first">Martin</forename><surname>Potthast</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Web Technology &amp; Information Systems</orgName>
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,247.72,171.88,50.56,8.64"><forename type="first">Steve</forename><surname>Göring</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Web Technology &amp; Information Systems</orgName>
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,309.63,171.88,47.41,8.64"><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Natural Language Engineering Lab</orgName>
								<orgName type="institution">Universitat Politècnica de València</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,385.35,171.88,48.99,8.64"><forename type="first">Benno</forename><surname>Stein</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Web Technology &amp; Information Systems</orgName>
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,169.29,115.90,276.78,12.90;1,155.65,133.83,297.84,12.90">Towards Data Submissions for Shared Tasks: First Experiences for the Task of Text Alignment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CCCC7E2592210075ABBE6D51CAD9434E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper reports on the organization of a new kind of shared task that outsources the creation of evaluation resources to its participants. We introduce the concept of data submissions for shared tasks, and we use our previous shared task on text alignment as a testbed. A total of eight evaluation datasets have been submitted by as many participating teams. To validate the submitted datasets, they have been manually peer-reviewed by the participants. Moreover, the submitted datasets have been fed to 31 text alignment approaches in order to learn about the datasets' difficulty. The text alignment implementations have been submitted to our shared task in previous years and since been kept operational on the evaluation-as-a-service platform TIRA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="29" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="30" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="31" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="32" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="33" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="34" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="35" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The term "shared task" refers to a certain kind of computer science event, where researchers working on a specific problem of interest, the task, convene to compare their latest algorithmic approaches at solving it in a controlled laboratory experiment. <ref type="foot" coords="1,458.22,444.77,3.49,6.05" target="#foot_0">1</ref> The organizers of a shared task usually take care of the lab setup by compiling evaluation resources, by selecting performance measures, and sometimes even by raising the task itself for the first time.</p><p>The way shared tasks are organized at present may strongly influence future evaluations: for instance, in case the event has sufficient success within its target community, subsequent research on the task may be compelled to follow the guidelines proposed by the shared task organizers, or else risk rejection from reviewers who are aware of the shared task. The fact that a shared task has been organized may amplify the importance of its experimental setup over others, stifling contributions off the beaten track. However, there is only anecdotal evidence in support of this narrowing effect of shared tasks. Nevertheless, it has been frequently pointed out in the position papers submitted to a workshop organized by the natural language generation community on the pros and cons of adopting shared tasks for evaluation <ref type="bibr" coords="1,312.19,601.86,10.58,8.64" target="#b0">[1]</ref>.</p><p>A summary of this report has been published as part of <ref type="bibr" coords="1,344.78,621.30,13.74,7.77" target="#b61">[62]</ref>, and some of the descriptions are borrowed from earlier reports on the shared task of text alignment at PAN.</p><p>One of the main contributions of organizing a shared task is that of creating a reusable experimental setup for future research, allowing for comparative evaluations even after the shared task has passed. Currently, however, this goal is only partially achieved: participants and researchers following up on a shared task may only compare their own approach to those of others, whereas other aspects of a shared task remain fixed, such as the evaluation datasets, the ground truth annotations, and the performance measures used. As time passes, these fixtures limit future research to using evaluation resources that may quickly become outdated in order to compare new approaches with the state of the art. Moreover, given that shared tasks are often organized by only a few dedicated people, this further limits the attainable diversity of evaluation resources.</p><p>To overcome the outlined shortcomings, we propose that shared task organizers attempt to remove as many fixtures from their shared tasks as possible, relinquishing control over the choice of evaluation resources to their community. We believe that, in fact, only data formats and interfaces between evaluation resources need to be fixed a priori to ensure compatibility of contributions submitted by community members. As a first step in this direction, we investigate for the first time the feasibility of data submissions to a well-known shared task by posing the construction of evaluation datasets as a shared task of its own.</p><p>As a testbed for data submissions, we use our established shared task on plagiarism detection at PAN <ref type="bibr" coords="2,208.23,346.46,15.27,8.64" target="#b61">[62]</ref>, and in particular the task on text alignment. Instead of inviting text alignment algorithms, we ask participants to submit datasets of their own design, validating the submitted datasets both via peer-review and by running the the text alignment softwares submitted in previous editions of the text alignment task against the submitted corpora. Altogether, eight datasets have been submitted by participants, ranging from automatically constructed ones to manually created ones, including various languages.</p><p>In what follows, after a brief discussion of related work in Section 2, we outline our approach to data submissions in Section 3, survey the submitted datasets in Section 4, and report on their validation and evaluation in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Research on plagiarism detection has a long history, both within PAN and without. We have been the first to organize shared tasks on plagiarism detection <ref type="bibr" coords="2,403.41,521.80,15.27,8.64" target="#b50">[51]</ref>, whereas since then, we have introduced a number of variations of the task as well as new evaluation resources: the first shared task was organized in 2009, studying two sub-problems of plagiarism detection, namely the traditional external plagiarism detection <ref type="bibr" coords="2,434.01,557.67,15.27,8.64" target="#b63">[64]</ref>, where a reference collection is used to identify plagiarized passages, and intrinsic plagiarism detection <ref type="bibr" coords="2,174.02,581.58,15.77,8.64" target="#b30">[31,</ref><ref type="bibr" coords="2,192.51,581.58,11.83,8.64" target="#b62">63]</ref>, where no such reference collection is at hand and plagiarism has to be identified from writing style changes within a document. For the this share task, we have created the first standardized, large-scale evaluation corpus for plagiarism detection <ref type="bibr" coords="2,153.44,617.44,15.27,8.64" target="#b49">[50]</ref>. As part of this effort, we have devised novel performance measures which, for the first time, took into account task-specific characteristics of plagiarism detection, such as detection granularity. Finally, in the first three years of PAN, we have introduced cross-language plagiarism detection as a sub-task of plagiarism detection for the first time <ref type="bibr" coords="3,202.22,119.31,15.27,8.64" target="#b41">[42]</ref>, adding corresponding problem instances into the evaluation corpus. Altogether, in the first three years of our shared task, we successfully acquired and evaluated plagiarism detection approaches of 42 research teams from around the world, some participating more than once. Many insights came out of this endeavor which informed our subsequent activities <ref type="bibr" coords="3,274.53,167.13,15.77,8.64" target="#b50">[51,</ref><ref type="bibr" coords="3,292.79,167.13,12.45,8.64" target="#b40">41,</ref><ref type="bibr" coords="3,307.74,167.13,11.83,8.64" target="#b42">43]</ref>.</p><p>Starting in 2012, we have completely overhauled our evaluation approach to plagiarism detection <ref type="bibr" coords="3,205.69,191.04,15.27,8.64" target="#b43">[44]</ref>. Since then, we have separated external plagiarism detection into the two tasks source retrieval and text alignment. The former task deals with information retrieval approaches to retrieve potential sources for a suspicious document from a large text collection, such as the web, which are indexed with traditional retrieval models. The latter task of text alignment focuses on the problem of extracting matching passages from pairs of documents, if there are any. Both tasks have never been studied in this way before.</p><p>For source retrieval, we went to considerable lengths to set up a realistic evaluation environment: we indexed the entire English portion of the ClueWeb09 corpus, building the research search engine ChatNoir <ref type="bibr" coords="3,300.78,298.64,15.27,8.64" target="#b47">[48]</ref>. ChatNoir served two purposes, namely as an API for plagiarism detectors for those who cannot afford to index the ClueWeb themselves, but also as an end user search engine for authors which were hired to construct a new, realistic evaluation resource for source retrieval. We have hired 18 semiprofessional authors from the crowdsourcing platform oDesk (now Upwork) and asked them to write essays of length at least 5000 words on pre-defined TREC web track topics. To write their essays, the authors were asked to conduct their research using ChatNoir, reusing text from the web pages they found. This way, we have created realistic information needs which in turn lead the authors to use our search engine in a realistic way to fulfill their task. AS part of this activity, we gained new insights into the nature of how humans reuse text, some building up a text as they go, whereas others first collect a lot of text and then boil it down to the final essay <ref type="bibr" coords="3,392.55,430.15,15.27,8.64" target="#b48">[49]</ref>. Finally, we have devised and developed new evaluation measures for source retrieval that, for the first time, take into account the retrieval of near-duplicate results when calculating precision and recall <ref type="bibr" coords="3,175.76,466.01,15.77,8.64" target="#b44">[45,</ref><ref type="bibr" coords="3,193.77,466.01,11.83,8.64" target="#b46">47]</ref>. We report on the latest results of the source retrieval subtask in <ref type="bibr" coords="3,461.50,466.01,15.27,8.64" target="#b19">[20]</ref>.</p><p>Regarding text alignment, we focus on the text reuse aspects of the task by stripping down the problem to its very core, namely comparing two text documents to identify reused passages of text. In this task, we have started in 2012 to experiment with software submissions for the first time, which lead to the development of the TIRA experimentation platform <ref type="bibr" coords="3,189.63,525.79,15.27,8.64" target="#b17">[18]</ref>. TIRA is an implementation of the emerging evaluation-as-a-service paradigm <ref type="bibr" coords="3,174.93,537.74,15.27,8.64" target="#b21">[22]</ref>. We have since scaled TIRA in order to also collect participant software for source retrieval and for the entire PAN evaluation lab as of 2013, thus improving the reproducibility of PAN's shared tasks for the foreseeable future <ref type="bibr" coords="3,397.52,561.65,15.77,8.64" target="#b16">[17,</ref><ref type="bibr" coords="3,416.71,561.65,11.83,8.64" target="#b45">46]</ref>. Altogether, in the second three-year cycle of this task, we have acquired and evaluated plagiarism detection approaches of 20 research teams on source retrieval and 31 research teams on text alignment <ref type="bibr" coords="3,194.39,597.52,15.77,8.64" target="#b43">[44,</ref><ref type="bibr" coords="3,212.65,597.52,12.45,8.64" target="#b44">45,</ref><ref type="bibr" coords="3,227.60,597.52,11.83,8.64" target="#b46">47]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data Submissions: Crowdsourcing Evaluation Resources</head><p>Data submissions for shared tasks have not been systematically studied until now, so that no best practices have been established, yet. Asking shared task participants to submit data is nothing short of crowdsourcing, albeit the task of creating an evaluation resource is by comparison much more complex than average crowdsourcing tasks found in the literature. In what follows, we outline the rationale of data submissions, review important aspects of defining a data submissions task that may inform instructions to be handed out to participants, and detail two methods to evaluating submitted datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Rationale of Data Submissions to Shared Tasks</head><p>Traditionally, the evaluation resources required to run a shared task are created by its organizers-but the question remains: why? The following reasons can be identified:</p><p>-Quality control. The success of a shared task rests with the quality of its evaluation resources. A poorly built evaluation dataset may invalidate evaluation results, which is one of the risks of organizing shared tasks. This is why organizers have a vested interest in maintaining close control over evaluation resources, and how they are constructed. -Seniority. Senior community members may have the best vantage point in order to create representative evaluation resources. -Access to proprietary data. Having access to an otherwise closed data source (e.g., from a company) gives some community members an advantage over others in creating evaluation resources with a strong connection to the real world. -Task inventorship. The inventor of a new task (i.e., tasks that have not been considered before), is in a unique position to create normative evaluation resources, shaping future evaluations. -Being first to the table. The first one to pick up the opportunity may take the lead in constructing evaluation resources (e.g., when a known task has never been organized as a shared task before, or, to mitigate a lack of evaluation resources).</p><p>All of the above are good reasons for an individual or a small group of researchers to organize a shared task, and, to create corresponding evaluation resources themselves. However, from reviewing dozens of shared tasks that have been organized in the human language technologies, neither of them are a necessary requirement <ref type="bibr" coords="4,409.41,529.39,15.49,8.64" target="#b45">[46]</ref>: shared tasks are being organized using less-than-optimal datasets, by newcomers to a given research field, without involving special or proprietary data, and without inventing the task in the first place. Hence, we question the traditional connection of shared task organization and evaluation resource construction. This connection limits the scale and diversity, and therefore the representativeness of the evaluation resources that can be created:</p><p>-Scale. The number of man-hours that can be invested in the construction of evaluation resources is limited by the number of organizers and their personal commitment. This limits the scale of the evaluation resources. Crowdsourcing may be employed as a means to increase scale in many situations, however, this is mostly not the case when task-specific expertise is required.</p><p>-Diversity. The combined task-specific capabilities of all task organizers may be limited regarding the task's domain. For example, the number of languages spoken by task organizers is often fairly small, whereas true representativeness across languages would require evaluation resources from at least all major language families spoken today.</p><p>By involving participants in a structured way into the construction of evaluation resources, task organizers may build on their combined expertise, man-power, and diversity. However, there is no free lunch, and outsourcing the construction of evaluation resources introduces the new organizational problem that the datasets created and submitted by third parties must be validated and evaluated for quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Defining a Data Submission Task</head><p>When casting a data submission task, there are a number of desiderata that participants should meet:</p><p>-Data format compliance. The organizers should agree on a specific data format suitable for the task in question. The format should be defined with the utmost care, since it may be impossible to fix mistakes discovered later on. Experience shows that the format of the evaluation datasets has a major effect on how participants implement their softwares for a task, which is especially true when inviting software submissions for a shared task. Regarding data submissions, a dataset should comprise a set of problem instances with respect to the task, where each problem instance shall be formatted according to the specifications handed out by the organizers. To ensure compliance, the organizers should prepare a format validation tool, which allows participants to check the format of their dataset in progress, and whether it complies with the format specifications. This way, participants move into the right direction from the start, and less back and forth will be necessary after a dataset has been submitted. The format validation tool should check every aspect of the required data format in order to foreclose any unintended deviation. -Annotation validity. All problem instances of a dataset should comprise annotations that reveal their true solution with regard to the task in question. It goes without saying, that all annotations should be valid. Datasets that do not comprise annotations are of course useless for evaluation purposes, whereas annotation validity as well as the quality and representativeness of the problem instances selected by participants determines the usefulness of a submitted dataset. -Representative size. The datasets submitted should be of sufficient size, so that dividing them into training and test datasets can be done without sacrificing representativeness, and so that evaluations conducted based on the resulting test datasets are meaningful and not prone to noise. -Choice of data source. The choice of a data source should be left up to participants, and should open the possibility of using manually created data either from the real world or by asking human test subjects to emulate problem instances, as well as automatically generated data based on a computer simulation of problem instances for the task at hand.</p><p>-Copyright and sensitive data. Participants must ensure that they have the usage rights of the data, for transferring usage rights to the organizers of the shared task, and for allowing the organizers to transfer usage rights to other participants. The data must further be compliant with privacy laws and ethically innocuous. Dependent on the task at hand and what the organizers of a shared task desire, accepting confidential or otherwise sensitive data may still be possible: in case the shared task also invites software submissions, the organizers may promise participants that the sensitive data does not leak to participants by running submitted software at their site against the submitted datasets. Nevertheless, special security precautions must be taken to ensure that sensitive data does not leak when feeding it to untrusted software.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluating Submitted Datasets: Peer-Review and Software Submissions</head><p>The construction of new evaluation datasets must be done with the utmost care, since datasets are barely double-checked or questioned again once they have been accepted as authoritative. This presents the organizers of a dataset construction task with the new challenge of evaluating submitted datasets, where the evaluation of a dataset should aim at establishing its validity. In general, the organizers of data submission tasks should ensure not to advertise submitted datasets as valid unless they are, since such an endorsement may carry a lot of weight in a shared task's community. Unlike with shared tasks that invite algorithmic contributions, the validity of a dataset typically can not be established via an automatically computed performance measure, but requires manual reviewing effort. Therefore, as part of their participation, all participants who submit a dataset should be compelled to also peer-review the datasets submitted by other participants. Moreover, inviting other community members to conduct independent reviews may ensure impartial results. Reviewers may be instructed as follows:</p><p>The peer-review is about dataset validity, i.e. the quality and realism of the problem instances. Conducting the peer-review includes:</p><p>-Manual review of as many examples as possible from all datasets -Make observations about how the dataset has been constructed -Make observations about potential quality problems or errors -Make observations on the realism of each dataset's problem instances -Write about your observations in your notebook (make sure to refer to examples from the datasets for your findings).</p><p>Handing out the complete submitted datasets for peer-review, however, is out of the question, since this would defeat the purpose of subsequent shared task evaluations by revealing the ground truth prematurely. Here, the organizers of a dataset construction task serve as mediators, splitting submitted datasets into training and test datasets, and handing out only the training portion for peer-review. The participants who submitted a given dataset, however, may never be reliably evaluated based on their own dataset. Also, colluding participants may not be ruled out entirely. Finally, when a shared task has previously invited software submissions, this creates ample opportunity to re-evaluate the existing softwares on the submitted datasets. This allows for evaluating submitted datasets in terms of their difficulty: the performances of existing software on submitted datasets, when compared to their respective performances on established datasets, allow for a relative assessment of dataset difficulty. If a shared task did not invite software submissions so far, then the organizers should set up a baseline software for the shared task and run that against submitted datasets to allow for a relative comparison among them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Data Submissions for Text Alignment</head><p>In text alignment, given a pair of documents, the task is to identify all contiguous passages of reused text between them. The challenge with this task is to identify passages of text that have been obfuscated, sometimes to the extent that, apart from stop words, little lexical similarity remains between an original passage and its reused counterpart. Consequently, for task organizers, the challenge is to provide a representative dataset of documents that emulate this situation. For the previous editions of PAN, we have created such datasets ourselves, whereas obfuscated text passages have been generated automatically, semi-automatically via crowdsourcing <ref type="bibr" coords="7,349.93,311.71,10.58,8.64" target="#b5">[6]</ref>, and by collecting real cases. Until now, however, we neglected participants of our shared task as potential assistants in creating evaluation resources. Given that a stable community has formed around our task in previous years, and that the data format has not changed in the past three years, we felt confident to experiment with this task and to switch from algorithm development to data submissions. We cast the task to construct an evaluation dataset as follows:</p><p>-Dataset collection. Gather real-world instances of text reuse or plagiarism, and annotate them. -Dataset generation. Given pairs of documents, generate passages of reused or plagiarized text between them. Apply a means of obfuscation of your choosing.</p><p>The task definition is kept as open as possible, imposing no particular restrictions on the way in which participants approach this task, which languages they consider, or which kinds of obfuscation they collect or generate. In particular, the task definition highlights the two possible avenues of dataset construction, namely manual collection, and automatic construction. To ensure compatibility among each other and with previous datasets, however, the format of all submitted datasets had to conform with that of the existing datasets used in previous years. By fixing the dataset format, future editions of the text alignment task may build on the evaluation resources created within this task without further effort, and the pieces of software that have been submitted in previous editions of the text alignment task, which are available on the TIRA platform for evaluation as a service, may be re-evaluated on the new datasets. In our case, more than 30 text alignment approaches have been submitted since 2012. To ensure compatibility, we handed out a dataset validation tool that checked all format restrictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Survey of Submitted Text Alignment Datasets</head><p>A total of eight datasets have been submitted to the PAN 2015 text alignment task on dataset construction. The datasets are of varying sizes and have been built with a variety of methods. In what follows, after a brief discussion of the dataset format, we survey the datasets with regard their source of documents and languages, and the construction process employed by their authors, paying special attention to the obfuscation approaches.</p><p>The section closes with an overview of dataset statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset Format</head><p>We asked participants to comply with the dataset format of the PAN plagiarism corpora that have been used for the text alignment task at PAN since 2012. A compliant dataset consists of documents in the form of plain text files encoded in UTF-8 in which cases of plagiarism are found, plus XML files comprising meta data about them. The documents are divided into so-called source documents and suspicious documents, where suspicious documents are supposed to be analyzed for evidence of plagiarism. Each suspicious document is a priori paired with one or more source document, i.e., the task in text alignment is to extract passages of plagiarized text from a given pair of documents, if there are any. Text alignment does not involve retrieval of source documents, a different problem studied in the accompanying source retrieval task <ref type="bibr" coords="8,403.40,310.17,15.27,8.64" target="#b19">[20]</ref>. The meta data for each pair of documents reveals if and where plagiarism cases are found within in the form of character offsets and lengths for each plagiarism case. These ground truth annotations are used to measures the performance of a text alignment algorithm in extracting these plagiarism cases. While the problem is trivial for situations where text has been lifted verbatim from a source document into a suspicious document, the problem gets a lot more difficult in case the plagiarized text passage has been obfuscated, e.g., by being paraphrased, translated, or summarized. There are many ways to obfuscate a plagiarized test passage, both in the real world as well as using (semi-)automatic emulations of the real thing. Therefore, each dataset is supposed to contain an additional folder for each obfuscation strategy applied; the XML meta data files revealing the ground truth annotations are divided accordingly into these folders. To assist participants in getting the format of their datasets right, we supplied them with a format validation tool that checks formatting details and that performs basic sanity checks. This tool, of course, cannot ascertain whether the text passages annotated as plagiarized are actually meaningful or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dataset Overview</head><p>Table <ref type="table" coords="8,159.79,536.89,4.98,8.64" target="#tab_0">1</ref> compiles an overview of the submitted datasets. The table shows the sizes of each corpus in terms of documents and plagiarism cases within. The sizes vary greatly from around 160 documents and about the same number of plagiarism cases to more than 27000 documents and more than 11000 plagiarism cases. Most of the datasets comprise English documents, whereas two feature cross-language plagiarism from Urdu and Persian to English. Two datasets contain only non-English documents in Persian and Chinese. Almost all datasets also contain a portion of suspicious documents that do not contain any plagiarism, whereas the datasets of Alvi et al. <ref type="bibr" coords="8,377.19,620.57,10.58,8.64" target="#b3">[4]</ref>, Mohtaj et al. <ref type="bibr" coords="8,445.09,620.57,15.27,8.64" target="#b31">[32]</ref>, and Palkovskii and Belov <ref type="bibr" coords="8,220.67,632.53,16.60,8.64" target="#b35">[36]</ref> contain only few such documents, and that of Kong et al. <ref type="bibr" coords="8,463.99,632.53,16.60,8.64" target="#b26">[27]</ref> none. The documents are mostly short (up to 10 pages), where a page is measured as 1500 chars (a norm page in print publishing), which corresponds to about 288 words in </p><formula xml:id="formula_0" coords="9,134.77,339.63,332.77,99.30">- - - - - - - human-retelling 33% - - - - - - - synonym-replacement 33% - - - - - - - translation-obfuscation - 100% - - - - - 24% no-obfuscation - - - - 31% - 10% 24% random-obfuscation - - - - 69% - 87% 24% simulated-obfuscation - - - - - - 3% - summary-obfuscation - - - - - - - 28% undergrad - - 61% - - - - - phd - - 16% - - - - - masters - - 6% - - - - - undergrad-in-progress - - 17% - - - - - plagiarism - - - 100% - - - - real-plagiarism - - - - - 100% - -</formula><p>English. Some datasets also contain medium-length documents of about 10-100 pages, however, only the datasets of Kong et al. <ref type="bibr" coords="9,297.33,472.16,16.60,8.64" target="#b26">[27]</ref> and Mohtaj et al. <ref type="bibr" coords="9,385.49,472.16,16.60,8.64" target="#b31">[32]</ref> have more medium documents than short ones. No datasets have long documents; for comparison, the PAN plagiarism corpora 2009-2012 contain about 15% long documents. The portion of plagiarism per document is below 50% of a given document in almost all cases, suggesting that the plagiarism cases are mostly short, too. This is corroborated when looking at the distributions of case length; almost all cases unanimously below 1000 characters, except for the dataset of Khoshnavataher et al. <ref type="bibr" coords="9,398.56,543.89,16.60,8.64" target="#b24">[25]</ref> and, to a lesser extent, that of Kong et al. <ref type="bibr" coords="9,243.35,555.85,16.60,8.64" target="#b26">[27]</ref> and Cheema et al. <ref type="bibr" coords="9,340.51,555.85,10.58,8.64" target="#b8">[9]</ref>. Only the dataset of Alvi et al. <ref type="bibr" coords="9,134.77,567.80,11.62,8.64" target="#b3">[4]</ref> contains documents with much (up to 80%) plagiarism. Again, no dataset contains documents that are entirely plagiarized, and only Kong et al. <ref type="bibr" coords="9,373.39,579.76,16.60,8.64" target="#b26">[27]</ref> has a small percentage of long plagiarism cases.</p><p>With regard to the obfuscation synthesis approaches, we report the names used by the dataset authors for consistency with the respective datasets' folder names. The approaches will be discussed in more detail below, however, we depart from the names used by the dataset authors for the obfuscation synthesis approaches, since they are inconsistent with the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Document Sources</head><p>The first building block of every text alignment dataset is the set of documents used, which is divided into suspicious documents and source documents. One of the main obstacles in this connection is to pair suspicious documents with source documents that are roughly about the same topic, or that partially share a topic. Ideally, one would choose a set of documents that naturally possess such a relation, however, such documents are not readily available at scale. Although it is not a strict necessity to ensure topical relations for pairs of suspicious and source documents, doing so adds to the realism of an evaluation dataset for text alignment, since, in the real world, spurious similarities between topically related documents that are not plagiarism are much more likely than otherwise.</p><p>For the PAN plagiarism corpora 2009-2012, we employed documents obtained from the Project Gutenberg for the most part <ref type="bibr" coords="10,297.09,270.74,15.27,8.64" target="#b49">[50]</ref>, pairing documents at random and disregarding their topical relation. We have experimented with clustering algorithms to select document pairs with at least a basic topical relation, but this had only limited success. For PAN 2013, we switched from Project Gutenberg documents to using ClueWeb09 documents, based on the Webis text reuse corpus 2012, Webis-TRC-12 <ref type="bibr" coords="10,431.56,318.57,15.27,8.64" target="#b48">[49]</ref>. In this corpus, as well as the text alignment corpus that we derived from it, a strong topical relation between documents can be assumed, since the source documents have been manually retrieved for a predefined TREC topic.</p><p>Regarding the submitted datasets, those of Asghari et al. <ref type="bibr" coords="10,387.90,366.39,10.58,8.64" target="#b4">[5]</ref>, Cheema et al. <ref type="bibr" coords="10,466.48,366.39,10.58,8.64" target="#b8">[9]</ref>, Hanif et al. <ref type="bibr" coords="10,186.71,378.34,15.27,8.64" target="#b20">[21]</ref>, Khoshnavataher et al. <ref type="bibr" coords="10,303.48,378.34,15.27,8.64" target="#b24">[25]</ref>, and Mohtaj et al. <ref type="bibr" coords="10,403.27,378.34,16.60,8.64" target="#b31">[32]</ref> employ documents drawn from Wikipedia. Cheema et al. <ref type="bibr" coords="10,317.14,390.30,11.62,8.64" target="#b8">[9]</ref> also employ documents from Project Gutenberg, but it is unclear whether pairs of suspicious and source documents are selected from both, or whether they are always from the same document source. In all cases, the authors make an effort to pair documents that are topically related. In this regard, Khoshnavataher et al. <ref type="bibr" coords="10,252.85,438.12,16.60,8.64" target="#b24">[25]</ref> and Mohtaj et al. <ref type="bibr" coords="10,340.97,438.12,16.60,8.64" target="#b31">[32]</ref> both employ the same strategy of clustering Wikipedia articles using the bipartite document-category graph and the graph-based clustering algorithm of Rosvall and Bergstrom <ref type="bibr" coords="10,373.59,462.03,15.27,8.64" target="#b54">[55]</ref>. Asghari et al. <ref type="bibr" coords="10,451.01,462.03,11.62,8.64" target="#b4">[5]</ref> rely on cross-language links between Wikipedia articles in different languages to identify documents about the same topic.</p><p>Alvi et al. <ref type="bibr" coords="10,194.71,497.89,11.62,8.64" target="#b3">[4]</ref> employs translations of Grimm's fairy tales into English, obtained from Project Gutenberg, pairing documents which have been translated by different authors. Therefore this dataset is also limited to a very specific genre comprising sometimes rather old forms of usage and style. Nevertheless, pairs of documents that tell the same fairy tale are bound to have strong topical relation.</p><p>Kong et al. <ref type="bibr" coords="10,198.30,557.67,16.60,8.64" target="#b26">[27]</ref> follow our strategy to construct the Webis-TRC-12 <ref type="bibr" coords="10,428.87,557.67,15.27,8.64" target="#b48">[49]</ref>, namely they asked 10 volunteers to genuinely write as many essays on predefined topics. The volunteers were asked to use a web search engine to manually retrieve topic-related sources and to reuse text passages found on the web pages to compose their essays. This way, the resulting suspicious documents and their corresponding source documents also possess a strong topical relation.</p><p>Palkovskii and Belov <ref type="bibr" coords="10,237.45,629.40,16.60,8.64" target="#b35">[36]</ref> took a shortcut by simply reusing the training dataset of the PAN 2013 shared task on text alignment <ref type="bibr" coords="10,312.39,641.36,15.27,8.64" target="#b44">[45]</ref>, simply applying an additional means of obfuscation across the corpus as detailed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Obfuscation Synthesis</head><p>The second building block of every text alignment dataset is the set of obfuscation approaches used to emulate human plagiarists who try to hide their plagiarism. Obfuscation of plagiarized text passages is what makes the task of text alignment difficult for detection algorithms as well as for constructing datasets. The difficulty for the latter arises from the fact that real plagiarism is hard to find at scale, especially when plagiarists invest a lot of effort in hiding it. It can be assumed that there is a bias in all plagiarism cases that make the news toward cases that are easier to be detected. Therefore, approaches have to be devised that will yield obfuscated plagiarism that comes close to the real thing, but can be created at scale with reasonable effort.</p><p>There are basically two alternatives to obfuscation synthesis, namely within context and without: within context, the entire document surrounding a plagiarized passage is created simultaneously, either manually or automatically, whereas without context, a plagiarized passage is created independently and afterwards embedded into a host document. The latter is easier to be accomplished, but lacks realism since plagiarized passages are not interleaved with the surrounding host document or other plagiarized passages around it. Moreover, when embedding an independently created plagiarized passages into a host document, the selected host document should be topically related, or else a basic topic drift analysis will reveal a plagiarized passage.</p><p>For the PAN plagiarism corpora 2009-2013, we devised a number of obfuscation approaches ranging from automatic obfuscation to manual obfuscation. This was done without context, embedding plagiarized passages into host documents after obfuscation. In particular, the obfuscation approaches are the following:</p><p>-Random text operations. Random shuffling, insertion, replacement, or removal of phrases and sentences. Insertions and replacements are obtained from context documents <ref type="bibr" coords="11,182.97,433.76,15.27,8.64" target="#b50">[51]</ref>. -Semantic word variation. Random replacement words with synonyms, antonyms, hyponyms, or hypernyms <ref type="bibr" coords="11,255.35,457.63,15.27,8.64" target="#b50">[51]</ref>. -Part-of-speech-preserving word shuffling. Shuffling of phrases while maintaining the original POS sequence <ref type="bibr" coords="11,259.62,481.50,15.27,8.64" target="#b50">[51]</ref>. -Machine translation. Automatic translation from one language to another <ref type="bibr" coords="11,445.16,493.42,15.27,8.64" target="#b50">[51]</ref>.</p><p>-Machine translation and manual copyediting. Manually corrected output of machine translation <ref type="bibr" coords="11,220.32,517.29,15.27,8.64" target="#b42">[43]</ref>. -Manual translation from parallel corpus. Usage of translated passages from an existing parallel corpus <ref type="bibr" coords="11,237.76,541.16,15.27,8.64" target="#b43">[44]</ref>. -Manual paraphrasing via crowdsourcing. Asking human volunteers to paraphrase a given passage of text, possibly on crowdsourcing platforms, such as Amazon's Mechanical Turk <ref type="bibr" coords="11,222.08,576.98,10.79,8.64" target="#b5">[6,</ref><ref type="bibr" coords="11,235.36,576.98,12.45,8.64" target="#b40">41,</ref><ref type="bibr" coords="11,250.30,576.98,11.83,8.64" target="#b49">50]</ref>. -Cyclic translation. Automatic translation of a text passage from one language via a sequence of other languages to the original language [45] -Summarization. Summaries of long text passages or complete documents obtained from the corpora of summaries, such as the DUC corpora <ref type="bibr" coords="11,382.72,624.72,15.27,8.64" target="#b44">[45]</ref>.</p><p>Regarding the submitted datasets, Kong et al. <ref type="bibr" coords="11,338.29,644.48,16.60,8.64" target="#b26">[27]</ref> recreate our previous work on generating completely manual text reuse cases for the Webis-TRC-12 <ref type="bibr" coords="11,418.94,656.44,16.60,8.64" target="#b48">[49]</ref> on a small scale. They asked volunteers to write essays on predefined topics, reusing text passages from the web pages they found during manual retrieval. From all submitted datasets, Kong et al. <ref type="bibr" coords="12,183.27,143.22,16.60,8.64" target="#b26">[27]</ref> present the only one where obfuscation has been synthesized within context. They also introduce an interesting twist: to maximize the obfuscation of the plagiarized text passages, the student essays have been submitted to a plagiarism detection system widely used at Chinese universities, and the volunteers have paraphrased their essays until the system could not detect the plagiarism, anymore.</p><p>For all other datasets, obfuscated plagiarized passages have been synthesized without context, and then embedded into host documents. Here, Alvi et al. <ref type="bibr" coords="12,414.02,214.95,11.62,8.64" target="#b3">[4]</ref> employ manual translations from a pseudo-parallel corpus, namely different editions of translations of Grimm's fairy tales. These translation pairs are then embedded into other, unrelated fairy tales, assuming that the genre of fairy tales in general will, to some extent, provide context with a more or less matching topic. It remains to be seen whether a topic drift analysis may reveal the plagiarized passages. At any rate, independent translations of fairy tales will provide for an interesting challenge for text alignment algorithms. In addition to that, Alvi et al. <ref type="bibr" coords="12,241.58,298.64,11.62,8.64" target="#b3">[4]</ref> also employ the above obfuscation approach of semantic word variation, and a new approach which we call UTF character substitution. Here, characters are replaced by look-alike characters from the UTF table, which makes it more difficult, though not impossible, for text alignment algorithms to match words at a lexical level. Note in this connection that Palkovskii and Belov <ref type="bibr" coords="12,393.04,346.46,16.60,8.64" target="#b35">[36]</ref> have also applied UTF character substitution on top of the reused PAN 2013 training dataset; they have already pointed out back at PAN 2009 that students sometimes apply this approach in practice <ref type="bibr" coords="12,168.79,382.33,15.27,8.64" target="#b36">[37]</ref>.</p><p>Cheema et al. <ref type="bibr" coords="12,209.79,394.28,11.62,8.64" target="#b8">[9]</ref> employ manual paraphrasing via crowdsourcing; they have recruited colleagues, friends, and students at different stages of their education, namely undergrads, bachelors, masters, and PhDs, and asked them to paraphrase a total of 250 text passages selected from their respective study domain (e.g., technology, life sciences, and humanities). These paraphrased text passages have then been embedded into documents drawn from Wikipedia and Project Gutenberg which were selected according to topical similarity to the paraphrased text passages.</p><p>Hanif et al. <ref type="bibr" coords="12,200.10,477.97,16.60,8.64" target="#b20">[21]</ref> employ machine translation with and without manual copyediting, and machine translation with random text operations to obfuscate text passages obtained from the Urdu Wikipedia. The translated passage are then embedded into host documents selected so that they match the topic of the translated passages.</p><p>Since the datasets of Asghari et al. <ref type="bibr" coords="12,288.10,525.79,10.58,8.64" target="#b4">[5]</ref>, Mohtaj et al. <ref type="bibr" coords="12,356.91,525.79,15.27,8.64" target="#b31">[32]</ref>, and Khoshnavataher et al. <ref type="bibr" coords="12,134.77,537.74,16.60,8.64" target="#b24">[25]</ref> have been compiled by more or less the same people, their construction process if very similar. In all cases, obfuscated text passages obtained from Wikipedia articles are embedded into other Wikipedia articles that serve as suspicious documents. For the monolingual datasets Mohtaj et al. <ref type="bibr" coords="12,280.54,573.61,16.60,8.64" target="#b31">[32]</ref> and Khoshnavataher et al. <ref type="bibr" coords="12,410.89,573.61,16.60,8.64" target="#b24">[25]</ref> employ random text operations as obfuscation approach. In addition, for both monolingual and cross-language datasets, a new way of creating obfuscation is devised: Asghari et al. <ref type="bibr" coords="12,134.77,609.47,11.62,8.64" target="#b4">[5]</ref> and Mohtaj et al. <ref type="bibr" coords="12,218.41,609.47,16.60,8.64" target="#b31">[32]</ref> employ what we call "sentence stitching" with sentence pairs obtained from parallel corpora when creating cross-language plagiarism, or paraphrase corpora for monolingual plagiarism. To create a plagiarized passage and its corresponding source passage, sentence pairs from such corpora are selected and then simply ap-pended to each other to form aligned passages of text. Various degrees of obfuscation difficulty can be introduced by measuring the similarity of sentence pairs with an appropriate similarity measure, and by combining sentences with high similarity to create low obfuscation, and vice versa. The authors try to ensure that combined sentences pairs have at least some similarity to other sentences found in a generated pair of passages by first clustering the sentences in the corpora used by their similarity. However, the success of the latter depends on how many semantically related sentence pairs are actually found in the corpora used, since clustering algorithms will even find clusters of sentences when there are only unrelated sentence pairs. In summary, the authors of the submitted dataset propose the following new obfuscation synthesis approaches:</p><p>-UTF character substitution. Replacement of characters with look-alike UTF characters <ref type="bibr" coords="13,177.42,270.74,10.79,8.64" target="#b3">[4,</ref><ref type="bibr" coords="13,190.70,270.74,11.83,8.64" target="#b35">36]</ref>. -Sentence stitching using parallel corpora or paraphrase corpora. Generation of pairs of text passages from a selection of translated or paraphrased passages <ref type="bibr" coords="13,434.58,294.66,10.79,8.64" target="#b4">[5,</ref><ref type="bibr" coords="13,447.86,294.66,11.83,8.64" target="#b31">32]</ref>. -Manual paraphrasing against plagiarism detection. Paraphrasing a text passage until a plagiarism detector fails to detect the text passage as plagiarism <ref type="bibr" coords="13,414.58,318.57,15.27,8.64" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>In the first years of PAN and the plagiarism detection task, we have harvested a lot of the low-hanging fruit in terms of constructing evaluation resources, and in particular in devising obfuscation synthesis approaches. In this regard, it is not surprising that, despite the fact that eight datasets haven been submitted, only three completely new approaches have been proposed. If things were different, and we would start a task from scratch in this way, participants who decide to construct datasets would certainly have come up with most of these approaches themselves. Perhaps, by having proposed so many of the existing obfuscation synthesis approaches ourselves, we may be stifling creativity by anchoring the thoughts of participants to what is already there instead of what is missing. For example, it is interesting to note that none of the participants have actually implemented and employed automatic paraphrasing algorithms or any other form of text generation, e.g., based on language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Dataset Validation and Evaluation</head><p>Our approach at validating data submissions for shared tasks is twofold: (1) all participants who submit a dataset have been asked to peer-review the datasets of all other participants, and (2) running all 31 pieces of software that have been submitted to previous editions of our shared task on text alignment against the submitted datasets. In what follows, we review the reports of the participants' peer-reviews that have been submitted as part of their notebook papers to this year's shared task, we introduce the performance measures used to evaluate text alignment software, and we report on the evaluation results obtained from running the text alignment software against the submitted datasets using the TIRA experimentation platform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset Peer-Review</head><p>Peer-review is one of the traditional means of the scientific community to check and ensure quality. Data submissions introduce new obstacles to the successful organization of a peer-review for the following reasons:</p><p>-Dataset size. Datasets for shared tasks tend to be huge, which renders individual reviewers incapable of reviewing them all. Here, the selection of statistically representative subset may alleviate the problem, allowing for an estimation of the total amount of errors or other quality issues in a given dataset. -Assessment difficulty. Even if the ground truth of a dataset is revealed, it may not be enough to easily understand and follow up on their construction principles of a dataset. Additional tools may be required to review problem instances at scale; in some cases, these tools need to solve the task's underlying problem themselves, e.g., to properly visualize problem instances, whereas, without visualization, the review time per problem instance may prohibitively long. -Reviewer bias. Given a certain assessment difficulty for problem instances, even if the ground truth is revealed, reviewers may be biased to favor easy decisions over difficult ones. -Curse of variety. While shared tasks typically tackle very clear-cut problems, the number of application domains where the task in question occurs may be huge. In these situations, it is unlikely that the reviewers available possess all the required knowledge, abilities, and experience to review and judge a given dataset with confidence. -Lack of motivation. While it is fun and motivating to create a new evaluation resource, that is less so when reviewing those of others. Reviewers in shared task that invite data submissions may therefore feel less inclined to invest their time into reviewing other participants' contributions. -Privacy concerns. Some reviewers may feel uncomfortable when passing open judgment onto their peers' work for fear of repercussions, especially when they find datasets to be sub-standard. However, an open discussion of the quality of evaluation resources of all kinds is an import prerequisite for progress.</p><p>All of the above obstacles have been observed in our case: some submitted datasets that are huge, comprising thousands of generated plagiarism cases; reviewing pairs of entire text documents up to dozens of pages long, and comparing plagiarism cases that may be extremely obfuscated is a laborious task, especially when no tools are around to help; some submitted datasets have been constructed in languages that none of the reviewers speak, except for those who constructed the dataset; and some of the invited reviewers apparently lacked the motivation to actually conduct a review in a useful manner. The most comprehensive review has been submitted by volunteer reviewers, who did not submit a dataset of their own: Franco-Salvador et al. <ref type="bibr" coords="14,387.26,584.71,16.60,8.64" target="#b11">[12]</ref> systematically analyzed the submitted datasets both by computing dataset statistics and by manual inspection. The dataset statistics computed are mostly consistent with those we show in Table <ref type="table" coords="14,159.15,620.57,3.74,8.64" target="#tab_0">1</ref>. Since most of the datasets have been submitted without further explanation by their authors, Franco-Salvador et al. <ref type="bibr" coords="14,282.49,632.53,16.60,8.64" target="#b11">[12]</ref> suggest to ask participants for short descriptions of how the datasets have been constructed in the future. Altogether, the reviewers reverse-engineer the datasets in their review, making educated guesses at how they have been constructed and what are their document sources. Regarding the datasets of Asghari et al. <ref type="bibr" coords="15,204.61,131.27,10.58,8.64" target="#b4">[5]</ref>, Cheema et al. <ref type="bibr" coords="15,281.37,131.27,10.58,8.64" target="#b8">[9]</ref>, Mohtaj et al. <ref type="bibr" coords="15,354.27,131.27,15.27,8.64" target="#b31">[32]</ref>, and Palkovskii and Belov <ref type="bibr" coords="15,134.77,143.22,15.27,8.64" target="#b35">[36]</ref>, the reviewers find unusual synonym replacements as well as garbled text, which is probably due to the automatic obfuscation synthesis approaches used. Here, the automatic construction of datasets has its limits. Regarding datasets that are partially or completely non-English, the reviewers went to great lengths to study them, despite not being proficient in the datasets' languages: the reviewers translated non-English plagiarized passages to English using Google Translate in order to get an idea of whether paired text passages actually match with regard to their topic. This approach to overcoming the language barrier is highly commendable, and shows that improvisation can greatly enhance a reviewer's abilities. Even if the finer details of the obfuscation synthesis strategies applied in the non-English datasets are lost or skewed using Google Translate, the reviewers at least get an impression of the plagiarism cases. Altogether, the reviewers did not identify any extreme errors that invalidate any of the datasets for use in an evaluation.</p><p>The review submitted by Alvi et al. <ref type="bibr" coords="15,302.60,298.64,11.62,8.64" target="#b3">[4]</ref> has been conducted similarly to that of Franco-Salvador et al. <ref type="bibr" coords="15,223.73,310.60,15.27,8.64" target="#b11">[12]</ref>. The reviewers note inconsistencies of the annotations where character offsets and lengths do not appear to match the plagiarism cases in the datasets of Hanif et al. <ref type="bibr" coords="15,192.12,334.51,16.60,8.64" target="#b20">[21]</ref> and Mohtaj et al. <ref type="bibr" coords="15,280.62,334.51,15.27,8.64" target="#b31">[32]</ref>. Moreover, the reviewers also employ Google Translate to double-check the cross-language and non-English plagiarism cases for topical similarity. Altogether, the reviewers sometimes have difficulties in discerning the meaning of certain obfuscation synthesis names used by dataset authors, which is due to the fact that no explanation about them has been provided by the dataset authors. Again, they did not identify any detrimental errors. Furthermore, to help other reviewers in their task, Alvi et al. <ref type="bibr" coords="15,246.67,406.24,11.62,8.64" target="#b3">[4]</ref> shared a visual tool to help review plagiarism cases in the datasets.</p><p>The author lists of the datasets of Asghari et al. <ref type="bibr" coords="15,342.06,430.15,10.58,8.64" target="#b4">[5]</ref>, Mohtaj et al. <ref type="bibr" coords="15,412.46,430.15,15.27,8.64" target="#b31">[32]</ref>, and Khoshnavataher et al. <ref type="bibr" coords="15,199.11,442.10,16.60,8.64" target="#b24">[25]</ref> overlap, so that they decided to submit a joint review, written by Zarrabi et al. <ref type="bibr" coords="15,189.76,454.06,15.27,8.64" target="#b66">[67]</ref>. The reviewers compile some dataset statistics and report on manually reviewing 20 plagiarism cases per dataset. Despite some remarks on small errors identified, the authors do not find any systematic errors.</p><p>Finally, Cheema et al. <ref type="bibr" coords="15,242.73,489.92,11.62,8.64" target="#b8">[9]</ref> provide only a very short and superficial review, Kong et al. <ref type="bibr" coords="15,156.39,501.88,16.60,8.64" target="#b26">[27]</ref> compile only some corpus statistics without any remarks, and Palkovskii and Belov <ref type="bibr" coords="15,160.91,513.83,16.60,8.64" target="#b35">[36]</ref> did not take part in the review phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>The outcome of the review phase of our shared task is a mixed bag. While some reviewers made an honest attempt to conduct thorough reviews, most did so only superficially. From what we learned, the datasets can be used for evaluation with some confidence, they are not systematically compromised. With hindsight, data submissions should still involve a review phase, however, there should be more time for peer-review than only one or two weeks. Also, the authors of submitted datasets should have a chance of seeing their reviews before the final submission deadline, so that they have a chance of improving their datasets. Reviewers should also be allowed to provide anonymous feedback. Nevertheless, the reviews should be published to allow later users of the datasets to get an impartial idea of its quality. Finally, once the datasets are actually used for evaluation purposes either in another shared task, or by independent researchers, the researchers using them have a much higher motivation to actually look deep into the datasets they are using.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Plagiarism Detection Performance Measures</head><p>To assess the performance of the submitted datasets, we employ the text alignment software that has been submitted in previous years to the shared task of text alignment at PAN <ref type="bibr" coords="16,156.07,200.50,15.49,8.64" target="#b49">[50]</ref>: precision, recall, and granularity, which are combined into the plagdet score. Moreover, as of last year, we also compute case-level and document-level precision, recall, and F 1 . In what follows, we recap these performance measures.</p><p>Character level performance measures Let S denote the set of plagiarism cases in the corpus, and let R denote the set of detections reported by a plagiarism detector for the suspicious documents. A plagiarism case s = s plg , d plg , s src , d src , s ∈ S, is represented as a set s of references to the characters of d plg and d src , specifying the passages s plg and s src . Likewise, a plagiarism detection r ∈ R is represented as r. We say that r detects s iff s ∩ r = ∅ and s plg overlaps with r plg and s src overlaps with r src . Based on this notation, precision and recall of R under S can be measured as follows:</p><formula xml:id="formula_1" coords="16,145.38,331.75,324.59,27.47">prec(S, R) = 1 |R| r∈R | s∈S (s r)| |r| , r ec(S, R) = 1 |S| s∈S | r∈R (s r)| |s| ,</formula><p>where s r = s ∩ r if r detects s, ∅ otherwise.</p><p>Observe that neither precision nor recall account for the fact that plagiarism detectors sometimes report overlapping or multiple detections for a single plagiarism case. This is undesirable, and to address this deficit also a detector's granularity is quantified as follows:</p><formula xml:id="formula_2" coords="16,245.48,440.07,124.39,27.42">gran(S, R) = 1 |S R | s∈S R |R s |,</formula><p>where S R ⊆ S are cases detected by detections in R, and R s ⊆ R are detections of s; i.e., S R = {s | s ∈ S and ∃r ∈ R : r detects s} and R s = {r | r ∈ R and r detects s}. Note further that the above three measures alone do not allow for a unique ranking among detection approaches. Therefore, the measures are combined into a single overall score as follows:</p><formula xml:id="formula_3" coords="16,224.30,528.54,166.75,24.17">plagdet(S, R) = F 1 log 2 (1 + gran(S, R)) ,</formula><p>where F 1 is the equally weighted harmonic mean of precision and recall.</p><p>Case level performance measures Let S and R be defined as above. Further, let S = {s | s ∈ S and r ec char (s, R) &gt; τ 1 and ∃r ∈ R: r detects s and prec char (S, r) &gt; τ 2 } denote the subset of all plagiarism cases S which have been detected with more than a threshold τ 1 in terms of character recall r ec char and more than a threshold τ 2 in terms of character precision prec char . Likewise, let R = {r | r ∈ R and prec char (S, r) &gt; τ 2 and ∃s ∈ S: r detects s and r ec char (s, R) &gt; τ 1 } denote the subset of all detections R which contribute to detecting plagiarism cases with more than a threshold τ 1 in terms of character recall r ec char and more than a threshold τ 2 in terms of character precision prec char . Here, character recall and precision derive from the character level performance measures defined above:</p><formula xml:id="formula_4" coords="17,165.51,174.23,284.35,22.98">prec char (S, r) = | s∈S (s r)| |r| , r ec char (s, R) = | r∈R (s r)| |s| .</formula><p>Based on this notation, we compute case level precision and recall as follows:</p><formula xml:id="formula_5" coords="17,205.46,226.90,204.44,22.31">prec case (S, R) = |R | |R| , r ec case (S, R) = |S | |S| .</formula><p>The thresholds τ 1 and τ 2 can be used to adjust the minimal detection accuracy with regard to passage boundaries. Threshold τ 1 adjusts how accurate a plagiarism case has to be detected, whereas threshold τ 2 adjusts how accurate a plagiarism detection has to be. Beyond the minimal detection accuracy imposed by these thresholds, however, a higher detection accuracy does not contribute to case level precision and recall. If τ 1 → 1 and τ 2 → 1, the minimal required detection accuracy approaches perfection, whereas if τ 1 → 0 and τ 2 → 0, it is sufficient to report an entire document as plagiarized to achieve perfect case level precision and recall. In between these extremes, it is an open question which threshold settings are valid with regard to capturing the minimally required detection quality beyond which most users of a plagiarism detection system will not perceive improvements, anymore. Hence, we choose τ 1 = τ 2 = 0.5 as a reasonable trade off, for the time being: for case level precision, a plagiarism detection r counts a true positive detection if it contributes to detecting at least τ 1 = 0.5 ∼ 50% of a plagiarism case s, and, if at least τ 2 = 0.5 ∼ 50% of r contributes to detecting plagiarism cases. Likewise, for case level recall, a plagiarism case s counts as detected if at least 50% of s are detected, and, if a plagiarism detection r contributes to detecting s while at least 50% of r contributes to detecting plagiarism cases in general. Likewise, D pairs|R denotes the subset of D pairs for which plagiarism was detected when requiring a minimal detection accuracy as per R defined above. Based on this notation, we compute document level precision and recall as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document level performance measures</head><formula xml:id="formula_6" coords="17,136.41,643.30,342.54,24.22">prec doc (S, R) = |D pairs|S ∩ D pairs|R | |D pairs|R | , r ec doc (S, R) = |D pairs|S ∩ D pairs|R | |D pairs|S | .</formula><p>Again, the thresholds τ 1 and τ 2 allow for adjusting the minimal required detection accuracy for R , but for document level recall, it is sufficient that at least one plagiarism case is detected beyond that accuracy in order for the corresponding document pair (d plg , d src ) to be counted as true positive detection. If none of the plagiarism cases present in (d plg , d src ) is detected beyond the minimal detection accuracy, it is counted as false negative, whereas if detections are made for a pair of documents in which no plagiarism case is present, it is counted as false positive.</p><p>Discussion Compared to the character level measures, the case level measures relax the fine-grained measurement of plagiarism detection quality to allow for judging a detection algorithm by its capability of "spotting" plagiarism cases reasonably well with respect to the minimum detection accuracy fixed by the thresholds τ 1 and τ 2 . For example, a user who is interested in maximizing case level performance may put emphasis on the coverage of all plagiarism cases rather than the precise extraction of each individual plagiarized pair of passages. The document level measures further relax the requirements to allow for judging a detection algorithm by its capability "to raise a flag" for a given pair of documents, disregarding whether it finds all plagiarism cases contained. For example, a user who is interested in maximizing these measures puts emphasis on being made suspicious, which might lead to further, more detailed investigations. In this regard the three levels of performance measurement complement each other. To rank plagiarism detection with regard to their case level performance and their document level performance, we currently use the F α -Measure. While the best setting of α is also still unclear, we resort to α = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation Results per Dataset</head><p>This section reports on the detection performances of 31 text alignment approaches that have been submitted to the corresponding shared task at PAN 2012-2015, when run against the eight datasets submitted to this year's PAN shared task on text alignment dataset construction. To cut a long story short, we distinguish three kinds of datasets among the submitted ones: (1) datasets that yield typical detection performance results with state-of-the-art text alignment approaches, (2) datasets that yield poor detection performance results because state-of-the-art text alignment approaches are not prepared for them, and (3) datasets that are entirely solved by at least one of the state-of-the-art text alignment approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets with typical results</head><p>The datasets submitted by Alvi et al. <ref type="bibr" coords="18,410.10,541.23,10.58,8.64" target="#b3">[4]</ref>, Cheema et al. <ref type="bibr" coords="18,134.77,553.19,10.58,8.64" target="#b8">[9]</ref>, and Mohtaj et al. <ref type="bibr" coords="18,222.99,553.19,16.60,8.64" target="#b31">[32]</ref> yield typical detection performances among state-of-the-art text alignment approaches; Tables <ref type="table" coords="18,277.52,565.14,7.47,8.64" target="#tab_1">2,</ref><ref type="table" coords="18,288.69,565.14,3.74,8.64">3</ref>, and 4 show the results. In all cases, the top plagdet detection performance is around 0.8, whereas F 1 at case level is around 085-0.88, and F 1 at document level at 0.86-0.9. However, the top-performing text alignment approach differs: the approach of Glinos <ref type="bibr" coords="18,297.73,601.01,16.60,8.64" target="#b15">[16]</ref> performs best on the dataset of Alvi et al. <ref type="bibr" coords="18,134.77,612.96,10.58,8.64" target="#b3">[4]</ref>, whereas the approach of Oberreuter and Eiselt <ref type="bibr" coords="18,343.08,612.96,16.60,8.64" target="#b34">[35]</ref> performs best on the datasets of Cheema et al. <ref type="bibr" coords="18,205.38,624.92,11.62,8.64" target="#b8">[9]</ref> and Mohtaj et al. <ref type="bibr" coords="18,293.03,624.92,15.27,8.64" target="#b31">[32]</ref>. The latter approach ranks among the top text alignment approaches on all three datasets, including its preceding version from 2012 <ref type="bibr" coords="18,157.18,648.83,15.27,8.64" target="#b33">[34]</ref>. For comparison, the winning text alignment approach of PAN 2014 from Sanchez-Perez et al. <ref type="bibr" coords="22,182.61,131.27,15.27,8.64" target="#b56">[57]</ref>, as well as its 2015 successor <ref type="bibr" coords="22,323.60,131.27,15.27,8.64" target="#b55">[56]</ref>, achieves mid-range performances on the dataset of Alvi et al. <ref type="bibr" coords="22,253.30,143.22,10.58,8.64" target="#b3">[4]</ref>, low performance on that of Cheema et al. <ref type="bibr" coords="22,448.41,143.22,10.58,8.64" target="#b8">[9]</ref>, and second rank, following the approaches of Oberreuter on the dataset of Mohtaj et al. <ref type="bibr" coords="22,461.50,155.18,15.27,8.64" target="#b31">[32]</ref>. Some of these performance differences may be attributed to the fact that the approach of Sanchez-Perez et al. <ref type="bibr" coords="22,232.35,179.09,16.60,8.64" target="#b56">[57]</ref> has been optimized to work well on the previous year's PAN plagiarism corpus, whereas it has not, yet, been optimized against the submitted datasets, nor against all of them in combination.</p><p>Apparently, the obfuscation synthesis approaches applied during construction of the three aforementioned datasets compare in terms of difficulty to the ones applied during construction of the PAN plagiarism corpora. The detection performances on the submitted datasets are not perfect so that further algorithmic improvements are required. These datasets complement the ones that are already used, and, since they have been constructed independently, while still allowing for the existing text alignment approaches to work well, they verify that the previous datasets which have been constructed exclusively by ourselves, are fit for purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets with perfect results</head><p>For one of the submitted datasets, the text alignment approaches exert an odd performance characteristic, that is: either they detect almost all plagiarism, or none at all. Table <ref type="table" coords="22,279.46,341.23,4.98,8.64" target="#tab_2">5</ref> shows the performances obtained on the dataset of Khoshnavataher et al. <ref type="bibr" coords="22,237.67,353.19,15.27,8.64" target="#b24">[25]</ref>. The plagdet performances of the eight top-performing text alignment approaches range from 0.89 to 0.98, the top-performing one being the approach of Glinos <ref type="bibr" coords="22,215.89,377.10,15.27,8.64" target="#b15">[16]</ref>. At case level and at document level, all of them achieve F 1 scores above 0.9. There are only four approaches that achieve mid-range performances, including the Baseline, whereas the performances of all other approaches is negligible. The Baseline implements a basic text alignment approach using 4-grams for seeding, and rule-based merging.</p><p>The dataset of Khoshnavataher et al. <ref type="bibr" coords="22,295.03,436.87,16.60,8.64" target="#b24">[25]</ref> comprises Persian documents only, so that this performance characteristic demonstrates which of the approaches can cope with this language and which cannot. An explanation for the fact that not all approaches work may be that some work at the lexical level, whereas others employ sophisticated linguistic processing pipelines that may not be adapted to processing Persian text. Moreover, the fact that those approaches which are capable of detecting plagiarism within Persian documents do so almost flawlessly, hints that the obfuscation synthesis approaches applied by Khoshnavataher et al. <ref type="bibr" coords="22,267.68,520.56,16.60,8.64" target="#b24">[25]</ref> do not seem to yield notable obfuscation.</p><p>From reviewing the notebooks of the respective approaches, however, it is not entirely clear whether they indeed work at the lexical level. Glinos <ref type="bibr" coords="22,400.06,544.47,16.60,8.64" target="#b15">[16]</ref> mentions some pre-processing at word-level, but applies the character-based Smith-Waterman algorithm to align text passages between a pair of documents. Palkovskii and Belov <ref type="bibr" coords="22,461.50,568.38,15.27,8.64" target="#b37">[38]</ref>, who provides the second-best performing approach on Khoshnavataher et al. <ref type="bibr" coords="22,457.34,580.33,15.50,8.64" target="#b24">[25]</ref>'s dataset, report to employ a basic Euclidian distance-based clustering approach in their approach, which also hints that no linguistic pre-processing is applied. Interestingly, the best-performing approach of PAN 2014 from Sanchez-Perez et al. <ref type="bibr" coords="22,425.79,616.20,16.60,8.64" target="#b56">[57]</ref> does not work well, whereas this year's refined version Sanchez-Perez et al. <ref type="bibr" coords="22,401.86,628.15,16.60,8.64" target="#b55">[56]</ref> is ranked third. This suggests that the authors added a fallback solution for situations where only lexical matching applies, e.g., based on their approach to predict on-the-fly what kind of obfuscation is at hand to adjust their detection approach accordingly. Since the dataset of Khoshnavataher et al. <ref type="bibr" coords="24,238.72,131.27,16.60,8.64" target="#b24">[25]</ref> apparently does not comprise noteworthy obfuscation, which results in mostly verbatim overlap of text passages between a given pair of suspicious document and source document, this may trigger the classification approach used by Sanchez-Perez et al. <ref type="bibr" coords="24,230.68,167.13,15.27,8.64" target="#b55">[56]</ref>, which then applies a basic algorithm that deals with such situations at the lexical level.</p><p>Another noteworthy issue about this dataset is that some of its reviewers note that it has a high quality, which may hint at reviewer bias toward plagiarism cases that are more easy to be detected, compared to ones that are difficult to identify, even for a human. In this regard, implementing obfuscation synthesis approaches, which are supposed to construct difficult plagiarism cases, also makes the task of reviewing their results a lot more difficult, so that reviewers may tend to favor easy decisions over the difficult ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets with poor results</head><p>On four of the submitted datasets from Kong et al. <ref type="bibr" coords="24,461.50,281.45,15.27,8.64" target="#b26">[27]</ref>, Asghari et al. <ref type="bibr" coords="24,192.11,293.41,10.58,8.64" target="#b4">[5]</ref>, Palkovskii and Belov <ref type="bibr" coords="24,297.83,293.41,15.27,8.64" target="#b35">[36]</ref>, and Hanif et al. <ref type="bibr" coords="24,385.75,293.41,15.27,8.64" target="#b20">[21]</ref>, the text alignment approaches perform poorly, detecting almost none of the plagiarism cases. The best performances are obtained by the two versions of the approach of Oberreuter and Eiselt <ref type="bibr" coords="24,134.77,329.28,16.60,8.64" target="#b34">[35]</ref> and the two versions of the approach of Suchomel et al. <ref type="bibr" coords="24,382.25,329.28,16.60,8.64" target="#b65">[66]</ref> on the dataset comprising Chinese plagiarism cases from Kong et al. <ref type="bibr" coords="24,335.74,341.23,15.27,8.64" target="#b26">[27]</ref>. However, the top plagdet score achieved is still only 0.18. Most of the text alignment approaches detect almost none of the plagiarism cases on this dataset, whereas all approaches fail on the remaining datasets from Asghari et al. <ref type="bibr" coords="24,246.55,377.10,10.58,8.64" target="#b4">[5]</ref>, Palkovskii and Belov <ref type="bibr" coords="24,350.33,377.10,15.27,8.64" target="#b35">[36]</ref>, and Hanif et al. <ref type="bibr" coords="24,435.81,377.10,15.27,8.64" target="#b20">[21]</ref>.</p><p>This does not hint at any flaws in the datasets, but is testimony to the datasets' difficulty. The datasets of Asghari et al. <ref type="bibr" coords="24,303.54,401.01,11.62,8.64" target="#b4">[5]</ref> and Hanif et al. <ref type="bibr" coords="24,388.31,401.01,16.60,8.64" target="#b20">[21]</ref> are the only ones comprising cross-language plagiarism from Persian and Urdu to English, respectively. Since no lexical similarity between these languages can be expected, apart from perhaps a few named entities, and since apparently none of the text alignment approaches feature any kind of translation module or cross-language similarity measure, they cannot cope with these kinds of plagiarism cases.</p><p>Regarding the dataset submitted by Kong et al. <ref type="bibr" coords="24,338.86,472.74,15.27,8.64" target="#b26">[27]</ref>, which comprises monolingual Chinese plagiarism cases, a set of text alignment approaches seems to work, to a small extent, that corresponds to those working on the dataset of Khoshnavataher et al. <ref type="bibr" coords="24,461.50,496.65,15.27,8.64" target="#b24">[25]</ref>, probably for similar reasons as outlined above. However, for their dataset, Kong et al. <ref type="bibr" coords="24,134.77,520.56,16.60,8.64" target="#b26">[27]</ref> have optimized the obfuscation of the plagiarism cases until a Chinese plagiarism detector was unable to detect them, which makes the task of detecting these plagiarism cases very difficult, since lexical similarities are not to be expected. Moreover, most of the existing approaches are probably not optimized to process Chinese text, since each letter may carry a lot more semantics than letters from the Latin alphabet. Therefore, shorter character sequences may already hint a significant semantic similarity.</p><p>Regarding the dataset of Palkovskii and Belov <ref type="bibr" coords="24,343.79,592.29,15.27,8.64" target="#b35">[36]</ref>, the obfuscation approach of substituting characters with look-alike UTF characters seems to successfully confound all of the existing text alignment approaches. The dataset of Alvi et al. <ref type="bibr" coords="24,423.92,616.20,10.58,8.64" target="#b3">[4]</ref>, where the performances have otherwise been typical, also contains plagiarism cases where this kind of obfuscation has been applied. Discussion The evaluation of the existing text alignment approaches on the submitted datasets leaves us with more confidence in their quality. The datasets on which the approaches perform with results comparable to their performances on the PAN plagiarism corpora mutually verify that neither dataset is too far off from the others. Since the datasets have been constructed independently, this suggest that the intuition of the authors who constructed them corresponds to ours, albeit we may have strongly influenced them with our prior work.</p><p>Regarding the datasets where the existing text alignment approaches fail, we cannot deduce that the datasets are flawed. Rather, the characteristics of the datasets suggest that either tailored detection modules are required, or rather a better abstraction of the problem domain to allow for generic approaches. Finally, regarding the dataset where a number of text alignment approaches achieve almost perfect performance, this dataset has been beaten, which means that it may be used to confirm basic capabilities of a text alignment approach, but it does inform further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Analysis of Execution Errors</head><p>Not all of the existing text alignment approaches work without execution errors on the submitted datasets. On some datasets, approaches fail or print error output for various reasons. Table <ref type="table" coords="29,193.65,348.45,9.96,8.64" target="#tab_0">10</ref> gives an overview of common errors as reported by the output of failing approaches. Most of the errors observed hint at internal software errors, whereas others are opaque because hardly any error messages are printed by the software. Despite printing error messages, some pieces of software still generate output that can be evaluated, whereas others do not. Many of the errors observed occur on the datasets containing non-English documents, but also on datasets that make use letters from non-Latin alphabets. Moreover, we have excluded a number of approaches for being too slow.</p><p>We have repeatedly tried to get the respective approaches to work, however, the errors prevailed. We have also considered to invite the original authors to fix the errors in their software, but refrained from doing so, since then the already successfully obtained performance results on other evaluation corpora would be invalidated. Changing a software after it has been submitted to a shared task, even if only a small execution error is fixed, may have side effects on the software's performance when it is re-executed on a given dataset compared to its original performance. Arguably, a submitted software should be kept a fixture for the future and participants should rather be invited to submit a new version of their software where the errors are fixed and where they are free to make other improvements as they see fit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Outlook</head><p>In conclusion, we can say that data submissions to shared tasks are a viable option of creating evaluation resources. The datasets that have been submitted to our shared task have a variety that we would not have been able to create on our own. Despite the important role that the organizers of a shared task play in keeping things together, and Table <ref type="table" coords="30,193.85,115.83,7.47,8.06" target="#tab_0">10</ref>. Overview of executions errors observed on the submitted datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Software Submission</head><p>Alvi <ref type="bibr" coords="30,241.93,132.41,8.13,6.27" target="#b3">[4]</ref> Cheema <ref type="bibr" coords="30,313.59,132.41,8.13,6.27" target="#b8">[9]</ref> Khoshnavataher <ref type="bibr" coords="30,385.80,132.41,11.62,6.27" target="#b24">[25]</ref> Mohtaj <ref type="bibr" coords="30,433.26,132.41,11.62,6.27" target="#b31">[32]</ref> Team Year Asghari <ref type="bibr" coords="30,281.05,141.18,8.13,6.27" target="#b4">[5]</ref> Hanif <ref type="bibr" coords="30,338.77,141.18,11.62,6.27" target="#b20">[21]</ref> Kong <ref type="bibr" coords="30,399.57,141.18,11.62,6.27" target="#b26">[27]</ref> Palkovskii <ref type="bibr" coords="30,468.67,141.18,11.62,6.27" target="#b35">[36]</ref> Alvi <ref type="bibr" coords="30,149.21,154.70,8.13,6.05" target="#b2">[3]</ref> 2014 internal internal internal Gillam <ref type="bibr" coords="30,156.18,163.47,11.62,6.05" target="#b13">[14]</ref> 2012 runtime runtime runtime runtime runtime runtime runtime runtime Gillam <ref type="bibr" coords="30,156.18,172.24,11.62,6.05" target="#b12">[13]</ref> 2013 internal internal internal internal internal internal internal internal Gillam <ref type="bibr" coords="30,156.18,181.00,11.62,6.05" target="#b14">[15]</ref> 2014 no output no output no output no output no output no output no output no output Glinos <ref type="bibr" coords="30,155.41,189.77,11.62,6.05" target="#b15">[16]</ref> 2014 internal internal internal internal Jayapal <ref type="bibr" coords="30,157.73,198.54,11.62,6.05">[23]</ref> 2012 runtime runtime runtime runtime runtime runtime runtime runtime Kong <ref type="bibr" coords="30,152.06,207.31,11.62,6.05" target="#b28">[29]</ref> 2012 runtime runtime runtime runtime runtime runtime runtime runtime Kueppers <ref type="bibr" coords="30,163.43,216.07,11.62,6.05" target="#b29">[30]</ref> 2012 internal internal internal internal Oberreuter <ref type="bibr" coords="30,167.02,224.84,11.62,6.05" target="#b34">[35]</ref> 2014 memory R. Torrejón <ref type="bibr" coords="30,168.79,233.61,11.62,6.05" target="#b51">[52]</ref> 2012 internal internal internal internal R. Torrejón <ref type="bibr" coords="30,168.79,242.37,11.62,6.05" target="#b52">[53]</ref> 2013 internal internal internal internal internal internal internal no output R. Torrejón <ref type="bibr" coords="30,168.79,251.14,11.62,6.05" target="#b53">[54]</ref> 2014 internal internal internal internal internal internal internal Sánchez-Vega <ref type="bibr" coords="30,176.17,259.91,11.62,6.05" target="#b57">[58]</ref> 2012 runtime runtime runtime runtime runtime runtime runtime runtime Shrestha <ref type="bibr" coords="30,160.83,268.68,11.62,6.05" target="#b60">[61]</ref> 2013 runtime runtime runtime runtime runtime runtime runtime runtime Shrestha <ref type="bibr" coords="30,160.83,277.44,11.62,6.05" target="#b59">[60]</ref> 2014 runtime runtime runtime runtime runtime runtime runtime runtime making sure that all moving parts fit together, it is curious that data submissions are so uncommon.</p><p>In our case, we have been able to validate and evaluate the submitted datasets not only by manual peer-review, but also by executing all software submissions to previous editions of our shared task on the submitted datasets. Perhaps, asking for data submissions in a pilot task, where no software has been developed, yet, is not so attractive. It remains to be seen whether data submissions can only be successful in connection with software submissions. The organizational procedures that we have outlined in this paper may serve as first draft of a recipe to successful data submissions. However, in other contexts and other research fields, data submissions may not be as straightforward to be implemented. For example, if the data is sensitive, if it raises privacy concerns, or if it is difficult to obtain, data submissions may not be possible.</p><p>With software submissions and data submissions, we are only one step short of involving participants in all aspects of a shared tasks; what is still missing are the performance measures. Here, the organizers of a shared task often decide on a set of performance measures, whereas the community around a shared task may have different ideas as to how to measures performance. Involving participants in performance measure development and result analysis seems to be an obvious next step. Here, for example, it may well be possible to invite theoretical contributions to a shared task, since the development of performance measures for a given shared task is closely related to developing a theory around it.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="17,309.61,469.24,170.98,8.96;17,134.77,481.20,345.82,9.65;17,134.77,493.15,345.82,9.65;17,134.77,505.43,191.17,8.64;17,141.69,525.84,331.98,9.96;17,134.77,546.57,345.83,9.65;17,140.67,567.30,334.01,9.96;17,134.77,588.03,345.83,9.65"><head></head><label></label><figDesc>Let S, R, and R be defined as above. Further, let D plg be the set of suspicious documents and D src be the set potential source documents. Then D pairs = D plg × D src denotes the set of possible pairs of documents that a plagiarism detector may analyze, whereas D pairs|S = {(d plg , d src ) | (d plg , d src ) ∈ D pairs and ∃s ∈ S : d plg ∈ s and d src ∈ s} denotes the subset of D pairs whose document pairs contain the plagiarism cases S, and D pairs|R = {(d plg , d src ) | (d plg , d src ) ∈ D pairs and ∃r ∈ R : d plg ∈ r and d src ∈ r} denotes the corresponding subset of D pairs for which plagiarism was detected in R.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="9,134.77,115.83,345.82,229.85"><head>Table 1 .</head><label>1</label><figDesc>Overview of dataset statistics for the eight submitted datasets</figDesc><table coords="9,134.77,133.13,345.82,212.54"><row><cell>Dataset Statistics</cell><cell>Alvi [4]</cell><cell cols="2">Cheema [9]</cell><cell cols="3">Khoshnavataher [25]</cell><cell cols="2">Mohtaj [32]</cell></row><row><cell></cell><cell></cell><cell>Asghari [5]</cell><cell cols="2">Hanif [21]</cell><cell></cell><cell cols="2">Kong [27]</cell><cell cols="2">Palkovskii [36]</cell></row><row><cell>Generic</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>documents</cell><cell>272</cell><cell>27115</cell><cell>1000</cell><cell>1000</cell><cell>2111</cell><cell>160</cell><cell>4261</cell><cell></cell><cell>5057</cell></row><row><cell>plagiarism cases</cell><cell>150</cell><cell>11200</cell><cell>250</cell><cell>270</cell><cell>823</cell><cell>152</cell><cell>2781</cell><cell></cell><cell>4185</cell></row><row><cell>languages</cell><cell>en</cell><cell>en-fa</cell><cell>en</cell><cell>en-ur</cell><cell>fa</cell><cell>zh</cell><cell>en</cell><cell></cell><cell>en</cell></row><row><cell>Document purpose</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>source documents</cell><cell>37%</cell><cell>74%</cell><cell>50%</cell><cell>50%</cell><cell>50%</cell><cell>95%</cell><cell cols="2">78%</cell><cell>64%</cell></row><row><cell>suspicious documents</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>-with plagiarism</cell><cell>55%</cell><cell>13%</cell><cell>25%</cell><cell>27%</cell><cell>25%</cell><cell>5%</cell><cell cols="2">15%</cell><cell>33%</cell></row><row><cell>-w/o plagiarism</cell><cell>8%</cell><cell>13%</cell><cell>25%</cell><cell>23%</cell><cell>25%</cell><cell>0%</cell><cell cols="2">5%</cell><cell>3%</cell></row><row><cell>Document length</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>short (&lt;10 pp.)</cell><cell>92%</cell><cell>68%</cell><cell>100%</cell><cell>95%</cell><cell>95%</cell><cell>42%</cell><cell cols="2">46%</cell><cell>93%</cell></row><row><cell>medium (10-100 pp.)</cell><cell>8%</cell><cell>32%</cell><cell>0%</cell><cell>5%</cell><cell>5%</cell><cell>57%</cell><cell cols="2">54%</cell><cell>7%</cell></row><row><cell>long (&gt;100 pp.)</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell cols="2">0%</cell><cell>0%</cell></row><row><cell>Plagiarism per document</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>hardly (&lt;20%)</cell><cell>48%</cell><cell>68%</cell><cell>97%</cell><cell>89%</cell><cell>27%</cell><cell>75%</cell><cell cols="2">68%</cell><cell>80%</cell></row><row><cell>medium (20%-50%)</cell><cell>19%</cell><cell>32%</cell><cell>3%</cell><cell>10%</cell><cell>72%</cell><cell>25%</cell><cell cols="2">32%</cell><cell>19%</cell></row><row><cell>much (50%-80%)</cell><cell>33%</cell><cell>0%</cell><cell>0%</cell><cell>1%</cell><cell>0%</cell><cell>0%</cell><cell cols="2">0%</cell><cell>1%</cell></row><row><cell>entirely (&gt;80%)</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell cols="2">0%</cell><cell>0%</cell></row><row><cell>Case length</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>short (&lt;1k characters)</cell><cell>99%</cell><cell>99%</cell><cell>90%</cell><cell>99%</cell><cell>41%</cell><cell>85%</cell><cell>100%</cell><cell></cell><cell>97%</cell></row><row><cell>medium (1k-3k characters)</cell><cell>1%</cell><cell>1%</cell><cell>10%</cell><cell>1%</cell><cell>59%</cell><cell>11%</cell><cell cols="2">0%</cell><cell>3%</cell></row><row><cell>long (&gt;3k characters)</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>4%</cell><cell cols="2">0%</cell><cell>0%</cell></row><row><cell>Obfuscation synthesis approaches</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>character-substitution</cell><cell>33%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="19,134.77,173.83,345.83,269.02"><head>Table 2 .</head><label>2</label><figDesc>Cross-year evaluation of text alignment software submissions from 2012 to 2015 with respect to performance measures at character level, case level, and document level for the submitted dataset from Alvi et al.<ref type="bibr" coords="19,244.59,196.10,9.52,7.77" target="#b3">[4]</ref>.</figDesc><table coords="19,135.23,212.55,340.64,230.30"><row><cell cols="2">Software Submission</cell><cell></cell><cell>Character Level</cell><cell>Case Level</cell><cell></cell><cell cols="2">Document Level</cell></row><row><cell>Team</cell><cell>Year</cell><cell cols="2">plagdet prec r ec gran</cell><cell>prec r ec</cell><cell>F1</cell><cell>prec r ec</cell><cell>F1</cell></row><row><cell>Glinos [16]</cell><cell>2014</cell><cell>0.80</cell><cell>0.85 0.76 1.01</cell><cell cols="2">0.84 0.87 0.85</cell><cell cols="2">0.86 0.87 0.87</cell></row><row><cell>Oberreuter [35]</cell><cell>2014</cell><cell>0.76</cell><cell>0.87 0.67 1.00</cell><cell cols="2">0.90 0.69 0.78</cell><cell cols="2">0.91 0.69 0.79</cell></row><row><cell>Oberreuter [34]</cell><cell>2012</cell><cell>0.74</cell><cell>0.90 0.63 1.00</cell><cell cols="2">0.89 0.67 0.76</cell><cell cols="2">0.91 0.67 0.77</cell></row><row><cell>Palkovskii [40]</cell><cell>2014</cell><cell>0.67</cell><cell>0.67 0.66 1.00</cell><cell cols="2">0.69 0.75 0.72</cell><cell cols="2">0.76 0.75 0.75</cell></row><row><cell cols="2">Sanchez-Perez [56] 2015</cell><cell>0.66</cell><cell>0.86 0.53 1.00</cell><cell cols="2">0.78 0.53 0.63</cell><cell cols="2">0.78 0.53 0.63</cell></row><row><cell cols="2">Sanchez-Perez [57] 2014</cell><cell>0.62</cell><cell>0.90 0.48 1.00</cell><cell cols="2">0.85 0.49 0.62</cell><cell cols="2">0.85 0.49 0.62</cell></row><row><cell>Kong [26]</cell><cell>2014</cell><cell>0.57</cell><cell>0.86 0.44 1.00</cell><cell cols="2">0.86 0.48 0.62</cell><cell cols="2">0.86 0.48 0.62</cell></row><row><cell>Kong [28]</cell><cell>2013</cell><cell>0.57</cell><cell>0.80 0.46 1.00</cell><cell cols="2">0.88 0.49 0.63</cell><cell cols="2">0.88 0.49 0.63</cell></row><row><cell>Palkovskii [38]</cell><cell>2012</cell><cell>0.56</cell><cell>0.85 0.43 1.00</cell><cell cols="2">0.80 0.48 0.60</cell><cell cols="2">0.80 0.48 0.60</cell></row><row><cell>Gross [19]</cell><cell>2014</cell><cell>0.56</cell><cell>0.98 0.40 1.00</cell><cell cols="2">0.92 0.47 0.62</cell><cell cols="2">0.92 0.47 0.62</cell></row><row><cell>Suchomel [66]</cell><cell>2013</cell><cell>0.45</cell><cell>0.46 0.44 1.00</cell><cell cols="2">0.33 0.49 0.40</cell><cell cols="2">0.33 0.49 0.40</cell></row><row><cell>Palkovskii [39]</cell><cell>2013</cell><cell>0.44</cell><cell>0.70 0.34 1.05</cell><cell cols="2">0.53 0.40 0.45</cell><cell cols="2">0.62 0.40 0.49</cell></row><row><cell>Suchomel [65]</cell><cell>2012</cell><cell>0.41</cell><cell>0.82 0.27 1.00</cell><cell cols="2">0.71 0.32 0.44</cell><cell cols="2">0.71 0.32 0.44</cell></row><row><cell>Nourian [33]</cell><cell>2013</cell><cell>0.39</cell><cell>0.91 0.25 1.00</cell><cell cols="2">0.75 0.24 0.36</cell><cell cols="2">0.75 0.24 0.36</cell></row><row><cell>Saremi [59]</cell><cell>2013</cell><cell>0.36</cell><cell>0.70 0.28 1.19</cell><cell cols="2">0.33 0.28 0.31</cell><cell cols="2">0.43 0.28 0.35</cell></row><row><cell cols="2">R. Torrejón [52] 2012</cell><cell>0.28</cell><cell>0.96 0.17 1.00</cell><cell cols="2">0.43 0.16 0.23</cell><cell cols="2">0.43 0.16 0.23</cell></row><row><cell>Kueppers [30]</cell><cell>2012</cell><cell>0.26</cell><cell>0.92 0.15 1.00</cell><cell cols="2">0.65 0.15 0.24</cell><cell cols="2">0.65 0.15 0.24</cell></row><row><cell>Alvi [3]</cell><cell>2014</cell><cell>0.24</cell><cell>1.00 0.13 1.00</cell><cell cols="2">0.93 0.17 0.28</cell><cell cols="2">0.93 0.17 0.28</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="23,134.77,173.77,345.83,227.22"><head>Table 5 .</head><label>5</label><figDesc>Cross-year evaluation of text alignment software submissions from 2012 to 2015 with respect to performance measures at character level, case level, and document level for the submitted dataset from Khoshnavataher et al.<ref type="bibr" coords="23,286.51,196.04,13.74,7.77" target="#b24">[25]</ref>.</figDesc><table coords="23,135.23,212.49,340.64,188.50"><row><cell cols="2">Software Submission</cell><cell></cell><cell>Character Level</cell><cell>Case Level</cell><cell></cell><cell cols="2">Document Level</cell></row><row><cell>Team</cell><cell>Year</cell><cell cols="2">plagdet prec r ec gran</cell><cell>prec r ec</cell><cell>F1</cell><cell>prec r ec</cell><cell>F1</cell></row><row><cell>Glinos [16]</cell><cell>2014</cell><cell>0.98</cell><cell>0.97 0.99 1.00</cell><cell cols="2">0.98 0.99 0.99</cell><cell cols="2">0.98 0.99 0.99</cell></row><row><cell>Palkovskii [38]</cell><cell>2012</cell><cell>0.95</cell><cell>0.94 0.95 1.00</cell><cell cols="2">0.98 0.96 0.97</cell><cell cols="2">0.98 0.96 0.97</cell></row><row><cell cols="2">Sanchez-Perez [56] 2015</cell><cell>0.94</cell><cell>0.91 0.96 1.00</cell><cell cols="2">0.97 0.98 0.98</cell><cell cols="2">0.98 0.98 0.98</cell></row><row><cell>Suchomel [65]</cell><cell>2012</cell><cell>0.93</cell><cell>0.93 0.92 1.00</cell><cell cols="2">0.93 0.97 0.95</cell><cell cols="2">0.93 0.97 0.95</cell></row><row><cell>Palkovskii [40]</cell><cell>2014</cell><cell>0.92</cell><cell>0.88 0.95 1.00</cell><cell cols="2">0.94 0.97 0.96</cell><cell cols="2">0.94 0.97 0.96</cell></row><row><cell>Palkovskii [39]</cell><cell>2013</cell><cell>0.91</cell><cell>0.89 0.92 1.00</cell><cell cols="2">0.89 0.95 0.92</cell><cell cols="2">0.96 0.95 0.96</cell></row><row><cell>Suchomel [66]</cell><cell>2013</cell><cell>0.91</cell><cell>0.86 0.95 1.00</cell><cell cols="2">0.88 0.99 0.93</cell><cell cols="2">0.88 0.99 0.93</cell></row><row><cell>Alvi [3]</cell><cell>2014</cell><cell>0.89</cell><cell>0.95 0.90 1.05</cell><cell cols="2">0.91 0.95 0.93</cell><cell cols="2">0.99 0.95 0.97</cell></row><row><cell>Gross [19]</cell><cell>2014</cell><cell>0.80</cell><cell>0.76 0.98 1.09</cell><cell cols="2">0.72 1.00 0.84</cell><cell cols="2">0.78 1.00 0.88</cell></row><row><cell>Abnar [2]</cell><cell>2014</cell><cell>0.55</cell><cell>0.73 0.45 1.01</cell><cell cols="2">0.57 0.46 0.52</cell><cell cols="2">0.63 0.46 0.53</cell></row><row><cell>Oberreuter [35]</cell><cell>2014</cell><cell>0.40</cell><cell>0.25 1.00 1.00</cell><cell cols="2">0.13 1.00 0.23</cell><cell cols="2">0.15 1.00 0.25</cell></row><row><cell>Baseline</cell><cell>2015</cell><cell>0.37</cell><cell>0.97 0.50 2.41</cell><cell cols="2">0.14 0.37 0.20</cell><cell cols="2">0.34 0.37 0.35</cell></row><row><cell>Kong [26]</cell><cell>2014</cell><cell>0.07</cell><cell>0.04 0.80 1.00</cell><cell cols="2">0.00 0.80 0.00</cell><cell cols="2">0.00 0.80 0.00</cell></row><row><cell>Kong [28]</cell><cell>2013</cell><cell>0.07</cell><cell>0.04 0.80 1.00</cell><cell cols="2">0.00 0.80 0.00</cell><cell cols="2">0.00 0.80 0.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="25,134.77,173.53,345.83,176.65"><head>Table 6 .</head><label>6</label><figDesc>Cross-year evaluation of text alignment software submissions from 2012 to 2015 with respect to performance measures at character level, case level, and document level for the submitted dataset from Kong et al.<ref type="bibr" coords="25,248.26,195.80,13.74,7.77" target="#b26">[27]</ref>.</figDesc><table coords="25,135.23,212.75,340.64,137.43"><row><cell cols="2">Software Submission</cell><cell></cell><cell>Character Level</cell><cell></cell><cell>Case Level</cell><cell></cell><cell>Document Level</cell></row><row><cell>Team</cell><cell>Year</cell><cell cols="3">plagdet prec r ec gran</cell><cell>prec r ec</cell><cell>F1</cell><cell>prec r ec</cell><cell>F1</cell></row><row><cell>Oberreuter [35]</cell><cell>2014</cell><cell>0.18</cell><cell cols="2">0.16 0.21 1.00</cell><cell cols="2">0.16 0.23 0.19</cell><cell>0.21 0.23 0.22</cell></row><row><cell>Suchomel [66]</cell><cell>2013</cell><cell>0.17</cell><cell cols="2">0.16 0.18 1.00</cell><cell cols="2">0.10 0.19 0.13</cell><cell>0.12 0.18 0.14</cell></row><row><cell>Suchomel [65]</cell><cell>2012</cell><cell>0.16</cell><cell cols="2">0.17 0.15 1.00</cell><cell cols="2">0.11 0.16 0.13</cell><cell>0.14 0.15 0.15</cell></row><row><cell>Oberreuter [34]</cell><cell>2012</cell><cell>0.12</cell><cell cols="2">0.14 0.11 1.00</cell><cell cols="2">0.11 0.11 0.11</cell><cell>0.14 0.11 0.12</cell></row><row><cell>Gross [19]</cell><cell>2014</cell><cell>0.11</cell><cell cols="2">0.13 0.10 1.08</cell><cell cols="2">0.13 0.11 0.12</cell><cell>0.23 0.12 0.16</cell></row><row><cell>Abnar [2]</cell><cell>2014</cell><cell>0.07</cell><cell cols="2">0.21 0.08 2.17</cell><cell cols="2">0.10 0.11 0.10</cell><cell>0.25 0.14 0.18</cell></row><row><cell>Alvi [3]</cell><cell>2014</cell><cell>0.07</cell><cell>0.15 0.08</cell><cell>2.0</cell><cell cols="2">0.04 0.09 0.06</cell><cell>0.17 0.14 0.15</cell></row><row><cell>Palkovskii [38]</cell><cell>2012</cell><cell>0.06</cell><cell cols="2">0.10 0.05 1.00</cell><cell cols="2">0.07 0.05 0.06</cell><cell>0.07 0.05 0.06</cell></row><row><cell>Glinos [16]</cell><cell>2014</cell><cell>0.06</cell><cell cols="2">0.07 0.05 1.00</cell><cell cols="2">0.04 0.04 0.04</cell><cell>0.11 0.07 0.08</cell></row><row><cell>Kong [26]</cell><cell>2014</cell><cell>0.05</cell><cell cols="2">0.03 0.56 1.01</cell><cell cols="2">0.00 0.57 0.01</cell><cell>0.01 0.56 0.03</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,643.41,335.85,7.77;1,144.73,654.37,284.16,7.77"><p>The etymology of the term "shared task" is unclear; conceivably, it was coined to describe a special kind of conference track and was picked up into general use from there.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1" coords="23,135.23,593.29,37.38,6.05;23,191.04,593.29,13.95,6.05;23,233.96,594.06,3.49,6.05;23,259.01,594.06,3.49,6.05;23,281.12,594.06,3.49,6.05;23,303.24,594.06,3.49,6.05;23,341.40,594.06,3.49,6.05;23,363.51,594.06,3.49,6.05;23,385.63,594.06,3.49,6.05;23,423.79,594.06,3.49,6.05;23,445.90,594.06,3.49,6.05;23,468.02,594.06,3.49,6.05"><p>Shrestha<ref type="bibr" coords="23,160.99,593.29,11.62,6.05" target="#b59">[60]</ref> 2014----------</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2" coords="28,135.23,557.34,69.75,6.05;28,233.96,558.11,3.49,6.05;28,259.01,558.11,3.49,6.05;28,281.12,558.11,3.49,6.05;28,303.24,558.11,3.49,6.05;28,341.40,558.11,3.49,6.05;28,363.51,558.11,3.49,6.05;28,385.63,558.11,3.49,6.05;28,423.79,558.11,3.49,6.05;28,445.90,558.11,3.49,6.05;28,468.02,558.11,3.49,6.05;28,135.23,569.14,69.75,6.05;28,233.96,569.90,3.49,6.05;28,259.01,569.90,3.49,6.05;28,281.12,569.90,3.49,6.05;28,303.24,569.90,3.49,6.05;28,341.40,569.90,3.49,6.05;28,363.51,569.90,3.49,6.05;28,385.63,569.90,3.49,6.05;28,423.79,569.90,3.49,6.05;28,445.90,569.90,3.49,6.05;28,468.02,569.90,3.49,6.05;28,135.23,580.93,37.38,6.05;28,191.04,580.93,13.95,6.05;28,233.96,581.70,3.49,6.05;28,259.01,581.70,3.49,6.05;28,281.12,581.70,3.49,6.05;28,303.24,581.70,3.49,6.05;28,341.40,581.70,3.49,6.05;28,363.51,581.70,3.49,6.05;28,385.63,581.70,3.49,6.05;28,423.79,581.70,3.49,6.05;28,445.90,581.70,3.49,6.05;28,468.02,581.70,3.49,6.05;28,135.23,592.73,37.38,6.05;28,191.04,592.73,13.95,6.05;28,233.96,593.50,3.49,6.05;28,259.01,593.50,3.49,6.05;28,281.12,593.50,3.49,6.05;28,303.24,593.50,3.49,6.05;28,341.40,593.50,3.49,6.05;28,363.51,593.50,3.49,6.05;28,385.63,593.50,3.49,6.05;28,423.79,593.50,3.49,6.05;28,445.90,593.50,3.49,6.05;28,468.02,593.50,3.49,6.05"><p>R. Torrejón<ref type="bibr" coords="28,170.70,557.34,11.62,6.05" target="#b53">[54]</ref> 2014----------Sánchez-Vega [58] 2012 ----------Shrestha [61] 2013 ----------Shrestha [60] 2014 ----------</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank the participating teams of this shared task as well as those of previous editions for their devoted work.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Jayapal <ref type="bibr" coords="19,157.89,447.98,11.62,6.05" target="#b23">[24]</ref> 2013 0.10 0.47 0.09 2.0 0.00 0.00 0.00 0.00 0.00 0.00 Baseline 2015 0.03 1.00 0.02 1.00 0.00 0.00 0.00 0.00 0.00 0.00</p><p>Abnar <ref type="bibr" coords="19,154.40,475.80,8.13,6.05" target="#b1">[2]</ref> 2014 0.03 0.94 0.02 1.00 1.00 0.03 0.05 1.00 0.03 0.05</p><p>Gillam <ref type="bibr" coords="19,156.35,487.41,11.62,6.05" target="#b12">[13]</ref> 2013 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00</p><p>Gillam <ref type="bibr" coords="19,156.35,498.98,11.62,6.05" target="#b14">[15]</ref> 2014 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00</p><p>Gillam <ref type="bibr" coords="19,156.35,510.67,11.62,6.05" target="#b13">[14]</ref> 2012</p><p>Table <ref type="table" coords="20,158.53,173.55,3.36,8.06">3</ref>. Cross-year evaluation of text alignment software submissions from 2012 to 2015 with respect to performance measures at character level, case level, and document level for the submitted dataset from Cheema et al. <ref type="bibr" coords="20,258.04,195.82,9.52,7.77" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Software Submission</head><p>Character Level Case Level Document Level Team Year plagdet prec r ec gran prec r ec F1 prec r ec F1</p><p>Oberreuter <ref type="bibr" coords="20,167.18,238.20,2.90,6.05">[</ref>  Gillam <ref type="bibr" coords="21,156.35,499.44,11.62,6.05" target="#b12">[13]</ref> 2013 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00</p><p>Oberreuter <ref type="bibr" coords="23,167.18,405.93,2.90,6.05">[</ref> Saremi <ref type="bibr" coords="23,156.73,429.12,11.62,6.05" target="#b58">[59]</ref> 2013 0.00 0.09 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 R. Torrejón <ref type="bibr" coords="23,170.70,440.73,11.62,6.05" target="#b51">[52]</ref> 2012 0.00 1.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 Jayapal <ref type="bibr" coords="23,157.89,452.30,11.62,6.05" target="#b23">[24]</ref> 2013 0.00 1.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 Kueppers <ref type="bibr" coords="23,163.59,463.88,11.62,6.05" target="#b29">[30]</ref> 2012 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00</p><p>Nourian <ref type="bibr" coords="23,159.83,475.45,11.62,6.05" target="#b32">[33]</ref> 2013 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 Jayapal <ref type="bibr" coords="25,157.89,452.77,11.62,6.05" target="#b23">[24]</ref> 2013 0.00 0.08 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00</p><p>Gillam <ref type="bibr" coords="25,156.35,464.34,11.62,6.05" target="#b14">[15]</ref> 2014 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 Kueppers <ref type="bibr" coords="25,163.59,475.92,11.62,6.05" target="#b29">[30]</ref> 2012 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00</p><p>Nourian <ref type="bibr" coords="25,159.83,487.49,11.62,6.05" target="#b32">[33]</ref> 2013 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 Kong <ref type="bibr" coords="26,152.23,262.30,11.62,6.05" target="#b27">[28]</ref> 2013 0.00 0.01 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00</p><p>Kong <ref type="bibr" coords="26,152.23,273.88,11.62,6.05" target="#b25">[26]</ref> 2014 0.00 0.01 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 Jayapal <ref type="bibr" coords="26,157.89,285.49,11.62,6.05" target="#b23">[24]</ref> 2013 0.00 0.89 0.00 1.12 0.00 0.00 0.00 0.00 0.00 0.00</p><p>Palkovskii <ref type="bibr" coords="26,166.15,297.11,11.62,6.05" target="#b37">[38]</ref> 2012 0.00 1.00 0.00 1.00 1.00 0.00 0.00 1.00 0.00 0.00</p><p>Suchomel <ref type="bibr" coords="26,164.87,308.68,11.62,6.05" target="#b64">[65]</ref> 2012 0.00 1.00 0.00 1.00 1.00 0.00 0.00 1.00 0.00 0.00</p><p>Suchomel <ref type="bibr" coords="26,164.87,320.25,11.62,6.05" target="#b65">[66]</ref> 2013 0.00 1.00 0.00 1.00 1.00 0.00 0.00 1.00 0.00 0.00</p><p>Alvi <ref type="bibr" coords="26,149.37,331.82,8.13,6.05" target="#b2">[3]</ref> 2014 0.00 1.00 0.00 1.00 1.00 0.00 0.00 1.00 0.00 0.00</p><p>Palkovskii <ref type="bibr" coords="26,166.15,343.40,11.62,6.05" target="#b39">[40]</ref> 2014 0.00 1.00 0.00 1.00 1.00 0.00 0.00 1.00 0.00 0.00 Sanchez-Perez <ref type="bibr" coords="26,178.02,354.97,11.62,6.05" target="#b55">[56]</ref> 2015 0.00 1.00 0.00 1.00 1.00 0.00 0.00 1.00 0.00 0.00</p><p>Abnar <ref type="bibr" coords="26,154.40,366.54,8.13,6.05" target="#b1">[2]</ref> 2014 0.00 1.00 0.00 1.00 1.00 0.00 0.00 1.00 0.00 0.00</p><p>Saremi <ref type="bibr" coords="26,156.73,378.11,11.62,6.05" target="#b58">[59]</ref> 2013 0.00 0.06 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00</p><p>Palkovskii <ref type="bibr" coords="26,166.15,389.69,11.62,6.05" target="#b38">[39]</ref> 2013 0.00 1.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 Baseline 2015 0.00 1.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00</p><p>Glinos <ref type="bibr" coords="26,155.57,417.31,11.62,6.05" target="#b15">[16]</ref> 2014 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 Kueppers <ref type="bibr" coords="26,163.59,428.89,11.62,6.05" target="#b29">[30]</ref> 2012 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00</p><p>Gross <ref type="bibr" coords="26,153.24,440.46,11.62,6.05" target="#b18">[19]</ref> 2014 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00</p><p>Nourian <ref type="bibr" coords="26,159.83,452.03,11.62,6.05" target="#b32">[33]</ref> 2013 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00</p><p>Oberreuter <ref type="bibr" coords="26,167.18,463.60,11.62,6.05" target="#b34">[35]</ref> 2014 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 R. Torrejón <ref type="bibr" coords="26,170.70,475.18,11.62,6.05" target="#b51">[52]</ref> 2012 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00  <ref type="bibr" coords="27,170.70,378.43,11.62,6.05" target="#b52">[53]</ref> 2013 0.00 0.99 0.00 1.00 1.00 0.00 0.00 1.00 0.00 0.00</p><p>Suchomel <ref type="bibr" coords="27,164.87,390.05,11.62,6.05" target="#b64">[65]</ref> 2012 0.00 1.00 0.00 1.00 0.67 0.00 0.00 0.67 0.00 0.00</p><p>Alvi <ref type="bibr" coords="27,149.37,401.62,8.13,6.05" target="#b2">[3]</ref> 2014 0.00 0.83 0.00 1.25 0.17 0.00 0.00 0.25 0.00 0.00 Baseline 2015 0.00 0.89 0.00 1.33 0.11 0.00 0.00 0.20 0.00 0.00 R. Torrejón <ref type="bibr" coords="27,170.70,429.33,11.62,6.05" target="#b51">[52]</ref> 2012 0.00 1.00 0.00 1.00 1.00 0.00 0.00 1.00 0.00 0.00 Sanchez-Perez <ref type="bibr" coords="27,178.02,440.95,11.62,6.05" target="#b55">[56]</ref> 2015 0.00 0.94 0.00 1.00 1.00 0.00 0.00 1.00 0.00 0.00 Jayapal <ref type="bibr" coords="27,157.89,452.56,11.62,6.05" target="#b23">[24]</ref> 2013 0.00 1.00 0.00 1.25 0.00 0.00 0.00 0.00 0.00 0.00</p><p>Gillam <ref type="bibr" coords="27,156.35,464.13,11.62,6.05" target="#b12">[13]</ref> 2013 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00</p><p>Gillam <ref type="bibr" coords="27,156.35,475.71,11.62,6.05" target="#b14">[15]</ref> 2014 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 Kong <ref type="bibr" coords="27,152.23,487.28,11.62,6.05" target="#b27">[28]</ref> 2013 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00</p><p>Kong <ref type="bibr" coords="27,152.23,498.85,11.62,6.05" target="#b25">[26]</ref> 2014 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00</p><p>Nourian <ref type="bibr" coords="27,159.83,510.42,11.62,6.05" target="#b32">[33]</ref> 2013 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00</p><p>Abnar <ref type="bibr" coords="27,154.40,522.00,8.13,6.05" target="#b1">[2]</ref> 2014 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 Oberreuter <ref type="bibr" coords="28,167.18,273.67,11.62,6.05" target="#b34">[35]</ref> 2014 0.01 1.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 Jayapal <ref type="bibr" coords="28,157.89,285.24,11.62,6.05" target="#b23">[24]</ref> 2013 0.00 1.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00</p><p>Alvi <ref type="bibr" coords="28,149.37,296.81,8.13,6.05" target="#b2">[3]</ref> 2014 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00</p><p>Glinos <ref type="bibr" coords="28,155.57,308.39,11.62,6.05" target="#b15">[16]</ref> 2014 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 Suchomel <ref type="bibr" coords="28,164.87,319.96,11.62,6.05" target="#b64">[65]</ref> 2012 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 Kong <ref type="bibr" coords="28,152.23,331.53,11.62,6.05" target="#b27">[28]</ref> 2013 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00</p><p>Kong <ref type="bibr" coords="28,152.23,343.10,11.62,6.05" target="#b25">[26]</ref> 2014 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 Kueppers <ref type="bibr" coords="28,163.59,354.68,11.62,6.05" target="#b29">[30]</ref> 2012 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00</p><p>Gross <ref type="bibr" coords="28,153.24,366.25,11.62,6.05" target="#b18">[19]</ref> 2014 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00</p><p>Nourian <ref type="bibr" coords="28,159.83,377.82,11.62,6.05" target="#b32">[33]</ref> 2013 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00</p><p>Oberreuter <ref type="bibr" coords="28,167.18,389.39,11.62,6.05" target="#b33">[34]</ref> 2012 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00</p><p>Palkovskii <ref type="bibr" coords="28,166.15,400.96,11.62,6.05" target="#b37">[38]</ref> 2012 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00</p><p>Palkovskii <ref type="bibr" coords="28,166.15,412.54,11.62,6.05" target="#b38">[39]</ref> 2013 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00</p><p>Palkovskii <ref type="bibr" coords="28,166.15,424.11,11.62,6.05" target="#b39">[40]</ref> 2014 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 Baseline 2015 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 Sanchez-Perez <ref type="bibr" coords="28,178.02,451.74,11.62,6.05" target="#b55">[56]</ref> 2015 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00</p><p>Suchomel <ref type="bibr" coords="28,164.87,463.31,11.62,6.05" target="#b65">[66]</ref> 2013 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00</p><p>Abnar <ref type="bibr" coords="28,154.40,474.88,8.13,6.05" target="#b1">[2]</ref> 2014 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="30,142.61,646.13,317.36,7.77;30,150.95,657.08,306.49,7.77;31,150.95,119.96,117.10,7.77" xml:id="b0">
	<monogr>
		<title level="m" coord="30,150.96,646.13,309.01,7.77;30,150.95,657.08,135.85,7.77">Proceedings of the Workshop on Shared Tasks and Comparative Evaluation in Natural Language Generation, Position Papers</title>
		<meeting>the Workshop on Shared Tasks and Comparative Evaluation in Natural Language Generation, Position Papers<address><addrLine>Arlington, Virginia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">April 20-21, 2007. 2007</date>
		</imprint>
		<respStmt>
			<orgName>The Ohio State University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="31,142.61,130.28,330.03,7.77;31,150.95,141.24,192.43,7.77" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="31,334.95,130.28,137.69,7.77;31,150.95,141.24,147.61,7.77">Expanded N-Grams for Semantic Text Alignment-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Shakery</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,142.61,151.56,308.69,7.77;31,150.95,162.52,188.93,7.77" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="31,279.90,151.56,171.40,7.77;31,150.95,162.52,144.12,7.77">Hashing and Merging Heuristics for Text Reuse Detection-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alvi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Stevenson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,142.61,172.85,337.46,7.77;31,150.95,183.80,42.57,7.77" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="31,279.90,172.85,200.17,7.77">The Short Stories Corpus-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alvi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Stevenson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,142.61,194.13,323.99,7.77;31,150.95,205.09,319.31,7.77;31,150.95,216.04,46.06,7.77" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="31,348.90,194.13,117.70,7.77;31,150.95,205.09,319.31,7.77">Developing Bilingual Plagiarism Detection Corpus Using Sentence Aligned Parallel Corpus-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Asghari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Khoshnavataher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Fatemi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Faili</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,142.61,226.37,308.12,7.77;31,150.95,237.33,327.91,7.77;31,150.95,248.28,238.77,7.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="31,281.75,226.37,168.98,7.77;31,150.95,237.33,64.38,7.77">Paraphrase Acquisition via Crowdsourcing and Machine Learning</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Burrows</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2483676" />
	</analytic>
	<monogr>
		<title level="j" coord="31,222.41,237.33,237.04,7.77">Transactions on Intelligent Systems and Technology (ACM TIST)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="43" />
			<date type="published" when="2013-06">Jun 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,142.61,258.61,326.94,7.77;31,150.95,269.57,312.91,7.77;31,150.95,280.53,131.08,7.77;31,150.95,291.48,198.21,7.77" xml:id="b6">
	<analytic>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
	</analytic>
	<monogr>
		<title level="m" coord="31,351.72,258.61,117.83,7.77;31,150.95,269.57,124.46,7.77">CLEF 2014 Evaluation Labs and Workshop -Working Notes Papers</title>
		<title level="s" coord="31,401.07,269.57,62.79,7.77;31,150.95,280.53,68.09,7.77">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Halvey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</editor>
		<meeting><address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-09">September. 2014</date>
			<biblScope unit="page" from="15" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,142.61,301.81,327.88,7.77;31,150.95,312.77,320.38,7.77;31,150.95,323.72,131.08,7.77;31,150.95,334.68,198.21,7.77" xml:id="b7">
	<analytic>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
	</analytic>
	<monogr>
		<title level="m" coord="31,352.67,301.81,117.83,7.77;31,150.95,312.77,124.46,7.77">CLEF 2015 Evaluation Labs and Workshop -Working Notes Papers</title>
		<title level="s" coord="31,408.54,312.77,62.79,7.77;31,150.95,323.72,68.09,7.77">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>San Juan</surname></persName>
		</editor>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09">September. 2015</date>
			<biblScope unit="page" from="8" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,142.61,345.01,306.32,7.77;31,150.95,355.96,322.18,7.77;31,150.95,366.92,22.66,7.77" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="31,401.61,345.01,47.32,7.77;31,150.95,355.96,299.76,7.77">A Corpus for Analyzing Text Reuse by People of Different Groups-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Cheema</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Najib</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bukhari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sittar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nawab</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,142.24,377.25,309.46,7.77;31,150.95,388.20,266.78,7.77;31,150.95,399.16,198.21,7.77" xml:id="b9">
	<monogr>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
		<title level="m" coord="31,333.87,377.25,117.83,7.77;31,150.95,388.20,124.46,7.77">CLEF 2012 Evaluation Labs and Workshop -Working Notes Papers</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Karlgren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Womser-Hacker</surname></persName>
		</editor>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-09">September. 2012</date>
			<biblScope unit="page" from="17" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,142.24,409.49,313.03,7.77;31,150.95,420.44,234.61,7.77;31,150.95,431.40,198.21,7.77" xml:id="b10">
	<monogr>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
		<title level="m" coord="31,291.84,409.49,163.43,7.77;31,150.95,420.44,78.86,7.77">CLEF 2013 Evaluation Labs and Workshop -Working Notes Papers</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Navigli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Tufis</surname></persName>
		</editor>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-09">September. 2013</date>
			<biblScope unit="page" from="23" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,142.24,441.73,333.59,7.77;31,150.95,452.68,327.67,7.77;31,150.95,463.64,70.22,7.77" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="31,392.45,441.73,83.38,7.77;31,150.95,452.68,327.67,7.77;31,150.95,463.64,21.92,7.77">PAN 2015 Shared Task on Plagiarism Detection: Evaluation of Corpora for Text Alignment-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Franco-Salvador</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bensalem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Flores</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,142.24,473.97,291.57,7.77;31,150.95,484.92,196.90,7.77" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="31,192.80,473.97,241.01,7.77;31,150.95,484.92,144.12,7.77">Guess Again and See if They Line Up: Surrey&apos;s Runs at Plagiarism Detection-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gillam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,142.24,495.25,325.43,7.77;31,150.95,506.21,328.25,7.77;31,150.95,517.17,66.24,7.77" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="31,284.13,495.25,183.54,7.77;31,150.95,506.21,328.25,7.77;31,150.95,517.17,30.64,7.77">Educated Guesses and Equality Judgements: Using Search Engines and Pairwise Match for External Plagiarism Detection-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gillam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Newbold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Cooke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,142.24,527.49,333.97,7.77;31,150.95,538.45,233.02,7.77" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="31,232.68,527.49,243.53,7.77;31,150.95,538.45,184.72,7.77">Evaluating Robustness for &apos;IPCRESS&apos;: Surrey&apos;s Text Alignment for Plagiarism Detection-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gillam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Notley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,142.24,548.77,331.39,7.77;31,150.95,559.73,46.06,7.77" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="31,192.80,548.77,280.83,7.77">A Hybrid Architecture for Plagiarism Detection-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Glinos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,142.24,570.05,331.64,7.77;31,150.95,581.01,317.98,7.77;31,150.95,591.97,329.64,7.77;31,150.95,602.93,307.09,7.77;31,150.95,613.89,272.70,7.77" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="31,163.91,581.01,207.78,7.77">Recent Trends in Digital Text Forensics and its Evaluation</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Busse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="31,288.22,591.97,192.37,7.77;31,150.95,602.93,307.09,7.77;31,150.95,613.89,37.80,7.77">Information Access Evaluation meets Multilinguality, Multimodality, and Visualization. 4th International Conference of the CLEF Initiative (CLEF 13)</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Paredes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Rosso</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Stein</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013-09">Sep 2013</date>
			<biblScope unit="page" from="282" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,142.24,624.21,301.81,7.77;31,150.95,635.17,319.88,7.77;31,150.95,646.13,327.49,7.77;31,150.95,657.08,246.99,7.77" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="31,274.61,624.21,169.44,7.77;31,150.95,635.17,182.43,7.77">Ousting Ivory Tower Research: Towards a Web Framework for Providing Experiments as a Service</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Burrows</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="31,245.84,646.13,232.61,7.77;31,150.95,657.08,119.02,7.77">International ACM Conference on Research and Development in Information Retrieval (SIGIR 12)</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Hersh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Callan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Maarek</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012-08">Aug 2012</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1125" to="1126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="32,142.24,119.96,284.88,7.77;32,150.95,130.92,178.48,7.77" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="32,236.38,119.96,190.73,7.77;32,150.95,130.92,130.17,7.77">Plagiarism Alignment Detection by Merging Context Seeds-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Modaresi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="32,142.24,141.88,327.87,7.77;32,150.95,152.84,328.65,7.77;32,150.95,163.80,273.07,7.77;32,150.95,174.76,198.21,7.77" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="32,277.48,141.88,192.63,7.77;32,150.95,152.84,120.68,7.77">Source Retrieval for Plagiarism Detection from Large Web Corpora: Recent Approaches</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
	</analytic>
	<monogr>
		<title level="m" coord="32,291.16,152.84,188.45,7.77;32,150.95,163.80,194.39,7.77">Working Notes Papers of the CLEF 2015 Evaluation Labs. CEUR Workshop Proceedings, CLEF and CEUR</title>
		<imprint>
			<date type="published" when="2015-09">Sep 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="32,142.24,185.71,305.21,7.77;32,150.95,196.67,324.65,7.77" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="32,388.68,185.71,58.77,7.77;32,150.95,196.67,276.35,7.77">Cross-Language Urdu-English (CLUE) Text Alignment Corpus-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Hanif</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nawab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Arbab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jamshed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Riaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Munir</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="32,142.24,207.63,315.73,7.77;32,150.95,218.59,287.00,7.77;32,150.95,229.55,324.14,7.77;32,150.95,240.51,140.58,7.77" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="32,389.14,218.59,48.81,7.77;32,150.95,229.55,175.51,7.77">Report on the Evaluation-as-a-Service (EaaS) Expert Workshop</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hopfgartner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kando</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<ptr target="http://sigir.org/forum/issues/june-2015/" />
	</analytic>
	<monogr>
		<title level="j" coord="32,334.01,229.55,49.43,7.77">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="65" />
			<date type="published" when="2015-06">Jun 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="32,142.24,251.47,329.71,7.77;32,150.95,262.43,199.14,7.77;32,150.95,273.39,198.21,7.77" xml:id="b22">
	<monogr>
		<title level="m" type="main" coord="32,195.77,251.47,276.17,7.77;32,150.95,262.43,144.12,7.77">Similarity Overlap Metric and Greedy String Tiling at PAN 2012: Plagiarism Detection-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jayapal</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="32,142.24,284.34,329.92,7.77;32,150.95,295.30,267.51,7.77;32,150.95,306.26,317.51,7.77;32,150.95,317.22,17.93,7.77" xml:id="b23">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jayapal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Goswami</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
		<title level="m" coord="32,247.24,284.34,224.92,7.77;32,150.95,295.30,33.39,7.77">Submission to the 5th International Competition on Plagiarism Detection</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Nuance Communications</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="32,142.24,328.18,333.91,7.77;32,150.95,339.14,312.30,7.77;32,150.95,350.10,98.29,7.77" xml:id="b24">
	<monogr>
		<title level="m" type="main" coord="32,357.46,328.18,118.69,7.77;32,150.95,339.14,312.30,7.77;32,150.95,350.10,49.99,7.77">Developing Monolingual Persian Corpus for Extrinsic Plagiarism Detection Using Artificial Obfuscation-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Khoshnavataher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Zarrabi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mohtaj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Asghari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="32,142.24,361.06,334.87,7.77;32,150.95,372.02,291.15,7.77;32,150.95,382.97,233.02,7.77" xml:id="b25">
	<monogr>
		<title level="m" type="main" coord="32,382.18,361.06,94.93,7.77;32,150.95,372.02,291.15,7.77;32,150.95,382.97,184.72,7.77">Source Retrieval Based on Learning to Rank and Text Alignment Based on Plagiarism Type Recognition for Plagiarism Detection-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="32,142.24,393.93,328.81,7.77;32,150.95,404.89,323.67,7.77;32,150.95,415.85,70.22,7.77" xml:id="b26">
	<monogr>
		<title level="m" type="main" coord="32,411.49,393.93,59.56,7.77;32,150.95,404.89,323.67,7.77;32,150.95,415.85,21.92,7.77">Source Retrieval and Text Alignment Corpus Construction for Plagiarism Detection-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="32,142.24,426.81,321.98,7.77;32,150.95,437.77,287.81,7.77" xml:id="b27">
	<monogr>
		<title level="m" type="main" coord="32,313.79,426.81,150.43,7.77;32,150.95,437.77,235.03,7.77">Approaches for Source Retrieval and Text Alignment of Plagiarism Detection-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="32,142.24,448.73,338.35,7.77;32,150.95,459.69,322.98,7.77;32,150.95,470.65,33.36,7.77" xml:id="b28">
	<monogr>
		<title level="m" type="main" coord="32,347.01,448.73,133.58,7.77;32,150.95,459.69,322.98,7.77">Approaches for Candidate Document Retrieval and Detailed Comparison of Plagiarism Detection-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="32,142.24,481.60,325.74,7.77;32,150.95,492.56,85.59,7.77" xml:id="b29">
	<monogr>
		<title level="m" type="main" coord="32,241.36,481.60,226.62,7.77;32,150.95,492.56,49.99,7.77">A Set-Based Approach to Plagiarism Detection-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Küppers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Conrad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="32,142.24,503.52,333.68,7.77;32,150.95,514.48,308.53,7.77;32,150.95,525.44,294.63,7.77;32,150.95,536.40,321.10,7.77;32,150.95,547.36,23.90,7.77" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="32,260.15,503.52,106.12,7.77">Intrinsic Plagiarism Detection</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Meyer Zu Eißen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="32,370.07,514.48,89.42,7.77;32,150.95,525.44,229.08,7.77">Advances in Information Retrieval. 28th European Conference on IR Research (ECIR 06)</title>
		<title level="s" coord="32,385.82,525.44,59.76,7.77;32,150.95,536.40,64.47,7.77">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Lalmas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Macfarlane</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Rüger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Tombros</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Yavlinsky</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">3936</biblScope>
			<biblScope unit="page" from="565" to="569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="32,142.24,558.32,300.58,7.77;32,150.95,569.28,320.05,7.77;32,150.95,580.23,70.22,7.77" xml:id="b31">
	<monogr>
		<title level="m" type="main" coord="32,282.28,558.32,160.53,7.77;32,150.95,569.28,320.05,7.77;32,150.95,580.23,21.92,7.77">Developing Monolingual English Corpus for Plagiarism Detection using Human Annotated Paraphrase Corpus-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mohtaj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Asghari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Zarrabi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="32,142.24,591.19,320.29,7.77;32,150.95,602.15,227.18,7.77;32,150.95,613.11,301.45,7.77;32,150.95,624.07,87.52,7.77" xml:id="b32">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nourian</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
		<title level="m" coord="32,198.27,591.19,260.56,7.77">Submission to the 5th International Competition on Plagiarism Detection</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
		<respStmt>
			<orgName>University of Science and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="32,142.24,635.03,321.15,7.77;32,150.95,645.99,184.05,7.77;32,150.95,656.95,227.18,7.77;33,150.95,119.96,308.69,7.77;33,150.95,130.92,160.52,7.77" xml:id="b33">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Oberreuter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Carrillo-Cisneros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Scherson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Velásquez</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
		<title level="m" coord="32,385.41,635.03,77.97,7.77;32,150.95,645.99,180.34,7.77">Submission to the 4th International Competition on Plagiarism Detection</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
		<respStmt>
			<orgName>University of Chile, Chile, and the University of California</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="33,142.24,141.88,328.15,7.77;33,150.95,152.84,267.51,7.77;33,150.95,163.80,291.00,7.77" xml:id="b34">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Oberreuter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Eiselt</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
		<title level="m" coord="33,245.46,141.88,224.92,7.77;33,150.95,152.84,33.39,7.77">Submission to the 6th International Competition on Plagiarism Detection</title>
		<meeting><address><addrLine>Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="33,142.24,174.76,326.30,7.77;33,150.95,185.71,267.51,7.77;33,150.95,196.67,293.56,7.77;33,150.95,207.63,104.36,7.77" xml:id="b35">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Palkovskii</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Belov</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
		<title level="m" coord="33,243.62,174.76,224.92,7.77;33,150.95,185.71,33.39,7.77">Submission to the 7th International Competition on Plagiarism Detection</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
		<respStmt>
			<orgName>Zhytomyr State University and SkyLine LLC</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="33,142.24,218.59,334.10,7.77;33,150.95,229.55,322.20,7.77;33,150.95,240.51,304.01,7.77;33,150.95,251.47,327.13,7.77;33,150.95,262.43,120.28,7.77" xml:id="b36">
	<analytic>
		<title level="a" type="main" coord="33,209.34,218.59,267.00,7.77;33,150.95,229.55,70.51,7.77">Counter Plagiarism Detection Software&quot; and &quot;Counter Counter Plagiarism Detection&quot; Methods</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Palkovskii</surname></persName>
		</author>
		<idno>-WS.org</idno>
		<ptr target="http://ceur-ws.org/Vol-502" />
	</analytic>
	<monogr>
		<title level="m" coord="33,150.95,240.51,304.01,7.77;33,150.95,251.47,61.13,7.77">SEPLN 2009 Workshop on Uncovering Plagiarism, Authorship, and Social Software Misuse (PAN 09)</title>
		<title level="s" coord="33,258.22,251.47,166.58,7.77">Universidad Politécnica de Valencia and CEUR</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Stein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Rosso</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Koppel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Agirre</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2009-09">Sep 2009</date>
			<biblScope unit="page" from="67" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="33,142.24,273.39,306.85,7.77;33,150.95,284.34,270.26,7.77;33,150.95,295.30,179.72,7.77" xml:id="b37">
	<monogr>
		<title level="m" type="main" coord="33,243.62,273.39,205.47,7.77;33,150.95,284.34,270.26,7.77;33,150.95,295.30,144.12,7.77">Applying Specific Clusterization and Fingerprint Density Distribution with Genetic Algorithm Overall Tuning in External Plagiarism Detection-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Palkovskii</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Belov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="33,142.24,317.22,275.48,7.77;33,150.95,328.18,196.90,7.77" xml:id="b38">
	<monogr>
		<title level="m" type="main" coord="33,243.62,317.22,174.10,7.77;33,150.95,328.18,144.12,7.77">Using Hybrid Similarity Methods for Plagiarism Detection-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Palkovskii</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Belov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="33,142.24,339.14,316.61,7.77;33,150.95,350.10,228.85,7.77" xml:id="b39">
	<monogr>
		<title level="m" type="main" coord="33,243.62,339.14,215.23,7.77;33,150.95,350.10,180.55,7.77">Developing High-Resolution Universal Multi-Type N-Gram Plagiarism Detector-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Palkovskii</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Belov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="33,142.24,361.06,314.45,7.77;33,150.95,372.02,323.88,7.77;33,150.95,382.97,284.57,7.77;33,150.95,393.93,198.21,7.77" xml:id="b40">
	<analytic>
		<title level="a" type="main" coord="33,383.09,361.06,73.60,7.77;33,150.95,372.02,180.34,7.77">Overview of the 2nd International Competition on Plagiarism Detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Eiselt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
	</analytic>
	<monogr>
		<title level="m" coord="33,183.32,382.97,208.12,7.77">Working Notes Papers of the CLEF 2010 Evaluation Labs</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Harman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Pianta</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2010-09">Sep 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="33,142.24,404.89,302.11,7.77;33,150.95,415.85,294.35,7.77" xml:id="b41">
	<analytic>
		<title level="a" type="main" coord="33,344.98,404.89,99.37,7.77;33,150.95,415.85,33.39,7.77">Cross-Language Plagiarism Detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="33,191.28,415.85,162.13,7.77">Language Resources and Evaluation (LREV)</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="62" />
			<date type="published" when="2011-03">Mar 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="33,142.24,426.81,312.95,7.77;33,150.95,437.77,313.65,7.77;33,150.95,448.73,274.60,7.77;33,150.95,459.69,198.21,7.77" xml:id="b42">
	<analytic>
		<title level="a" type="main" coord="33,383.09,426.81,72.10,7.77;33,150.95,437.77,180.34,7.77">Overview of the 3rd International Competition on Plagiarism Detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Eiselt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
	</analytic>
	<monogr>
		<title level="m" coord="33,173.36,448.73,208.12,7.77">Working Notes Papers of the CLEF 2011 Evaluation Labs</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Petras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2011-09">Sep 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="33,142.24,470.65,329.94,7.77;33,150.95,481.60,310.40,7.77;33,150.95,492.56,282.41,7.77;33,150.95,503.52,322.05,7.77;33,150.95,514.48,223.61,7.77" xml:id="b43">
	<analytic>
		<title level="a" type="main" coord="33,389.75,481.60,71.61,7.77;33,150.95,492.56,180.34,7.77">Overview of the 4th International Competition on Plagiarism Detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Graßegger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Oberländer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tippmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
	</analytic>
	<monogr>
		<title level="m" coord="33,246.21,503.52,208.12,7.77">Working Notes Papers of the CLEF 2012 Evaluation Labs</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Karlgren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Womser-Hacker</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2012-09">Sep 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="33,142.24,525.44,324.55,7.77;33,150.95,536.40,306.96,7.77;33,150.95,547.36,326.84,7.77;33,150.95,558.32,261.96,7.77" xml:id="b44">
	<analytic>
		<title level="a" type="main" coord="33,186.83,536.40,254.19,7.77">Overview of the 5th International Competition on Plagiarism Detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tippmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
	</analytic>
	<monogr>
		<title level="m" coord="33,289.35,547.36,188.45,7.77;33,150.95,558.32,17.43,7.77">Working Notes Papers of the CLEF 2013 Evaluation Labs</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Navigli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Tufis</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2013-09">Sep 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="33,142.24,569.28,318.55,7.77;33,150.95,580.23,318.49,7.77;33,150.95,591.19,295.88,7.77;33,150.95,602.15,297.56,7.77;33,150.95,613.11,307.09,7.77;33,150.95,624.07,272.70,7.77" xml:id="b45">
	<analytic>
		<title level="a" type="main" coord="33,409.87,569.28,50.92,7.77;33,150.95,580.23,318.49,7.77;33,150.95,591.19,57.91,7.77">Improving the Reproducibility of PAN&apos;s Shared Tasks: Plagiarism Detection, Author Identification, and Author Profiling</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="33,255.81,602.15,192.71,7.77;33,150.95,613.11,307.09,7.77;33,150.95,624.07,37.80,7.77">Information Access Evaluation meets Multilinguality, Multimodality, and Visualization. 5th International Conference of the CLEF Initiative (CLEF 14)</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Lupu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hall</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Toms</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014-09">Sep 2014</date>
			<biblScope unit="page" from="268" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="33,142.24,635.03,305.05,7.77;33,150.95,645.99,326.63,7.77;33,150.95,656.95,290.32,7.77;34,150.95,119.96,313.94,7.77;34,150.95,130.92,198.21,7.77" xml:id="b46">
	<analytic>
		<title level="a" type="main" coord="33,150.95,645.99,254.19,7.77">Overview of the 6th International Competition on Plagiarism Detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Busse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tippmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
	</analytic>
	<monogr>
		<title level="m" coord="33,293.70,656.95,147.58,7.77;34,150.95,119.96,235.25,7.77">Working Notes Papers of the CLEF 2014 Evaluation Labs. CEUR Workshop Proceedings, CLEF and CEUR</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Halvey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2014-09">Sep 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,142.24,141.88,328.71,7.77;34,150.95,152.84,321.27,7.77;34,150.95,163.80,329.64,7.77;34,150.95,174.76,229.31,7.77" xml:id="b47">
	<analytic>
		<title level="a" type="main" coord="34,150.95,152.84,195.48,7.77">ChatNoir: A Search Engine for the ClueWeb09 Corpus</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Graßegger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tippmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Welsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="34,257.57,163.80,223.02,7.77;34,150.95,174.76,128.24,7.77">International ACM Conference on Research and Development in Information Retrieval (SIGIR 12)</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Hersh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Callan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Maarek</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012-08">Aug 2012</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">1004</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,142.24,185.71,305.28,7.77;34,150.95,196.67,316.03,7.77;34,150.95,207.63,306.23,7.77;34,150.95,218.59,243.58,7.77;34,150.95,229.55,157.21,7.77" xml:id="b48">
	<analytic>
		<title level="a" type="main" coord="34,321.97,185.71,125.54,7.77;34,150.95,196.67,132.36,7.77">Crowdsourcing Interaction Logs to Understand Text Reuse from the Web</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Völske</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P13-1119" />
	</analytic>
	<monogr>
		<title level="m" coord="34,400.25,196.67,66.74,7.77;34,150.95,207.63,289.23,7.77">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 13)</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Fung</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Poesio</surname></persName>
		</editor>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics (ACL 13)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-08">Aug 2013</date>
			<biblScope unit="page" from="1212" to="1221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,142.24,240.51,310.53,7.77;34,150.95,251.47,329.23,7.77;34,150.95,262.43,317.55,7.77;34,150.95,273.39,183.74,7.77" xml:id="b49">
	<analytic>
		<title level="a" type="main" coord="34,344.98,240.51,107.78,7.77;34,150.95,251.47,73.99,7.77">An Evaluation Framework for Plagiarism Detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="34,379.59,251.47,100.59,7.77;34,150.95,262.43,147.38,7.77">International Conference on Computational Linguistics (COLING 10)</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</editor>
		<meeting><address><addrLine>Stroudsburg, Pennsylvania</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010-08">Aug 2010</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="997" to="1005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,142.24,284.34,311.46,7.77;34,150.95,295.30,326.03,7.77;34,150.95,306.26,326.19,7.77;34,150.95,317.22,271.13,7.77;34,150.95,328.18,94.88,7.77" xml:id="b50">
	<analytic>
		<title level="a" type="main" coord="34,383.09,284.34,70.61,7.77;34,150.95,295.30,180.34,7.77">Overview of the 1st International Competition on Plagiarism Detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Eiselt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-502" />
	</analytic>
	<monogr>
		<title level="m" coord="34,256.47,306.26,220.67,7.77;34,150.95,317.22,135.49,7.77">SEPLN 09 Workshop on Uncovering Plagiarism, Authorship, and Social Software Misuse (PAN 09)</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Stein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Rosso</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Koppel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Agirre</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2009-09">Sep 2009</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,142.24,339.14,328.02,7.77;34,150.95,350.10,216.15,7.77" xml:id="b51">
	<monogr>
		<title level="m" type="main" coord="34,305.39,339.14,164.87,7.77;34,150.95,350.10,180.55,7.77">Detailed Comparison Module In CoReMo 1.9 Plagiarism Detector-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Rodríguez Torrejón</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Martín Ramos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,142.24,372.02,307.35,7.77;34,150.95,382.97,233.33,7.77" xml:id="b52">
	<monogr>
		<title level="m" type="main" coord="34,305.39,372.02,144.20,7.77;34,150.95,382.97,180.55,7.77">Text Alignment Module in CoReMo 2.1 Plagiarism Detector-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Rodríguez Torrejón</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Martín Ramos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,142.24,393.93,301.11,7.77;34,150.95,404.89,226.05,7.77" xml:id="b53">
	<monogr>
		<title level="m" type="main" coord="34,305.39,393.93,137.96,7.77;34,150.95,404.89,177.75,7.77">CoReMo 2.3 Plagiarism Detector Text Alignment Module-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Rodríguez Torrejón</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Martín Ramos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,142.24,415.85,306.89,7.77;34,150.95,426.81,291.61,7.77;34,150.95,437.77,66.49,7.77" xml:id="b54">
	<analytic>
		<title level="a" type="main" coord="34,252.44,415.85,196.69,7.77;34,150.95,426.81,76.69,7.77">Maps of Random Walks on Complex Networks Reveal Community Structure</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rosvall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bergstrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="34,234.39,426.81,179.79,7.77">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1118" to="1123" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,142.24,448.73,329.81,7.77;34,150.95,459.69,267.41,7.77" xml:id="b55">
	<monogr>
		<title level="m" type="main" coord="34,316.90,448.73,155.15,7.77;34,150.95,459.69,219.11,7.77">Dynamically Adjustable Approach through Obfuscation Type Recognition-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanchez-Perez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sidorov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,142.24,470.65,332.43,7.77;34,150.95,481.60,282.73,7.77" xml:id="b56">
	<monogr>
		<title level="m" type="main" coord="34,316.90,470.65,157.76,7.77;34,150.95,481.60,234.43,7.77">A Winning Approach to Text Alignment for Text Reuse Detection at PAN 2014-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanchez-Perez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sidorov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,142.24,492.56,338.30,7.77;34,150.95,503.52,233.02,7.77" xml:id="b57">
	<monogr>
		<title level="m" type="main" coord="34,360.01,492.56,120.53,7.77;34,150.95,503.52,197.41,7.77">Optimized Fuzzy Text Alignment for Plagiarism Detection-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Sánchez-Vega</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Villaseñor-Pineda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,142.24,525.44,331.39,7.77;34,150.95,536.40,267.51,7.77;34,150.95,547.36,311.33,7.77" xml:id="b58">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Saremi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Yaghmaee</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
		<title level="m" coord="34,248.70,525.44,224.92,7.77;34,150.95,536.40,33.39,7.77">Submission to the 5th International Competition on Plagiarism Detection</title>
		<meeting><address><addrLine>Iran</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
		<respStmt>
			<orgName>From Semnan University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="34,142.24,558.32,319.79,7.77;34,150.95,569.28,195.92,7.77" xml:id="b59">
	<monogr>
		<title level="m" type="main" coord="34,290.26,558.32,171.77,7.77;34,150.95,569.28,147.61,7.77">Machine Translation Evaluation Metric for Text Alignment-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Maharjan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Solorio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,142.24,580.23,337.54,7.77;34,150.95,591.19,200.40,7.77" xml:id="b60">
	<monogr>
		<title level="m" type="main" coord="34,239.71,580.23,240.07,7.77;34,150.95,591.19,147.61,7.77">Using a Variety of n-Grams for the Detection of Different Kinds of Plagiarism-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Solorio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,142.24,602.15,328.37,7.77;34,150.95,613.11,285.36,7.77;34,150.95,624.07,307.09,7.77;34,150.95,635.03,223.39,7.77" xml:id="b61">
	<analytic>
		<title level="a" type="main" coord="34,368.94,602.15,101.67,7.77;34,150.95,613.11,73.18,7.77">Overview of the PAN/CLEF 2015 Evaluation Lab</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="34,243.61,613.11,192.71,7.77;34,150.95,624.07,307.09,7.77;34,150.95,635.03,37.80,7.77">Information Access Evaluation meets Multilinguality, Multimodality, and Visualization. 6th International Conference of the CLEF Initiative (CLEF 15)</title>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015-09">Sep 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="34,142.24,645.99,326.37,7.77;34,150.95,656.95,170.85,7.77" xml:id="b62">
	<analytic>
		<title level="a" type="main" coord="34,284.58,645.99,102.61,7.77">Intrinsic Plagiarism Analysis</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Lipka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="34,394.16,645.99,74.45,7.77;34,150.95,656.95,78.97,7.77">Language Resources and Evaluation (LRE)</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="82" />
			<date type="published" when="2011-03">Mar 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="35,142.24,119.96,304.93,7.77;35,150.95,130.92,316.76,7.77;35,150.95,141.88,315.29,7.77;35,150.95,152.84,307.82,7.77;35,150.95,163.80,317.31,7.77;35,150.95,174.76,62.53,7.77" xml:id="b63">
	<analytic>
		<title level="a" type="main" coord="35,260.15,119.96,170.09,7.77">Near Similarity Search and Plagiarism Analysis</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Meyer Zu Eißen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="35,413.93,130.92,53.79,7.77;35,150.95,141.88,315.29,7.77;35,150.95,152.84,217.09,7.77;35,423.15,152.84,35.62,7.77;35,150.95,163.80,211.97,7.77">Selected papers from the 29th Annual Conference of the German Classification Society (GFKL 05)</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Spiliopoulou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Kruse</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Borgelt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Nürnberger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Gaul</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="430" to="437" />
		</imprint>
	</monogr>
	<note>Studies in Classification, Data Analysis, and Knowledge Organization</note>
</biblStruct>

<biblStruct coords="35,142.24,185.71,321.02,7.77;35,150.95,196.67,308.97,7.77;35,150.95,207.63,57.53,7.77" xml:id="b64">
	<monogr>
		<title level="m" type="main" coord="35,321.30,185.71,141.96,7.77;35,150.95,196.67,308.97,7.77;35,150.95,207.63,21.92,7.77">Three Way Search Engine Queries with Multi-feature Document Comparison for Plagiarism Detection-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Šimon</forename><surname>Suchomel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kasprzak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brandejs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="35,142.24,218.59,336.60,7.77;35,150.95,229.55,251.92,7.77" xml:id="b65">
	<monogr>
		<title level="m" type="main" coord="35,321.30,218.59,157.54,7.77;35,150.95,229.55,199.13,7.77">Diverse Queries and Feature Type Selection for Plagiarism Discovery-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Šimon</forename><surname>Suchomel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kasprzak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brandejs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="35,142.24,240.51,324.19,7.77;35,150.95,251.47,316.22,7.77;35,150.95,262.43,42.57,7.77" xml:id="b66">
	<monogr>
		<title level="m" type="main" coord="35,376.02,240.51,90.41,7.77;35,150.95,251.47,316.22,7.77">Evaluation of Text Reuse Corpora for Text Alignment Task of Plagiarism Detection-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Zarrabi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rafiei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Khoshnava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Asghari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mohtaj</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
