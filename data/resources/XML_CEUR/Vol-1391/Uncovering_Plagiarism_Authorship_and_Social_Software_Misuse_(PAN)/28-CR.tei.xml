<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,211.34,169.42,172.66,12.00">Notebook for PAN at CLEF 2015</title>
				<funder ref="#_cXEYw42">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,238.46,207.74,57.87,8.95"><forename type="first">Mirco</forename><surname>Kocher</surname></persName>
							<email>mirco.kocher@unine.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">University of NeuchÃ¢tel</orgName>
								<address>
									<addrLine>rue ; Emile Argand</addrLine>
									<postCode>11 2000</postCode>
									<settlement>NeuchÃ¢tel</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,306.22,207.74,61.94,8.95"><forename type="first">Jacques</forename><surname>Savoy</surname></persName>
							<email>jacques.savoy@unine.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">University of NeuchÃ¢tel</orgName>
								<address>
									<addrLine>rue ; Emile Argand</addrLine>
									<postCode>11 2000</postCode>
									<settlement>NeuchÃ¢tel</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,211.34,169.42,172.66,12.00">Notebook for PAN at CLEF 2015</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E87F882F8532E78BA3879098E08367EA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes and evaluates an unsupervised authorship verification model called SPATIUM-L1. The suggested strategy can be adapted without any problem to different languages (such as Dutch, English, Greek, and Spanish) with their genre and topic differ significantly. As features, we suggest using the k most frequent terms of the disputed text (isolated words and punctuation symbols with k may vary from 200 to 300). Applying a simple distance measure and a set of impostors, we determine whether or not the disputed text was written by the proposed author. Moreover, based on a simple rule, we can define when there is enough evidence to propose an answer with a high degree of confidence or when the attribution scheme is given without certainty. The evaluations are based on four test collections (PAN</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.38" lry="841.98"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.38" lry="841.98"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.38" lry="841.98"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.38" lry="841.98"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.38" lry="841.98"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.38" lry="841.98"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.38" lry="841.98"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.38" lry="841.98"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic authorship identification aims to determine, as accurately as possible, if the proposed author of a document or a text excerpt is the real one <ref type="bibr" coords="1,403.35,480.17,10.64,9.71" target="#b8">[9]</ref>. To achieve this, a sample of texts written by the proposed author and each of the possible impostors must be available. The verification problem knows some interesting historical questions such as "are all the Letters of Paul written by the same person?", "is President L. Johnson the real author of the 1964 State of the Union Address (just weeks after the assassination of Kennedy)?", or "is Madison the true author of the 12 disputed Federalist Papers?". With the Web 2.0 technologies, the number of anonymous or pseudonymous texts is increasing and in many cases we face a single possible author (e.g., is John the real author of this blog post or tweet?). Therefore, proposing an effective algorithm to the verification problem presents a real interest. A justification supporting the proposed answer and a probability that the given answer is correct can be given to improve the confidence attached to the response <ref type="bibr" coords="1,414.10,606.67,10.64,9.71" target="#b5">[6]</ref>.</p><p>This authorship verification question seems simpler than the classical authorship attribution problem, but it is not. For example, if we want to know if a newly discovered poem was really written by Shakespeare <ref type="bibr" coords="1,351.36,641.19,15.36,9.71" target="#b10">[11]</ref>, the computer needs to compare a model based on Shakespeare's texts with all other possible representative non-Shakespeare models. This second part is hard to generate. Are we sure we have included all other writers having a style similar to Shakespeare? This paper is organized as follows. The next section presents the test collections and the evaluation methodology used in the experiments. The third section explains our proposed algorithm called SPATIUM-L1. In the last section, we evaluate the proposed scheme and compare it to the best performing schemes using four different test collections. A conclusion draws the main findings of this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Test Collections and Evaluation Methodology</head><p>The experiments supporting previous studies were usually limited to one language, one author, and one or a few texts. For real cases, this limitation makes sense; for example we have only one newly discovered poem that might be attributed to Shakespeare <ref type="bibr" coords="2,190.35,295.78,15.36,9.71" target="#b10">[11]</ref>. To evaluate the effectiveness of a verification algorithm, the number of tests must however be larger. To create such benchmarks, and to promote studies in this domain, the PAN CLEF evaluation campaign was launched <ref type="bibr" coords="2,165.50,330.27,15.36,9.71" target="#b9">[10]</ref>. The evaluation was performed using the TIRA platform, which is an automated tool for deployment and evaluation of the software <ref type="bibr" coords="2,411.84,341.77,10.64,9.71" target="#b1">[2]</ref>. The data access is restricted such that during a software run the system is encapsulated and thus ensuring that there is no data leakage back to the task participants <ref type="bibr" coords="2,426.08,364.76,10.64,9.71" target="#b4">[5]</ref>. This evaluation procedure may raises some difficulties (possible system compatibilities) but offers also a fair evaluation of the time needed to produce an answer.</p><p>During the PAN CLEF 2015 evaluation campaign, four test collections were built, each containing at least 200 problems (training + testing). In each collection, all the texts matched the same language but can be cross-topic or cross-genre and may differ significantly. In this context, a problem is defined as: Given a small set of "known" documents (no more than seven, possibly as few as one) written by a single person, is the new "unknown" document also written by that author? The four benchmarks are composed of a Dutch and Spanish cross-genre collection and an English and Greek cross-topic corpus. An overview of these collections is depicted in Table <ref type="table" coords="2,196.46,513.96,3.76,9.11" target="#tab_0">1</ref>. The training set will be used to evaluate our approach and the test set will be used in order to be able to compare our results with those of the The number of problems is given under the label "No of Problems". The mean number of known documents for each problem is indicated in the column "Mean document", and the mean number of words per known document under the label "Mean words". The mean number of words in the unknown documents is close to the latter.</p><p>The last two metrics are not available for the test corpora because the datasets remained undisclosed thanks to the TIRA system.</p><p>When inspecting the English training collection, the number of words available is rather small (in mean 341 words for each document, and exactly one document per problem). Similarly the Dutch collection only provides 808 words in mean per problem (in mean 449 words for each document, and 1.8 documents in each problem). This collection has mostly one or two document per problem but also some with 5, 6 or even 7 known documents. Therefore, we can expect the mean performance for these languages to be lower than for the other languages under the assumption that all languages present the same level of complexity to solve this problem. For the Spanish corpus we have always four documents and rather long ones to learn the stylistic features of the proposed author. A relatively higher performance can be assumed with this benchmark. A similar conclusion can be expected with the Greek collection consisting of longer documents (in mean, 1,995 words).</p><p>When considering the four benchmarks as a whole, we have 865 problems to solve and 400 to train (pre-evaluate) our system. When inspecting the distribution of the correct answers, we can find the same number (432 in test and 200 in training) as positive or negative answers. In each of the individual test collections, we can also find a balanced number of positive and negative answers.</p><p>During the PAN CLEF 2015 campaign, a system must return a value between 0.0 and 1.0 for each problem with a precision down to a thousandth. A value strictly larger than 0.5 indicates that the query text was written by the proposed author and a value strictly lower than 0.5 the opposite. Returning the value 0.5 indicates that the system is unable to take a decision based on the given information. Of course, a value closer to 1.0 (or to 0.0) is a stronger evidence in favor of (or against) a positive answer.</p><p>As performance measure, two evaluation measures were used during the PAN CLEF campaign. The first performance measure is the AUC (Area Under the Curve) of the ROC (Receiver Operating Characteristic) curve <ref type="bibr" coords="3,336.22,460.46,15.36,9.71" target="#b11">[12]</ref>. This curve is generated according to the percentage of false positives (or false alarms) in the x-axis and the percentage of true positives in the y-axis over the entire test set. The maximum value of 1.0 indicates a perfect performance. Both the ROC and the AUC measures are, however, rather complex and difficult to interpret by a final user.</p><p>As another measure, the PAN CLEF campaign adopts the c@1 measure <ref type="bibr" coords="3,432.66,517.96,10.64,9.71" target="#b3">[4]</ref>. This evaluation measure takes into account both the number of correct answers and the number of problems left unsolved in the test set. The exact formulation is given in Equation 1 with a minimal value of 0.0 and 1.0 as an optimum value.</p><formula xml:id="formula_0" coords="3,167.30,565.60,300.60,17.94">ğ‘@1 = ğ‘›ğ‘ ğ‘›ğ‘ â€¢ (1 + ğ‘›ğ‘¢ ğ‘›ğ‘ )<label>(1)</label></formula><p>in which np is the number of problems, nc the number of correct answers, and nu the number of problems left without an answer. This measure differentiates between an incorrect answer and the absence of an answer (indicating that the provided evidence is not enough to take a definitive decision) <ref type="bibr" coords="3,297.84,626.35,15.36,9.71" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Simple Verification Algorithm</head><p>To solve the verification problem, we suggest an unsupervised approach based on a simple feature extraction and distance metric called SPATIUM-L1 (Latin word meaning distance). The selected stylistic features correspond to the top k most frequent terms (isolated words without stemming but with the punctuation symbols). For determining the value of k, previous studies have shown that a value between 200 and 300 tends to provide the best performance <ref type="bibr" coords="4,263.87,235.26,10.85,9.71" target="#b0">[1,</ref><ref type="bibr" coords="4,278.87,235.26,7.23,9.71" target="#b6">7]</ref>. Some unknown documents were rather short and we further excluded the words only appearing once in the text. This filtering decision was taken to prevent overfitting to single occurrences. The effective number of terms k was set to at most 200 terms but was in most cases well below. With this reduced number the justification of the decision will be simpler to understand because it will be based on words instead of letters, bigrams of letters or combinations of several representation schemes or distance measures.</p><p>In the current study, a verification problem is defined as a query text, denoted Q, and a set of texts (between 1 and 7) written by the same proposed author. The concatenation of these texts forms the author profile A. To measure the distance between Q and A, SPATIUM-L1 uses the L1-norm as follows:</p><formula xml:id="formula_1" coords="4,167.30,363.08,300.60,13.38">âˆ†(ğ‘„, ğ´) = âˆ† 0 = âˆ‘ |ğ‘ƒ ğ‘„ [ğ‘¡ ğ‘– ] -ğ‘ƒ ğ´ [ğ‘¡ ğ‘– ]| ğ‘˜ ğ‘–=1<label>(2)</label></formula><p>where k indicates the number of terms (words or punctuation symbols), and P Q [t i ] and P A [t i ] represent the estimated occurrence probability of the term t i in the query text Q and in the author profile A respectively. To estimate these probabilities, we divide the term occurrence frequency (tf i ) by the length in tokens of the corresponding text (n), Prob[t i ] = tf i / n, without smoothing and therefore accepting a 0.0 probability. To verify whether the resulting âˆ† 0 value is small or rather large, we need to select a set of impostors. To achieve this, three profiles from other problems in the test set were chosen randomly with preference to candidates that show the same number of known documents. This value of three is arbitrary and will be denoted by the variable m. After computing the distance between Q and each of these m profiles, we retain only the smallest distance.</p><p>Instead of limiting the number of possible impostors to m, we iterate this last stage r times, and we suggest to fix the value r = 5. After this last step, we have r values denoted âˆ† ğ‘š1 , â€¦, âˆ† ğ‘šğ‘Ÿ , each of them corresponding to the minimum value of a set of m impostors. Instead of working with r values, we compute the arithmetic mean, denoted âˆ† ğ‘š , of the sample âˆ† ğ‘š1 , â€¦, âˆ† ğ‘šğ‘Ÿ .</p><p>Finally, the decision rule is based on the value of the ratio âˆ† 0 / âˆ† ğ‘š as follows:</p><formula xml:id="formula_2" coords="4,167.30,581.92,300.60,50.89">{ ğ‘–ğ‘“ âˆ† 0 âˆ† ğ‘š &lt; 0.975 ğ‘ ğ‘ğ‘šğ‘’ ğ‘ğ‘¢ğ‘¡â„ğ‘œğ‘Ÿ ğ‘–ğ‘“ âˆ† 0 âˆ† ğ‘š &gt; 1.025 ğ‘‘ğ‘–ğ‘“ğ‘“ğ‘’ğ‘Ÿğ‘’ğ‘›ğ‘¡ ğ‘ğ‘¢ğ‘¡â„ğ‘œğ‘Ÿ ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’ ğ‘‘ğ‘œğ‘› â€² ğ‘¡ ğ‘˜ğ‘›ğ‘œğ‘¤<label>(3)</label></formula><p>Thus when the âˆ† 0 value is similar to âˆ† ğ‘š (in the range Â±2.5%), the system specifies that the solution of this problem cannot be determined with good certainty and provides the answer don't know. On the other hand, when âˆ† 0 is small compared to âˆ† ğ‘š , the evidence is in favor of assuming that the author of the profile A is the real author.</p><p>Finally, when âˆ† ğ‘š is small compared to âˆ† 0 , we conclude that Q and A are written by different authors. The limit of two times 2.5% was chosen arbitrarily but corresponds to a well-known limit value in statistical tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>Since our system is based on an unsupervised approach we were able to directly evaluate it using the training set. In Table <ref type="table" coords="5,298.77,249.70,3.76,9.11" target="#tab_1">2</ref>, we have reported the same performance measure applied during the PAN CLEF campaign, namely the final score, which is the product of the AUC and the c@1.</p><p>Language The algorithm returns the best results for the Spanish collection with a final score of 0.5694 closely followed by the Greek corpus possibly due to the fact of the longer and numerous documents in these two languages. The worst result is achieved with the Dutch collection that shows a highly diverging number of known documents per problem. Usually the AUC values should be consistent and comparable with the c@1 values but in some cases the AUC values are a lot higher than the c@1 values (mainly in the Spanish collection but also observable in the English and Greek corpus). As possible reason we saw a few misclassifications that have very high probability scores. The AUC of the ROC is biased in a way that the ROC gives more emphasis on the first position and therefore increases the total AUC. A misclassification with a lower probability is less penalized in this performance measure.</p><formula xml:id="formula_3" coords="5,229.52,287.96,22.27,8.95">Final</formula><p>Due to the fact that our algorithm is based on a probabilistic approach (random selection of candidates) the results in Table <ref type="table" coords="5,305.05,500.16,5.01,9.11" target="#tab_1">2</ref> may vary between runs. To verify the impact of this selection in the reported performance measures, Table <ref type="table" coords="5,419.73,511.68,5.01,9.11">3</ref> shows the standard deviation and the estimated confidence interval covering 95% of the cases for the c@1 value based on 200 restarts with random impostor selection.</p><p>c  <ref type="table" coords="5,175.88,621.82,3.76,8.95">3</ref>. Variation around the c@1 performance for the SPATIUM-L1 system We can see the possible variation around the reported performance is noticeable but relatively small. Similarly when changing the values of the two numbers m (number of impostors) and r (number of iterations) then the difference of the best possible combination of the two parameters to the performance reported in Table <ref type="table" coords="5,437.32,670.70,5.01,9.11" target="#tab_1">2</ref> is not significant.</p><p>The test set is then used to rank the performance of all 18 participants in this task. Based on the same evaluation methodology, we achieve the results depicted in Table <ref type="table" coords="6,465.60,161.14,5.01,9.11" target="#tab_3">4</ref> corresponding to the 865 problems present in the four test corpora.</p><p>As we can see the final score with the Greek corpus is as high as expected from the training set. The results we achieved in the Dutch collection is as low as in the training set. On the other hand the English results are better than anticipated and the Spanish score is worse than the estimation from the training set. It seems like the system performs better on the two cross-topic corpora (English and Greek) than on the two cross-genre corpora (Dutch and Spanish).</p><p>Language  <ref type="table" coords="6,170.18,524.80,3.76,8.95">5</ref>. Evaluation over all four test collections using macro-averaging for the effectiveness measures and the sum for the runtimes. Another pertinent observation is the fast runtime of our system in comparison with other solutions <ref type="foot" coords="6,184.34,561.92,3.00,5.45" target="#foot_0">1</ref> . The median execution time of the other systems is almost one hour. Also the runtime only shows the actual time spent to classify the test set. On TIRA there was the possibility to first train the system using the training set which had no influence on the final runtime. Since we have an unsupervised system it did not need to train any parameters, but this possibility might have been used by other participants.</p><formula xml:id="formula_4" coords="6,202.28,256.94,22.27,8.95">Final</formula><p>In text categorization studies, we are convinced that a deeper analysis of the evaluation results is important to obtain a better understanding of the advantages and drawbacks of a suggested scheme. By just focusing on overall performance measures, we only observe a general behavior or trend without being able to acquire a better explanation of the proposed assignment. To achieve this deeper understanding, we could analyze some problems extracted from the English corpus. Usually, the relative frequency (or probability) differences with very frequent words such as when, is, in, that, to, or it can explain the decision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper proposes a simple unsupervised technique to solve the authorship verification problem. As features to discriminate between the proposed author and different impostors, we propose using the top 200 most frequent terms (words and punctuations). This choice was found effective for other related tasks such as authorship attribution <ref type="bibr" coords="7,233.06,295.76,10.64,9.71" target="#b0">[1]</ref>. Moreover, compared to various feature selection strategies used in text categorization <ref type="bibr" coords="7,281.50,307.26,10.64,9.71" target="#b7">[8]</ref>, the most frequent terms tend to select the most discriminative features when applied to stylistic studies <ref type="bibr" coords="7,373.59,318.75,10.64,9.71" target="#b6">[7]</ref>. In order to take the attribution decision, we propose using a simple distance metric called SPATIUM-L1 based on the L1 norm.</p><p>The proposed approach tends to perform very well in two different languages (English and Greek) on cross-topic collections and well in a Spanish cross-genre corpus. Such a classifier strategy can be described as having a high bias but a low variance <ref type="bibr" coords="7,181.27,387.77,10.64,9.71" target="#b2">[3]</ref>. Even if the proposed system cannot capture all possible stylistic features (bias), changing the available data does not modify significantly the overall performance (variance).</p><p>It is common to fix some parameters (such as time period, size, genre, or length of the data) to minimize the possible source of variation in the corpus. However, our goal was to present a simple and unsupervised approach without many predefined arguments. This turned out to not work well on the Dutch cross-genre corpus. We suspect this to be mostly related to the genre variation than the language itself. SPATIUM-L1 returns a numerical value (between 0 and 1) that can be used to determine a degree of certainty <ref type="bibr" coords="7,295.00,491.26,10.64,9.71" target="#b5">[6]</ref>.</p><p>More importantly, the proposed attribution could be clearly explained because it is based on a reduced set of features on the one hand and, on the other, those features are words or punctuation symbols. Thus the interpretation for the final user is clearer than when working with a huge number of features, when dealing with n-grams of letters or when combing several similarity measures. The SPATIUM-L1 decision can be explained by large differences in relative frequencies of frequent words, usually corresponding to functional terms.</p><p>To improve the current classifier, we will investigate the consequence of some smoothing techniques, the effect of other distance measures, and different feature selection strategies. In the latter case, we want to maintain a reduced number of terms. In a better feature selection scheme, we can take account of the underlying text genre, as for example, the most frequent use of personal pronouns in narrative texts. As another possible improvement, we can ignore specific topical terms or character names appearing frequently in an author profile, terms that can be selected in the feature set without being useful in discriminating between authors. We might also try to exploit PAN specific properties such as the requirement for equally distributed positive and negative problems. In case our system decides in over half the cases for (or against) a verification we could assign for the least certain part that it is a don't know answer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,124.76,525.48,345.86,119.09"><head>Table 1 .</head><label>1</label><figDesc>PAN CLEF 2015 corpora statistics</figDesc><table coords="2,423.78,525.48,46.84,9.11"><row><cell>PAN CLEF</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,174.56,287.68,244.92,69.11"><head>Table 2 .</head><label>2</label><figDesc>Evaluation for the four training collections</figDesc><table coords="5,174.56,287.68,244.92,57.23"><row><cell></cell><cell>AUC</cell><cell>c@1</cell><cell>Runtime (h:m:s)</cell></row><row><cell>Dutch</cell><cell cols="2">0.2161 0.4738 0.4560</cell><cell>00:00:08</cell></row><row><cell>English</cell><cell cols="2">0.3450 0.6032 0.5720</cell><cell>00:00:07</cell></row><row><cell>Greek</cell><cell cols="2">0.5415 0.7648 0.7080</cell><cell>00:00:12</cell></row><row><cell>Spanish</cell><cell cols="2">0.5694 0.8320 0.6844</cell><cell>00:00:12</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,124.76,256.66,345.90,277.09"><head>Table 4 .</head><label>4</label><figDesc>Evaluation for the four testing collectionsTo put those values in perspective we can see in Table5our result in comparison with the other 17 participants using macro-averaging. We have also added a baseline corresponding to a system that always produces the answer yes (trivial acceptor). The bad performance in the Dutch collection clearly worsens our overall results.</figDesc><table coords="6,137.00,256.66,320.06,277.09"><row><cell></cell><cell></cell><cell></cell><cell>AUC</cell><cell>c@1</cell><cell cols="2">Runtime (h:m:s)</cell><cell>Position</cell></row><row><cell cols="2">Dutch</cell><cell cols="4">0.2175 0.4495 0.4840</cell><cell>00:00:07</cell><cell>14</cell></row><row><cell cols="2">English</cell><cell cols="4">0.5082 0.7375 0.6890</cell><cell>00:00:24</cell><cell>4</cell></row><row><cell cols="2">Greek</cell><cell cols="4">0.6310 0.8216 0.7680</cell><cell>00:00:11</cell><cell>3</cell></row><row><cell cols="2">Spanish</cell><cell cols="4">0.3665 0.6498 0.5640</cell><cell>00:00:22</cell><cell>10</cell></row><row><cell>Rank</cell><cell></cell><cell>Run</cell><cell cols="2">Final</cell><cell>AUC</cell><cell>c@1</cell><cell>Runtime (h:m:s)</cell></row><row><cell>1</cell><cell cols="2">bagnall15</cell><cell cols="4">0.6340 0.8199 0.7663</cell><cell>55:14:16</cell></row><row><cell>2</cell><cell cols="2">moreau15</cell><cell cols="4">0.6103 0.8186 0.7409</cell><cell>55:24:10</cell></row><row><cell>3</cell><cell cols="2">pacheco15</cell><cell cols="4">0.5606 0.8164 0.6833</cell><cell>00:26:31</cell></row><row><cell>4</cell><cell cols="2">nissim15</cell><cell cols="4">0.5416 0.7457 0.7221</cell><cell>00:04:53</cell></row><row><cell>5</cell><cell cols="2">bartoli15</cell><cell cols="4">0.5182 0.7398 0.6837</cell><cell>00:44:35</cell></row><row><cell>6</cell><cell cols="2">mezaruiz15</cell><cell cols="4">0.4829 0.7218 0.6621</cell><cell>02:10:28</cell></row><row><cell>7</cell><cell cols="2">halvani15</cell><cell cols="4">0.4618 0.7354 0.6282</cell><cell>00:01:01</cell></row><row><cell>8</cell><cell cols="2">kocher15</cell><cell cols="4">0.4308 0.6646 0.6263</cell><cell>00:01:04</cell></row><row><cell cols="2">â€¦ â€¦</cell><cell></cell><cell>â€¦</cell><cell></cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell></row><row><cell cols="3">13 Baseline (yes)</cell><cell cols="4">0.2500 0.5000 0.5000</cell><cell>00:00:00</cell></row><row><cell cols="2">â€¦ â€¦</cell><cell></cell><cell>â€¦</cell><cell></cell><cell>â€¦</cell><cell>â€¦</cell><cell>â€¦</cell></row><row><cell cols="2">Table</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="6,130.04,686.22,168.96,8.18"><p>http://www.tira.io/task/authorship-verification/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The author wants to thank the task coordinators for their valuable effort to promote test collections in authorship attribution. This research was supported, in part, by the <rs type="funder">NSF</rs> under Grant #<rs type="grantNumber">200021_149665/1</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_cXEYw42">
					<idno type="grant-number">200021_149665/1</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,139.86,315.05,3.76,9.71;8,160.76,315.05,301.78,9.71;8,124.76,326.57,301.01,9.71" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,246.98,315.05,215.56,9.71;8,124.76,326.57,84.52,9.71">Delta: A Measure of Stylistic Difference and a Guide to Likely Authorship</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Burrows</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,218.48,326.57,137.60,9.71">Literary and Linguistic Computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="267" to="287" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,139.86,338.09,3.76,9.71;8,160.76,338.09,303.00,9.71;8,124.76,349.61,330.29,9.71;8,124.76,361.13,336.18,9.71;8,124.76,372.65,47.60,9.71" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,337.36,338.09,126.40,9.71;8,124.76,349.61,269.66,9.71">Ousting Ivory Tower Research: Towards a Web Framework for Providing Experiments as a Service</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Burrows</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,313.15,361.13,141.87,9.71">SIGIR. The 35th International ACM</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Hersh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Callan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Maarek</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1125" to="1126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,139.86,384.17,3.76,9.71;8,160.76,384.17,303.57,9.71;8,124.76,395.69,322.65,9.71;8,124.76,407.21,23.65,9.71" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<title level="m" coord="8,357.38,384.17,106.95,9.71;8,124.76,395.69,201.44,9.71">The Elements of Statistical Learning. Data Mining, Inference, and Prediction</title>
		<meeting><address><addrLine>New York (NY</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,139.86,418.73,3.76,9.71;8,160.76,418.73,302.45,9.71;8,124.76,430.25,155.22,9.71" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,293.80,418.73,164.84,9.71">A Single Measure to Assess Nonresponse</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>PeÃ±as</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rodrigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,137.98,430.25,88.43,9.71">Proceedings 49th ACL</title>
		<meeting>49th ACL</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1415" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,139.86,441.77,3.76,9.71;8,160.76,441.77,296.15,9.71;8,124.76,453.29,340.07,9.71;8,124.76,464.81,335.42,9.71;8,124.76,476.33,330.83,9.71;8,124.76,487.85,197.80,9.71" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,149.74,453.29,315.09,9.71;8,124.76,464.81,171.78,9.71">Improving the Reproducibility of PAN&apos;s Shared Tasks: -Plagiarism Detection, Author Identification, and Author Profiling</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="8,360.06,476.33,95.53,9.71;8,124.76,487.85,71.48,9.71">CLEF. Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Lupu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hall</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Handbury</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Toms</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">8685</biblScope>
			<biblScope unit="page" from="268" to="299" />
			<date type="published" when="2014">2014</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,139.86,499.37,3.76,9.71;8,160.76,499.37,297.86,9.71;8,124.76,510.89,317.85,9.71" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,233.89,499.37,220.82,9.71">Estimating the Probability of an Authorship Attribution</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,124.76,510.89,271.50,9.71">Journal of American Society for Information Science &amp; Technology</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="8,139.86,522.41,3.76,9.71;8,160.76,522.41,299.57,9.71;8,124.76,533.93,295.98,9.71;8,124.76,545.45,127.47,9.71" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,232.01,522.41,228.31,9.71;8,124.76,533.93,93.08,9.71">Comparative Evaluation of Term Selection Functions for Authorship Attribution</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
		<idno>doi.org/10.1093</idno>
		<ptr target="/llc/fqt047" />
	</analytic>
	<monogr>
		<title level="j" coord="8,226.63,533.93,150.32,9.71">Digital Scholarship in the Humanities</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,139.86,556.97,3.76,9.71;8,160.76,556.97,299.84,9.71;8,124.76,568.49,150.28,9.71" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,247.25,556.97,209.25,9.71">Machine Learning in Automatic Text Categorization</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,124.76,568.49,96.07,9.71">ACM Computing Survey</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,139.86,580.01,3.76,9.71;8,160.76,580.01,304.88,9.71;8,124.76,591.53,322.09,9.71;8,124.76,603.05,35.90,9.71" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,248.15,580.01,212.80,9.71">A Survey of Modern Authorship Attribution Methods</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,124.76,591.53,290.97,9.71">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="433" to="214" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,144.45,614.57,316.96,9.71;8,124.76,626.09,337.08,9.71;8,124.76,637.61,345.56,9.71;8,124.76,649.13,118.49,9.71" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,255.59,626.09,206.25,9.71">Overview of the Author Identification Task at PAN</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Daelemans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Verhoeven</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Juola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lopez Lopez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="8,162.96,637.61,231.31,9.71">Working Notes Papers of the CLEF 2015 Evaluation Labs</title>
		<title level="s" coord="8,401.09,637.61,69.23,9.71;8,124.76,649.13,76.18,9.71">CEUR Workshop Proceedings, CEUR</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,144.45,660.65,322.67,9.71;8,124.76,672.17,144.71,9.71" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,289.34,660.65,177.78,9.71;8,124.76,672.17,27.27,9.71">Did Shakespeare Write a Newly-Discovered Poem?</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Thisted</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,156.90,672.17,43.27,9.71">Biometrika</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="445" to="456" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,144.45,683.69,292.31,9.71;8,124.76,695.21,329.65,9.71;8,124.76,706.73,25.32,9.71" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Hall</surname></persName>
		</author>
		<title level="m" coord="8,341.23,683.69,95.53,9.71;8,124.76,695.21,162.61,9.71">Data Mining. Practical Machine Learning Tools and Techniques</title>
		<meeting><address><addrLine>Burlington (MA)</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>3rd ed.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
