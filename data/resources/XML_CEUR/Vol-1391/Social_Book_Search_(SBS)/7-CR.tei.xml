<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,195.96,115.96,223.42,12.62">Multimodal Social Book Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,172.28,153.63,62.82,8.74"><forename type="first">Melanie</forename><surname>Imhof</surname></persName>
							<email>imhf@zhaw.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">Université de Neuchâtel</orgName>
								<address>
									<settlement>Neuchâtel</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Zurich University of Applied Sciences</orgName>
								<address>
									<settlement>Winterthur</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,251.99,153.63,66.34,8.74"><forename type="first">Ismail</forename><surname>Badache</surname></persName>
							<email>badache@irit.fr</email>
							<affiliation key="aff2">
								<orgName type="institution">IRIT -Paul Sabatier University</orgName>
								<address>
									<settlement>Toulouse</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,348.26,153.63,90.35,8.74"><forename type="first">Mohand</forename><surname>Boughanem</surname></persName>
							<email>boughanem@irit.fr</email>
							<affiliation key="aff2">
								<orgName type="institution">IRIT -Paul Sabatier University</orgName>
								<address>
									<settlement>Toulouse</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,195.96,115.96,223.42,12.62">Multimodal Social Book Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">41FF7D293ABF7C8753A322CC17DF1E38</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Relevance feedback</term>
					<term>random forest</term>
					<term>non-textual modalities</term>
					<term>social signals</term>
					<term>document prior</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Today's information retrieval applications have become increasingly complex. The Social Book Search (SBS) lab at CLEF 2015 allows evaluating retrieval methods on a complex search task with several textual and non-textual meta-data fields. The challenge is to incorporate the different information types (modalities) into a single ranked list. We build a strong textual baseline and combine it with a document prior based on social signals. Further, we include non-textual modalities in relation to the user preferences using random forest learning to rank. Our experiments show that both the social document prior and the learning to rank approach improve the search results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The suggestion track of the INEX Social Book Search (SBS) lab at CLEF 2015 challenges researchers to find methods to retrieve books as requested by real users of LibraryThing. The complex collection consists of more than 50 metadata fields of real books from Amazon. Thus, the retrieval methods can not rely on the content of the books but only on meta-data such as product descriptions, user-generated reviews and ratings. The lab's evaluation metric nDCG@10 reflects the user behavior that in such an application only the first few "recommendations" are considered. Hence, to maximize the number of relevant books in the first few results both the textual description of the user's query and the user's profile including his personal catalog matter. For such a complex task with that many information types, methods are required to handle and fuse them into a single ranked list. Analogously to multimedia retrieval, we call these different information types "modalities". Hence, our goal in this complex task was to fuse a strong textual baseline approach with several non-textual and social modalities that respect the user preferences. Therefore, we established and refined a textual baseline using traditional information retrieval weighting schemes, blind relevance feedback, user-profile based filtering and example book based relevance feedback. We enhanced this with document priors based on social signals such as the ratings and tags. Finally, we applied a random forest learning that further improves the results by including the non-textual modalities price and number of pages with respect to the user preferences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Collection and Data</head><p>The SBS collection consists of 2.8 million book records from Amazon, extended with social meta-data from LibraryThing. Each book record is an XML file with fields like isbn, title, review, summary, rating and tag. The full list of fields is shown in Table <ref type="table" coords="2,204.01,243.47,3.87,8.74" target="#tab_0">1</ref>. There are 208 topics in the SBS 2015 lab. Each topic is a query that was posted on LibraryThing for a list of books and consists of five fields: title, mediated query, narrative, example and group. Hereby, the narrative is the textual description of the query from which a hand-crafted mediated query is derived. Further, the example field contains a list of books that the user has mentioned as positive or negative examples. Additionally, the personal LibraryThing catalog of each topic creator is available, which includes a list of the books the user has archived on LibraryThing along with his personal ratings.</p><p>The relevance assessments are based on the actual suggestions to the original query on the LibraryThing forum. The relevance values are weighted using a decision tree that includes reliability information such as whether the user who suggested a book has read it. The SBS 2015 topics are a subset of the topics used in 2014. However, the relevance assessments have been extended with additional book suggestions that have not been included in 2014.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Retrieval Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Textual Models</head><p>As a basis for our methods we employ a textual baseline using a traditional information retrieval system. Therefore, we merge all textual fields of the document into a single textual index field. Further, we construct queries from the three topic fields title, mediated query and narrative that are analogously merged into a single textual representation.</p><p>We extend the textual baseline with a query expansion (blind relevance feedback) based on Rocchio's method <ref type="bibr" coords="3,282.04,239.95,9.96,8.74" target="#b3">[4]</ref>. Therefore, the n most characteristic terms of the m top-ranked documents are added to the query. Hereby, the most characteristic terms of a document are chosen by the term weight determined by the weighting scheme.</p><p>As described in Section 2 the topics contain example books mentioned by the topic creators. We use the contents of the example books that are associated with a positive or neutral sentiment to expand the queries similar to the blind relevance feedback.</p><p>Additionally, we filter the books already read by the topic creator from the final ranked list, since this is a hard criterion in the relevance assessments <ref type="bibr" coords="3,467.31,348.53,9.96,8.74" target="#b1">[2]</ref>. Hereby, we determine the read books from the catalog of the topic creator as well as from the example books that are marked as read.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Social Signals-Based Model</head><p>Our approach consists of exploiting social data as a priori knowledge to take into account in the retrieval model. We combine textual relevance of a given document to a query and its social importance modeled as a prior probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Preliminaries</head><p>The social information that we exploit within the framework of our model can be represented by 3-tuple &lt; U, D, A &gt; where U, D and A are finite sets of instances Users, Documents and Actions.</p><p>Documents. We consider a collection C ={D 1 , D 2 ,...D n } of n documents, where each document D represents a book. We assume that a book can be represented by both a set of textual keywords D w ={w 1 , w 2 ,...w y } and a set of social actions A performed on the book, D a ={a 1 , a 2 ,...a z }.</p><p>Actions. We consider a set A={a 1 , a 2 ,...a m } of m types of actions (signals) that users can perform on the documents. These actions represent the relation between users U ={u 1 , u 2 ,...u h } and documents C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Social Document Prior</head><p>We exploit textual models to estimate the relevance of a document to a query. Our approach combines the social document prior P (D) and the relevance status value RSV textual (Q, D) between a query Q and document D as</p><formula xml:id="formula_0" coords="4,205.79,139.55,274.81,11.88">RSV (D, Q) rank = P (D) • RSV textual (Q, D)<label>(1)</label></formula><formula xml:id="formula_1" coords="4,259.48,157.59,221.11,22.29">rank = P (D) • wi∈Q RSV textual (w i , D),<label>(2)</label></formula><p>where w i represents the terms in the query Q and RSV textual (w i , D) can be estimated with different models such as BM25 and language model. The document prior P (D) is a query-independent probability of seeing the document. It is useful for representing and incorporating other sources of evidence to the retrieval process. Our main contribution is a method to estimate P (D) by exploiting social signals.</p><p>According to our previous approach <ref type="bibr" coords="4,311.58,265.98,9.96,8.74" target="#b0">[1]</ref>, the priors are estimated by simply counting the number of actions performed on the documents. We assume that the signals are independent. Thus the general formula for calculating P (D) is</p><formula xml:id="formula_2" coords="4,265.32,315.55,215.27,20.06">P (D) = ai∈A P (a i ),<label>(3)</label></formula><p>where P (a i ) is estimated using maximum-likelihood. It is calculated as</p><formula xml:id="formula_3" coords="4,256.63,367.38,223.96,23.23">P (a i ) = log(1 + |D ai |) log(1 + |D a |) ,<label>(4)</label></formula><p>where |D ai | is the number of actions of type a i on document D and |D a | is the total number of actions on document D. Further, we use Dirichlet to smooth P (a i ) by collection C to avoid zero probabilities. This leads to</p><formula xml:id="formula_4" coords="4,208.21,445.51,272.38,26.80">P (D) = ai∈A log(1 + |D ai |) + µ • P (a i |C) log(1 + |D a |) + µ ,<label>(5)</label></formula><p>where P (a i |C), analogously to P (a i ), is estimated using maximum-likelihood.</p><formula xml:id="formula_5" coords="4,237.23,504.08,243.36,25.38">P (a i |C) = log(1 + D∈C |D ai |) log(1 + D∈C |D a |)<label>(6)</label></formula><p>In addition to considering social features separately as described above, we propose to incorporate the ratings as a measurement of the popularity and the reputation of a book. For this purpose, we use the Bayesian average (BA) of the ratings as a document prior, which takes into account how many users have rated a book. As more users rate the same book, the average becomes more reliable and less sensitive to outliers. Books that have many ratings are boosted with respect to books that have little ratings and books with high ratings are boosted more than books with low ratings. Hereby, the BA of a book is computed as</p><formula xml:id="formula_6" coords="4,198.27,643.35,278.08,25.38">BA(D) = avg(D r ) • |D r | + D ∈C avg(D r ) • |D r | |D r | + D ∈C |D r | , (<label>7</label></formula><formula xml:id="formula_7" coords="4,476.35,650.75,4.24,8.74">)</formula><p>where avg is the average function and D r is the set of ratings of document D.</p><p>We note that considering logarithmic priors helps to compress the score range and thereby reduces the impact of the priors on the global score.</p><formula xml:id="formula_8" coords="5,229.06,163.33,251.53,24.72">P BA (D) = log(1 + BA(D)) log(1 + D ∈C BA(D ))<label>(8)</label></formula><p>For books with no ratings this would result in a prior probability of zero. In order to avoid a multiplication by zero and thus ignoring the textual score, we use the Add-One smoothing method:</p><formula xml:id="formula_9" coords="5,219.10,236.45,257.25,24.72">P BA (D) = 1 + log(1 + BA(D)) 1 + log(1 + D ∈C BA(D )) . (<label>9</label></formula><formula xml:id="formula_10" coords="5,476.35,243.19,4.24,8.74">)</formula><p>3.3 Learning to Rank (Random Forests)</p><p>Besides the textual modalities, the SBS collection contains several non-textual modalities. We use random forests <ref type="bibr" coords="5,294.14,305.98,10.52,8.74" target="#b2">[3]</ref> to learn how to combine not only the different textual runs but also the non-textual modalities into a single ranked list. In particular, we use the price and number of pages of a book with respect to the user's preference as well as the book's ratings. Hereby, the user's preference is estimated by the average of the attributes in the topic creator's catalog; e.g. a user that only has short books in his catalog prefers short books. We assume that a user prefers to retrieve books that have similar attributes as the books he has read in the past. To achieve this, we add the difference between the average of the book prices in the topics creator's catalog and the price of the book to the random forest algorithm as an additional feature. Similarly, we add such a feature for the number of pages. For the ratings we assume that a general preference towards higher rated books exists for all users. Thus, we add the absolute average rating of a book as an additional feature to the random forests.</p><p>To allow the algorithm to incorporate the significance of the average rating, we also add the number of ratings as a separate feature. The ratings are the ratings of the reviews of the book as well as the ratings in the catalogs of all topic creators. In order to combine these ratings, we divide the ratings in the catalogs by two, so that all ratings are in the same range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Evaluation</head><p>We evaluated our approaches based on a series of experiments on the SBS 2015 task. Our goals in these experiments are to evaluate whether social signals (tags and rating) and other non-textual modalities can improve the search results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>For the textual baseline we used Lucene 4 for indexing and searching. We used the EnglishAnalyzer, which removes a small set of stopwords and stems terms using the Porter stemming algorithm. The weighting scheme used for most of the official runs is BM25 with b = 0.75 and k 1 = 1.2. We have also ran some experiments using language model with Dirichlet smoothing with µ = 2500, however, we found that the BM25 achieved a better mean average precision (MAP) and nDCG@10 for the textual baseline. In order to validate the effectiveness of our approaches we used the topics and relevance assessments from SBS 2014.</p><p>For the blind relevance feedback, we experimented with the number of topranked documents used for the relevance feedback as well as with the number of terms extracted. However, we found that none of the combinations improve the textual baseline.</p><p>Since the topics from SBS 2015 are a subset of the topics from 2014, we were able to automatically add the example books from the 2015 topics to the corresponding topics in 2014. We found that expanding the queries with 35 terms extracted from the example books maximizes the nDCG@10 on the topics from 2014. Since we only have the example books for about 30% of the 2014 topics, the overall performance gain was not very big, however we have seen that the performance for the topics with example books has increased significantly.</p><p>Lucene does not provide a filter implementation that allows rejecting a list of documents, which is required to filter the read books. Thus we implemented our own filter with a similar concept as the Lucene's FieldCacheTermsFilter, which rejects all the documents that are not in the given list of documents.</p><p>As described in Section 3.2, we integrated social signals into the traditional textual model by re-ranking the results. The social signals are modeled as an a priori probability P (D). We ran different experiments using all available social signals on the SBS collection (ratings, totalvotes, helpfulvotes, tags, etc.), but we found that the signals tags and ratings, estimated based on the formulas 5 and 9, achieved a better MAP and nDCG@10 compared to the other signals. We conducted our experiments in two ways: for Run3 and Run4 we multiplied P (D) by the textual language model score; for Run5 and Run6, we combined the social signals score (P (tags) multiplied by P BA (D)) linearly with Run1, respectively with random forests trained with 100 trees. We set the smoothing parameter µ of formula 5 to 200, although more experiments will be necessary to get the best parameter. Experiments showed that the best combination parameter γ for the social score is 0.25 for Run5 and 0.2 for Run6.</p><p>We used RankLib 5 to train the random forests. For all the experiments, we left the default parameters unchanged except for the number of trees and the train metric which was set to nDCG@10. Unsurprisingly, increasing the number of trees results in a longer computation time, but also higher nDCG@10 values when training and testing on the SBS 2014 topics. However, with a higher number of trees the risk of over-fitting the data increases. The input for the random forests was built from the top 500 documents of six different textual runs together with the three non-textual modalities as described in Section 3.3. The textual runs were the textual baseline, the textual baseline with the read book filter, the textual baseline plus example based relevance feedback with and without filtering the read books and two runs using blind relevance feedback (total of 80 terms from 10 documents and total of 40 terms from 5 documents). Even though the blind relevance feedback runs on their own did not improve the textual baseline, we decided to add two runs using different parameters to the random forest in order to increase the variance of the input ranked lists.</p><p>As training data we used the SBS 2014 topics and relevance assessments with the example books added from the 2015 topics. This is not an ideal situation, since the training data and the test data have an overlap. However, since we do not have example books for all the 2014 topics, we were not able to exclude the topics which are also in 2015 without losing the benefit of our example based relevance feedback.</p><p>For our participation to INEX SBS 2015 track, we built six runs by applying different configurations:</p><p>-Run1: Textual baseline using BM25 with example based relevance feedback using 35 terms and read book filtering. -Run2: Random forests trained with 10 trees based on six textual runs and three non-textual modalities (price, number of pages and ratings). -Run3: Run1 using language model combined with Bayesian average reranking based on ratings. -Run4: Run1 using language model combined with re-ranking based on the tags. -Run5: Run1 combined with re-ranking based on the tags and Bayesian average of ratings. -Run6: Random forests trained with 100 trees based on six textual runs and three non-textual modalities (price, number of pages and ratings) combined with re-ranking based on the tags and Bayesian average of ratings.</p><p>In the next section we discuss the evaluation results of our official submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results and Discussion</head><p>Table <ref type="table" coords="7,162.42,500.65,4.98,8.74" target="#tab_1">2</ref> summarizes our official results of SBS 2015 evaluated using nDCG@10 (Normalized Discounted Cumulative Gain), MRR (Mean Reciprocal Rank), MAP (Mean Average Precision) and R@1000 (Recall), whereas nDCG@10 is the official evaluation measure. We can see that the runs (Run2 and Run6) using random forest training far exceed the effectiveness of the runs using no training. During our experiments we saw that including the three non-textual modalities in the learning helps to increase the nDCG@10, which means that these modalities contain relevant information regarding the book suggestions.</p><p>Our textual baseline, although not submitted, achieves an nDCG@10 of 0.0768. Thus, the filtering together with the example based relevance feedback (Run1) significantly improves the nDCG@10 by 6.7% with a significance level of 58.4% calculated using the significance paired randomization test <ref type="bibr" coords="8,422.80,214.86,9.96,8.74" target="#b4">[5]</ref>.</p><p>According to our experiments, Run3 and Run4 improve Run1 with language model (nDCG@10 of 0.0834) significantly (significance level α = 18.4%, respectively α = 15.3%). Using both the ratings and the tags (Run5) improves the effectiveness more than just using one of them. We note that the Run3 provides slightly better results in terms of MRR and MAP compared to Run4. One of the reasons of this is that the signal (rating) for Run3 that quantifies the reputation may be seen as expressing the engagement of a user who provides his explicit endorsement. For example, the document having more positive signals (ratings, likes, etc.) are more trustworthy than the ones that do not possess these social signals. If multiple users have found that the document is useful, then it is more likely that other users will find this document useful too. The social signals that quantify the popularity (number of reviews, tags, etc.) do not represent approval votes, as for example the reviews can be positive or negative, but they represent trend factors and a measure of information propagation. Therefore, a popular information always arouses the interest of the user.</p><p>The R@1000 is approximately the same for all runs, since they mostly are based on a re-ranking of Run1, for which we only retrieved the top 1000 documents. Since the learning based runs only used slight variations of Run1, they do not retrieve additional relevant documents beyond the top 1000 documents of Run1. For a recall-centric application, using a higher variety of runs as well as more documents per run would be beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we described our participation to the suggestion track of the INEX SBS 2015 lab. We showed how to build a textual baseline and how to improve this using blind relevance feedback as well as example book based relevance feedback. Further, we proposed a method to include the social signals as a priori social knowledge that further enhanced the effectiveness of our system. The learning based approach using random forests, allowed us to incorporate the user preferences with respect to the book price and the number of pages as well as to combine the best aspects of the different variations of our textual methods.</p><p>So far, we did not use the anonymized user profiles from LibraryThing which would allow us to add additional ratings to the social model. Also we would like to test our learning approach with completely separated training and test datasets. Hence, we need to extract the example books for all the topics of SBS 2014. As a long term goal however, we think it is important to find methods that do not rely on learning. Although it might help to develop these by investigating the output of the random forests in order to better understand the modalities including their importance and their dependencies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,182.23,274.81,250.88,193.44"><head>Table 1 .</head><label>1</label><figDesc>A list of all element names in the book descriptions.</figDesc><table coords="2,182.51,306.54,250.34,161.71"><row><cell></cell><cell cols="2">tag name</cell><cell></cell></row><row><cell>book</cell><cell>similarproducts</cell><cell>title</cell><cell>imagecategory</cell></row><row><cell>dimensions</cell><cell>tags</cell><cell>edition</cell><cell>name</cell></row><row><cell>reviews</cell><cell>isbn</cell><cell>dewey</cell><cell>role</cell></row><row><cell>editorialreviews</cell><cell>ean</cell><cell>creator</cell><cell>blurber</cell></row><row><cell>images</cell><cell>binding</cell><cell>review</cell><cell>dedication</cell></row><row><cell>creators</cell><cell>label</cell><cell>rating</cell><cell>epigraph</cell></row><row><cell>blurbers</cell><cell>listprice</cell><cell>authorid</cell><cell>firstwordsitem</cell></row><row><cell>dedications</cell><cell>manufacturer</cell><cell>totalvotes</cell><cell>lastwordsitem</cell></row><row><cell>epigraphs</cell><cell cols="2">numberofpages helpfulvotes</cell><cell>quotation</cell></row><row><cell>firstwords</cell><cell>publisher</cell><cell>date</cell><cell>seriesitem</cell></row><row><cell>lastwords</cell><cell>height</cell><cell>summary</cell><cell>award</cell></row><row><cell>quotations</cell><cell>width</cell><cell cols="2">editorialreview browseNode</cell></row><row><cell>series</cell><cell>length</cell><cell>content</cell><cell>character</cell></row><row><cell>awards</cell><cell>weight</cell><cell>source</cell><cell>place</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,138.03,566.16,339.29,94.81"><head>Table 2 .</head><label>2</label><figDesc>Official results at SBS 2015. The runs are ranked according to nDCG@10.</figDesc><table coords="7,207.30,586.96,200.76,74.02"><row><cell cols="4">Rank Run nDCG@10 MRR MAP R@1000 Train</cell></row><row><cell>1 Run6</cell><cell>0.186</cell><cell cols="2">0.394 0.105 0.374 yes</cell></row><row><cell>3 Run2</cell><cell>0.130</cell><cell cols="2">0.290 0.074 0.374 yes</cell></row><row><cell>8 Run5</cell><cell>0.095</cell><cell>0.235 0.062 0.374</cell><cell>no</cell></row><row><cell>10 Run3</cell><cell>0.094</cell><cell>0.237 0.062 0.374</cell><cell>no</cell></row><row><cell>11 Run4</cell><cell>0.094</cell><cell>0.232 0.061 0.375</cell><cell>no</cell></row><row><cell>21 Run1</cell><cell>0.082</cell><cell>0.189 0.054 0.375</cell><cell>no</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="5,144.73,656.80,128.61,7.86"><p>https://lucene.apache.org/core/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="6,144.73,656.80,191.60,7.86"><p>http://sourceforge.net/p/lemur/wiki/RankLib/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,138.35,210.33,342.24,7.86;9,146.91,221.29,333.68,7.86;9,146.91,232.25,182.03,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,283.35,210.33,197.24,7.86;9,146.91,221.29,23.76,7.86">Social priors to estimate relevance of a resource</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Badache</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Boughanem</surname></persName>
		</author>
		<idno type="DOI">10.1145/2637002.2637016</idno>
		<idno>IIiX&apos;14</idno>
		<ptr target="http://doi.acm.org/10.1145/2637002.2637016" />
	</analytic>
	<monogr>
		<title level="m" coord="9,198.76,221.29,65.54,7.86">IIiX Conference</title>
		<meeting><address><addrLine>NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="106" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,243.21,342.24,7.86;9,146.91,254.17,333.68,7.86;9,146.91,265.13,76.80,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,396.02,243.21,84.57,7.86;9,146.91,254.17,119.10,7.86">Overview of the inex 2014 social book search track</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Bogers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koolen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Jaap</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kazai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Preminger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,288.12,254.17,187.68,7.86">Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="462" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,276.09,276.88,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,200.70,276.09,61.96,7.86">Random forests</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,269.30,276.09,69.14,7.86">Machine learning</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,287.05,342.24,7.86;9,146.91,298.01,333.67,7.86;9,146.91,308.96,133.42,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,202.75,287.05,169.50,7.86">Relevance feedback in information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Rocchio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,391.07,287.05,89.52,7.86;9,146.91,298.01,234.16,7.86">The SMART Retrieval System: Experiments in Automatic Document Processing</title>
		<meeting><address><addrLine>Englewood Cliffs NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1971">1971</date>
			<biblScope unit="page" from="313" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,319.92,342.24,7.86;9,146.91,330.88,333.68,7.86;9,146.91,341.84,333.68,7.86;9,146.91,352.80,177.92,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,320.13,319.92,160.46,7.86;9,146.91,330.88,158.30,7.86">A comparison of statistical significance tests for information retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,324.70,330.88,155.89,7.86;9,146.91,341.84,311.92,7.86">CIKM &apos;07: Proceedings of the sixteenth ACM conference on Conference on information and knowledge management</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="623" to="632" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
