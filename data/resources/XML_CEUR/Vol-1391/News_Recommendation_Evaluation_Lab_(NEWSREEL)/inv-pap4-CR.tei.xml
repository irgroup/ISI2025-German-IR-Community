<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,177.44,115.96,260.47,12.62;1,168.53,133.89,278.29,12.62">Overview of CLEF NewsREEL 2015: News Recommendation Evaluation Lab</title>
				<funder ref="#_yTubeut">
					<orgName type="full">German Federal Ministry for Economic Affairs and Energy</orgName>
				</funder>
				<funder ref="#_3x7gRST">
					<orgName type="full">European Unions Seventh Framework Programme</orgName>
				</funder>
				<funder ref="#_3QnW4Zt">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,143.53,171.56,65.45,8.74"><forename type="first">Benjamin</forename><surname>Kille</surname></persName>
							<email>benjamin.kille@dai-labor.de</email>
							<affiliation key="aff0">
								<orgName type="institution">TU Berlin</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,219.53,171.56,93.82,8.74"><forename type="first">Andreas</forename><surname>Lommatzsch</surname></persName>
							<email>andreas.lommatzsch@dai-labor.de</email>
							<affiliation key="aff0">
								<orgName type="institution">TU Berlin</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,323.91,171.56,66.64,8.74"><forename type="first">Roberto</forename><surname>Turrin</surname></persName>
							<email>roberto.turrin@moviri.com</email>
							<affiliation key="aff1">
								<orgName type="institution">ContentWise R&amp;D -Moviri</orgName>
								<address>
									<settlement>Milan</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,401.10,171.56,63.49,8.74"><forename type="first">András</forename><surname>Serény</surname></persName>
							<email>sereny.andras@gravityrd.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Gravity R&amp;D</orgName>
								<address>
									<settlement>Budapest</settlement>
									<country key="HU">Hungary</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,150.79,183.51,65.28,8.74"><forename type="first">Martha</forename><surname>Larson</surname></persName>
							<email>m.a.larson@tudelft.nl</email>
							<affiliation key="aff3">
								<orgName type="institution">TU Delft</orgName>
								<address>
									<settlement>Delft</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,226.63,183.51,59.97,8.74"><forename type="first">Torben</forename><surname>Brodt</surname></persName>
							<email>torben.brodt@plista.com</email>
							<affiliation key="aff4">
								<orgName type="institution">Plista GmbH</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,297.16,183.51,51.70,8.74"><forename type="first">Jonas</forename><surname>Seiler</surname></persName>
							<email>jonas.seiler@plista.com</email>
							<affiliation key="aff4">
								<orgName type="institution">Plista GmbH</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,378.79,183.51,81.30,8.74"><forename type="first">Frank</forename><surname>Hopfgartner</surname></persName>
							<email>frank.hopfgartner@glasgow.ac.uk</email>
							<affiliation key="aff5">
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<settlement>Glasgow</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,177.44,115.96,260.47,12.62;1,168.53,133.89,278.29,12.62">Overview of CLEF NewsREEL 2015: News Recommendation Evaluation Lab</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4A68E1459267DF5AF3AD4119AAFC9209</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>news recommendation</term>
					<term>recommender systems</term>
					<term>evaluation</term>
					<term>living lab</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>News reader struggle as they face ever increasing numbers of articles. Digital news portals are becoming more and more popular. They route news items to visitors as soon as they are published. The rapid rate at which new news is published gives rise to a selection problem, since the capacity of new portal videos to absorb news is limited. To address this problem, new portals deploy news recommender systems in order to support their visitors in selecting items to read. This paper summarizes the settings and results of CLEF NewsREEL 2015. The lab challenged participants to compete in either a "living lab" (Task 1) or an evaluation that replayed recorded streams (Task 2). The goal was to create an algorithm that was able to generate news items that users would click, respecting a strict time constraint.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>News recommendation continues to draw the attention of researchers. Last year's edition of CLEF NewsREEL <ref type="bibr" coords="1,259.58,584.39,10.52,8.74" target="#b3">[4]</ref> introduced the Open Recommendation Platform (ORP) operated by plista. ORP provides an interface to researchers interested in news recommendation algorithms. They can easily plug in their algorithms and receive requests from various news publishers. Subsequently, the systems records recipients' reaction. This feedback allows participants to improve their algorithms. In contrast to traditional offline evaluation, this "living lab" approach reflects the application setting of an actual news recommender system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Lab Setup</head><p>CLEF NewsREEL'15 consisted of two subtasks. Task 1 was a repetition of the online evaluation task ("Task 2") of NewsREEL'14. In Section 2.1, we briefly introduce the recommendation use case of this task. For a more detailed overview, the reader is referred to <ref type="bibr" coords="2,243.32,644.16,9.96,8.74" target="#b3">[4]</ref>. Section 2.2 introduces the second subtask that focuses on simulating constant data streams, hence allowing evaluation of real-time recommenders using an offline data set. For a more detailed overview of this use case, we refer to <ref type="bibr" coords="3,208.49,130.95,9.96,8.74" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task 1: Benchmark News Recommendations in a Living Lab</head><p>This task implements the idea of evaluation in a living lab. As such, participants were given the chance to directly interact with a real-time recommender system. After registering with The Open Recommendation Platform (ORP) <ref type="bibr" coords="3,470.08,205.02,10.52,8.74" target="#b0">[1]</ref> provided by plista GmbH, participants receive recommendation requests from various websites offering news articles. Requests were triggered by users visiting those websites.</p><p>The task followed the idea of providing evaluation as a service <ref type="bibr" coords="3,430.88,252.87,9.96,8.74" target="#b2">[3]</ref>. Participants had access to a virtual machine where they could install their algorithm. The recommender system forwarded the incoming requests to a random virtual machine which produced the recommendation to be delivered to the requester. The random choice was uniformly distributed over all participants. Alternatively, participants could set up their own server to respond to incoming requests.</p><p>As a fixed response time limitation was set, the participants experienced typical restrictions for real-world recommender systems. Such restrictions pose requirements regarding scalability and computational complexity for the recommendation algorithms.</p><p>ORP monitored the performance of all participants during the challenge duration by measuring the recommenders' click through rate (CTR). CTR represents the ratio of clicks by requests. Participants had the chance to continuously update their parameter settings in order to improve their performance levels. Results were published on a regular basis to allow participants to compare their performance with respect to baseline and competing approaches. An overview of the results is given by Kille et al. <ref type="bibr" coords="3,294.08,444.23,9.96,8.74" target="#b5">[6]</ref>, and also in this paper in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Task 2: Benchmarking News Recommendations in a Simulated Environment</head><p>For the second task, we employed the benchmarking framework Idomaar<ref type="foot" coords="3,454.26,504.77,3.97,6.12" target="#foot_0">7</ref> that makes it possible to simulate data streams by "replaying" a recorded stream. The framework is being developed in the CrowdRec project <ref type="foot" coords="3,375.22,528.68,3.97,6.12" target="#foot_1">8</ref> It makes it possible to execute and test the proposed news recommendation algorithms, independently of the execution framework and the language used for the development. Participants of this task had to predict users clicks on recommended news articles in simulated real-time. The proposed algorithms were evaluated against both functional (i.e., recommendation quality) and non-functional (i.e., response time) metrics. The data set used for this task consists of news updates from diverse news publishers, user interactions and clicks on recommendations. An overview of the features of the data set is provided by Kille et al. <ref type="bibr" coords="3,381.57,625.89,9.96,8.74" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>In this section, we detail results of CLEF NewsREEL 2015. We start by giving some statistics about the participation in general. Then, we discuss the results for both tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Participation</head><p>Forty-two teams registered for CLEF NewsREEL 2015. Of these teams, 38 teams expressed interest in both tasks. A single participant registered for Task 2 only.</p><p>Three teams wanted to focus on Task 1. Participating teams distribute across the world including all continents except Australia. ORP's operators, plista, provided five virtual machines to participants who were located far from Berlin, Germany. Without these machines participants would have faced issues with network latency, already discussed above.</p><p>Nine teams actively competed in Task 1. The competition's schedule consisted of three evaluation time frames: 17-23 March, 7-13 April, and 5 May to 2 June 2015. Seven out of nine teams competed in all three periods. Team "irit-imt" stopped competing after the second period. Team "university of essex" entered the competition as the final period started. Each team could operate several recommendation services. Each recommendation service obtained a similar volume of requests if active for similar times. We received a submission describing the idea and results of team "cwi" <ref type="bibr" coords="4,304.55,393.05,9.96,8.74" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Baselines</head><p>Within the evaluation, we sought to obtain comparable results. Baselines allow us to determine how well a participant performs relative to a very basic approach. In last year's edition of NewsREEL, we established the baseline discussed in <ref type="bibr" coords="4,467.31,476.79,9.96,8.74" target="#b3">[4]</ref>. This baseline allocates an array of fixed length for item references. As we observe visitors interacting with the news portal, we put item references into the array. As we receive a recommendation request, we reversely iterate the array returning the first item references that are unknown to the target user. In this way, the baseline considers both freshness and popularity. We operated the baseline on two machines, "riemannzeta" and "gaussiannoise", which represented two different levels of machine power. The team "riemannzeta" administered a virtual machine with a dual-core Intel Xeon X7560 @ 2.27 GHz, 2 GB of RAM, and 8 GB hard drive. The team "gaussiannoise" operated a more powerful virtual machine with a quad-core Intel Xeon X7550 @ 2.0 GHz, 8 GB of RAM, and 26 GB hard drive. We released the baseline approach in form of a tutorial. Participants could take advantage of the baseline. Additionally, we sought to establish comparability with respect to last year's winner. Last year's winning approach has been documented in <ref type="bibr" coords="4,202.51,644.16,9.96,8.74" target="#b6">[7]</ref>. The approach competed as "abc" and in a slightly adjusted version as "artificial intelligence", also described in <ref type="bibr" coords="4,360.33,656.12,9.96,8.74" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task 1</head><p>We observed nine teams actively participating throughout CLEF News-REEL 2015. We recorded the performance of participants during three periods: 17-23 March, 7-13 April, and 5 May -2 June 2015. The former two periods span a week each; the latter amounts to four weeks of data. The schedule had intentional gaps between the periods allowing participants to improve their algorithms. Table <ref type="table" coords="5,207.52,200.06,4.98,8.74">1</ref> summarizes the performances on team level. Each team has the number of requests (R), number of clicks (C), and their proportion (C/R) assigned for each of the three periods. Fields with 'n/a' refer to lack of participation. The highest average CTR per time slot is typeset in bold face. We observe that these values increased as the competition progressed. This indicates that teams managed to improve their recommendation algorithms over time. In addition, this could signal that teams learned to adjust their systems better to the challenge's requirements. Team "irit-imt" received 44 clicks at 5597 requests leading to the highest CTR (0.79 %) in the time slot from 17-23 March. Team "abc" received 56 clicks at 6483 requests obtaining a CTR of 0.86 % surpassing all competitors in the time slot from 7-13 April. Team "artificial intelligence" collected 302 clicks at 23 756 requests resulting in a CTR of 1.27 % in the final four week time slot.</p><p>Table <ref type="table" coords="5,164.25,385.59,4.13,7.89">1</ref>. We present the results of nine participating teams. Each participant could operate several algorithms simultaneously. Results are aggregated over all algorithms. The evaluation includes three periods. We report the number of clicks (C), requests (R), and their relation (C/R). We highlight the highest CTR for each interval by bold typeface. 17-23 March 7-13 April 5 May -2 June Each participant could simultaneously operate several recommendation engines. Some participants took advantage of this offer. Consequently, those teams accumulated considerably more requests than others. Figure <ref type="figure" coords="5,394.39,632.21,4.98,8.74">1</ref> illustrates the performance of individual algorithms. We present performance on a plane defined by the number of clicks and requests. A point on this plane refers to a specific CTR. Points' colors refer to the respective team. The teams "cwi" and "riadigdl" deployed several algorithms. Two lines depict two CTR levels. A drawn through line marks the 1.0 % level. A dashed line represents the 0.5 % level. The illustration confirms that teams "abc" and "artificial intelligence" outperformed their competitors. Fig. <ref type="figure" coords="6,154.40,385.57,4.13,7.89">1</ref>. Team were eligible to run several algorithms simultaneously. We observe some teams operating various recommenders. Teams "abc" and "artificial intelligence" managed to achieve a CTR of more than 1 %.</p><formula xml:id="formula_0" coords="5,136.16,464.34,344.21,7.89">Team C R C/R C R C/R C R C/R</formula><p>We investigate how individual algorithms perform over time. Figure <ref type="figure" coords="6,455.99,440.92,4.98,8.74">2</ref> displays 16 algorithms' CTR relative to the average CTR over the final evaluation period's 28 days. Areas below 0 indicate a CTR lower than the average CTR of that day. Areas above 0 represent days with above average CTR. First, we observe that only a subset of algorithms ran throughout the period. Algorithms A, C, E, and K operated only scarcely. Algorithms F ("artificial intelligence") and J ("abc") managed to perform above the average CTR on almost all days. The majority of algorithms' CTR fluctuates around the system's average CTR. This confirms the difficulty inherent to news recommendation. The choice of an algorithm may depend on factors which are subject to change.</p><p>The competition featured a variety of news publishers. Some provide general as well as regional news. Other news portals specialize on topics such as sports or information technology. Figure <ref type="figure" coords="6,330.20,584.39,4.98,8.74">3</ref> relates 16 competing algorithms with four major publishers. Publishers "418" (www.ksta.de) and "1677" (www. tagesspiegel.de) provide general and regional news. Publisher "35774" (www. sport1.de) targets sport-related news stories. Publisher "694" (www.gulli.com) presents information technology news. Combined, they account for ≈ 85 % of recommendation requests. The heatmap illustrates higher CTR with darker shades. CTR ranges up to 2.5 % for some combinations of publishers and algorithms. We Fig. <ref type="figure" coords="7,154.40,357.70,4.13,7.89">2</ref>. We compare the performance of 16 algorithms over the final evaluation period's span of 29 days. We compute the average CTR for each day. Subsequently, we subtract the result from each algorithm's individual CTR for the same day. The labels map to teams as follows: A-E → "riadi-gld", F → "artificial intelligence", G → "gaussiannoise", H → "riemannzeta", I → "insight-centre", J → "abc", K → "university of essex", and L-P → "cwi".</p><p>observe that publishers "694" and "1677" have lesser CTR for almost all algorithms compared to "418" and "35774". This might be partially due to how the publishers present the recommendations. Some presentation might draw more attention toward the suggested articles than other. The top-performing algorithms "andreas" (team "abc") and "Recommender" (team "artificial intelligence") achieve the relatively highest CTR independent of the publisher. We expect a recommendation service's reliability to affect the overall performance. Failing to serve plenty of requests will negatively affect CTR. Successfully suggesting news items will harness valuable feedback to further improve the recommendation algorithm. Figure <ref type="figure" coords="7,274.02,560.48,4.98,8.74" target="#fig_3">4</ref> contrasts CTR and error rates observed during the final evaluation period. CTR refers to the ratio of clicked suggestions to received requests. Error rates reflect the proportion of requests that could not be served by the algorithm. Performances are colored with respect to the team operating the recommendation service. Most teams managed to keep error rates below 10 % with the exceptions of "riemannzeta", "riadi-gdl", and "university of essex". Remarkably, team "riadi-gdl" achieved a CTR of ≈ 0.9 % at an error rate of ≈ 53 %. This indicates that their algorithm frequently failed to provide suggestions. Simultaneously, the suggestions given were particularly relevant to the Fig. <ref type="figure" coords="8,154.40,403.53,4.13,7.89">3</ref>. The heatmap shows the Click-Through-Rates observed for combinations of algorithms and publishers. The four publishers account for ≈ 85 % of requests. Publishers "418" and "35774" obtain a higher CTR compared to "694" and "1677" on average.</p><p>recipients. Conversely, team "insight-centre" achieved a rather low error rate of ≈ 5.4 %. Still, their CTR did not exceed 0.2 %. Thereby, we conclude that while reliability can affect CTR, we have to consider additional factors. We note the difference in computing power between the baselines "riemannzeta" and "gaussiannoise" described in Section 3.2 The more powerful "gaussiannoise" achieved an error rate close to 0. In constrast, "riemannzeta" failed to respond to ≈ 16 % of its requests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task 2</head><p>The offline evaluation (based on a dataset recorded in July and August 2015) enables the reproducible evaluation of stream-based recommender algorithms. Having complete knowledge about the data set allows us to implement new baseline strategies. In addition to the baseline recommender used in Task 1, we implemented the "optimal" recommender. The recommender searches in the data set the items that will be rewarded for the current request by the evaluation component. This strategy used knowledge about the future. Thus, the strategy is not a recommender algorithm; it only implements a data set look-up. Consequently, this strategy cannot be used in the online "live" evaluation. Nevertheless the measured CTR of the optimal recommender algorithm is interesting since the strategy allows us to measure the upper bound for the CTR in the analyzed setting.</p><p>Figure <ref type="figure" coords="9,181.03,520.90,4.98,8.74">5</ref> shows the maximal achievable CTR for the three different domains in the offline dataset. The graphs show that the CTR varies highly from day to day. In addition, the graphs show that the average offline CTR for each of the analyzed news portals is specific for each of the portals. This can be explained by the different user groups and the differences in the number of messages per day. Due to the definition of the offline CTR, the expected CTR correlates with the number of messages forwarded as requests to a participant.</p><p>The evaluation with respect to scalability focused on maximizing the throughput. Since the teams in the competition used different hardware configurations, the measured results cannot be compared directly. A common optimization objective that has been addressed by the teams working on Task 2 is the effective synchronization of concurrently executed threads. This can be reached by using 0.0% 0.5% Fig. <ref type="figure" coords="10,154.40,289.37,4.13,7.89">5</ref>. The figure visualizes the offline CTR for the optimal "recommender algorithm". The optimal recommendation strategy is implemented by looking up the items that will be rewarded by the evaluator. The strategy defines the upper bound of the CTR reachable in Task 2.</p><p>highly optimized data structures (such as concurrent collections or Guava 9 ) <ref type="bibr" coords="10,470.08,356.60,10.52,8.74" target="#b7">[8]</ref> or by using frameworks for building asynchronous, distributable systems <ref type="bibr" coords="10,462.33,368.55,14.61,8.74" target="#b9">[10]</ref>. Distributing a recommender algorithm over several machines adds extra overhead but gives a high degree for flexibility.</p><p>For the next year, we plan to use standardized virtual machines for the scalability evaluation, ensuring that all teams run the algorithms on exactly the same "virtual" hardware. In order to hide the complexity of building the evaluation environment, we plan to improve the Idomaar framework 10 and facilitate getting started with it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Submissions</head><p>We received two submissions detailing the efforts of two teams. Gebremeskel and de Vries <ref type="bibr" coords="10,190.43,526.98,10.52,8.74" target="#b1">[2]</ref> explored the utility of geographic information. They hypothesize that visitors have special interest in news stories about their local community. The implement a recommender which leverages geographic data when matching visitors and news articles. We refer to their results as team "cwi".</p><p>Verbitskiy, Probst, and Lommatzsch <ref type="bibr" coords="10,314.24,575.98,15.50,8.74" target="#b9">[10]</ref> developed a most-popular recommender. Their investigation targets scalability. They use the Akka framework to benefit from concurrent message passing. They conducted their evaluation outside the final evaluation period. Still, they managed to obtain higher CTR than the continued baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Discussion</head><p>NewsREEL aims to discover strategies that filter relevant news articles. Last year's edition introduced a "living lab" setting. This allows participants to evaluate their algorithms with actual users' feedback. This year's edition extended the previous setting. We developed the Idomaar framework. It not only keeps track of recommendation quality but records other performance metrics.</p><p>We continued competing with our baseline and last year's winning approach in order to demonstrate the ability of approaches to improve over both a basic system, and also the state of the art. Task 1 provided results which confirmed last year's findings. The baseline proved to be hard to beat. Last year's winner re-claimed the title. What produced this success story? Which factors determine the superior recommendation quality of the "artifcial intelligence" approach?</p><p>A team might have an advantage as it receive a larger or lower volume of requests than its competitors. We observed a comparable volume of requests for all algorithms active for the full evaluation period. These algorithms collected on average ≈ 1000 requests per day. The few exceptions with less requests were exactly those teams exhibiting higher error rates. Table1 shows requests on team level. Teams running several algorithms simultaneously have more request in total. Nevertheless, individual algorithms obtained similar shares of requests considering error rates and periods of inactivity. Has "artificial intelligence" received disproportionately many requests of visitors disproportionately likely to click? In that case, we would expect to observe varying performances at different days and on different publishers. In other words, we assume only marginal chances of receiving a specific subset of visitors consistently throughout time and publishers. Contrarily, Figure <ref type="figure" coords="11,267.97,416.65,4.98,8.74">2</ref> shows consistent performance over average for almost all days. Similarly, Figure <ref type="figure" coords="11,281.63,428.61,4.98,8.74">3</ref> lacks evidence for variations with respect to publishers. Is "artificial intelligence" running more reliably than its competitors? In fact, Figure <ref type="figure" coords="11,198.36,452.52,4.98,8.74" target="#fig_3">4</ref> shows extremely low error rates. On the other hand, competitors including "gaussiannoise" and "cwi" achieve similar error rates but fall behind with respect to CTR. We conclude that combining popularity, freshness, and trend-awareness gives "artificial intelligence" a competitive advantage. Neither chance, bias, nor reliability explain the superior performance over four weeks.</p><p>We observed team "riadi-gdl" achieving the third best performance for an individual algorithm. This algorithms suffered from high error rates. We lack knowledge of the approach as we have not receive a working note for this performance. Still, it appears to involve promising algorithms which we would like to see more from in the future. Compensating the errors, the approach could potentially achieve even higher CTR than "artificial intelligence".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>CLEF NewsREEL 2015 has been an interesting challenge motivating teams to develop and benchmark recommender algorithms online and offline. An addition to the online evaluation focused on the maximizing the CTR, the offline task (Task 2) also considered technical issues (scalability, throughput). This year, the participating teams tested several different approaches for recommending news, ranging from a location-based approaches to most-popular algorithms optimized for streams to ensemble recommender for streams. Analyzing the results we found, that the provided baseline is hard to beat. Further, CTR varied with respect to the publisher indicating additional factors that affect performance. We observed higher CTR levels compared to last year's edition. This indicates that teams continue to optimize their algorithms.</p><p>The technical challenges have been addressed by means of applying optimized data structures supporting the simultaneous access by concurrently running threads. One team focused on machines with multiple cores; another team implemented an approach enabling the distribution over different machines (using the Akka framework).</p><p>Finally, we detected issues with the challenge and derived ways to further improve participants' experience. Users struggled to get started. We had provided tutorials for both tasks but participants appeared to require additional support. The Idomaar framework had been updated during the competition. On the one hand, this was necessary to fix technical issues. On the other hand, this required participants to adjust and monitor their systems to a larger degree. Besides improving participants' support, we seek to increase the interchange between both tasks. Participants who evaluate their news recommender with ORP should take advantage of the recorded data to better tune their algorithms. Conversely, participants working with the recorded data should check their algorithms' performance with ORP. Thereby, they assure that their algorithms not only scale well but provide relevant suggestions. Said et al. <ref type="bibr" coords="12,390.55,405.92,10.52,8.74" target="#b8">[9]</ref> strongly advocate such multi-objective evaluation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,134.77,407.28,345.83,7.89;9,134.77,418.27,345.82,7.86;9,134.77,429.22,345.82,7.86;9,134.77,440.18,213.98,7.86"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.<ref type="bibr" coords="9,154.40,407.28,4.13,7.89" target="#b3">4</ref>. The figure illustrates the relation between error rates and CTR as observed in the final evaluation period. Algorithms are colored according to their team membership. CTR refers to the ratio of clicks to requests. Error rates represent the proportion of requests which could not be served by the algorithm.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_0" coords="3,144.73,646.48,103.56,7.47"><p>http://rf.crowdrec.eu/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_1" coords="3,144.73,657.44,89.44,7.47"><p>http://crowdrec.eu/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>The work leading to these results has received funding (or partial funding) from the <rs type="programName">Central Innovation Programme for SMEs</rs> of the <rs type="funder">German Federal Ministry for Economic Affairs and Energy</rs>, as well as from the <rs type="funder">European Unions Seventh Framework Programme</rs> (<rs type="grantNumber">FP7/2007-2013</rs>) under grant agreement number <rs type="grantNumber">610594</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_yTubeut">
					<orgName type="program" subtype="full">Central Innovation Programme for SMEs</orgName>
				</org>
				<org type="funding" xml:id="_3x7gRST">
					<idno type="grant-number">FP7/2007-2013</idno>
				</org>
				<org type="funding" xml:id="_3QnW4Zt">
					<idno type="grant-number">610594</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>//github.com/google/guava 10 https://github.com/crowdrec/idomaar</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="12,142.96,569.95,337.63,7.86;12,151.52,580.91,329.07,7.86;12,151.52,591.87,306.75,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,272.59,569.95,208.00,7.86;12,151.52,580.91,159.69,7.86">Shedding Light on a Living Lab: The CLEF NEWS-REEL Open Recommendation Platform</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brodt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hopfgartner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,330.91,580.91,149.68,7.86;12,151.52,591.87,147.67,7.86">Proceedings of the Information Interaction in Context conference, IIiX&apos;14</title>
		<meeting>the Information Interaction in Context conference, IIiX&apos;14</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="223" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,602.42,337.64,7.86;12,151.52,613.37,329.07,7.86;12,151.52,624.33,127.10,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,311.83,602.42,168.76,7.86;12,151.52,613.37,121.93,7.86">The degree of randomness in a live recommender systems evaluation</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gebremeskel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,298.72,613.37,177.59,7.86">Working Notes for CLEF 2015 Conference</title>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<publisher>CEUR</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,634.88,337.64,7.86;12,151.52,645.84,329.07,7.86;12,151.52,656.80,329.07,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,426.73,645.84,53.86,7.86;12,151.52,656.80,187.68,7.86">Report of the evaluation-as-a-service (EaaS) expert workshop</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hopfgartner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kando</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollup</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,347.15,656.80,53.13,7.86">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="65" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,119.67,337.64,7.86;13,151.52,130.63,329.07,7.86;13,151.52,141.59,211.31,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,151.52,130.63,215.03,7.86">Benchmarking news recommendations in a living lab</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hopfgartner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kille</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lommatzsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Plumbaum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brodt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Heintz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,388.14,130.63,92.45,7.86;13,151.52,141.59,120.67,7.86">5th International Conference of the CLEF Initiative</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="250" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,152.55,337.64,7.86;13,151.52,163.51,329.07,7.86;13,151.52,174.47,152.10,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,356.45,152.55,71.42,7.86">The plista dataset</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kille</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hopfgartner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brodt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Heintz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,446.81,152.55,33.79,7.86;13,151.52,163.51,329.07,7.86;13,151.52,174.47,30.23,7.86">NRS&apos;13: Proceedings of the International Workshop and Challenge on News Recommender Systems</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">10 2013</date>
			<biblScope unit="page" from="14" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,185.43,337.63,7.86;13,151.52,196.39,329.07,7.86;13,151.52,207.34,329.07,7.86;13,151.52,218.30,114.85,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,224.19,196.39,256.39,7.86;13,151.52,207.34,47.77,7.86">Stream-based recommendations: Online and offline evaluation as a service</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kille</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lommatzsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Turrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sereny</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brodt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Seiler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hopfgartner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,224.46,207.34,256.13,7.86;13,151.52,218.30,85.91,7.86">Proceedings of the 6th International Conference of the CLEF Association, CLEF&apos;15</title>
		<meeting>the 6th International Conference of the CLEF Association, CLEF&apos;15</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,229.26,337.63,7.86;13,151.52,240.22,329.07,7.86;13,151.52,251.18,227.64,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,306.51,229.26,174.08,7.86;13,151.52,240.22,29.23,7.86">Real-time recommendations for user-item streams</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lommatzsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Albayrak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,200.01,240.22,280.58,7.86;13,151.52,251.18,10.75,7.86">Proc. of the 30th Symposium On Applied Computing, SAC 2015, SAC &apos;15</title>
		<meeting>of the 30th Symposium On Applied Computing, SAC 2015, SAC &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1039" to="1046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,262.14,337.64,7.86;13,151.52,273.10,329.07,7.86;13,151.52,284.06,329.07,7.86;13,151.52,295.02,36.40,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,290.25,262.14,190.34,7.86;13,151.52,273.10,110.67,7.86">Optimizing and evaluating stream-based news recommendation algorithms</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lommatzsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Werner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,281.85,273.10,198.74,7.86;13,151.52,284.06,139.72,7.86">Proceedings of the Sixth International Conference of the CLEF Association, CLEF&apos;15</title>
		<meeting>the Sixth International Conference of the CLEF Association, CLEF&apos;15<address><addrLine>Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9283</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,305.98,337.63,7.86;13,151.52,316.93,329.07,7.86;13,151.52,327.89,329.07,7.86;13,151.52,338.85,106.74,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,423.60,305.98,56.99,7.86;13,151.52,316.93,147.90,7.86">Recommender systems evaluation: A 3D benchmark</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Said</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tikk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Stumpf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cremonesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,319.69,316.93,160.90,7.86;13,151.52,327.89,270.28,7.86">Proceedings of the Workshop on Recommendation Utility Evaluation: Beyond RMSE (RUE 2012), RUE&apos;12</title>
		<title level="s" coord="13,151.52,338.85,44.28,7.86">CEUR-WS</title>
		<meeting>the Workshop on Recommendation Utility Evaluation: Beyond RMSE (RUE 2012), RUE&apos;12</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">910</biblScope>
			<biblScope unit="page" from="21" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,349.81,337.97,7.86;13,151.52,360.77,329.07,7.86;13,151.52,371.73,127.10,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,331.76,349.81,148.83,7.86;13,151.52,360.77,137.90,7.86">Developing and evaluation of a highly scalable news recommender system</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Verbitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Probst</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lommatzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,307.85,360.77,168.46,7.86">Working Notes for CLEF 2015 Conference</title>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<publisher>CEUR</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
