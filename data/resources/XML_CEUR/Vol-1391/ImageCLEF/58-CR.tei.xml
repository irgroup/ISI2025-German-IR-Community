<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,179.07,116.95,257.23,12.62;1,180.39,134.89,254.59,12.62">INAOE-UNAL at ImageCLEF 2015: Scalable Concept Image Annotation</title>
				<funder ref="#_nMusTkZ">
					<orgName type="full">CONACYT</orgName>
				</funder>
				<funder ref="#_CNBKQF9">
					<orgName type="full">Clasificación y recuperación de imágenes mediante técnicas de minería de textos</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,143.59,172.56,59.85,8.74"><forename type="first">Luis</forename><surname>Pellegrin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Óptica y Electrónica (INAOE)</orgName>
								<orgName type="institution">Instituto Nacional de Astrofísica</orgName>
								<address>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,214.00,172.56,75.78,8.74"><forename type="first">Jorge</forename><forename type="middle">A</forename><surname>Vanegas</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">MindLab research group</orgName>
								<orgName type="institution">Universidad Nacional de Colombia (UNAL)</orgName>
								<address>
									<country key="CO">Colombia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,300.33,172.56,57.73,8.74"><forename type="first">John</forename><surname>Arevalo</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">MindLab research group</orgName>
								<orgName type="institution">Universidad Nacional de Colombia (UNAL)</orgName>
								<address>
									<country key="CO">Colombia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,368.61,172.56,69.63,8.74"><forename type="first">Viviana</forename><surname>Beltrán</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">MindLab research group</orgName>
								<orgName type="institution">Universidad Nacional de Colombia (UNAL)</orgName>
								<address>
									<country key="CO">Colombia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,448.80,172.56,22.97,8.74;1,159.91,184.51,61.51,8.74"><forename type="first">Hugo</forename><forename type="middle">Jair</forename><surname>Escalante</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Óptica y Electrónica (INAOE)</orgName>
								<orgName type="institution">Instituto Nacional de Astrofísica</orgName>
								<address>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,231.98,184.51,108.88,8.74"><forename type="first">Manuel</forename><surname>Montes-Y-Gómez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Óptica y Electrónica (INAOE)</orgName>
								<orgName type="institution">Instituto Nacional de Astrofísica</orgName>
								<address>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,370.80,184.51,80.18,8.74"><forename type="first">Fabio</forename><forename type="middle">A</forename><surname>González</surname></persName>
							<email>fagonzalezo@unal.edu.co</email>
							<affiliation key="aff1">
								<orgName type="laboratory">MindLab research group</orgName>
								<orgName type="institution">Universidad Nacional de Colombia (UNAL)</orgName>
								<address>
									<country key="CO">Colombia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,179.07,116.95,257.23,12.62;1,180.39,134.89,254.59,12.62">INAOE-UNAL at ImageCLEF 2015: Scalable Concept Image Annotation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1D7E57D81A8AECCCC9CA1392EBA78780</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>multimodal representation</term>
					<term>textual description of images</term>
					<term>visual prototype</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the joint participation of the TIA-LabTL (INAOE) and the MindLab research group (UNAL) at the ImageCLEF 2015 Scalable Concept Image Annotation challenge subtask 2: generation of textual descriptions of images -noisy track. Our strategy relies on a multimodal representation that is built in an unsupervised way by using the associated text to images and the visual features that represent them. In the multimodal representation for every word extracted from the indexed web pages a visual prototype is formed, each prototype being a distribution over visual descriptors. Then, the process of generation of a textual description is formulated as a two-step IR problem. First, the image to be described is used as visual query and compared with all the visual prototypes in the multimodal representation; next the k-nearest prototypes are used as a textual query to search for a phrase in a collection of textual descriptions, the retrieved phrase is then used to describe the image.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the 4th edition of the Scalable Concept Image Annotation challenge <ref type="bibr" coords="1,442.27,519.75,9.96,8.74" target="#b1">[2]</ref>, there are two main subtasks: 1) image concept detection and localization, and 2) generation of textual description of images. There are two separate tracks for SubTask 2: clean and noisy tracks. The goal of the noisy track is to develop a system that generates a textual description for an input image, by using only the information that provides the visual representation of the image. Additionally, we have a collection of images downloaded from Internet, for which we have the web page where images appear and the keywords that were used for searching the images.</p><p>This paper describes the joint participation of the TIA-LabTL 1 (INAOE) and the MindLab 2 research group (UNAL) in the 2015 Scalable Concept Image Annotation challenge subtask 2: generation of textual descriptions of imagesnoisy track.</p><p>Our approach to generate textual descriptions relies on the use of the textual information of web pages to describe the visual content. First, we associate visual descriptors to text and build a multimodal representation where we have a visual prototype for every word extracted from the reference text-collection. Second, the multimodal representation is used to retrieve words related to an input image to be described, then the k-nearest words are used as query to retrieve captions in a reference textual description set. Therefore, our method of image captioning can be seen as a two-step information retrieval (IR) task (first retrieve words, then retrieve descriptions). An important remark, is that we build the multimodal representation following an unsupervised approach where we do not require labeled data at all, and can describe images with any term in our vocabulary. The official results are encouraging and show us that there are ways to improve, including to explore different visual features and different reference captioning collections.</p><p>The remainder of the paper is organized as follows: Section 2 describes our method; Section 3 shows the experimental settings that we used; Section 4 shows the experimental results obtained; finally, in Section 5 some conclusion of this work are presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Two-step IR process for Automatic Image Captioning</head><p>As mentioned above, our proposed method is divided in two IR stages: wordretrieval and caption-retrieval. Intuitively, the second stage requires a textual query to search for a caption, and the first stage produces as output a textual query. Where the first stage matches a query image with words. Hence, our method requires a way to associate images with words, more specifically, we require a representation for words in the visual feature space. In addition to the two IR steps, we have a preliminary multimodal-indexing (MI) step, but first we preprocess text and images in order to build the multimodal representation. Fig. <ref type="figure" coords="2,155.97,497.66,4.98,8.74">1</ref> shows a diagram of the overall process (including the MI step). In the following we describe in detail each of these steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preprocessing</head><p>As we can see in the Fig. <ref type="figure" coords="2,240.40,558.51,4.98,8.74">1</ref> the preprocessing is carried in two datasets, a reference image collection and reference textual description set. In the reference web-image collection C we have represented every image I i by means of visual V i and textual T i features defined by bags-of-words (textual and visual 3 ): Fig. <ref type="figure" coords="3,192.70,340.96,3.87,8.74">1</ref>: Diagram of the proposed approach to describe images.</p><formula xml:id="formula_0" coords="3,258.24,396.12,98.37,9.65">C = I i : {V i , T i } i=1,...,n</formula><p>The visual and textual representations of images I i can be denoted by:</p><formula xml:id="formula_1" coords="3,263.32,438.59,84.35,42.07">T i = t i,1 , . . . , t i,|X| V i = v i,1 , . . . , v i,|Y |</formula><p>where t i,j ∈ R |X| and v i,j ∈ R |Y | respectively, where |X| and |Y | denote the sizes of the textual and visual vocabularies. Where T and V are matrices whose rows correspond to the text and visual descriptions of each image with size n × |X| and n × |Y | respectively. The corresponding preprocessing is detailed as follows:</p><p>Textual preprocessing Textual information (i.e., the web pages associated to images) is represented with a standard term-frequency bag of words representation, then classic scheme of weighting TF-IDF is applied. We consider that to express the visual content of the images is more important the use of nouns than the use of pronouns or verbs, so we eliminate these latter.</p><p>Visual preprocessing Visual information (i.e., the visual descriptors that represented to images) is normalized and treated as a standard frequency bag-ofvisual-words representation, then classic scheme of weighting TF-IDF is applied, the purpose of this weighting here is to prioritize to those visual words that occur not in the most of the images. Here we did not consider the elimination of any visual words.</p><p>Furthermore, our method also uses as input a reference textual description set D (see Fig. <ref type="figure" coords="4,200.87,167.81,4.43,8.74">1</ref>) that contains a set of captions that can be used to describe to images. We use three different set of sentences as reference in our tests for the generation of the textual descriptions (see Table <ref type="table" coords="4,348.35,191.72,4.98,8.74" target="#tab_0">1</ref> for examples):</p><p>-Set A. The set of sentences given in the evaluation ground truth for the subtask 2 by the organizers at the ImageCLEF, with ≈19,000 sentences. -Set B. A set of sentences used in the evaluation of MS COCO 2014 dataset <ref type="bibr" coords="4,151.70,249.31,9.96,8.74" target="#b5">[6]</ref>, with ≈200,000 sentences. -Set C. A subset of PASCAL dataset <ref type="bibr" coords="4,316.75,261.23,9.96,8.74" target="#b2">[3]</ref>, with ≈5,000 sentences. 'Well kept kitchen with marble counter tops and stainless steel fridge.' set C 'One jet lands at an airport while another takes off next to it.' 'Duck grooming itself while standing in the water.'</p><p>Textual description preprocessing The sentences of the reference textual description set are indexed and represented with a standard term-frequency of bag-of-words representation removing only stop words. For the case of this indexing we did not consider the elimination of pronouns or verbs, the reason is that by including the phrases can be weighted by frequency considering the amount of words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multimodal Indexing</head><p>Once it performed the preprocessing, the preprocessed textual and visual features are used in the construction of the multimodal representation (as we can see in the diagram of the Fig. <ref type="figure" coords="4,258.53,549.52,3.87,8.74">1</ref>). As we mentioned before, we require a way to associate images with words in order to perform the first stage of our method. We hypothesized a space where each word can be associated with a visual prototype, a multimodal representation. In this way, any query image can be compared with prototypes and we can determine what words are more related to the query. For the construction of our multimodal representation we rely on term-occurrence statistics, where the terms are both textual and visual features. The main idea behind it is that both the textual view as the visual view of an image have a salience in the same objects represented by two different features (textual and visual), that is, if in a web page the main topic in text is about 'dogs' there exist a likelihood that the visual descriptors or images in the web page are associated to a 'dog'. This assumption can be confirmed when a majority of images share the same characteristics forming a distinctive visual prototype. The hypothesis is that one word has predominant visual descriptors associated to it, so every word can be seen as a prototypical of image that represents to the word. The multimodal representation associates each word with a distribution of weights over the visual vocabulary forming a visual prototype for every word. Our multimodal representation is obtained as follows:</p><formula xml:id="formula_2" coords="5,149.71,225.86,230.30,35.44">M = T T • V Therefore, we can see that M i,j = n k=1 T i,k • V k,j</formula><p>is a scalar value that express the degree of association between word i and visual-word j, across the whole collection of documents. In this way, each row of the matrix M (each associated to a word), can be seen as a visual prototype. Finally, our M is a matrix of size |X| × |Y |, that is, the dimension is determined by the sizes of the vocabularies that represent both textual and visual features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Step 1: Word-Retrieval (WR)</head><p>The multimodal representation is used in this step in a similar way to a IR task, first we use the visual representation of an image to be described as a query, next using a cosine similarity we score the distances with all the visual prototypes from the multimodal representation (see WR step in <ref type="bibr" coords="5,360.55,396.62,24.47,8.74">Fig 1)</ref>. The cosine similarity is defined by:</p><formula xml:id="formula_3" coords="5,247.85,428.58,118.46,23.22">sim(I q , M w ) = I * M w ||I||||M w ||</formula><p>where I q is the input image represented by a vector of visual words, and M w is one visual prototype from the multimodal representation, that is, the row w of the matrix M above, remember that this formula is applied to every word, that is, w = {1, . . . , |X|}. Finally, the k words associated to the most similar prototypes are used in the next stage to describe the visual content of the input image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Step 2: Caption-Retrieval (CR)</head><p>Using the output of the WR step, with the k words associated to the k most similar visual prototypes, we formulate a second query, this time, it is a textual query (represented by a TF weighting scheme as the reference captions). We measure a cosine similarity between the textual query and the indexed captions, where Q T is a vector of size |X| that contains the values of k words with the highest scores obtained in the WR step, and the D c is one sentence in the set of indexed captions. The cosine similarity is applied to every caption in D, that is, c = {1, . . . , d}. The quantity of words used in the textual query the specificity of the textual description to retrieve, that is, we can obtain a general textual description using only a few words. Finally, the m indexed captions that maximize the cosine similarity can be used to describe the input image. We used the caption with highest similarity as textual description for the image I q . The CR step is depicted the Fig. <ref type="figure" coords="6,261.07,167.81,3.87,8.74">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Settings</head><p>The dataset for the approached task is composed by 500,000 images extracted from a database of millions of images downloaded from the Internet. Every image in the dataset has an associated text, i.e. the web pages where the images were found it or the keywords used for the searching at the Internet.</p><p>Several pre-processed visual features are provided by the organizers of the task, we experimented with three different: OPPONENT-SIFT, relu7 layer of activations in a CNN model of 16-layer, and fc8 layer of activations in a CNN model of 16-layer. At the end, we chose relu7 layer as visual features for the representation of the images in the training set, we refer to relu7 layer as visual words<ref type="foot" coords="6,160.03,331.11,3.97,6.12" target="#foot_1">4</ref> but strictly this representation is an activation of a CNN model.</p><p>Table <ref type="table" coords="6,178.19,344.77,4.98,8.74" target="#tab_0">1</ref> describes the 10 runs that we submitted for evaluation. The first row of the Table <ref type="table" coords="6,212.09,356.73,4.98,8.74" target="#tab_0">1</ref> shows the set of reference captions used: denoted as A the set of sentences given by organizers in the ImageCLEF; denoted as B the set of sentences of MS COCO dataset; and denoted as C a subset of sentences of PASCAL.</p><p>From the output of the WR step, we formed a textual query as we mentioned before by the k most similar words defined by visual prototypes from our multimodal representation. We experimented with two ways to select words, in the second row of the Table <ref type="table" coords="6,242.07,440.55,4.98,8.74">2</ref> with the letter w corresponds to use the k most similar words; and with the letter c we filtered visual prototypes corresponding to words that matches concepts taken from a list of 251 (the concept list provided by the organizers, for more info. see <ref type="bibr" coords="6,291.94,476.41,10.30,8.74" target="#b1">[2]</ref>), so in this latter case the k most similar concepts are used.</p><p>Selection of k value was established empirically, that is, the number of words that we used in textual query. We established a threshold for the k value using information from the visual descriptor of the images. We noted that if image to be described was represented by few visual words, that is, if it had great number of zeros values in visual words (see Fig. <ref type="figure" coords="6,308.11,548.28,4.98,8.74" target="#fig_0">2</ref> for some examples of images) then for these kind of images we can use a minimum k value to form the textual query. On the other hand, the images with a low sparsity (lower number of zeros) in their visual representation we need to use more data. The ranges of threshold for k values are summarized follows:</p><p>-Concepts: 3 for &gt;40% sparsity; and 5 for &lt;40% sparsity.</p><p>-Words: 10 words for &gt;50% sparsity; 30 words between 40% and 50% sparsity, and 50 words for &lt;40% sparsity.</p><p>Another aspect considered in our settings was to use different representation for the textual query: in the third row in the Table <ref type="table" coords="7,352.87,176.47,4.98,8.74">2</ref> with weights expressing real values with the letter r, that is, the score obtained by cosine similarity for the textual query and the normalized frequency for the sentences in the reference textual description set; and denoted by the letter b a binary value for both textual query and indexed captions; the reason is because when we use a binary representation we are giving importance to sentences with a greater number of words or concepts, on the other hand, if we use the weights then the privileged sentences are those with a better matching. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>Table <ref type="table" coords="7,162.28,418.01,4.98,8.74" target="#tab_1">3</ref> shows the results obtained by each of the submitted runs. For comparison, we also show the result of the best run submitted to the task (RUC run3 ).</p><p>The reported values corresponds to the Meteor Score <ref type="bibr" coords="7,366.68,441.92,10.52,8.74" target="#b0">[1]</ref> when using a minimum of five human-authored textual descriptions as the gold standard reference. Columns of Table <ref type="table" coords="7,229.87,465.83,4.98,8.74" target="#tab_1">3</ref> express across all test images: the average, median, min and max Meteor scores. According to the mean and median performance when we used only concepts in the textual query (runs 1, 3, 5 and 6) we obtained a better score than using words (runs 2, 4, 7, 8 and 10), we believe it is because of the confidence of the detected concept and to the existence of sentences that describe the concept.</p><p>In ours runs the best mean score was using the set A (run 3), we believe that this set A (sentences of evaluation of ImageCLEF dataset) is more controlled in the quality of the description due to the amount of sentences in comparison to set B (MS COCO dataset) where there are 10 times more of sentences. Another characteristic of the run 3 is that uses a binary representation and concepts as data, we believe that exist a relationship between the amount of data used in the textual query, when the textual query is short is beneficial a binary representation to retrieve sentences with the more quantity of concepts.</p><p>The results in comparison with the best score of RUC team show us that we are not so far, we would like to evaluate our method with a list of concepts bigger and others sets of sentences that can complement to those used already. In Fig. <ref type="figure" coords="8,181.66,310.64,3.87,8.74" target="#fig_0">2</ref>, we can see two examples of images annotated with textual description using our method. For both images, we can see the output of the WR step, the textual query, and the final output using two sets of sentences (CR step). In image a, we can see that the top two concepts can describe the image completely, and that the difference between the two generated textual descriptions seems to be that in set A are not enough phrases that describing 'watches' or 'clocks'.</p><p>The generated textual description for image b fails because the image shows 'two spatulas' but the detected concept is 'knife' (there is not 'spatula' concept in the list), using words the 'spatula' could be detected, however, there is not a textual description with this particular word in both A or B sets. One natural way to correct the description of the image b is to add new set of sentences that contain the words that have been detected.</p><p>In Fig. <ref type="figure" coords="8,183.61,483.65,3.87,8.74">3</ref>, we show two images with scenes that are ambiguous and can be described in several ways. For both images, we can see the final output using two sets of sentences (corresponding to the 4 and the 8 runs respectively). We use words as textual query to find the textual description with binary values. For image c, the two generated captions could be used as description, the sentences describe different aspect in the image. On the other hand, the image d can be described by human annotator as a 'videogame' or 'animation', here our method describe the image using the detected objects like 'car' or 'alleyway' but not as 'animation'. noisy track. The proposed method works in an unsupervised way using the information of textual and visual features to build a multimodal representation. The overall process of generation of textual description is formulated as two-step IR task. Our method is flexible and can be applied with different visual features encouraging us to explore visual features learned by using different approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>The experimental results showed the competitiveness of our technique and that it can be improved, including refined reference sentences for the textual description or filtering of words in the vector representation by means of measures of relatedness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,229.26,289.30,156.83,8.74;7,335.82,302.62,35.75,8.77;7,245.38,314.97,158.65,8.77;7,213.47,327.35,198.47,8.74;7,250.89,339.71,151.01,8.74;7,203.42,352.06,197.65,8.74;7,207.96,363.96,199.44,8.74"><head>Table 2 :</head><label>2</label><figDesc>Settings of submitted runs. runs → ↓ settings 1 2 3 4 5 6 7 8 9 10 sentences set used A A A A B B B B * A,B,C data used c w c w c c w w * w query representation r r b b r b r b * b * Only using top 5 concepts from defined list.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,134.77,344.63,345.83,8.74;9,134.77,356.58,345.82,8.74;9,134.77,368.54,309.81,8.74"><head>Fig. 2 :Fig. 3 :</head><label>23</label><figDesc>Fig. 2: Two images with their text description generated under two different sets of sentences, and the top 5 concepts, where the firsts three were used for the query. These examples correspond to the 1 and the 5 runs respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="3,157.27,116.83,300.81,212.60"><head></head><label></label><figDesc></figDesc><graphic coords="3,157.27,116.83,300.81,212.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,136.16,291.88,335.49,67.12"><head>Table 1 :</head><label>1</label><figDesc>Examples of sentences extracted from three reference caption sets.</figDesc><table /><note coords="4,136.16,313.57,85.73,8.77;4,142.74,325.95,213.06,8.74;4,175.13,337.91,296.49,8.74;4,142.95,350.26,249.41,8.74"><p>dataset sentences set A 'A person shaking another persons hand.' 'Bunk beds in a room by a window with vibrant flowered bed covers.' set B 'A Honda motorcycle parked in a grass driveway.'</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,178.24,138.32,258.88,158.38"><head>Table 3 :</head><label>3</label><figDesc>METEOR score of the our submitted runs.</figDesc><table coords="8,178.24,153.66,258.88,143.03"><row><cell cols="3">RUN MEAN +-STDDEV MEDIAN MIN MAX</cell></row><row><cell>run1</cell><cell>0.1255+-0.0650</cell><cell>0.1140 0.0194 0.5679</cell></row><row><cell>run2</cell><cell>0.1143+-0.0552</cell><cell>0.1029 0.0175 0.4231</cell></row><row><cell cols="2">run3 0.1403+-0.0564</cell><cell>0.1342 0.0256 0.3745</cell></row><row><cell>run4</cell><cell>0.1230+-0.0531</cell><cell>0.1147 0.0220 0.5256</cell></row><row><cell>run5</cell><cell>0.1192+-0.0521</cell><cell>0.1105 0.0000 0.4206</cell></row><row><cell>run6</cell><cell>0.1260+-0.0580</cell><cell>0.1172 0.0000 0.4063</cell></row><row><cell>run7</cell><cell>0.1098+-0.0527</cell><cell>0.1005 0.0000 0.4185</cell></row><row><cell>run8</cell><cell>0.1079+-0.0498</cell><cell>0.1004 0.0000 0.3840</cell></row><row><cell>run9</cell><cell>0.0732+-0.0424</cell><cell>0.0700 0.0135 0.2569</cell></row><row><cell>run10</cell><cell>0.1202+-0.0528</cell><cell>0.1123 0.0000 0.5256</cell></row><row><cell cols="2">RUC run3 0.1806+-0.0817</cell><cell>0.1683 0.0192 0.5696</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="2,144.73,613.96,335.86,7.86;2,144.73,624.92,335.86,7.86;2,144.73,635.88,335.86,7.86;2,144.73,646.84,335.87,7.86;2,144.73,657.79,246.25,7.86"><p>To obtain a bag of visual words representation first, a corpus of images are used to extract visual features, i.e. points of interest in images; second, the sampled features are clustered and the centroids quantize in a discrete number of visual words; third, the nearest visual words are identified in images using their visual features; fourth, a bag-of-visual-words histogram is used to represents images.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="6,144.73,613.96,335.87,7.86;6,144.73,624.92,335.87,7.86;6,144.73,635.88,335.86,7.86;6,144.73,646.84,335.87,7.86;6,144.73,657.79,209.68,7.86"><p>Although, relu7 layer is not based on visual words, for our purpose we can see this representation like one based in visual words. We normalize every vector of activation and use the normalized activation of neurons like frequencies of a group of visual words. Using the activation of neurons at the end is a kind of codebook, because all the images share a common space of representation.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This work was supported by <rs type="funder">CONACYT</rs> under project grant <rs type="grantNumber">CB-2014-241306</rs> (<rs type="funder">Clasificación y recuperación de imágenes mediante técnicas de minería de textos</rs>). Also this work was partially supported by the <rs type="programName">LACCIR programme</rs> under project <rs type="grantNumber">ID R1212LAC006</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_nMusTkZ">
					<idno type="grant-number">CB-2014-241306</idno>
				</org>
				<org type="funding" xml:id="_CNBKQF9">
					<idno type="grant-number">ID R1212LAC006</idno>
					<orgName type="program" subtype="full">LACCIR programme</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,138.35,336.86,342.24,7.86;10,146.91,347.82,333.68,7.86;10,146.91,358.78,157.55,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,264.03,336.86,216.56,7.86;10,146.91,347.82,129.47,7.86">Meteor Universal: Language Specific Translation Evaluation for Any Target Language</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lavie</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,298.60,347.82,181.99,7.86;10,146.91,358.78,124.85,7.86">Proceedings of the EACL 2014 Workshop on Statistical Machine Translation</title>
		<meeting>the EACL 2014 Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,369.74,342.24,7.86;10,146.91,380.70,333.67,7.86;10,146.91,391.65,333.68,7.86;10,146.91,402.61,152.98,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,230.82,380.70,249.77,7.86;10,146.91,391.65,167.67,7.86">Overview of the ImageCLEF 2015 Scalable Image Annotation, Localization and Sentence Generation task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dellandrea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,333.78,391.65,146.81,7.86;10,146.91,402.61,46.41,7.86">CLEF2015 Working Notes Workshop Proceedings</title>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,413.57,342.24,7.86;10,146.91,424.53,333.68,7.86;10,146.91,435.49,333.68,7.86;10,146.91,446.45,50.71,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,393.34,413.57,87.25,7.86;10,146.91,424.53,176.70,7.86">Collecting Image Annotations Using Amazon&apos;s Mechanical Turk</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,346.30,424.53,134.29,7.86;10,146.91,435.49,333.68,7.86;10,146.91,446.45,17.63,7.86">Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk</title>
		<meeting>the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,457.41,342.24,7.86;10,146.91,468.37,333.68,7.86;10,146.91,479.33,333.68,7.86;10,146.91,490.28,333.68,7.86;10,146.91,501.24,249.90,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,407.33,479.33,73.26,7.86;10,146.91,490.28,159.80,7.86">General Overview of ImageCLEF at the CLEF 2015 Labs</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bromuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">K</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Uskudarli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">B</forename><surname>Marvasti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Aldana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roldán</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,329.36,490.28,151.24,7.86;10,146.91,501.24,46.41,7.86">CLEF2015 Working Notes Workshop Proceedings</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,512.20,342.24,8.12;10,146.91,523.81,127.10,7.47" xml:id="b4">
	<monogr>
		<ptr target="htt://caffe.berkeleyvision.org/" />
		<title level="m" coord="10,146.91,512.20,306.84,7.86">Caffe: An open source convolutional architecture for fast feature embedding</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,534.12,332.03,8.12" xml:id="b5">
	<monogr>
		<ptr target="http://mscoco.org/dataset/" />
		<title level="m" coord="10,146.91,534.12,189.48,7.86">Microsoft COCO (Common Objects in Context</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
