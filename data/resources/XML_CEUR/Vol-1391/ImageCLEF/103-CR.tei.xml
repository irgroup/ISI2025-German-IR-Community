<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,145.76,116.95,323.83,12.62;1,168.68,134.89,277.99,12.62;1,248.94,152.82,117.47,12.62;1,144.43,170.75,326.49,12.62">KDEVIR at ImageCLEF 2015 Scalable Image Annotation, Localization, and Sentence Generation task: Ontology based Multi-label Image Annotation</title>
				<funder ref="#_zCqHM94">
					<orgName type="full">HORI FOUNDATION of JAPAN</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,234.02,208.43,58.67,8.74"><roleName>Md</roleName><forename type="first">Zia</forename><surname>Ullah</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Toyohashi University of Technology</orgName>
								<address>
									<addrLine>1-1 Hibarigaoka, Tempaku-Cho</addrLine>
									<postCode>441-8580</postCode>
									<settlement>Toyohashi, Aichi</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,319.84,206.85,59.18,10.32"><forename type="first">Masaki</forename><surname>Aono</surname></persName>
							<email>aono@tut.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Toyohashi University of Technology</orgName>
								<address>
									<addrLine>1-1 Hibarigaoka, Tempaku-Cho</addrLine>
									<postCode>441-8580</postCode>
									<settlement>Toyohashi, Aichi</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,145.76,116.95,323.83,12.62;1,168.68,134.89,277.99,12.62;1,248.94,152.82,117.47,12.62;1,144.43,170.75,326.49,12.62">KDEVIR at ImageCLEF 2015 Scalable Image Annotation, Localization, and Sentence Generation task: Ontology based Multi-label Image Annotation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DCC6718400491EA18B2E2D7901207054</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image Annotation</term>
					<term>Classification</term>
					<term>Feature-wise learning</term>
					<term>Ontology</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we describe our participation in the Image-CLEF 2015 Scalable Concept Image Annotation task. In this participation, we propose an approach of image annotation by using ontology at several steps of supervised learning with noisy unlabeled data. In this regard, we construct tree-like ontology for each annotating concept of images using WordNet and Wikipedia. The constructed ontologies are exploited throughout the proposed framework including several phases of training and testing of one-vs-all SVM classifiers. Several classifiers are trained on local or global visual features separately and results are ensemble using the classifiers' probability scores. The result turns out that our system achieves an average performance in this task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Due to the explosive growth of digital technologies, collections of images are increasing tremendously in every moment. The ever growing size of the image collections has evolved the necessity of image retrieval (IR) systems; however, the task of IR from a large volume of images is formidable since binary stream data is often hard to decode, and we have very limited semantic contextual information about the image content.</p><p>To enable the user for searching images using semantic meaning, automatically annotating images with some concepts or keywords using machine learning is a popular technique. During last two decades, there are a large number of researches being lunched using state-of-the-art machine learning techniques <ref type="bibr" coords="1,467.87,609.30,8.49,8.74" target="#b0">[1]</ref><ref type="bibr" coords="1,476.36,609.30,4.24,8.74" target="#b1">[2]</ref><ref type="bibr" coords="1,476.36,609.30,4.24,8.74" target="#b2">[3]</ref><ref type="bibr" coords="1,134.76,621.25,7.75,8.74" target="#b3">[4]</ref> (e.g. SVMs, Logistic Regression). In such efforts, most often each image is assumed to have only one class label. However, this is not necessarily true for real world applications, as an image might be associated with multiple semantic tags. Therefore, it is a practical and important problem to accurately assign multiple labels to one image. To alleviate above problem i.e. to annotate each image with multiple labels, a number of research have been carried out; among them adopting probabilistic tools such as the Bayesian methods is popular <ref type="bibr" coords="2,457.36,143.90,7.75,8.74" target="#b4">[5]</ref><ref type="bibr" coords="2,465.11,143.90,3.87,8.74" target="#b5">[6]</ref><ref type="bibr" coords="2,468.98,143.90,7.75,8.74" target="#b6">[7]</ref>. More review can be found in <ref type="bibr" coords="2,260.84,155.86,10.52,8.74" target="#b7">[8,</ref><ref type="bibr" coords="2,273.02,155.86,7.01,8.74" target="#b8">9]</ref>. However, accuracy of such approach depends on expensive human labeled training data.</p><p>Fortunately, some initiatives have been taken to reduce the reliability on manually labeled image data <ref type="bibr" coords="2,244.44,191.77,13.05,8.74" target="#b9">[10]</ref><ref type="bibr" coords="2,257.48,191.77,4.35,8.74" target="#b10">[11]</ref><ref type="bibr" coords="2,257.48,191.77,4.35,8.74" target="#b11">[12]</ref><ref type="bibr" coords="2,261.83,191.77,13.05,8.74" target="#b12">[13]</ref> by using cheaply gathered web data. Although the "Semantic gaps" between low-level visual features and high-level semantics still remain and accuracy is not improved remarkably.</p><p>In order to reduce the dependencies of human-labeled image data, Image-CLEF <ref type="bibr" coords="2,165.55,239.64,15.50,8.74" target="#b13">[14]</ref> has been organizing the image annotation task for the last several years, where training data is a large collection of Web images without ground truth labels. Despite the proposed methods in this task shown encouraging performance on a large scale dataset, unfortunately none of them utilizes the semantic relations among annotating concepts.</p><p>In this paper, we describe the participation of KDEVIR at ImageCLEF 2015 Scalable Image Annotation, Localization, and Sentence Generation task <ref type="bibr" coords="2,462.33,311.41,14.61,8.74" target="#b14">[15]</ref>, where, we have focused on image annotation subtask. In this regard, we have proposed an approach, ontology based learning that exploits both textual and visual features of images during training and testing. The evaluation results reveal the effectiveness of proposed framework.</p><p>The rest of the paper is organized as follows: Section 2 describes the proposed framework. Section 3 describes our submitted runs to this task as well as comparison results with other participants' runs. Finally, concluded remarks and some future directions of our work are described in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Framework</head><p>In this section, we describe our method for annotating images with a list of semantic concepts. We divide our method into four steps: 1) Constructing Ontology, 2) Pre-processing of Training Data, 3) Training Classifier, and 4) Predicting Annotations. An overview of our proposed framework is depicted in Fig. <ref type="figure" coords="2,454.02,499.21,3.87,8.74">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Constructing Ontology</head><p>Ontologies are the structural frameworks for organizing information about the world or some part of it. In computer science and information science, ontology is defined as an explicit, formal specification of a shared conceptualization <ref type="bibr" coords="2,465.11,573.39,15.50,8.74" target="#b15">[16,</ref><ref type="bibr" coords="2,134.76,585.34,12.73,8.74" target="#b16">17]</ref> and it formally represents knowledge as a set of concepts within a domain, and the relationships between those concepts. To utilize these relationships in image annotation, we construct ontology for each concept of a predefined list of concept used to annotate images.</p><p>In real world, an image might contain multiple objects (aka concepts) in a single frame, where concepts are inter-related and maintain a natural way of being co-appearance. We use these hypotheses to construct ontologies for Fig. <ref type="figure" coords="3,265.47,355.13,3.87,8.74">1</ref>: Proposed Framework concepts <ref type="bibr" coords="3,175.03,393.17,14.61,8.74" target="#b17">[18]</ref>. In this regard, we utilize WordNet <ref type="bibr" coords="3,348.43,393.17,15.50,8.74" target="#b18">[19]</ref> and Wikipedia as primary sources of knowledge. Let C be a set of concepts. We construct a tree-like ontology <ref type="bibr" coords="3,426.17,417.99,15.50,8.74" target="#b19">[20]</ref> for each concept c c ∈ C. In order to build ontologies, first of all, we select some types of relations including: 1) taxonomical R t , 2) functional R f , and 3) weak hierarchical, R wh . The relations are extracted empirically according to our observations on WordNet and Wikipedia articles. For each type of relations, we extract a set of relationship property as listed below: R t = {"inHypernymPathOf", "subClassOf", "isA"} R f ={"habitat", "inhabit", "liveIn", "foundOn", "foundIn", "locateAt", "na-tiveTo", "liveOn", "feedOn"} R wh ={"kindOf", "typeOf", "representationOf", "methodOf", "appearedAt", "ap-pearedIn", "ableToProduce"} Finally, we apply some "if-then" type inference rules to add an edge from a parent-concept to a child-concept by leveraging the above relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pre-processing of Training Data</head><p>Given a list of concepts, we select the potential images for each concept from the noisy training images by exploiting their metadata (details about metadata are given in <ref type="bibr" coords="3,187.72,645.16,15.50,8.74" target="#b14">[15]</ref>) and pre-constructed concept ontologies. In this regards, first of all, we detect the nouns and adjectives from metadata using WordNet followed by singularizing with Pling Stemmer<ref type="foot" coords="4,293.09,118.42,3.97,6.12" target="#foot_0">1</ref> . Secondly, detected terms from metadata: Web text (scofeat), keywords, and URLs are weighted by BM25 <ref type="bibr" coords="4,420.14,131.95,14.61,8.74" target="#b20">[21]</ref>, mean reciprocal rank (MRR), and a constant weight, ϑ ∈ (0, 1) respectively, which is followed by detecting concepts from the weighted sources on appearance basis. Thus, we have three lists of possible weighted concepts from three different sources of metadata for each image.</p><p>We take the inverted index of image-wise weighted concepts, thus generate the concept-wise weighted images. To aggregate the images for a concept from three sources, we normalize the weight of images using Max-Min normalization technique, and linearly combine the BM25, MRR, and constant ϑ to generate the final weight of images. From the resultant aggregated list of images, top-m images are primarily selected for each concept.</p><p>Finally, in order to increase the recall, we merge the primarily selected training images of each concept with its parent concepts of highest semantic confident (i.e. parents connected by r t ∈ R t ) by leveraging our concept ontologies. Thus, we enhance training images per-concept as well as number of annotated concepts per-image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training Classifier</head><p>Image annotation is a multi-class multi-label classification problem; current state-of-the-art classifiers are not able to solve this problem in their usual format. Towards this problem, we propose a technique of using ontologies during different phases of learning a classifier. In this regard, we choose Support Vector Machines (SVMs) as a classifier for its robustness of generalization. We subdivide the whole problem into several sub-problems according to the number of concepts, i.e. train SVMs for each concept separately, since using a large dataset at a time is not rational in terms of memory and time.</p><p>Another problem is that, along with the different parameters, the classification accuracy of SVMs depends on the positive and negative examples which are used to train the classifier. It is obvious that if classifiers are trained with wrong examples, the prediction will be wrong. However, selecting appropriate training example is formidable without any semantic clues. In this regard, for a concept, we take positive examples from its image-list which is generated in the preprocessing stage and the negative examples from all other concepts' imagelists those are not semantically related to the current concept. To handle this issue, we use our pre-constructed concept ontologies.</p><p>For each local or global visual feature, we train one-vs-all SVM for all concepts. With positive and negative examples, we train |F | probabilistic one-vs-all SVM models for each concept, where F is a set of visual feature types including CNN, GIST, Color Histograms, SIFT, C-SIFT, RGB-SIFT, and OPPONENT-SIFT. We use LIBSVM <ref type="bibr" coords="4,237.55,623.38,15.50,8.74" target="#b21">[22]</ref> to learn the SVM models. As kernel, instead of using the default choice of Linear kernel or Gaussian kernel, since image classification is a nonlinear problem and distribution of image data is unknown, we choose histogram intersection kernel (HIK) <ref type="bibr" coords="5,294.09,131.95,14.61,8.74" target="#b22">[23]</ref>. The HIK is defined as:</p><formula xml:id="formula_0" coords="5,230.59,152.39,250.00,30.21">k HI (h (a) , h (b) ) = l ∑ q=1 min(h (a) q , h (b) q )<label>(1)</label></formula><p>where h (a) and h (b) are two normalized histograms of l bins; in context of image data, two feature vectors of l dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Predicting Annotations</head><p>The trained models for all concepts generated based on each visual features in the previous subsection are used to predict annotations. Given a test image, if a model of particular concept responds positively, the image is considered as voted by current model i.e. the corresponding concept is primarily selected for annotation. At the same time, the tracks of predicted probability and vote are kept. This process is repeated for all learned models for all concepts. The concept-wise predicted probabilities and votes are accumulated for all visual features. In second level selection, empirical thresholds for accumulated probabilities and votes are used to select more relevant annotations. Finally, we take top-k weighted concepts as annotation for the test image. In ImageCLEF 2015 <ref type="bibr" coords="5,423.66,363.65,14.61,8.74" target="#b14">[15]</ref>, the test dataset and train dataset are same. This makes the concept detection of test data possible by using only the textual features of train dataset. In our proposed framework, we have both textual and visual features to recognize test images. However, in experiments, we conducted some runs using only the textual features of train data to annotate the test images. These runs confirm the validity of our preprocessing of noisy training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">KDEVIR Runs and Comparative Results</head><p>We submitted total five runs, which are differ from each other in terms of: use of ontology or not; number of primarily selected training images, m; and based on textual, visual features or both; number of topK concepts selected for annotations. The configurations of all runs are given in Table <ref type="table" coords="5,419.13,525.61,3.87,8.74" target="#tab_0">1</ref>, where runs are arranged according to their original name to ease the flow of description.</p><p>Here, run 1, 2, and 3 are employed based on both textual and visual features. However, run 4 and 5 are constructed based on the textual features of trained data, because both train and test dataset are same. In Table . 2, evaluation results of our submitted runs are illustrated. It reveals that "run 4" produces the best performance in terms of mean average precision (MAP), although we did not use any visual features in this run. It shows the effectiveness of our preprocessing stage of training data. However, the performance of "run 1", "run 2", and "run 3" are not satisfactory. It turns out that feature-wise learning of several visual features is not effective, although we could not afford to process all visual features including CNN and SIFT variants due to </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we described the participation of KDEVIR at ImageCLEF 2015 Scalable Concept Image Annotation task, where we proposed an approach for annotating images using ontologies at several phases of supervised learning from large scale noisy training data.</p><p>The evaluation result reveals that our proposed approach achieved an average performance among all submitted runs in terms of MAP 0.5 and MAP 0 overlap measures. However, in some runs, our system performance is not satisfactory. We could not afford to process all visual features due to time constraint. In future, we will consider deep learning to detect concepts in the noisy web images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,134.76,128.33,345.85,206.85"><head>Table 1 :</head><label>1</label><figDesc>Configurations of our submitted runs. The run pairs Run {1, 2, and 3 were conducted to show the effectiveness of using ontology and visual features with Histogram intersection kernel; while, the run pairs Run {4 and 5} were conducted to show the effect of data preprocessing using ontology and weighting methods. Either, we need more effective visual features or more efficient kernel and ensemble methods to boost up the performance of image annotation. Details about all the performance measures are given in<ref type="bibr" coords="6,382.04,326.44,14.61,8.74" target="#b14">[15]</ref>.</figDesc><table coords="6,134.76,198.45,337.68,112.81"><row><cell cols="2">Run Ontology?</cell><cell>Visual Feature</cell><cell>Kernel m topK</cell></row><row><cell>Run 1</cell><cell>Yes</cell><cell>ColorHist, GETLF, GIST</cell><cell>HIK 3000 10</cell></row><row><cell>Run 2</cell><cell>Yes</cell><cell>ColorHist, GETLF, GIST</cell><cell>HIK 3000 15</cell></row><row><cell>Run 3</cell><cell>Yes</cell><cell cols="2">OpponentSIFT, ColorHist, GETLF, GIST HIK 3000 15</cell></row><row><cell>Run 4</cell><cell>No</cell><cell>Textual Feature</cell><cell>No 1000 20</cell></row><row><cell>Run 5</cell><cell>Yes</cell><cell>Textual Feature</cell><cell>No 1000 20</cell></row><row><cell cols="2">time constrain.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,134.76,371.59,345.84,98.90"><head>Table 2 :</head><label>2</label><figDesc>Evaluation results of our submitted runs in terms of MAP 0.5 overlap and MAP 0 overlap.</figDesc><table coords="6,211.98,405.81,191.40,64.68"><row><cell cols="3">Run MAP 0.5 Overlap MAP 0 Overlap</cell></row><row><cell>Run 1</cell><cell>0.019876</cell><cell>0.048681</cell></row><row><cell>Run 2</cell><cell>0.021726</cell><cell>0.050720</cell></row><row><cell>Run 3</cell><cell>0.024631</cell><cell>0.055277</cell></row><row><cell>Run 4</cell><cell>0.228856</cell><cell>0.386693</cell></row><row><cell>Run 5</cell><cell>0.14093</cell><cell>0.305518</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,144.73,657.80,244.77,7.86"><p>http://www.mpi-inf.mpg.de/yago-naga/javatools/index.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This research was partially supported by the <rs type="funder">HORI FOUNDATION of JAPAN</rs>, <rs type="grantName">Grant-in-Aid</rs> <rs type="grantNumber">C114</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_zCqHM94">
					<idno type="grant-number">C114</idno>
					<orgName type="grant-name">Grant-in-Aid</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,142.95,204.69,337.63,7.86;7,151.52,215.64,329.06,7.86;7,151.52,226.60,329.06,7.86;7,151.52,237.56,140.28,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,363.61,204.69,116.98,7.86;7,151.52,215.64,280.95,7.86">Fast multi-class image annotation with random windows and multiple output randomized trees</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dumont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Marée</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wehenkel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Geurts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,459.19,215.64,21.39,7.86;7,151.52,226.60,329.06,7.86">Proc. International Conference on Computer Vision Theory and Applications (VISAPP)</title>
		<meeting>International Conference on Computer Vision Theory and Applications (VISAPP)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="196" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.95,248.01,337.64,7.86;7,151.52,258.94,329.06,7.89;7,151.52,269.93,32.25,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,282.64,248.01,197.95,7.86;7,151.52,258.97,117.84,7.86">Parallelizing multiclass support vector machines for scalable image annotation</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">K</forename><surname>Alham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,278.51,258.97,147.36,7.86">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="367" to="381" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.95,280.37,337.63,7.86;7,151.52,291.30,170.88,7.89" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,226.87,280.37,249.52,7.86">Incorporating multiple svms for automatic image annotation</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,151.52,291.33,81.43,7.86">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="728" to="741" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.95,301.78,337.63,7.86;7,151.52,312.71,240.98,7.89" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,285.68,301.78,194.90,7.86;7,151.52,312.74,30.27,7.86">Content-based image classification using a neural network</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">B</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,190.18,312.74,112.87,7.86">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="287" to="300" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.95,323.19,337.63,7.86;7,151.52,334.15,329.06,7.86;7,151.52,345.11,329.07,7.86;7,151.52,356.06,86.13,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,275.31,323.19,205.27,7.86;7,151.52,334.15,260.77,7.86">A novel approach to auto image annotation based on pairwise constrained clustering and semi-naïve bayesian model</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,434.25,334.15,46.33,7.86;7,151.52,345.11,86.31,7.86;7,270.62,345.11,206.05,7.86">MMM 2005. Proceedings of the 11th International</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="page" from="322" to="327" />
		</imprint>
	</monogr>
	<note>Multimedia Modelling Conference</note>
</biblStruct>

<biblStruct coords="7,142.95,366.51,337.63,7.86;7,151.52,377.47,329.06,7.86;7,151.52,388.43,269.59,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,287.16,366.51,193.43,7.86;7,151.52,377.47,169.58,7.86">Image content annotation using bayesian framework and complement components analysis</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Fotouhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,340.79,377.47,67.49,7.86;7,437.80,377.47,42.78,7.86;7,151.52,388.43,136.10,7.86">ICIP 2005. IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">1193</biblScope>
		</imprint>
	</monogr>
	<note>Image Processing</note>
</biblStruct>

<biblStruct coords="7,142.95,398.88,337.64,7.86;7,151.52,409.84,170.74,7.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="7,311.02,398.88,169.57,7.86;7,151.52,409.84,136.69,7.86">Automatic image annotation and retrieval using crossmedia relevance models</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.95,420.28,337.63,7.86;7,151.52,431.24,274.74,7.86" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4894</idno>
		<title level="m" coord="7,356.43,420.28,124.15,7.86;7,151.52,431.24,112.03,7.86">Deep convolutional ranking for multilabel image annotation</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,142.95,441.69,337.64,7.86;7,151.52,452.62,202.92,7.89" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,289.44,441.69,191.15,7.86;7,151.52,452.65,23.96,7.86">A review on automatic image annotation techniques</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,183.56,452.65,81.43,7.86">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="346" to="362" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,463.10,337.97,7.86;7,151.52,474.05,329.06,7.86;7,151.52,485.01,325.66,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,332.43,463.10,148.15,7.86;7,151.52,474.05,228.15,7.86">Hierarchical clustering of www image search results using visual, textual and link information</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,404.74,474.05,75.84,7.86;7,151.52,485.01,232.39,7.86">Proceedings of the 12th annual ACM international conference on Multimedia</title>
		<meeting>the 12th annual ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="952" to="959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,495.46,337.97,7.86;7,151.52,506.39,185.54,7.89" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,300.78,495.46,142.95,7.86">Training highly multiclass classifiers</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,450.23,495.46,30.35,7.86;7,151.52,506.42,121.70,7.86">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1" to="48" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,516.87,337.97,7.86;7,151.52,527.80,295.23,7.89" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,295.46,516.87,185.12,7.86;7,151.52,527.83,137.11,7.86">Large scale image annotation: learning to rank with joint word-image embeddings</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,297.24,527.83,69.14,7.86">Machine learning</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="35" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,538.27,337.97,7.86;7,151.52,549.23,329.06,7.86;7,151.52,560.19,236.17,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,335.48,538.27,145.10,7.86;7,151.52,549.23,36.88,7.86">Annosearch: Image auto-annotation by search</title>
		<author>
			<persName coords=""><forename type="first">X</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,212.64,549.23,171.88,7.86">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1483" to="1490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,570.64,337.97,7.86;7,151.52,581.60,329.06,7.86;7,151.52,592.56,329.06,7.86;7,151.52,603.52,329.06,7.86;7,151.52,614.47,126.65,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="7,396.68,592.56,83.90,7.86;7,151.52,603.52,142.38,7.86">General Overview of ImageCLEF at the CLEF 2015 Labs</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bromuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">K</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Uskudarli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">B</forename><surname>Marvasti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Aldana</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Del Mar Roldán</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>García</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="7,301.18,603.52,138.64,7.86">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,624.92,337.97,7.86;7,151.52,635.88,329.06,7.86;7,151.52,646.84,329.06,7.86;7,151.52,657.80,324.59,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="7,235.20,635.88,245.37,7.86;7,151.52,646.84,170.17,7.86">Overview of the ImageCLEF 2015 Scalable Image Annotation, Localization and Sentence Generation task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dellandrea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,342.98,646.84,137.60,7.86;7,151.52,657.80,89.74,7.86">CLEF2015 Working Notes. CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Toulouse, France, CEUR-WS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-11">September 8-11 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.61,120.67,337.99,7.86;8,151.52,131.60,323.65,7.89" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="8,213.38,120.67,267.21,7.86;8,151.52,131.63,33.60,7.86">Toward principles for the design of ontologies used for knowledge sharing?</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">R</forename><surname>Gruber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,189.20,131.63,196.51,7.86">International journal of human-computer studies</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="907" to="928" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.61,142.59,337.97,7.86;8,151.52,153.52,253.62,7.89" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="8,323.80,142.59,156.79,7.86;8,151.52,153.55,32.07,7.86">Knowledge engineering: principles and methods</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">R</forename><surname>Benjamins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fensel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,192.26,153.55,123.29,7.86">Data &amp; knowledge engineering</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="161" to="197" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.61,164.51,337.97,7.86;8,151.52,175.47,274.77,7.86" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="8,310.04,164.51,170.54,7.86;8,151.52,175.47,274.77,7.86">Kdevir at imageclef 2014 scalable concept image annotation task: Ontology based automatic image annotation</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">A</forename><surname>Reshma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">Z</forename><surname>Ullah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Aono</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.61,186.43,337.98,7.86;8,151.52,197.36,81.77,7.89" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="8,205.10,186.43,152.19,7.86">Wordnet: a lexical database for english</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,364.05,186.43,116.54,7.86">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.61,208.35,337.98,7.86;8,151.52,219.30,329.06,7.86;8,151.52,230.26,329.06,7.86;8,151.52,241.22,13.82,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="8,246.14,208.35,234.45,7.86;8,151.52,219.30,43.11,7.86">Sentiment learning on product reviews via sentiment ontology tree</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Gulla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,219.83,219.30,260.75,7.86;8,151.52,230.26,279.90,7.86">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="404" to="413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.61,252.18,337.97,7.86;8,151.52,263.14,329.06,7.86;8,151.52,274.10,32.25,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="8,373.36,252.18,107.22,7.86;8,151.52,263.14,172.97,7.86">Okapi at trec-7: automatic ad hoc, filtering, vlc and interactive track</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Willett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,335.66,263.14,115.17,7.86">Nist Special Publication SP</title>
		<imprint>
			<biblScope unit="page" from="253" to="264" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.61,285.06,337.98,7.86;8,151.52,295.99,305.80,7.89" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="8,256.42,285.06,190.56,7.86">Libsvm: a library for support vector machines</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,458.83,285.06,21.76,7.86;8,151.52,296.02,241.62,7.86">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.61,306.98,337.97,7.86;8,151.52,317.90,98.27,7.89" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="8,272.16,306.98,58.70,7.86">Color indexing</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Swain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">H</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,342.17,306.98,138.41,7.86;8,151.52,317.93,23.34,7.86">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="32" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
