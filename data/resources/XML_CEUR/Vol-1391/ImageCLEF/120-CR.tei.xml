<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,162.02,114.60,291.41,15.06;1,174.35,132.53,266.73,15.06">BUAA-iCC at ImageCLEF 2015 Scalable Concept Image Annotation Challenge</title>
				<funder ref="#_4TrCt6M">
					<orgName type="full">HongKong, Macao and Taiwan Science &amp; Technology Cooperation Program of China</orgName>
				</funder>
				<funder ref="#_xyTNy2J">
					<orgName type="full">Fundamental Research Funds for the Central University in UIBE</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,236.63,180.86,67.00,10.46"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
							<email>yhwang@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Intelligent Recognition and Image Processing Lab</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,326.32,180.86,52.46,10.46"><forename type="first">Jiaxin</forename><surname>Chen</surname></persName>
							<email>chenjiaxinx@gmail.com.</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Intelligent Recognition and Image Processing Lab</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,247.90,245.16,57.45,10.46"><forename type="first">Ningning</forename><surname>Liu</surname></persName>
							<email>ningning.liu@uibe.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Information Technology and Management</orgName>
								<orgName type="institution">University of International Business and Economics</orgName>
								<address>
									<postCode>100029</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,328.05,245.16,39.44,10.46"><forename type="first">Li</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information Technology and Management</orgName>
								<orgName type="institution">University of International Business and Economics</orgName>
								<address>
									<postCode>100029</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,162.02,114.60,291.41,15.06;1,174.35,132.53,266.73,15.06">BUAA-iCC at ImageCLEF 2015 Scalable Concept Image Annotation Challenge</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">116DE451EF8CFC999A89E9F0837DB712</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Textual similarity information</term>
					<term>visual similarity information</term>
					<term>deep convolutional neural network (CNN)</term>
					<term>face and facial attribute detection</term>
					<term>late fusion</term>
					<term>ImageCLEF</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this working note, we mainly focus on the image annotation subtask of ImageCLEF 2015 challenge that BUAA-iCC research group participated. For this task, we firstly explore textual similarity information between each test sample and predefined concept. Subsequently, two different kinds of semantic information are extracted from visual images: visual tags using generic object recognition classifiers and visual tags relevant to human being related concepts. For the former information, the visual tags are predicted by using deep convolutional neural network (CNN) and a set of support vector machines trained on ImageNet, and finally transferred to textual information. For the latter visual information, human related concepts are extracted via face and facial attribute detection, and finally transferred to similarity information by using manually designed mapping rules, in order to enhance the performance of annotating human related concepts. Meanwhile, a late fusion strategy is developed to incorporate aforementioned various kinds of similarity information. Results validate that the combination of the textual and visual similarity information and the adopted late fusion strategy could yield significantly better performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.27" lry="841.82"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>For the ImageCLEF 2015 Scalable Concept Image Annotation Challenge <ref type="bibr" coords="1,470.10,607.58,10.52,10.46" target="#b0">[1,</ref><ref type="bibr" coords="1,134.77,619.54,7.01,10.46" target="#b1">2]</ref>, we aim to develop a scalable image annotation approach, which could also yield high performance.</p><p>As shown in Fig. <ref type="figure" coords="1,222.45,643.49,4.13,10.46" target="#fig_0">1</ref>, our proposed framework mainly consists of three components: exploration of textual similarity information between each testing sample For textual similarity information, we directly utilize the path <ref type="bibr" coords="2,425.70,446.47,12.58,10.46" target="#b3">[4]</ref> semantic distance that is based on WordNet <ref type="bibr" coords="2,286.31,458.42,15.04,10.46" target="#b4">[5]</ref> ontology to construct textual similarity matrix SimM at text, of which the element in row i, column j indicates the similarity score between the i -th sample and the j -th predefined concept.</p><p>For visual similarity information, two different kinds of semantical information from visual images are firstly extracted: visual tags using generic objection recognition classifier and human-being related visual tags. As to the former one, a deep convolutional neural network is trained to extract discriminative features, and a set of support vector machine are trained on on ImageNet <ref type="bibr" coords="2,424.84,547.85,9.97,10.46" target="#b5">[6]</ref>, based on which each visual image from ImageCLEF 2015 testing data are tagged by using object categories with top 5 probabilities. Through this way, a given visual image is then transferred into textual semantic information, based on which one kind of visual similarity matrices SimM at vis could be calculated by following the same method for constructing SimM at text. As to the latter one, we use existing face and facial attribute detectors <ref type="bibr" coords="2,305.69,619.58,15.50,10.46" target="#b16">[17]</ref> to obtain the following information from visual images: the number of faces, facial age and facial gender, in order to enhance the image annotation of human related concepts such as 'female-child ', 'man' and 'woman'. Subsequently, these face and facial attribute detection re-sults are transferred into another kind of visual similarity matrix SimM at f ace via manually designed mapping rules.</p><p>Each kind of similarity information could individually yield an image annotation result. However, in order to further enhance the performance of the proposed annotation method, a late fusion strategy is adopt to learn the optimal weight coefficient using develop set. Results demonstrate that the incorporation of the proposed textual and visual similarity information could boost the performance, and the late fusion strategy could further enhance the annotation accuracy compared with trivial fusion schemes.</p><p>The remainder of this working notes is organized as follows. Section 2 describes the details of the textual/visual similarity information extraction, and late fusion strategy of fusion multiple modal similarity information. In Section 3, we summarize the implementation details of the submitted runs, and demonstrate the experimental results together with the corresponding experimental analysis. Finally, in section 4, we draw the conclusion of this working note.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The proposed Framework</head><p>In this section, we will elaborate the details of each components. We firstly describe the textual similarity information exploration in Section 2.1, followed by the visual similarity information exploration via deep convolutional neural networks in Section 2.2. In section 2.3, we present the visual similarity information exploration by using face and facial attribute detection, and finally detail the late fusion strategy of fusing multiple sources of similarity information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Textual Similarity Information Exploration</head><p>As provided by ImageCLEF 2015, each sample is composed by a visual image and corresponding web textual xml file. Textual similarity is therefore an important information for image annotation. In our submission, the textual similarity is explored by the strategy elaborated in Algorithm 1.</p><p>Specifically, given the textual description of the textual xml files of all 500000 test samples {W i } 500000 i=1 and the predefined 251 concepts {D j } 251 j=1 , our goal is to calculate the similarity score s ij between the i -th sample W i to the j -th concept D j , which finally consists of the similarity matrix SimM at text with SimM at text(i, j) = s ij . s ij is calculated as the following:</p><formula xml:id="formula_0" coords="3,134.77,551.21,236.60,53.76">s ij = Ni k=1 Nj m=1 dist(w i,k , d j,m ), where W i = {w i,k } Ni k=1 and D j = {d j,m } Nj m=1 .</formula><p>In our implementation, we follow the method depicted in <ref type="bibr" coords="3,417.45,604.06,10.52,10.46" target="#b2">[3]</ref> and utilize path <ref type="bibr" coords="3,151.54,616.02,12.58,10.46" target="#b3">[4]</ref> distance that is based on the WordNet ontology <ref type="bibr" coords="3,381.04,616.02,12.99,10.46" target="#b4">[5]</ref> to measure the semantic similarity dist(w i,k , d j,m ) between two synsets w i,k and d j,m :</p><formula xml:id="formula_1" coords="3,185.80,644.01,243.76,24.93">dist(w i,k , d j,m ) = d path (w i,k , d j,m ) = 1 1 + spl(w i,k , d j,m ) ,</formula><p>where spl(w i,k , d j,m ) returns the distance of the shortest path linking the two synsets (if one exists). Finally, we normalized the similarity matrix SimM at text to [0,1] as the following:</p><formula xml:id="formula_2" coords="4,166.60,174.81,282.15,11.36">SimM at text(i, j) = (SimM at text(i, j) -v min )/(v max -v min ),</formula><p>where v max = max i,j SimM at text(i, j) and v min = min i,j SimM at text(i, j).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 (Textual Similarity Computation)</head><p>Input: textual xml file {Wi} 500000 i=1 of the complete samples; textual description {Dj } 251 j=1 of predefined concepts Output: A similarity matrix SimM at text ∈ R 500000×251 , of which the element in the i -th row and j -th column is the similarity score between the textual file of each sample and the predefined concept Steps:</p><p>1. Preprocess {Wi} 500000 i=1 and {Dj } 251 j=1 by using a stop-words filter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">For each textual description of concept i = 1 to 50000</head><p>For each textual xml file of sample j = 1 to 251</p><p>SimM at text(i, j)</p><formula xml:id="formula_3" coords="4,157.34,318.35,206.65,36.58">= N i k=1 N j m=1 dist(w i,k , dj,m), end end where Wi = {w i,k } N i</formula><p>k=1 and Dj = {dj,m} N j m=1 , and dist(w i,k , dj,m) is the semantic similarity distance defined by WordNet.</p><p>3. Calculate the maximal value vmax and minimal value vmin of SimM at text, and normalize SimM at text: SimM at text(i, j) = (SimM at text(i, j) -vmin)/(vmax -vmin).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Visual Similarity Exploration Based on Objection Recognition Via Deep Convolutional Neural Network</head><p>In the past few years, significant progress in generic visual object recognition has been achieved, by virtue of the availability of large scale datasets such as ImageNet <ref type="bibr" coords="4,193.79,476.12,10.52,10.46" target="#b5">[6]</ref> and advances in recognition algorithms such as deep learning <ref type="bibr" coords="4,134.77,488.08,10.52,10.46" target="#b6">[7,</ref><ref type="bibr" coords="4,146.94,488.08,7.01,10.46" target="#b7">8]</ref>. Current visual recognition systems based on deep learning are capable of recognizing thousands of object categories with promising accuracy. For instance, by using the deep convolutional neural network (CNN), He et al. <ref type="bibr" coords="4,452.52,511.99,10.52,10.46" target="#b8">[9]</ref> has reduced the visual recognition error rate on ImageNet 2012 dataset to 4.94%, which has amazingly surpassed human-level performance (with error rate 5.1% for comparison).</p><p>It is therefore reasonable to adopt deep CNN trained on ImageNet to help automatically annotate a visual image with a list of terms representing concepts depicted in the image. In our proposed framework, we follow the similar way as depicted in <ref type="bibr" coords="4,185.09,595.67,15.50,10.46" target="#b10">[11,</ref><ref type="bibr" coords="4,202.24,595.67,12.73,10.46" target="#b11">12]</ref> for visual objection recognition using deep CNN. Specifically, we use the 1,571,576 ImageNet images in the 1,372 synsets as our training set, and the 500,000 images in the image annotation task of imageCLEF 2015 as our test set. For images with different sizes, we uniformly wrapped all training and testing images into 256x256. For each image in both sets, we extracted activation of a pre-trained CNN model as its feature. The model is a reference implementation of the structure proposed in Krizhevsky et al. <ref type="bibr" coords="5,416.78,118.33,10.52,10.46" target="#b6">[7]</ref> with minor modifications, and is made publicly available through the Caffe project <ref type="bibr" coords="5,448.26,130.28,14.62,10.46" target="#b12">[13]</ref>.</p><p>Once the feature is extracted for both training and test sets, 1,372 binary classifiers are trained and applied using LIBSVM <ref type="bibr" coords="5,349.22,154.73,14.62,10.46" target="#b13">[14]</ref>, which give probability estimates for the test images. For each image, the 1,372 classifiers are then ranked in order of their probability estimates. In order to reliably capture the semantic information contained in the test image, we only choose the categories with top 5 probabilities, through which visual images are finally transferred into textual tags {T i } 500000 i=1 . We therefore could construct visual similarity information SimM at vis ∈ R 500000×251 via the same way as the textual xml file, i.e., by using Algorithm 1 in Section 2.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Visual Similarity Exploration Via Face and Facial Attribute Detection</head><p>Human being is one of the most frequently occurred objects in visual images, of which face is one of the most representive and critical biometrics. Recent years have seen the substantial progress in face detection, face recognition together with facial attribute recognition. For example, on the largest unconstrained face dataset Label Face in the Wild (LFW) <ref type="bibr" coords="5,308.38,353.36,14.62,10.46" target="#b14">[15]</ref>, the most state-of-the-art approach <ref type="bibr" coords="5,134.77,365.31,15.50,10.46" target="#b15">[16]</ref> has archived 99.63% accuracy using deep learning. Many open access cloud planform for face recognition have also merged, such as Face++ <ref type="bibr" coords="5,409.03,377.27,14.62,10.46" target="#b16">[17]</ref>, which could provide free API services for face and facial attributes (such as age, gender, race and etc.) detection.</p><p>In our framework, we assume that the face and facial attribute detection could explore useful semantic information from visual images. Considering its promising performance <ref type="bibr" coords="5,237.86,437.58,15.50,10.46" target="#b17">[18,</ref><ref type="bibr" coords="5,255.02,437.58,12.73,10.46" target="#b18">19]</ref> and open access, we adopt Face++ as a tool for face and facial attribute detection, and finally utilize the following three results to enhance performance of visual image automatic annotation:</p><p>1). F A num f ace : number of face detected.</p><p>2). F A age : age of each detected face.</p><p>3). F A gender : gender of each detected face.</p><p>It should be noted that F A num f ace ∈ N, F A age ∈ N, F A gender ∈ {'female', 'male'}, where F A num f ace and F A age are numeric variables, and could not be directly used for image annotation. Here, we manually design a mapping from F A num f ace , F A age , F A gender to the similarity score between each visual image and the predefined 251 concepts.</p><p>Specifically, we firstly select a concept subset C 14 containing 14 concepts related to human being from the complete 251 concept set C All : C 14 = { 'arm', 'eye', 'face', 'female-child ', 'foot', 'hair ', 'head ', 'leg', 'male', 'man', 'mouth', 'neck ', 'nose', 'woman' }. For the i-th input visual image, we obtain F A num f ace , F A age and F A gender using Face++. Subsequently, we calculate the 251 dimensional vector s i describing the similarities between the visual image and C All as the following: if C All (j) / ∈ C 14 , s(i, j) = 0; if C All (j) ∈ C 14 , s(i, j) is evaluated according to F A num f ace , F A age , F A gender and the mapping rules described in Table <ref type="table" coords="6,162.41,118.33,3.88,10.46">1</ref>. For instance, assuming that the face and facial attribute detection results of the i -th visual image are F A</p><formula xml:id="formula_4" coords="6,300.11,129.03,100.87,15.50">(i) num f ace , F A (i) age , F A (i)</formula><p>gender and the j -th concept C all (j) is 'men', s(i, j) = 1, if F A (i) num f ace &gt; 0 and F A (i) gender = male , and s(i, j) = 0 otherwise. It is worth noting that different concepts in C 14 are assigned different values according to their prior tightness to facial attributes. Considering that { 'eye', 'face', 'female-child ', 'male', 'man', 'mouth', 'nose', 'woman' } could be determined by the face and facial attribute with high confidence, they are assigned the highest similarity score 1 once the condition is satisfied as described in Tabel 1. { 'arm', 'hair ', 'head ', 'leg', 'neck ' } are assigned similarity score 0.8, since they are less closely related to facial attributes. { 'foot' } is evaluated 0.6, which is further less relevant to facial attributes.</p><p>For each visual images of the 500000 testing samples, we could calculate a 251 dimensional similarity score vector, which finally consists the similarity score matrix SimM at f ace ∈ R 500000×251 .</p><p>Table <ref type="table" coords="6,162.50,304.16,4.13,9.41">1</ref>. The mapping rules of transferring face and facial detection results to similarity scores between input visual image and selected 14 human related concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concept</head><p>Description of mapping rule 'male' 1, if F A num f ace &gt; 0 and F A gender == male ; 0, otherwise.</p><formula xml:id="formula_5" coords="6,139.22,341.58,250.24,81.89">'arm' 0.8, if F A num f ace &gt; 0; 0, if F A num f ace = 0 'eye' 1, if F A num f ace &gt; 0; 0, if F A num f ace = 0. 'face' 1, if F A num f ace &gt; 0; 0, if F A num f ace = 0. 'female-child' 1, if F A num f</formula><p>'man' 1, if F A num f ace &gt; 0 and F Aage &gt; 18 and F A gender = male ; 0, otherwise.</p><formula xml:id="formula_6" coords="6,150.75,564.74,238.71,65.01">'mouth' 1, if F A num f ace &gt; 0; 0, otherwise. 'neck' 0.8, if F A num f ace &gt; 0; 0, otherwise. 'nose' 1, if F A num f ace &gt; 0; 0, otherwise.</formula><p>'woman' 1, if F A num f ace &gt; 0 and F Aage &gt; 18 and F A gender = f emale ; 0, otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Late Fusion of Various Similarity Information</head><p>Until now, we have obtained three different kinds of similarity matrices: SimM at text, SimM at vis, SimM at f ace, each of which could yield an individual image annotation result. For instance, we could sort each row of SimM at text, and select the concepts with top k similarity scores as the final annotations. However, SimM at text, SimM at vis, SimM at f ace are three different sources of similarity information between each sample and concept. It could be expected that better performance could be archived by fusing these three similarity matrices.</p><p>In our submission, we adopt similar late fusion scheme proposed by <ref type="bibr" coords="7,443.32,232.92,9.97,10.46" target="#b2">[3]</ref>. Generally, given K different similarity matrix {SimM at i} K i=1 , the overall similarity matrix SimM at f inal is a weighted sum of {SimM at i} K i=1 as follows:</p><formula xml:id="formula_7" coords="7,226.02,277.47,163.33,31.53">SimM at f inal = K i=1 w i * SimM at i,</formula><p>where K i=1 w i = 1 and w i ≥ 0. In our implementation, the optimal weights {w i } K i=1 are determined by using the Selective Weighted Late Fusion (SWLF) algorithm <ref type="bibr" coords="7,383.54,343.55,10.52,10.46" target="#b2">[3]</ref> on the develop set with 1980 annotated samples.</p><p>After obtained the final similarity matrix SimM at f inal, we could assign the label to each sample. In this submission, we mainly adopt two different schemes:</p><p>Annotation scheme 1 : we select concepts with top N similarity scores as the final annotations;</p><p>Annotation scheme 2 : we select concepts, of which the similarity score is greater than the given threshold T , as the final annotations.</p><p>The optimal N and T could be chosen by maximizing the Mean Average Precision (MAP) on the develop set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BUAA-iCC Runs and Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Description of Submissions</head><p>We submitted total ten runs, which are differ from similarity information fused, fusion strategies, and annotation schemes. The brief description of each submission are summarized as follows:</p><p>IRIP-iCC 01 : SimMat final=SimMat vis; adopt Annotation scheme 1, where N = 6 is chosen by maximizing MAP on develop set.</p><p>IRIP-iCC 02 : SimMat final=SimMat text; adopt Annotation scheme 1, where N = 6 is chosen by maximizing MAP on develop set.</p><p>IRIP-iCC 03 : SimMat final=SimMat text+SimMat vis+SimMat face; adopt Annotation scheme 1, where N = 6 is chosen by maximizing MAP on develop set.</p><p>IRIP-iCC 04 : SimMat final=SimMat text+SimMat vis+SimMat face; adopt Annotation scheme 1, where N = 7 is chosen manually. IRIP-iCC 10 : SimMat final=w*(SimMat text+SimMat vis)+(1-w)*SimMat face, where w = 0.6 is chosen by SWLF; adopt Annotation scheme 2, where T = 0.4 is chosen manually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IRIP-iCC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results of Submitted runs</head><p>As described in <ref type="bibr" coords="8,219.22,354.01,10.52,10.46" target="#b0">[1,</ref><ref type="bibr" coords="8,231.41,354.01,7.01,10.46" target="#b1">2]</ref>, the annotation accuracy together with the localization precision are evaluated by Mean Average Precision (MAP) with α overlap. For instance, mAP 0 overlap stands for MAP without considering localization overlap, and mAP 0.5 overlap stands for MAP with o.5 localization overlap. In our submission, we mainly focus on evaluate the performance of the proposed framework on annotation accuracy. So we mainly analyze the experimental results of mAP 0 overlap. For mAP 0.5 overlap, we simply use the objectness detector proposed in <ref type="bibr" coords="8,188.82,437.70,15.50,10.46" target="#b19">[20]</ref> for concept localization.</p><p>The experimental results are shown in Table <ref type="table" coords="8,359.16,449.65,3.88,10.46" target="#tab_1">2</ref>. From submissions IRIP-iCC 01, IRIP-iCC 02 and IRIP-iCC 03, we can see that visual similarity by using CNN performs better than textual similarity information extracted from xml file, and fusion of visual similarity information and textual similarity information could significantly boost the performance of each single similarity information. It also could be seen that IRIP-iCC 06 and IRIP-iCC 09 yield top 1 and top 2 mAPs, respectively, indicating that the weight w via late fusion strategy using SWLF could enhance the performance. The mAP of IRIP-iCC 09 is about 2% higher than IRIP-iCC 06, which could verify that the performance could be further boosted by choosing optimal threshold T via develop set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we described the participation of BUAA-iCC at ImageCLEF 2015 Scalable Concept Image Annotation Challenge. We proposed a novel image annotation framework by fusing textual similarity information, and visual similarity information explored by deep convolutional neural network (CNN), face and facial attribute detection. Experimental results reveals that the visual similarity information extracted by deep CNN, face and facial attribute detection could enhance the performance of the textual similarity information extracted from xml files. The similarity information fusion strategy using selective weighted late fusion could significantly boosts the performance. The annotation scheme by selecting concepts with similarity score larger than an automatically determined threshold, which maximizes the MAP of samples from develop set, yields better performance than other annotation schemes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,192.92,371.78,229.48,9.41;2,134.76,116.86,345.80,244.84"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The overall flowchart of the proposed framework.</figDesc><graphic coords="2,134.76,116.86,345.80,244.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,195.96,118.33,287.46,10.46;8,134.77,130.28,251.17,10.46;8,149.71,142.24,359.58,10.46;8,134.77,154.19,345.83,10.46;8,134.77,166.15,83.70,10.46;8,149.71,178.10,359.58,10.46;8,134.77,190.06,345.82,10.46;8,134.77,202.01,190.66,10.46;8,149.71,213.97,359.58,10.46;8,134.77,225.92,345.82,10.46;8,134.77,237.88,73.69,10.46;8,149.71,249.83,359.58,10.46;8,134.77,261.79,345.83,10.46;8,134.77,273.74,200.67,10.46"><head></head><label></label><figDesc>05 : SimMat final=SimMat text+SimMat vis+SimMat face; adopt Annotation scheme 1, where N = 251 is chosen manually. IRIP-iCC 06 : SimMat final=w*(SimMat text+SimMat vis)+(1-w)*SimMat face, where w = 0.6 is chosen by SWLF; adopt Annotation scheme 2, where T = 0.5 is chosen manually. IRIP-iCC 07 : SimMat final=w*(SimMat text+SimMat vis)+(1-w)*SimMat face, where w = 0.6 is chosen by SWLF; adopt Annotation scheme 1, where N = 6 is chosen by maximizing MAP on develop set. IRIP-iCC 08 : SimMat final=w*(SimMat text+SimMat vis)+(1-w)*SimMat face, where w = 0.6 is chosen by SWLF; adopt Annotation scheme 1, where N = 7 is chosen manually. IRIP-iCC 09 : SimMat final=w*(SimMat text+SimMat vis)+(1-w)*SimMat face, where w = 0.6 is chosen by SWLF; adopt Annotation scheme 2, where T = 0.6 is chosen by maximizing MAP on develop set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,193.74,101.26,227.87,136.35"><head>Table 2 .</head><label>2</label><figDesc>The results of our submitted runs. Submitted runs mAP 0 overlap(%) mAP 0.5 overlap(%)</figDesc><table coords="9,199.42,125.98,192.77,111.63"><row><cell>IRIP-iCC 01</cell><cell>22.98</cell><cell>10.72</cell></row><row><cell>IRIP-iCC 02</cell><cell>16.62</cell><cell>7.67</cell></row><row><cell>IRIP-iCC 03</cell><cell>51.40</cell><cell>14.58</cell></row><row><cell>IRIP-iCC 04</cell><cell>51.56</cell><cell>13.54</cell></row><row><cell>IRIP-iCC 05</cell><cell>43.03</cell><cell>4.64</cell></row><row><cell>IRIP-iCC 06</cell><cell>58.95</cell><cell>11.15</cell></row><row><cell>IRIP-iCC 07</cell><cell>50.65</cell><cell>14.43</cell></row><row><cell>IRIP-iCC 08</cell><cell>50.91</cell><cell>13.44</cell></row><row><cell>IRIP-iCC 09</cell><cell>60.92</cell><cell>11.96</cell></row><row><cell>IRIP-iCC 10</cell><cell>51.07</cell><cell>9.00</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This work was supported in part by the <rs type="funder">HongKong, Macao and Taiwan Science &amp; Technology Cooperation Program of China</rs> under the grant <rs type="grantNumber">L2015TGA9004</rs>, the <rs type="funder">Fundamental Research Funds for the Central University in UIBE</rs> under grant <rs type="grantNumber">14QD21</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_4TrCt6M">
					<idno type="grant-number">L2015TGA9004</idno>
				</org>
				<org type="funding" xml:id="_xyTNy2J">
					<idno type="grant-number">14QD21</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,138.35,512.73,342.08,9.41;9,146.91,523.69,333.54,9.41;9,146.91,534.64,333.58,9.41;9,146.91,545.60,314.01,9.41" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,263.76,534.64,212.40,9.41">General Overview of ImageCLEF at CLEF2015 Labs</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bromri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Uskudarli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Marvasti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Aldana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>García</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="9,146.91,545.60,141.39,9.41">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,556.93,342.13,9.41;9,146.91,567.89,333.62,9.41;9,146.91,578.85,122.62,9.41" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,336.60,556.93,143.88,9.41;9,146.91,567.89,285.99,9.41">Overview of the ImageCLEF 2015 Scalable Image Annotation, Localization and Sentence Generation task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,453.93,567.89,26.61,9.41;9,146.91,578.85,91.38,9.41">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,590.18,342.10,9.41;9,146.91,601.13,333.59,9.41;9,146.91,612.09,333.61,9.41;9,146.91,623.05,158.02,9.41" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,192.17,601.13,288.34,9.41;9,146.91,612.09,212.50,9.41">Multimodal recognition of visual concepts using histograms of textual concepts and selective weighted late fusion scheme</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dellandrea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">E</forename><surname>Bichot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Tellez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,364.18,612.09,116.35,9.41;9,146.91,623.05,57.35,9.41">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="493" to="512" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,634.38,342.16,9.41;9,146.91,645.34,333.62,9.41;9,146.91,656.30,162.93,9.41" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,277.20,634.38,203.30,9.41;9,146.91,645.34,195.99,9.41">Semantic distance in WordNet: An experimental, application-oriented evaluation of five measures</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Budanitsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,365.05,645.34,115.49,9.41;9,146.91,656.30,97.91,9.41">Workshop on WordNet and Other Lexical Resources</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,119.17,342.15,9.41;10,146.91,130.13,83.97,9.41" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,202.05,119.17,158.90,9.41">WordNet: a lexical database for English</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,365.49,119.17,108.93,9.41">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,141.09,342.10,9.41;10,146.91,152.05,333.62,9.41;10,146.91,163.01,252.89,9.41" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,405.79,141.09,74.66,9.41;10,146.91,152.05,134.87,9.41">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,302.60,152.05,173.74,9.41;10,170.97,163.01,134.52,9.41">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct coords="10,138.35,173.97,342.13,9.41;10,146.91,184.93,333.60,9.41;10,146.91,195.89,93.24,9.41" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,346.69,173.97,133.79,9.41;10,146.91,184.93,120.18,9.41">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,287.38,184.93,193.13,9.41;10,146.91,195.89,18.99,9.41">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,206.84,342.09,9.41;10,146.91,217.80,333.61,9.41;10,146.91,228.76,95.75,9.41" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<title level="m" coord="10,282.12,217.80,130.32,9.41">Going deeper with convolutions</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,138.35,239.72,342.15,9.41;10,146.91,250.68,333.62,9.41;10,146.91,261.64,100.36,9.41" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.01852</idno>
		<title level="m" coord="10,337.76,239.72,142.74,9.41;10,146.91,250.68,260.17,9.41">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,142.61,272.60,337.83,9.41;10,146.91,283.56,333.58,9.41;10,146.91,294.52,313.88,9.41" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0575</idno>
		<title level="m" coord="10,441.58,283.56,38.91,9.41;10,146.91,294.52,210.18,9.41">ImageNet Large Scale Visual Recognition Challenge (ILSVRC)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,305.47,337.85,9.41;10,146.91,316.43,333.61,9.41;10,146.91,327.39,156.99,9.41" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="10,167.91,316.43,308.68,9.41">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">L</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.1531</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,142.61,338.35,337.89,9.41;10,146.91,349.31,333.63,9.41;10,146.91,360.27,120.53,9.41" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,363.57,338.35,116.93,9.41;10,146.91,349.31,333.63,9.41;10,146.91,360.27,44.67,9.41">A Poodle or a Dog? Evaluating Automatic Image Annotation Using Human Descriptions at Different Levels of Granularity</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,198.70,360.27,37.50,9.41">V&amp;L Net</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,371.23,337.93,9.41;10,146.91,382.19,182.06,9.41" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<ptr target="http://caffe.berkeleyvision.org" />
		<title level="m" coord="10,185.02,371.23,295.53,9.41;10,146.91,382.19,15.97,9.41">Caffe: An open source convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,393.15,337.89,9.41;10,146.91,404.10,331.95,9.41" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,279.80,393.15,196.35,9.41">LIBSVM: a library for support vector machines</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,146.91,404.10,264.46,9.41">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,415.06,337.86,9.41;10,146.91,426.02,333.62,9.41;10,146.91,436.98,333.61,9.41;10,146.91,447.94,28.16,9.41" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="10,413.04,415.06,67.43,9.41;10,146.91,426.02,333.62,9.41">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>07-49</idno>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">1</biblScope>
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="10,142.61,458.90,337.88,9.41;10,146.91,469.86,203.31,9.41" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,343.63,458.90,136.87,9.41;10,146.91,469.86,121.99,9.41">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,289.83,469.86,29.17,9.41">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,480.82,337.87,9.41;10,146.91,491.78,20.99,9.41" xml:id="b16">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Megvii</forename><surname>Inc</surname></persName>
		</author>
		<ptr target="http://www.faceplusplus.com" />
		<title level="m" coord="10,202.96,480.82,102.36,9.41">Face++ Research Toolkit</title>
		<imprint>
			<date type="published" when="2013-12">December 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,502.73,337.88,9.41;10,146.91,513.69,281.29,9.41" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="10,279.66,502.73,200.84,9.41;10,146.91,513.69,112.22,9.41">Naive-Deep Face Recognition: Touching the Limit of LFW Benchmark or Not?</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1501.04690</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,142.61,524.65,337.88,9.41;10,146.91,535.61,186.67,9.41" xml:id="b18">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Doudou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1403.2802</idno>
		<title level="m" coord="10,366.40,524.65,114.09,9.41;10,146.91,535.61,22.82,9.41">Learning deep face representation</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,142.61,546.57,337.87,9.41;10,146.91,557.53,333.61,9.41;10,146.91,568.49,71.17,9.41" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="10,319.72,546.57,160.76,9.41;10,146.91,557.53,17.85,9.41">Measuring the objectness of image windows</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,173.26,557.53,270.95,9.41">Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2189" to="2202" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>IEEE Transactions on</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
