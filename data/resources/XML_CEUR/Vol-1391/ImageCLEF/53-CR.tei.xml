<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,183.62,116.95,248.12,12.62;1,225.07,134.89,165.22,12.62">Convolutional Neural Networks for Subfigure Classification</title>
				<funder ref="#_aRQaeB4">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,137.05,172.56,62.26,8.74"><forename type="first">David</forename><surname>Lyndon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Technologies</orgName>
								<orgName type="institution">University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,209.88,172.56,61.80,8.74"><forename type="first">Ashnil</forename><surname>Kumar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Technologies</orgName>
								<orgName type="institution">University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Biomedical Engineering and Technology</orgName>
								<orgName type="institution">University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,288.57,172.56,54.39,8.74"><forename type="first">Jinman</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Technologies</orgName>
								<orgName type="institution">University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Biomedical Engineering and Technology</orgName>
								<orgName type="institution">University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,359.84,172.56,85.51,8.74"><forename type="first">Philip</forename><forename type="middle">H W</forename><surname>Leong</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Electrical and Information Engineering</orgName>
								<orgName type="institution">University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Biomedical Engineering and Technology</orgName>
								<orgName type="institution">University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,276.26,184.51,52.03,8.74"><forename type="first">Dagan</forename><surname>Feng</surname></persName>
							<email>dagan.feng@sydney.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Technologies</orgName>
								<orgName type="institution">University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Biomedical Engineering and Technology</orgName>
								<orgName type="institution">University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,183.62,116.95,248.12,12.62;1,225.07,134.89,165.22,12.62">Convolutional Neural Networks for Subfigure Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">875B62F1944F107B3AAD53731413FECD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep Learning</term>
					<term>Convolutional Neural Networks</term>
					<term>Medical Image Retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A major challenge for Medical Image Retrieval (MIR) is the discovery of relationships between low-level image features (intensity, gradient, texture, etc.) and high-level semantics such as modality, anatomy or pathology. Convolutional Neural Networks (CNNs) have been shown to have an inherent ability to automatically extract hierarchical representations from raw data. Their successful application in a variety of generalised imaging tasks suggests great potential for MIR. However, a major hurdle to their deployment in the medical domain is the relative lack of robust training corpora when compared to general imaging benchmarks such as ImageNET and CIFAR. In this paper, we present the adaptation of CNNs to the subfigure classification subtask of the medical classification task at ImageCLEF 2015.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper documents the Biomedical Engineering and Technology (BMET) team from the University of Sydney's submissions for the ImageCLEF 2015 <ref type="bibr" coords="1,470.07,519.42,10.52,8.74" target="#b0">[1]</ref> Medical Classification task <ref type="bibr" coords="1,256.76,531.38,9.96,8.74" target="#b1">[2]</ref>. Specifically, BMET's work was directed at the Subfigure Modality Classification subtask.</p><p>The objective of our experiments was to evaluate the effectiveness of Convolutional Neural Networks (CNNs) for this subtask. In particular, we propose a deep learning framework that could learns high-level representations of different image modalities and use these to classify the modality of each subfigure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Convolutional Neural Networks, a type of deep learning algorithm, have been used to produce state-of-the-art results for a variety of machine learning tasks such as image recognition, acoustic recognition and natural language processing since 2012 <ref type="bibr" coords="2,184.75,131.95,7.75,8.74" target="#b2">[3]</ref><ref type="bibr" coords="2,192.50,131.95,3.87,8.74" target="#b3">[4]</ref><ref type="bibr" coords="2,196.38,131.95,7.75,8.74" target="#b4">[5]</ref>. CNNs share the common features of all deep learning algorithms: stacked layers of neuronal subunits that learn hierarchical representations (allowing the data to be understood at various levels of abstraction, in isolation or combination <ref type="bibr" coords="2,245.96,167.81,10.30,8.74" target="#b2">[3]</ref>), the ability to perform unsupervised pre-training on unlabeled data and efficient parallelization on multiple core GPUs which can result in improvements of up to 5000% over CPU-only implementations <ref type="bibr" coords="2,450.36,191.72,9.96,8.74" target="#b3">[4]</ref>.</p><p>A more subtle implication of deep learning is that it can automatically extract features from raw data <ref type="bibr" coords="2,241.35,216.24,7.75,8.74" target="#b2">[3]</ref><ref type="bibr" coords="2,249.10,216.24,3.87,8.74" target="#b3">[4]</ref><ref type="bibr" coords="2,252.98,216.24,7.75,8.74" target="#b4">[5]</ref>. Typically, a key factor in the success of typical machine learning algorithms is extracting salient features from the raw data. Taking image recognition as an example, a feature set such as edges or SIFT <ref type="bibr" coords="2,470.08,240.15,10.52,8.74" target="#b5">[6]</ref> would be extracted from the raw data and it is these new features per se or in combination with the original raw data that would be fed into the machine learning algorithm. While some aspects of the process can be automated or implemented with well known algorithms, a major drawback is that it generally requires expert domain knowledge to define which features should be used and evaluate their success.</p><p>Deep learning algorithms, however, are able to directly utilise raw data instead of hand-crafted features. By feeding the data sequentially through many successive layers of subunits, the higher levels of the system are able to understand the data in terms of successively abstract representations <ref type="bibr" coords="2,414.55,360.31,9.96,8.74" target="#b2">[3]</ref>.</p><p>Medical Image Retrieval (MIR) tasks, such as the tests devised for Image-CLEF, require learning precisely these kinds of highly abstract representations, i.e. image modality or the anatomical semantics of the image. However, to the best of our knowledge it is not currently a well established method in this domain. This is due to not only the inherent challenges of medical images <ref type="bibr" coords="2,423.64,420.69,12.80,8.74" target="#b6">[7]</ref>, but also because state-of-the-art deep learning results are typically obtained using huge sets of labelled training data<ref type="foot" coords="2,257.49,443.03,3.97,6.12" target="#foot_0">4</ref> on tasks that are arguably less subtle. As a justification for these claims, consider that the ImageNET general object recognition task corpora consists of millions of robustly labelled images and was created with the assistance of crowdsourcing via Amazon Mechanical Turk <ref type="bibr" coords="2,407.17,480.47,9.96,8.74" target="#b8">[9]</ref>. On the other hand, medical imaging datasets require careful labelling by domain experts, often specialists in a particular area <ref type="bibr" coords="2,287.16,504.38,10.52,8.74" target="#b6">[7,</ref><ref type="bibr" coords="2,299.34,504.38,12.73,8.74" target="#b9">10,</ref><ref type="bibr" coords="2,313.73,504.38,12.73,8.74" target="#b10">11]</ref> and as a result are generally much smaller.</p><p>Large training sets are a current necessity of very deep systems because they contain many millions of internal parameters that must be estimated from the data. Too little data can result in the the higher-level neurons' activation being the result of salient features of the training set and not reflecting the high-level representations. If this 'overfitting' occurs then the system's ability to generalise on new data is severely impaired <ref type="bibr" coords="2,280.77,588.67,14.61,8.74" target="#b11">[12]</ref>.</p><p>In addition to the issues regarding the volume of data required, it must be mentioned that while deep learning can automatically perform excellent feature extraction, this comes at the significant cost of the larger number of hyperpa-rameters that must be evaluated in order to find an optimal system <ref type="bibr" coords="3,443.60,119.99,14.61,8.74" target="#b12">[13]</ref>. For example, compared to a commonly used machine learning algorithm such as the Support Vector Machine (SVM) that has a basic hyperparameter search space with dimensions of choice of kernel, regularization constant and kernel hyperparameter, even the simplest implementation of a CNN requires fundamental choices about the number and type of layers, filter size and number of filters per layer, and the learning rate. More advanced implementations include factors such as unit activation function and the use of dropout. While there are guidelines for these choices in the literature <ref type="bibr" coords="3,331.72,215.63,14.61,8.74" target="#b12">[13]</ref>, the difficulty of even a small parameter search is compounded by the increased computational requirements of training the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Image Preprocessing</head><p>A requirement of our classifiers was uniformly sized input vectors, however, the supplied training data varied greatly in size. This was achieved by squarecropping the image to 500px, any dimension of the image smaller than 500px was filled with black pixels.</p><p>Even prior to training the CNN, we were aware that the computational requirements were quite demanding and this would be exacerbated by using large images. We resized the images to 160x160px to reduce the computational overhead that would have been required by using higher resolution images. Good results have been reported in the literature for complex tasks with 48x48px images <ref type="bibr" coords="3,173.43,446.27,15.50,8.74" target="#b13">[14]</ref> and Krizhevsky et. al. <ref type="bibr" coords="3,302.88,446.27,10.52,8.74" target="#b7">[8]</ref> achieved state-of-the art general object recognition with 256x256px images (technically, the system had an input of 224x224px, but these were subimages of the original 256x256px images).</p><p>After resizing the images were 160x160x3px, the third dimension describing the three colour channels. For the purposes of simplicity and to further reduce the computational requirements we reduced the 3 channel colour representation to a single channel (red).</p><p>We randomly divided the training data into a 70/30 split for training and validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Softmax Classification</head><p>We evaluated the effectiveness of CNN-derived features by comparing it to the results achieved by a Softmax classifier on the raw data. This experiment is important because the CNN's final layer is the input to a Softmax classifier. This experiment can therefore be used to quantify the effectiveness of the unsupervised feature extraction performed by the CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Convolutional Neural Network</head><p>The architecture for the CNN used for our experimentation was based on a simplified version of Yann LeCun et. al.'s <ref type="bibr" coords="4,317.29,160.14,15.50,8.74" target="#b14">[15]</ref> LeNet-5 5 . This CNN is capable of correctly classifying the MNIST handwritten digit database with 1.7% test error. We modified the input to account for larger images and output a greater number of classifications. The network consists of two convolutional pooling layers, with one fully connected hidden layer. The features that are output by the hidden layer are used for classification by a Softmax classifier. The architecture of the system is shown in Figure <ref type="figure" coords="4,251.11,231.87,3.87,8.74" target="#fig_0">1</ref>. The specifications of the convolutional-pooling layers are detailed in Table <ref type="table" coords="4,472.84,487.24,3.87,8.74" target="#tab_0">1</ref>. As mentioned earlier the CNN requires a great deal of computational resource to run. It took approximately 3.5 hours to train a single epoch for each model, while training two models simultaneously on the CPU of a powerful system<ref type="foot" coords="5,473.36,256.57,3.97,6.12" target="#foot_1">6</ref> . However, the models were not able to converge before the submission deadline. As such the runs that we submitted were based on only partially converged models. The details of the four runs submitted are detailed in Table <ref type="table" coords="5,435.05,294.01,3.87,8.74" target="#tab_2">3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Validation Results</head><p>The validation error for all runs is displayed in Table <ref type="table" coords="5,369.74,493.46,3.87,8.74" target="#tab_3">4</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Softmax Classification</head><p>The first Softmax model, trained for 1000 epochs, produced a 0% validation error. This was interpreted as being the result of severe over-fitting to the supplied training data. Despite the fact that this classification scheme was essentially a baseline to evaluate the performance of CNN-extracted representations over the raw data, it was thought prudent to perform second run, with less training and hopefully less overfitting, in order to see the results of a more general model. Thus, for sf run 5 we submitted the results of training the same model for only 155 epochs, this resulted in a 5.3% error rate on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Convolutional Neural Networks</head><p>The validation errors displayed in Table <ref type="table" coords="6,313.33,275.89,4.98,8.74" target="#tab_3">4</ref> for the CNN runs (sf run 2,3,4,6) demonstrated a clear correlation between the number of epochs they were trained for and increasing performance (decreasing validation error). This is demonstrated visually in Figure <ref type="figure" coords="6,248.26,311.76,3.87,8.74" target="#fig_1">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Test Results</head><p>The test results for the six runs as supplied by ImageCLEF are displayed in Table <ref type="table" coords="6,134.77,633.20,3.87,8.74" target="#tab_4">5</ref>. The CNNs demonstrated improved performance over the Softmax classification and their accuracy approximately corresponded to the amount of training that was performed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Softmax Classification</head><p>The test accuracy for both runs of the Softmax classifier were 37.56%. This indicates that despite cutting short the training for sf run 5 compared to sf run 1, both models had effectively the same representation of the data when it came to classifying the test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Convolutional Neural Networks</head><p>Compared to the validation results for the CNNs, the improvements with regard to the number of training epochs are not so clear-cut. For the model trained with learning rate of 0.005% there is a clear improvement between the test submitted at epoch 47 and the test submitted at epoch 55. However, the test accuracy decreased in the run submitted at epoch 59.</p><p>6 Analysis of Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Validation vs. Test variance</head><p>An examination of the validation error and test results in Tables <ref type="table" coords="7,434.17,477.75,4.98,8.74" target="#tab_3">4</ref> and<ref type="table" coords="7,464.34,477.75,4.98,8.74" target="#tab_4">5</ref> is very illuminating. Clearly the 70/30 training/validation method we applied was inappropriate in this case, as demonstrated by the significant variance between the validation and test performance. While it is possible that the test set was substantially different to the training set, it's more likely the 30% chosen for validation was not fully representative of the data. Given that the training data was not evenly distributed in all classes it is likely that the models overfit the data corresponding to the more common classes and that the validation set was heavily skewed towards the common classes. However, we still believe that CNNs are suitable for this task despite the evidence of overfitting in this case. Techniques for overcoming this issue are discussed in Section 7.</p><p>Having already pointed out the tremendous computational demands required by the CNNs, more robust validation procedures such as 10-fold cross-validation are clearly not feasible with the system employed in this test. That said, it may be possible to perform this kind of validation on a simpler model such as Softmax, in order to discover a more indicative training/validation split. Another option would be to take a more manual approach to splitting the sets, ensuring that all classes are evenly represented in the validation set.</p><p>It's worth noting that the models used for testing were only trained on the 70% training split. In future, we can expect better results by retraining the best model (based on some validation metric) on the entire dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">CNN Training</head><p>As alluded to out earlier, although the CNNs did not converge during training, they may have already begun to overfit the training data with the result that the test performance actually decreased for the model at epoch 59 compared to the model at epoch 55. However, this is not entirely certain, as it is also possible that the model at epoch 59 was a better fit for the validation data (table 4) at that point, but simultaneously a worse fit for the test data. Had the models been able to train for longer, we may have had a clearer indication of their true performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">CNN-Learnt Features</head><p>The CNNs were able to extract improved representations from raw data without the requirement for domain knowledge. This is an important result both for this task and for MIR generally as it suggests that there is potential in using CNN or other deep learning strategies as a 'black box', whereby we will be able to achieve excellent machine learning performance without the need of expertdesigned feature extraction or domain knowledge.</p><p>We would have liked to train the network further, but need to prematurely halt the system for the purposes of submission. We believe that additional training would yield a better result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Perspectives for Future Work</head><p>We believe that that these results can be significantly improved upon by making use of a variety of techniques. Primarily we would want to explore training the CNNs using GPUs, as this will allow us to expand our hyperparamter and architecture search. Rectified Linear Units (ReLUs), as opposed the Tanh units used in our network are also known to improve training performance <ref type="bibr" coords="8,437.82,561.14,15.50,8.74" target="#b15">[16,</ref><ref type="bibr" coords="8,454.98,561.14,11.62,8.74" target="#b16">17]</ref>.</p><p>Although this network is very capable of learning quality representations of the MNIST dataset, it is both less deep and less dense than networks used to achieve state-of-the-art results in more sophisticated tasks <ref type="bibr" coords="8,407.08,597.34,9.96,8.74" target="#b7">[8]</ref>. For instance, Krizhevsky et. al. <ref type="bibr" coords="8,216.42,609.29,10.52,8.74" target="#b7">[8]</ref> used a network with 2 convolutional-max pooling layers, 3 convolutional layers and 3 fully connected layers, all of which were more neurondense that ours, to achieve their result in ImageNET 2012. Improved training performance will allow us to implement a larger and deeper network along these lines.</p><p>Larger and deeper networks introduce issues with overfitting, but we believe this can be controlled using well-tried techniques such as dropout <ref type="bibr" coords="9,416.56,131.95,10.52,8.74" target="#b7">[8,</ref><ref type="bibr" coords="9,428.74,131.95,12.73,8.74" target="#b11">12,</ref><ref type="bibr" coords="9,443.12,131.95,11.62,8.74" target="#b17">18]</ref>, data augmentation <ref type="bibr" coords="9,197.58,143.90,10.52,8.74" target="#b7">[8,</ref><ref type="bibr" coords="9,209.76,143.90,12.73,8.74" target="#b18">19]</ref> and unsupervised pretraining <ref type="bibr" coords="9,356.63,143.90,15.50,8.74" target="#b19">[20,</ref><ref type="bibr" coords="9,373.78,143.90,11.62,8.74" target="#b20">21]</ref>.</p><p>Finally, the validation method we utilised for these experiments did not produce an accurate understanding of the performance of our systems. In approaching this task in future we would be careful to construct a more representative validation set or use the 2015 test data for validation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,190.47,450.15,234.42,7.89;4,136.23,266.60,342.90,168.78"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The architecture of CNN used for the experiments</figDesc><graphic coords="4,136.23,266.60,342.90,168.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,134.77,521.64,345.82,7.89;6,134.77,532.62,345.83,7.86;6,134.77,543.58,67.83,7.86;6,134.77,343.50,347.42,163.36"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The validation error of the models decreases as they approach convergence. The scatter points correspond to the epoch and validation error for each of the four test submissions.</figDesc><graphic coords="6,134.77,343.50,347.42,163.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,137.50,532.06,270.41,133.85"><head>Table 1 .</head><label>1</label><figDesc>Details of Convolutional Pooling LayersOther hyperparameters for the CNN are detailed in Table2.</figDesc><table coords="4,137.50,552.86,242.06,113.05"><row><cell cols="3">Hyperparameter Layer0 Layer1</cell></row><row><cell>Number of Filters</cell><cell>20</cell><cell>50</cell></row><row><cell cols="3">Size of Filters 15x15px 15x15px</cell></row><row><cell>Max Pooling</cell><cell>2x2</cell><cell>2x2</cell></row><row><cell>Stride</cell><cell>1</cell><cell>1</cell></row><row><cell cols="2">5 http://deeplearning.net/tutorial/lenet.html</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,208.45,150.43,198.46,53.16"><head>Table 2 .</head><label>2</label><figDesc>Other details for CNN</figDesc><table coords="5,208.45,169.49,198.46,34.10"><row><cell>Hyperparameter</cell><cell>Value</cell></row><row><cell cols="2">Number of Units in Fully Connected Layer 500</cell></row><row><cell>Batch Size</cell><cell>20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,202.35,322.52,210.67,98.99"><head>Table 3 .</head><label>3</label><figDesc>Run-specific details of all submissions.</figDesc><table coords="5,202.35,343.32,210.67,78.19"><row><cell cols="4">Submission Model Learning Rate Training Epochs</cell></row><row><cell cols="2">sf run 1 Softmax</cell><cell>0.05</cell><cell>1000</cell></row><row><cell>sf run 2</cell><cell>CNN</cell><cell>0.005</cell><cell>47</cell></row><row><cell>sf run 3</cell><cell>CNN</cell><cell>0.005</cell><cell>55</cell></row><row><cell>sf run 4</cell><cell>CNN</cell><cell>0.007</cell><cell>46</cell></row><row><cell cols="2">sf run 5 Softmax</cell><cell>0.05</cell><cell>1000</cell></row><row><cell>sf run 6</cell><cell>CNN</cell><cell>0.005</cell><cell>59</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,217.42,523.91,180.51,97.25"><head>Table 4 .</head><label>4</label><figDesc>Validation error of all submissions.</figDesc><table coords="5,250.59,542.96,114.18,78.19"><row><cell cols="2">Submission Validation Error</cell></row><row><cell>sf run 1</cell><cell>0.0%</cell></row><row><cell>sf run 2</cell><cell>13.85%</cell></row><row><cell>sf run 3</cell><cell>8.94%</cell></row><row><cell>sf run 4</cell><cell>10.53%</cell></row><row><cell>sf run 5</cell><cell>5.3%</cell></row><row><cell>sf run 6</cell><cell>6.53%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,206.72,116.91,201.91,98.99"><head>Table 5 .</head><label>5</label><figDesc>Test results as supplied by ImageCLEF.</figDesc><table coords="7,244.34,137.71,126.68,78.19"><row><cell cols="2">Submission Correctly Classified</cell></row><row><cell>sf run 1</cell><cell>37.56%</cell></row><row><cell>sf run 2</cell><cell>43.62%</cell></row><row><cell>sf run 3</cell><cell>45.63%</cell></row><row><cell>sf run 4</cell><cell>44.34%</cell></row><row><cell>sf run 5</cell><cell>37.56%</cell></row><row><cell>sf run 6</cell><cell>45.00%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="2,144.73,646.84,335.86,7.86;2,144.73,657.79,172.53,7.86"><p>Krizhevsky et. al. [8] used approximately 1.2 million labelled examples for their breakthrough result in ImageNET in 2012.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_1" coords="5,144.73,657.79,236.33,7.86"><p>Azure Standard A4 VM: 8-core 2.1GHz CPU, 14GB RAM</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was supported in part by a <rs type="grantName">Microsoft Azure for Research grant</rs>, which provided the cloud infrastructure to conduct our experiments.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_aRQaeB4">
					<orgName type="grant-name">Microsoft Azure for Research grant</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,138.35,300.90,342.24,7.86;9,146.91,311.86,333.68,7.86;9,146.91,322.81,333.68,7.86;9,146.91,333.77,314.77,7.86" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="9,395.70,322.81,84.89,7.86;9,146.91,333.77,145.85,7.86">General Overview of ImageCLEF at the CLEF 2015 Labs</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bromuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">K</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Uskudarli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">B</forename><surname>Marvasti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Aldana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Del Mar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roldn</forename><surname>Garcia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,344.26,342.24,7.86;9,146.91,355.22,333.68,7.86;9,146.91,366.18,102.92,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,364.79,344.26,115.80,7.86;9,146.91,355.22,124.91,7.86">Overview of the ImageCLEF 2015 medical classification task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcia Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bromuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,289.74,355.22,190.86,7.86;9,146.91,366.18,74.23,7.86">Working Notes of CLEF 2015 (Cross Language Evaluation Forum)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,376.66,342.24,7.86;9,146.91,387.62,333.68,7.86;9,146.91,398.58,43.26,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,310.70,376.66,169.89,7.86;9,146.91,387.62,47.38,7.86">Representation learning: a review and new perspectives</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,201.07,387.62,161.49,7.86">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">17981828</biblScope>
			<date type="published" when="2013-08">Aug. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,409.06,342.25,7.86;9,146.91,420.02,75.01,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,299.10,409.06,54.04,7.86">Deep learning</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,359.73,409.06,26.13,7.86">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page">436444</biblScope>
			<date type="published" when="2015-05">May 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,430.50,342.24,7.86;9,146.91,441.46,100.47,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,215.31,430.50,184.72,7.86">Deep learning in neural networks: an overview</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,407.21,430.50,51.68,7.86">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page">85117</biblScope>
			<date type="published" when="2015-01">Jan. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,451.95,342.24,7.86;9,146.91,462.91,333.68,7.86;9,146.91,473.87,129.02,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,202.02,451.95,218.27,7.86">Object recognition from local scale-invariant features</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,440.12,451.95,40.47,7.86;9,146.91,462.91,23.96,7.86;9,203.47,462.91,273.02,7.86">The Proceedings of the Seventh IEEE International Conference on</title>
		<imprint>
			<date type="published" when="1999">1999. 1999</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11501157</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct coords="9,138.35,484.35,342.25,7.86;9,146.91,495.31,333.68,7.86;9,146.91,506.27,222.19,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,362.93,484.35,117.67,7.86;9,146.91,495.31,318.76,7.86">Content-based medical image retrieval: a survey of applications to multidimensional and multimodality data</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fulham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,473.30,495.31,7.30,7.86;9,146.91,506.27,56.67,7.86">J. Digit. Imaging</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">10251039</biblScope>
			<date type="published" when="2013-12">Dec. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,516.75,342.25,7.86;9,146.91,527.71,333.67,7.86;9,146.91,538.67,333.68,7.86;9,146.91,549.63,147.35,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,340.12,516.75,140.48,7.86;9,146.91,527.71,123.79,7.86">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,288.09,527.71,192.50,7.86;9,146.91,538.67,29.49,7.86">Advances in Neural Information Processing Systems 25</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page">10971105</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,560.11,342.25,7.86;9,146.91,571.07,333.68,7.86;9,146.91,582.03,235.91,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,402.36,560.11,78.24,7.86;9,146.91,571.07,135.15,7.86">ImageNet: A largescale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,302.30,571.07,174.10,7.86;9,170.98,582.03,134.54,7.86">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page">248255</biblScope>
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct coords="9,142.62,592.52,337.98,7.86;9,146.91,603.47,333.68,7.86;9,146.91,614.43,333.68,7.86;9,146.91,625.39,185.93,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,236.89,603.47,243.70,7.86;9,146.91,614.43,291.53,7.86">Evaluating performance of biomedical image retrieval system-sAn overview of the medical image retrieval task at ImageCLEF 20042013</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,445.27,614.43,35.33,7.86;9,146.91,625.39,85.28,7.86">Comput. Med. Imaging Graph</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">5561</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,635.88,337.98,7.86;9,146.91,646.84,333.68,7.86;9,146.91,657.79,65.55,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,312.20,635.88,168.39,7.86;9,146.91,646.84,274.69,7.86">A review of content-based image retrieval systems in medical applicationsclinical benefits and future directions</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Michoux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bandon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,428.34,646.84,52.25,7.86;9,146.91,657.79,38.17,7.86">International journal</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,120.67,337.98,7.86;10,146.91,131.63,333.68,7.86;10,146.91,142.59,208.83,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,146.91,131.63,289.86,7.86">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,444.69,131.63,35.91,7.86;10,146.91,142.59,44.53,7.86">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">19291958</biblScope>
			<date type="published" when="2014-01">Jan. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,153.55,337.98,7.86;10,146.91,164.51,148.42,7.86" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="10,198.44,153.55,282.15,7.86;10,146.91,164.51,30.56,7.86">Practical recommendations for gradient-based training of deep architectures</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>arXiv [cs.LG</idno>
		<imprint>
			<date type="published" when="2012-06">Jun-2012</date>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,175.46,337.98,7.86;10,146.91,186.42,333.67,7.86;10,146.91,197.38,191.63,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,369.72,175.46,110.88,7.86;10,146.91,186.42,134.27,7.86">A committee of neural networks for traffic sign classification</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,297.84,186.42,182.74,7.86;10,146.91,197.38,105.11,7.86">Neural Networks (IJCNN), The 2011 International Joint Conference on</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page">19181921</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,208.34,337.98,7.86;10,146.91,219.30,319.77,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,351.18,208.34,129.41,7.86;10,146.91,219.30,96.96,7.86">Gradient-based learning applied to document recognition</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,250.87,219.30,44.23,7.86">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">22782324</biblScope>
			<date type="published" when="1998-11">Nov. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,230.26,337.98,7.86;10,146.91,241.22,333.67,7.86;10,146.91,252.18,119.41,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,268.24,230.26,212.36,7.86;10,146.91,241.22,34.86,7.86">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,198.81,241.22,281.77,7.86;10,146.91,252.18,41.58,7.86">Proceedings of the 27th International Conference on Machine Learning (ICML-10)</title>
		<meeting>the 27th International Conference on Machine Learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">807814</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,263.14,337.98,7.86;10,146.91,274.09,205.80,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,322.78,263.14,157.81,7.86;10,146.91,274.09,101.37,7.86">Rectifier Nonlinearities Improve Neural Network Acoustic Models</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,255.83,274.09,33.81,7.86">W-&amp;CP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,285.05,337.98,7.86;10,146.91,296.01,333.68,7.86;10,146.91,306.97,82.98,7.86" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="10,146.91,296.01,304.18,7.86">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-07">Jul-2012</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>arXiv [cs.NE</note>
</biblStruct>

<biblStruct coords="10,142.62,317.93,337.98,7.86;10,146.91,328.89,333.68,8.12;10,146.91,339.85,23.55,7.86" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="10,151.52,317.93,187.88,7.86">Classifying plankton with deep neural networks</title>
		<ptr target="http://benanne.github.io/2015/03/17/plankton.html" />
		<editor>Sander Dieleman</editor>
		<imprint>
			<date type="published" when="2015-05-30">30-May-2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,350.81,337.97,7.86;10,146.91,361.77,333.68,7.86;10,146.91,372.73,255.01,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="10,302.95,350.81,177.64,7.86;10,146.91,361.77,160.75,7.86">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,327.18,361.77,153.41,7.86;10,146.91,372.73,177.18,7.86">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page">513520</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,383.68,337.97,7.86;10,146.91,394.64,333.68,7.86;10,146.91,405.60,67.07,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="10,351.24,383.68,129.35,7.86;10,146.91,394.64,187.55,7.86">Deep learning with non-medical training used for chest pathology identification</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Diamant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,352.00,394.64,89.35,7.86">SPIE Medical Imaging</title>
		<imprint>
			<biblScope unit="page" from="94140V" to="94140V" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
