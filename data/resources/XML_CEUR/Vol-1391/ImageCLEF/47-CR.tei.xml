<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,146.27,116.95,322.82,12.62;1,283.13,134.89,49.11,12.62">Automatic Classification of Body Parts X-ray Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,183.32,172.71,60.66,8.74"><forename type="first">Moshe</forename><surname>Aboud</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Software Engineering</orgName>
								<orgName type="institution">Jerusalem College of Engineering</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,254.53,172.71,72.51,8.74"><forename type="first">Assaf</forename><forename type="middle">B</forename><surname>Spanier</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Software Engineering</orgName>
								<orgName type="institution">Jerusalem College of Engineering</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">The Selim and Rachel Benin School of Engineering</orgName>
								<orgName type="institution">The Hebrew Univ</orgName>
								<address>
									<settlement>Jerusalem</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,363.31,172.71,64.25,8.74"><forename type="first">Leo</forename><surname>Joskowicz</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">The Selim and Rachel Benin School of Engineering</orgName>
								<orgName type="institution">The Hebrew Univ</orgName>
								<address>
									<settlement>Jerusalem</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,146.27,116.95,322.82,12.62;1,283.13,134.89,49.11,12.62">Automatic Classification of Body Parts X-ray Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6918206301D6ABF598520366AF144890</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Classification</term>
					<term>X-ray images</term>
					<term>ImageCLEF-2015</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The development of automatic analysis and classification methods for large databases of X-ray images is a pressing need that may have a great impact on clinical practice. To advance this objective the ImageCLEF-2015 clustering of body part X-ray images challenge was created. The aim of the challenge is to group digital X-ray images into five structural groups: head-neck, upper-limb, body, lower-limb, and other. This paper presents the results of an experimental evaluation of X-ray images classification in the ImageCLEF-2015 challenge. We apply state-of-the-art classification and feature extraction methods for image classification and optimize them for the challenge task with emphasis on features indicating bone size and structure. The best classification results were obtained using the intensity, texture and HoG features and the KNN classifier. This combination has an accuracy of 86% and 73% for the 500 training images and 250 test images, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The increasing amount of medical imaging data acquired in clinical practice constitutes a vast database of untapped diagnostically-relevant information, millions of images are acquired worldwide each year. Clinicians are struggling under the burden of diagnosis and follow up of such an immense amount of images. This phenomenon gave rise to a plethora of methods to improve and assist clinicians using efficient search capabilities.</p><p>Content-Based Image Retrieval (CBIR) is a popular growing research topic <ref type="bibr" coords="1,134.77,621.25,9.96,8.74" target="#b0">[1]</ref>. The goal of CBIR is to assist physicians with diagnosis by finding similar cases to the case at hand. Therefore, CBIR requires efficient search capabilities in a vast database of medical images. The problem is emphasized in X-ray imaging, the most widely used medical imaging modality today as many clinical home health-care centers are equipped with X-ray scanners and maintain their own database of images.</p><p>This paper elucidates the problem of classification of digital X-ray image into five groups: head-neck, upper-limb, body, lower-limb and other ( <ref type="bibr" coords="2,416.48,156.80,24.66,8.74">Fig 1)</ref>.</p><p>A variety of methods exist for medical image feature extraction and classification, Haralick et al. <ref type="bibr" coords="2,240.93,181.65,15.50,8.74" target="#b13">[14]</ref> suggest feature extraction based on gray level cooccurrences matrices, whereas Weszka et al. <ref type="bibr" coords="2,330.62,193.61,14.87,8.74" target="#b22">[23]</ref> perform a classification based on local binary patterns (LBP). Another strategy is to combine local and global features presented by Rublee et al. <ref type="bibr" coords="2,293.59,217.52,14.61,8.74" target="#b17">[18]</ref>, using pixel values and shape features extracted with the Canny edge detection method. The pixel values and shape features are then used as a unique multi-feature vector used for classification.</p><p>Advanced methods include image classification based on the IRMA code <ref type="bibr" coords="2,134.77,266.28,14.61,8.74" target="#b15">[16]</ref>. In this method, features are extracted from the modality, body orientation, anatomic region and biological system. More recently, the Bag of Visual Words model (BoVW) <ref type="bibr" coords="2,205.05,290.19,10.52,8.74" target="#b3">[4]</ref> was used for X-ray images. In the BoVW approach, a visual word vocabulary is created from local image patches to represent an image, which is obtained by extracting feature descriptors around interest points.</p><p>Ghofrani et al. recently proposed the classification-based fuzzy set theory <ref type="bibr" coords="2,134.77,338.95,15.50,8.74" target="#b11">[12]</ref> They performed a fuzzy set classification with feature extraction based on a combination of shape and texture using the Canny Edge Detector and the Discrete Gabor Transform. Zare et al. <ref type="bibr" coords="2,308.21,362.86,15.50,8.74" target="#b23">[24]</ref> present three techniques for image annotation: the probabilistic latent semantic analysis (PLSA) image annotation, binary classification annotation, and annotation based on similar images. In their approach, semantic information is captured from textual and visual modalities and the correlation between them is learned.</p><p>This paper presents the results of an experimental evaluation of X-ray images classification <ref type="bibr" coords="2,192.20,435.54,10.52,8.74" target="#b2">[3]</ref> in the ImageCLEF-2015 challenge <ref type="bibr" coords="2,353.35,435.54,14.61,8.74" target="#b20">[21]</ref>. The goal of the challenge is to group digital X-ray images into five groups: head-neck, upper-limb, body, lower-limb, and other. In the context of the challenge, we apply state-of-theart classification and feature extraction methods for image classification and optimize them for the challenge task with an emphasis on features indicating bone size and structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>The aim of our method is to group digital X-ray images into five groups: headneck, upper-limb, body, lower-limb, and other (see <ref type="bibr" coords="2,360.57,572.49,26.83,8.74">Fig 1)</ref> . Our objective is to apply state-of-the-art classifiers and feature selection methods and to optimize them for the challenge task with an emphasis on features indicating bone size and structure.</p><p>The input to our method is a set of (1) label X-ray images from five groups, (2) features extraction techniques and; (3) classifiers. The output of our method is a combination of 10 features-classifier pairs that achieve the highest classification accuracy on the given five groups classification task. Our method consists of two steps. (1) a two-class experiment was used in order to select the features-classifier pair that best distinguished between X-ray images containing big and long bones (e.g. skull, arm and leg) against small and short bones (e.g. chest and abdomen bones). ( <ref type="formula" coords="3,331.33,409.21,4.24,8.74">2</ref>) The features-classifier pairs that that provide an average accuracy of grater then 90% in the first step evaluate on the five groups of X-ray images (head-neck, upper-limb, body, lower-limb and other </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Features Extraction</head><p>Nine features extraction methods were evaluated in this study. Below we describe the various features that were examined in our study.</p><p>-Color Extracting. A gray-scale histogram <ref type="bibr" coords="3,335.12,549.60,15.50,8.74" target="#b21">[22]</ref> is used to represent the color distribution of the image. We divide the image into equal patches (7x7) and compute an 8 bit histogram for each region. Then we add all patch based histogram into a single vector that serves as the color feature of our method. -Texture Extracting. Texture features are examined using Local Binary Pattern (LBP) <ref type="bibr" coords="3,206.46,609.29,15.50,8.74" target="#b12">[13]</ref> which provides highly discriminative texture information and is used to provide robust pattern-related information. We divide each image into equal (10x10) patches and extract LBP values for each patch.</p><p>The patches are then represented in a single vector to serve as the texture feature of our method.</p><p>Fig. <ref type="figure" coords="4,154.40,331.85,4.13,7.89">2</ref>. The input to our method is a set of (1) labeled X-ray images ( <ref type="formula" coords="4,436.81,331.88,3.93,7.86">2</ref>) features extraction techniques, and (3) classifiers. Our method consists of two steps.</p><p>Step 1: 2-class experiment was used in order to select the best features-classifier pair that distinguish between X-ray images containing big and long bones against small and short bones.</p><p>Step 2: The features-classifier pairs that that provide an average accuracy of grater then 90% in the first step train on the five groups of X-ray images set to find 10 best combination of features-classifier. Those 10 best features-classifier pairs were submitted to the challenge evaluation.</p><p>-HoG This is a histogram of neighborhood pixels according to their gradient orientation, weighted by their gradient magnitude. HoG features were shown to be particularly discriminative of people and body shapes. We extract the HoG values for each 10x10 patch in the image. These values are then represented as a single vector to serve as the HoG feature of our method. <ref type="bibr" coords="4,467.31,489.46,9.96,8.74" target="#b7">[8]</ref>.</p><p>-BoVW <ref type="bibr" coords="4,184.54,501.25,10.52,8.74" target="#b8">[9]</ref> This method produces a visual vocabulary. The method descriptors were extracted from detected key points using the following algorithms:</p><p>• Scale invariant feature transform (SIFT) <ref type="bibr" coords="4,349.10,524.83,14.61,8.74" target="#b16">[17]</ref>.</p><p>• Speded up robust features (SURF) <ref type="bibr" coords="4,324.05,536.62,9.96,8.74" target="#b4">[5]</ref>.</p><p>• Binary robust independent elementary features Brief (BRIEF) <ref type="bibr" coords="4,444.24,548.41,9.96,8.74" target="#b5">[6]</ref>.</p><p>• Oriented fast and rotated BRIEF (ORB) <ref type="bibr" coords="4,351.12,560.20,14.61,8.74" target="#b19">[20]</ref>. These descriptors were then clustered using the k-means algorithm. The cluster centers act as the BoVW feature of our method. Applying this scheme using the mentioned descriptor algorithms provides four additional methods. Thus, we have 4 different types of BoVW features.</p><p>-Color+Texture A combination of the color and texture values represented as as a multi-feature vector. -Color+Texture+HoG A combination of the color, texture and HoG values represented as a multi-feature vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Classifiers</head><p>We tested the following four classification methods:</p><p>1. KNN assigns a label according to the majority labels of the K-nearest neighbor in space <ref type="bibr" coords="5,207.96,167.04,9.96,8.74" target="#b6">[7]</ref>. 2. SVM is a linear classification of the points in space into two distinct classes <ref type="bibr" coords="5,151.70,190.42,15.50,8.74" target="#b10">[11]</ref> . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Model Selection</head><p>Given a set of feature extraction and classifier methods our goal is to find the 10 best combinations of feature-classifier that will provide the highest classification accuracy for the five groups of X-ray images.</p><p>To reduce the number of combinations and the complexity of the problem, a two-class selection is first applied to distinguish between X-ray images containing big and long bones (e.g., skull, arm and leg) and those with small and short bones (e.g., chest and abdomen bones). An additional motivation is to identify the features that will isolate different bone structures.</p><p>Next, we select the feature-classifier pairs that provide and average accuracy of greater than 90% in the two-class experiment and train them on the five groups of X-ray images set to find the 10 best combinations of feature-classifiers. We test each feature-classifiers pair in leave-one-out cross-validation, in which training is learned based on all cases besides a single case that is not part of the training process and used for testing.</p><p>The 10 best feature-classifier combination were then submitted to the challenge to be tested on an unlabeled test set of images released by the ImageCLEF organization <ref type="bibr" coords="5,191.37,497.96,15.50,8.74" target="#b20">[21]</ref>  <ref type="bibr" coords="5,209.77,497.96,9.96,8.74" target="#b2">[3]</ref>. We use the OpenCV-Python library <ref type="bibr" coords="5,384.56,497.96,15.50,8.74" target="#b14">[15]</ref> for the feature extraction and Python scikit-Learn Machine learning tool <ref type="bibr" coords="5,377.47,509.91,15.49,8.74" target="#b18">[19]</ref> to examine the four selected classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>The data set released by ImageCLEF <ref type="bibr" coords="5,295.55,573.43,21.08,8.74" target="#b20">[21]</ref> [3] consists of 750 X-ray images: (a) 500 images were labeled images (100 images from each group) and were released for training purposes. The five image groups are Head-Neck, Body, Upper-Limb, Lower-Limb and other (Fig <ref type="figure" coords="5,260.26,609.29,3.87,8.74" target="#fig_0">1</ref>). and (b) 250 unlabeled images released for the challenge evaluation and benchmarking.</p><p>We first present the results of the 500 labeled images training set of X-ray images obtained in our two-step approach. Then, we present the results of the 250 unlabeled images as validated by the challenge organizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training</head><p>In the two-class experiment to we use all four classifiers and nine sets of features, thus creating combination of 36(4 * 9) feature-classifiers. In the first step we select feature-classifier pairs that provide an average accuracy greater than 90%. This selection reduced the number of combination from 36 to 16. The results for all 36 feature-classifiers are shown in Table <ref type="table" coords="6,311.35,194.10,3.87,8.74" target="#tab_0">1</ref>: The BoVW and HoG features exhibit low accuracy regardless of the classifier tested. The combination of color and texture yield high accuracy rate of 89-93%. Using color, texture and HoG features all together yields the highest average classification accuracy in all classifiers.</p><p>In the second step, 5-class, classification was preformed on all five group. We select 16 combinations of features-classifier that yield an accuracy greater than 90%. The goal of this second step is to investigate the performance of the methods and to reduce the number of feature-classifiers to the best 10 .</p><p>Table <ref type="table" coords="6,177.11,520.47,4.98,8.74" target="#tab_1">2</ref> presents the results of the second step on the training set. Color and KNN 10. Texture and KNN. These 10 best combinations are marked in Table with (*) were sent to the challenge organizer for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Challenge Results</head><p>Table <ref type="table" coords="7,162.74,230.39,4.98,8.74" target="#tab_2">3</ref> shows the classification results of the 250 training images with the 10 best combinations of feature-classifier methods which were submitted to the ImageCLEF organization for evaluation. Our best summation ranked 12th out of 30 using the color histogram features and the KNN classifier achieving an accuracy of 73.2%. The challenge results for all 10 methods are presented in the table below: Note that our best submission (73.2% accuracy) is lower than the best accuracy obtained in the five group training set (80-85%) using the same combination of color texture and HoG features. The challenge results are similar to the average results achieved on the training set. This may indicate that the variability of the training dataset does not fully reflect the images variability of the challenge dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>In this work we have evaluated four state-of-the-art classifier and nine sets of features, resulting in 36 combinations of feature-classifiers. Surprisingly, the simplest classifier, in terms of implementation and computational complexity, KNN, exhibited the best results. Moreover, despite the major trend of using Deep Belief Network (DBN) methods for many image-based classification problems, the use of DBN in our study exhibited reliable results only when applied to a large-scale dataset. However, suboptimal results were obtained when used on small-scale datasets (see Table <ref type="table" coords="7,222.72,633.20,3.87,8.74" target="#tab_1">2</ref>). This is in line with the theory of DBN, which requires large-scale databases for reliable performance. The BoVW method was the least efficient method among all feature extraction schemes that have been tested.</p><p>From all feature extraction methods we evaluated, the color feature yielded the highest accuracy. This is surprising, considering that X-ray images are graylevel based images. This could be explained by the algorithm implementation, which extracts the gray-scale histogram features from different regions of the image and provides more specific information and a better perspective on the distribution of the color. Another advantage of using the color features is low computational complexity as compared to the texture, HoG and BoVW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>This paper presents research on medical X-ray image classification. We analyze state-of-the-art classifiers and feature extraction methods for image classification. The image features that have been used include the color, texture, HoG and BoVW, which were used by our tested classifiers: SVM, KNN, LR and DBN. We used the datasets of the ImageCLEF-2015 <ref type="bibr" coords="8,343.78,298.58,15.50,8.74" target="#b18">[19]</ref> clustering of body part Xray challenge <ref type="bibr" coords="8,195.84,310.53,9.96,8.74" target="#b2">[3]</ref>: 500 X-ray images were used for training and 250 for testing. The highest classification accuracy results were obtained when using the intensity, texture and HoG features and the KNN classifier. This combination has an accuracy of 86% and 73% for the 500 training images and 250 test images, respectively.</p><p>Future work consists of examining an additional set of classifiers and extending the completeness of our algorithm to estimate the partitioning of the initial clusters into sub-clusters. For example, the upper-limb cluster can be further divided into the following categories: clavicle, scapula, humerus, radius, ulna and hand.</p><p>Future work consists of examining an additional set of classifiers and extending the completeness of our algorithm to estimate the partitioning of the initial clusters into sub-clusters, for example the upper-limb cluster can be farther divided into: Clavicle, Scapula, Humerus, Radius, Ulna, and Hand.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,134.77,330.73,345.83,7.89;3,134.77,341.71,62.22,7.86;3,134.77,116.83,338.85,199.13"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Examples of the five X-ray images groups: Body, Head-Neck, Upper limb, lowerlimb and other.</figDesc><graphic coords="3,134.77,116.83,338.85,199.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,160.71,445.07,319.88,8.74;3,134.77,457.03,291.52,8.74;3,149.71,468.98,330.88,8.74"><head>Fig 1 )</head><label>1</label><figDesc>set to find 10 best combination of features-classifier. Those 10 best features-classifier pairs were submitted to the challenge evaluation. Fig 2 illustrates the flow of our method. Next, we describe each step in details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,138.97,201.81,341.62,8.77;5,151.70,213.80,102.26,8.74;5,138.97,225.20,341.62,8.77;5,151.70,237.18,328.89,8.74;5,151.70,249.14,328.89,8.74;5,151.70,261.09,128.29,8.74"><head>3 .</head><label>3</label><figDesc>LR is a probability model that predicts a binary output based on the model predictor variables<ref type="bibr" coords="5,235.69,213.80,14.61,8.74" target="#b9">[10]</ref>. 4. DBN constructs deep hierarchical layers based on a representation of the training data. The DBN performs an unsupervised pre-training learning and then sets the weights of the network in order to successfully use a supervised learning for classification<ref type="bibr" coords="5,263.39,261.09,10.52,8.74" target="#b1">[2]</ref> .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="4,134.77,116.83,324.90,200.25"><head></head><label></label><figDesc></figDesc><graphic coords="4,134.77,116.83,324.90,200.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,134.77,226.20,345.83,163.75"><head>Table 1 .</head><label>1</label><figDesc>Results of the first training step: Classify between long and short bones, the nine sets of features were used, each represented as a row in the table. The columns represent the four classifiers. Values in table are the results of the leave-one-out cross-validation process.</figDesc><table coords="6,188.22,279.88,238.91,110.08"><row><cell>Feature/Classifier</cell><cell>LR</cell><cell>DBN SVM KNN</cell></row><row><cell>BoVW BRIEF</cell><cell cols="2">79.55% 74.06% 80.05% 75.81%</cell></row><row><cell>BoVW ORB</cell><cell cols="2">79.05% 74.06% 80.30% 78%</cell></row><row><cell>BoVW SIFT</cell><cell cols="2">82.54% 74.06% 81.30% 76.81%</cell></row><row><cell>BoVW SURF</cell><cell>87%</cell><cell>74.31% 87.28% 84.29%</cell></row><row><cell>HoG</cell><cell cols="2">90.77% 79.55% 92.52% 93.02%</cell></row><row><cell>TEXTURE</cell><cell cols="2">89.78% 91.27% 92.27% 90.52%</cell></row><row><cell>COLOR</cell><cell cols="2">89.28% 92.77% 91.02% 92.52%</cell></row><row><cell>COLOR+TEXTURE</cell><cell cols="2">91.27% 93.27% 91.52% 91.77%</cell></row><row><cell cols="3">COLOR+TEXTURE+HoG 93.52% 92.77% 92.27% 92%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,134.77,554.51,345.83,107.47"><head>Table 2 .</head><label>2</label><figDesc>Results of the second training phase for the 5-groups training set classification, the four sets of features were used, each represented as a row in the table. The columns represent the four classifiers. The 10 best combinations are marked in the table with (*) and were sent for the challenge To sum-up, the 10 best features-classifier pairs are: 1. Color+Texture+HoG and KNN 2. Color+Texture+HoG and SVM 3. Color+Texture + HoG and DBN 4. Color+Texture+HoG and LR 5. Texture+HoG and KNN 6. Tex-ture+HoG and SVM 7. Texture+HoG and DBN 8. Color+Texture and LR 9.</figDesc><table coords="6,164.67,608.68,286.01,53.29"><row><cell>Feature/Classifier</cell><cell>LR</cell><cell>DBN</cell><cell>SVM</cell><cell>KNN</cell></row><row><cell>TEXTURE</cell><cell>73.40%</cell><cell>77.40%</cell><cell>75.00%</cell><cell>78.80(*)%</cell></row><row><cell>COLOR</cell><cell>72.40%</cell><cell>79.20%</cell><cell>74.80%</cell><cell>80.80(*)%</cell></row><row><cell>COLOR+TEXTURE</cell><cell cols="4">75.60(*)% 80.20(*)% 79.80(*)% 83.20(*)%</cell></row><row><cell cols="5">COLOR+TEXTURE+HoG 80.80(*)% 83.40(*)% 83.80(*)% 86(*)%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,134.77,319.89,345.83,83.31"><head>Table 3 .</head><label>3</label><figDesc>Challenge Results for 250 training images with our 10 best combinations features-classifier methods</figDesc><table coords="7,197.44,349.90,220.48,53.29"><row><cell>Feature/Classifier</cell><cell>LR</cell><cell>DBN SVM KNN</cell></row><row><cell>TEXTURE</cell><cell></cell><cell>66.4%</cell></row><row><cell>COLOR</cell><cell></cell><cell>73.2%</cell></row><row><cell>COLOR+TEXTURE</cell><cell cols="2">71.2% 68.0% 71.2% 71.2%</cell></row><row><cell cols="3">COLOR+TEXTURE+HoG 69.2% 69.2% 72.8% 72.4%</cell></row></table></figure>
		</body>
		<back>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>: https://bitbucket.org/mosheab/classifying-medical-images</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="8,142.96,525.31,337.64,7.86;8,151.52,536.27,329.07,7.86;8,151.52,547.23,197.64,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,151.52,536.27,325.30,7.86">Content-based image retrieval in radiology: Current status and future directions</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">B</forename><surname>Akgül</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">L</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Napel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">F</forename><surname>Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Acar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,151.52,547.23,107.02,7.86">Journal of Digital Imaging</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="208" to="222" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,558.52,337.64,7.86;8,151.52,569.47,329.07,7.86;8,151.52,580.43,225.12,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,241.38,558.52,239.21,7.86;8,151.52,569.47,80.90,7.86">Learning features for action recognition and identity with deep belief networks</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">H</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,254.67,569.47,225.93,7.86;8,151.52,580.43,108.58,7.86">2014 International Conference on Audio, Language and Image Processing (ICALIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="129" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,591.72,337.64,7.86;8,151.52,602.68,329.07,7.86;8,151.52,613.63,197.87,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,285.98,591.72,194.61,7.86;8,151.52,602.68,41.80,7.86">Overview of the ImageCLEF 2015 medical clustering task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">K</forename><surname>Mohammed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,213.83,602.68,266.76,7.86;8,151.52,613.63,27.88,7.86">CLEF2015 Working Notes. CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-11">September 8-11 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,624.92,337.63,7.86;8,151.52,635.88,329.07,7.86;8,151.52,646.84,329.07,7.86;8,151.52,657.79,50.43,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,411.16,624.92,69.43,7.86;8,151.52,635.88,261.11,7.86">Chest x-ray characterization: from organ identification to pathology categorization</title>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Avni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sharon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Konen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,432.52,635.88,48.07,7.86;8,151.52,646.84,271.35,7.86">Proceedings of the international conference on Multimedia information retrieval</title>
		<meeting>the international conference on Multimedia information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="155" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,120.67,337.64,7.86;9,151.52,131.63,267.14,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,342.23,120.67,134.70,7.86">Speeded-up robust features (surf)</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,151.52,131.63,171.92,7.86">Computer vision and image understanding</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="346" to="359" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,143.24,337.64,7.86;9,151.52,154.20,329.07,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,337.77,143.24,142.82,7.86;9,151.52,154.20,70.13,7.86">Brief: Binary robust independent elementary features</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Calonder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Strecha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,241.23,154.20,118.55,7.86">Computer Vision-ECCV 2010</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="778" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,165.81,337.64,7.86;9,151.52,176.77,98.66,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,278.54,165.81,120.79,7.86">k-nearest neighbour classifiers</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Delany</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,406.37,165.81,74.22,7.86;9,151.52,176.77,32.62,7.86">Multiple Classifier Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,188.38,337.63,7.86;9,151.52,199.34,329.07,7.86;9,151.52,210.30,158.20,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,242.26,188.38,219.47,7.86">Histograms of oriented gradients for human detection</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,151.52,199.34,324.88,7.86">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,221.91,337.63,7.86;9,151.52,232.87,329.07,7.86;9,151.52,243.83,176.43,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,311.31,221.91,169.27,7.86;9,151.52,232.87,121.82,7.86">Bag-of-visual-words models for adult image classification and filtering</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Pimenidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,296.01,232.87,80.87,7.86;9,410.36,232.87,70.23,7.86;9,151.52,243.83,110.89,7.86">ICPR 2008. 19th International Conference on</title>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
	<note>Pattern Recognition</note>
</biblStruct>

<biblStruct coords="9,142.62,255.44,337.98,7.86;9,151.52,266.40,329.07,7.86;9,151.52,277.36,87.55,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,286.89,255.44,193.70,7.86;9,151.52,266.40,180.67,7.86">Logistic regression and artificial neural network classification models: a methodology review</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dreiseitl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ohno-Machado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,340.93,266.40,139.66,7.86">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="352" to="359" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,288.97,337.98,7.86;9,151.52,299.93,329.07,7.86;9,151.52,310.89,329.07,7.86;9,151.52,321.85,25.60,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,298.98,288.97,181.61,7.86;9,151.52,299.93,249.78,7.86">Classification of structural images via highdimensional image warping, robust feature extraction, and svm</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,422.18,299.93,58.41,7.86;9,151.52,310.89,253.45,7.86">Medical Image Computing and Computer-Assisted Intervention-MICCAI 2005</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,333.46,337.97,7.86;9,151.52,344.42,326.68,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,397.47,333.46,83.12,7.86;9,151.52,344.42,100.24,7.86">Fuzzy-based medical x-ray image classification</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ghofrani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Helfroush</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rashidpour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kazemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,258.39,344.42,153.77,7.86">Journal of medical signals and sensors</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">73</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,356.03,337.98,7.86;9,151.52,366.99,323.51,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,279.67,356.03,200.93,7.86;9,151.52,366.99,147.16,7.86">Rotation invariant texture classification using lbp variance (lbpv) with global matching</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,306.16,366.99,78.25,7.86">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="706" to="719" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,378.60,337.98,7.86;9,151.52,389.56,329.07,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,345.17,378.60,135.42,7.86;9,151.52,389.56,28.67,7.86">Textural features for image classification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Haralick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">H</forename><surname>Dinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,186.36,389.56,214.30,7.86">IEEE Transactions on Systems, Man and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="610" to="621" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,401.17,334.56,7.86" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="9,195.73,401.17,158.93,7.86">OpenCV Computer Vision with Python</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Howse</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Packt Publishing Ltd</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,412.78,337.98,7.86;9,151.52,423.74,329.07,7.86;9,151.52,434.70,254.72,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="9,442.20,412.78,38.40,7.86;9,151.52,423.74,195.35,7.86">The irma code for unique classification of medical images</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">M</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kohnen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">B</forename><surname>Wein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,370.39,423.74,68.14,7.86">Medical Imaging</title>
		<imprint>
			<biblScope unit="page" from="440" to="451" />
			<date type="published" when="2003">2003. 2003</date>
			<publisher>International Society for Optics and Photonics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,446.31,337.97,7.86;9,151.52,457.27,329.07,7.86;9,151.52,468.23,189.35,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="9,261.99,446.31,199.84,7.86">Ontology-based image retrieval with sift features</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,151.52,457.27,329.07,7.86;9,151.52,468.23,76.69,7.86">First International Conference on Pervasive Computing Signal Processing and Applications (PCSPA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="464" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,479.84,337.97,7.86;9,151.52,490.80,278.71,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="9,310.25,479.84,170.34,7.86;9,151.52,490.80,77.12,7.86">Automatic multilevel medical image annotation and retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mueen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zainuddin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Baba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,235.27,490.80,104.34,7.86">Journal of digital imaging</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="290" to="295" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,502.41,337.98,7.86;9,151.52,513.37,329.07,7.86;9,151.52,524.33,329.07,7.86;9,151.52,535.29,25.60,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="9,411.80,513.37,68.80,7.86;9,151.52,524.33,94.03,7.86">Scikit-learn: Machine learning in python</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,252.28,524.33,170.39,7.86">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,546.90,337.98,7.86;9,151.52,557.86,329.07,7.86;9,151.52,568.82,70.14,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="9,358.54,546.90,122.05,7.86;9,151.52,557.86,42.11,7.86">Orb: an efficient alternative to sift or surf</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Rublee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,215.31,557.86,244.17,7.86">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2564" to="2571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,580.43,337.97,7.86;9,151.52,591.39,329.07,7.86;9,151.52,602.35,329.07,7.86;9,151.52,613.31,329.07,7.86;9,151.52,624.26,126.65,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="9,398.63,602.35,81.96,7.86;9,151.52,613.31,142.90,7.86">General Overview of ImageCLEF at the CLEF 2015 Labs</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bromuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">K</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Uskudarli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">B</forename><surname>Marvasti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Aldana</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Del Mar Roldán</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>García</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="9,301.24,613.31,139.06,7.86">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,635.88,337.98,7.86;9,151.52,646.84,329.07,7.86;9,151.52,657.79,57.33,7.86" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="9,246.58,635.88,234.02,7.86;9,151.52,646.84,49.79,7.86">Information-based color feature representation for image classification</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Liew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,221.48,646.84,212.22,7.86">International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">353</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,120.67,337.98,7.86;10,151.52,131.63,329.07,7.86;10,151.52,142.59,60.92,7.86" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="10,314.89,120.67,165.71,7.86;10,151.52,131.63,92.87,7.86">A comparative study of texture measures for terrain classification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Weszka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,250.38,131.63,213.44,7.86">IEEE Transactions on Systems, Man and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="269" to="285" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,153.55,337.97,7.86;10,151.52,164.51,259.00,7.86" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="10,299.90,153.55,180.69,7.86;10,151.52,164.51,66.00,7.86">Automatic medical x-ray image classification using annotation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Zare</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mueen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">C</forename><surname>Seng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,224.78,164.51,104.34,7.86">Journal of digital imaging</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="89" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
