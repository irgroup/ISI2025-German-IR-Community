<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,161.38,115.96,292.59,12.62;1,185.26,133.89,244.85,12.62;1,246.43,151.82,122.49,12.62">CEA LIST&apos;s participation to the Scalable Concept Image Annotation task of ImageCLEF 2015</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,184.81,189.50,69.46,8.74"><forename type="first">Etienne</forename><surname>Gadeski</surname></persName>
							<email>etienne.gadeski@cea.fr</email>
							<affiliation key="aff0">
								<orgName type="department">LIST</orgName>
								<orgName type="laboratory">Laboratory of Vision and Content Engineering</orgName>
								<orgName type="institution">CEA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,262.29,189.50,71.62,8.74"><forename type="first">Hervé</forename><surname>Le Borgne</surname></persName>
							<email>herve.le-borgne@cea.fr</email>
							<affiliation key="aff0">
								<orgName type="department">LIST</orgName>
								<orgName type="laboratory">Laboratory of Vision and Content Engineering</orgName>
								<orgName type="institution">CEA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,361.41,189.50,69.13,8.74"><forename type="first">Adrian</forename><surname>Popescu</surname></persName>
							<email>adrian.popescu@cea.fr</email>
							<affiliation key="aff0">
								<orgName type="department">LIST</orgName>
								<orgName type="laboratory">Laboratory of Vision and Content Engineering</orgName>
								<orgName type="institution">CEA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,161.38,115.96,292.59,12.62;1,185.26,133.89,244.85,12.62;1,246.43,151.82,122.49,12.62">CEA LIST&apos;s participation to the Scalable Concept Image Annotation task of ImageCLEF 2015</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7B8F073B97DD5A912CA8C1066A135E4E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>image annotation</term>
					<term>classification</term>
					<term>concept localization</term>
					<term>convolutionnal neural networks. 2</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our participation to the ImageCLEF 2015 scalable concept image annotation task. Our system is only based on visual features extracted with a deep Convolutional Neural Network (CNN). The network is trained with noisy web data corresponding to the concepts to detect in this task. We introduce a simple concept localization pipeline that provides the localization of the detected concepts, among the 251 concepts, within the images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The scalable concept image Annotation subtask of ImageCLEF 2015 <ref type="bibr" coords="1,442.43,418.75,10.52,8.74" target="#b0">[1]</ref> is described in detail in <ref type="bibr" coords="1,224.48,430.71,9.96,8.74" target="#b1">[2]</ref>. The system we propose only relies on visual features extracted with a deep Convolutional Neural Network (CNN). In addition to the specific training of this CNN for this task we propose a concept localization pipeline which uses the spatial information that CNNs offer. We are able to propose a single concept annotation and localization framework for the 251 concepts. Section 2 outlines our method in both training and testing steps. In Sect. 3 we present our experiments, report our results and discuss them. synonyms (if present) to query the search engine. This dataset is of course noisy but we believe it is not a big issue for training a deep CNN <ref type="bibr" coords="2,396.30,130.95,10.52,8.74" target="#b4">[5,</ref><ref type="bibr" coords="2,408.48,130.95,7.00,8.74" target="#b5">6]</ref>. We only used this additional data to train a 16-layer CNN <ref type="bibr" coords="2,329.24,142.90,9.96,8.74" target="#b6">[7]</ref>. We used 90% of the dataset for training and 10% for validation.</p><p>Network Settings. The network is initialized with ImageNet weights. The initial learning rate is set to 0.001 and the batch size is set to 256. The last layer (the classifier) is trained from scratch, i.e. it is initialized with random weights sampled from a Gaussian distribution (σ = 0.01 and µ = 0) and its learning rate is 10 times larger than for other layers. During training, the dataset is enhanced with random transformations: RGB jittering, scale jittering, shearing, rotation, contrast adjustment, etc. It is known that data augmentation leads to better models <ref type="bibr" coords="2,168.37,268.14,10.52,8.74" target="#b7">[8]</ref> and reduces overfitting. Finally, the network takes a 224 × 224 RGB image as input and produces 251 outputs, i.e. the number of concepts. The models were trained on a single Nvidia Titan Black with our modified version of the Caffe framework <ref type="bibr" coords="2,241.78,304.01,9.96,8.74" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Testing</head><p>Feature Extraction. We convert the fine-tuned CNN model to a fully convolutional network. To do so, we explicitly convert the first fully-connected layer to a 7 × 7 convolution layer. The last two fully-connected layers are then converted to 1 × 1 convolution layers. We are able to do such a conversion because fully connected layers are nothing more than 1 × 1 convolutional kernels and a full connection table. This conversion allows us to feed images of any size (H × W pixels) to the network. In practice, any image with a side smaller than 256 pixels is isotropically rescaled so that its smallest side equals 256. In consequence, the output of the last layer (i.e. the classifier) is no longer a single vector with 251 dimensions but rather a spatial map S of M × N × 251 dimensions. The network has five 2 × 2 pooling layers, therefore it has a global stride of 32 pixels. We have:</p><formula xml:id="formula_0" coords="2,211.38,493.45,269.22,22.31">M = H 32 -7 + 1 and N = W 32 -7 + 1 .<label>(1)</label></formula><p>For instance, with a 512 × 384 image the network will produce a 10 × 6 × 251 feature map.</p><p>Concept Localization. For each image, our network outputs a spatial feature map, S, of size M × N × 251. In every of the M × N cells of this feature map, we obtain the probability for each concept to appear at this location. In our experiments, we chose to apply a max-pooling to each cell (over all 251 concepts) to only get the most probable concept within the location. Thus, we obtain a unique map giving the most probable concept at each cell location:  where s i,j is the 251-dimensional vector at position (i, j) in S. As illustrated in Fig. <ref type="figure" coords="3,154.17,406.58,4.98,8.74" target="#fig_0">1</ref> this gives us an approximate but yet interesting localization of the different concepts within the image. To limit the number of regions where a concept is likely to appear, we further merge the neighboring cells which have that concept, by retaining the largest rectangular region containing all the contiguous cells with a given most probable concept. We map these regions coordinates to the original image coordinates. The final result can be seen in Fig. <ref type="figure" coords="3,411.06,466.36,3.87,8.74">2</ref>.</p><formula xml:id="formula_1" coords="2,175.71,651.60,300.64,16.10">R = ∀i ∈ [1; M ], ∀j ∈ [1; N ], r i,j : r i,j = arg max c (s i,j (c)) ,<label>(2</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>We report the results obtained for the two runs submitted to the campaign and propose ways of improvement to our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Runs</head><p>Two runs were submitted for this task. We used two models, one per run. They only differ in the number of iterations accomplished in the training step. Results are reported in Table <ref type="table" coords="3,233.02,632.21,3.87,8.74">1</ref>. Figure <ref type="figure" coords="3,276.74,632.21,4.98,8.74">3</ref> shows the mean average precision (mAP) of our runs using varying percentage of overlap between our bounding boxes and the ground truth.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,134.77,336.54,345.82,7.89;3,134.77,347.52,345.82,7.86;3,134.77,358.48,67.09,7.86;3,137.64,123.43,163.16,179.96"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Example of one image of 600 × 544 pixels (on the left) with its corresponding 13 × 11 map (on the right). The concept 97 corresponds to "flower", 21 to "bee" and 104 to "garden".</figDesc><graphic coords="3,137.64,123.43,163.16,179.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="5,134.77,115.83,345.82,259.37"><head></head><label></label><figDesc></figDesc><graphic coords="5,134.77,115.83,345.82,259.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,476.35,651.63,4.24,8.74"><head></head><label></label><figDesc>)</figDesc><table coords="3,324.73,123.25,149.00,197.33"><row><cell>97 97 97 97 97 97 97 97 97 97 97</cell></row><row><cell>97 97 97 97 97 97 97 97 97 97 97</cell></row><row><cell>97 97 97 97 97 97 97 97 97 97 97</cell></row><row><cell>97 97 97 97 97 97 97 97 97 97 97</cell></row><row><cell>97 97 97 97 97 97 97 97 97 97 97</cell></row><row><cell>97 97 97 97 97 97 97 97 97 97 97</cell></row><row><cell>97 97 97 97 97 97 97 97 97 97 97</cell></row><row><cell>97 97 97 97 97 97 21 21 21 21 21</cell></row><row><cell>97 104 104 97 97 97 21 21 21 21 21</cell></row><row><cell>11</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fig. <ref type="figure" coords="4,154.40,321.13,4.13,7.89">2</ref>. On the left, we show the concepts regions proposed by our pipeline. The purple region corresponding to "flower" covers the whole area. The yellow region corresponding to "bee" is in the bottom right corner and there is also a green region corresponding to "garden" on the bottom left corner. We also display these mapped regions on the original image. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Discussion</head><p>We note that, although there is only a small gap of 8,000 iterations of stochastic gradient descent between the two models, the second one performs significantly better (+2.5%) than the first one. Unfortunately, due to the lack of time, we could not have trained the model further and we believe it would provide a non negligible improvement.</p><p>Regarding the localization, our method is quite efficient but it limits its ability to detect several similar objects forming a compact group into the image, such as a cow herd.</p><p>However, we think there are several ways of improving our method. The first one would be to directly train the CNN in a "fully convolutional" way to improve the localization of concepts that can appear at different scales, therefore we could also improve the robustness of our method by extracting the spatial feature map at different scales. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We proposed a simple and effective framework to annotate and localize concepts within images. The results obtained in this campaign are an encouraging step for us toward building a better framework for concept annotation and localization.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="5,138.35,536.43,342.24,7.86;5,146.91,547.39,333.68,7.86;5,146.91,558.35,333.68,7.86;5,146.91,569.31,333.68,7.86;5,146.91,580.27,126.65,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,395.98,558.35,84.61,7.86;5,146.91,569.31,144.26,7.86">General Overview of ImageCLEF at the CLEF 2015 Labs</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bromuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">K</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Uskudarli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">B</forename><surname>Marvasti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Aldana</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Del Mar Roldán</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>García</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="5,299.08,569.31,140.14,7.86">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,591.13,342.24,7.86;5,146.91,602.09,333.68,7.86;5,146.91,613.05,333.68,7.86;5,146.91,624.01,324.61,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,232.18,602.09,248.42,7.86;5,146.91,613.05,171.98,7.86">Overview of the ImageCLEF 2015 Scalable Image Annotation, Localization and Sentence Generation task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dellandrea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,341.63,613.05,138.97,7.86;5,146.91,624.01,89.74,7.86">CLEF2015 Working Notes. CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Toulouse, France, CEUR-WS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-11">September 8-11 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,634.88,342.24,7.86;5,146.91,645.84,333.68,7.86;5,146.91,656.80,265.01,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,396.52,634.88,84.07,7.86;5,146.91,645.84,173.34,7.86">Cnn features off-theshelf: An astounding baseline for recognition</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,339.74,645.84,140.85,7.86;5,146.91,656.80,209.55,7.86">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2014-06">June 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,119.67,342.24,7.86;6,146.91,130.63,333.68,7.86;6,146.91,141.59,299.63,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,353.47,119.67,127.12,7.86;6,146.91,130.63,206.86,7.86">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,378.91,130.63,101.68,7.86;6,146.91,141.59,265.77,7.86">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,152.55,342.25,7.86;6,146.91,163.51,333.67,7.86;6,146.91,174.47,119.03,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,435.85,152.55,44.75,7.86;6,146.91,163.51,127.15,7.86">Large-scale image mining with flickr groups</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">L</forename><surname>Ginsca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Le Borgne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Kanellos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,297.08,163.51,183.51,7.86;6,146.91,174.47,86.52,7.86">21th International Conference on Multimedia Modelling (MMM 15)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,185.43,342.24,7.86;6,146.91,196.39,333.68,7.86;6,146.91,207.34,279.62,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,363.08,185.43,117.52,7.86;6,146.91,196.39,152.52,7.86">Effective training of convolutional networks using noisy web images</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">D</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">L</forename><surname>Ginsca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Le Borgne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,319.48,196.39,161.11,7.86;6,146.91,207.34,144.08,7.86">13th International Workshop on Content Based Multimedia Indexing (CBMI)</title>
		<meeting><address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,218.30,342.24,7.86;6,146.91,229.24,173.71,7.89" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="6,265.18,218.30,215.41,7.86;6,146.91,229.26,43.20,7.86">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>CoRR abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,240.22,342.24,7.86;6,146.91,251.15,158.00,7.89" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="6,332.01,240.22,148.58,7.86;6,146.91,251.18,22.38,7.86">Deep image: Scaling up image recognition</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<idno>CoRR abs/1501.02876</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,262.14,342.24,7.86;6,146.91,273.10,333.68,7.86;6,146.91,284.06,154.44,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="6,235.61,273.10,240.38,7.86">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
