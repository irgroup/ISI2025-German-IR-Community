<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,183.62,116.95,248.12,12.62;1,240.56,134.89,134.23,12.62">Convolutional Neural Networks for Medical Clustering</title>
				<funder ref="#_FDMGR5J">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,137.05,172.56,62.26,8.74"><forename type="first">David</forename><surname>Lyndon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Technologies</orgName>
								<orgName type="institution">University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,209.88,172.56,61.80,8.74"><forename type="first">Ashnil</forename><surname>Kumar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Technologies</orgName>
								<orgName type="institution">University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Biomedical Engineering and Technology</orgName>
								<orgName type="institution">University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,288.57,172.56,54.39,8.74"><forename type="first">Jinman</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Technologies</orgName>
								<orgName type="institution">University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Biomedical Engineering and Technology</orgName>
								<orgName type="institution">University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,359.84,172.56,85.51,8.74"><forename type="first">Philip</forename><forename type="middle">H W</forename><surname>Leong</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Electrical and Information Engineering</orgName>
								<orgName type="institution">University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Biomedical Engineering and Technology</orgName>
								<orgName type="institution">University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,276.26,184.51,52.03,8.74"><forename type="first">Dagan</forename><surname>Feng</surname></persName>
							<email>dagan.feng@sydney.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Technologies</orgName>
								<orgName type="institution">University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Biomedical Engineering and Technology</orgName>
								<orgName type="institution">University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,183.62,116.95,248.12,12.62;1,240.56,134.89,134.23,12.62">Convolutional Neural Networks for Medical Clustering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4AE38490D0DF63AAD0BD87D8B5541C86</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep Learning</term>
					<term>Convolutional Neural Networks</term>
					<term>Medical Image Retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A major challenge for Medical Image Retrieval (MIR) is the discovery of relationships between low-level image features (intensity, gradient, texture, etc.) and high-level semantics such as modality, anatomy or pathology. Convolutional Neural Networks (CNNs) have been shown to have an inherent ability to automatically extract hierarchical representations from raw data. Their successful application in a variety of generalised imaging tasks suggests great potential for MIR. However, a major hurdle to their deployment in the medical domain is the relative lack of robust training corpora when compared to general imaging benchmarks such as ImageNET and CIFAR. In this paper, we present the adaptation of CNNs to the medical clustering task at Image-CLEF 2015.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper documents the Biomedical Engineering and Technology (BMET) team from the University of Sydney's submissions for the ImageCLEF 2015 <ref type="bibr" coords="1,470.07,519.42,10.52,8.74" target="#b0">[1]</ref> Medical clustering task <ref type="bibr" coords="1,238.96,531.38,9.96,8.74" target="#b1">[2]</ref>.</p><p>The objective of our experiments was to evaluate the effectiveness of Convolutional Neural Networks (CNNs) for this task. In particular, we propose a deep learning framework that learns high-level representations of anatomical elements contained in each image and uses these to cluster the images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Convolutional Neural Networks, a type of deep learning algorithm, have been used to produce state-of-the-art results for a variety of machine learning tasks such as image recognition, acoustic recognition and natural language processing since 2012 <ref type="bibr" coords="2,184.75,119.99,7.75,8.74" target="#b2">[3]</ref><ref type="bibr" coords="2,192.50,119.99,3.87,8.74" target="#b3">[4]</ref><ref type="bibr" coords="2,196.38,119.99,7.75,8.74" target="#b4">[5]</ref>. CNNs share the common features of all deep learning algorithms: stacked layers of neuronal subunits that learn hierarchical representations (allowing the data to be understood at various levels of abstraction, in isolation or combination <ref type="bibr" coords="2,245.96,155.86,10.30,8.74" target="#b2">[3]</ref>), the ability to perform unsupervised pre-training on unlabeled data and efficient parallelization on multiple core GPUs which can result in improvements of up to 5000% over CPU-only implementations <ref type="bibr" coords="2,450.36,179.77,9.96,8.74" target="#b4">[5]</ref>.</p><p>A more subtle implication of deep learning is that it can automatically extract features from raw data <ref type="bibr" coords="2,241.35,204.22,7.75,8.74" target="#b2">[3]</ref><ref type="bibr" coords="2,249.10,204.22,3.87,8.74" target="#b3">[4]</ref><ref type="bibr" coords="2,252.98,204.22,7.75,8.74" target="#b4">[5]</ref>. Typically, a key factor in the success of typical machine learning algorithms is extracting salient features from the raw data. Taking image recognition as an example, a feature set such as edges or SIFT <ref type="bibr" coords="2,470.08,228.13,10.52,8.74" target="#b5">[6]</ref> would be extracted from the raw data and it is these new features per se or in combination with the original raw data that would be fed into the machine learning algorithm. While some aspects of the process can be automated or implemented with well known algorithms, a major drawback is that it generally requires expert domain knowledge to define which features should be used and evaluate their success.</p><p>Deep learning algorithms, however, are able to directly utilise raw data instead of hand-crafted features. By feeding the data sequentially through many successive layers of subunits, the higher levels of the system are able to understand the data in terms of successively abstract representations <ref type="bibr" coords="2,414.55,348.23,9.96,8.74" target="#b2">[3]</ref>.</p><p>Medical Image Retrieval (MIR) tasks, such as the tests devised for Image-CLEF, require learning precisely these kinds of highly abstract representations, i.e. image modality or the anatomical semantics of the image. However, to the best of our knowledge it is not currently a well established method in this domain. This is due to not only the inherent challenges of medical images <ref type="bibr" coords="2,423.64,408.55,12.80,8.74" target="#b6">[7]</ref>, but also because state-of-the-art deep learning results are typically obtained using huge sets of labelled training data<ref type="foot" coords="2,257.49,430.89,3.97,6.12" target="#foot_0">4</ref> on tasks that are arguably less subtle. As a justification for these claims, consider that the ImageNET general object recognition task corpora consists of millions of robustly labelled images and was created with the assistance of crowdsourcing via Amazon Mechanical Turk <ref type="bibr" coords="2,407.17,468.33,9.96,8.74" target="#b8">[9]</ref>. On the other hand, medical imaging datasets require careful labelling by domain experts, often specialists in a particular area <ref type="bibr" coords="2,286.91,492.24,10.52,8.74" target="#b6">[7,</ref><ref type="bibr" coords="2,299.08,492.24,9.22,8.74" target="#b9">[10]</ref><ref type="bibr" coords="2,308.31,492.24,4.61,8.74" target="#b10">[11]</ref><ref type="bibr" coords="2,312.92,492.24,13.84,8.74" target="#b11">[12]</ref> and as a result are generally much smaller.</p><p>Large training sets are a current necessity of very deep systems because they contain many millions of internal parameters that must be estimated from the data. Too little data can result in the the higher-level neurons' activation being the result of salient features of the training set and not reflecting the high-level representations. If this 'overfitting' occurs then the system's ability to generalise on new data is severely impaired <ref type="bibr" coords="2,280.77,576.47,14.61,8.74" target="#b12">[13]</ref>.</p><p>In addition to the issues regarding the volume of data required, it must be mentioned that while deep learning can automatically perform excellent feature extraction, this comes at the significant cost of the larger number of hyperparameters that must be evaluated in order to find an optimal system <ref type="bibr" coords="2,443.60,624.84,14.61,8.74" target="#b13">[14]</ref>. For example, compared to a commonly used machine learning algorithm such as the Support Vector Machine (SVM) that has a basic hyperparameter search space with dimensions of choice of kernel, regularization constant and kernel hyperparameter, even the simplest implementation of a CNN requires fundamental choices about the number and type of layers, filter size and number of filters per layer, and the learning rate. More advanced implementations include factors such as unit activation function and the use of dropout. While there are guidelines for these choices in the literature <ref type="bibr" coords="3,331.72,203.68,14.61,8.74" target="#b13">[14]</ref>, the difficulty of even a small parameter search is compounded by the increased computational requirements of training the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Image Preprocessing</head><p>A requirement of our classifiers was uniformly sized input vectors, however, there was some variation in the training data size. The sizes of the images were at least 1600px in one dimension and then between 1600 and 2348px in the other. In order to use consistently sized images and not lose any crucial information, we created a new square image with the dimensions of the largest dimension of the original image, filling any empty space with black pixels.</p><p>Even prior to training the CNN, we were aware that the computational requirements were quite demanding and this would be exacerbated by using large images. With that in mind, we resized the images to 256x256px to reduce computational overhead. Good results have been reported in the literature for complex tasks with 48x48px images <ref type="bibr" coords="3,256.05,415.52,15.50,8.74" target="#b14">[15]</ref> and Krizhevsky et. al. <ref type="bibr" coords="3,376.25,415.52,10.52,8.74" target="#b7">[8]</ref> achieved state-of-the art general object recognition with 256x256px images (technically, the system had an input of 224x224px, but these were subimages of the original 256x256px images).</p><p>After resizing the images were 256x256x3px, the third dimension describing the three colour channels. The images supplied were in actual fact gray scale, i.e. all colour channels were equal-valued, so we simply sliced the array preserving only the 'red' channel.</p><p>We chose to train a single run of four models using 100% of the training data with no parameter optimization and use the ImageCLEF results as the test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Convolutional Neural Network</head><p>The task requires multi-label classification across four anatomical classes, with a null set indicating that the data is a true-negative image taken with the same camera, but not of the human body. To facilitate this output from our experiments we constructed 4x1 vs. All CNN models.</p><p>The architecture for the CNN used for our experimentation was based on a simplified version of Yann LeCun et. al.'s <ref type="bibr" coords="3,319.40,627.81,15.50,8.74" target="#b15">[16]</ref> LeNet-5 <ref type="foot" coords="3,373.14,626.23,3.97,6.12" target="#foot_1">5</ref> . This basic CNN is capable of correctly classifying the MNIST handwritten digit database with 1.7% test error. We modified the input to account for larger images and output a different number of classifications. The network consists of two convolutional pooling layers, with one fully connected hidden layer. The features that are output by the hidden layer are used for binary classification by a logistic regression classifier. The architecture of the system is shown in Figure <ref type="figure" coords="4,397.97,167.81,3.87,8.74" target="#fig_0">1</ref>. The specifications of the convolutional-pooling layers are detailed in Table <ref type="table" coords="4,472.84,477.17,3.87,8.74" target="#tab_0">1</ref>. Other hyperparameters for the CNN are detailed in Table <ref type="table" coords="4,405.91,657.11,3.87,8.74" target="#tab_1">2</ref>. As mentioned earlier the CNN requires a great deal of computational resource to run. We initially began training the four models on a CPU-only solution and despite it being a very powerful machine <ref type="foot" coords="5,311.96,246.71,3.97,6.12" target="#foot_2">6</ref> , it took approximately 90 minutes to train a model for a single epoch (albeit, training four models simultaneously). Fortunately, we were given an opportunity to run these models on a system with two Nvidia K20 GPUs. Even training the four models simultaneously (two per GPU), it only took approximately 11 minutes to train a model for a single epoch -an 8-fold speedup. We planned to submit a single run, having trained each of the four classifiers for 100 epochs. This process would have taken over a week on a CPU-only system, instead it took less than a day on the GPU server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>The test results for our submission as supplied by ImageCLEF are displayed in Table <ref type="table" coords="5,162.16,403.48,3.87,8.74">4</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">CNN-Learnt Features</head><p>The CNNs were able to extract improved representations from raw data without the requirement for domain knowledge. This was done without any hyperparamater tuning suggesting that there are further improvements that could be made. This is an important result both for this task and for MIR generally as it suggests that there is potential in using CNN or other deep learning strategies as a 'black box', whereby we will be able to achieve excellent machine learning performance without the need of expert-designed feature extraction or domain knowledge.</p><p>We believe that that these results can be significantly improved upon by making use of a variety of techniques. Primarily we would want to continue to explore training the CNNs using GPUs, because as we have demonstrated, the performance increase is non-trival and allows us to expand our hyperparamter and architecture search. Rectified Linear Units (ReLUs), as opposed the Tanh units used in our network are also known to improve training performance <ref type="bibr" coords="6,437.82,204.90,15.50,8.74" target="#b16">[17,</ref><ref type="bibr" coords="6,454.98,204.90,11.62,8.74" target="#b17">18]</ref>. Although this network is very capable of learning quality representations of the MNIST dataset, it is both less deep and less dense than networks used to achieve state-of-the-art results in more sophisticated tasks <ref type="bibr" coords="6,407.08,241.02,9.96,8.74" target="#b7">[8]</ref>. For instance, Krizhevsky et. al. <ref type="bibr" coords="6,216.42,252.97,10.52,8.74" target="#b7">[8]</ref> used a network with 2 convolutional-max pooling layers, 3 convolutional layers and 3 fully connected layers, all of which were more neurondense that ours, to achieve their result in ImageNET 2012. Improved training performance will allow us to implement a larger and deeper network along these lines.</p><p>Larger and deeper networks introduce issues with overfitting, but we believe this can be controlled using well-tried techniques such as dropout <ref type="bibr" coords="6,416.56,324.95,10.52,8.74" target="#b7">[8,</ref><ref type="bibr" coords="6,428.74,324.95,12.73,8.74" target="#b12">13,</ref><ref type="bibr" coords="6,443.12,324.95,11.62,8.74" target="#b18">19]</ref>, data augmentation <ref type="bibr" coords="6,197.58,336.90,10.52,8.74" target="#b7">[8,</ref><ref type="bibr" coords="6,209.76,336.90,12.73,8.74" target="#b19">20]</ref> and unsupervised pretraining <ref type="bibr" coords="6,356.63,336.90,15.50,8.74" target="#b20">[21,</ref><ref type="bibr" coords="6,373.78,336.90,11.62,8.74" target="#b21">22]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,190.47,432.19,234.42,7.89;4,137.36,210.42,340.65,207.00"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The architecture of CNN used for the experiments</figDesc><graphic coords="4,137.36,210.42,340.65,207.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,207.44,529.87,200.47,76.82"><head>Table 1 .</head><label>1</label><figDesc>Details of Convolutional Pooling Layers</figDesc><table coords="4,235.79,550.67,143.78,56.02"><row><cell cols="3">Hyperparameter Layer0 Layer1</cell></row><row><cell>Number of Filters</cell><cell>20</cell><cell>50</cell></row><row><cell cols="3">Size of Filters 15x15px 15x15px</cell></row><row><cell>Max Pooling</cell><cell>2x2</cell><cell>2x2</cell></row><row><cell>Stride</cell><cell>1</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,208.45,116.91,198.46,75.07"><head>Table 2 .</head><label>2</label><figDesc>Other details for CNN</figDesc><table coords="5,208.45,135.96,198.46,56.02"><row><cell>Hyperparameter</cell><cell>Value</cell></row><row><cell cols="2">Number of Units in Fully Connected Layer 500</cell></row><row><cell>Batch Size</cell><cell>20</cell></row><row><cell>Learning Rate</cell><cell>0.005</cell></row><row><cell>Training Epochs</cell><cell>100</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,206.72,433.78,201.91,43.94"><head>Table 3 .</head><label>3</label><figDesc>Test results as supplied by ImageCLEF.</figDesc><table coords="5,215.35,454.58,184.65,23.14"><row><cell cols="3">Exact Match Any Match Hamming Similarity</cell></row><row><cell>49.7%</cell><cell>59.6%</cell><cell>84.9%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="2,144.73,646.84,335.86,7.86;2,144.73,657.79,172.53,7.86"><p>Krizhevsky et. al. [8] used approximately 1.2 million labelled examples for their breakthrough result in ImageNET in 2012.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="3,144.73,658.44,202.42,7.47"><p>http://deeplearning.net/tutorial/lenet.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2" coords="5,144.73,657.79,236.33,7.86"><p>Azure Standard A4 VM: 8-core 2.1GHz CPU, 14GB RAM</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was supported in part by a <rs type="grantName">Microsoft Azure for Research grant</rs>, which provided the cloud infrastructure to conduct our experiments.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_FDMGR5J">
					<orgName type="grant-name">Microsoft Azure for Research grant</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,138.35,459.06,342.24,7.86;6,146.91,470.02,333.67,7.86;6,146.91,480.98,333.67,7.86;6,146.91,491.94,314.77,7.86" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="6,395.71,480.98,84.88,7.86;6,146.91,491.94,145.85,7.86">General Overview of ImageCLEF at the CLEF 2015 Labs</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bromuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">K</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Uskudarli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">B</forename><surname>Marvasti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Aldana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Del Mar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roldn</forename><surname>Garcia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,503.14,342.24,7.86;6,146.91,514.10,207.92,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,303.96,503.14,176.62,7.86;6,146.91,514.10,57.45,7.86">Overview of the ImageCLEF 2015 medical clustering task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">K</forename><surname>Mohammed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,222.05,514.10,104.50,7.86">CLEF2015 Working Notes</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,525.30,342.24,7.86;6,146.91,536.26,333.67,7.86;6,146.91,547.22,43.26,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,310.70,525.30,169.89,7.86;6,146.91,536.26,47.38,7.86">Representation learning: a review and new perspectives</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,201.07,536.26,161.49,7.86">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">17981828</biblScope>
			<date type="published" when="2013-08">Aug. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,558.43,342.25,7.86;6,146.91,569.39,75.00,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,299.10,558.43,54.04,7.86">Deep learning</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,359.73,558.43,26.13,7.86">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page">436444</biblScope>
			<date type="published" when="2015-05">May 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,580.59,342.24,7.86;6,146.91,591.55,100.47,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,215.31,580.59,184.72,7.86">Deep learning in neural networks: an overview</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,407.21,580.59,51.68,7.86">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page">85117</biblScope>
			<date type="published" when="2015-01">Jan. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,602.75,342.24,7.86;6,146.91,613.71,333.68,7.86;6,146.91,624.67,129.02,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,202.02,602.75,218.27,7.86">Object recognition from local scale-invariant features</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,440.12,602.75,40.47,7.86;6,146.91,613.71,23.96,7.86;6,203.47,613.71,273.02,7.86">The Proceedings of the Seventh IEEE International Conference on</title>
		<imprint>
			<date type="published" when="1999">1999. 1999</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11501157</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct coords="6,138.35,635.88,342.25,7.86;6,146.91,646.84,333.68,7.86;6,146.91,657.79,222.19,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,362.93,635.88,117.67,7.86;6,146.91,646.84,318.75,7.86">Content-based medical image retrieval: a survey of applications to multidimensional and multimodality data</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fulham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,473.30,646.84,7.30,7.86;6,146.91,657.79,56.66,7.86">J. Digit. Imaging</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">10251039</biblScope>
			<date type="published" when="2013-12">Dec. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,120.67,342.25,7.86;7,146.91,131.63,333.67,7.86;7,146.91,142.59,333.68,7.86;7,146.91,153.55,147.35,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,340.12,120.67,140.48,7.86;7,146.91,131.63,123.79,7.86">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,288.09,131.63,192.50,7.86;7,146.91,142.59,29.49,7.86">Advances in Neural Information Processing Systems 25</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page">10971105</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,164.51,342.25,7.86;7,146.91,175.46,333.68,7.86;7,146.91,186.42,235.91,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,402.36,164.51,78.24,7.86;7,146.91,175.46,135.15,7.86">ImageNet: A largescale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,302.30,175.46,174.10,7.86;7,170.98,186.42,134.54,7.86">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page">248255</biblScope>
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct coords="7,142.62,197.38,337.98,7.86;7,146.91,208.34,333.68,7.86;7,146.91,219.30,333.68,7.86;7,146.91,230.26,185.93,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,236.89,208.34,243.70,7.86;7,146.91,219.30,291.53,7.86">Evaluating performance of biomedical image retrieval system-sAn overview of the medical image retrieval task at ImageCLEF 20042013</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,445.27,219.30,35.33,7.86;7,146.91,230.26,85.28,7.86">Comput. Med. Imaging Graph</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">5561</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.62,241.22,337.98,7.86;7,146.91,252.18,333.68,7.86;7,146.91,263.14,65.55,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,312.20,241.22,168.39,7.86;7,146.91,252.18,274.69,7.86">A review of content-based image retrieval systems in medical applicationsclinical benefits and future directions</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Michoux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bandon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,428.34,252.18,52.25,7.86;7,146.91,263.14,38.17,7.86">International journal</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.62,274.09,337.97,7.86;7,146.91,285.05,333.68,7.86;7,146.91,296.01,333.68,7.86;7,146.91,306.97,216.26,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,266.24,285.05,214.36,7.86;7,146.91,296.01,172.08,7.86">Teaching &amp; Learning System for Diagnostic Imaging -Phase I: X-Ray Image Analysis &amp; Retrieval</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S S</forename><surname>Faruque</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Shahriar Faruque</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Shourav</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kazi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">Ashraful</forename><surname>Mahady</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Amin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,335.95,296.01,144.64,7.86;7,146.91,306.97,187.83,7.86">Proceedings of the 7th International Conference on Computer Supported Education</title>
		<meeting>the 7th International Conference on Computer Supported Education</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.62,317.93,337.98,7.86;7,146.91,328.89,333.68,7.86;7,146.91,339.85,208.83,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,146.91,328.89,289.86,7.86">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,444.69,328.89,35.91,7.86;7,146.91,339.85,44.53,7.86">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">19291958</biblScope>
			<date type="published" when="2014-01">Jan. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.62,350.81,337.98,7.86;7,146.91,361.77,148.42,7.86" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="7,198.44,350.81,282.15,7.86;7,146.91,361.77,30.56,7.86">Practical recommendations for gradient-based training of deep architectures</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>arXiv [cs.LG</idno>
		<imprint>
			<date type="published" when="2012-06">Jun-2012</date>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.62,372.73,337.98,7.86;7,146.91,383.68,333.67,7.86;7,146.91,394.64,191.63,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="7,369.72,372.73,110.88,7.86;7,146.91,383.68,134.27,7.86">A committee of neural networks for traffic sign classification</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,297.84,383.68,182.74,7.86;7,146.91,394.64,105.11,7.86">Neural Networks (IJCNN), The 2011 International Joint Conference on</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page">19181921</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.62,405.60,337.98,7.86;7,146.91,416.56,319.77,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="7,351.18,405.60,129.41,7.86;7,146.91,416.56,96.96,7.86">Gradient-based learning applied to document recognition</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,250.87,416.56,44.23,7.86">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">22782324</biblScope>
			<date type="published" when="1998-11">Nov. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.62,427.52,337.98,7.86;7,146.91,438.48,333.67,7.86;7,146.91,449.44,119.41,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="7,268.24,427.52,212.36,7.86;7,146.91,438.48,34.86,7.86">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,198.81,438.48,281.77,7.86;7,146.91,449.44,41.58,7.86">Proceedings of the 27th International Conference on Machine Learning (ICML-10)</title>
		<meeting>the 27th International Conference on Machine Learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">807814</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.62,460.40,337.98,7.86;7,146.91,471.36,205.80,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="7,322.78,460.40,157.81,7.86;7,146.91,471.36,101.37,7.86">Rectifier Nonlinearities Improve Neural Network Acoustic Models</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,255.83,471.36,33.81,7.86">W-&amp;CP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.62,482.31,337.98,7.86;7,146.91,493.27,333.68,7.86;7,146.91,504.23,82.98,7.86" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="7,146.91,493.27,304.18,7.86">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-07">Jul-2012</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>arXiv [cs.NE</note>
</biblStruct>

<biblStruct coords="7,142.62,515.19,337.98,7.86;7,146.91,526.15,333.68,8.12;7,146.91,537.11,23.55,7.86" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="7,151.52,515.19,187.88,7.86">Classifying plankton with deep neural networks</title>
		<ptr target="http://benanne.github.io/2015/03/17/plankton.html" />
		<editor>Sander Dieleman</editor>
		<imprint>
			<date type="published" when="2015-05-30">30-May-2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.62,548.07,337.97,7.86;7,146.91,559.03,333.68,7.86;7,146.91,569.99,255.01,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="7,302.95,548.07,177.64,7.86;7,146.91,559.03,160.75,7.86">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,327.18,559.03,153.41,7.86;7,146.91,569.99,177.18,7.86">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page">513520</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.62,580.94,337.97,7.86;7,146.91,591.90,333.68,7.86;7,146.91,602.86,67.07,7.86" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="7,351.24,580.94,129.35,7.86;7,146.91,591.90,187.55,7.86">Deep learning with non-medical training used for chest pathology identification</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Diamant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,352.00,591.90,89.35,7.86">SPIE Medical Imaging</title>
		<imprint>
			<biblScope unit="page" from="94140V" to="94140V" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
