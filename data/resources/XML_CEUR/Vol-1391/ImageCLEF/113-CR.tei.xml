<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,145.80,142.31,319.99,11.85;1,224.88,158.75,161.73,11.85">Using Textual and Visual Processing in Scalable Concept Image Annotation Challenge</title>
				<funder ref="#_GHMtaXu">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,152.88,194.80,60.85,8.48"><forename type="first">Alexandru</forename><surname>Calfa</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UAIC</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,219.96,194.80,50.76,8.48"><forename type="first">Dragoș</forename><surname>Silion</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UAIC</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,276.72,194.80,63.79,8.48"><forename type="first">Cristina</forename><surname>Andreea</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UAIC</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,342.96,194.80,75.92,8.48"><roleName>Cornel</roleName><forename type="first">Paul</forename><surname>Bursuc</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UAIC</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,421.32,194.80,33.53,8.48;1,152.88,205.60,52.31,8.48"><forename type="first">Răzvan</forename><forename type="middle">Iulian</forename><surname>Acatrinei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UAIC</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,207.60,205.60,93.71,8.48"><forename type="first">Alexandru</forename><forename type="middle">Eduard</forename><surname>Lupu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UAIC</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,303.72,205.60,61.07,8.48"><forename type="first">Cristian</forename><surname>Cozma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UAIC</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,367.20,205.60,67.19,8.48"><forename type="first">Adrian</forename><surname>Pădurariu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UAIC</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,436.80,205.60,21.79,8.48"><surname>Iftene</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UAIC</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,145.80,142.31,319.99,11.85;1,224.88,158.75,161.73,11.85">Using Textual and Visual Processing in Scalable Concept Image Annotation Challenge</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2C388E098A725E4DDC52333637772CF8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Text processing</term>
					<term>Visual Processing</term>
					<term>Text Generation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes UAIC 1 's system built for participating in the Scalable Concept Image Annotation challenge 2015. We submitted runs both for Subtask 1 (Image Concept detection and localisation) and for Subtask 2 (Generation of Textual Descriptions of Images). For the first subtask we created an ontology with relations between concepts and their synonyms, hyponyms and hypernyms and also with relations between concepts and related words. For the second subtask, we created a resource that contains triplets (concept1, verb, concept2), where concepts are from the list of concepts provided by the organizers and verb is a relation between concepts. With this resource we build sentences in which concept1 is subject, verb is predicate and concept2 is complement.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In 2015, UAIC group participated again in CLEF labs <ref type="bibr" coords="1,357.24,457.72,11.03,8.48" target="#b0">[1]</ref> in few ImageCLEF tasks <ref type="bibr" coords="1,143.28,468.52,10.79,8.48" target="#b1">[2]</ref> and in this way we continued our previous participation from 2013 when we participated in Plant Identification task <ref type="bibr" coords="1,295.80,479.44,10.12,8.48" target="#b2">[3]</ref>. Like in the 2014 campaign, the Scalable Concept Image Annotation challenge (from Image CLEF 2015 -Image Annotation<ref type="foot" coords="1,462.36,489.92,2.81,5.11" target="#foot_1">2</ref> ) task in 2015aims to develop systems that receive as input an image and produce as output a prediction of which concepts are present in that image, selected from a predefined list of concepts. In addition, this year the participants must describe images, localize the different concepts in the images and generate a description of the scene. This year the task was composed by two subtasks using a data source with 500,000 web page items. For each item we have a corresponding web page, an image and the keywords extracted from the web page. The participants must annotate and localize concepts and/or generate sentence descriptions for all 500,000 items. More details about challenge from 2015 are in <ref type="bibr" coords="1,297.12,587.44,11.03,8.48" target="#b3">[4]</ref> and details about challenge from 2014 are in <ref type="bibr" coords="1,152.88,598.24,10.12,8.48" target="#b4">[5]</ref>.</p><p>In 2014, three teams <ref type="bibr" coords="2,240.48,140.44,48.95,8.48">[6, 7 and 8]</ref> based their system on Convolutional Neural Networks (CNN) pre-trained using ImageNet <ref type="bibr" coords="2,322.68,151.24,11.14,8.48" target="#b8">[9]</ref>.Also, most of the teams proposed approaches based on classifiers that need to be learned <ref type="bibr" coords="2,356.88,162.04,11.03,8.48" target="#b5">[6]</ref> or based on classification with constructed ontologies <ref type="bibr" coords="2,249.36,172.96,14.54,8.48" target="#b9">[10]</ref>.</p><p>The rest of the paper is structured as follows: Section 2 details the general architecture of our system, Section 3 presents the results and an error analysis, while the last Section discusses the conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System components</head><p>In 2015, UAIC submitted runs both for concept identification (Subtask 1) and for generation of a description of the scene (Subtask 2). For that, we built a system, consisting in modules specialized for text processing and visual processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Subtask 1 -Textual processing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Google Translate</head><p>This module calls the Google Translation service<ref type="foot" coords="2,328.56,372.68,2.81,5.11" target="#foot_2">3</ref> and it uses a file which contains the most frequent words in English and a cache file with already translated words. This cache file contains pairs of words in a foreign language and their translation in English.</p><p>Thus, for translating a current word from the initial file, we consider the following cases:</p><p>• Case 1: If the current word is in the file with most used words in English then the program uses this English form and then it skips to the new word from the initial file. If the new word isn't in this file then we search for it in the cache file (Case 2). • Case 2: If the current word is in the cache with translated words we take the translated form and we skip to the new word from initial file. If not, we call the translation service (Case 3). • Case 3: Before calling the translation service, we identify the language of the word and then we translate this word from the identified language in English.</p><p>Example: The file with English words contains around 50.000 words and the file with cache contains 343.121 pairs like (initial_word, translated_word) (until last submission). From what we see, the files were accessed an average of 400 times per minute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Stop-words Elimination</head><p>This component receives a file with 500,000 lines and tries to remove every stopword from every line with its associated number which represents its frequency. We consider additional elimination of classical stop-words (the, from, it, he, be, is, has,…) and the elimination of all words with one or two characters. The number of classical stop-words was 667.</p><p>For example, for input line: In the end, from a total of 43.448.058 words in the input file, 16.802.111 stop words were eliminated. From 2 files summing up 457.1 Mb we obtained one file with 299 Mb.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Concept Identification</head><p>At this step, we start to build an ontology with relations between the initial 250 concepts and related words to them. For that we used the WordNet<ref type="foot" coords="3,413.16,486.44,2.81,5.11" target="#foot_3">4</ref>  <ref type="bibr" coords="3,420.12,486.76,15.71,8.48" target="#b10">[11]</ref> and we extracted in average around three synonyms and an average of five words that are somehow related to the concepts (automatically extracted from WordNet (hyponyms or hypernyms) and manually verified by human annotators or manually added by human annotators).</p><p>For example, for a concept we have the following information:</p><p>Concept: bicycle Synonyms: bike cycle, two-wheeler, mountain bike, ten-speed, racing bike, recumbent fixie, penny-farthing, ordinary velocipede Lexical family: park garden, tree, flower, wood, grass, path, kid, child, sport, sun, marathon, equipment, outfit, protection, street, saddle, handle bar</p><p>With this file we execute on the processed file with 500.000 lines (after steps 2.1.1 and 2.1.2) a module which identifies related concepts for every line. For that, for each word, we try to find a way to connect it to concepts. That implies searching for it in the list of concepts (case 1) or in list of synonyms (case 2) or in list related to lexical family (case 3). If a match is found, the word is replaced with its related concept and placed in the output file along with its initial number (case 1 and case 2) or with a lower value (case 3).All words that could not be associated with any concept have been eliminated along with their number.</p><p>At the next step we sum the frequencies for the same concept and we put in the output only one value, with a unique appearance of this concept ID and with the individual sum value. Next, we normalize all the values on lines and we replace the individual sum value for every concept with a percentage value obtained after we calculate a global sum with all individual sum values.</p><p>For example, an input line looks like: Additional, to this we built a file with relations between concepts and in case that one concept is present, we consider also the related concepts with a smaller percentage. For example, we consider a relation between "ear" and eye (because they are both body parts), and if we identify in an image the concept "eye" with score 0.0941, we consider also the concept "ear" but with a lower score equal with 0.0001. Although both the input file and the output file for this module are very close in size, they differ a lot due to the lack of concept-related words on some lines of the initial file.</p><formula xml:id="formula_0" coords="4,143.28,301.85,50.67,6.45">timepieces</formula><p>Because, it would have taken us 400 minutes to parse all 500.000 line of data single-threaded but our program takes around 45 second per 50.000 lines because we decided to work with 10 execution threads. We need some additional time for the reconstruction of the final file with 500.000 data, but in the end we reduced the execution time from 400 minutes to about 15 minutes. All programs were run on an 8core i7 Intel processor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Subtask 1 -Visual Processing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Face recognition</head><p>After downloading all pictures from the URL file using a script, we ran the JJIL <ref type="foot" coords="5,451.68,193.28,2.81,5.11" target="#foot_4">5</ref> for face recognition on all of them. The information we got from this part was merged with the information obtained by the textual processing component and converted in the output form. It's worth mentioning that in around 10% of images faces were detected (in 48.000 images), though only 75% of images where put through the face recognition JJIL API (375.000 files) since only this many links were valid.</p><p>For example from the following line from the input file:</p><formula xml:id="formula_1" coords="5,143.28,554.81,280.23,16.05">n03046257 0.7206 n02782093 0.1077 n02866578 0.1071 n04199027 0.0374 ….</formula><p>This is how the corresponding output looks:</p><p>1 000bRjJGbnndqJxV n03046257 0.7:128x126+0+0 n02782093 0.1:128x126+0+0 n02866578 0.1:128x126+0+0 n04199027 … Downloading about 25.000 images took around 4 hours and after that, detecting faces in those images took another 2 hours. All programs were run on a single Thread on multiple computers to reduce the overall time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Body parts identification</head><p>Based on results obtained at 2.2.1 and on image size (width and height), we tried to approximate the position of the face features using basic human proportions. If the resulting bounding boxes where inside the image then we used them as they were. However if one of them was exiting the image boundaries we would cut the outside part or, if necessary, removed them totally. We used this process for eyes, nose, lips, head, legs and feet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Subtask 2 -Text Generation</head><p>For Subtask 2 we built a matrix with relations between concepts. In fact we built triplets in form (concept1, verb, concept2), where concepts are from concepts provided by the organizers and verb is a relation between concepts. With this resource we build sentences in which concept1 is subject, verb is predicate and concept2 is complement. For example, in our matrix are the following types of triplets:</p><p>• body_part -wearing -accessories;</p><p>• animal -drinking -drink;</p><p>• insect -in on -land vehicle;</p><p>• animal -near -man made object;</p><p>• animal -playing -sport_item_or_toy. The rate of success in creating sentences for the given images was greater for the clean track in comparison with noisy track. The program used was single threaded and took about 4 hours to complete all 500.000 data on a dual core processor. This is how lines of input data looks like:</p><p>1. 000bRjJGbnndqJxV n03046257 n04555897 n02866578 n03249569 n04574999 n06410904 n05600637 n02778669 n05564590 n03479952 n06277280 n02958343 2. 006fmXbGJW3UhmjI n03623556 3. 00DIvt1Zik2Vo1yY n07739125 n05254795 n04100174 n02849154 n03135532 n07848338 n05563770 n02801525 n09328904 n10287213 n03479952 n07747607 n04197391 n05311054 n05598147 n04379243</p><p>This is how the same lines of data looks like at output:</p><formula xml:id="formula_2" coords="6,143.28,584.81,106.59,6.45">1. 2 000bRjJGbnndqJxV</formula><p>The clock is near a ball, the drum is in a hallway and the wheel is near a clock. 2. 2 006fmXbGJW3UhmjI Empty sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">2 00DIvt1Zik2Vo1yY</head><p>The apple is on a table, the blanket is on a table and the cross is near a table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">2 00k_Jt7GwBTWPDIP</head><p>The tower has a door, the pen is on a table and the radio is on a table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results and Evaluation</head><p>For the 2015 task, our team submitted 4 runs for Subtask 1 and 4 runs for Subtask 2.</p><p>The description and duration for every run is presented in bellow Table. Based on basic textual processing, like stop-words elimination, lemmatization, using of ontology 3 hours</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run 2</head><p>Additional to Run 1 we used the Google Translation service 5 days Run 3</p><p>Additional to Run 2 we used visual processing for face recognition 12 hours</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run 4</head><p>Additional to Run 3, based on rectangle associated to face, we to add concepts related to body, arms, foots, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">hours</head><p>For Run 2, we used the Google Translation service <ref type="foot" coords="7,336.72,308.96,2.81,5.11" target="#foot_5">6</ref> and because we were limited to a number of requests per second, we inserted some delays between successive calls of the service. Also, we created a cache with already translated words and before calling the translation service we checked to see if we have the current word in our cache. If we did, we skipped the current word and we used the translation from cache. Because in the input file were millions of words, this component for translation runs more than 5 days, and in the end we didn't succeeded to translate all words from the initial file.</p><p>For the Run 3 and Run 4 we used only the created cache and for this reason the duration is lower than that of Run 2. For Run 3 we run the face recognition component on all images, on a distributed architecture with 10 different threads and in the end we concatenated the results. For Run 4 we used the partial results from runs 2 and 3 and for this reason it took less hours. In the case of Subtask 2 noisy track, from run to run we completed our resource file with triplets of type (concept1, verb, concept2), and similar we add more new rules for selection of the most relevant concepts. Of course, the time duration for execution increased from run to run. Similar with noisy track, we submitted five runs from R9 to R13 for Subtask 2 clean track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Evaluation for Subtask 1</head><p>Table <ref type="table" coords="8,168.00,203.92,4.67,8.48" target="#tab_5">3</ref> below gives the results for the runs from Subtask 1 described above. More details are in <ref type="bibr" coords="8,193.56,214.72,10.12,8.48" target="#b3">[4]</ref>. As we can see from Table <ref type="table" coords="8,254.88,306.64,3.51,8.48" target="#tab_5">3</ref>, the better run is the R4, where we use all created components (translation, stop-word elimination, concept identification, face recognition, body components identification, etc.). Also, in this run we used the final versions for our resources files with English words, with translated pairs, with concept synonyms, hyponyms and hypernyms, and related words. The most important component is component related to the identification of body parts (which is used only in R4). We can see how results for R1, R2 and R3 are much closed, but the results for R5 are more than twice as good. Because, between runs (R1 and R2), (R2 and R3) and (R3 and R4), we improve continuous all our resources (resources for translation, ontology, list with stop-words, etc.) it is hard for us to say what was the impact of every step performed by us.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation for Subtask 2</head><p>Tables 4and 5 from below give the results for the runs from Subtask 2noisy track and Subtask 2 clean track described above. were from run to run we improve our resource with triplets or our rules for building sentences, we obtained our runs from R9 to R13. For these runs we start from resources obtained for R8 and then from run to run we analyse our results and we changed our resources in order to obtain better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>This paper presents the system developed by UAIC for the Scalable Concept Image Annotation Challenge from ImageCLEF 2015.This system contains components for Subtask 1 (Image Concept detection and localisation) and for Subtask 2 (Generation of Textual Descriptions of Images).</p><p>For Subtask 1, the main components of the system are related to text processing (the translation of non-English words, stop-words elimination, and concept identification) and to visual processing (face recognition and body parts identification). From the presented results we can conclude that the most important component is component related to body parts identification which increased significantly our results.</p><p>For Subtask 2, the main components are related to applying templates on selected concepts, based on a resource with triplets (concept1, verb, concept2). From what we see, the most important part is related to the selection of most important concepts, and from this reason the results for clean track are much better than results for noisy track.</p><p>For the future, we aim to use more visual processing in order to identify more concepts from images. Also, on textual processing we want to reduce the time duration for translation, which was the most time consuming component.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,224.64,439.59,162.17,7.70;5,153.96,460.84,282.08,8.48;5,143.28,481.61,248.67,6.45;5,169.44,245.76,272.76,185.88"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An example of image that contains facesFor example for image from Fig.1, the output after we use the JJIL API is: n05538625 0.5:165x180+220+273,0.5:155x167+439+227</figDesc><graphic coords="5,169.44,245.76,272.76,185.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,143.28,560.51,325.11,54.66"><head>portada 1104 toronto 1080 por 1029 vuelve 990 sorprender 987 cada 974 escenario 970 pisa 961 concierto 933 … English: … home 1104 toronto 1080 by 1029back 990 surprise 987 each 974 scenario 970 pisa 961 concert 933 …</head><label></label><figDesc></figDesc><table /><note coords="2,153.96,560.69,87.71,6.45"><p>Other language:…</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,143.28,301.85,325.11,77.13"><head></head><label></label><figDesc>This is how the same line of data looks like at output:</figDesc><table coords="4,143.28,301.85,325.11,77.13"><row><cell>81 timepiece 7988 time 3252 thesaurus 1595 device</cell></row><row><cell>1569 measuring 1286 instrument 1285 clock 1268 wheel 1232 noun</cell></row><row><cell>1170 balance 1162 legend 1105 dictionary 1095 …</cell></row><row><cell>n03046257 0.527 n04555897 0.1802 n02866578 0.0769 n03249569</cell></row><row><cell>0.063 n04574999 0.0604 n06410904 0.0147 …</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,163.32,199.47,290.18,34.84"><head>Table 1 :</head><label>1</label><figDesc>Description of runs for Subtask 1</figDesc><table coords="7,163.32,216.27,290.18,18.04"><row><cell>Description</cell><cell>Duration</cell></row><row><cell>Run 1</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,163.32,450.15,286.37,124.46"><head>Table 2 :</head><label>2</label><figDesc>Description of runs for Subtask 2 Noisy track</figDesc><table coords="7,163.32,466.95,286.37,107.66"><row><cell></cell><cell>Description</cell><cell>Duration</cell></row><row><cell>Run 5</cell><cell>Using file with triplets (concept1, verb, concept2), we</cell><cell>75 minutes</cell></row><row><cell></cell><cell>build sentences based on first two concepts received as</cell><cell></cell></row><row><cell></cell><cell>input</cell><cell></cell></row><row><cell>Run 6</cell><cell>Similar to Run 5, where we improve the number of</cell><cell>2.5 hours</cell></row><row><cell></cell><cell>triplets</cell><cell></cell></row><row><cell>Run 7</cell><cell>Similar to Run 6, with an improved rule for selection of</cell><cell>2.5 hours</cell></row><row><cell></cell><cell>concepts</cell><cell></cell></row><row><cell>Run 8</cell><cell>Similar to Run 7, with a new version for file with triplets</cell><cell>5 hours</cell></row><row><cell></cell><cell>and with new rules used in selection of most relevant</cell><cell></cell></row><row><cell></cell><cell>concepts</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,176.40,236.67,262.82,55.10"><head>Table 3 :</head><label>3</label><figDesc>Results of UAIC's runs from Subtask 1</figDesc><table coords="8,176.40,253.59,262.82,38.18"><row><cell>% Overlap with GT</cell><cell>R1</cell><cell>R2</cell><cell>R3</cell><cell>R4</cell></row><row><cell>labels</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>50 %</cell><cell>0.020719</cell><cell>0.020009</cell><cell>0.021134</cell><cell>0.055917</cell></row><row><cell>0 %</cell><cell>0.185071</cell><cell>0.185288</cell><cell>0.18522</cell><cell>0.265927</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="8,203.64,500.19,209.54,76.58"><head>Table 4 :</head><label>4</label><figDesc>Results of UAIC's runs from Subtask 2 noisy track</figDesc><table coords="8,205.56,516.99,207.62,59.78"><row><cell></cell><cell>R5</cell><cell>R6</cell><cell>R7</cell><cell>R8</cell></row><row><cell>MEAN</cell><cell>0.0409</cell><cell>0.0389</cell><cell>0.0483</cell><cell>0.0813</cell></row><row><cell>STDDEV</cell><cell>0.0310</cell><cell>0.0286</cell><cell>0.0389</cell><cell>0.0513</cell></row><row><cell>MEDIAN</cell><cell>0.0309</cell><cell>0.0309</cell><cell>0.0331</cell><cell>0.0769</cell></row><row><cell>MIN</cell><cell>0.0142</cell><cell>0.0142</cell><cell>0.0142</cell><cell>0.0142</cell></row><row><cell>MAX</cell><cell>0.2954</cell><cell>0.2423</cell><cell>0.2954</cell><cell>0.3234</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="8,188.76,591.51,236.06,55.82"><head>Table 5 :</head><label>5</label><figDesc>Results of UAIC's runs from Subtask 2 Clean trackWe can see how from run to run the results are improved. For noisy track R8 is much better than R5, R6 and R7 which have almost similar values. For clean track results are more closed, but we can see how these results are much better than results from noisy task. This mean that the selection of most relevant concept is the hardest part and by this step will depend the final result of this track. Similar to runs R5 to R8</figDesc><table coords="8,188.76,608.31,236.06,39.02"><row><cell></cell><cell>R9</cell><cell>R10</cell><cell>R11</cell><cell>R12</cell><cell>R13</cell></row><row><cell>MEAN</cell><cell>0.1709</cell><cell>0.2055</cell><cell>0.2080</cell><cell>0.2093</cell><cell>0.2097</cell></row><row><cell>STDDEV</cell><cell>0.0771</cell><cell>0.0589</cell><cell>0.0654</cell><cell>0.0661</cell><cell>0.0660</cell></row><row><cell>MEDIAN</cell><cell>0.1762</cell><cell>0.2078</cell><cell>0.2082</cell><cell>0.2082</cell><cell>0.2085</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,146.16,635.10,175.95,7.66"><p>University "Alexandru Ioan Cuza" of Iasi, Romania</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,146.16,644.94,279.86,7.66"><p>ImageCLEF 2015 -Image Annotation: http://www.imageclef.org/2015/annotation</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,148.32,644.94,229.00,7.66"><p>Google Translation service: https://cloud.google.com/translate/docs</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="3,148.32,644.94,188.66,7.66"><p>WordNet: http://wordnetweb.princeton.edu/perl/webwn</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="5,150.36,635.22,318.05,7.66;5,151.20,645.06,80.35,7.66"><p>JJIL Face Recognition: http://www.richardnichols.net/2011/01/java-facial-recognition-haarcascade-with-jjil-guide/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="7,148.32,644.94,229.00,7.66"><p>Google Translation service: https://cloud.google.com/translate/docs</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. The research presented in this paper was funded by the project <rs type="projectName">MUCKE (Multimedia and User Credibility Knowledge Extraction</rs>), number <rs type="grantNumber">2 CHIST-ERA/01.10.2012</rs>. Special thanks go to all colleagues from the <rs type="institution">Faculty of Computer Science</rs>, second year, group A1, who were involved in this project.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_GHMtaXu">
					<idno type="grant-number">2 CHIST-ERA/01.10.2012</idno>
					<orgName type="project" subtype="full">MUCKE (Multimedia and User Credibility Knowledge Extraction</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,146.43,621.30,321.95,7.66;9,160.20,631.02,308.11,7.66;9,160.20,640.74,22.49,7.66" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,391.08,621.30,77.30,7.66;9,160.20,631.02,36.57,7.66">CLEF 2015 Labs and Workshops</title>
	</analytic>
	<monogr>
		<title level="s" coord="9,203.04,631.02,218.65,7.66">Notebook Papers. CEUR Workshop Proceedings (CEUR-WS.org</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>And San Juan</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">1391</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,146.43,140.34,321.95,7.66;10,160.20,150.18,308.23,7.66;10,160.20,159.90,308.19,7.66;10,160.20,169.62,308.11,7.66;10,160.20,179.34,22.49,7.66" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,330.48,159.90,137.91,7.66;10,160.20,169.62,57.92,7.66">General Overview of ImageCLEF at the CLEF 2015 Labs</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bromuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">K</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Uskudarli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Marvasti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Aldana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="10,343.08,169.62,121.68,7.66">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,146.44,189.06,321.83,7.73;10,160.20,198.78,308.30,7.66;10,160.20,208.50,308.23,7.66;10,160.20,218.22,192.41,7.66" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,430.20,189.06,38.06,7.66;10,160.20,198.78,308.30,7.66">Combining image retrieval, metadata processing and naive Bayes classification at Plant Identification</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Șerban</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sirițeanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gheorghiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Iftene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Alboaie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Breabăn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,184.20,208.50,284.23,7.66;10,160.20,218.22,44.42,7.66">Notebook Paper for the CLEF 2013 LABs Workshop -ImageCLEF -Plant Identification</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-09">2013. September. 2013</date>
			<biblScope unit="page" from="23" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,146.44,228.06,321.99,7.66;10,160.20,237.66,307.99,7.66;10,160.20,247.38,308.27,7.66;10,160.20,257.10,308.23,7.66;10,160.20,266.82,80.45,7.66" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,227.40,237.66,240.79,7.66;10,160.20,247.38,154.69,7.66">Overview of the ImageCLEF 2015 Scalable Image Annotation, Localization and Sentence Generation task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dellandrea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,334.68,247.38,133.79,7.66;10,160.20,257.10,79.36,7.66">CLEF2015 Working Notes -CEUR Workshop Proceedings</title>
		<title level="s" coord="10,247.44,257.10,67.62,7.66">Publisher CEUR-WS</title>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">September 8-11. (2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,146.44,276.66,321.83,7.66;10,160.20,286.38,308.23,7.66;10,160.20,295.98,22.49,7.66" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,255.48,276.66,212.79,7.66;10,160.20,286.38,55.38,7.66">Overview of the ImageCLEF 2014 Scalable Concept Image Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,233.52,286.38,148.54,7.66">CLEF 2014 Evaluation Labs and Workshop</title>
		<title level="s" coord="10,388.56,286.38,76.22,7.66">Online Working Notes</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,146.44,305.70,321.99,7.66;10,160.20,315.54,308.08,7.66;10,160.20,325.26,271.49,7.66" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,442.56,305.70,25.87,7.66;10,160.20,315.54,195.50,7.66">MIL at ImageCLEF 2014: Scalable System for Image Annotation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kanehira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hidaka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Mukuta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tsuchiya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,370.92,315.54,97.36,7.66;10,160.20,325.26,129.28,7.66">CLEF 2014 Evaluation Labs and Workshop, Online Working Notes</title>
		<meeting><address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">September 15-18. (2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,146.44,334.98,321.99,7.66;10,160.20,344.70,308.18,7.66;10,160.20,354.42,308.23,7.66;10,160.20,364.14,22.49,7.66" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,160.20,344.70,245.77,7.66">MindLab at ImageCLEF 2014: Scalable Concept Image Annotation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Vanegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Arevalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Otálora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Páez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Pérez-Rubiano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">A</forename><surname>González</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,425.88,344.70,42.50,7.66;10,160.20,354.42,188.92,7.66">CLEF 2014 Evaluation Labs and Workshop, Online Working Notes</title>
		<meeting><address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">September 15-18. (2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,146.44,373.86,321.99,7.66;10,160.20,383.58,308.19,7.66;10,160.20,393.42,192.65,7.66" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,310.92,373.86,157.51,7.66;10,160.20,383.58,99.87,7.66">MLIA at ImageCLEF 2014 Scalable Concept Image Annotation Challenge</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Shimada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ichiro Taniguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,281.04,383.58,187.35,7.66;10,160.20,393.42,50.42,7.66">CLEF 2014 Evaluation Labs and Workshop, Online Working Notes</title>
		<meeting><address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">September 15-18. (2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,146.44,403.14,322.07,7.66;10,160.20,412.74,308.27,7.66;10,160.20,422.46,304.34,7.66" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,381.84,403.14,86.67,7.66;10,160.20,412.74,94.76,7.66">ImageNet: A large scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2009.5206848</idno>
	</analytic>
	<monogr>
		<title level="m" coord="10,271.92,412.74,145.15,7.66;10,446.28,412.74,22.19,7.66;10,160.20,422.46,80.67,7.66">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009-06">2009. June. (2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference</note>
</biblStruct>

<biblStruct coords="10,150.37,432.30,318.06,7.66;10,160.20,442.02,308.18,7.66;10,160.20,451.74,308.23,7.66;10,160.20,461.46,22.49,7.66" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,297.96,432.30,170.47,7.66;10,160.20,442.02,248.65,7.66">KDEVIR at ImageCLEF 2014 Scalable Concept Image Annotation Task: Ontology based Automatic Image Annotation</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">A</forename><surname>Reshma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">Z</forename><surname>Ullah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Aono</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,426.96,442.02,41.42,7.66;10,160.20,451.74,188.92,7.66">CLEF 2014 Evaluation Labs and Workshop, Online Working Notes</title>
		<meeting><address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">September 15-18. (2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,150.37,471.18,318.06,7.66;10,160.20,480.90,22.49,7.66" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="10,211.32,471.18,148.08,7.66">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
