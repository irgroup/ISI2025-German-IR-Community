<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,160.72,116.95,293.92,12.62;1,176.29,134.89,262.77,12.62">Medical Image Classification via 2D color feature based Covariance Descriptors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,239.02,172.78,54.96,8.74"><forename type="first">Pol</forename><surname>Cirujeda</surname></persName>
							<email>pol.cirujeda@upf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information and Communication Technologies</orgName>
								<orgName type="institution">Universitat Pompeu Fabra</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,316.67,172.78,59.66,8.74"><forename type="first">Xavier</forename><surname>Binefa</surname></persName>
							<email>xavier.binefa@upf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information and Communication Technologies</orgName>
								<orgName type="institution">Universitat Pompeu Fabra</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,160.72,116.95,293.92,12.62;1,176.29,134.89,262.77,12.62">Medical Image Classification via 2D color feature based Covariance Descriptors</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">21F4221204E373145ED0AAF2938208D8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Covariance descriptor</term>
					<term>Medical image</term>
					<term>classification</term>
					<term>retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In these notes we present an image classification method which has been submitted to the ImageCLEF 2015 Medical Classification challenge. The aim is to classify images from 30 heterogeneous classes ranging from diagnose images coming from different acquisition techniques, to various biomedical publication illustrations. The presented work is intended to be a proof of concept of how our method, which uses only visual information, performs in the modelling of such image classes. Our approach uses 1 st and 2 nd order color features obtained at a whole image level. These features are considered as samples of a multidimensional statistical distribution, and a distinctive signature of the represented image can be built in the form of a Covariance-matrix based descriptor. The Riemannian manifold structure of such descriptors can be exploited in order to formulate an image classification methodology. Despite the challenging task due to unbalanced classes and image homogeneity, the obtained results in the task place our method on the top of the most accurate ones using purely visual features. This asserts the feasibility of our methodology and proves that its performance can be on par with other methods which use also complementary textual features for complex image retrieval.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Medical image classification provides a challenge on the identification of similar medical images: this is an interesting problem due to the subtle changes between different image sources. For instance, inside the range of microscopy images there exist different acquisition devices (light, electron, fluorescence or transmission) which are able to capture different tissue details. Despite of that, the resemblance between image cues is high and poses a challenging problem from a classification perspective <ref type="bibr" coords="1,186.87,609.07,9.96,8.74" target="#b5">[6]</ref>.</p><p>The ImageCLEF Medical Classification challenge <ref type="bibr" coords="1,369.88,621.25,10.52,8.74" target="#b7">[8]</ref> provides a benchmark to test the impact of different image classification and feature selection methods in retrieval, specially those using visual and/or textual information. We are presenting our results in the medical subfigure classification task, which provides 30 different classes including diagnose images (radiology, visible light photography, microscopy, etc.) and also generic biomedical illustrations. More details can be found in <ref type="bibr" coords="2,174.34,143.90,9.96,8.74" target="#b4">[5]</ref>.</p><p>In the Computer Vision research area, many pattern recognition methods have been developed for image classification and retrieval. Most of them include the development of content and feature selection functions, or the usage of keypoint extractors and associated descriptors which can be later categorized by supervised classification methods (Support Vector Machines, Boosting, Neural Networks, etc). Our presented approach is based in our ongoing research in Covariance-based descriptors, and we are specially motivated by the demanding conditions found in the different images of the medical classification subtask. Our method provides a simplistic formulation, which provides a discriminative signature for a whole image according to the variation of different features at its pixels. We are particularly interested in seeing if this proposed description, using purely visual information, is discriminative enough.</p><p>The following sections of this report present an overview of our methodology, the results obtained on the train data and the own challenge, and a final discussion about some aspects of the presented approach and associated future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>An inspection of the provided images of this medical classification task makes evident that class separation from purely visual cues is not a trivial task. Different image sources might share visual features, or suffer from a lack of discriminative salient cues (see Fig. <ref type="figure" coords="2,248.50,433.98,3.87,8.74" target="#fig_0">1</ref>). Nevertheless, this also yields to our first intuition of what should be taken into account. First of all, there are several information cues that are equally important: not only texture patterns, but also color, sparsity, structure features... And in a second place, even more important than the features themselves: the modelling must take into account all the feature interactions together. That is, a diagram figure in a medical publication can be in grayscale just as an electron microscopy image, but structural features in a diagram contain pure lines or geometrical shapes which are not present on a biological tissue captured by the microscope. At the same time, different microscopy devices might capture similar natural tissue patterns, but for instance a visible light microscope can capture a different range of color spectra than a transmission microscope. Therefore, in an analogy with a natural visual perceptual system, our goal is to model the space of different visual cues and their joint relationships, and correlate them to the wide range of image classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Visual features Covariance-based descriptor</head><p>An ideal image representation must encode all images in a common compact, size invariant notation regardless of the different image sizes. The description must also be robust to intraclass spatial transformations, such as rotations, and if possible it should not depend on computationally loading intermediate stages, such as keypoint extractions. The intuition behind our method is not to use image features themselves, but rather than that observe the features along the complete image and consider them as unstructured samples of a multidimensional statistical distribution, using their covariance as a descriptive signature. Covariance matrices were introduced as descriptors in the Computer Vision domain by Tuzel et al. in <ref type="bibr" coords="3,245.44,469.17,10.52,8.74" target="#b6">[7]</ref> where they presented an object recognition method for 2D color images. In our ongoing research we have extended this framework to other domains such as 3D object recognition in unstructured point clouds <ref type="bibr" coords="3,467.32,493.08,9.96,8.74" target="#b2">[3]</ref>, gesture recognition in depth image sequences <ref type="bibr" coords="3,337.65,505.04,10.52,8.74" target="#b1">[2]</ref> or also tissue classification in 3D CT medical images <ref type="bibr" coords="3,239.37,516.99,9.96,8.74" target="#b3">[4]</ref>. By their construction, covariance-based descriptors are robust to noisy inputs and lose structural information about the observed features. Their representation capability is based on the statistical notion of covariance as a measure of how several random variables change together -a set of visual cues for any image in our case. Therefore, the proposed descriptor characterizes a given distribution of feature variations along the image, rather than using feature absolute values, which is independent of the number of used samples (the image size). This provides invariance to size and spatial rigid transformations such as rotations.</p><p>In order to formally define this 2D color feature based Covariance Descriptors, we denote a feature selection function Φ(I) for a given image I as:</p><formula xml:id="formula_0" coords="3,255.06,657.11,225.53,9.65">Φ(I) = {φ x,y ∀x, y ∈ I} ,<label>(1)</label></formula><p>which provides a set of feature vectors φ x,y for each one of the pixel coordinates {x, y} inside all the image I. These 11-dimensional feature vectors are expressed as:</p><formula xml:id="formula_1" coords="4,143.72,170.15,336.87,23.23">φ x,y = x, y, R x,y , G x,y , B x,y , |I x |, |I y |, |I xx |, |I yy |, I 2 x + I 2 y , arctan |I x | |I y | ,<label>(2)</label></formula><p>and include the pixel coordinates, the different RGB color values, first and second order image intensity derivatives and their magnitude and pixel curvature. These cues provide information about the color distribution of a given image class, as well as their texture patterns and visual structure -as found in the first and second order gradient and curvature features. Then, for a given color image I the associated Covariance Descriptor can be obtained as:</p><formula xml:id="formula_2" coords="4,204.74,280.62,271.61,30.32">Cov (Φ(I)) = 1 N -1 N i=1 (φ x,y -µ) (φ x,y -µ) T , (<label>3</label></formula><formula xml:id="formula_3" coords="4,476.35,291.03,4.24,8.74">)</formula><p>where µ is the vector mean of the set of vectors {Φ} within the image I.</p><p>The resulting 11 × 11 matrix Cov is a symmetric matrix where the diagonal entries will represent the variance of each feature channel, and the non-diagonal elements represent their pairwise covariance, as seen in Fig. <ref type="figure" coords="4,401.31,351.47,3.87,8.74" target="#fig_1">2</ref>. This provides a signature of how feature behave in a characteristic way for each one of the images of the different classes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Riemannian geometry of the descriptor space</head><p>Covariance Descriptors have the form of covariance matrices which, besides providing a compact and flexible representation, causes them to lie in the Riemannian manifold of symmetric definite positive matrices Sym + d . This has a major impact on their interest as descriptive units, as their spatial variety is geometrically meaningful: samples of classes sharing similar feature characteristics will remain under close areas in this descriptor space. Nevertheless, it is important to bear in mind that this spatial distribution is non Euclidean and has to be treated with its particular Riemannian metric in order to perform analytic operations with the descriptors.</p><p>According to <ref type="bibr" coords="5,211.99,191.72,9.96,8.74" target="#b0">[1]</ref>, the Riemannian manifold can be approximated in close neighborhoods by the Euclidean metric in its tangent space, T Y , where the symmetric matrix Y is a reference projection point in the manifold. T Y is formed by a vector space of d × d symmetric matrices, and the tangent mapping of a manifold element X to x ∈ T Y is made by the point-dependent log Y operation:</p><formula xml:id="formula_4" coords="5,218.68,260.08,257.66,13.97">x = log Y (X) = Y 1 2 log Y -1 2 XY -1 2 Y 1 2 . (<label>4</label></formula><formula xml:id="formula_5" coords="5,476.35,263.45,4.24,8.74">)</formula><p>For computational simplicity in certain problems, the projection point can be established to the Identity matrix, and therefore the tangent mapping becomes:</p><formula xml:id="formula_6" coords="5,261.11,319.30,219.48,8.74">log(X) = U log(D)U ,<label>(5)</label></formula><p>where U and D are the elements of the single value decomposition (SVD) of X ∈ Sym + d . In an analogous manner, the exponential mapping of a point y ∈ T Y returns its original point representation Y in the Sym + d manifold:</p><formula xml:id="formula_7" coords="5,260.51,396.61,220.08,8.74">exp(y) = U exp(D)U ,<label>(6)</label></formula><p>One property of the projected symmetric matrices in the tangent space T Y is that they contain only d(d+1)/2 independent coefficients, in their upper or lower triangular parts. Therefore it is possible to apply the vectorization operation in order to obtain a linear orthonormal space for the independent coefficients: x = vect(x) = (x 1,1 , x 1,2 , ..., x 1,d , x 2,2 , x 2,3 , ..., x d,d ), <ref type="bibr" coords="5,467.86,471.29,12.73,8.74" target="#b6">(7)</ref> where x is the mapping of X ∈ Sym + d to the tangent space, resulting from Eq. (4). The obtained vector x will lie in the Euclidean space R m , where m = d(d + 1)/2 -R 6 6 in the current approach.</p><p>This set of operations is useful for data visualization, feature selection, and for developing Machine Learning and classification techniques on top of the particular geometric space of the proposed Covariance Descriptors, specially taking into account the following Riemannian metric which expresses the geodesic distance between two points X 1 and X 2 on Sym + d :</p><formula xml:id="formula_8" coords="5,208.91,600.75,271.68,18.03">δ(X 1 , X 2 ) = T race log X -1 2 1 X 2 X -1 2 1 2 ,<label>(8)</label></formula><p>or more simply δ(X 1 , X 2 ) = d i=1 log(λ i ) 2 , where λ i are the positive eigenvalues of X</p><formula xml:id="formula_9" coords="5,184.03,652.02,50.32,15.91">-1 2 1 X 2 X -1 2 1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Classification via a Manifold-regularized sparse representation</head><p>For the classification of the proposed 2D color feature based Covariance Descriptors we propose a manifold-based sparse classification method which is part of our research as presented in previous approaches <ref type="bibr" coords="6,350.18,161.59,9.96,8.74" target="#b1">[2]</ref>. We intend to test the performance of this approach in the heterogeneous class distribution found in the ImageCLEF Medical classification task, and see if it is on par with other textualbased methods eventually presented to the challenge by other participants.</p><p>The topological layout of the proposed Covariance Descriptor yields to focus on a geometrically sensitive classification method which can exploit the Riemannian manifold distribution. Sparse representation based methods <ref type="bibr" coords="6,430.64,233.32,10.52,8.74" target="#b8">[9,</ref><ref type="bibr" coords="6,443.37,233.32,12.73,8.74" target="#b9">10]</ref> have shown a recent rise in the Machine Learning community in the context of face recognition. In this application, two key concepts are very relevant: sparsity and collaborativeness. They are related to the complexity of the model learning: not only because a complete set of learning samples is hardly available, but also because an unknown element can share characteristics from different classes. As this also the case in medical image retrieval, where images from a particular class might be scarce and the low-level visual cues provide a complex class definition, we propose a new sparse method formulation adapted to the manifold of 2D color based Covariance Descriptors.</p><p>The base intuition is that an unknown sample should be ideally represented, as accurate as possible, by using the smallest group of most similar samples from a learning set A. Then, a test sample in the form of a new vectorized Covariance descriptor C ∈ R 66 can be expressed as a linear combination on top of the tangent space T of the Sym + d manifold of the available set of training samples:</p><formula xml:id="formula_10" coords="6,134.77,412.65,37.77,8.74">C = Aα.</formula><p>Let A be the whole set of n training samples, in its vectorized form according to eq. 7, from K different classes: A = [A 1 , A 2 , ..., A K ] ∈ R 66×n , where each A i = {vect( i )} is the set of vectorized Covariance descriptors which form the subset of training samples for the class i. And let α = [α 1 , α 2 , ..., α K ] be a vector of weights corresponding to each one of the training samples in A. Then, the sparsity restriction on α can be achieved via its L2 norm minimization, proposing a manifold-aware minimization constraint which relaxes the computational expense of the method and adds numerical stability:</p><formula xml:id="formula_11" coords="6,231.94,530.13,244.41,18.14">α = argmin α C -Aα 2 2 + Dα 2 2 (<label>9</label></formula><formula xml:id="formula_12" coords="6,476.35,532.20,4.24,8.74">)</formula><p>where D is a diagonal matrix of size n × n which allows the imposition of prior knowledge on the solution with respect to the training set, using the Riemannian metric defined in eq. ( <ref type="formula" coords="6,229.68,577.24,3.87,8.74" target="#formula_8">8</ref>). This term contributes also on making the least squares solution stable, and on introducing forehand sparsity conditions to the vector α as well. D is defined as:</p><formula xml:id="formula_13" coords="6,239.43,620.99,241.16,47.42">D =     δ(A 1 , C ) 0 . . . 0 δ(A n , C )    <label>(10)</label></formula><p>where A i and C are the unvectorized covariance descriptors for training and test samples respectively. The solution to the sparse collaborative representation, α, can be calculated by the following derived expression according to <ref type="bibr" coords="7,426.66,143.90,14.61,8.74" target="#b9">[10]</ref>:</p><formula xml:id="formula_14" coords="7,248.93,165.37,231.67,13.11">α = A T A + D T D -1 A T C<label>(11)</label></formula><p>Finally, the classification label of the test sample C can be obtained by observing the regularized reconstruction residuals from the resulting sparse vector α:</p><formula xml:id="formula_15" coords="7,229.94,236.11,250.65,23.23">class(C) = argmin i C -A i αi 2 αi 2<label>(12)</label></formula><p>3 Results</p><p>The evaluation score used on the task performance assessment is the classification accuracy ratio for all the classes, computed as the ratio of true positives and negatives over the total number of samples. We collect the top results in Table <ref type="table" coords="7,134.77,344.15,3.87,8.74">3</ref>, which are also publicly available on the challenge website<ref type="foot" coords="7,398.91,342.58,3.97,6.12" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Features True positive ratio</head><p>Participants Before the submission of the task, we tested our method on the training data set, using a 10-fold cross-validation. Each fold was adapted so at least 20% of samples of each class were kept in each subset. In classes with a very low number of samples which would cause to have some folds without class representation, some samples where duplicated. Therefore, classes with very few samples where guaranteed to be balanced and represented on the training set of our classification method. After iterating the cross-validation runs, we obtained an average accuracy of 73.24 %. As we have commented in section 2.3, the presented classifier arises as a method for expressing unknown samples as the best sparse representation regarding to a learning set. Therefore, we explain this increase on the accuracy as a direct effect of the balancing preprocessing of those classes with very few elements.</p><p>Once the groundtruth annotations of the testing set have been made publicly available, we can analyse the different Precision and Recall values for each class as presented in Table <ref type="table" coords="8,230.67,167.90,3.87,8.74" target="#tab_1">2</ref>, and observe if there is a particular correlation between these values and the different cardinality of each class or their visual nature. These results assert our hypothesis of a mandatory class balancing stage in order to boost the accuracy performance of our proposed sparse classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and future work</head><p>The presented approach provides two main outcomes: on one side, a Covariancebased descriptor which uses only low-level visual features and requires very low computational cost for its construction. On the other side, a classification method which takes into account the geometric properties of such representation. All together, the system provides an image retrieval method which is fast and has demonstrated to be of similar accuracy levels to other methods using complementary textual information.</p><p>Despite of that, we firmly believe that this method can be further extended in the future, in many directions. Descriptor features could be extended with a codification of medical terms associated to different image classes. Thus, visual and textual feature fusion would take place within the nature of our descriptor. On the other side, after analysing the results and the available groundtruth annotations, we have observed a major dependency of our method on class cardinality due to its sparse representation formulation. Classes with minor representation can lead to higher classification error as a consequence of the minimization formulation of our method. Therefore, we have observed that this can be solved by incorporating a class balancing stage before the sparse regularization.</p><p>So far, the participation on the ImageCLEF Medical Classification task has provided an interesting benchmark which has contributed to test our ongoing research and identify some improvements for our methodology thanks to the particular nature of the provided testing data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,134.77,330.92,345.81,7.89;3,134.77,341.90,345.81,7.86;3,134.77,352.86,67.60,7.86"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example of different samples of the different 30 classes present on the Image-CLEF Medical classification task. Please refer to [5] for more details on class hierarchy and terminology.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,134.77,524.52,345.81,7.89;4,134.77,535.51,345.81,7.86;4,134.77,546.47,345.81,7.86;4,134.77,557.43,345.81,7.86;4,134.77,568.39,123.81,7.86"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Different cues involved in the descriptor building for an image of the endoscopy class (leftmost subimage). The resulting Covariance Descriptor is shown in the rightmost subfigure. Images of the same class share similar Covariance Descriptor signatures, while images from classes with different color distributions and shape features have differentiated descriptors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,134.77,393.56,345.82,98.62"><head>Table 1 .</head><label>1</label><figDesc>Top accuracy performances after submission evaluation of the ImageCLEF Medical Classification task. Our method accuracy is placed after the most accurate method. Using only visual features we are close to the best method, which also exploits textual information associated to the training samples.</figDesc><table coords="7,204.03,393.56,171.47,48.51"><row><cell>1 Visual + text</cell><cell>67.60</cell></row><row><cell>Participants 1 Only visual</cell><cell>60.91</cell></row><row><cell>Our method Only visual</cell><cell>52.98</cell></row><row><cell>Participants 3 Only visual</cell><cell>45.63</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,134.77,215.89,361.32,216.94"><head>Table 2 .</head><label>2</label><figDesc>Analysis of the cardinality of different classes in the testing set and their associated Precision and Recall values. These are clearly affected by the unbalanced class sets, which has a direct impact on our method due to its underlying formulation.</figDesc><table coords="8,136.56,215.89,359.53,173.05"><row><cell>Class</cell><cell cols="10">D3DR DMEL DMFL DMLI DMTR DRAN DRCO DRCT DRMR DRPE</cell></row><row><cell>Class #</cell><cell>112</cell><cell>60</cell><cell>312</cell><cell>266</cell><cell>77</cell><cell>7</cell><cell>27</cell><cell>6</cell><cell>43</cell><cell>4</cell></row><row><cell>Precision</cell><cell cols="5">0.5300 0.1584 0.6629 0.6810 0.3875</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.1579</cell><cell>0</cell></row><row><cell>Recall</cell><cell cols="5">0.4732 0.2667 0.7436 0.5376 0.4026</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.1395</cell><cell>0</cell></row><row><cell>Class</cell><cell cols="10">DRUS DRXR DSEC DSEE DSEM DVDM DVEN DVOR GCHE GFIG</cell></row><row><cell>Class #</cell><cell>0</cell><cell>20</cell><cell>0</cell><cell>4</cell><cell>1</cell><cell>12</cell><cell>4</cell><cell>17</cell><cell>8</cell><cell>764</cell></row><row><cell>Precision</cell><cell>0</cell><cell>0.0526</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="5">0.3333 0.1250 0.0217 0.1667 0.6600</cell></row><row><cell>Recall</cell><cell>0</cell><cell>0.0500</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="5">0.1667 0.2500 0.0588 0.5000 0.8154</cell></row><row><cell>Class</cell><cell cols="10">GFLO GGEL GGEN GHDR GMAT GNCP GPLI GSCR GSYS GTAB</cell></row><row><cell>Class #</cell><cell>6</cell><cell>116</cell><cell>173</cell><cell>52</cell><cell>8</cell><cell>34</cell><cell>0</cell><cell>13</cell><cell>66</cell><cell>32</cell></row><row><cell>Precision</cell><cell>0</cell><cell>0.4806</cell><cell>0</cell><cell>0.0857</cell><cell>0</cell><cell cols="4">0.2143 0 0.0833 0</cell><cell>0.1707</cell></row><row><cell>Recall</cell><cell>0</cell><cell>0.5345</cell><cell>0</cell><cell>0.0577</cell><cell>0</cell><cell cols="4">0.0882 0 0.0769 0</cell><cell>0.2188</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="7,144.73,657.80,162.32,7.86"><p>http://www.imageclef.org/2015/medical</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,142.96,290.64,337.62,7.86;9,151.52,301.60,329.07,7.86;9,151.52,312.56,160.98,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,462.53,290.64,18.05,7.86;9,151.52,301.60,262.00,7.86">Logeuclidean metrics for fast and simple calculus on diffusion tensors</title>
		<author>
			<persName coords=""><forename type="first">Vincent</forename><surname>Arsigny</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><surname>Fillard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xavier</forename><surname>Pennec</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicholas</forename><surname>Ayache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,421.28,301.60,59.31,7.86;9,151.52,312.56,71.28,7.86">Magnetic resonance in medicine</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="411" to="421" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,324.21,337.62,7.86;9,151.52,335.16,329.07,7.86;9,151.52,346.12,154.98,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,284.30,324.21,196.27,7.86;9,151.52,335.16,251.19,7.86">4DCov: A nested covariance descriptor of spatiotemporal features for gesture recognition in depth sequences</title>
		<author>
			<persName coords=""><forename type="first">Pol</forename><surname>Cirujeda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xavier</forename><surname>Binefa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,428.00,335.16,52.59,7.86;9,151.52,346.12,126.19,7.86">International Conference on 3D vision (3DV)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,357.77,337.62,7.86;9,151.52,368.73,329.05,7.86;9,151.52,379.69,78.69,7.86;9,290.18,379.69,189.59,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,436.04,357.77,44.54,7.86;9,151.52,368.73,329.05,7.86;9,151.52,379.69,75.02,7.86">A 3d scene registration method via covariance descriptors and an evolutionary stable strategy game theory solver</title>
		<author>
			<persName coords=""><forename type="first">Pol</forename><surname>Cirujeda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yashin</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xavier</forename><surname>Mateo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xavier</forename><surname>Binefa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,290.18,379.69,112.23,7.86">Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,391.34,337.62,7.86;9,151.52,402.30,329.06,7.86;9,151.52,413.25,329.06,7.86;9,151.52,424.21,329.06,7.86;9,151.52,435.17,71.19,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,416.51,402.30,64.07,7.86;9,151.52,413.25,314.15,7.86">3d riesz-wavelet based covariance descriptors for texture classification of lung nodule tissue in ct</title>
		<author>
			<persName coords=""><forename type="first">Pol</forename><surname>Cirujeda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Todd</forename><forename type="middle">A</forename><surname>Aguilera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Billy</forename><forename type="middle">W</forename><surname>Loo</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maximilian</forename><surname>Diehn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xavier</forename><surname>Binefa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrien</forename><surname>Depeursinge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,151.52,424.21,329.06,7.86">International Conference of the IEEE Engineering in Medicine and Biology Society</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="9,142.96,446.82,337.62,7.86;9,151.52,457.78,329.07,7.86;9,151.52,468.74,329.06,7.86;9,151.52,479.70,102.18,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,442.92,446.82,37.66,7.86;9,151.52,457.78,206.65,7.86">Overview of the ImageCLEF 2015 medical classification task</title>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Seco</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefano</forename><surname>Bromuri</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="9,380.05,457.78,100.54,7.86;9,151.52,468.74,165.72,7.86">Working Notes of CLEF 2015 (Cross Language Evaluation Forum)</title>
		<title level="s" coord="9,324.78,468.74,149.87,7.86">CEUR Workshop Proceedings. CEUR</title>
		<imprint>
			<date type="published" when="2015-09">September 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,491.35,337.62,7.86;9,151.52,502.30,329.05,7.86;9,151.52,513.26,329.07,7.86;9,151.52,524.22,20.99,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,460.05,491.35,20.53,7.86;9,151.52,502.30,329.05,7.86;9,151.52,513.26,95.94,7.86">A review of content-based image retrieval systems in medical applications-clinical benefits and future directions</title>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicolas</forename><surname>Michoux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Bandon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antoine</forename><surname>Geissbuhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,253.93,513.26,175.84,7.86">International Journal of Medical Informatics</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,535.87,337.62,7.86;9,151.52,546.83,329.06,7.86;9,151.52,557.79,58.87,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,333.65,535.87,146.93,7.86;9,151.52,546.83,122.21,7.86">Region covariance: A fast descriptor for detection and classification</title>
		<author>
			<persName coords=""><forename type="first">Onzel</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,281.83,546.83,169.36,7.86">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="589" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,569.44,337.62,7.86;9,151.52,580.39,329.06,7.86;9,151.52,591.35,329.06,7.86;9,151.52,602.31,329.06,7.86;9,151.52,613.27,329.05,7.86;9,151.52,624.23,124.60,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,398.90,602.31,81.68,7.86;9,151.52,613.27,142.62,7.86">General Overview of ImageCLEF at the CLEF 2015 Labs</title>
		<author>
			<persName coords=""><forename type="first">Mauricio</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Josiah</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefano</forename><surname>Bromuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">Ashraful</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mahmood</forename><surname>Kazi Mohammed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Burak</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Suzan</forename><surname>Uskudarli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Neda</forename><forename type="middle">B</forename><surname>Marvasti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">José</forename><forename type="middle">F</forename><surname>Aldana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">María</forename><surname>Del Mar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roldán</forename><surname>García</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="9,301.52,613.27,138.84,7.86">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,635.88,337.62,7.86;9,151.52,646.84,329.06,7.86;9,151.52,657.80,192.41,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,221.01,646.84,259.57,7.86;9,151.52,657.80,14.74,7.86">Sparse representation for computer vision and pattern recognition</title>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,174.04,657.80,70.22,7.86">Proc. of the IEEE</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1031" to="1044" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,120.67,337.96,7.86;10,151.52,131.63,329.06,7.86;10,151.52,142.59,158.70,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,337.09,120.67,143.48,7.86;10,151.52,131.63,196.49,7.86">Sparse representation or collaborative representation: Which helps face recognition</title>
		<author>
			<persName coords=""><forename type="first">Meng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiangchu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,367.40,131.63,113.19,7.86;10,151.52,142.59,67.52,7.86">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="471" to="478" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
