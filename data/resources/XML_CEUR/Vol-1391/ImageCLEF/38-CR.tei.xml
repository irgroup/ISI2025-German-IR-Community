<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,183.47,116.95,248.43,12.62;1,144.88,134.89,325.60,12.62;1,268.37,152.82,78.61,12.62">RUC-Tencent at ImageCLEF 2015: Concept Detection, Localization and Sentence Generation</title>
				<funder ref="#_jVGRb8V">
					<orgName type="full">National Science Foundation of China</orgName>
				</funder>
				<funder>
					<orgName type="full">Scientific Research Foundation for the Returned Overseas Chinese Scholars, State Education Ministry</orgName>
				</funder>
				<funder ref="#_htR4r6P">
					<orgName type="full">Research Funds of Renmin University of China</orgName>
				</funder>
				<funder ref="#_mVcu9Yg">
					<orgName type="full">Specialized Research Fund for the Doctoral Program of Higher Education</orgName>
				</funder>
				<funder>
					<orgName type="full">Fundamental Research Funds for the Central Universities</orgName>
				</funder>
				<funder ref="#_QMuHfcF">
					<orgName type="full">Beijing Natural Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,144.73,190.72,41.95,8.74"><forename type="first">Xirong</forename><surname>Li</surname></persName>
							<email>xirong@ruc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="laboratory">Multimedia Computing Lab</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,201.82,190.72,32.80,8.74"><forename type="first">Qin</forename><surname>Jin</surname></persName>
							<email>qjin@ruc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="laboratory">Multimedia Computing Lab</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,249.75,190.72,46.36,8.74"><forename type="first">Shuai</forename><surname>Liao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="laboratory">Multimedia Computing Lab</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,306.67,190.72,57.84,8.74"><forename type="first">Junwei</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="laboratory">Multimedia Computing Lab</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,375.06,190.72,33.49,8.74"><forename type="first">Xixi</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="laboratory">Multimedia Computing Lab</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,419.10,190.72,44.28,8.74"><forename type="first">Yujia</forename><surname>Huo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="laboratory">Multimedia Computing Lab</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,194.82,202.68,47.46,8.74"><forename type="first">Weiyu</forename><surname>Lan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="laboratory">Multimedia Computing Lab</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,252.84,202.68,38.89,8.74"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Pattern Recognition Center</orgName>
								<orgName type="department" key="dep2">WeChat Technical Architecture Department</orgName>
								<orgName type="institution">Tencent</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,302.28,202.68,55.76,8.74"><forename type="first">Yanxiong</forename><surname>Lu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Pattern Recognition Center</orgName>
								<orgName type="department" key="dep2">WeChat Technical Architecture Department</orgName>
								<orgName type="institution">Tencent</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,368.60,202.68,47.47,8.74"><forename type="first">Jieping</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="laboratory">Multimedia Computing Lab</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,183.47,116.95,248.43,12.62;1,144.88,134.89,325.60,12.62;1,268.37,152.82,78.61,12.62">RUC-Tencent at ImageCLEF 2015: Concept Detection, Localization and Sentence Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">68106F383A11589365C8EAD47645F6F7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Concept detection</term>
					<term>Concept localization</term>
					<term>Image Captioning</term>
					<term>Deep Learning</term>
					<term>Hierarchical Semantic Embedding</term>
					<term>Negative Bootstrap</term>
					<term>CNN</term>
					<term>LSTM-RNN</term>
					<term>Sentence Reranking</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we summarize our experiments in the Image-CLEF 2015 Scalable Concept Image Annotation challenge. The RUC-Tencent team participated in all subtasks: concept detection and localization, and image sentence generation. For concept detection, we experiments with automated approaches to gather high-quality training examples from the Web, in particular, visual disambiguation by Hierarchical Semantic Embedding. Per concept, an ensemble of linear SVMs is trained by Negative Bootstrap, with CNN features as image representation. Concept localization is achieved by classifying object proposals generated by Selective Search. For the sentence generation task, we adopt Google's LSTM-RNN model, train it on the MSCOCO dataset, and finetune it on the ImageCLEF 2015 development dataset. We further develop a sentence re-ranking strategy based on the concept detection information from the first task. Overall, our system is ranked the 3rd for concept detection and localization, and is the best for image sentence generation in both clean and noisy tracks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This year we participated in all the subtasks, i.e., concept detection and localization and image sentence generation, in the ImageCLEF 2015 Scalable Concept Image Annotation challenge. In addition to the 500k web image set (webupv2015 hereafter) and the 2k develop set (Dev2k) provided by the organizers <ref type="bibr" coords="1,451.58,592.29,9.96,8.74" target="#b4">[1]</ref>, we leverage a number of external resources, listed below:</p><p>-Task 1: WordNet, ImageNet <ref type="bibr" coords="1,278.12,625.10,9.96,8.74" target="#b5">[2]</ref>, Flickr images, a pretrained Caffe CNN <ref type="bibr" coords="1,467.30,625.10,9.96,8.74" target="#b6">[3]</ref>,</p><p>and a pretrained HierSE model <ref type="bibr" coords="1,291.40,637.06,9.96,8.74" target="#b7">[4]</ref>.</p><p>X. Li and Q. Jin contributed equally to this work.</p><p>-Task 2: MSCOCO <ref type="bibr" coords="2,238.98,119.99,9.96,8.74" target="#b8">[5]</ref>, a pretrained VGGNet CNN <ref type="bibr" coords="2,386.94,119.99,9.96,8.74" target="#b9">[6]</ref>, and a pretrained word2vec model <ref type="bibr" coords="2,224.79,131.95,9.96,8.74" target="#b10">[7]</ref>.</p><p>Next, we introduce in Section 2 our concept detection and localization system, followed by our image sentence generation system in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task 1: Concept Detection and Localization</head><p>We develop a concept detection and localization system that learns concept classifiers from image-level annotations and localizes concepts within an image by region-level classification, as illustrated in Fig. <ref type="figure" coords="2,336.93,251.82,3.87,8.74" target="#fig_0">1</ref>. Compared with our earlier systems <ref type="bibr" coords="2,158.30,263.77,10.52,8.74" target="#b11">[8,</ref><ref type="bibr" coords="2,169.82,263.77,7.01,8.74" target="#b12">9]</ref>, this year we make two technical improvements for concept modeling. First, we replace bag of visual words features by an off-the-shelf CNN feature, i.e., the last fully connected layer (fc7) of Caffe <ref type="bibr" coords="2,346.42,287.68,9.96,8.74" target="#b6">[3]</ref>. Second, we employ Hierarchical Semantic Embedding <ref type="bibr" coords="2,257.09,299.64,10.52,8.74" target="#b7">[4]</ref> for concept disambiguation, which is found to be effective for acquiring positive examples for some ambiguous concepts such as 'mouse', 'basin', and 'blackberry'.  Since the annotations of webupv2015 and flickr2m are noisy, per concept we collect its positive training examples by a two-step procedure. In the first step, we conduct tag-based image search using tags of the concept as the query to generate a set of candidate images. For webupv2015, an image is chosen as a candidate as long as its meta data, e.g., url and query logs, overlap the tags. In the second step, these candidate images are sorted in descending order in terms of their relevance to the concept, and the top-n ranked images are preserved as the positive training set. To compute the relevance score between the given concept and a specific image, we embed them into a common semantic space by the Hierarchical Semantic Embedding (HierSE) algorithm <ref type="bibr" coords="3,403.67,227.59,9.96,8.74" target="#b7">[4]</ref>. Consequently, the cross-media relevance score is computed as the cosine similarity between the embedding vectors.</p><p>Since HierSE takes the WordNet hierarchy into account, it can resolve semantic ambiguity by embedding a label into distinct vectors, depending on its position in WordNet. Consider the label 'basin' for instance. In the context of ImageCLEF, it is 'basin.n.01', referring to a bow-shaped vessel. On the other hand, it can also be 'basin.n.03', meaning a natural depression in the surface of the land. In HierSE, a label with a given sense is represented by a convex combination of the embedding vectors of this label and its ancestors tracing back to the root. Fig. <ref type="figure" coords="3,196.97,347.78,4.98,8.74">2</ref> shows the top-ranked images of 'basin' from flickr2m, returned by the tag relevance (tagrel) algorithm <ref type="bibr" coords="3,307.26,359.73,15.50,8.74" target="#b14">[11]</ref> and HierSE, respectively.</p><p>Given the HierSE ranked images per concept, we empirically preserve the top n = 1000 images as positive training examples. Since the number of genuine positives varies over concepts, this year we also consider an adaptive selection strategy. We train an SVM model using the top 20 images as positives and 20 images at the bottom as negatives. The previously ranked images are classified by the model and labeled as positive if the classification scores exceed 0. We denote this strategy as HierSE + SVM.</p><p>Negative Bootstrap To effectively exploit an overwhelming number of (pseudo) negative examples, we learn an ensemble of linear SVMs by the Negative Bootstrap algorithm <ref type="bibr" coords="3,208.18,501.06,14.61,8.74" target="#b15">[12]</ref>. The base classifiers are trained using LIBLINEAR <ref type="bibr" coords="3,462.33,501.06,14.61,8.74" target="#b16">[13]</ref>. The classifier ensemble is compressed into a single model using the technique developed in <ref type="bibr" coords="3,193.62,524.97,15.50,8.74" target="#b17">[14]</ref> so that the prediction time complexity is independent of the ensemble size, and linear w.r.t. the visual feature dimension.</p><p>To evaluate the multiple methods for selecting positive and negative examples, we split the ImageNet set into two disjoint subsets, imagenet-clef15train and imagenet-clef15test, which contain positive examples for the 208 Image-CLEF concepts. Using imagenet-clef15test as the test set, the performance of the individual methods is summarized in Table <ref type="table" coords="3,323.75,597.34,3.87,8.74" target="#tab_0">1</ref>. For webupv2015, the combination of HierSE + SVM and Negative Bootstrap performs the best. For flickr2m, the combination of HierSE and Negative Bootstrap performs the best. Still, there is a substantial gap between the model trained on auto-selected examples (MAP 0.375) and the model trained on hand labeled examples (MAP 0.567). So for the 208 concepts, we combine all the models trained on imagenet-clef15train,  webupv2015, and flickr2m, while for the other 43 concepts, we combine all the models trained on webupv2015 and flickr2m. Although learned weights are found to be helpful according to our previous experiences <ref type="bibr" coords="4,358.45,571.52,10.52,8.74" target="#b11">[8,</ref><ref type="bibr" coords="4,370.02,571.52,11.62,8.74" target="#b18">15]</ref>, we use model averaging in this evaluation due to time constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Concept Localization</head><p>Object Proposal Generation For each of the 500k images, we use Selective Search <ref type="bibr" coords="4,166.22,645.16,15.50,8.74" target="#b19">[16]</ref> to get a number of bounding-box object proposals. In particular, the image is first over-segmented by the graph-based image segmentation algorithm <ref type="bibr" coords="5,134.77,119.99,14.61,8.74" target="#b20">[17]</ref>. The segmented regions are iteratively merged by hierarchical grouping as depicted in <ref type="bibr" coords="5,185.38,131.95,14.61,8.74" target="#b19">[16]</ref>. As the CNN feature is extracted per bounding box, the number of chosen object proposals is set to 20 given our computational power.</p><p>Detection Given an image and its 20 object proposals, we classify each of them using the 251 concept models, and label it with the concept of maximum response. Notice that for each concept, we have two choices to implement its model. One choice is HierSE, as it was developed for zero-shot learning, and is thus directly applicable to compute cross-media relevance scores. The other choice is linear SVMs trained from the selective positives combined with Negative Bootstrap. HierSE is the same as used for positive example selection, while the SVMs used here are distinct from the SVMs in Section 2.1.</p><p>To reduce false alarms, we refine the detection as follows. For object proposals labeled as the same concept, if their number is lower than a given Minimum Detection threshold md (we tried 1, 2, and 3), they are discarded, otherwise we sort them in descending order in terms of detection scores. We go through the ranked proposals, preserving a proposal if its bounding box has less than 30% overlap with the previously preserved proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Submitted Runs</head><p>We submitted eight runs in the concept detection and localization task. ruc task1 hierse md1 is our baseline run, directly using HierSE to compute the relevance score between an object proposal and a concept, with the Minimum Detection threshold md set to 1, namely no removal. ruc task1 hierse md2 is the same as ruc task1 hierse md1, but with md = 2. ruc task1 hierse md3 is the same as ruc task1 hierse md1, but with md = 3. ruc task1 svm md1 uses SVMs for concept detection, with md = 1. ruc task1 svm md2 is the same as ruc task1 svm md1, but with md = 2. ruc task1 svm md3 is the same as ruc task1 svm md1, but with md = 3. ruc task1 svm md2 nostar is the same as ruc task1 svm md2, but empirically thresholding the detection results of two concepts 'planet' and 'star', as they tend to be over fired. ruc task1 svm md3 nostar is the same as ruc task1 svm md3, but empirically thresholding the detection results of two concepts 'planet' and 'star'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Result Analysis</head><p>The performance of the eight runs is shown in Fig. <ref type="figure" coords="6,454.75,159.38,3.87,8.74" target="#fig_3">3</ref>. We attribute the low performance of the HierSE runs to the following two reasons. First, HierSE is designed for zero-shot learning. It does not use any examples of the ImageCLEF concepts. Second, the current implementation of HierSE relies on the ImageNet 1k label set <ref type="bibr" coords="6,263.07,207.20,15.50,8.74" target="#b21">[18]</ref> to embed an image, while the 1k set is loosely related with the ImageCLEF concepts. We can see that the Minimum Detection threshold is helpful, improving MAP 0.5Overlap from 0.452 to 0.496. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task 2: Image Sentence Generation</head><p>We have used the MSCOCO dataset <ref type="bibr" coords="6,300.08,564.39,10.52,8.74" target="#b8">[5]</ref> in addition to the Dev2k dataset provided by the organizers. For the Dev2k dataset, we split it into a training set with 1,600 images, a validation set with 200 images, and a test set with 200 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Deep Models based Image Sentence Generation</head><p>Our image description system is built based on deep models proposed by Vinyals et al. from Google <ref type="bibr" coords="6,216.60,657.11,14.61,8.74" target="#b22">[19]</ref>. The deep model contains the following key components: (1) a Convolutional Neural Network (CNN) for image encoding, and (2) a Long-Short Term Memory based Recurrent Neural Network (LSTM-RNN) for sentence encoding, and (3) a LSTM-RNN for sentence decoding. In our system as shown in Fig. <ref type="figure" coords="7,195.84,367.25,3.87,8.74" target="#fig_4">4</ref>, we use the pre-trained VGGNet <ref type="bibr" coords="7,347.21,367.25,10.52,8.74" target="#b9">[6]</ref> for CNN feature extraction. The LSTM-RNN implementation is from the NeuralTalk project<ref type="foot" coords="7,412.32,377.63,3.97,6.12" target="#foot_0">1</ref> . The encoding LSTM-RNN and the decoding LSTM-RNN are shared.</p><p>In the training stage, we first train the LSTM-RNN on the MSCOCO dataset. We then fine-tune the model on the ImageCLEF Dev2k dataset using a low learning rate. Beam search is used in text decoding as in <ref type="bibr" coords="7,381.43,427.05,14.61,8.74" target="#b22">[19]</ref>. We finally re-rank the hypothesis sentences utilizing the concept detection results. Given an image, our system first generates k best sentences with confidence scores, meanwhile the concept detection system from task 1 provides m best detected concepts with confidence scores. If a detected concept appears in the hypothesis sentence, we call it a matched concept. We then compute the ranking score for each hypothesis sentence by matching the detected concepts with the hypothesis sentences as follows:</p><formula xml:id="formula_0" coords="7,152.25,532.72,328.34,8.74">RankScore(hypoSent) = θ • conceptScore + (1 -θ) • sentenceScore,<label>(1)</label></formula><p>where conceptScore refers to the average of the confidence scores of all the matched concepts in the hypothesis sentence (hypoSent), sentenceScore refers to the confidence score assigned to the hypothesis sentence by the RNN model. The parameter θ has been tuned on the Dev2k validation set. Fig. <ref type="figure" coords="7,429.34,590.57,4.98,8.74" target="#fig_5">5</ref> showcases some examples of how re-ranking helps the system produce better image descriptions.</p><p>In the clean track, "golden" concepts, i.e., ground truth concepts by hand labelling, are provided. So we set the confidence score for each concept as 1. In this condition, if the top-ranked sentence does not contain any of the "golden" concepts, the word in the top-ranked sentence with the closest distance to any one of the "golden" concepts will be replaced by that concept. The word distance is computed using a pre-trained word2vec<ref type="foot" coords="8,316.78,623.69,3.97,6.12" target="#foot_1">2</ref> model <ref type="bibr" coords="8,354.18,625.27,9.96,8.74" target="#b10">[7]</ref>. RUC run2 finetune-mscoco obtains a higher precision score. This is probably because the reranking process based on the "golden" concepts makes the system more precision oriented.</p><p>Since the details of the test sets used in the two tracks are not available to us, we cannot do more in-depth analysis.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Summary and Discussions</head><p>For the concept detection and localization task, we find Hierarchical Semantic Embedding effective for resolving visual ambiguity. Negative Bootstrap improves the classification performance further. For the image sentence generation task, Google's LSTM-RNN model can be improved by sentence re-ranking.</p><p>Notice that for varied reasons including the limited number of submitted runs and the unavailability of detailed information about the test data, our analysis is preliminary, in particular a component-wise analysis is largely missing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,134.77,482.97,345.83,7.89"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An illustration of the RUC-Tencent concept detection and localization system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,134.77,538.49,118.44,8.77;2,134.77,561.60,345.83,8.77;2,134.77,573.59,322.15,8.74;2,138.97,596.06,341.62,8.74;2,151.70,608.02,125.17,8.74;2,138.97,620.61,339.09,8.74;2,138.97,633.20,341.62,8.74;2,151.70,645.16,328.89,8.74;2,151.70,657.11,172.05,8.74"><head>2. 1</head><label>1</label><figDesc>Concept Modeling Positive Training Examples Since hand labeled data is allowed this year, we collect positive training examples from multiple sources of data, including 1. ImageNet. For 208 of the 251 ImageCLEF concepts, we can find labeled examples from ImageNet [2]. 2. webupv2015. It consists of 500K web images provided by the organizers [1] 3. flickr2m. We start with the 1.2 million Flickr image set from [10], and extend it by adding more Flickr images labeled with the ImageCLEF concepts, resulting in a set of two million images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,134.77,304.99,345.83,7.89;4,134.77,315.97,345.83,7.86;4,137.84,205.24,345.82,70.03"><head></head><label></label><figDesc>Fig. 2. Images of 'basin' retrieved by (a) tagrel and (b) HierSE, respectively. The HierSE results are more relevant to the ImageCLEF concept, i.e., a bow-shaped vessel.</figDesc><graphic coords="4,137.84,205.24,345.82,70.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,134.77,481.96,345.82,7.89;6,134.77,492.94,244.85,7.86"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Comparison of RUC-Tencent runs with other runs for the concept detection and localization task. Performance metric: MAP 0.5Overlap.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,149.67,297.59,316.02,7.89;7,134.77,116.83,345.82,165.99"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. An illustration of the RUC-Tencent image sentence generation system.</figDesc><graphic coords="7,134.77,116.83,345.82,165.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="8,134.77,526.89,345.82,7.89;8,134.77,537.88,345.83,7.86;8,134.77,548.84,342.76,7.86;8,137.84,320.38,345.82,177.30"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Examples illustrating sentence re-ranking. Candidate sentences are shown in descending order on the left side, while the chosen sentences are shown on the right side. In square brackets are Flickr tags predicted by the tag relevance algorithm [11].</figDesc><graphic coords="8,137.84,320.38,345.82,177.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="10,134.77,526.08,345.83,7.89;10,134.77,537.07,182.40,7.86"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Comparison of RUC-Tencent runs with other runs for the image sentence generation task. Performance metric: METEOR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="11,134.77,546.95,345.83,7.89;11,134.77,557.94,326.85,7.86;11,143.41,192.91,328.53,339.27"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Images with sentences generated by the RUC-Tencent image sentence generation system. The images are hand picked that showing our system performs well.</figDesc><graphic coords="11,143.41,192.91,328.53,339.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,134.77,335.85,345.83,181.72"><head>Table 1 .</head><label>1</label><figDesc>Evaluating methods for positive and negative training example selection. Test set: imagenet-clef15test. Methods are sorted in descending order by MAP.</figDesc><table coords="4,175.81,371.64,263.74,145.94"><row><cell>Data source</cell><cell cols="3">Positive examples Negative examples MAP</cell></row><row><cell>flickr2m</cell><cell>HierSE</cell><cell>Negative Bootstrap</cell><cell>0.375</cell></row><row><cell>flickr2m</cell><cell>tagrel</cell><cell>Negative Bootstrap</cell><cell>0.362</cell></row><row><cell>flickr2m</cell><cell>HierSE</cell><cell>random sampling</cell><cell>0.356</cell></row><row><cell>flickr2m</cell><cell>HierSE + SVM</cell><cell>Negative Bootstrap</cell><cell>0.350</cell></row><row><cell>webupv2015</cell><cell>HierSE + SVM</cell><cell>Negative Bootstrap</cell><cell>0.338</cell></row><row><cell>flickr2m</cell><cell>HierSE + SVM</cell><cell>random sampling</cell><cell>0.337</cell></row><row><cell>webupv2015</cell><cell>HierSE</cell><cell>Negative Bootstrap</cell><cell>0.333</cell></row><row><cell>webupv2015</cell><cell>Hierse + SVM</cell><cell>random sampling</cell><cell>0.314</cell></row><row><cell>webupv2015</cell><cell>Hierse</cell><cell>random sampling</cell><cell>0.278</cell></row><row><cell cols="2">imagenet-clef15train Manual</cell><cell>Negative Bootstrap</cell><cell>0.567</cell></row><row><cell cols="2">imagenet-clef15train Manual</cell><cell>random sampling</cell><cell>0.547</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="11,137.53,116.91,342.17,58.18"><head>Table 2 .</head><label>2</label><figDesc>Performance of the task 2 clean track.</figDesc><table coords="11,137.53,139.99,342.17,35.10"><row><cell>Run</cell><cell cols="4">Mean METEOR Mean Precision Mean Recall Mean F1</cell></row><row><cell>RUC run1 dev2k</cell><cell>0.2393</cell><cell>0.6845</cell><cell>0.4771</cell><cell>0.5310</cell></row><row><cell>RUC run2 finetune-mscoco</cell><cell>0.2213</cell><cell>0.7015</cell><cell>0.4496</cell><cell>0.5147</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="7,144.73,658.44,178.88,7.47"><p>https://github.com/karpathy/neuraltalk</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="8,144.73,658.44,122.39,7.47"><p>code.google.com/p/word2vec</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This research was supported by the <rs type="funder">National Science Foundation of China</rs> (No. <rs type="grantNumber">61303184</rs>), the <rs type="funder">Fundamental Research Funds for the Central Universities</rs> and the <rs type="funder">Research Funds of Renmin University of China</rs> (No. <rs type="grantNumber">14XNLQ01</rs>), the <rs type="funder">Beijing Natural Science Foundation</rs> (No. <rs type="grantNumber">4142029</rs>), the <rs type="funder">Specialized Research Fund for the Doctoral Program of Higher Education</rs> (No. <rs type="grantNumber">20130004120006</rs>), and the <rs type="funder">Scientific Research Foundation for the Returned Overseas Chinese Scholars, State Education Ministry</rs>. The authors are grateful to the ImageCLEF coordinators for the benchmark organization efforts [1, 20].</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_jVGRb8V">
					<idno type="grant-number">61303184</idno>
				</org>
				<org type="funding" xml:id="_htR4r6P">
					<idno type="grant-number">14XNLQ01</idno>
				</org>
				<org type="funding" xml:id="_QMuHfcF">
					<idno type="grant-number">4142029</idno>
				</org>
				<org type="funding" xml:id="_mVcu9Yg">
					<idno type="grant-number">20130004120006</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,169.46,169.20,311.14,8.77;9,134.77,181.19,124.09,8.74" xml:id="b0">
	<monogr>
		<title level="m" coord="9,201.28,169.20,279.31,8.77;9,134.77,181.19,119.78,8.74">RUC run1 dev2k is our baseline trained on the Dev2k dataset without sentence re-ranking</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,172.30,210.76,308.29,8.77;9,134.77,222.75,345.83,8.74" xml:id="b1">
	<monogr>
		<title level="m" coord="9,206.95,210.76,273.64,8.77;9,134.77,222.75,95.39,8.74">RUC run2 dev2k rerank-hierse is trained on the Dev2k dataset, using the 251</title>
		<imprint/>
	</monogr>
	<note>concepts predicted by HierSE [for sentence re-ranking</note>
</biblStruct>

<biblStruct coords="9,171.78,359.35,308.81,8.77;9,134.77,371.33,345.82,8.74;9,134.77,383.29,150.30,8.74" xml:id="b2">
	<monogr>
		<title level="m" coord="9,205.92,359.35,274.67,8.77;9,134.77,371.33,192.08,8.74">RUC run5 finetune-mscoco rerank-hierse is trained the same way as for RUC run4 finetune-mscoco</title>
		<imprint/>
	</monogr>
	<note>using the 251 concepts predicted by HierSE for sentence re-ranking</note>
</biblStruct>

<biblStruct coords="9,171.86,412.86,308.74,8.77;9,134.77,424.84,345.83,8.74;9,134.77,436.80,224.91,8.74" xml:id="b3">
	<monogr>
		<title level="m" coord="9,206.07,412.86,274.52,8.77;9,134.77,424.84,189.85,8.74">RUC run6 finetune-mscoco rerank-tagrel is trained the same way as for RUC run4 finetune-mscoco</title>
		<imprint/>
	</monogr>
	<note>using the Flickr tags predicted by the tag relevance algorithm for sentence re-ranking</note>
</biblStruct>

<biblStruct coords="12,142.96,315.79,337.64,7.86;12,151.52,326.75,329.07,7.86;12,151.52,337.71,312.26,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,235.21,326.75,245.38,7.86;12,151.52,337.71,171.27,7.86">Overview of the ImageCLEF 2015 Scalable Image Annotation, Localization and Sentence Generation task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dellandrea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,344.80,337.71,86.07,7.86">CLEF Working Notes</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,348.86,337.64,7.86;12,151.52,359.82,190.03,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,385.93,348.86,94.67,7.86;12,151.52,359.82,109.80,7.86">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,283.70,359.82,29.18,7.86">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,370.97,337.63,7.86;12,151.52,381.93,329.07,7.86;12,151.52,392.88,93.19,7.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="12,238.21,381.93,237.78,7.86">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,404.03,337.63,7.86;12,151.52,414.99,159.46,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,323.80,404.03,156.79,7.86;12,151.52,414.99,79.67,7.86">Zero-shot image tagging by hierarchical semantic embedding</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,253.96,414.99,28.35,7.86">SIGIR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,426.14,337.64,7.86;12,151.52,437.07,329.07,7.89;12,151.52,448.06,25.60,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="12,197.52,437.10,181.32,7.86">Microsoft COCO: common objects in context</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>CoRR abs/1405.0312</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,459.21,337.63,7.86;12,151.52,470.14,200.34,7.89" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="12,280.72,459.21,199.87,7.86;12,151.52,470.17,69.82,7.86">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>CoRR abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,481.31,337.63,7.86;12,151.52,492.27,297.63,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,395.12,481.31,85.47,7.86;12,151.52,492.27,222.60,7.86">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,396.30,492.27,24.19,7.86">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,503.42,337.64,7.86;12,151.52,514.38,329.07,7.86;12,151.52,525.34,52.27,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,393.92,503.42,86.67,7.86;12,151.52,514.38,247.68,7.86">Renmin University of China at ImageCLEF 2013 scalable concept image annotation</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,421.20,514.38,59.39,7.86;12,151.52,525.34,19.67,7.86">CLEF working notes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,536.49,337.64,7.86;12,151.52,547.45,295.40,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,308.67,536.49,171.92,7.86;12,151.52,547.45,158.17,7.86">Renmin University of China at ImageCLEF 2014 scalable concept image annotation</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,332.04,547.45,82.28,7.86">CLEF working notes</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,558.60,337.97,7.86;12,151.52,569.53,324.76,7.89" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,350.62,558.60,129.97,7.86;12,151.52,569.55,57.34,7.86">Harvesting social images for biconcept search</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,216.86,569.55,138.48,7.86">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1091" to="1104" />
			<date type="published" when="2012-08">Aug. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,580.70,337.97,7.86;12,151.52,591.64,258.91,7.89" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,282.07,580.70,194.57,7.86">Learning social tag relevance by neighbor voting</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,151.52,591.66,138.48,7.86">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1310" to="1322" />
			<date type="published" when="2009-11">Nov. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,602.81,337.97,7.86;12,151.52,613.74,329.07,7.89;12,151.52,624.73,81.53,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,396.50,602.81,84.09,7.86;12,151.52,613.77,152.67,7.86">Bootstrapping visual categorization with relevant negatives</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Koelma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,314.12,613.77,140.38,7.86">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="933" to="945" />
			<date type="published" when="2013-06">Jun. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,635.88,337.97,7.86;12,151.52,646.81,329.07,7.89;12,151.52,657.79,70.14,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="12,415.25,635.88,65.34,7.86;12,151.52,646.84,147.35,7.86">LIBLINEAR: A library for large linear classification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><forename type="middle">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,310.46,646.84,160.42,7.86">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,120.67,337.97,7.86;13,151.52,131.63,130.34,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="13,231.22,120.67,249.37,7.86;13,151.52,131.63,34.87,7.86">Classifying tag relevance with relevant positive and negative examples</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,208.91,131.63,44.28,7.86">ACM MM</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,142.59,337.97,7.86;13,151.52,153.52,294.58,7.89" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="13,346.10,142.59,134.49,7.86;13,151.52,153.55,49.79,7.86">Tag features for geo-aware image classification</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,208.96,153.55,138.47,7.86">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1058" to="1067" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,164.51,337.98,7.86;13,151.52,175.44,314.14,7.89" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="13,378.01,164.51,102.59,7.86;13,151.52,175.46,43.20,7.86">Selective search for object recognition</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,202.74,175.46,168.17,7.86">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,186.42,337.98,7.86;13,151.52,197.36,228.42,7.89" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="13,270.36,186.42,167.78,7.86">Efficient graph-based image segmentation</title>
		<author>
			<persName coords=""><forename type="first">Huttenlocher</forename><surname>Felzenszwalb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,448.32,186.42,32.27,7.86;13,151.52,197.38,138.96,7.86">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="181" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,208.34,337.97,7.86;13,151.52,219.30,329.07,7.86;13,151.52,230.26,242.10,7.86" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="13,415.16,219.30,65.43,7.86;13,151.52,230.26,140.62,7.86">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0575</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,241.22,337.98,7.86;13,151.52,252.15,199.85,7.89" xml:id="b22">
	<monogr>
		<title level="m" type="main" coord="13,353.55,241.22,127.04,7.86;13,151.52,252.18,69.21,7.86">Show and tell: A neural image caption generator</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<idno>CoRR abs/1411.4555</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,263.14,337.97,7.86;13,151.52,274.09,329.07,7.86;13,151.52,285.05,329.07,7.86;13,151.52,296.01,329.07,7.86" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="13,396.69,285.05,83.90,7.86;13,151.52,296.01,145.74,7.86">General Overview of ImageCLEF at the CLEF 2015 Labs</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bromuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">K</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Uskudarli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">B</forename><surname>Marvasti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Aldana</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Del Mar Roldán</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>García</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="13,305.65,296.01,141.32,7.86">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
