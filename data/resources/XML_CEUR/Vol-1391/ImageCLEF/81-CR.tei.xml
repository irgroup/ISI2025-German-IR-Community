<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,151.45,116.95,312.46,12.62;1,164.27,134.89,286.81,12.62">Hybrid Learning Framework for Large-Scale Web Image Annotation and Localization</title>
				<funder ref="#_G399y3A">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_eGkmmm2 #_AhCexnK #_vkczqYS">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,142.65,172.56,34.45,8.74"><forename type="first">Yong</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory" key="lab1">IVA Group</orgName>
								<orgName type="laboratory" key="lab2">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,187.66,172.56,36.26,8.74"><forename type="first">Jing</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory" key="lab1">IVA Group</orgName>
								<orgName type="laboratory" key="lab2">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,234.47,172.56,61.44,8.74"><forename type="first">Yuhang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory" key="lab1">IVA Group</orgName>
								<orgName type="laboratory" key="lab2">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,306.46,172.56,59.51,8.74"><forename type="first">Bingyuan</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName coords="1,376.52,172.56,30.72,8.74"><forename type="first">Jun</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory" key="lab1">IVA Group</orgName>
								<orgName type="laboratory" key="lab2">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,417.80,172.56,47.67,8.74"><forename type="first">Yunze</forename><surname>Gao</surname></persName>
							<email>gaoyunze2015@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory" key="lab1">IVA Group</orgName>
								<orgName type="laboratory" key="lab2">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,190.67,184.51,34.04,8.74"><forename type="first">Hui</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory" key="lab1">IVA Group</orgName>
								<orgName type="laboratory" key="lab2">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,235.27,184.51,47.33,8.74"><forename type="first">Hang</forename><surname>Song</surname></persName>
							<email>hangsongv@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory" key="lab1">IVA Group</orgName>
								<orgName type="laboratory" key="lab2">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,293.15,184.51,45.53,8.74"><forename type="first">Ying</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory" key="lab1">IVA Group</orgName>
								<orgName type="laboratory" key="lab2">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,368.61,184.51,51.61,8.74"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory" key="lab1">IVA Group</orgName>
								<orgName type="laboratory" key="lab2">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,151.45,116.95,312.46,12.62;1,164.27,134.89,286.81,12.62">Hybrid Learning Framework for Large-Scale Web Image Annotation and Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">57910EE23CE3BD62C52EE3730848F962</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Hybrid Learning</term>
					<term>SVM</term>
					<term>Fast R-CNN</term>
					<term>Annotation</term>
					<term>Concept Localization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we describe the details of our participation in the ImageCLEF 2015 Scalable Image Annotation task. The task is to annotate and localize different concepts depicted in images. We propose a hybrid learning framework to solve the scalable annotation task, in which the supervised methods given limited annotated images and the searchbased solutions on the whole dataset are explored jointly. We adopt a two-stage solution to first annotate images with possible concepts and then localize the concepts in the images. For the first stage, we adopt the classification model to get the class-predictions of each image. To overcome the overfitting problem of the trained classifier with limited labelled data, we use a search-based approach to annotate an image by mining the textual information of its similar neighbors, which are similar on both visual appearance and semantics. We combine the results of classification and the search-based solution to obtain the annotations of each image. For the second stage, we train a concept localization model based on the architecture of Fast R-CNN, and output the top-k predicted regions for each concept obtained in the first stage. Meanwhile, localization by search is adopted, which works well for the concepts without obvious objects. The final result is achieved by combing the two kinds of localization results. The submitted runs of our team achieved the second place among the different teams. This shows the outperformance of the proposed hybrid two-stage learning framework for the scalable annotation task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the advance of digital cameras and high quality mobile devices as well as the Internet technologies, there are increasingly huge number of images available on the web. This necessitates scalable image annotation techniques to effectively organize and retrieval the large scale dataset. Although some possibly related textual information to images is presented on their associated web pages, the relationship between the surrounding text and images varies greatly, with much of the text being redundant and unrelated. Therefore, how to best explore the weak supervision from textual information is a challenging problem for the task of scalable image annotation.</p><p>The goal of scalable image annotation task in ImageCLEF 2015 is to describe visual content of images with concepts, and to localize the concepts in the images <ref type="bibr" coords="2,134.77,233.14,10.52,8.74" target="#b6">[7,</ref><ref type="bibr" coords="2,146.95,233.14,11.62,8.74" target="#b16">17]</ref>. The task provides a dataset of 500,000 web images with the textual information extracted from web pages, in which 1979 items with ground truth localized concept labels form the development set. The overall performance will be evaluated to annotate and localize concepts on the full 500,000 images. The large scale test data and the new task of concept localization are the main differences from the previous ImageCLEF challenges. Unlike the other popular challenges like ILSVRC <ref type="bibr" coords="2,244.98,304.87,15.50,8.74" target="#b13">[14]</ref> and Pascal <ref type="bibr" coords="2,318.79,304.87,9.96,8.74" target="#b4">[5]</ref>, such task has few fully labelled training data but a large-amount of raw web resources used for model learning.</p><p>For the participation of the scalable image annotation task, we adopt a twostage hybrid learning framework to fully use the limited labelled data and the large scale web resource. In the first stage, we train a SVM classifier for each concept in a one-vs-rest manner. To avoid the overfitting problem brought by the small scale training data, we adopt another unsupervised solution as a complement to enhance the scalability of our work. We attempt to annotate an image by search on the whole 500,000 dataset, in which the visual and semantical similarities are jointly estimated with deep visual features <ref type="bibr" coords="2,384.93,418.01,15.50,8.74" target="#b9">[10]</ref> and deep textual features (i.e., Word2Vec <ref type="bibr" coords="2,245.62,429.97,14.76,8.74" target="#b11">[12]</ref>), and the WordNet is used to mine the relevant concepts from the textual information of those similar images. After the concept annotation stage, we obtain a set of concepts relevant to each image. We will localize the concepts through the second stage, in which the latest deep model, Fast R-CNN <ref type="bibr" coords="2,206.98,477.79,10.52,8.74" target="#b7">[8]</ref> is adopted to predict the possible locations of the concepts obtained in the first stage. Although the deep model can directly predict and localize the concepts depicted in each images, the performance is unstable possibly due to the too small number of training data with ground truth localized concept labels, which can be demonstrated from the experimental results. Thus, we use the top-K predicted regions to each concept obtained in the first stage as outputs. Besides, we adopt a search-based approach to localize the scene-related concepts (e.g., "sea", "beach" and "river"). Specifically, the location of each predicted concept for an image is decided by the spatial layout of its visually similar images in training dataset. The experimental results show that the hybrid twostage learning framework contributes to the improvement of image annotation and localization. Furthermore, there are a few concepts related with the concept "face" ( e.g., "head", "eye", "nose", "mouth" and "beard"). Since face detection and facial point detection have been actively studied over the past years and achieve satisfactory performance <ref type="bibr" coords="2,279.97,645.16,15.50,8.74" target="#b18">[19,</ref><ref type="bibr" coords="2,297.13,645.16,7.75,8.74" target="#b3">4,</ref><ref type="bibr" coords="2,306.54,645.16,11.62,8.74" target="#b14">15]</ref>, we employ face detection and facial point detection to localize face related concepts exactly. Then the SVM classifier is adopted to make prediction for the annotation. Furthermore, visual based retrieval is performed and the result is reranked with the surrounding text. The annotations to the testing image will be mined from the textual descriptions of the above obtained similar image set with the WordNet. The lower part shows the second stage of concept localization. Two methods are employed during localization. The Fast R-CNN is adopted, which works well for the concepts with obvious object. Meanwhile, localization by search method is adopted, which works well for the scene related concepts ( e.g., "sea" and "beach"). Finally, localization results from the two methods are combined.</p><p>The remainder of this working note is structured as follows. Section 2 presents the details about data preparation for model training. In Section 3, we elaborate the details about how we obtain the results of image annotation. Section 4 introduces the annotation localization approach. In section 5, we discuss the results of experiments and the parameter settings. Finally, we conclude our participation in ImageCLEF 2015 in section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data Preparation</head><p>In this year, hand labeled data is allowed in the image annotation and localization task. We prefer multiple online resources to perform such task, including the ImageNet database <ref type="bibr" coords="3,220.98,657.11,14.61,8.74" target="#b13">[14]</ref>, the Sun database <ref type="bibr" coords="3,319.77,657.11,14.61,8.74" target="#b17">[18]</ref>, the WordNet <ref type="bibr" coords="3,400.34,657.11,15.50,8.74" target="#b12">[13]</ref> and the online image sharing website Flicker<ref type="foot" coords="4,266.72,118.42,3.97,6.12" target="#foot_0">1</ref> . To perform image annotation by classification, we attempt to collect the training images from the well labeled dataset ImageNet and Sun dataset. There are 175 concepts concurrent in the ImageNet dataset and the ImageCLEF task simultaneously. Meanwhile, there are 217 concepts concurrent in the Sun dataset and the ImageCLEF task. For the concepts not in the ImageNet and Sun database, images are crawled from the online image sharing website Flicker and filtered by humans with 50 images left for each concept. In our work, the visual features of images are represented with deep features <ref type="bibr" coords="4,467.32,203.68,9.96,8.74" target="#b8">[9]</ref>, we employ the VGG19 model <ref type="bibr" coords="4,272.01,215.63,15.50,8.74" target="#b9">[10]</ref> pretrained on the ImageNet dataset (1000 classes) and average the output of its relu6 layer for 10-view image patches (4 corners and 1 center patches of an image as well as their mirrors) as our visual feature.</p><p>There are 1979 images has been released to test the proposed method. The frequency of different concept is unbalanced and there are 17 concepts do not occur in the development dataset. Then we have collected some images for such concepts to make the development set more applicable to set hyper-parameters and validate the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Concept Annotation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Annotation By Classification</head><p>Image annotation by classification is to train a multi-class classifier or one-vs-rest classifier corresponding to different concepts. Such solution is simple, and usually can achieve satisfactory performance given abundant training data. Towards this problem, we choose a linear Support Vector Machine (SVM) <ref type="bibr" coords="4,403.05,428.25,10.52,8.74" target="#b5">[6]</ref> to train a onevs-rest classifier for each concept. Due to images usually being labelled with multiple concepts in training data, the negative samples for a given concept classifier are selected as the ones whose all labels do not include the concept. For a testing image, we select the most confident concepts by thresholding the classification confidences to obtain the annotations of each image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Annotation By Search</head><p>The search-based approach for image annotation works on the assumption that visual similar images should reflect similar semantical concepts, and most textual information of web images is relevant to their visual content. Thus, the searchbased annotation process can be divided into two phases: one is the search for similar images, and the other is relevant concept selection from the textual information of those similar images. First, given a testing image with textual information, we will search its similar neighbors on the whole 500,000 dataset. As mentioned in section 2, images are represented with 4096-dimensional deep features. To speed up similar image retrieval for the large scale image database, we adopt a hash encoding algorithm. Specially, we map the deep features to 32768-dimensional binary hash codes leveraging the random projection algorithm proposed in <ref type="bibr" coords="5,412.76,143.90,9.96,8.74" target="#b1">[2]</ref>, and employ hamming distance to rank the images in the dataset.</p><p>To further improve the results of visual similarity search, we explore the textual information of the given image, and perform the semantic similarity search on the top-N A visually similar images to rerank the similar image set. Here, we use a publicly available tool of Word2Vec <ref type="bibr" coords="5,315.28,204.05,15.50,8.74" target="#b11">[12]</ref> to compute vector representations of textual information of images, which are provided with the scofeat descriptors. With the word vector representations, the cosine distance is used to rerank images in order to obtain a set of visually and semantically similar images.</p><p>Next, the annotations to the testing image will be mined from the textual descriptions of the above obtained similar image set. For the annotation mining, we employ a WordNet-based approach, which is similar to the solution in <ref type="bibr" coords="5,467.32,276.16,9.96,8.74" target="#b2">[3]</ref>. The major difference is that we mine the concepts from a set of visually and semantically similar images, while they considered only the visual similarities among images. A candidate concept graph is built with the help of WordNet, and the top-N W concepts with higher number of links are selected as the final image description.</p><p>We combine the results of the above classification-based solution and the search-based solution with different thresholding settings, while their different performances will be discussed in the experimental session.</p><p>Concept extension is adopted to deal with the strong correlation among concepts to make the annotation result more sufficient. For the given 251 concepts, some concepts have strong correlation like "eyes" and "nose", which usually occur together. Hierarchy relation may exist like concepts "apple" and "fruit". When the child node concept "apple" occurs, the parent node concept "fruit" must occur. These relations can be achieved by exploring the WordNet concept hierarchy and the provided ImageCLEF concept set with general level categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Annotation Localization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Localization by Fast RCNN</head><p>To localize the objects in the given images, we follow the Fast R-CNN framework (FRCN) proposed in <ref type="bibr" coords="5,227.85,549.52,9.96,8.74" target="#b7">[8]</ref>, which provides classification result and regressed location simultaneously for each candidate object proposal. The FRCN approach is conducted on a number of regions of interest (RoIs), which are sampled from the object proposals of the images. To reduce repetitive computation of the overlapped regions, the last pooling layer of the FRCN network is replaced with a RoI pooling layer compared with traditional CNN network, which observably speeds up the training and testing process. Furthermore, the network uses two sibling loss terms as supervision to learn the classification and localization information collaboratively, which are proved to be helpful to improving the performance and make the approach a one-stage detection framework.</p><p>Each RoI corresponds to a region in the feature map provided by the last convolutional layer. The RoI pooling layer carries out max pooling on each of the corresponding regions and pools them into fixed-size feature maps. The scale of pooling mask in RoI pooling layer is auto-adjusted according to the spatial size of the input feature map regions, to make the outputs all have the same size. Therefore, the feature map of each RoI can match the following fully connected layers seamlessly after RoI pooling and contribute to the network as an independent instance.</p><p>As for the supervision, the FRCN network employs two sibling output layers to predict classification probability and bounding box regression offsets respectively for each RoI on each category. The first output layer is a softmax layer which outputs a probability distribution over all categories. And we use the standard cross-entropy loss function to constrain it as follows,</p><formula xml:id="formula_0" coords="6,271.12,283.81,209.47,9.65">L cls = -log(p k * ) (1)</formula><p>where k * is the groundtruth label and pk * is the predicted probability on this class, assuming that there are totally K + 1 categories including K object classes and a background class. The second output layer is a regression layer which predicts the bounding box regression offsets for each category as</p><formula xml:id="formula_1" coords="6,399.09,338.45,81.50,12.55">t k = (t k x , t k y , t k h , t k w ),</formula><p>where k is the index of the K object classes. Assuming that the groundtruth offsets for the class k * is t * = (t *</p><p>x , t * y , t * h , t * w ), the regression loss function is formulated as follows,</p><formula xml:id="formula_2" coords="6,232.46,393.97,248.13,24.24">L loc = i∈{x,y,h,w} smooth L1 (t k * i , t * i )<label>(2)</label></formula><p>where</p><formula xml:id="formula_3" coords="6,225.00,439.17,255.59,24.66">smooth L1 (x) = 0.5x 2 if |x| &lt; 1 |x| -0.5 otherwise (3)</formula><p>Thus, the loss function for the whole network can be formulated as follows,</p><formula xml:id="formula_4" coords="6,254.47,491.23,221.88,11.72">L = L cls + λ[k * ≥ 1]L loc (<label>4</label></formula><formula xml:id="formula_5" coords="6,476.35,493.30,4.24,8.74">)</formula><p>where λ is a weighting parameter to balance the two loss terms. And [k * ≥ 1] is an indicator function with the convention that the background class is labeled as k * = 0 and the object classes as k * = 1, 2, • • • , K, which means that the localization regression loss term is ignored for the background RoIs.</p><p>In practice, we first extract object proposals with the selective search method <ref type="bibr" coords="6,134.77,573.43,15.50,8.74" target="#b15">[16]</ref> and then sample RoIs from them for training. We take 25% of the RoIs from the object proposals that overlap certain groundtruth bounding boxes with more than 0.5 IoU (intersection over union) and label them the same with the groundtruth bounding boxes. The rest RoIs are sampled from the object proposals with a maximum IoU between 0.1 and 0.5 with the groundtruth bounding boxes and are labeled as background, as instructed in <ref type="bibr" coords="6,371.39,633.20,9.96,8.74" target="#b7">[8]</ref>. While during testing, we input all the object proposals into the network and predict labels and regression offsets for all of them. A preliminary screening is also implemented in this step with non-maximum suppression to take out some object proposals with too low classification probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Localization by Search</head><p>We give a special consideration to the scene related concepts for the annotation localization ( e.g., "beach", "sea", "river" and "valley"). If an image is predicted as a scenery concept, we first find its top-N L visually similar neighbors with the same concept in the localization training data, and use their merged bounding box as the location of the scenery concept. Figure <ref type="figure" coords="7,352.15,481.93,4.98,8.74" target="#fig_1">2</ref> shows some examples about search-based location results. The final localization results will be composed of the predicted results of the Fast R-CNN model and search-based localization results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Localization of Face Related Concepts</head><p>There are a few concepts related with the concept "face" ( e.g., "head", "eye", "nose", "mouth" and "beard"). To localize face related concepts exactly, we employ face detection algorithm with aggregate channel features <ref type="bibr" coords="7,423.47,585.38,15.50,8.74" target="#b18">[19,</ref><ref type="bibr" coords="7,440.62,585.38,7.01,8.74" target="#b3">4]</ref>. Facial point detection is performed to locate key points in the face <ref type="bibr" coords="7,405.50,597.34,14.61,8.74" target="#b14">[15]</ref>. Localization of the concepts "eye", "nose", "mouth" and "beard" is got based on the key points. Concepts of "ear" and "neck" are located based on the relative location of face with experience. Besides, linear classifiers <ref type="bibr" coords="7,351.94,633.20,10.52,8.74" target="#b5">[6]</ref> are trained with the SIFT <ref type="bibr" coords="7,134.77,645.16,15.50,8.74" target="#b10">[11]</ref> features extracted on the facial points to determine the concepts of "man", "woman", "male child" and "female child". </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We have submitted 8 runs with 5 different settings of combinations with the above model modules, including Annotation By Classification (ABC), Annotation By Search (ABS), localization by Fast R-CNN (FRCN), Localization By Search (LBS) and Concept Extension (CE). Some runs are of the same setting with different parameter values. The experimental results of our method with different settings are presented in Table <ref type="table" coords="8,172.96,621.25,3.87,8.74" target="#tab_0">1</ref>. The last two columns show the mean average precision with different overlap percentage with ground truth labels. Detailed results with increasing overlap percentage is shown in Fig. <ref type="figure" coords="8,286.60,645.16,3.87,8.74" target="#fig_2">3</ref>. We can find the setting 1 with the modules ABC, ABS, FRCN and LBS achieves the best result, and setting 2 extending  Map with 50% overlap with the GT labels Fig. <ref type="figure" coords="9,154.40,318.61,4.13,7.89">4</ref>. Submission results of different teams with 50% overlap with the ground truth labels. Results of our method with different runs are colored red.</p><p>setting 1 with CE achieves comparable results. By comparing the result of setting 2 and setting 3, we can find the effectiveness by taking annotation by search into consideration. Furthermore, we have validated the effect of FRCN, and we find that result of localization by detection only is unsatisfactory. This is mainly due to two reasons, one is that the training data is very limited for each concept, the other is that the content in the web images do not have obvious objectness <ref type="bibr" coords="9,134.77,433.40,10.52,8.74" target="#b0">[1]</ref> for some concepts. The proposed hybrid learning framework with two stage process is more suitable to deal such task. Comparisons of our runs (denoted IVANLPR-*) and other participants' runs are illustrated in figure <ref type="figure" coords="9,452.37,457.31,3.87,8.74">4</ref>. The submitted runs of our team achieved the second place among the different teams, which shows the outperformance of the proposed hybrid two-stage learning framework for the scalable annotation and localization task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we described the participation of IVANLPR team at ImageCLEF 2015 Scalable Concept Image Annotation task. We proposed a hybrid learning framework to solve the scalable annotation task. We adopt a two-stage solution to first annotate images with possible concepts and then localize the concepts in the images. For the first stage, both supervised method and unsupervised method are adopted to make full use of the available hand-labeled data and surrounding text in the webpage. For the second stage, Fast R-CNN and search-based method are adopted to locate the annotation concepts. Extensive experiments demonstrate the outperformance of the proposed hybrid two-stage learning framework for the scalable annotation task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,134.77,362.02,345.81,7.89;3,134.77,373.01,345.81,7.86;3,134.77,383.97,345.81,7.86;3,134.77,394.93,345.82,7.86;3,134.77,405.89,345.81,7.86;3,134.77,416.85,345.81,7.86;3,134.77,427.80,345.81,7.86;3,134.77,438.76,345.81,7.86;3,134.77,449.72,345.81,7.86;3,134.77,460.68,345.81,7.86;3,134.77,471.64,148.31,7.86;3,350.01,248.17,56.34,56.34"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Flowchart of the proposed method. The top part shows the first stage of image annotation. For a given query image, deep visual feature is extracted with the VGG 19 network. Then the SVM classifier is adopted to make prediction for the annotation. Furthermore, visual based retrieval is performed and the result is reranked with the surrounding text. The annotations to the testing image will be mined from the textual descriptions of the above obtained similar image set with the WordNet. The lower part shows the second stage of concept localization. Two methods are employed during localization. The Fast R-CNN is adopted, which works well for the concepts with obvious object. Meanwhile, localization by search method is adopted, which works well for the scene related concepts ( e.g., "sea" and "beach"). Finally, localization results from the two methods are combined.</figDesc><graphic coords="3,350.01,248.17,56.34,56.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,134.77,314.94,345.82,7.89;7,134.77,325.93,345.81,7.86;7,134.77,336.89,345.81,7.86;7,134.77,347.85,210.91,7.86"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Examples about search-based localization. Annotation of the query image is achieved by the method introduced in Section 3. Similar images is achieved by visual similarity. Localization of a given concept for the query image can be achieved by transferring the bounding box in the similar images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,190.20,355.38,231.87,7.89"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Mean average precision with different recall rates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,155.49,488.09,304.37,84.25"><head>Table 1 .</head><label>1</label><figDesc>Results for the submitted runs with different settings</figDesc><table coords="8,155.49,508.89,304.37,63.45"><row><cell cols="5">Method ABC CE ABS LBS FRCN SVM Threshold Overlap 0.5 Overlap 0</cell></row><row><cell cols="2">Setting 1 yes no yes yes yes</cell><cell>0.5</cell><cell>0.510</cell><cell>0.642</cell></row><row><cell cols="2">Setting 2 yes yes yes yes yes</cell><cell>0.4</cell><cell>0.510</cell><cell>0.635</cell></row><row><cell cols="2">Setting 3 yes yes no yes yes</cell><cell>0.4</cell><cell>0.486</cell><cell>0.613</cell></row><row><cell cols="2">Setting 4 yes no no yes yes</cell><cell>0.4</cell><cell>0.432</cell><cell>0.552</cell></row><row><cell>Setting 5 no no no no</cell><cell>yes</cell><cell>0.4</cell><cell>0.368</cell><cell>0.469</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,144.73,657.80,98.62,7.86"><p>https://www.flickr.com/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>7 Acknowledgments. This work was supported by <rs type="programName">973 Program</rs> (<rs type="grantNumber">2012CB316304</rs>) and <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">61332016</rs>, <rs type="grantNumber">61272329</rs> and <rs type="grantNumber">61472422</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_G399y3A">
					<idno type="grant-number">2012CB316304</idno>
					<orgName type="program" subtype="full">973 Program</orgName>
				</org>
				<org type="funding" xml:id="_eGkmmm2">
					<idno type="grant-number">61332016</idno>
				</org>
				<org type="funding" xml:id="_AhCexnK">
					<idno type="grant-number">61272329</idno>
				</org>
				<org type="funding" xml:id="_vkczqYS">
					<idno type="grant-number">61472422</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,142.96,215.39,337.62,7.86;10,151.52,226.34,25.60,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,307.17,215.39,80.53,7.86">What is an object?</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,407.13,215.39,23.34,7.86">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,237.57,337.62,7.86;10,151.52,248.53,329.06,7.86;10,151.52,259.49,329.06,7.86;10,151.52,270.45,77.30,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,264.84,237.57,215.73,7.86;10,151.52,248.53,133.36,7.86">Random projection in dimensionality reduction: Applications to image and text data</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Bingham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mannila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,306.30,248.53,174.28,7.86;10,151.52,259.49,271.05,7.86">Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="245" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,281.68,337.62,7.86;10,151.52,292.64,329.06,7.86;10,151.52,303.60,148.24,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,359.37,281.68,121.21,7.86;10,151.52,292.64,205.66,7.86">DISA at imageclef 2014: The search-based solution for scalable image annotation</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Budíková</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Botorek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Batko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zezula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,378.61,292.64,101.97,7.86;10,151.52,303.60,64.11,7.86">Working Notes for CLEF 2014 Conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="360" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,314.83,337.62,7.86;10,151.52,325.79,329.05,7.86;10,151.52,336.74,70.13,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,347.32,314.83,133.25,7.86;10,151.52,325.79,35.49,7.86">Fast feature pyramids for object detection</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,193.38,325.79,261.22,7.86">Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1532" to="1545" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>IEEE Transactions on</note>
</biblStruct>

<biblStruct coords="10,142.96,347.97,337.62,7.86;10,151.52,358.93,329.06,7.86;10,151.52,369.89,116.00,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,464.70,347.97,15.87,7.86;10,151.52,358.93,180.13,7.86">The pascal visual object classes (VOC) challenge</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,339.19,358.93,141.39,7.86;10,151.52,369.89,25.39,7.86">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,381.12,337.62,7.86;10,151.52,392.08,304.08,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,401.63,381.12,78.95,7.86;10,151.52,392.08,112.06,7.86">Liblinear: A library for large linear classification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><forename type="middle">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,270.21,392.08,83.91,7.86">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008-06">Jun 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,403.31,337.62,7.86;10,151.52,414.27,329.06,7.86;10,151.52,425.23,329.06,7.86;10,151.52,436.18,324.59,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,234.93,414.27,245.64,7.86;10,151.52,425.23,170.53,7.86">Overview of the ImageCLEF 2015 Scalable Image Annotation, Localization and Sentence Generation task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dellandrea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,342.69,425.23,137.88,7.86;10,151.52,436.18,154.07,7.86">CLEF2015 Working Notes. CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-11">September 8-11 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,447.41,268.41,7.86" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.08083</idno>
		<title level="m" coord="10,205.54,447.41,39.79,7.86">Fast r-cnn</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,142.96,458.64,337.62,7.86;10,151.52,469.60,329.05,7.86;10,151.52,480.56,154.43,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="10,237.79,469.60,238.18,7.86">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,142.61,491.79,337.96,7.86;10,151.52,502.75,154.43,7.86" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="10,207.63,491.79,269.02,7.86">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,142.61,513.98,337.96,7.86;10,151.52,524.94,198.86,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,193.30,513.98,227.74,7.86">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,428.33,513.98,52.25,7.86;10,151.52,524.94,112.85,7.86">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,536.17,337.96,7.86;10,151.52,547.13,263.41,7.86" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<title level="m" coord="10,341.32,536.17,139.26,7.86;10,151.52,547.13,101.88,7.86">Efficient estimation of word representations in vector space</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,142.61,558.35,337.96,7.86;10,151.52,569.31,82.93,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,204.31,558.35,153.97,7.86">Wordnet: A lexical database for english</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,364.36,558.35,116.21,7.86">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,580.54,337.96,7.86;10,151.52,591.50,329.05,7.86;10,151.52,602.46,329.06,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,420.30,591.50,60.28,7.86;10,151.52,602.46,129.33,7.86">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,287.15,602.46,165.43,7.86">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,613.69,337.96,7.86;10,151.52,624.65,202.60,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,272.55,613.69,208.03,7.86;10,151.52,624.65,35.49,7.86">Deep convolutional network cascade for facial point detection</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,208.10,624.65,29.18,7.86">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3476" to="3483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,635.88,337.96,7.86;10,151.52,646.84,329.06,7.86;10,151.52,657.80,25.60,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,417.53,635.88,63.05,7.86;10,151.52,646.84,87.13,7.86">Selective search for object recognition</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,246.61,646.84,165.49,7.86">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,120.67,337.96,7.86;11,151.52,131.63,329.06,7.86;11,151.52,142.59,329.05,7.86;11,151.52,153.55,329.05,7.86;11,151.52,164.51,126.65,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="11,398.62,142.59,81.95,7.86;11,151.52,153.55,142.90,7.86">General Overview of ImageCLEF at the CLEF 2015 Labs</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bromuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">K</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Uskudarli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">B</forename><surname>Marvasti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Aldana</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Del Mar Roldán</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>García</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="11,301.24,153.55,139.06,7.86">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,175.47,337.96,7.86;11,151.52,186.42,283.46,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="11,376.45,175.47,104.12,7.86;11,151.52,186.42,144.03,7.86">Sun database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,316.73,186.42,29.18,7.86">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3485" to="3492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,197.38,337.96,7.86;11,151.52,208.34,169.58,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="11,293.62,197.38,186.95,7.86;11,151.52,208.34,35.49,7.86">Aggregate channel features for multi-view face detection</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,208.10,208.34,23.80,7.86">IJCB</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
