<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,156.22,115.96,302.91,12.62;1,146.86,133.89,321.64,12.62;1,251.18,151.82,113.00,12.62">Overview of the ImageCLEF 2015 Scalable Image Annotation, Localization and Sentence Generation task</title>
				<funder ref="#_hyRCyJS">
					<orgName type="full">European Science Foundation</orgName>
					<orgName type="abbreviated">ESF</orgName>
				</funder>
				<funder ref="#_pEHJ2tE">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder ref="#_fKeFt4F #_gXTGJwF">
					<orgName type="full">EPSRC</orgName>
				</funder>
				<funder ref="#_zgC7Uwb">
					<orgName type="full">EU</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,143.56,189.96,67.26,8.74"><forename type="first">Andrew</forename><surname>Gilbert</surname></persName>
						</author>
						<author>
							<persName coords="1,218.41,189.96,45.44,8.74"><forename type="first">Luca</forename><surname>Piras</surname></persName>
						</author>
						<author>
							<persName coords="1,271.35,189.96,52.77,8.74"><forename type="first">Josiah</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName coords="1,332.98,189.96,31.14,8.74"><forename type="first">Fei</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<persName coords="1,372.41,189.96,94.86,8.74"><forename type="first">Emmanuel</forename><surname>Dellandrea</surname></persName>
						</author>
						<author>
							<persName coords="1,167.53,201.92,79.86,8.74"><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
						</author>
						<author>
							<persName coords="1,255.33,201.92,75.91,8.74"><forename type="first">Mauricio</forename><surname>Villegas</surname></persName>
						</author>
						<author>
							<persName coords="1,353.93,201.92,93.90,8.74"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
						</author>
						<title level="a" type="main" coord="1,156.22,115.96,302.91,12.62;1,146.86,133.89,321.64,12.62;1,251.18,151.82,113.00,12.62">Overview of the ImageCLEF 2015 Scalable Image Annotation, Localization and Sentence Generation task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6C26CB86B89A9A0263A5DE650DFA72BF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The ImageCLEF 2015 Scalable Image Annotation, Localization and Sentence Generation task was the fourth edition of a challenge aimed at developing more scalable image annotation systems. In particular this year the focus of the three subtasks available to participants had the goal to develop techniques to allow computers to reliably describe images, localize the different concepts depicted in the images and generate a description of the scene. All three tasks use a single mixed modality data source of 500,000 web page items which included raw images, textual features obtained from the web pages on which the images appeared, as well as various visual features extracted from the images themselves. Unlike previous years the test set was also the training set and in this edition of the task hand-labelled data has been allowed. The images were obtained from the Web by querying popular image search engines. The development and subtasks 1 and 2 test sets were both taken from the "training set" and had 1,979 and 3,070 samples, and the subtask 3 track had 500 and 450 samples. The 251 concepts this year were chosen to be visual objects that are localizable and that are useful for generating textual descriptions of visual content of images and were mined from the texts of our large database of image-webpage pairs. This year 14 groups participated in the task, submitting a total of 122 runs across the 3 subtasks and 11 of the participants also submitted working notes papers. This result is very positive, in fact if compared to the 11 participants and 58 submitted runs of the last year it is possible to see how the interest in this topic is still very high.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="792.0" lry="612.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Every day, users struggle with the ever-increasing quantity of data available to them. Trying to find "that" photo they took on holiday last year, the image on Google of their favourite actress or band, or the images of the news article someone mentioned at work. There is a large number of images that can be cheaply found and gathered from the Internet. However, more valuable is mixed modality data, for example, web pages containing both images and text. A large amount of information about the image is present on these web pages and viceversa. However, the relationship between the surrounding text and images varies greatly, with much of the text being redundant and/or unrelated. Moreover, images and the webpages on which they appear can be easily obtained for virtually any topic using a web crawler. In existing work such noisy data has indeed proven useful, e.g. <ref type="bibr" coords="2,218.78,142.90,15.90,8.74" target="#b18">[19,</ref><ref type="bibr" coords="2,234.68,142.90,11.93,8.74" target="#b28">29,</ref><ref type="bibr" coords="2,246.61,142.90,11.93,8.74" target="#b26">27]</ref>. Despite the obvious benefits of using such information in automatic learning, the very weak supervision it provides means that it remains a challenging problem. The Scalable Image Annotation, Localization and Sentence Generation task aims to develop techniques to allow computers to reliably describe images, localize the different concepts depicted in the images and generate a description of the scene.</p><p>The Scalable Image Annotation, Localization and Sentence Generation task is a continuation of the general image annotation and retrieval task that has been part of ImageCLEF since its very first edition in 2003. In the early years the focus was on retrieving relevant images from a web collection given (multilingual) queries, while from 2006 onwards annotation tasks were also held, initially aimed at object detection, but more recently also covering semantic concepts. In its current form, the 2015 Scalable Concept Image Annotation task is its fourth edition, having been organized in 2012 <ref type="bibr" coords="2,307.46,298.34,14.61,8.74" target="#b22">[23]</ref>, 2013 <ref type="bibr" coords="2,352.76,298.34,15.50,8.74" target="#b24">[25]</ref> and 2014 <ref type="bibr" coords="2,414.87,298.34,14.61,8.74" target="#b23">[24]</ref>. In light of recent interest in annotating images beyond just concept labels, we introduced two new subtasks this year where participants developed systems to describe an image with a textual description of the visual content depicted in the image.</p><p>This paper presents the overview of the fourth edition of the Scalable Concept Image Annotation task <ref type="bibr" coords="2,263.84,358.14,15.90,8.74" target="#b22">[23,</ref><ref type="bibr" coords="2,279.74,358.14,11.93,8.74" target="#b24">25,</ref><ref type="bibr" coords="2,291.66,358.14,11.93,8.74" target="#b23">24]</ref>, one of the four benchmark campaigns organized by ImageCLEF <ref type="bibr" coords="2,253.19,370.10,15.50,8.74" target="#b20">[21]</ref> in 2015 under the CLEF initiative 1 . Section 2 describes the task in detail, including the participation rules and the provided data and resources. Followed by this, Section 3 presents and discusses the results of the submissions. Finally, Section 4 concludes the paper with final remarks and future outlooks.</p><p>2 Overview of the Task</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Motivation and Objectives</head><p>Image concept annotation, localization and natural sentence generation generally has relied on training data that has been manually, and thus reliably annotated, an expensive and laborious endeavour that cannot easily scale, particularly as the number of concepts grow. However, images for any topic can be cheaply gathered from the web, along with associated text from the webpages that contain the images. The degree of relationship between these web images and the surrounding text varies greatly, i.e., the data are very noisy, but overall these data contain useful information that can be exploited to develop annotation systems. Motivated by this need for exploiting this useful data, the ImageCLEF 2015 Scalable Concept annotation, localization and sentence generation task aims to develop techniques to allow computers to reliably describe images, localize the different concepts depicted in the images and generate a description of the scene. Figure <ref type="figure" coords="2,134.77,637.45,4.98,8.74" target="#fig_1">1</ref> shows examples of typical images found by querying search engines. As can  be seen, the data obtained are useful and furthermore a wider variety of images is expected, not only photographs but also drawings and computer generated graphics. This diversity has the advantage that this data can also handle the possible different senses that a word can have, or the different types of images that exist. Likewise, there are other resources available that can help to determine the relationships between text and semantic concepts, such as dictionaries or ontologies. There are also tools that can help to deal with noisy text commonly found on webpages, such as language models, stop word lists and spell checkers. The goal of this task was to evaluate different strategies to deal with the noisy data so that it can be reliably used for annotating, localizing, and generating natural sentences from practically any topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Challenge Description</head><p>This year the challenge 2 consisted of 3 subtasks 1. Subtask 1: The image annotation task continues in the same line of past years. The objective required the participants to develop a system that receives as input an image and produces as output a prediction of which concepts are present in that image, selected from a predefined list of concepts and starting this year, where they are located within the image. 2. Subtask 2 (Noisy Track ): In light of recent interest in annotating images beyond just concept labels, this subtask required the participants to describe images with a textual description of the visual content depicted in the image. It is thought of as an extension of subtask 1. This track was geared towards participants interested in developing systems that generated textual descriptions directly from images, e.g. by using visual detectors to identify concepts and generating textual descriptions from the detected concepts. This had a large overlap with subtask 1. 3. Subtask 3 (Clean track ): Aimed primarily at those interested only in the Natural Language Generation aspects of the subtask, therefore a gold standard input (bounding boxes labelled with concepts) was provided to develop systems that generate sentence, natural language based descriptions based on these gold standard annotations as input.</p><p>As common training set the participants were provided with 500,000 images crawled from the Internet, the corresponding webpages on which they appeared, as well as precomputed visual and textual features. Apart from the image and webpage data, the participants were also permitted and encouraged to use similar datasets and any other automatically obtainable resources to help in the processing and usage of the training data. In contrast to previous years, in this edition of the task hand labelled data has been allowed. Thus, the available trained ImageNet CNNs could be used, and the participants were encouraged to use also other resources such as ontologies, word disambiguators, language models, language detectors, spell checkers, and automatic translation systems. Unlike previous years, the test set was also the training set.</p><p>For the development of the annotation systems, the participants were provided with the following:</p><p>-A training dataset of images and corresponding webpages compiled specifically for the three subtasks, including precomputed visual and textual features (see Section 2.3). -A development set of images with ground truth labelled bounding box annotations and precomputed visual features for estimating the system performance. -A development set of images with at least five textual descriptions per image for Subtask 2 and Subtask 3. -A subset of the development set for Subtask 3 with gold standard inputs (bounding boxes labelled with concepts) and correspondence annotation between bounding box inputs and terms in textual descriptions.</p><p>This year the training and the test images are all contained within the 500,000 images released at the beginning of the competition. At test time, it was expected that participants provided a classification for all images. After a period of two months, the development set, which included ground truth localized annotations, was released and about two months were given for participants to work on the development data. A maximum of 10 submissions per subtask (also referred to as runs) was allowed per participating group. The 251 concepts this year were chosen to be visual objects that are localizable and that are useful for generating textual descriptions of the visual content of images. They include animate objects such as people, dogs and cats, inanimate objects such as houses, cars and balls, and scenes such as city, sea and mountains. The concepts were mined from the texts of our database of 31 million image-webpage pairs <ref type="bibr" coords="5,249.13,130.95,14.61,8.74" target="#b21">[22]</ref>. Nouns that are subjects or objects of sentences are extracted and mapped onto WordNet synsets <ref type="bibr" coords="5,356.54,142.90,9.96,8.74" target="#b4">[5]</ref>. These were then filtered to 'natural', basic-level categories (dog rather than a Yorkshire terrier ), based on the WordNet hierarchy and heuristics from a large-scale text corpora <ref type="bibr" coords="5,462.33,166.81,14.61,8.74" target="#b25">[26]</ref>. The final list of concepts were manually shortlisted by the organisers such that they were (i) visually concrete and localizable; (ii) suitable for use in image descriptions; (iii) at a suitable 'every day' level of specificity that were neither too general nor too specific. The complete list of concepts, as well as the number of samples in the test sets, is included in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Dataset</head><p>The dataset<ref type="foot" coords="5,187.18,269.60,3.97,6.12" target="#foot_1">3</ref> used was very similar to the one of the first three editions of the task <ref type="bibr" coords="5,157.46,283.13,15.90,8.74" target="#b22">[23,</ref><ref type="bibr" coords="5,173.36,283.13,11.93,8.74" target="#b24">25,</ref><ref type="bibr" coords="5,185.29,283.13,11.93,8.74" target="#b23">24]</ref>. To create the dataset, initially a database of over 31 million images was created by querying Google, Bing and Yahoo! using words from the Aspell English dictionary <ref type="bibr" coords="5,251.71,307.04,14.61,8.74" target="#b21">[22]</ref>. The images and corresponding webpages were downloaded, taking care to avoid data duplication. Then, a subset of 500,000 images was selected from this database by choosing the top images from a ranked list. For further details on how the dataset was created, please refer to <ref type="bibr" coords="5,442.09,342.90,14.61,8.74" target="#b22">[23]</ref>. The ranked list was generated by retrieving images from our database using the list of concepts, in essence, more or less as if the search engines had only been queried for these. From the ranked list, some types of problematic images were removed, and it was guaranteed that each image had at least one webpage in which they appeared.</p><p>The development and test sets were both taken from the "training set". A set of 5,520 images was selected for this purpose using a CNN trained to identify images suitable for sentence generation. The images were then annotated via crowd-sourcing in three stages: (i) image level annotation for the 251 concepts; (ii) bounding box annotation; (iii) textual description annotation. For the textual descriptions, basic spell correction was performed manually by the organisers using Aspell<ref type="foot" coords="5,236.14,484.79,3.97,6.12" target="#foot_2">4</ref> . Both American and British English spelling variants (color vs. colour ) were retained to reflect the challenge of real-world English spelling variants. A subset of these samples was then selected for subtask 3 and further annotated by the organisers with correspondence annotations between bounding box instances and terms in textual descriptions.</p><p>The development set contained 2,000 samples, out of which 500 samples were further annotated and used as the development set for the subtask 3. Note that only 1,979 samples from the development set contain at least one bounding box annotation. The number of textual descriptions for the development set ranged from 5 to 51 per image (with a mean of 9.5 and a median of 8 descriptions). The test set for subtasks 1 and 2 contains 3,070 samples, while the test set for subtask 3 comprises 450 samples which are disjoint from the test set of subtasks 1 and 2.</p><p>Textual Data: Four sets of data were made available to the participants. The first one was the list of words used to find the image when querying the search engines, along with the rank position of the image in the respective query and search engine it was found on. The second set of textual data contained the image URLs as referenced in the webpages they appeared in. In many cases, the image URLs tend to be formed with words that relate to the content of the image, which is why they can also be useful as textual features. The third set of data was the webpages in which the images appeared, for which the only preprocessing was a conversion to valid XML just to make any subsequent processing simpler. The final set of data were features obtained from the text extracted near the position(s) of the image in each webpage it appeared in.</p><p>To extract the text near the image, after conversion to valid XML, the script and style elements were removed. The extracted texts were the webpage title, and all the terms closer than 600 in word distance to the image, not including the HTML tags and attributes. Then a weight s(t n ) was assigned to each of the words near the image, defined as</p><formula xml:id="formula_0" coords="6,210.23,315.52,270.36,26.88">s(t n ) = 1 ∀t∈T s(t) ∀tn,m∈T F n,m sigm(d n,m ) ,<label>(1)</label></formula><p>where t n,m are each of the appearances of the term t n in the document T , F n,m is a factor depending on the DOM (e.g. title, alt, etc.) similar to what is done in the work of La Cascia et al. <ref type="bibr" coords="6,273.32,376.87,9.96,8.74" target="#b7">[8]</ref>, and d n,m is the word distance from t n,m to the image. The sigmoid function was centered at 35, had a slope of 0.15 and minimum and maximum values of 1 and 10 respectively. The resulting features include for each image at most the 100 word-score pairs with the highest scores.</p><p>Visual Features: Before visual feature extraction, images were filtered and resized so that the width and height had at most 240 pixels while preserving the original aspect ratio. These raw resized images were provided to the participants but also eight types of precomputed visual features. The first feature set Colorhist consisted of 576-dimensional color histograms extracted using our own implementation. These features correspond to dividing the image in 3×3 regions and for each region obtaining a color histogram quantified to 6 bits. The second feature set GETLF contained 256-dimensional histogram based features. First, local color-histograms were extracted in a dense grid every 21 pixels for windows of size 41 × 41. Then, these local color-histograms were randomly projected to a binary space using 8 random vectors and considering the sign of the resulting projection to produce the bit. Thus, obtaining an 8-bit representation of each local color-histogram that can be considered as a word. Finally, the image is represented as a bag-of-words, leading to a 256-dimensional histogram representation. The third set of features consisted of GIST <ref type="bibr" coords="6,359.32,608.30,15.50,8.74" target="#b12">[13]</ref> descriptors. The following four feature types were obtained using the colorDescriptors software <ref type="bibr" coords="6,462.33,620.25,14.61,8.74" target="#b16">[17]</ref>, namely SIFT, C-SIFT, RGB-SIFT and OPPONENT-SIFT. The configuration was dense sampling with default parameters and a hard assignment 1,000 codebook using a spatial pyramid of 1×1 and 2×2 <ref type="bibr" coords="6,334.13,656.12,9.96,8.74" target="#b8">[9]</ref>. Since the vectors of the spatial pyramid were concatenated, this resulted in 5,000-dimensional feature vectors. The codebooks were generated using 1.25 million randomly selected features and the k-means algorithm. And finally, CNN feature vectors have been provided computed as the seventh layer feature representations extracted from a deep CNN model pre-trained with the ImageNet dataset <ref type="bibr" coords="7,382.25,166.81,15.50,8.74" target="#b14">[15]</ref> using the Berkeley Caffe library<ref type="foot" coords="7,189.62,177.20,3.97,6.12" target="#foot_3">5</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Performance Measures</head><p>Subtask 1 Ultimately the goal of an image annotation system is to make decisions about which concepts to assign and localize to a given image from a predefined list of concepts. Thus to measure annotation performance, what should be considered is how good and accurate are those decisions the precision of a system. Ideally a recall measure would also be used to penalize a system that has additional false positive output. However given difficulties and unreliability of with the hand labeling of the concepts for the test images it wasn't possible to guarantee all concepts were labeled, however, it was assumed that the labels present were accurate and of a high quality.</p><p>The annotation and localization of Subtask 1 were evaluated using the PAS-CAL VOC <ref type="bibr" coords="7,184.44,351.45,10.52,8.74" target="#b3">[4]</ref> style metric of intersection over union (IoU), IoU is defined as</p><formula xml:id="formula_1" coords="7,261.28,372.42,219.31,23.22">IoU = BB f g ∩ BB gt BB f g ∪ BB gt<label>(2)</label></formula><p>where BB is a rectangle bounding box, f g is a foreground proposed annotation label, gt is the ground truth label of the concept. It calculates the area of intersection between the foreground in the proposed output localization and the ground-truth bounding box localization, divided by the area of their union.</p><p>IoU is superior to a more naive measure of the percentage of correctly labelled pixels as IoU is automatically normalized by the size of the object and penalizes segmentation's that include the background. This means that small changes in the percentage of correctly labelled pixels can correspond to large differences in IoU, and as the data-set has a wide variation in object size, the performance increases from our approach are more reliably measured. The evaluation of the ground truth and proposed output overlap was recorded from 0% to 90%. At 0%, this is equivalent to an image level annotation output, and 50% is the standard PASCAL VOC style metric used. The localized IoU is then used to compute the mean average precision (MAP) of each concept independently. This is then reported both per concept and averaged over all concepts.</p><p>Subtask 2 Subtask 2 was evaluated using the METEOR evaluation metric <ref type="bibr" coords="7,467.31,602.41,9.96,8.74" target="#b1">[2]</ref>, which is an F -measure of word overlaps taking into account stemmed words, synonyms, and paraphrases, with a fragmentation penalty to penalize gaps and word order differences. This measure was chosen as it was shown to correlate well with human judgments in evaluating image descriptions <ref type="bibr" coords="8,401.62,118.99,9.96,8.74" target="#b2">[3]</ref>. Please refer to Denkowski and Lavie <ref type="bibr" coords="8,230.58,130.95,10.52,8.74" target="#b1">[2]</ref> for details about this measure.</p><p>Subtask 3 Subtask 3 was also evaluated using the METEOR evaluation metric (see above). In addition, we have pioneered a fine-grained metric to evaluate the content selection capabilities of the sentence generation system. The content selection metric is the F 1 score averaged across all 450 test images, where each F 1 score is computed from the precision and recall averaged over all gold standard descriptions for the image. Intuitively, this measure evaluates how well the sentence generation system selects the correct concepts to be described against gold standard image descriptions. Formally, let</p><formula xml:id="formula_2" coords="8,134.77,243.81,345.82,22.77">I = {I 1 , I 2 , ...I N } be the set of test images. Let G Ii = {G Ii 1 , G Ii 2 , .</formula><p>.., G Ii M } be the set of gold standard descriptions for image I i , where each G Ii m represents the set of unique bounding box instances referenced in gold standard description m of image I i . Let S Ii be the set of unique bounding box instances referenced by the participant's generated sentence for image I i . The precision P Ii for test image I i is computed as:</p><formula xml:id="formula_3" coords="8,254.00,323.40,226.60,30.03">P Ii = 1 M M m |G Ii m ∩ S Ii | |S Ii |<label>(3)</label></formula><p>where |G Ii m ∩ S Ii | is the number of unique bounding box instances referenced in both the gold standard description and the generated sentence, and M is the number of gold standard descriptions for image I i .</p><p>Similarly, the recall R Ii for test image I i is computed as:</p><formula xml:id="formula_4" coords="8,254.06,420.39,226.53,30.03">R Ii = 1 M M m |G Ii m ∩ S Ii | |G Ii m |<label>(4)</label></formula><p>The content selection score for image I i , F Ii , is computed as the harmonic mean of P Ii and R Ii :</p><formula xml:id="formula_5" coords="8,262.79,481.47,217.80,23.89">F Ii = 2 × P Ii × R Ii P Ii + R Ii<label>(5)</label></formula><p>The final P , R and F scores are computed as the mean P , R and F scores across all test images. The advantage of the macro-averaging process in equations ( <ref type="formula" coords="8,408.53,534.71,4.24,8.74" target="#formula_3">3</ref>) and ( <ref type="formula" coords="8,442.23,534.71,4.24,8.74" target="#formula_4">4</ref>) is that it implicitly captures the relative importance of the bounding box instances based on how frequently to which they are referred across the gold standard descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Participation</head><p>The participation was excellent, with a greater number of teams including a number of new groups. In total 14 groups took part in the task and submitted overall 122 system runs. The number of runs is nearly double the previous year. Among the 14 participating groups, 11 of them submitted a corresponding paper describing their system, thus for these there were specific details available. The following 14 teams submitted a working paper:</p><p>-SMIVA <ref type="bibr" coords="9,185.75,176.97,10.52,8.74" target="#b6">[7]</ref> The  <ref type="bibr" coords="9,199.00,428.30,15.50,8.74" target="#b19">[20]</ref> The team from Toyohashi University of Technology, Japan was represented by Md Zia Ullah and Masaki Aono.. -UAIC: <ref type="bibr" coords="9,181.59,452.25,10.52,8.74" target="#b0">[1]</ref> The team from UAIC: Faculty of Computer Science, "Alexandru Ioan Cuza" University, Romania was represented by Alexandru Calfa and Adrian Iftene. -IRIP iCC: <ref type="bibr" coords="9,199.00,488.16,15.50,8.74" target="#b27">[28]</ref> The team from Intelligent Recognition and Image Processing Lab, Beihang University, Beijing was represented by Yunhong Wang, Jiaxin Chen, Ningning Liu and Li Zhang. -LIP6: <ref type="bibr" coords="9,176.03,524.06,15.50,8.74" target="#b17">[18]</ref> The team from Sorbonne Universits, CNRS, LIP6, Paris was represented by Ludovic Dos Santos, Benjamin Piwowarski and Ludovic Denoyer.</p><p>Table <ref type="table" coords="9,177.64,558.09,4.98,8.74" target="#tab_6">6</ref> provides the main key details for a number of the top groups submission describing their system. This table serves as a summary of the systems, and also is quite illustrative for quick comparisons. For a more in-depth look of the annotation systems of each team, please refer to their corresponding paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results for Subtask 1</head><p>Subtask 1 was well received despite the additional requirement of labelling and localizing all 500,000 images. All submissions were able to provide results on all 500,000 images, indicating that all groups have developed systems that are scalable enough to annotate large amounts of images. The final results are presented in Table <ref type="table" coords="10,205.15,142.90,4.98,8.74" target="#tab_1">1</ref>  be seen that three groups have achieved over 0.50 MAP across the evaluation set with 50% overlap with the ground-truth. This seems an excellent result given the challenging nature of the images used and the wide range of concepts provided. The graph in Figure <ref type="figure" coords="10,257.41,435.24,4.98,8.74" target="#fig_2">2</ref> shows the performance of each submission for an increasing amount of overlap of the ground truth labels. The results from the groups seem encouraging and it would seem that the use of CNNs has allowed for large improvements in performance. Of the top 4 groups all use CNNs in their pipeline for the feature description.</p><p>SMIVA used a deep learning framework with additional annotated data, while IVANLPR implemented a two-stage process, initially classifying at an image level with an SVM classifier, and then applying deep learning feature classification to provide localization. While RUC trained a per concept, an ensemble of linear SVMs trained by Negative Bootstrap using CNN features as image representation. Concept localization was achieved by classifying object proposals generated by Selective Search. The approach by CEA LIST could be thought of as the baseline, they just use the CNN learnt features in a small grid based approach for localization.</p><p>Examples of the most and least successful localized concepts are shown in tables 2 and 3 respectively, together with the number of labelled occurrences of these concepts in the test data. Discussion for subtask 1 As can be observed in Table <ref type="table" coords="11,396.89,524.05,3.87,8.74" target="#tab_1">1</ref>, the performance of many submissions was high this year, even given the additional constraint of localization. In fact, the 4 teams managed to achieve over 0.5 MAP, with 50% overlap with the ground truth. This perhaps indicates that in conjunction with the improvements from the CNN's real progress is starting to be made in the image annotation.</p><p>Figure <ref type="figure" coords="11,180.20,596.34,4.98,8.74" target="#fig_2">2</ref> shows the change in performance as the requirements for intersection with the ground truth labels increases. All the approaches show a steady drop off in performance which is encouraging, illustrating that the approaches don't fail to detect a number of concepts correctly even with a high degree of accuracy. Even 90% overlap with the groundtruth the MAP for SMIVA was 0.35, which is impressive. Table <ref type="table" coords="11,224.06,656.12,4.98,8.74" target="#tab_2">2</ref> shows the most correctly localized concepts and also the of occurrences of the concept. As it is important to remember that due to imperfect annotation no recall level is calculated. This is likely to be why the concept bee is so high. However there is encouraging performance for mountain, statue, bench and suit. These are all quite varied concepts, in term of scale and percentage of the image the concept will cover. However examining Table <ref type="table" coords="12,447.94,347.17,4.98,8.74" target="#tab_3">3</ref> shows a number of concepts that should be detected and are not such as leaf and wheel. However, many in that table are quite small concepts and, therefore, harder to localize and intersect with the labelled ground truth. This could be an area to direct the challenge objectives in future years.</p><p>From a computer vision perspective, we would argue that the ImageCLEF challenge has two key differences in its dataset construction to that of the other popular data sets ImageNet <ref type="bibr" coords="12,261.54,431.59,15.50,8.74" target="#b14">[15]</ref> and MSCOCO <ref type="bibr" coords="12,349.39,431.59,14.61,8.74" target="#b11">[12]</ref>. All 3 are working on detection and classification of concepts within images. However, the ImageCLEF dataset is created from Internet web pages. This gives a key difference to the other popular datasets. The web pages are unsorted and unconstrained meaning the relationship or quality of the text and image in relation to a concept can be very variable. Therefore instead of a high-quality Flickr style photo of a car from ImageNet, the image in the ImageCLEF dataset could be a fuzzy abstract car shape in the corner of the image. This allows the ImageCLEF image annotation challenge to provide additional opportunities to test proposed approaches on. Another important difference is that in addition to the image, text data from web pages can be used to train and generate the output description of the image in a natural language form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results for Subtask 2</head><p>For subtask 2, participants were asked to generate sentence-level textual descriptions for all 500,000 training images. The systems were evaluated on a subset of 3,070 instances. Four teams participated in this pilot subtask. Table <ref type="table" coords="12,430.74,644.16,4.98,8.74" target="#tab_4">4</ref> shows the METEOR scores for subtask 3, for all submitted runs by all four participants. Three teams achieved METEOR scores of over 0.10. RUC achieved the highest METEOR score, followed by ISIA, MindLab, and UAIC. We observed a large variety of approaches used by participants to tackle this subtask. RUC used the state of the art deep learning based CNN-LSTM caption generation system, MindLab employed a joint image-text retrieval approach, and UAIC a template-based approach.</p><p>As a comparison, we estimated a human upper-bound for this subtask by evaluating one description against the other descriptions for the same image and repeating the process for all descriptions. The METEOR score for the human upper-bound is estimated to be 0.3385 (Table <ref type="table" coords="13,361.01,226.59,3.87,8.74" target="#tab_4">4</ref>). Therefore, there is clear scope for future improvement and work to improve image description generation systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results for Subtask 3</head><p>For subtask 3, participants were provided with gold standard labelled bounding box inputs for 450 test images (released one week before the submission deadline), and were asked to generate textual descriptions for each image based on the gold standard input bounding boxes. To enable evaluation using the content selection metric (Section 2.4), participants were also asked to indicate within the textual descriptions the bounding box(es) to which the relevant term(s) correspond.</p><p>Two teams participated in this subtask (both of whom also participated in subtask 2). Table <ref type="table" coords="13,212.79,393.10,4.98,8.74" target="#tab_5">5</ref> shows the content section and METEOR scores for the subtask, again for all submitted runs by the two participants. RUC performed marginally better than UAIC in terms of the F and METEOR scores. Interestingly, it can be observed that RUC's sentence generation system has higher precision P , while UAIC achieved higher recall R in general than RUC. This is possibly due to RUC's use of a deep learning based sentence generator coupled with re-ranking based on the gold standard input which yielded higher precision, while UAIC's template-based generator selected more bounding boxes to be described resulting in a higher recall. Note that the METEOR scores are generally higher in subtask 3 compared to subtask 2 as participants are provided with gold standard input concepts, as well as the subtask having a smaller test set of 450 samples.</p><p>As a baseline, we generated textual descriptions per image by selecting at most three bounding boxes from the gold standard at random (the average number of unique instance mentions per description in the development set is 2.89). These concepts terms were then connected with random words or phrases selected randomly from a predefined list of prepositions and conjunctions followed by an optional article the. Like subtask 2, we also computed a human upperbound. The results for these are shown in Table <ref type="table" coords="13,345.85,608.30,3.87,8.74" target="#tab_5">5</ref>. As observed, all participants performed significantly better than the random baseline. Compared to the human upper-bound, again much work can still be done. An interesting note is that RUC achieved a high precision P almost on par with the human upper-bound, at the expense of a lower R. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Limitations of the challenge</head><p>There are two major limitations that we have identified with the challenge this year. Very few of the groups used the provided data set and features, we found this surprising, considering the state of the art CNN features and many others were included. However, this is likely to be due to the complexity and challenge of the 500,000 web page based images. Given they were collected from the Internet with little, a large number of the images are poor representations of the concept.</p><p>In fact a number of participants annotated a large amount of their own more perfect training data, as their learning process assumes perfect or near perfect training examples, it will fail. As the number of classes increases and become more varied annotating all perfect data will become more difficult. Another shortcoming of the overall challenge is the difficulty of ensuring the ground truth has 100% of concepts labelled, thus allowing a recall measure to be used. This is especially problematic as the concepts selected include fine-grained categories such as eyes and hands that are generally small but occur frequently in the dataset. In addition, it was difficult for annotators to reach a consensus in annotating bounding boxes for less well-defined categories such as trees and field. Given the current crowd-source based hand-labelling of the ground truth, the concepts have missed annotations. Thus, in this edition a recall measure is not evaluated for subtask 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>This paper presented an overview of the ImageCLEF 2015 Scalable Concept Image Annotation task, the fourth edition of a challenge aimed at developing more scalable image annotation systems. The focus of the three subtasks available to participants had the goal to develop techniques to allow computers to reliably annotate images, localize the different concepts depicted in the images and generate a description of the scene.</p><p>The participation increased this year compared to last year with 14 teams submitting in total 122 system runs. The performance of the submitted systems was somewhat superior to last year's results for sub task 1. Especially considering the requirement to label all 500,000 images in the training/test set. This was in part probably due to the increased CNN usage as the feature representation. The clear winner of this year's subtask 1 evaluation was the SMIVA <ref type="bibr" coords="16,470.07,202.68,10.52,8.74" target="#b6">[7]</ref> team, which placed heavy emphasis on the visual aspect of annotating images and improved their overall annotation performance by branching off secondary recognition pipelines for certain highly common concepts. The participation rate for subtasks 2 and 3 is encouraging as pilot subtasks. For subtask 3, we also pioneered a concept selection metric to encourage fine-grained evaluation of image descriptions. RUC <ref type="bibr" coords="16,217.28,274.41,15.50,8.74" target="#b9">[10]</ref> led both subtasks using the state of the art CNN-LSTM caption generator, improving performance by exploiting concept detections from subtask 1. Other teams, however, varied in their approaches to the problem. The encouraging participation rate and promising results in these pilot subtasks are sufficient motivations for them to be included in future editions of the challenge.</p><p>The results of the task have been very interesting and show that useful annotation systems can be built using noisy web crawled data. Since the problem requires to cover many fronts, there is still a lot of work that can be done, so it would be interesting to continue this line of research. Papers on this topic should be published, demonstration systems based on these ideas be built and more evaluation of this sort be organized. Also, it remains to see how this can be used to complement systems that are based on clean hand labeled data and find ways to take advantage of both the supervised and unsupervised data. The network is trained with noisy web data corresponding to the concepts to detect in this taskjust using simple CNNs.</p><p>Cell based regions to localize the concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Concept List 2015</head><p>The following tables present the 251 concepts used in the ImageCLEF 2015 Scalable Concept Image Annotation task. In the electronic version of this document, each concept name is a hyperlink to the corresponding WordNet synset webpage. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,215.02,190.51,185.32,7.89;3,223.72,285.57,167.92,7.89"><head>( a )</head><label>a</label><figDesc>Images from a search query of "rainbow".(b) Images from a search query of "sun".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,155.74,306.49,300.81,7.89;3,146.50,210.90,69.17,69.17"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Example of images retrieved by a commercial image search engine.</figDesc><graphic coords="3,146.50,210.90,69.17,69.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="10,173.13,653.48,266.02,7.89;10,134.77,482.75,325.08,159.26"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Increasing Precision Overlap of submissions for sub task 1</figDesc><graphic coords="10,134.77,482.75,325.08,159.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="9,134.02,176.97,346.58,260.07"><head></head><label></label><figDesc>team from Social Media and Internet Vision Analytics Lab, the Institute for Infocomm Research, Singapore was represented by Pravin Kakar, Xiangyu Wang and Alex Yong-Sang Chia. -CEA LIST: [6] The team from CEA, LIST, Laboratory of Vision and Content Engineering, France was represented by Etienne Gadeski, Herve Le Borgne, and Adrian Popescu. -CNRS TPT: [16] The team from CNRS TELECOM ParisTech , France was represented by Hichem Sahbi. -RUC-Tencent: [10] The team from RUC-Tenecent, 1Multimedia Computing Lab, School of Information, Renmin University of China was represented by Xirong Li, Qin Jin, Shuai Liao, Junwei Liang, Xixi He, Yu-Jia Huo, Weiyu Lan, Bin Xiao, Yanxiong Lu and Jieping Xu. -REGIM [30] The team from REGIM: Research Groups on Intelligent Machines, University of Sfax, Tunisa was represented by Mohamed Zarka, Anis Ben Ammar and Adel Alimi. -Mindlab: [14] The team from INAOE in Mexico and UNAL in Colombia was represented by Luis Pellegrin, Jorge A. Vanegas, John Arevalo, Viviana Beltrán, Hugo Jair Escalante, Manuel Montes-Y-Gómez and Fabio Gonzalez. -IVANLPR: [11] The team from IVA Group, Chinese Academy of Sciences was represented by Yong Li, Jing Liu, Yuhang Wang, Bingyuan Liu, Jun Fu, Yunze Gao, Hui Wu, Hang Song, Peng Ying and Hanqing Lu..</figDesc><table /><note coords="9,134.02,428.27,60.78,8.77"><p>-KDEVIR:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,134.77,142.90,345.83,215.48"><head>Table 1 :</head><label>1</label><figDesc>in terms of mean average precision (MAP) over all images of all concepts, with both 0% overlap (i.e. no localization) and 50% overlap. It can Subtask 1 results.</figDesc><table coords="10,220.13,192.89,175.11,144.18"><row><cell>Group</cell><cell cols="2">0% Overlap 50% Overlap</cell></row><row><cell>SMIVA</cell><cell>0.79</cell><cell>0.66</cell></row><row><cell>IVANLPR</cell><cell>0.64</cell><cell>0.51</cell></row><row><cell>RUC</cell><cell>0.61</cell><cell>0.50</cell></row><row><cell>CEA</cell><cell>0.45</cell><cell>0.29</cell></row><row><cell>Kdevir</cell><cell>0.39</cell><cell>0.23</cell></row><row><cell>ISIA</cell><cell>0.25</cell><cell>0.17</cell></row><row><cell>CNRS-TPT</cell><cell>0.31</cell><cell>0.17</cell></row><row><cell>IRIP-iCC</cell><cell>0.61</cell><cell>0.12</cell></row><row><cell>UAIC</cell><cell>0.27</cell><cell>0.06</cell></row><row><cell>MLVISP6</cell><cell>0.06</cell><cell>0.02</cell></row><row><cell>REGIM</cell><cell>0.03</cell><cell>0.02</cell></row><row><cell>Lip6</cell><cell>0.04</cell><cell>0.01</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="11,172.34,321.97,270.67,142.79"><head>Table 2 :</head><label>2</label><figDesc>Successfully localized Concepts</figDesc><table coords="11,172.34,321.97,270.67,121.46"><row><cell cols="3">Concept Ave MAP across all Groups Num. Occurrences</cell></row><row><cell>Bee</cell><cell>0.64</cell><cell>5</cell></row><row><cell>Telephone</cell><cell>0.64</cell><cell>20</cell></row><row><cell>Fish</cell><cell>0.60</cell><cell>36</cell></row><row><cell>Suit</cell><cell>0.60</cell><cell>199</cell></row><row><cell>Mountain</cell><cell>0.60</cell><cell>77</cell></row><row><cell>Anchor</cell><cell>0.59</cell><cell>7</cell></row><row><cell>Bench</cell><cell>0.59</cell><cell>81</cell></row><row><cell>Fruit</cell><cell>0.58</cell><cell>17</cell></row><row><cell>Statue</cell><cell>0.58</cell><cell>84</cell></row><row><cell>Hog</cell><cell>0.54</cell><cell>24</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="12,171.16,117.75,273.04,142.78"><head>Table 3 :</head><label>3</label><figDesc>Least Successfully localized Concepts</figDesc><table coords="12,171.16,117.75,273.04,121.46"><row><cell cols="3">Concept Ave MAP across all Groups Num. Occurrences</cell></row><row><cell>Temple</cell><cell>0</cell><cell>26</cell></row><row><cell>Wheel</cell><cell>0</cell><cell>331</cell></row><row><cell>Letter</cell><cell>0</cell><cell>46</cell></row><row><cell>Apple</cell><cell>0</cell><cell>8</cell></row><row><cell>Cheese</cell><cell>0</cell><cell>1</cell></row><row><cell>Ribbon</cell><cell>0</cell><cell>45</cell></row><row><cell>Mushroom</cell><cell>0</cell><cell>6</cell></row><row><cell>leaf</cell><cell>0</cell><cell>134</cell></row><row><cell>rocket</cell><cell>0</cell><cell>9</cell></row><row><cell>Mattress</cell><cell>0</cell><cell>10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="14,134.77,197.61,345.82,377.13"><head>Table 4 :</head><label>4</label><figDesc>Results for subtask 2, showing the METEOR scores for all runs from all participants. We consider the mean METEOR score as the primary measure, but for completeness we also present the median, min and max scores.</figDesc><table coords="14,136.16,197.61,343.03,331.78"><row><cell>Group</cell><cell>Run</cell><cell></cell><cell>METEOR</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Mean ± Std</cell><cell>Median</cell><cell>Min</cell><cell>Max</cell></row><row><cell></cell><cell>1</cell><cell>0.1659 ± 0.0834</cell><cell>0.1521</cell><cell>0.0178</cell><cell>0.5737</cell></row><row><cell></cell><cell>2</cell><cell>0.1781 ± 0.0853</cell><cell>0.1634</cell><cell>0.0178</cell><cell>0.5737</cell></row><row><cell>RUC</cell><cell>3 4</cell><cell>0.1806 ± 0.0817 0.1759 ± 0.0860</cell><cell>0.1683 0.1606</cell><cell>0.0192 0.0178</cell><cell>0.5696 0.5737</cell></row><row><cell></cell><cell>5</cell><cell>0.1684 ± 0.0828</cell><cell>0.1565</cell><cell>0.0178</cell><cell>0.5696</cell></row><row><cell></cell><cell>6</cell><cell>0.1875 ± 0.0831</cell><cell>0.1744</cell><cell>0.0201</cell><cell>0.5696</cell></row><row><cell></cell><cell>1</cell><cell>0.1425 ± 0.0796</cell><cell>0.1269</cell><cell>0.0151</cell><cell>0.5423</cell></row><row><cell></cell><cell>2</cell><cell>0.1449 ± 0.0811</cell><cell>0.1295</cell><cell>0.0000</cell><cell>0.5737</cell></row><row><cell>ISIA</cell><cell>3</cell><cell>0.1644 ± 0.0842</cell><cell>0.1495</cell><cell>0.0181</cell><cell>1.0000</cell></row><row><cell></cell><cell>4</cell><cell>0.1502 ± 0.0812</cell><cell>0.1352</cell><cell>0.0000</cell><cell>0.5737</cell></row><row><cell></cell><cell>5</cell><cell>0.1687 ± 0.0852</cell><cell>0.1529</cell><cell>0.0387</cell><cell>1.0000</cell></row><row><cell></cell><cell>1</cell><cell>0.1255 ± 0.0650</cell><cell>0.1140</cell><cell>0.0194</cell><cell>0.5679</cell></row><row><cell></cell><cell>2</cell><cell>0.1143 ± 0.0552</cell><cell>0.1029</cell><cell>0.0175</cell><cell>0.4231</cell></row><row><cell></cell><cell>3</cell><cell>0.1403 ± 0.0564</cell><cell>0.1342</cell><cell>0.0256</cell><cell>0.3745</cell></row><row><cell></cell><cell>4</cell><cell>0.1230 ± 0.0531</cell><cell>0.1147</cell><cell>0.0220</cell><cell>0.5256</cell></row><row><cell>MindLab</cell><cell>5 6</cell><cell>0.1192 ± 0.0521 0.1260 ± 0.0580</cell><cell>0.1105 0.1172</cell><cell>0.0000 0.0000</cell><cell>0.4206 0.4063</cell></row><row><cell></cell><cell>7</cell><cell>0.1098 ± 0.0527</cell><cell>0.1005</cell><cell>0.0000</cell><cell>0.4185</cell></row><row><cell></cell><cell>8</cell><cell>0.1079 ± 0.0498</cell><cell>0.1004</cell><cell>0.0000</cell><cell>0.3840</cell></row><row><cell></cell><cell>9</cell><cell>0.0732 ± 0.0424</cell><cell>0.0700</cell><cell>0.0135</cell><cell>0.2569</cell></row><row><cell></cell><cell>10</cell><cell>0.1202 ± 0.0528</cell><cell>0.1123</cell><cell>0.0000</cell><cell>0.5256</cell></row><row><cell></cell><cell>1</cell><cell>0.0409 ± 0.0310</cell><cell>0.0309</cell><cell>0.0142</cell><cell>0.2954</cell></row><row><cell>UAIC</cell><cell>2 3</cell><cell>0.0389 ± 0.0286 0.0483 ± 0.0389</cell><cell>0.0309 0.0331</cell><cell>0.0142 0.0142</cell><cell>0.2423 0.2954</cell></row><row><cell></cell><cell>4</cell><cell>0.0813 ± 0.0513</cell><cell>0.0769</cell><cell>0.0142</cell><cell>0.3234</cell></row><row><cell>Human</cell><cell>-</cell><cell>0.3385 ± 0.1556</cell><cell>0.3355</cell><cell>0.0000</cell><cell>1.0000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="15,134.77,276.96,345.83,18.85"><head>Table 5 :</head><label>5</label><figDesc>Results for subtask 3, showing the content selection and the METEOR scores for all runs from all participants.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="17,129.34,224.12,541.51,177.17"><head>Table 6 :</head><label>6</label><figDesc>Key details of the best system for top performing groups that submitted a paper describing their system.</figDesc><table coords="17,129.34,237.24,541.51,164.04"><row><cell>System</cell><cell>Visual Features [Total Dim.]</cell><cell>Other Used Resources</cell><cell>Training Data Processing Highlights</cell><cell>Annotation Technique Highlights</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Uses selective search to generate object</cell></row><row><cell>SMIVA [7]</cell><cell>1024-dim GoogLeNet [1] [T.Dim. = 21312]</cell><cell>* WordNet * Bing Image Search</cell><cell>Training data created by augmenting target concept with WordNet hyponyms and lemmas, retrieving images from Bing Image Search and filtering out too small or uniform images.</cell><cell>proposals, runs classifiers on each proposal and performs non-maximal suppression. Secondary pipelines add further context/processing from faces and</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>difficult to localize concepts (e.g. trees).</cell></row><row><cell>IVANLPR</cell><cell>ImageNet CNNs</cell><cell>-</cell><cell>Annotation by classification with deep visual features and linear SVM. Annotation by search with</cell><cell>Localization by Fast RCNN for concepts with obvious object. Localization by</cell></row><row><cell>[11]</cell><cell></cell><cell></cell><cell>surrounding text.</cell><cell>search for the scene related concepts.</cell></row><row><cell>RUC-Tencent [10]</cell><cell>Caffe CNNs</cell><cell>* Flickr Images</cell><cell>Hierarchical Semantic Embedding (HierSE) for selecting positive examples, Negative Bootstrap for building concept classifiers.</cell><cell>Selective Search for generating object proposals and refinement to reduce false alarms.</cell></row><row><cell>CEA [6] LIST</cell><cell>[T.Dim. = 256] ImageNet CNNs</cell><cell>* Bing Image Search</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="3,144.73,656.80,255.80,8.12"><p>Challenge website at http://imageclef.org/2015/annotation</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="5,144.73,645.84,286.29,8.12"><p>Dataset available at http://risenet.prhlt.upv.es/webupv-datasets</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="5,144.73,657.44,84.73,7.47"><p>http://aspell.net/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="7,144.73,656.80,327.24,8.12"><p>More details can be found at https://github.com/BVLC/caffe/wiki/Model-Zoo</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The <rs type="projectName">Scalable Image Annotation</rs>, Localization and Sentence Generation task was coorganized by the <rs type="institution">ViSen consortium</rs> under the <rs type="funder">EU</rs> <rs type="programName">CHIST-ERA D2K Programme</rs>, supported by <rs type="funder">EPSRC</rs> Grants <rs type="grantNumber">EP/K01904X/1</rs> and <rs type="grantNumber">EP/K019082/1</rs>, and by <rs type="funder">French ANR</rs> Grant <rs type="grantNumber">ANR-12-CHRI-0002-04</rs>. This work was also supported by the <rs type="funder">European Science Foundation (ESF)</rs> through the research networking programme <rs type="projectName">Evaluating Information Access Systems (ELIAS</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_zgC7Uwb">
					<orgName type="project" subtype="full">Scalable Image Annotation</orgName>
					<orgName type="program" subtype="full">CHIST-ERA D2K Programme</orgName>
				</org>
				<org type="funding" xml:id="_fKeFt4F">
					<idno type="grant-number">EP/K01904X/1</idno>
				</org>
				<org type="funding" xml:id="_gXTGJwF">
					<idno type="grant-number">EP/K019082/1</idno>
				</org>
				<org type="funding" xml:id="_pEHJ2tE">
					<idno type="grant-number">ANR-12-CHRI-0002-04</idno>
				</org>
				<org type="funded-project" xml:id="_hyRCyJS">
					<orgName type="project" subtype="full">Evaluating Information Access Systems (ELIAS</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="18,142.96,248.10,337.64,7.86;18,151.52,259.06,329.07,7.86;18,151.52,270.01,88.64,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="18,235.50,248.10,245.10,7.86;18,151.52,259.06,102.32,7.86">Using Textual and Visual Processing in Scalable Concept Image Annotation Challenge</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Calfa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Iftene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,274.46,259.06,206.14,7.86;18,151.52,270.01,59.97,7.86">CLEF 2015 Evaluation Labs and Workshop, Online Working Notes</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.96,280.20,337.64,7.86;18,151.52,291.16,329.07,7.86;18,151.52,302.12,154.99,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="18,261.01,280.20,219.59,7.86;18,151.52,291.16,119.67,7.86">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,294.44,291.16,186.15,7.86;18,151.52,302.12,126.33,7.86">Proceedings of the EACL 2014 Workshop on Statistical Machine Translation</title>
		<meeting>the EACL 2014 Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.96,312.30,337.63,7.86;18,151.52,323.26,329.07,7.86;18,151.52,334.22,329.07,7.86;18,151.52,345.18,329.07,8.12;18,151.52,356.78,85.23,7.47" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="18,238.65,312.30,241.94,7.86;18,151.52,323.26,14.75,7.86">Comparing automatic evaluation measures for image description</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P14-2074" />
	</analytic>
	<monogr>
		<title level="m" coord="18,186.34,323.26,294.26,7.86;18,151.52,334.22,69.20,7.86">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06">June 2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="452" to="457" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct coords="18,142.96,366.32,337.64,7.86;18,151.52,377.28,329.07,7.86;18,151.52,388.24,221.01,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="18,189.45,377.28,232.06,7.86">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="18,428.35,377.28,52.24,7.86;18,151.52,388.24,112.86,7.86">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015-01">Jan 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.96,398.43,337.64,7.86;18,151.52,409.39,150.15,7.86" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="18,234.71,398.43,170.44,7.86">WordNet An Electronic Lexical Database</title>
		<editor>Fellbaum, C.</editor>
		<imprint>
			<date type="published" when="1998-05">May 1998</date>
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge, MA; London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.96,419.57,337.64,7.86;18,151.52,430.53,329.07,7.86;18,151.52,441.49,204.35,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="18,313.84,419.57,166.75,7.86;18,151.52,430.53,213.75,7.86">CEA LIST&apos;s participation to the Scalable Concept Image Annotation task of ImageCLEF 2015</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Gadeski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">L</forename><surname>Borgne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,387.27,430.53,93.32,7.86;18,151.52,441.49,175.67,7.86">CLEF 2015 Evaluation Labs and Workshop, Online Working Notes</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.96,451.67,337.63,7.86;18,151.52,462.63,329.07,7.86;18,151.52,473.59,164.23,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="18,292.77,451.67,187.82,7.86;18,151.52,462.63,178.63,7.86">SMIVA at ImageCLEF 2015: Automatic Image Annotation using Weakly Labelled Web Data</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Kakar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Y S</forename><surname>Chia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,350.46,462.63,130.13,7.86;18,151.52,473.59,38.06,7.86">CLEF 2015 Evaluation Labs and Workshop</title>
		<title level="s" coord="18,197.41,473.59,89.67,7.86">Online Working Notes</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.96,483.78,337.64,7.86;18,151.52,494.74,329.07,7.86;18,151.52,505.69,329.07,7.86;18,151.52,516.65,116.86,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="18,295.97,483.78,184.63,7.86;18,151.52,494.74,181.88,7.86">Combining textual and visual cues for contentbased image retrieval on the World Wide Web</title>
		<author>
			<persName coords=""><forename type="first">La</forename><surname>Cascia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename></persName>
		</author>
		<idno type="DOI">10.1109/IVL.1998.694480</idno>
	</analytic>
	<monogr>
		<title level="m" coord="18,354.94,494.74,125.65,7.86;18,151.52,505.69,80.94,7.86;18,265.77,505.69,121.54,7.86">Content-Based Access of Image and Video Libraries</title>
		<imprint>
			<date type="published" when="1998">1998. 1998</date>
			<biblScope unit="page" from="24" to="28" />
		</imprint>
	</monogr>
	<note>Proceedings. IEEE Workshop</note>
</biblStruct>

<biblStruct coords="18,142.96,526.84,337.64,7.86;18,151.52,537.80,329.07,7.86;18,151.52,548.76,329.07,7.86;18,151.52,559.72,329.07,7.86;18,151.52,570.67,162.29,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="18,304.47,526.84,176.13,7.86;18,151.52,537.80,209.26,7.86">Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2006.68</idno>
	</analytic>
	<monogr>
		<title level="m" coord="18,383.06,537.80,97.53,7.86;18,151.52,548.76,329.07,7.86;18,262.90,559.72,40.20,7.86">Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
	<note>CVPR &apos;06</note>
</biblStruct>

<biblStruct coords="18,142.62,580.86,337.97,7.86;18,151.52,591.82,329.07,7.86;18,151.52,602.78,329.07,7.86;18,151.52,613.74,88.64,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="18,199.94,591.82,280.65,7.86;18,151.52,602.78,99.03,7.86">RUC-Tencent at ImageCLEF 2015: Concept Detection, Localization and Sentence Generation</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">J</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,272.06,602.78,208.54,7.86;18,151.52,613.74,59.97,7.86">CLEF 2015 Evaluation Labs and Workshop, Online Working Notes</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.62,623.92,337.97,7.86;18,151.52,634.88,329.07,7.86;18,151.52,645.84,329.07,7.86;18,151.52,656.80,51.50,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="18,199.40,634.88,281.19,7.86;18,151.52,645.84,65.23,7.86">Hybrid Learning Framework for Large-Scale Web Image Annotation and Localization</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Bingyuan Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,237.25,645.84,243.35,7.86;18,151.52,656.80,22.84,7.86">CLEF 2015 Evaluation Labs and Workshop, Online Working Notes</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.62,119.67,337.98,7.86;19,151.52,130.63,329.07,7.86;19,151.52,141.59,172.45,8.11" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="19,208.04,130.63,180.59,7.86">Microsoft COCO: common objects in context</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>CoRR abs/1405.0312</idno>
		<ptr target="http://arxiv.org/abs/1405.0312" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.62,153.48,337.97,7.86;19,151.52,164.43,329.07,7.86;19,151.52,175.39,119.03,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="19,249.62,153.48,230.97,7.86;19,151.52,164.43,112.89,7.86">Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1011139631724</idno>
	</analytic>
	<monogr>
		<title level="j" coord="19,272.24,164.43,92.79,7.86">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001-05">May 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.62,187.28,337.98,7.86;19,151.52,198.24,329.07,7.86;19,151.52,209.19,329.07,7.86;19,151.52,220.15,51.50,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="19,259.24,198.24,221.35,7.86;19,151.52,209.19,70.44,7.86">INAOE-UNAL at ImageCLEF 2015: Scalable Concept Image Annotation</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Pellegrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Vanegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Arevalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Beltrán</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">J</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes-Y-Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,241.64,209.19,238.95,7.86;19,151.52,220.15,22.84,7.86">CLEF 2015 Evaluation Labs and Workshop, Online Working Notes</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.62,232.04,337.97,7.86;19,151.52,243.00,329.07,7.86;19,151.52,253.95,329.07,7.86;19,151.52,264.91,118.66,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="19,415.77,243.00,64.82,7.86;19,151.52,253.95,145.50,7.86">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="19,305.92,253.95,174.68,7.86;19,151.52,264.91,24.00,7.86">International Journal of Computer Vision (IJCV</title>
		<imprint>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2015-04">April 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.62,276.80,337.98,7.86;19,151.52,287.76,329.07,7.86;19,151.52,298.72,300.08,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="19,196.93,276.80,283.66,7.86;19,151.52,287.76,311.25,7.86">CNRS TELECOM ParisTech at ImageCLEF 2015 Scalable Concept Image Annotation Task: Concept Detection with Blind Localization Proposals</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sahbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,151.52,298.72,271.41,7.86">CLEF 2015 Evaluation Labs and Workshop, Online Working Notes</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.62,310.60,337.97,7.86;19,151.52,321.56,329.07,7.86;19,151.52,332.52,257.14,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="19,332.92,310.60,147.67,7.86;19,151.52,321.56,107.30,7.86">Evaluating Color Descriptors for Object and Scene Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2009.154</idno>
	</analytic>
	<monogr>
		<title level="j" coord="19,266.09,321.56,214.50,7.86;19,151.52,332.52,45.57,7.86">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1582" to="1596" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.62,344.40,337.98,7.86;19,151.52,355.36,329.07,7.86;19,151.52,366.32,164.23,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="19,324.78,344.40,155.81,7.86;19,151.52,355.36,174.73,7.86">Graph Based Method Approach to the ImageCLEF2015 Task1 -Image Annotation</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Piwowarski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,347.81,355.36,132.78,7.86;19,151.52,366.32,135.56,7.86">CLEF 2015 Evaluation Labs and Workshop, Online Working Notes</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.62,378.20,337.97,7.86;19,151.52,389.16,329.07,7.86;19,151.52,400.12,329.07,7.86;19,151.52,411.08,117.87,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="19,331.73,378.20,148.86,7.86;19,151.52,389.16,250.24,7.86">80 Million Tiny Images: A Large Data Set for Nonparametric Object and Scene Recognition</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2008.128</idno>
	</analytic>
	<monogr>
		<title level="j" coord="19,411.02,389.16,69.58,7.86;19,151.52,400.12,200.39,7.86">Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1958" to="1970" />
			<date type="published" when="2008-11">nov 2008</date>
		</imprint>
	</monogr>
	<note>IEEE Transactions on</note>
</biblStruct>

<biblStruct coords="19,142.62,422.96,337.98,7.86;19,151.52,433.92,329.07,7.86;19,151.52,444.88,329.07,7.86;19,151.52,455.84,25.60,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="19,245.78,422.96,234.81,7.86;19,151.52,433.92,329.07,7.86;19,151.52,444.88,43.98,7.86">KDEVIR at ImageCLEF 2015 Scalable Image Annotation, Localization, and Sentence Generation task: Ontology based Multi-label Image Annotation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">Z</forename><surname>Ullah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Aono</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,215.48,444.88,265.11,7.86">CLEF 2015 Evaluation Labs and Workshop, Online Working Notes</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.62,467.72,337.97,7.86;19,151.52,478.68,329.07,7.86;19,151.52,489.64,329.07,7.86;19,151.52,500.60,329.07,7.86;19,151.52,511.56,126.65,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="19,398.63,489.64,81.96,7.86;19,151.52,500.60,142.90,7.86">General Overview of ImageCLEF at the CLEF 2015 Labs</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bromuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">K</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Uskudarli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">B</forename><surname>Marvasti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Aldana</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Del Mar Roldán</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>García</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="19,301.24,500.60,139.06,7.86">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.62,523.44,337.98,7.86;19,151.52,534.40,329.07,7.86;19,151.52,545.36,329.07,7.86;19,151.52,556.32,166.02,7.86" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="19,259.48,523.44,221.11,7.86;19,151.52,534.40,53.91,7.86">Image-Text Dataset Generation for Image Annotation and Retrieval</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,355.91,534.40,124.68,7.86;19,151.52,545.36,145.24,7.86">II Congreso Español de Recuperación de Información, CERI 2012</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Berlanga</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Rosso</surname></persName>
		</editor>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">June 18-19 2012</date>
			<biblScope unit="page" from="115" to="120" />
		</imprint>
		<respStmt>
			<orgName>Universidad Politécnica de Valencia</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.62,568.20,337.97,7.86;19,151.52,579.16,329.07,7.86;19,151.52,590.12,329.08,7.86;19,151.52,601.08,329.07,8.12;19,151.52,612.68,108.76,7.47" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="19,261.43,568.20,219.16,7.86;19,151.52,579.16,86.92,7.86">Overview of the ImageCLEF 2012 Scalable Web Image Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<ptr target="http://mvillegas.info/pub/Villegas12_CLEF_Annotation-Overview.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="19,151.52,590.12,293.11,7.86">CLEF 2012 Evaluation Labs and Workshop, Online Working Notes</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Karlgren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Womser-Hacker</surname></persName>
		</editor>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">September 17-20 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.62,623.92,337.98,7.86;19,151.52,634.88,329.07,7.86;19,151.52,645.84,329.07,7.86;19,151.52,656.80,326.10,8.12" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="19,262.58,623.92,218.02,7.86;19,151.52,634.88,96.12,7.86">Overview of the ImageCLEF 2014 Scalable Concept Image Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-1180/CLEF2014wn-Image-VillegasEt2014.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="19,271.70,634.88,107.11,7.86">CLEF2014 Working Notes</title>
		<title level="s" coord="19,387.41,634.88,93.18,7.86;19,151.52,645.84,31.91,7.86">CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<publisher>CEUR-WS.org</publisher>
			<date type="published" when="2014">September 15-18 2014</date>
			<biblScope unit="volume">1180</biblScope>
			<biblScope unit="page" from="308" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,142.62,119.67,337.97,7.86;20,151.52,130.63,329.07,7.86;20,151.52,141.59,329.07,8.11;20,151.52,153.20,282.93,7.47" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="20,315.57,119.67,165.02,7.86;20,151.52,130.63,167.36,7.86">Overview of the ImageCLEF 2013 Scalable Concept Image Annotation Subtask</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<ptr target="http://mvillegas.info/pub/Villegas13_CLEF_Annotation-Overview.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="20,342.84,130.63,137.75,7.86;20,151.52,141.59,133.20,7.86">CLEF 2013 Evaluation Labs and Workshop, Online Working Notes</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">September 23-26 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,142.62,163.51,337.97,7.86;20,151.52,174.47,329.07,7.86;20,151.52,185.43,329.07,7.86;20,151.52,196.39,329.07,7.86;20,151.52,207.34,283.57,8.12" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="20,351.55,163.51,129.04,7.86;20,151.52,174.47,329.07,7.86;20,151.52,185.43,19.99,7.86">A poodle or a dog? Evaluating automatic image annotation using human descriptions at different levels of granularity</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W14-5406" />
	</analytic>
	<monogr>
		<title level="m" coord="20,191.46,185.43,240.56,7.86">Proceedings of the Third Workshop on Vision and Language</title>
		<meeting>the Third Workshop on Vision and Language<address><addrLine>Dublin City; Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-08">August 2014</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
		<respStmt>
			<orgName>University and the Association for Computational Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="20,142.62,218.30,337.97,7.86;20,151.52,229.26,329.07,7.86;20,151.52,240.22,329.07,7.86;20,151.52,251.18,132.09,7.86" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="20,378.66,218.30,101.93,7.86;20,151.52,229.26,173.33,7.86">ARISTA -image search to annotation on billions of web photos</title>
		<author>
			<persName coords=""><forename type="first">X</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2010.5540046</idno>
	</analytic>
	<monogr>
		<title level="m" coord="20,351.73,229.26,128.86,7.86;20,151.52,240.22,84.05,7.86">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010-06">2010. June 2010</date>
			<biblScope unit="page" from="2987" to="2994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,142.62,262.14,337.98,7.86;20,151.52,273.10,329.07,7.86;20,151.52,284.06,164.23,7.86" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="20,322.57,262.14,158.02,7.86;20,151.52,273.10,171.91,7.86">BUAA-iCC at ImageCLEF 2015 Scalable Concept Image Annotation Challenge</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,345.79,273.10,134.80,7.86;20,151.52,284.06,135.56,7.86">CLEF 2015 Evaluation Labs and Workshop, Online Working Notes</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,142.62,295.02,337.97,7.86;20,151.52,305.98,329.07,7.86;20,151.52,316.93,122.41,7.86" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="20,306.46,295.02,174.12,7.86;20,151.52,305.98,166.64,7.86">Large scale image annotation: learning to rank with joint word-image embeddings</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10994-010-5198-3</idno>
	</analytic>
	<monogr>
		<title level="j" coord="20,327.82,305.98,74.40,7.86">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="21" to="35" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,142.62,327.89,337.98,7.86;20,151.52,338.85,329.07,7.86;20,151.52,349.81,300.08,7.86" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="20,300.77,327.89,179.83,7.86;20,151.52,338.85,310.42,7.86">Regimvid at ImageCLEF 2015 Scalable Concept Image Annotation Task: Ontology based Hierarchical Image Annotation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zarka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Alimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,151.52,349.81,271.41,7.86">CLEF 2015 Evaluation Labs and Workshop, Online Working Notes</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
