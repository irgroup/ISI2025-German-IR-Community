<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,170.36,116.95,274.64,12.62;1,153.91,134.89,307.53,12.62">Graph Based Method Approach to the ImageCLEF2015 Task1 -Image Annotation</title>
				<funder ref="#_Kuf2g6B">
					<orgName type="full">Research Agency ANR (Agence Nationale de la Recherche)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,165.34,174.00,54.30,8.74"><forename type="first">Dos</forename><surname>Ludovic</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sorbonne Universits</orgName>
								<orgName type="laboratory">LIP6 UMR 7606</orgName>
								<orgName type="institution" key="instit1">UPMC Univ Paris 06</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<addrLine>4 place Jussieu</addrLine>
									<postCode>75005</postCode>
									<settlement>Paris</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,222.96,174.00,76.29,8.74"><forename type="first">Benjamin</forename><surname>Santos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sorbonne Universits</orgName>
								<orgName type="laboratory">LIP6 UMR 7606</orgName>
								<orgName type="institution" key="instit1">UPMC Univ Paris 06</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<addrLine>4 place Jussieu</addrLine>
									<postCode>75005</postCode>
									<settlement>Paris</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,302.58,174.00,46.99,8.74;1,376.97,174.00,31.43,8.74"><forename type="first">Patrick</forename><surname>Piwowarski</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sorbonne Universits</orgName>
								<orgName type="laboratory">LIP6 UMR 7606</orgName>
								<orgName type="institution" key="instit1">UPMC Univ Paris 06</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<addrLine>4 place Jussieu</addrLine>
									<postCode>75005</postCode>
									<settlement>Paris</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,411.73,174.00,38.29,8.74"><surname>Gallinari</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sorbonne Universits</orgName>
								<orgName type="laboratory">LIP6 UMR 7606</orgName>
								<orgName type="institution" key="instit1">UPMC Univ Paris 06</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<addrLine>4 place Jussieu</addrLine>
									<postCode>75005</postCode>
									<settlement>Paris</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,170.36,116.95,274.64,12.62;1,153.91,134.89,307.53,12.62">Graph Based Method Approach to the ImageCLEF2015 Task1 -Image Annotation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A992DBDF87391AD037C513123CCFFEBE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Representation Learning</term>
					<term>Graph based method</term>
					<term>Deep Relations</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address the tasks of image classification and tagging through a transductive approach that automatically learns to project the different images onto a common latent space. This learned representation is then used to classify the images. We construst a graph between images and concepts using the deep representations given by ImageCLEF and WordNet database, and we exploit the idea that two connected nodes will tend to have a similar latent representation. This assumption allows us to learn correlations between the labels of connected images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ImageCLEF 2015 <ref type="bibr" coords="1,231.38,450.98,13.05,8.74" target="#b2">[3]</ref> image annotation <ref type="bibr" coords="1,330.52,450.98,10.52,8.74" target="#b1">[2]</ref> is composed of two subtasks : the Image Concept detection and localisation, and the Generation of Textual Descriptions of Images. We have participated in the former, whose objective is to predict which concepts are present in an image given in input. Use of labeled data is allowed, and we therefore used the available ImageNET Convolutional Neural Network (CNN).</p><p>The system we propose is based on a model [?] that projects nodes of a graph within a vector space (typically, of dimension 100-200). This model requires that some nodes be labeled, and uses this information within the embedding process. To leverage ImageNet data, where no relationship links images together, we had to build a graph. We experimented with different ways to construct a simple graph based on the output of the ImageNET CNN representations and the WordNet database <ref type="bibr" coords="1,228.83,595.89,12.88,8.74" target="#b0">[1]</ref>, and presents results on the CLEF task corresponding to the settings of different hyperparameters of the model.</p><p>The article is organized as follows: Section 2 presents an overview of our model and how we construct the graph based on the images and the WordNet database, Section 3 the representation and the classifier learning, Section 4 our algorithm, and Section 5 outlines our results and conclusions.</p><p>We propose a solution that relies on the exploitation of the correlations that exists between images that have a similar 1000 dimensional activations of the fc8 layer of Oxford VGGs 16-layer Convolutional Neural Network model (OV-CNN) given in the ImageCLEF challenge. It aims at taking directly into account the correlations between the CNN-labels of connected images. The underlying idea of this model is the following:</p><p>-We construct a graph mixing images and concepts based on the CNNrepresentation and the WordNet database. -Each node is mapped onto a latent representation in a vectorial space R Z .</p><p>The latent space is common to all node types (images and concepts). -This latent representation defines a metric in R Z such that two connected nodes tend to have a close representation (smoothness assumption). -All the nodes and relations are not equal w.r.t. their position in the graph, their number of neighbors, etc. We use two functions, ψ i and φ i,j , to model the importance of nodes and the relationships in the embedding process. -A classification function is learned for the images. It takes as input a latent image representation and computes associated class labels.</p><p>All the notation used in this working note are summarized in Table <ref type="table" coords="2,447.73,362.26,3.87,8.74" target="#tab_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Gaph construction</head><p>To apply our method, we need to construct a graph. The resulting graph is composed of nodes of two types, images and (WordNet) concepts. To construct the graph, we connect images to a subset of the concepts and then enrich this graph using Wordnet relationships between concepts.</p><p>We first directly use the representation A k of an image, i.e. the 1000 dimensional activations of the OV-CNN. Each dimension of this vector corresponds to a label, and we suppose that an image is labeled by the i th label if its corresponding component is above a threshold µ ∈ R. In that case, each label being a precise WordNet concept, we connect the image to the corresponding concept in the graph.</p><p>We then use WordNet to connect concepts together. We selected two relationships that may be important when trying to label images: the hypernymhyponym relationship ("is-a") and the meronymy relationship ("part of"). In order to keep the graph size manageable, we started from the concepts of the CNN representation and added all the concepts that would be necessary to link the different concepts together. For example, in the figure below, we have four images. Two are classified "bomber" by the VO-CNN two "jet plane". Both concepts are hyponyms of airplane. We thus have the corresponding graph : The final graph has 25,730,835 image-concept edges and 4,309 concept-concept edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Training set</head><p>The initial training set for subtask 1 contains 1,979 labeled images (there are 500,000 images in the entire dataset) for 250 concepts. That give us 7.9 images per concept on average, which is a relatively small number. To increase the number of training examples, we again used the A k representation and the corresponding OV-CNN concepts. We used another threshold ν ∈ R to determine when an image should be labeled with a given OV-CNN concept.</p><p>In practice, given an image k, if the i-th component of A k is bigger than ν we check if the corresponding OV-CNN concept is an hypernym of an ImageCLEF concept C clef . If then, we add the image k to the training set labeled with C clef .</p><p>When ν = 20, the labeled images in our training set contains 43.970 images (175.9 images per concept), and when ν = 10, 446.130 images (1784.5 images per concept).</p><p>3 Learning latent node representations</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Classifier</head><p>The mapping onto the latent space is learned so that the labels for each type of node can be predicted from the latent representations. For that, we consider a linear classification function for each type of node k denoted by f k θ . This function takes as input a node representation and outputs the predicted label(s).</p><p>The f -functions can be learned by minimizing a loss on labeled data as follows:</p><formula xml:id="formula_0" coords="4,267.00,349.23,213.59,22.30">i=1 ψ i ∆(f ti θ (z i ), y i )<label>(1)</label></formula><p>where ∆(f ti θ (z x i ), y i ) is the loss of predicting labels f ti θ (z i ) instead of observed labels y i . Here ψ i represents the relative importance of node i in the graph. For example, ψ i = 1/N would define a model where all nodes have the same importance.</p><p>In our case, in order to set the value of ψ i , we think our model should consider that all nodes have not the same importance, so we should be able to emphasis on more central nodes with regard to different relations. To do so, we use what makes a node more important is it has many neighbors of different types :</p><formula xml:id="formula_1" coords="4,140.85,485.16,71.45,16.28">ψ i = 1 R R r=1 N r i E r</formula><p>In our implementation, we use a hinge-loss function for ∆:</p><formula xml:id="formula_2" coords="4,218.20,534.84,262.39,32.20">∆(f t θ (z), y) = C t k=1 max(0; 1 -y k f t,k θ (z)) (2)</formula><p>where y k is the desired score of category k for node x (-1 or +1) and f t,k θ (z) is the predicted score of category k by the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Transductive classification model</head><p>Let us denote by z i ∈ R Z the hidden representation of node x i which is a vector of size Z. When updating z i , in order to capture the graph metric in the latent space, we use the following loss which embodies the metric smoothness assumption and forces linked nodes to have similar representations:</p><formula xml:id="formula_3" coords="5,275.09,153.78,205.50,21.98">j φ i,j ||z i -z j || 2<label>(3)</label></formula><p>We are using an L 2 norm in the latent space, but other metrics could be used as well. Furthermore, φ i,j aims at considering the importance of an edge w.r.t. the hole graph and the shape of it around this specific edge.</p><p>We assume that all the edges of the graph should have the same impact regardless of their predominance (φ i,j ∝ 1 E r(i,j) ). Then, as in the PageRank algorithm, we assume that the information coming from a neighbor j of i should be divided equally through his neighbors of the same type as i (φ i,j ∝ 1 N r(i,j) j</p><p>).</p><p>Thus we set φ i,j = 1 RN r(i,j) j E r(i,j)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Loss Function</head><p>The final expected objective loss of our model combines the classification and regularization losses 1 and 3:</p><formula xml:id="formula_4" coords="5,197.36,384.85,283.23,22.30">L(z, θ) = i=1 ψ i ∆(f ti θ (z i ), y i ) + λ i,j φ i,j ||z i -z j || 2<label>(4)</label></formula><p>The minimization of this function aims at finding a trade-off between the smoothness over the latent representations of correlated nodes Z and the predicted observed labels in Y k . Optimizing this loss allows us to learn:</p><p>-The projection z i of each node x i in the latent space.</p><p>-The classification functions f k θ for each nodes type k which transform the latent space to categories scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Algorithm</head><p>Learning consists in minimizing the loss function defined in Equation 4. Different optimization methods can be considered. We have used a Stochastic Gradient Descent Method to learn the latent representations.</p><p>Furthermore, we can remark that the φ i,j and ψ i can also be understood as a specific way of sampling during the SGD. Actually, if we firstly choose uniformly the type of edge then choose uniformly an edge given that type and pick the nodes from the chosen edge we update a given representation z i in expectation As our model is not able to localize a concept in a given image we only present the results of the mean average precision with zero overlap (MAP-0). As we can see, the bigger the training set is (ν is small) the better our model performs. As the graph has many edges our model needs a bigger dimension (N ) in order to adapt to the graph topology. Furthermore we can see that the more we constrain (λ is big) the representations of two nodes to be close, the worst the performances get.</p><p>The results are quite low compared to other opproaches, but we did not exploit (at least directly) any image related feature. Compared to a random approach, we are still able to extract some information from a graph constructed using some basic information about the classification of an image and the relationship between the labels/concepts.</p><p>Our methodology would benefit from a real graph linking the images (e.g. a social network), and results would be much improved if using image-based features.</p><p>In future work, we will combine this approach with image features to check whether they can bring some further information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,190.82,619.37,3.97,6.12;5,189.79,626.73,6.00,6.12;5,209.20,618.28,6.00,6.12;5,209.20,626.28,14.00,6.12;5,226.56,616.56,10.40,7.76;5,232.87,621.56,2.66,4.37;5,226.97,626.09,9.58,6.75;5,242.29,621.25,157.84,8.74;5,405.83,619.37,3.97,6.12;5,404.79,626.73,6.00,6.12;5,424.20,618.28,6.00,6.12;5,424.20,626.28,14.00,6.12;5,441.56,616.56,10.40,7.76;5,447.87,621.56,2.66,4.37;5,441.97,626.09,9.58,6.75;5,457.29,621.25,23.30,8.74;5,134.77,633.20,345.83,8.74;5,134.77,645.16,345.83,8.74;5,134.77,657.11,27.67,8.74"><head></head><label></label><figDesc>for the graph regularization one. Adding the PageRank division in the graph regularization, we find out our previous assumptions. The algorithm is detailed below.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,256.97,460.77,25.08,7.86;6,328.98,460.77,61.33,7.86;6,221.44,472.13,96.14,7.86;6,342.24,472.13,34.81,7.86;6,221.44,483.49,96.14,7.86;6,342.24,483.49,34.81,7.86;6,219.14,494.85,100.74,7.86;6,344.54,494.85,30.21,7.86;6,219.14,506.20,100.74,7.86;6,342.24,506.20,34.81,7.86;6,219.14,517.56,100.74,7.86;6,342.24,517.56,34.81,7.86;6,221.44,528.92,96.14,7.86;6,342.24,528.92,34.81,7.86"><head>Model MAP 0</head><label>0</label><figDesc>overlap N = 250, ν = 10, λ = 50 0.057422 N = 100, ν = 10, λ = 50 0.056822 N = 250, ν = 10, λ = 150 0.05625 N = 100, ν = 10, λ = 150 0.056141 N = 250, ν = 20, λ = 150 0.052665 N = 250, ν = 20, λ = 50 0.052585</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,154.35,395.13,306.39,237.99"><head>Table 1 .</head><label>1</label><figDesc>Notations</figDesc><table coords="2,154.35,395.13,306.39,107.92"><row><cell>Notation</cell><cell>Meaning</cell></row><row><cell>xi</cell><cell>Node of the graph</cell></row><row><cell>A k</cell><cell>1000 dimensional activations of the fc8 layer of an image k</cell></row><row><cell>zi</cell><cell>Latent representation of node xi</cell></row><row><cell>z copy i</cell><cell>Numerical copy of zi</cell></row><row><cell>T</cell><cell>Set of possible nodes types</cell></row><row><cell>T</cell><cell>Number of nodes types</cell></row><row><cell>ti</cell><cell>Type of node xi</cell></row><row><cell>r(i, j)</cell><cell>Relation type between node xi and xj</cell></row><row><cell>i</cell><cell></cell></row></table><note coords="2,162.84,492.10,3.47,5.24;2,160.07,495.19,15.54,7.86;2,224.14,495.19,118.75,7.86;2,154.35,506.15,6.97,7.86;2,224.14,506.15,103.78,7.86;2,154.35,517.11,7.37,7.86;2,224.14,517.11,68.94,7.86;2,154.35,528.07,28.42,7.86;2,224.14,528.07,182.17,7.86;2,154.35,539.02,7.37,7.86;2,162.69,537.26,3.47,5.24;2,161.72,542.73,3.06,5.24;2,224.14,539.02,212.38,7.86;2,154.35,549.98,6.78,7.86;2,224.14,549.98,89.93,7.86;2,154.35,560.94,6.78,7.86;2,161.65,559.17,3.47,5.24;2,224.14,560.94,128.39,7.86;2,154.35,571.90,6.14,7.86;2,161.24,570.13,2.85,5.24;2,224.14,571.90,196.79,7.86;2,154.35,582.86,4.87,7.86;2,159.72,581.09,2.85,5.24;2,224.14,582.86,65.29,7.86;2,290.18,581.09,2.85,5.24;2,154.35,593.82,7.20,7.86;2,224.14,593.82,142.41,7.86;2,154.35,604.78,4.54,7.86;2,159.21,603.01,3.34,5.24;2,158.89,608.49,2.66,5.24;2,224.14,604.78,140.43,7.86"><p>r → j Relation of type r from i to j R Number of relation's type N Number of nodes x1 • • • x Labeled nodes used for the classification task N r j Number of neighbors of xj considering the relation r E Total number of edges E r Total number of edges of type r Y t Set of categories associated to the type of node t C t Cardinality of Y t yi Vector of categories for the node xi y c i Value of category c for the node xi</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This work was supported in part by a grant from the <rs type="funder">Research Agency ANR (Agence Nationale de la Recherche)</rs> under the <rs type="projectName">MLVIS</rs> project.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_Kuf2g6B">
					<orgName type="project" subtype="full">MLVIS</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,138.35,300.99,342.24,7.86;7,146.91,311.95,20.99,7.86" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="7,236.65,300.99,169.56,7.86">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName coords=""><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Bradford Books</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,322.91,342.24,7.86;7,146.91,333.87,333.68,7.86;7,146.91,344.83,333.68,7.86;7,146.91,355.79,333.68,7.86;7,146.91,366.75,151.84,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,385.60,333.87,94.99,7.86;7,146.91,344.83,329.83,7.86">Overview of the Image-CLEF 2015 Scalable Image Annotation, Localization and Sentence Generation task</title>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Josiah</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fei</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emmanuel</forename><surname>Dellandrea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mauricio</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="7,160.01,355.79,239.55,7.86">CLEF2015 Working Notes, CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-11">September 8-11 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,377.71,342.25,7.86;7,146.91,388.67,333.68,7.86;7,146.91,399.62,333.68,7.86;7,146.91,410.58,333.68,7.86;7,146.91,421.54,333.68,7.86;7,146.91,432.50,124.61,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,395.79,410.58,84.81,7.86;7,146.91,421.54,144.61,7.86">General Overview of ImageCLEF at the CLEF 2015 Labs</title>
		<author>
			<persName coords=""><forename type="first">Mauricio</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Josiah</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefano</forename><surname>Bromuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">Ashraful</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mahmood</forename><surname>Kazi Mohammed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Burak</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Suzan</forename><surname>Uskudarli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Neda</forename><forename type="middle">B</forename><surname>Marvasti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">José</forename><forename type="middle">F</forename><surname>Aldana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">María</forename><surname>Del Mar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roldán</forename><surname>García</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="7,299.54,421.54,140.42,7.86">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
