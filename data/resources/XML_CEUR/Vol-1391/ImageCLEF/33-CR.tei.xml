<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,380.72,116.95,84.83,12.62;1,140.32,134.89,334.72,12.62;1,156.13,152.82,303.10,12.62;1,273.56,170.75,68.23,12.62">ImageCLEF 2015 Scalable Concept Image Annotation Task: Concept Detection with Blind Localization Proposals</title>
				<funder ref="#_6TF7QEU">
					<orgName type="full">French Research Agency ANR (Agence Nationale de la Recherche)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,274.12,208.42,67.11,8.74"><forename type="first">Hichem</forename><surname>Sahbi</surname></persName>
							<email>hichem.sahbi@telecom-paristech.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">CNRS TELECOM ParisTech at</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">CNRS TELECOM ParisTech</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,380.72,116.95,84.83,12.62;1,140.32,134.89,334.72,12.62;1,156.13,152.82,303.10,12.62;1,273.56,170.75,68.23,12.62">ImageCLEF 2015 Scalable Concept Image Annotation Task: Concept Detection with Blind Localization Proposals</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">64E90CB722C4F6E57C875403B3B1CF62</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Support vector machines</term>
					<term>histogram intersection kernels</term>
					<term>concept detection</term>
					<term>blind concept localization proposals</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce our participation at the ImageCLEF 2015 scalable concept detection and localization task. This edition focuses on generating not only annotations (concept detections) but also localizing concepts into a large image collection. Concept detection part of our runs is based on standard nonlinear support vector machines (SVMs). The localization part is blind and based on a priori learned statistics that generate multiple localization proposals. In spite of its blindness, the performance of this concept localization framework is promising.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The general problem of visual category recognition generally includes three different tasks: concept detection (also known as image annotation) <ref type="bibr" coords="1,420.88,465.83,7.75,8.74" target="#b0">[1]</ref><ref type="bibr" coords="1,428.63,465.83,3.87,8.74" target="#b1">[2]</ref><ref type="bibr" coords="1,432.50,465.83,7.75,8.74" target="#b2">[3]</ref>, concept localization <ref type="bibr" coords="1,186.80,477.79,10.52,8.74" target="#b3">[4,</ref><ref type="bibr" coords="1,198.98,477.79,7.75,8.74" target="#b4">5]</ref> and object category segmentation <ref type="bibr" coords="1,356.30,477.79,10.52,8.74" target="#b5">[6,</ref><ref type="bibr" coords="1,368.48,477.79,7.01,8.74" target="#b6">7]</ref>. Concept detection consists in inferring a list of keywords that best describes the visual and the semantic content of a given image while localization seeks to find a list of bounding boxes that defines the span of detected concepts. As a variant of concept localization, object category segmentation consists in delimiting the extent of detected concepts with a high precision.</p><p>We are interested in this paper in concept detection and localization; we present our solutions submitted to the ImageCLEF 2015 scalable concept image annotation task <ref type="bibr" coords="1,227.02,573.43,10.52,8.74" target="#b7">[8,</ref><ref type="bibr" coords="1,239.20,573.43,7.01,8.74" target="#b8">9]</ref>. This edition focuses on concept localization, which consists in finding all the occurrences of a list of concepts into a given test image. This task has been widely studied in different related challenges including Pascal VOC <ref type="bibr" coords="1,192.61,609.29,14.61,8.74" target="#b9">[10]</ref>, ImageNET <ref type="bibr" coords="1,266.72,609.29,15.50,8.74" target="#b10">[11]</ref> and more recently MS-COCO <ref type="bibr" coords="1,422.38,609.29,14.61,8.74" target="#b11">[12]</ref>. Existing solutions usually parse images using sliding windows <ref type="bibr" coords="1,371.61,621.25,14.60,8.74" target="#b12">[13]</ref>, image segmentation and superpixels <ref type="bibr" coords="1,207.81,633.20,15.50,8.74" target="#b13">[14]</ref> as well as multiple segmentation proposals <ref type="bibr" coords="1,422.30,633.20,14.61,8.74" target="#b14">[15]</ref>. In these methods, detection and segmentation results are scored using machine learning techniques (such as SVM <ref type="bibr" coords="1,246.74,657.11,14.61,8.74" target="#b15">[16]</ref>, deep networks <ref type="bibr" coords="1,332.82,657.11,14.61,8.74" target="#b16">[17]</ref>, and decision forests <ref type="bibr" coords="1,442.05,657.11,15.50,8.74" target="#b17">[18]</ref>) and consolidated using spatial layout and geometric relationships usually described with graphical models (such as conditional and Markov random fields <ref type="bibr" coords="2,444.06,382.15,15.50,8.74" target="#b18">[19,</ref><ref type="bibr" coords="2,461.21,382.15,11.62,8.74" target="#b19">20]</ref>). For more detailed discussions of related work in concept detection and localization, see <ref type="bibr" coords="2,174.12,406.06,15.50,8.74" target="#b20">[21]</ref> and references therein.</p><p>Among existing object localization (and segmentation) methods those based on region proposals are currently receiving a particular attention. Their general principle consists in defining multiple partitions of test images into sets of blobs that potentially correspond to actual objects. Only few of these partitions are scored and used to annotate and localize concepts in test images. Even though relatively successful, these approaches are highly dependent on the quality of image segmentation, which is known to be challenging especially when no a priori information is used about the statistics of these concept-localization proposals. Our proposed solution, discussed in this paper, avoids image segmentation and it is based on two steps: first, we train SVM classifiers that detect concepts belonging to different test images. Afterwards, we use an priori (trained) statistical model in order to infer their most likely locations, without observing the content of these test images. We will show that in spite of the simplicity of this approach, the results are reasonably decent, and very promising, and this opens a new direction towards refining these models and obtaining better performances by combining annotation and concept localization results.</p><p>The rest of this paper is organized as follows; first, we describe our concept detection algorithm, based on SVMs and an efficient evaluation of the histogram intersection kernel. Then, we describe our a priori statistical model for blind concept localization, and we present and discuss our ImageCLEF 2015 results. Finally, we conclude the paper, with possible extensions for a future work. Our concept detection and localization results are obtained according to the two following steps (see Fig. <ref type="figure" coords="3,242.17,496.55,3.87,8.74" target="#fig_1">2</ref>):</p><p>i) Holistic concept detection: this step is achieved using global (holistic) visual and textual features. For that purpose, we train "one versus all" SVMs for each concept, in order to detect whether that concept exists in a given test image (see extra details in Section 2.1).</p><p>ii) Blind concept localization proposals: in contrast to concept detection, concept localization is achieved blindly, i.e., without observing the content of a given test image. As will be shown subsequently, localization is achieved using a priori knowledge about possible locations of these bounding boxes. These knowledges correspond to learned localization statistics, of bounding boxes, taken from a training/dev set of concepts and their associated bounding boxes (i.e., from the file "imageclef2015.dev.bbox.v20150226"; see extra details in Section 2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Holistic concept detection: training and classification</head><p>We used only the holistic features provided in this ImageCLEF task including GIST, Color Histograms, SIFT, C-SIFT, RGB-SIFT, OPPONENT-SIFT, etc. We build 10 gram matrices (9 visual and 1 textual), based on efficient histogram intersection kernel, associated to these features. Then, we linearly combine those matrices into a single one. Notice that this combination does not result from multiple kernel learning but just a convex combination of kernels with uniform weights. We plug the resulting kernel into SVMs for training and testing.</p><p>For each concept, we train "one-versus-all" SVM classifiers; we use many random folds (taken from training/dev data in "imageclef2015.dev.bbox.v20150226") for multiple SVM training and we use these SVMs in order to predict the concepts on the test set<ref type="foot" coords="4,202.12,268.98,3.97,6.12" target="#foot_0">1</ref> . We repeat this training process, for each concept, through different random folds from the training set and we take the average scores of the underlying SVM classifiers. This makes classification results less sensitive to the sampling of the training set and also allows us to re-balance classification results mainly for concepts with unbalanced distributions of positive and negative data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Blind concept localization proposals</head><p>Several In the subsequent heuristics (2-5), we introduce the following notation: given a concept c, we consider T c = {p i c } i as the union of all the bounding boxes (in the training/dev set "imageclef2015.dev.bbox.v20150226") that belong to c. We also consider N c as the average number of bounding boxes (par image) associated to c; N c is evaluated from the training set. Prior to use the following heuristics (2-5), we consider offline step that clusters the coordinates in T c (using kmeans) with a number of clusters fixed to N c (see Fig. <ref type="figure" coords="5,375.83,299.12,7.75,8.74" target="#fig_2">3,</ref><ref type="figure" coords="5,386.90,299.12,13.84,8.74">left</ref>).</p><p>Heuristic 2 (concept-dependent BBs): for a detected concept c in a given test image, we generate N c bounding boxes whose coordinates correspond to the cluster centers obtained after applying k-means on T c .</p><p>In the remaining three heuristics (3-5), we update the coordinates of the bounding boxes, by manipulating i) their dimensions in heuristic 3, ii) their centers in heuristic 4, and iii) both their centers and dimensions in heuristic 5.</p><p>Heuristic 3 (re-scaled concept-dependent BBs): each bounding box p c = (x, y, w, h) generated in heuristic 2, is replaced by re-scaled BB. First, principal component analysis (PCA) is applied offline to the BB dimensions {(w i , h i )} i in the training set that also belong to concept c, afterwards, the dimensions (w, h) of p c are moved towards the first principal component of PCA <ref type="foot" coords="5,436.57,470.46,3.97,6.12" target="#foot_2">3</ref> , with an amplitude proportional to its eigenvalue (and this corresponds a re-scale of the dimensions of p c ). In this heuristic (x, y) remains unchanged (see Fig. <ref type="figure" coords="5,442.00,495.95,7.75,8.74" target="#fig_2">3,</ref><ref type="figure" coords="5,453.07,495.95,19.59,8.74">right</ref>).</p><p>Heuristic 4 (shifted concept-dependent BBs): for each bounding box p c = (x, y, w, h) generated in heuristic 2, we generate two extra BBs, with shifted coordinates. Again, PCA is applied offline to the BB coordinates {(x i , y i )} i in the training set that also belong to concept c, afterwards, the (x, y) coordinates of p c are shifted towards two opposite directions corresponding to the first principal component of PCA. In this heuristic (w, h) remains unchanged (see Fig. <ref type="figure" coords="5,472.84,581.48,3.87,8.74" target="#fig_2">3</ref>, middle).</p><p>Heuristic 5 (shifted and re-scaled concept-dependent BBs): this heuristic corresponds to the combination of the two heuristics 3 and 4.</p><p>The targeted task is, again, concept detection and localization: given a picture, the goal is to predict which concepts (classes) are present into that picture and a proposal of bounding boxes surrounding these concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">ImageCLEF 2015 Collection</head><p>A very large amount of images was gathered by the organizers, and using associated web pages, tags and meta-data were also provided. This set includes 500k images with only 2k images with known ground truth (i.e., labels and bounding boxes are given). These images belong to 251 concepts (see example in Fig. <ref type="figure" coords="6,468.97,253.46,3.87,8.74" target="#fig_0">1</ref>). Each image is again described with nine holistic visual features provided by the organizers, and we compute one extra textual feature using a normalized vector space model; first, a vocabulary of keywords V is defined<ref type="foot" coords="6,381.68,287.75,3.97,6.12" target="#foot_3">4</ref> in order to query the associated meta-data that include 500k textual descriptions. For each keyword ω ∈ V, only images whose textual descriptions include ω have their ω vector entry set to non-zeros.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Submitted Runs</head><p>All our submitted runs are based on SVM training and classification with the same kernel function (i.e., histogram intersection kernel), and the differences reside in the used decision criteria for concept detection and localization. Our ten submitted runs correspond to the combination of the five concept localization heuristics described earlier (see Section 2.2) and the two following concept detection criteria i) Criterion 1 (C1): the first concept detection results are obtained by following the setting in Section 2.1.</p><p>ii) Criterion 2 (C2): the second set of concept detection results is obtained using a slightly different criterion; more precisely, if an image has no detected concepts, i.e., all the SVM scores are negatives for all concepts, then we select the top 3 concepts (i.e., with the highest negative SVM scores) as annotations for that image. This makes it possible to increase the recall, with a possible impact on the precision.</p><p>Our runs are summarized in table <ref type="table" coords="6,295.96,578.22,3.87,8.74" target="#tab_1">1</ref>. For all the submitted runs, performances are evaluated, by the organizers, using a variant of the Jaccard measure; the latter is defined as the intersection over union of bounding boxes provided in the submitted runs and those in the ground truth. Mean average precision (MAP) measures based on different percentages of bounding box overlaps are given for each concept and also averaged through different concepts (see our results in  Tables <ref type="table" coords="7,165.57,387.65,7.75,8.74" target="#tab_2">2,</ref><ref type="table" coords="7,176.12,387.65,3.87,8.74" target="#tab_3">3</ref>). Details about these measures can be found in the ImageCLEF 2015 website <ref type="foot" coords="7,166.65,398.03,3.97,6.12" target="#foot_4">5</ref> .</p><p>From tables 2 and 3, we observe the following issues -Different methods for "concept localization proposals" provide much better results when concept detection is relatively successful (see runs 1, 3, 5, 7, 9 vs runs 2, 4, 6, 8, 10 in table 2 for different overlap ratios). Following the spirit of our two-step method, these results clearly corroborate the fact that concept detection could be decoupled from localization as long as concept detection is achieved with a relative success. This clearly opens a direction towards enhancing the performances of localization proposals by further improving concept detection results.</p><p>-From table 2, heuristic 3 (BB re-scaling) provides the best overall performances; indeed, even though shifting is important, it has less impact on performances compared to re-scaling. This is mainly due to the variability and non-rigidity of many concepts (such as animals), that require an adaptation of the dimensions of BBs, while shifting is already well captured by the statistical model (in heuristics 2, 4, 5); see again k-mean clustering in Section 2.2. -From table <ref type="table" coords="9,201.16,119.99,3.87,8.74" target="#tab_3">3</ref>, for almost all the concepts, statistical bounding box estimation (i.e., heuristics 2, 3, 4, 5) is very helpful in order to improve the quality of localization; for some concepts such as "frog", re-scaling and shifting are important, as this category is highly non-rigid while for other categories such as "bear", adaptation does not improve performances as "bear" localization is less predictable. Note also that for rigid (and man-made) objects, such as "cathedral" and "bicycle", re-scaling is more important than shifting as proportions of the w-h dimensions, in these concepts, are very changing while for others (including natural objects and also some other man-made objects such as "airplane", "balloon", "bucket", "camera"), the adaptation of shift is more important than scale; as the variability of w-h proportions is small in these concepts. In sum, bounding box re-scaling and shifting is important for some concepts and less for others. This suggests, as a future extension, to mix different heuristics for different concepts (and we already observe this improvement in the "concept-by-concept" results).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We discussed in this paper, our participation at the ImageCLEF 2015 Scalable Concept Image Annotation Task. Our runs are based on a two-step process that decouples concept detection from localization. The former is achieved using SVMs trained with linear combination of elementary histogram intersection kernels, while the latter is accomplished blindly using a simple statistical model that allows us to generate multiple localization proposals (without image segmentation). Observed results show that i) the accuracy of concept detection has an impact on the performance of localization, and ii) the adaptation of scale and shift of concept localization is essential to improve performances mainly for concepts with a large variability in their extents. A future possible extension, of this work, is to make concept localization nonblind and also coupled with concept detection. Another possible extension is to mix and select different localization heuristics for different concepts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,167.77,288.34,279.81,7.89;2,256.67,197.06,102.24,76.68"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Sample of pictures taken from the ImageCLEF2015 database.</figDesc><graphic coords="2,256.67,197.06,102.24,76.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,134.77,399.71,345.82,7.89;3,134.77,410.70,15.87,7.86"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. This figure shows the two-step process used for concept detection and localization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,134.77,203.41,345.83,7.89;5,134.77,214.40,345.83,7.86;5,134.77,225.36,308.52,7.86"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. This figure shows: (left) BB clustering process and the union of bounding boxes in the training set that belong to a given concept, (middle) BB shift using the first principal direction and (right) BB re-scale using the first principal direction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,134.77,116.95,345.82,53.37"><head>Table 1 :</head><label>1</label><figDesc>This table shows the definition of the ten runs submitted to the Image-CLEF 2015 challenge.</figDesc><table coords="7,139.09,139.12,337.18,31.20"><row><cell></cell><cell cols="5">Heuristic 1 Heuristic 2 Heuristic 3 Heuristic 4 Heuristic 5</cell></row><row><cell>Criterion 1 (C1)</cell><cell>Run 1</cell><cell>Run 3</cell><cell>Run 5</cell><cell>Run 7</cell><cell>Run 9</cell></row><row><cell>Criterion 2 (C2)</cell><cell>Run 2</cell><cell>Run 4</cell><cell>Run 6</cell><cell>Run 8</cell><cell>Run 10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,134.77,193.67,345.83,158.86"><head>Table 2 :</head><label>2</label><figDesc>Performances (in %) of our different concept detection and localization proposal heuristics sorted from the highest to the lowest (taken from ImageCLEF 2015 results). 30.73 26.11 20.79 16.81 13.29 10.49 08.53 06.81 04.99 03.17 6 (heuristic 3+C2) 19.40 17.39 15.83 14.08 12.44 10.63 09.25 08.00 06.56 05.20 10(heuristic 5+C2) 19.40 17.21 15.71 13.83 12.10 10.38 08.90 07.52 06.10 05.01 4 (heuristic 2+C2) 19.40 16.35 15.31 13.56 11.98 10.11 08.33 06.87 05.40 04.15 8 (heuristic 4+C2) 19.40 16.23 14.91 13.17 11.41 09.48 07.83 06.16 04.92 03.68 2 (heuristic 1+C2) 19.40 16.30 13.05 10.53 08.44 06.73 05.53 04.51 03.34 02.21</figDesc><table coords="7,137.67,227.66,338.72,70.74"><row><cell>X Runs # X X X 5 (heuristic 3+C1) 30.73 27.64 25.11 22.47 19.80 16.92 14.68 12.46 10.05 07.85 X X X X Overlap 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% X</cell></row><row><cell>9 (heuristic 5+C1) 30.73 27.21 24.87 21.69 19.01 16.58 14.08 11.65 09.32 07.62</cell></row><row><cell>3 (heuristic 2+C1) 30.73 26.13 24.48 21.52 18.72 15.82 13.01 10.30 08.03 06.32</cell></row><row><cell>7 (heuristic 4+C1) 30.73 25.73 23.50 20.58 17.77 14.61 11.80 09.38 07.38 05.55</cell></row><row><cell>1 (heuristic 1+C1)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,134.77,144.46,345.83,484.46"><head>Table 3 :</head><label>3</label><figDesc>Some "concept-by concept" performances (in %) of our different concept detection and localization proposal heuristics (taken from ImageCLEF 2015 results).</figDesc><table coords="8,138.72,178.70,337.93,450.22"><row><cell cols="2">concepts description</cell><cell>(r un 5) st at ist ic al + re sc al e</cell><cell>(r un 9) st at ist ic al + re sc al e+ sh ift</cell><cell>(r un 3) st at ist ic al</cell><cell>(r un 7) st at ist ic al + sh ift</cell><cell>(r un 1) fix ed</cell><cell>(r un 6) st at ist ic al + re sc al e</cell><cell>(r un 10 ) st at ist ic al + re sc al e+ sh ift</cell><cell>(r un 4) st at ist ic al</cell><cell>(r un 8) st at ist ic al + sh ift</cell><cell>(r un 2) fix ed</cell></row><row><cell>n01639765</cell><cell>frog</cell><cell cols="10">18.18 36.36 36.36 27.27 18.18 18.18 27.27 45.45 27.27 18.18</cell></row><row><cell cols="2">n01896031 feather</cell><cell cols="10">20.00 20.00 40.00 40.00 20.00 20.00 20.00 40.00 40.00 20.00</cell></row><row><cell>n02084071</cell><cell>dog</cell><cell cols="10">50.00 50.00 50.00 50.00 50.00 33.33 33.33 33.33 33.33 33.33</cell></row><row><cell>n02114100</cell><cell>wolf</cell><cell cols="10">22.86 20.00 28.57 25.71 20.00 22.86 20.00 28.57 25.71 20.00</cell></row><row><cell>n02129165</cell><cell>lion</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="5">02.22 02.22 02.22 02.22 02.22</cell></row><row><cell>n02131653</cell><cell>bear</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="2">0 50.00</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="2">0 50.00</cell></row><row><cell>n02206856</cell><cell>bee</cell><cell cols="10">66.67 66.67 66.67 66.67 66.67 50.00 50.00 50.00 50.00 50.00</cell></row><row><cell cols="2">n02330245 mouse</cell><cell cols="10">100.0 100.0 100.0 100.0 100.0 50.00 50.00 50.00 50.00</cell></row><row><cell>n02395406</cell><cell>hog</cell><cell cols="10">40.00 40.00 52.00 52.00 36.00 40.00 40.00 52.00 52.00 36.00</cell></row><row><cell>n02411705</cell><cell>sheep</cell><cell cols="10">52.94 52.94 35.29 41.18 47.06 52.94 52.94 23.53 29.41 47.06</cell></row><row><cell>n02416519</cell><cell>goat</cell><cell cols="2">50.00 50.00</cell><cell>0</cell><cell cols="4">0 50.00 33.33 33.33</cell><cell>0</cell><cell>0</cell><cell>33.33</cell></row><row><cell>n02430045</cell><cell>deer</cell><cell cols="5">25.00 25.00 25.00 25.00 0</cell><cell cols="5">25.00 25.00 25.00 25.00 0</cell></row><row><cell cols="12">n02484322 monkey 57.14 57.14 64.29 64.29 50.00 52.94 52.94 58.82 58.82 47.06</cell></row><row><cell cols="12">n02503517 elephant 100.0 100.0 100.0 100.0 100.0 28.57 28.57 28.57 28.57 14.29</cell></row><row><cell>n02512053</cell><cell>fish</cell><cell cols="10">60.00 60.00 70.00 60.00 60.00 46.67 46.67 53.33 46.67 40.00</cell></row><row><cell cols="2">n02691156 airplane</cell><cell>0</cell><cell>100.0</cell><cell cols="3">0 100.0 0</cell><cell>0</cell><cell>0</cell><cell cols="2">33.33 33.33</cell><cell>0</cell></row><row><cell cols="2">n02709367 anchor</cell><cell cols="10">63.16 73.68 42.11 47.37 52.63 63.64 63.64 45.45 59.09 54.55</cell></row><row><cell>n02774152</cell><cell>bag</cell><cell cols="2">10.00 10.00</cell><cell>0</cell><cell cols="4">0 10.00 07.69 07.69</cell><cell>0</cell><cell>0</cell><cell>07.69</cell></row><row><cell>n02778669</cell><cell>ball</cell><cell cols="5">16.67 16.67 16.67 16.67 0</cell><cell cols="4">12.50 12.50 12.50 12.50</cell><cell>0</cell></row><row><cell cols="2">n02782093 balloon</cell><cell>0</cell><cell>05.26</cell><cell cols="3">0 05.26 0</cell><cell>0</cell><cell>05.26</cell><cell cols="3">0 05.26 0</cell></row><row><cell cols="2">n02800213 baseball</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="5">01.49 01.49 01.49 01.49 01.49</cell></row><row><cell cols="2">n02828884 bench</cell><cell cols="10">20.00 20.00 25.00 25.00 20.00 20.00 20.00 25.00 25.00 20.00</cell></row><row><cell cols="2">n02834778 bicycle</cell><cell cols="10">10.00 05.00 10.00 05.00 05.00 13.16 05.26 10.53 05.26 07.89</cell></row><row><cell>n02839910</cell><cell>bin</cell><cell cols="10">30.43 30.43 30.43 30.43 30.43 30.43 30.43 30.43 30.43 30.43</cell></row><row><cell>n02883344</cell><cell>box</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="2">04.35 04.35</cell><cell cols="3">0 04.35 0</cell></row><row><cell cols="2">n02909870 bucket</cell><cell cols="10">46.67 53.33 53.33 53.33 53.33 41.18 47.06 47.06 47.06 47.06</cell></row><row><cell cols="12">n02933112 cabinet 60.00 60.00 80.00 80.00 60.00 24.24 21.21 27.27 24.24 21.21</cell></row><row><cell cols="2">n02942699 camera</cell><cell>0</cell><cell>05.26</cell><cell>0</cell><cell>05.26</cell><cell>0</cell><cell cols="5">05.41 05.41 02.70 08.11 05.41</cell></row><row><cell cols="12">n02984061 cathedral 36.76 37.50 13.97 08.82 34.56 36.76 37.50 15.44 16.18 34.56</cell></row><row><cell cols="2">n02990373 ceiling</cell><cell cols="10">36.36 36.36 36.36 36.36 36.36 23.26 23.26 23.26 25.58 18.60</cell></row><row><cell>n03001627</cell><cell>chair</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="5">10.26 10.26 10.26 15.38 0</cell></row><row><cell>n03046257</cell><cell>clock</cell><cell cols="10">11.32 09.43 07.55 07.55 05.66 09.43 09.43 09.43 07.55 05.66</cell></row><row><cell>n03135532</cell><cell>cross</cell><cell>25.00</cell><cell>0</cell><cell cols="2">25.00 0</cell><cell>0</cell><cell>20.00</cell><cell>0</cell><cell>20.00</cell><cell>0</cell><cell>0</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,144.73,624.92,335.87,7.86;4,144.73,635.88,33.59,7.86"><p>A given test image is assigned to a given concept, iff the underlying SVM score is positive.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,144.73,646.84,335.87,7.86;4,144.73,657.79,155.59,7.86"><p>Of course, the actual dimensions of the test images are taken into account in order to re-scale concept localization results.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="5,144.73,657.79,191.34,7.86"><p>i.e., the eigenvector with the largest eigenvalue.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="6,144.73,657.79,260.57,7.86"><p>Including relevant keywords that are used in concept definitions.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="7,144.73,657.79,177.68,7.86"><p>http://www.imageclef.org/2015/annotation.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This work is supported in part by a grant from the <rs type="funder">French Research Agency ANR (Agence Nationale de la Recherche)</rs> under the <rs type="projectName">MLVIS</rs> project.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_6TF7QEU">
					<orgName type="project" subtype="full">MLVIS</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,142.96,582.37,337.64,7.86;9,151.52,593.33,141.40,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,328.99,582.37,151.61,7.86;9,151.52,593.33,31.19,7.86">A model for learning the semantics of pictures</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,209.43,593.33,54.56,7.86">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,603.64,337.63,7.86;9,151.52,614.60,329.07,7.86;9,151.52,625.56,79.36,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,280.36,603.64,192.43,7.86">Real-time computerized annotation of pictures</title>
		<author>
			<persName coords=""><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Ze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,151.52,614.60,266.75,7.86">Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="985" to="1002" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>IEEE Transactions on</note>
</biblStruct>

<biblStruct coords="9,142.96,635.88,337.63,7.86;9,151.52,646.84,329.07,7.86;9,151.52,657.79,193.93,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,242.61,635.88,237.98,7.86;9,151.52,646.84,258.37,7.86">Context based support vector machines for interconnected image annotation (the saburo tsuji best regular paper award)</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sahbi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,440.60,646.84,40.00,7.86;9,151.52,657.79,164.62,7.86">the Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,120.67,337.64,7.86;10,151.52,131.63,329.07,7.86;10,151.52,142.59,329.07,7.86;10,151.52,153.55,50.68,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,430.54,120.67,50.06,7.86;10,151.52,131.63,267.76,7.86">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName coords=""><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,440.96,131.63,39.64,7.86;10,151.52,142.59,161.41,7.86;10,343.44,142.59,81.34,7.86">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
	<note>IEEE Conference on</note>
</biblStruct>

<biblStruct coords="10,142.96,163.67,337.63,7.86;10,151.52,174.63,195.90,7.86;10,389.56,174.63,91.03,7.86;10,151.52,185.59,289.36,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,394.54,163.67,86.05,7.86;10,151.52,174.63,187.56,7.86">On detection of multiple object instances using hough transforms</title>
		<author>
			<persName coords=""><forename type="first">Olga</forename><surname>Barinova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pushmeet</forename><surname>Kholi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,389.56,174.63,91.03,7.86;10,151.52,185.59,140.33,7.86">Analysis and Machine Intelligence, IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1773" to="1784" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,195.72,337.64,7.86;10,151.52,206.68,329.07,7.86;10,151.52,217.64,112.66,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,237.00,195.72,243.59,7.86;10,151.52,206.68,54.73,7.86">Superpixel based object class segmentation using conditional random fields</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sahbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,229.83,206.68,250.76,7.86;10,151.52,217.64,83.74,7.86">the International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,227.76,337.63,7.86;10,151.52,238.72,329.07,7.86;10,151.52,249.68,329.07,7.86;10,151.52,260.64,50.68,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,332.61,227.76,147.99,7.86;10,151.52,238.72,262.05,7.86">Describing the scene as a whole: Joint object detection, scene classification and semantic segmentation</title>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,440.96,238.72,39.64,7.86;10,151.52,249.68,161.41,7.86;10,343.44,249.68,81.34,7.86">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="702" to="709" />
		</imprint>
	</monogr>
	<note>IEEE Conference on</note>
</biblStruct>

<biblStruct coords="10,142.96,270.77,337.63,7.86;10,151.52,281.72,329.07,7.86;10,151.52,292.68,329.07,7.86;10,151.52,303.64,329.07,7.86;10,151.52,314.60,188.95,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,397.88,281.72,82.71,7.86;10,151.52,292.68,329.07,7.86;10,151.52,303.64,34.73,7.86">Overview of the Im-ageCLEF 2015 Scalable Image Annotation, Localization and Sentence Generation task</title>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Josiah</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fei</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emmanuel</forename><surname>Dellandrea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mauricio</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="10,208.71,303.64,105.00,7.86">CLEF2015 Working Notes</title>
		<title level="s" coord="10,151.52,314.60,147.47,7.86">CEUR Workshop Proceedings, CEUR</title>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-11">September 8-11 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,324.73,337.63,7.86;10,151.52,335.69,329.07,7.86;10,151.52,346.65,329.07,7.86;10,151.52,357.60,329.07,7.86;10,151.52,368.56,329.07,7.86;10,151.52,379.52,161.52,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,408.39,357.60,72.20,7.86;10,151.52,368.56,163.14,7.86">General Overview of ImageCLEF at the CLEF 2015 Labs</title>
		<author>
			<persName coords=""><forename type="first">Mauricio</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Josiah</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefano</forename><surname>Bromuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">Ashraful</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mahmood</forename><surname>Kazi Mohammed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Burak</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Suzan</forename><surname>Uskudarli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Neda</forename><forename type="middle">B</forename><surname>Marvasti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">José</forename><forename type="middle">F</forename><surname>Aldana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">María</forename><surname>Del Mar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roldán</forename><surname>García</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="10,330.80,368.56,145.83,7.86">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,389.65,337.97,7.86;10,151.52,400.61,329.07,7.86;10,151.52,411.57,244.55,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,226.44,400.61,189.48,7.86">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,428.00,400.61,52.60,7.86;10,151.52,411.57,105.16,7.86">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,421.69,337.98,7.86;10,151.52,432.65,329.07,7.86;10,151.52,443.61,300.66,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,440.73,421.69,39.86,7.86;10,151.52,432.65,161.46,7.86">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,334.18,432.65,146.42,7.86;10,151.52,443.61,22.78,7.86;10,206.33,443.61,136.11,7.86">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct coords="10,142.62,453.74,337.97,7.86;10,151.52,464.70,329.07,7.86;10,151.52,475.65,304.43,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,350.72,464.70,129.87,7.86;10,151.52,475.65,39.43,7.86">Microsoft coco: Common objects in context</title>
		<author>
			<persName coords=""><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zitnick</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,213.97,475.65,120.42,7.86">Computer Vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,485.78,337.98,7.86;10,151.52,496.74,329.07,7.86;10,151.52,507.70,329.07,7.86;10,151.52,518.66,67.83,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,286.40,485.78,194.19,7.86;10,151.52,496.74,71.77,7.86">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,248.98,496.74,171.50,7.86;10,454.20,496.74,26.39,7.86;10,151.52,507.70,274.10,7.86">Proceedings of the 2001 IEEE Computer Society Conference on</title>
		<meeting>the 2001 IEEE Computer Society Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">511</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct coords="10,142.61,528.78,337.98,7.86;10,151.52,539.74,329.07,7.86;10,151.52,550.70,329.07,7.86;10,151.52,561.66,123.25,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,391.78,528.78,88.81,7.86;10,151.52,539.74,133.25,7.86">Multiscale conditional random fields for image labeling</title>
		<author>
			<persName coords=""><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Carreira-Perpindn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,311.50,539.74,165.26,7.86;10,176.37,550.70,300.07,7.86">Proceedings of the 2004 IEEE computer society conference on</title>
		<meeting>the 2004 IEEE computer society conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">695</biblScope>
		</imprint>
	</monogr>
	<note>Computer vision and pattern recognition</note>
</biblStruct>

<biblStruct coords="10,142.61,571.79,337.97,7.86;10,151.52,582.75,329.07,7.86;10,151.52,593.71,311.88,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,371.92,571.79,108.67,7.86;10,151.52,582.75,184.21,7.86">Generating object segmentation proposals using global and local search</title>
		<author>
			<persName coords=""><forename type="first">Pekka</forename><surname>Rantalankila</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,359.42,582.75,121.17,7.86;10,151.52,593.71,80.78,7.86;10,262.95,593.71,81.50,7.86">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="2417" to="2424" />
		</imprint>
	</monogr>
	<note>IEEE Conference on</note>
</biblStruct>

<biblStruct coords="10,142.61,603.83,337.98,7.86;10,151.52,614.79,329.07,7.86;10,151.52,625.75,212.33,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,389.96,603.83,90.63,7.86;10,151.52,614.79,150.39,7.86">Ensemble of exemplarsvms for object detection and beyond</title>
		<author>
			<persName coords=""><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,326.23,614.79,101.63,7.86;10,458.32,614.79,22.27,7.86;10,151.52,625.75,111.82,7.86">IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV)</note>
</biblStruct>

<biblStruct coords="10,142.61,635.88,337.97,7.86;10,151.52,646.84,329.07,7.86;10,151.52,657.79,163.58,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,401.14,635.88,79.45,7.86;10,151.52,646.84,179.57,7.86">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,354.03,646.84,126.56,7.86;10,151.52,657.79,72.40,7.86">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,120.67,337.98,7.86;11,151.52,131.63,329.07,7.86;11,151.52,142.59,98.34,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="11,302.44,120.67,178.15,7.86;11,151.52,131.63,15.36,7.86">Class-specific hough forests for object detection</title>
		<author>
			<persName coords=""><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,190.66,131.63,269.72,7.86">Decision Forests for Computer Vision and Medical Image Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="143" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,153.55,337.98,7.86;11,151.52,164.51,329.07,7.86;11,151.52,175.46,165.66,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="11,289.47,153.55,191.12,7.86;11,151.52,164.51,51.61,7.86">An exemplar-based crf for multi-instance object segmentation</title>
		<author>
			<persName coords=""><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,225.54,164.51,202.84,7.86;11,458.31,164.51,22.28,7.86;11,151.52,175.46,55.93,7.86">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="296" to="303" />
		</imprint>
	</monogr>
	<note>IEEE Conference on</note>
</biblStruct>

<biblStruct coords="11,142.62,186.42,337.98,7.86;11,151.52,197.38,167.49,7.86" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="11,195.15,186.42,285.44,7.86;11,151.52,197.38,131.51,7.86">Markov random field modeling in Image Analysis (was: Markov random field modeling in computer vision</title>
		<author>
			<persName coords=""><forename type="first">Stan Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,208.34,251.45,7.86" xml:id="b20">
	<monogr>
		<ptr target="http://image-net.org/about-publication" />
		<title level="m" coord="11,151.52,208.34,77.67,7.86">ImageNET webpage</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
