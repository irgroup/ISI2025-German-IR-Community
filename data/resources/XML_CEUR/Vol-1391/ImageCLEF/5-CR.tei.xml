<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,157.34,116.98,300.68,15.15;1,191.39,138.90,232.57,15.15">Automatic Image Annotation using Weakly Labelled Web Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,163.40,179.66,66.33,10.48"><forename type="first">Pravin</forename><surname>Kakar</surname></persName>
							<email>kakarpv@i2r.a-star.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Social Media and Internet Vision Analytics Lab</orgName>
								<orgName type="institution">Institute for Infocomm Research</orgName>
								<address>
									<addrLine>#21-01, 1 Fusionopolis Way</addrLine>
									<postCode>138632</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,239.32,179.66,76.08,10.48"><forename type="first">Xiangyu</forename><surname>Wang</surname></persName>
							<email>wangx@i2r.a-star.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Social Media and Internet Vision Analytics Lab</orgName>
								<orgName type="institution">Institute for Infocomm Research</orgName>
								<address>
									<addrLine>#21-01, 1 Fusionopolis Way</addrLine>
									<postCode>138632</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,369.38,179.66,82.58,10.48"><forename type="first">Yong-Sang</forename><surname>Chia</surname></persName>
							<email>yschia@i2r.a-star.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Social Media and Internet Vision Analytics Lab</orgName>
								<orgName type="institution">Institute for Infocomm Research</orgName>
								<address>
									<addrLine>#21-01, 1 Fusionopolis Way</addrLine>
									<postCode>138632</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,157.34,116.98,300.68,15.15;1,191.39,138.90,232.57,15.15">Automatic Image Annotation using Weakly Labelled Web Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EE21B13EC6D35E29E465FCBC86D1FEA5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>visual recognition</term>
					<term>scalable annotation</term>
					<term>learning from noisy data</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we propose and describe a method for localizing and annotating objects in images for the Scalable Concept Image Annotation challenge at ImageCLEF 2015. The unique feature of our proposed method is in its almost exclusive reliance on a single modality -visual data -for annotating images. Additionally, we do not utilize any of the provided training data, but instead create our own similarly-sized training set. By exploiting the latest research in deep learning and computer vision, we are able to test the applicability of these techniques to a problem of extremely noisy learning. We are able to obtain state-of-theart results on an inherently multi-modal problem thereby demonstrating that computer vision can also be a primary classification modality instead of relying primarily on text to determine context prior to image annotation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Scalable Concept Image Annotation challenge (SCIA) at Im-ageCLEF 2015 <ref type="bibr" coords="1,212.77,529.73,18.21,10.48" target="#b13">[15]</ref> is designed to evaluate methods to automatically annotate, localize and/or describe concepts and objects in images. In contrast to previous years, there have been several notable changes to the challenge. Some of them are highlighted below:</p><p>-Localization of objects within images has been introduced. As a results, the focus on more "object"-like concepts has increased this year. -Use of hand-labelled data has been allowed. Although this is done to technically allow the use of deep learning models trained on the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) <ref type="bibr" coords="2,151.70,135.07,17.17,10.48" target="#b10">[12]</ref>, it opens up the possibility of potential clean-up of the training data. Note that we have not done this in this work, but it appears to be legal within the regulatory framework of the challenge.</p><p>-The training and test sets are identical. Therefore, a method that is able to exploit the noisy training data (e.g. via data cleaning) could, in theory, benefit from potentially overfitting the training data.</p><p>From a computer vision perspective, SCIA is more challenging than the current benchmark challenge <ref type="bibr" coords="2,337.54,272.44,18.21,10.48" target="#b10">[12]</ref> in at least two senses -1) the training data provided is fairly noisy, which makes learning a difficult problem and 2) the test set is 5× the size of <ref type="bibr" coords="2,403.00,301.33,17.17,10.48" target="#b10">[12]</ref>. While this does not indicate a clear increase in level of difficulty (for example, <ref type="bibr" coords="2,134.77,330.23,18.21,10.48" target="#b10">[12]</ref> has 4× the number of concepts of SCIA), certain aspects are definitely more demanding.</p><p>In the rest of these notes, we discuss our proposed method, including data collection, classifier training and post-processing tweaks. We also discuss the challenges posed due to the fact that test data is annotated via crowd-sourcing which adds another source of label noise to "ground-truth" data. Finally, we present our results on SCIA along with proposals for future research to improve the automatic annotation capabilities of techniques in this field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Algorithm Design</head><p>As mentioned earlier, our algorithm is designed to mostly rely on visual data. We do not employ extensive ontologies to augment training data, nor do we use them during the training process, thus helping understand the importance of having a strong visual recognition pipeline. The various stages of our annotation pipeline are discussed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data Collection</head><p>We do not use the provided training set for two main reasons: 1) as the training and test sets are identical, there is no penalty for overfitting on the training data, which could provide an artificial boost  to performance results, and 2) there is little direct relationship between the target concepts and the image keywords in the training data, making it difficult to decouple the significance of a good ontology from that of a good visual learning mechanism. Therefore, we create our own training data of approximately the same size as the SCIA dataset.</p><p>The data collection pipeline is shown in Figure <ref type="figure" coords="3,400.47,378.65,4.55,10.48" target="#fig_0">1</ref>. We first consider the target concept names as keywords for which appropriate images need to be found. There is an issue of non-specificity of some of the concept names. For example, the concept "dish" can refer to both the vessel as well as the food content, although only the former is the target concept. Additionally, it is difficult to achieve both specificity and diversity using a single keyword when doing a web search for images. As an example, searching for "candy" yields generic images of candy, which while containing diverse instances of candy do not closely match single, specific instances of candy.</p><p>Both the above issues are conventionally dealt with by using ontologies to determine the coverage for each concept. We do not build our own challenge-specific ontology here, but instead simply rely on WordNet [10] to augment the individual keywords. In particular, this is done by also considering hyponyms (sub-categories) and lemmas (similar words) of the target concept. The hyponyms help target specific instances of the target concept, while the lemmas help increase the coverage of the target concept.</p><p>This augmented set of keywords per concept is then passed into an image search engine. We use Bing Image Search <ref type="bibr" coords="3,388.68,655.75,12.36,10.48" target="#b6">[7]</ref> in this pipeline.</p><p>Note that we search for the hyponyms and lemmas of the target concept by appending the target concept, in order to ensure that the correct sense of images is being search for. For example, searching for "truffle" rather than "truffle candy" results in a very different set of images that include fungi, which fall outside the scope of the target concept.</p><p>We gather up to 4000 images per target concept from our crawling engine. These images are passed through a filtering step where images that are corrupted, too small or almost completely uniform in appearance are discarded. The remaining images then form our training dataset -an automatically created, noisily labelled dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Feature Extraction</head><p>For the images collected by the above process, we extract features that will be useful for image classification. We choose to use the features from the winner of the latest ILSVRC classification challenge -GoogLeNet <ref type="bibr" coords="4,207.30,366.28,17.17,10.48" target="#b11">[13]</ref>, a deep learning model trained on the ILSVRC dataset and consisting of a highly-interconnected network-in-network architecture. Nevertheless, their model size is small enough to fit within our available computing resources of a single GeForce GTX 480 GPU with 1.5 GB of memory.</p><p>For each training image, we scale it down to 256×256 pixels and use the center crop of 224×224 pixels. The intuition behind this is that as the images are retrieved using specific keywords, it is likely that the object of interest is the focus of the image and should be dominant. This also reduces the computational complexity of the feature extraction process considerably. We extract features from the pooled 5B layer of the GoogLeNet model (see <ref type="bibr" coords="4,398.33,525.46,18.21,10.48" target="#b11">[13]</ref> for details), yielding a 1024-dimensional vector per training image. Each feature vector is then normalized to unit length.</p><p>We then train linear SVM classifiers <ref type="bibr" coords="4,342.80,569.08,12.36,10.48" target="#b2">[3]</ref> in a one-versus-all fashion. This is not strictly correct, as some concepts will almost certainly overlap (e.g. "face" will contain "eye", "nose", "mouth", etc.). However, making such an independence assumption greatly simplifies the learning process. Moreover, it allows us to avoid using a relationship ontology to determine the appopriate weight of every concept for each classifier. This is also in line with the goal of the challenge to design a scalable system, as the addition of a new target concept does not necessitate a recomputation of the weights against every existing concept. In order to manage space constraints, we uniformly sample the negative training samples for each concept, only selecting 60,000 of them. Thus, we train a single 1024-dimensional linear classifier per target concept to use for annotating test images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Object Proposals Proposal Classification Non-maximal suppression</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Annotation Pipeline</head><p>Figure <ref type="figure" coords="5,171.67,568.82,5.85,10.48" target="#fig_1">2</ref> shows our processing pipeline for a single test image.</p><p>We first create object proposals using the technique of <ref type="bibr" coords="5,436.83,583.52,18.21,10.48" target="#b12">[14]</ref> that uses selective-search to find likely "object" regions. Experimentally, it is observed that many of these proposals are near-duplicates. In order to increase diversity, we limit the returned proposals to those that overlap others by at most 90%. This is found to be an acceptable value for controlling the tradeoff between increasing diversity, and losing significant objects. We restrict the number of proposals returned to the first 150, in decreasing order of size.</p><p>Each object is then passed through the same feature extraction pipeline as in Section 2.2, and the classifiers trained therein are run to yield the most likely concepts per region. In general, non-maximal suppression is done per concept across all regions to limit the effect of overlapping proposals reporting the same concept for the same object.</p><p>There are two branches from this primary pipeline that we employ based on our observations on the SCIA development set. Firstly, we observe that many object proposals are labeled as "tree" if they contain heavy foliage. While not incorrect for the individual region, it may be incorrect for an overall image, where it is often difficult to localize a single tree. In order to mitigate this effect, we perform morphological merging for all tree boxes, taking the convex hull for each merged region as the bounding box of the tree and assigning it the highest confidence of all the merged boxes. We observe this to help improve the localization performance for the "tree" concept on the development set. We also believe that this idea can be extended to other non-uniformly shaped, difficult-to-localize concepts such as "rock", "leaf", "brick", etc. but we do not have sufficient annotations in the development data to verify the same.</p><p>Secondly, we observe that for any generic image dataset, humans are an important object. This is true for SCIA as well as for <ref type="bibr" coords="6,440.92,453.35,17.63,10.48" target="#b10">[12,</ref><ref type="bibr" coords="6,458.56,453.35,8.82,10.48" target="#b7">8,</ref><ref type="bibr" coords="6,467.37,453.35,8.82,10.48" target="#b1">2]</ref>. Note that this is in contrast to domain-specific datasets such as <ref type="bibr" coords="6,134.77,482.24,13.10,10.48" target="#b8">[9,</ref><ref type="bibr" coords="6,147.87,482.24,13.10,10.48" target="#b9">11]</ref>. To this end, we use face and gender detection from <ref type="bibr" coords="6,434.47,482.24,12.36,10.48" target="#b0">[1]</ref> to detect persons with frontal faces in images. We supplement this with a simple regression to an upper-body annotation using the data from <ref type="bibr" coords="6,134.77,525.58,11.71,10.48" target="#b3">[4]</ref>. Finally, we use the information from <ref type="bibr" coords="6,348.74,525.58,12.36,10.48" target="#b5">[6]</ref> to determine the locations of various other person attributes.</p><p>A fusion step merges the results from the primary and two secondary pipelines. Specifically, person results from multiple pipelines are suppressed or modified based on overlaps between returned localizations for the same concepts. Additionally, localizations that have too high or low aspect ratios are suppressed, along with localizations that fall below a preset score threshold. Finally, if all localizations have been suppressed, then we report a single localization comprising of the entire image, corresponding to the global scene classification. This is based on the premise that all the development set images contain at least one concept, and we extend that assumption to all the test images.</p><p>Optionally, the fusion section can also contain multiple textual refinement steps. One option is to search URL filenames for concept names, and if found, assign them to the entire image. Another approach uses correlation between concepts from an ontology. This is done to test the impact of simple context addition to the annotation pipeline. Details of this latter approach are provided in the following subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Ontology and correlation</head><p>With the feature extraction and annotation pipeline, a set of bounding boxes {B i } are obtained for each test image. We denote the prediction scores for the target concepts in B i as S i = [s i1 , ..., s im ], where m is the total number of target concepts. By combining the prediction scores for all the bounding boxes {B i }, the prediction score for the image is calculated as S = [s 1 , ..., s m ] where s i = max j s ji .</p><p>Due to the fact that concepts do not occur in isolation (e.g. bathroom and bathtub, mountain and cliff), semantic context can be used to improve annotation accuracy. Following a way similar to <ref type="bibr" coords="7,447.21,419.17,11.71,10.48" target="#b4">[5]</ref>, we adopt semantic diffusion to refine the concept annotation score. We denote C = {c 1 , ..., c m } as the set of target concepts. Let W be the concept affinity matrix where W ij indicates the affinity between concepts c i and c j , and D denote the diagonal node degree matrix where D ii = d i = j W ij . Then the graph Laplacian is ∆ = D -W and the normalized graph Laplacian is L = I -D -1/2 W D 1/2 . In this problem, we measure the concept affinity based on Wikipedia dataset. Let M denote the total number of pages in Wikipedia. For concept c i , we denote y ik = 1 if concept keyword c i appears in page k, and y ik = 0 otherwise. The affinity W ij between concept c i and c j can then be computed using Pearson product moment correlation as:</p><formula xml:id="formula_0" coords="7,227.37,599.92,253.22,29.89">W ij = M k=1 (y ik -µ i )(y jk -µ j ) (M -1)σ i σ j<label>(1)</label></formula><p>where µ i and σ i are the sample mean and standard deviation for c i , respectively. Based on our study, the original prediction should be quite precise. We employ only positive correlation to boost the concepts to improve the recall. Let g ∈ R m×1 denote the refined score vector, the values g i and g j should be consistent with W ij (the affinity between concepts c i and c j ). Motivated by the semantic consistency, we formulate the score refinement problem by minimizing a loss function</p><formula xml:id="formula_1" coords="8,243.15,217.99,237.44,31.85">ε = 1 2 m i,j=1 W ij || g i d i - g j d j || 2<label>(2)</label></formula><p>The loss function can be rewritten as</p><formula xml:id="formula_2" coords="8,270.16,289.32,210.43,26.77">ε = 1 2 tr(g T Lg)<label>(3)</label></formula><p>The loss function can be optimized using gradient descent algorithm as</p><formula xml:id="formula_3" coords="8,272.58,360.50,208.02,11.50">g = g -α∇ g ε<label>(4)</label></formula><p>where ∇ g ε = Lg, and α is the learning rate. Intially, g = S. By iteratively optimizing the loss function, we obtain the refined smooth score vector g for the image. A threshold τ is chosen, so that we consider concept c i appears if g i &gt; τ , otherwise we think the concept does not appear in the image (consequently not in any of the bounding boxes B i ). That is, for each bounding box in an image, we report the concept with the maximum confidence given the concept appears in the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset Limitations</head><p>We tune the various parameters of our algorithm by validating its performance on the development set. Unfortunately, the development set (and by extension, the SCIA test set) has multiple problems that make it very difficult to correctly gauge the effect of tuning. Most of these problems arise from the limitations of crowd-sourcing ground-truth annotation, and need to be addressed to make SCIA a more consistent evaluation. We summarize the major issues involved below with the 4 I's. Inconsistency Many images in the development set exhibit inconsistent annotations. An example of this is shown in Figure <ref type="figure" coords="9,428.94,298.78,4.55,10.48" target="#fig_2">3</ref>. Despite there clearly being 4 legs in the picture, only 1 is annotated. This is inconsistent as none of the unannotated instances are any less of "leg"s than the one annotated. Other examples of inconsistencies seen in the development set include unclear demarcations between when multiple instances of the same concept are to be grouped into a single instance or vice versa and annotating partially-visible instances in some images while not annotating completely visible instances in other images.</p><p>Incompleteness Several images in the development set are incompletely annotated. This is most prevalent in the case of humans where various body parts are skipped altogether in the annotations. Apart from this, there appears to be a certain level of arbitrariness in choosing which concepts to annotate. For instance in Figure <ref type="figure" coords="9,471.49,498.94,4.55,10.48" target="#fig_2">3</ref>, "shirt" is annotated, but "short pants" is not when clearly both have about the same level of "interestingness". Additionally, concepts like "chair", "sock", "shoe", "stadium", etc. which are also present in the image are not annotated. This makes it extremely difficult to judge the performance of a proposed technique on the development set. Moreover, it seems to run counter to the challenge assertion that the proportion of missing or incomplete annotations is insignificant.</p><p>Incorrectness Althought not as prevalent as the previous two problems, many annotations are incorrect. In Figure <ref type="figure" coords="9,382.74,641.31,4.55,10.48" target="#fig_2">3</ref>, the two balls are labelled as balloons, which is clearly wrong. There are other cases of wrong annotations in items of clothing (shirt/jacket/suit) as well as gender and age of persons ("man" labelled as "male child", "woman" labelled as "man", etc.). Impossibility This issue is the least prevalent of the four discussed in this section. The image shown in Figure <ref type="figure" coords="10,369.51,384.40,5.85,10.48" target="#fig_3">4</ref> was flagged by our image annotation pipeline as having a very large number of object instances. It can be seen that the image contains more than 200 faces. This implies that there are more than 100 instances of at least one of "man" or "woman"<ref type="foot" coords="10,269.95,440.56,4.23,6.99" target="#foot_0">1</ref> . Within the rules of the challenge, each concept is limited to 100 instaces, making it impossible to annotate all instances correctly. Grouping multiple instances into a single instance, if one were inclined to do so, is not straightforward as there is no clear group of men and women as in some other images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>In this section, we evaluate the performance of different settings of the algorithm on the development and test datasets. It is to be noted that there appear to be significant differences in the quality of the annotations between the two sets, so results on one are not indicative of results on the other. Moreover, as there were no particulars provided about the measures being used for evaluation beyond "performance" on both concepts and images, we used the F-score as a measure on the development set, which formed the basis of 2 out of 3 measures in the previous iteration of the challenge. As it turns out, the evaluation measure used on the test set was the mAP and so, results between the development and test sets are again not directly comparable. These statistics are shown in Table <ref type="table" coords="11,384.16,207.30,4.55,10.48" target="#tab_1">1</ref>. We run two base versions of our pipeline, one aimed at garnering better precision (BP) and one aimed at getting better recall (BR). These are shown in the first two rows of the table. Following this, we notice that in some images, no concepts were predicted as their confidences scores fell below their respective thresholds. In these cases, we forced at least 1 prediction to be made (≥ 1 pred.) giving rise to two more variants, BP1 and BR1.</p><p>URL-search corresponds to the URL filename-concept name match discussed earlier. Agg. NMS refers to aggressive non-maximal suppression that employs a NMS threshold of 0, resulting in all overlapping bounding boxes for the same concept to be reduced to a single one. From human attributes, we either report hair + mouth, which showed no deleterious effects on the development set in the face of incomplete annotations, or face parts which also adds eyes and noses, or body parts which further adds in faces, heads, necks and arms. In the case of ontologies, while we obtain slightly better results on the development set, output errors in the submission cause the performance to be quite low, which is an outlier.</p><p>From the results, it can be seen that all human attributes significantly help boost performance. Moreover, URL-search causes a drop in performance, while aggressive NMS again boosts performance. Hence, a possible solution that yields even better performance could be BR1 + Agg. NMS + body parts.</p><p>It is also to be noted that the runner-up in the challenge attains a performance about 15% lower than ours. As the details of their technique are not available, it is difficult to pinpoint the cause of the large difference, but we believe that the use of an external training set, combined with human part extraction played an important role.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>In this work, we have presented our proposed method for the Image-CLEF Scalable Concept Image Annotation challenge. Our method places heavy emphasis on the visual aspect of annotating images and demonstrates the performance that can be achieved by building an appropriate pipeline of state-of-the-art visual recognition techniques. The interconnections between the techniques are modified and enhanced to improve overall annotation performance by branching off secondary recognition pipelines for certain highly common concepts.</p><p>We also highlight the limitations of the current challenge dataset with respect to the ground-truth annotations, categorizing the major shortcomings. Despite these and our technique's general lack of reliance on textual data, we are able to outperform competing methods by a margin of at least 15%. In the future, we plan to refine our annotation pipeline based on the analysis of the results. As most of the target concepts in this iteration of the challenge were localizable in a well-defined manner, it will be interesting to examine localization for other, more abstract concepts. We also hope to combine advances in natural language processing and semantic ontologies to appropriately weigh training instances in learning classifiers as well as look at the problem from a multi-modal point of view.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,244.52,253.66,126.33,7.89"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Data collection pipeline</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,239.46,389.57,136.43,7.89"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Image annotation pipeline</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,134.77,238.85,345.83,7.89;9,134.77,249.84,231.36,7.86;9,238.51,116.83,138.31,107.25"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Image demonstrating inconsistency, incompleteness and incorrectness of annotations. Annotations are from the SCIA development set.</figDesc><graphic coords="9,238.51,116.83,138.31,107.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="10,190.84,322.41,233.67,7.89;10,238.51,183.92,138.33,123.72"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Image demonstrating impossibility of annotations.</figDesc><graphic coords="10,238.51,183.92,138.33,123.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="11,155.65,243.39,300.98,162.81"><head>Table 1 .</head><label>1</label><figDesc>Performance statistics for various runs.</figDesc><table coords="11,155.65,262.45,300.98,143.75"><row><cell>Method</cell><cell cols="4">Development set F (0.5) F (0) mAP (0.5) mAP (0) Test set</cell></row><row><cell>Better precision (BP)</cell><cell cols="2">0.1463 0.2921</cell><cell>0.3039</cell><cell>0.4571</cell></row><row><cell>Better recall (BR)</cell><cell cols="2">0.1462 0.2934</cell><cell>0.2949</cell><cell>0.4466</cell></row><row><cell>BP + ≥ 1 pred (BP1)</cell><cell cols="2">0.1459 0.2940</cell><cell>0.2949</cell><cell>0.4466</cell></row><row><cell>BR + ≥ 1 pred (BR1)</cell><cell cols="2">0.1459 0.2927</cell><cell>0.3039</cell><cell>0.4569</cell></row><row><cell>BR1 + URL search</cell><cell cols="2">0.1450 0.2948</cell><cell>0.2948</cell><cell>0.4465</cell></row><row><cell>BR1 + Agg. NMS</cell><cell cols="2">0.1380 0.2894</cell><cell>0.3536</cell><cell>0.5045</cell></row><row><cell cols="3">BR1 + hair + mouth + URL search 0.1442 0.2923</cell><cell>0.5707</cell><cell>0.7603</cell></row><row><cell>BR1 + hair + mouth</cell><cell cols="2">0.1450 0.2899</cell><cell>0.5786</cell><cell>0.7685</cell></row><row><cell>BR1 + body parts + URL search</cell><cell cols="2">0.1279 0.2615</cell><cell>0.6024</cell><cell>0.7918</cell></row><row><cell>BR1 + face parts</cell><cell cols="2">0.1407 0.2818</cell><cell cols="2">0.6595 0.7954</cell></row><row><cell>Runner-up</cell><cell>NA</cell><cell>NA</cell><cell>0.5100</cell><cell>0.6426</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="10,146.47,646.84,334.12,7.86;10,146.47,657.79,178.79,7.86"><p>A quick inspection of the image shows no children, eliminating the possibility of instances of "male child" and "female child"</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="13,142.96,145.58,203.84,8.12" xml:id="b0">
	<monogr>
		<ptr target="http://openbiometrics.org/" />
		<title level="m" coord="13,152.39,145.58,64.93,7.86">Open biometrics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,156.54,337.64,7.86;13,152.39,167.49,328.20,8.11;13,152.39,179.10,287.64,7.47" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="13,464.72,156.54,15.88,7.86;13,152.39,167.49,168.92,7.86">The PASCAL Visual Object Classes Challenge</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2011/workshop/index.html" />
	</analytic>
	<monogr>
		<title level="m" coord="13,350.37,167.49,71.69,7.86">VOC2011) Results</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,189.41,337.64,7.86;13,152.39,200.37,328.20,7.86;13,152.39,211.33,47.10,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,401.78,189.41,78.81,7.86;13,152.39,200.37,111.82,7.86">Liblinear: A library for large linear classification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><forename type="middle">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,270.75,200.37,173.65,7.86">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,222.29,337.63,7.86;13,152.39,233.25,328.20,7.86;13,152.39,244.21,227.85,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,342.39,222.29,138.20,7.86;13,152.39,233.25,106.83,7.86">Progressive search space reduction for human pose estimation</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Marin-Jimenez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,281.05,233.25,170.97,7.86;13,152.39,244.21,123.28,7.86">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>CVPR 2008. IEEE Conference</note>
</biblStruct>

<biblStruct coords="13,142.96,255.17,337.63,7.86;13,152.39,266.12,328.20,7.86;13,152.39,277.08,278.04,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,354.49,255.17,126.10,7.86;13,152.39,266.12,212.43,7.86">Domain adaptive semantic diffusion for large scale context-based video annotation</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">W</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,386.88,266.12,67.86,7.86">Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1420" to="1427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,288.04,337.64,8.12;13,152.39,299.65,61.20,7.47" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Jusko</surname></persName>
		</author>
		<ptr target="http://www.realcolorwheel.com/human.htm" />
		<title level="m" coord="13,207.93,288.04,141.87,7.86">Human figure drawing proportions</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,309.96,283.13,8.12" xml:id="b6">
	<monogr>
		<ptr target="http://www.bing.com/images" />
		<title level="m" coord="13,195.98,309.96,73.42,7.86">Bing image search</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
		<respStmt>
			<orgName>Microsoft</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,320.92,337.63,7.86;13,152.39,331.88,328.20,7.86;13,152.39,342.84,60.92,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,350.70,320.92,129.89,7.86;13,152.39,331.88,32.81,7.86">Generic object recognition with boosting</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Opelt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fussenegger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,192.20,331.88,261.94,7.86">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="416" to="431" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,353.80,337.64,7.86;13,152.39,364.75,323.18,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,383.13,353.80,54.98,7.86">Cats and dogs</title>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,458.46,353.80,22.14,7.86;13,152.39,364.75,229.91,7.86">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3498" to="3505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,397.63,337.98,7.86;13,152.39,408.59,95.89,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,264.00,397.63,103.23,7.86">Recognizing indoor scenes</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,373.99,397.63,106.60,7.86;13,152.39,408.59,67.22,7.86">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,419.55,337.97,7.86;13,152.39,430.51,328.20,7.86;13,152.39,441.47,328.20,7.86;13,152.39,452.43,57.46,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,415.85,430.51,64.74,7.86;13,152.39,441.47,145.17,7.86">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,306.36,441.47,174.24,7.86;13,152.39,452.43,28.80,7.86">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,463.38,337.98,7.86;13,152.39,474.34,328.19,7.86;13,152.39,485.30,172.45,8.11" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="13,264.34,474.34,124.76,7.86">Going deeper with convolutions</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno>CoRR abs/1409.4842</idno>
		<ptr target="http://arxiv.org/abs/1409.4842" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,496.26,337.98,7.86;13,152.39,507.22,328.20,7.86;13,152.39,518.18,25.60,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="13,417.63,496.26,62.97,7.86;13,152.39,507.22,86.93,7.86">Selective search for object recognition</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,247.20,507.22,165.10,7.86">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,529.14,337.97,7.86;13,152.39,540.10,328.20,7.86;13,152.39,551.06,328.20,7.86;13,152.39,562.01,328.20,7.86;13,152.39,572.97,126.65,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,398.77,551.06,81.82,7.86;13,152.39,562.01,142.51,7.86">General Overview of ImageCLEF at the CLEF 2015 Labs</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bromuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">K</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Uskudarli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">B</forename><surname>Marvasti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Aldana</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Del Mar Roldán</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>García</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="13,301.64,562.01,138.74,7.86">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
