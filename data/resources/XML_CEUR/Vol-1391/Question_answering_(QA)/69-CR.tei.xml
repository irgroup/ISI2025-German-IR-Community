<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,141.97,115.90,331.43,12.90;1,187.39,133.83,240.59,12.90">HAWK@QALD5 -Trying to answer hybrid questions with various simple ranking techniques</title>
				<funder ref="#_7SafYj8">
					<orgName type="full">BMBF</orgName>
				</funder>
				<funder ref="#_Dqx2ZYP">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,203.28,171.89,63.91,8.64"><forename type="first">Ricardo</forename><surname>Usbeck</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Leipzig</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,291.03,171.89,116.57,8.64"><forename type="first">Axel-Cyrille</forename><forename type="middle">Ngonga</forename><surname>Ngomo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Leipzig</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,141.97,115.90,331.43,12.90;1,187.39,133.83,240.59,12.90">HAWK@QALD5 -Trying to answer hybrid questions with various simple ranking techniques</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C01A4BEDA4731611677E5CDD568CA400</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The growing amount of data available in the Document Web as well as in the Linked Data Web has lead to an information gap. Information needed to answer complex questions might often require full-text data as well as Linked Data. Thus, HAWK combines unstructured and structured data sources. In this article, we introduce HAWK, a novel entity search approach for hybrid question answering based on combining Linked Data and textual data. In this article, we compare three ranking mechanism and evaluate their performance on the QALD-5 challenge. Finally, we identify the weak points of our current version of HAWK and give directions for future development.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The fifth challenge on Question Answering over Linked Data (QALD-5) <ref type="foot" coords="1,434.00,388.50,3.49,6.05" target="#foot_0">1</ref> has introduced and extended a novel benchmark dataset for hybrid question answering. In this paper, we present our framework HAWK, the second best performing system w.r.t. hybrid question answering. The need for this challenge becomes obvious by comparing the growing amount of data, in the Document Web and the Linked Data Web, which introduces an information gap, i.e., a considerable number of complex questions can only be answered by using hybrid question answering approaches, which can find and combine information stored in both structured and textual data sources <ref type="bibr" coords="1,418.51,473.86,10.58,8.64" target="#b3">[4]</ref>.</p><p>In this paper, we will outline the single steps performed by HAWK to answer hybrid questions in Section 2. Section 3 discusses problems and opportunities within the current implementation. Finally, we conclude in Section 4 and explain future research directions. The source code of HAWK, benchmark data, a link to the demo, evaluation results as well as further information can be retrieved from the project website http://aksw.org/Projects/hawk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>In the following, we will shortly describe the 8-step, modular pipeline of HAWK absed on the following running example: Which anti-apartheid activist was born in Mvezo? For more information please have a look at the full method description <ref type="bibr" coords="1,172.12,637.38,10.58,8.64" target="#b4">[5]</ref>.</p><p>1. Segmentation and Part-of-Speech (POS)-Tagging. To be generic with respect to the language of the input question, HAWK uses a modular system that is able of tokenizing even languages without clear separation like Chinese. For English input questions our system relies on the clearNLP <ref type="bibr" coords="2,303.98,155.18,13.45,8.64" target="#b0">[1]</ref>-framework which provides a.o. a white space tokenizer, POS-tagger and transition-based dependency parsing. Afterwards, this framework annotates each token with its POS-tag which will be later used to identify possible semantic annotations. POS-tagging on the running example will result in the following: Which(WDT) anti-apartheid(JJ) activist(NN) was(VBD) born(VBN) in(IN) Mvezo(NNP)?</p><p>2. Entity Annotation. Next, our approach identifies named entities and tries to link them to the underlying knowledge base. The QALD-5 challenge relies on the DBpedia <ref type="bibr" coords="2,149.19,259.57,11.62,8.64" target="#b1">[2]</ref> as source for structured information in the form of Linked Data. For recognizing and linking named entities HAWK's default annotator is FOX <ref type="bibr" coords="2,387.97,271.53,10.58,8.64" target="#b2">[3]</ref>, a federated knowledge extraction framework based on Ensemble Learning. FOX has shown to outperform other entity annotation systems on the QALD 4 benchmark data <ref type="bibr" coords="2,393.33,295.44,10.58,8.64" target="#b4">[5]</ref>. An optimal annotator would annotate our running example Mvezo with http://dbpedia.org/ resource/Mvezo.</p><p>3. Dependency Parsing and Noun Phrase Detection. Subsequently, in order to capture linguistic and semantic relations, HAWK parses the query using dependency parsing and semantic role labeling <ref type="bibr" coords="2,277.79,363.96,10.58,8.64" target="#b0">[1]</ref>. The dependency parser generates a predicateargument tree based on the preprocessed question. Afterwards, HAWK identifies noun phrases, i.e., semantically meaningful word groups not yet recognized by the entity annotation system, using the result of the POS tagging step. Input tokens are combined following manually-crafted linguistic heuristics based on POS-tag sequences derived from the QALD 5 benchmark questions. Two domain experts implemented the deduced POS-tag sequences and safeguarded the quality of this algorithm w.r.t. the QA pipeline f-measure. The full algorithm can be found in our source code repository at https: //github.com/aksw/hawk. Here, the anti-apartheid activist would be detected as noun phrase.</p><p>4. Linguistic Pruning. HAWK has now captured entities from a given knowledge base, noun phrases as well as the semantic structure of the sentence. Still, this structure contains tokens that are meaningless for retrieving the target information or even introduce noise into the process. Thus, HAWK prunes nodes from the predicate-argument tree based on their POS-tags, e.g., deleting all DET nodes, interrogative phrases such as Give me or List, and auxiliary tokens such as did. The linguistically pruned dependency tree with combined noun phrases for our running example would only contain born as a root node with two children, namely anti-apartheid activist and http://dbpedia.org/resource/Mvezo. 5. Semantic Annotation. Now, the tree structure contains only semantically meaningful (combined) token and entities, i.e, individuals from the underlying knowledge base. To map the remaining token to properties and classes from the target knowledge base and its underlying ontology, our framework uses information about possible verbalizations of ontology concepts and leverages a fuzzy string search. These verbal-izations are based on both rdfs:label<ref type="foot" coords="3,306.34,117.64,3.49,6.05" target="#foot_1">2</ref> information from the ontology itself and (if available) verbalization information contained in lexica, in our case in the existing DBpedia English lexicon <ref type="foot" coords="3,234.51,141.55,3.49,6.05" target="#foot_2">3</ref> . After this step, either a node is annotated with a class, property or individual from the target knowledge base or it causes a full-text lookup in the targeted Document Web parts. With respect to the running example born would be annotated with the properties dbo:birthPlace and dbo:birthDate.</p><p>6. Generating hybrid SPARQL Queries. Given a (partly) annotated predicate argument, HAWK generates hybrid SPARQL queries. It uses an Apache Jena FUSEKI <ref type="foot" coords="3,476.61,201.33,3.49,6.05" target="#foot_3">4</ref>server, which implements the full-text search predicate text:query on a-priori defined literals. Those literals are basically mappings of textual information to a certain individual URI from a target knowledge, i.e., an implicit enrichment of structured knowledge from unstructured data.</p><p>Especially, our framework HAWK indexes a-priori the following information per individual: dbo:abstract, rdfs:label, dbo:redirect and dc:subject to capture document based information. This information is then retrieved by the text:query predicate by using exact matches or fuzzy matches on each non-stopword token of an indexed field.</p><p>The generation of SPARQL triple pattern is based on a pre-order walk to reflect the empirical observation that i) related information is situated close to each other in the tree and ii) information is more restrictive from left to right. This breadth-first search visits each node and generates several possible triple patterns based on the number of annotations and the POS-tag itself. With this approach HAWK is independent of SPARQL templates and to work on natural language input of any length and complexity. Each pattern contains at least one variable from a pre-defined set of variables, i.e., ?proj for the resource projection variable, ?const for resources covering constraints related to the projection variable as well as a variety of variables for predicates to inspect the surrounding of elements in the knowledge base graph. During this process, each iteration of the traversal appends the generated patterns to each of the already existing SPARQL queries. This combinatorial effort results in covering every possible SPARQL graph pattern given the predicate-argument tree. Amongst others, HAWK generates for the running example the following three hybrid SPARQL queries:</p><p>1. SELECT ?proj {?proj text:query 'anti-apartheid activist'.</p><p>?proj dbo:birthPlace dbr:Mvezo.} 2. SELECT ?proj {?proj text:query 'anti-apartheid activist'.</p><p>?proj dbo:birthDate dbr:Mvezo.} 3. SELECT ?proj {?proj text:query 'anti-apartheid activist'.</p><p>?const dbo:birthPlace ?proj.} 7. Semantic Pruning of SPARQL Queries. Covering each possible SPARQL graph pattern with the above algorithm results in a large number of generated SPARQL queries.</p><p>To effectively handle this large set of queries and reduce the computational effort, HAWK implements various methods for pruning. For example, it assumes that unconnected query graphs, missing projection variables and cyclic SPARQL triple pattern lead to empty or semantically meaningless results. Thus, HAWK discards those queries.</p><p>In the running example, the semantic pruning discards query number two from above because it violates the range restriction of the dbo:birthDate predicate. Although semantic pruning drastically reduces the amount of queries, it often does not result in only one query. Thus, a ranking of the remaining queries is applied before the best SPARQL query is send to the target triple store.</p><p>8. Ranking and Cardinality. For the QALD-5 challenge, we extended the basic ranking implementations -optimal ranking and feature-based ranking -of HAWK by one additional ranking method:</p><p>-Optimal Ranking. To ensure, we are able to generate hybrid SPARQL queries capable of answering the benchmark questions, the optimal ranker returns always those hybrid SPARQL queries which lead to a maximum f-measure. Obviously, the optimal ranking can only be used if the answers are know, i.e., HAWK operates on the training data. This ranking functions allows to determine the parts of the hybrid question answering pipeline which do not perform well. -Feature-based Ranking. The second ranking method is based on supervised learning using the gold standard answer set from the QALD-4 benchmark. In the training phase, all generated queries are run against the underlying SPARQL endpoint.</p><p>Comparing the results to the gold standard answer set, HAWK stores all queries resulting with the highest F-measures. Afterwards the stored queries are used to calculate an average feature vector comprising simple features mimicking a centroidbased cosine ranking. HAWK's ranking calculation comprises the following components:</p><p>• NR OF TERMS calculates the number of nodes used to form the full-text query part as described above. • NR OF CONSTRAINTS counts the amount of triple patterns per SPARQL query. • NR OF TYPES sums the amount of patterns of the form ?var rdf:type cls. • PREDICATES generates a vector containing an entry for each predicate used in the SPARQL query. While running the test phase of HAWK, the cosine similarity between each SPARQL query using the above mentioned features and the average feature vector of training queries is calculated to rank them.</p><p>-Overlap-based Ranking. The novel overlap-based ranking accounts for the intuition that the same result set can be generated by several hybrid SPARQL queries. Thus, this ranker, although computationally highly expensive, executes every hybrid SPARQL query and the resulting answer sets are then stored into hashed buckets. Finally, the ranker computes how many queries produced a certain answer set.</p><p>The answer set with the highest number is than returned.</p><p>Moreover, HAWK determines the target cardinality x, i.e., LIMIT x respectively the number of answers expected for a given query, of each query using the indicated car-dinality of the first seen POS-tag, e.g., the POS-tag NNS demands the plural while NN demands the singular case and thus leads to different x.</p><p>An optimal ranking will reveal that the winning SPARQL query for our running example is SELECT ?proj {?proj text:query 'anti-apartheid activist'. ?proj dbo:birthPlace dbr:Mvezo.}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation and Discussion</head><p>The QALD-5 benchmark has a training and a test dataset for question answering containing a subset of hybrid benchmark questions. In particular, HAWK is currently only capable of answering questions demanding a single or a set of URIs from a target knowledge base. Moreover, questions depending on Yago ontology 5 types cannot be answered. Thus, the QALD-5 dataset contains 26 training, respectively 8 test questions, suitable for the current implementation of HAWK. Using the online available evaluation tool 6 , Table <ref type="table" coords="5,184.83,292.48,4.98,8.64" target="#tab_0">1</ref> shows the results for the training and test dataset as well as well as for all three ranking approaches. Please note, that for the feature-based ranker the training data was taken from QALD-4. As can be seen in Table <ref type="table" coords="5,243.28,449.46,3.74,8.64" target="#tab_0">1</ref>, the implemented ranking function do not reach an optimal ranking implementation. Moreover, no implementation is able to discard whole queries, i.e., they are currently not aware of the possibility that the answer could not have been retrieved at all while the optimal ranker discards questions with incorrect answer sets.</p><p>Table <ref type="table" coords="5,174.72,497.28,4.98,8.64">2</ref> shows a detailed description of achieved measures per question and algorithm. Delving deeper into the results, we color-coded the single mistakes of the HAWK system. (Yellow) indicates that the ranking algorithms are not able to retrieve the correct SPARQL query out of the set of generated SPARQL queries. (Orange) cells point out that one ranking algorithm performs worse than the other. (Red) indicates the inability to retrieve correct answer sets. That is, HAWK is able to generate a set of SPARQL queries but amongst them none retrieves a correct answer set. Finally, (Blue) rows describe questions where HAWK is unable to generate at least one SPARQL query. Thus, those questions semantics cannot be captured by the system yet due to missing surface forms for individuals, classes and properties or missing indexed full-text information. Especially, in the test dataset all three ranking algorithms are only able to generate one correct answer while the feature-based and the overlap-based ranker perform differently on the train dataset. To experience the run times for single queries please visit our demo at http://hawk.aksw.org.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we briefly introduced HAWK, the first hybrid QA system for the Web of Data, and analysed its performance against the QALD 5 challenge. We showed that by using a generic approach to generate SPARQL queries from predicate-argument structures, HAWK is able to achieve an F-measure of up to 0.3 on the QALD-5 training benchmark. These results demand novel approaches for capturing the semantic meanings of natural language questions with hybrid SPARQL queries. Our work on HAWK, however, revealed several other open research questions, of which the most important lies in finding the correct ranking approach to map a predicate-argument tree to a possible interpretation. So far, our experiments reveal that the mere finding of the right features for this endeavor remains a challenging problem. Finally, several components of the HAWK pipeline are computationally very complex. Finding more time-efficient algorithms for these steps will be addressed in future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,134.77,348.61,345.82,71.16"><head>Table 1 :</head><label>1</label><figDesc>Results of the QALD-5 challenge for different ranking algorithms. Number in brackets show the amount of generated answers, i.e., HAWK outputs at least one result set.</figDesc><table coords="5,150.22,385.07,312.69,34.69"><row><cell>Dataset</cell><cell cols="3">Optimal Ranking Feature-based Ranking Overlap-based Ranking</cell></row><row><cell cols="2">QALD-5 -training 0.30 (15 out of 26)</cell><cell>0.06 (22 out of 26)</cell><cell>0.08 (22 out of 26)</cell></row><row><cell>QALD-5 -test</cell><cell>0.1 (1 out of 10)</cell><cell>0.10 (3 out of 10)</cell><cell>0.10 (3 out of 10)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,657.93,232.33,6.31"><p>http://www.sc.cit-ec.uni-bielefeld.de/qald/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,144.73,601.86,335.86,7.77;3,144.73,612.82,335.37,7.77;3,144.73,623.78,335.86,7.77;3,144.73,635.58,123.74,6.31"><p>We assume dbo stands for http://dbpedia.org/ontology, dbr for http: //dbpedia.org/resource/, rdfs for http://www.w3.org/2000/01/rdfschema#, dc for http://purl.org/dc/elements/1.1/ and text for http: //jena.apache.org/text#</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,139.00,644.05,177.88,9.02"><p>3 https://github.com/cunger/lemon.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="3,316.88,646.75,37.66,6.31;3,139.00,655.22,274.72,9.02"><p>dbpedia 4 http://jena.apache.org/documentation/serving_data/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_4" coords="7,148.02,694.75,31.68,6.05"><p>still active?</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments This work has been supported by the <rs type="programName">FP7</rs> project <rs type="projectName">GeoKnow</rs> (<rs type="grantNumber">GA No. 318159</rs>) and the <rs type="funder">BMBF</rs> project <rs type="projectName">SAKE</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_7SafYj8">
					<idno type="grant-number">GA No. 318159</idno>
					<orgName type="project" subtype="full">GeoKnow</orgName>
					<orgName type="program" subtype="full">FP7</orgName>
				</org>
				<org type="funded-project" xml:id="_Dqx2ZYP">
					<orgName type="project" subtype="full">SAKE</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="6,138.13,427.50,342.46,7.77;6,146.47,438.30,99.36,7.93" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,246.71,427.50,218.14,7.77">Getting the most out of transition-based dependency parsing</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">D</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,146.47,438.30,13.81,7.73">ACL</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="687" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.13,449.42,342.46,7.77;6,146.47,460.37,334.12,7.77;6,146.47,471.17,237.61,7.93" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,320.28,460.37,160.31,7.77;6,146.47,471.33,125.83,7.77">DBpedia -a large-scale, multilingual knowledge base extracted from wikipedia</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Isele</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jentzsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kontokostas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hellmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Morsey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Van Kleef</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bizer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,279.29,471.17,78.63,7.73">Semantic Web Journal</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.13,482.13,342.46,7.93;6,146.47,493.25,20.17,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,265.96,482.29,172.80,7.77">Ensemble learning for named entity recognition</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Speck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A.-C</forename><forename type="middle">N</forename><surname>Ngomo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,457.43,482.13,18.53,7.73">ISWC</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.13,504.05,342.47,7.93;6,146.47,515.01,84.18,7.93" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,192.75,504.21,223.32,7.77">Combining Linked Data and Statistical Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Usbeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,435.93,504.05,44.66,7.73;6,146.47,515.01,57.45,7.73">11th ESWC, PhD Symposium</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.13,526.13,342.46,7.77;6,146.47,536.93,167.22,7.93;7,196.89,117.67,221.59,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,383.81,526.13,96.78,7.77;6,146.47,537.09,102.38,7.77">HAWK -Hybrid Question Answering over Linked Data</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Usbeck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A.-C</forename><surname>Ngonga Ngomo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bühmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Unger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,265.62,536.93,20.52,7.73">ESWC</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Table 2: Detailed results of HAWK at the QALD-5 challenge</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
