<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,162.67,115.90,290.01,12.68;1,175.65,133.83,264.06,12.68">QAnswer -Enhanced Entity Matching for Question Answering over Linked Data</title>
				<funder ref="#_vr2vVab">
					<orgName type="full">Romanian Ministry of European Funds</orgName>
				</funder>
				<funder ref="#_pHry9tb #_ahdWWKR">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,161.49,171.87,58.57,8.80"><forename type="first">Stefan</forename><surname>Ruseti</surname></persName>
							<email>stefan.ruseti@cs.pub.ro</email>
							<affiliation key="aff0">
								<orgName type="institution">University Politehnica of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">TeamNet International</orgName>
								<address>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,236.95,171.87,73.93,8.80"><forename type="first">Alexandru</forename><surname>Mirea</surname></persName>
							<email>alexandru.daniel.mirea@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University Politehnica of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,321.43,171.87,68.78,8.80"><forename type="first">Traian</forename><surname>Rebedea</surname></persName>
							<email>traian.rebedea@cs.pub.ro</email>
							<affiliation key="aff0">
								<orgName type="institution">University Politehnica of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">TeamNet International</orgName>
								<address>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,426.47,171.87,27.40,8.80;1,274.41,183.83,62.06,8.80"><forename type="first">Stefan</forename><surname>Trausan-Matu</surname></persName>
							<email>stefan.trausan@cs.pub.ro</email>
							<affiliation key="aff0">
								<orgName type="institution">University Politehnica of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,162.67,115.90,290.01,12.68;1,175.65,133.83,264.06,12.68">QAnswer -Enhanced Entity Matching for Question Answering over Linked Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2F23C8433DE4587998EF204D5604B89A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>QAnswer is a question answering system that uses DBpedia as a knowledge base and converts natural language questions into a SPARQL query. In order to improve the match between entities and relations and natural language text, we make use of Wikipedia to extract lexicalizations of the DBpedia entities and then match them with the question. These entities are validated on the ontology, while missing ones can be inferred. The proposed system was tested in the QALD-5 challenge and it obtained a F1 score of 0.30, which placed QAnswer in the second position in the challenge, despite the fact that the system used only a small subset of the properties in DBpedia, due to the long extraction process.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Question answering systems have been considered an important goal of Artificial Intelligence and a subject of research for many years. While the traditional way of searching for information is most of the times based on keywords, a much more natural way of searching for information is using natural language questions. Also, instead of generating a list of documents where the answer can be found, such a system would return a precise answer.</p><p>Usually, question answering systems either use a knowledge base with structured information, or try to extract the answer from free text, or employ a combination of these two approaches. Knowledge bases contain more precise information, but cannot cover all the possible questions. In order for a system to answer a question, it must first be able to "understand" the meaning of the question. This part usually consists of translating the natural language in a form of structured information, which is used later for querying a knowledge base.</p><p>Our system uses the structured knowledge base DBpedia and a Wikipediabased approach to match phrases from the question to entities in the ontology. Different solutions for matching each type of entity were developed and the most probable interpretation is converted in a SPARQL query. Missing properties or types can also be inferred if they were not matched in the previous step. This paper will describe in the next section other state-of-the-art systems that participated in the QALD competition. Section 3 presents the general architecture of our approach and describes each of its steps. The obtained results in the QALD-5 challenge are included in section 4, along with analysis for some of the questions the system answered incorrectly. In the end, we present the conclusions of this paper and some future development directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>As QAnswer uses a knowledge base for question answering (QA), the focus of this section will be on QA systems and approaches that employ ontologies or other sources of structured knowledge to generate the answer. One of the most important competitions for this type of systems is Question Answering over Linked Data -QALD <ref type="foot" coords="2,228.99,277.97,3.97,6.16" target="#foot_0">3</ref> , which is part of the Question Answering lab at CLEF. This competition, already at the fifth edition, provides valuable datasets with questions and answers that can be used to evaluate and compare such systems. For one of the tasks, the questions are chosen so that they could be answered by using DBpedia resources, so this task was perfectly suited to evaluate our system.</p><p>The most successful system in QALD-4, but also QALD-5, was Xser <ref type="bibr" coords="2,447.26,351.26,9.96,8.80" target="#b5">[6]</ref>. The system uses a two-layered architecture, where the first step produces a Directed Acyclic Graph (DAG) from the question with phrases labelled as resource, relation, type or variable, by using a structured perceptron <ref type="bibr" coords="2,397.35,387.12,9.96,8.80" target="#b1">[2]</ref>. These tags are independent of any knowledge base, so their approach can be very easily applied on a different ontology. The second step of their system maps the discovered entities to the ones in a given knowledge base, such as DBpedia, by using a Na√Øve Bayes approach. The downside of their approach is the need of a large annotated corpus of questions with DAGs.</p><p>Another interesting solution was used by gAnswer <ref type="bibr" coords="2,372.41,458.85,14.61,8.80" target="#b13">[14]</ref>, which uses a graphdriven approach. The disambiguation takes place at the evaluation step, when the generated graph is matched with subgraphs in the ontology. While usually expressions are extracted for a given property, they use a database with common expressions <ref type="bibr" coords="2,185.71,506.68,15.49,8.80" target="#b11">[12]</ref> and then they map them to paths in the ontology. This way, more complex ontology constructs can be linked with expressions.</p><p>Casia <ref type="bibr" coords="2,176.30,530.59,10.51,8.80" target="#b4">[5]</ref> uses a similar approach for matching properties based on an expressions database. However, the system chooses between all possible phrases and their interpretation in the same step by using a Markov Logical Network <ref type="bibr" coords="2,455.60,554.50,14.61,8.80" target="#b12">[13]</ref>.</p><p>There are also other types of systems for question answering, which do not use a knowledge base, but rather try to select the most appropriate answer for a given question from texts available online. This kind of systems are usually able to answer a greater variety of questions, but they are not as precise. The systems which combine these two strategies are called hybrid systems, and are probably the most successful. The best example is IBM Watson <ref type="bibr" coords="2,372.53,626.23,9.96,8.80" target="#b2">[3]</ref>, who was able to beat the best human players in the Jeopardy game.</p><p>The system has a pipeline architecture, described in Fig. <ref type="figure" coords="3,386.81,141.80,3.87,8.80">1</ref>. The pipeline starts from the natural language question and ends with the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 1: The System Architecture</head><p>The answer is provided based on the generated SPARQL query, which is executed on a Virtuoso-opensource<ref type="foot" coords="3,289.79,400.34,3.97,6.16" target="#foot_1">4</ref> endpoint, loaded with the DBpedia 2014 dump <ref type="foot" coords="3,159.67,412.29,3.97,6.16" target="#foot_2">5</ref> . In order to be able to generate the query, two conditions are needed: the DBpedia entities must be discovered in text, and they have to be correctly linked. The links between the entities can only be based on syntactic dependency links between words. Because of this, the first step in the pipeline is the question parsing which is done using the Stanford CoreNLP library <ref type="foot" coords="3,389.33,460.11,3.97,6.16" target="#foot_3">6</ref> .</p><p>After this first step, a directed graph is generated, with vertices corresponding to tokens in the question, which are annotated with lemma and part-of-speech tags, and edges corresponding to the collapsed dependencies generated by Stanford CoreNLP <ref type="bibr" coords="3,201.09,509.49,9.96,8.80" target="#b8">[9]</ref>. Also, numbers and dates are detected in this step by using Stanford NER<ref type="foot" coords="3,197.04,519.89,3.97,6.16" target="#foot_4">7</ref> , a named entity recognition module.</p><p>Once the graph is generated, DBpedia entities must be detected. There are three types of entities in the ontology: individuals, types and properties. Methods for detecting each type of entity were developed, because of the particularities of each type. During these steps, multiple graphs will be generated because of the different possible matches, some of them containing multiple words. In the end, only one graph will be selected based on some scores as will be explained in the following sections.</p><p>The most probable graph enters the last stages of the pipeline, where missing entities are inferred, while existing ones are validated using the ontology. The SPARQL query is generated in the last step, creating triples and subqueries based on the structure of the graph and the direction of the properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Individual Detection</head><p>The detection of individuals from the ontology is probably the simplest task out of the three types of detection tasks, because the list of possible ways of expressing an individual is very limited. In the same time, it is a very important step, because individuals are much more specific than types and properties, which could be inferred in some cases based on the individuals discovered in this first stage.</p><p>In the general form, the named entity recognition (NER) task is a more difficult problem than the task at hand. This happens because the list of possible entities is usually open (almost anything can be a name). In our case, the entities must be part of DBpedia, any other match being useless. For each possible individual to match, most of the ways in which it can be expressed already exist in DBpedia, in the form of Wikipedia redirects. Although this approach might loose some expressions for certain individuals, it works well enough in most cases.</p><p>First, the individuals are filtered by their "importance", so we can ignore very unlikely candidates which can have a negative effect on the precision. The importance is estimated by the number of Wikipedia pages that have links to the individual's page. For each such individual, all the redirects that contain only ASCII characters are kept.</p><p>All these expressions are inserted in a trie data structure for words, in order to perform very fast searches. When searching for matches, the longest sequence of words that match is kept and for each sequence, the individuals are sorted based on how close their label is to the sequence of words, measured with the edit distance <ref type="bibr" coords="4,193.48,465.20,9.96,8.80" target="#b6">[7]</ref>, and also by their importance.</p><p>Matched sequences of words can also overlap, each sequence being linked to a different individual. In this case, there is no way we can decide at this point which is the correct one, so the current graph is replaced by two different ones. Also, the same sequence of words might produce different graphs if the possible individuals that match have very different types. The type of each individual is important in the properties matching, which will be described later.</p><p>Another rule added to improve accuracy is the idea of a "strong" link. These links correspond to a sequence of more than one word, where each word that is not a preposition or conjunction is written with capital letter. These cases strongly indicate a correct match, so any other possible sequence that overlaps with it, or possible type or property match, will be ignored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Type Detection</head><p>Although the number of types in DBpedia is much smaller than the number of individuals, their detection is harder because the ways of expressing them is not known a-priori, and because most of them are common words. Since for types we don't have Wikipedia redirects anymore, a new method of determining possible expressions needed to be found.</p><p>The "na√Øve" solution. Most of the times, a type appears in text by its label, which makes it very easy to discover. Starting from this, a database can be generated with entries containing pairs of types and word. Besides the actual label, synonyms of the label extracted from WordNet <ref type="bibr" coords="5,372.57,204.90,15.49,8.80" target="#b9">[10]</ref> are also added. This approach discovers many of the correct matches, but it is hard to apply on types whose label contain more than one word.</p><p>A Wikipedia-based approach. In order to overcome the shortcomings of the "na√Øve" approach, a more general solution was needed. Because of the strong connection between DBpedia and Wikipedia, each Wikipedia article corresponds to a DBpedia individual, who also has a type, which means that the page should contain a formulation of that type within the text. Luckily, the type is most of the times expressed in the first sentence of each article, which provides a short description for each individual.</p><p>Since the majority of the first sentences look like "&lt;individual&gt; is a &lt;type&gt;", the complement of the copulative verb should be an expression for that type. Some of the words connected with the complement of the copulative verb are also important for the expression of the type. Because of this, a subgraph containing vertices that can be reached starting from the type vertex by going only on edges corresponding to dependencies of a modifier type is generated. The words from this subgraph are memorized as a way of expressing the type.</p><p>We have found that, while some of the extra words are important for that type ( e.g. "formula one racing driver"), others don't add any extra information (e.g. "English actor"). However, using all these possibilities might alter the results or might include in the type words that are important for the question in other ways. These problems and others are solved in a later step using a method that removes information that is too specific or too general.</p><p>The remaining entries are added in another trie-like structure, where edges can be lemmas or whole words. When queried, the word edges have priority, but lemmas are used when matches are not found. This permits, for example, the existence of the plural form for some words. Type detection is applied for each resulted graph from the individual detection phase, so at least the same number of graphs result from this step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Property Detection</head><p>Out of the three types of DBpedia resources, properties are arguably the most difficult to detect. This is because there are more properties than types and most of them are complex and contain more than one word. Moreover, applying the same approach as for types is tedious because properties can appear anywhere in a Wikipedia article, and they can appear in many types of sentences, which makes developing rules for words extraction more difficult.</p><p>Extraction. Because of the strong connection to DBpedia, Wikipedia is a good source for pattern extraction, and was used for this purpose by systems like WikiFramework <ref type="bibr" coords="6,207.28,142.84,10.51,8.80" target="#b7">[8]</ref> and BOA <ref type="bibr" coords="6,265.26,142.84,9.96,8.80" target="#b3">[4]</ref>. We propose a similar approach, that also uses syntactic dependencies to select the pattern from the sentence.</p><p>Because dependency parsing is quite slow, only the sentences from a Wikipedia page where a property could occur were considered. This was achieved by finding sentences where both the label of the individual and the value of the property were found. For data properties, more possible values were looked for, especially for dates which have many possible formats. Although a lot of relevant sentences are ignored because the individual appears most of the times as a reference, not by label, using a coreference system would slow this task even more, and we concluded not to use coreference resolution at this point in our system.</p><p>From each selected sentence, the path between the discovered subject and object should be an expression that corresponds to the property between them. This is not always the case because other related words might be relevant for the meaning of the property, while sometimes, some words from the path might not be relevant. Starting from the path, other words, that could be reached from the ones on the path by going on edges that usually represent modifier dependencies, are added to the expression.</p><p>Sometimes, the extracted expression contains a lot of unnecessary words. An example of such a case is "Judd Trump is an English professional snooker player from Bristol and former world number one". This is a very common situation, where the individual is first defined, and then property value is connected to the definition. In this case, the individual is considered the word "player" so the extracted expression will be "from", which is much more relevant than "is an English professional snooker player from".</p><p>Matching. A subgraph is considered to match an expression if it has all the words in the expression, regardless of their order. Also, the words are compared by their lemma and POS tag, so all variations of a word can match. In this form, this problem is a subgraph matching one.</p><p>Finding all the possible matches is done by starting from each word in the sentence. An inverse index contains all the extracted expressions that contain a certain word, so a possible list of expressions is extracted fast. For each expression in the list, a search is performed starting from the vertex to check if all words in the expressions form a subgraph in the query graph. The validated expressions enter the next step of the algorithm.</p><p>A score is computed for each matched expression, based on the types that it connects. In the general case, the subgraph connects two words, which might have corresponding types. The type can be a detected type, the type of a detected individual or the type of a question word (who -Agent, when -Date, how many -Integer, etc.). Both types are then compared with the ones from the database in order to compute the score.</p><p>Considering that the types of the matched resource are (t 1 , t 2 ) and the types from the database are (t 1 , t 2 ), the distance between them is computed as the sum of the distances between t 1 and t 1 and between t 2 and t 2 . The distance between two types is computed as the number of nodes between them in the DBpedia taxonomy. There are two distinct taxonomies, one for resource classes, and one for data types. The distance between 2 classes from different taxonomies is considered maximum. Based on the computed distance between the pairs of types, the score of the matching is calculated with the formula (1):</p><formula xml:id="formula_0" coords="7,217.86,201.46,262.73,28.26">S(e, tp, p) = tp N (e, tp , p) N (e, tp ) * 2 -dist(tp,tp )<label>(1)</label></formula><p>where:</p><p>e = the matched expression tp = the type pair (t 1 , t 2 ) p = the property tp = a type pair for property p and expression e in the database -N = the number of appearances in the database for the given parameters</p><p>Because the same property appears in the database for different type pairs, the score of the match is summed for all occurrences. Also, any entry with the distance larger than a threshold is ignored. In some cases, the expression in the sentence does not occur between two types. One such example is the question "How often did Jane Fonda marry?". Here, marry has only one connected type, which is Person, because how many cannot be linked with a type. Another similar case is represented by questions containing superlative adjectives, like "Which is the highest mountain?", where the words that are supposed to be matched to a property have only one neighbour in the graph. For these cases, results that match only one type are included in the list of matches, but with a lower score than matches with two types.</p><p>Since the matches contain only sets of vertices from the query graph, prepositions cannot be captured by this method because they are mapped as dependencies in the graph. It would be useful to capture such matches too, because they appear quite often in questions. In order to match edges too, the same scoring for pairs of types is used, for every edge in the graph that is not included in another match and that exists in the database.</p><p>Graphs Generation. In the case of individuals and types, the matching process selects the longest sequence at discovery time and all smaller included sequences are ignored. However, in the case of properties, the match consists of a subgraph, so it is more difficult to determine which matches to ignore. All the detected matches must be distributed to as few graphs as possible, while having as many non-overlapping matches in each graph.</p><p>Determining if two matches can be in the same graph can be easily computed by checking if their lists of replaced elements are disjoint. By comparing all pairs of matches, an undirected graph can be constructed with vertices being the matches, and the existence of an edge between two vertices meaning that the two matches do not overlap. All maximum cliques in this new graph represent the sets of matches that should be grouped in the same graph. However, the problem of finding all maximal cliques is NP-complete. Several algorithms for this problem exist, but probably the Bron-Kerbosch algorithm <ref type="bibr" coords="8,416.32,154.80,10.51,8.80" target="#b0">[1]</ref> is the most famous one. The Bron-Kerbosch algorithm has a worst case complexity of O(3 n 3 ), which matches the maximum number of maximal cliques in a graph, which is 3 n 3 <ref type="bibr" coords="8,150.34,190.66,14.61,8.80" target="#b10">[11]</ref>.</p><p>The number of properties in a question depends on the number of vertices in the input graph, but generally it is very unlikely to be more than a couple (e.g. maximum 3-4 properties). This means that the size of the graph on which the search for the maximal cliques is performed is very small in most situations, so applying the algorithm won't cause any performance issues, although there isn't a theoretic upper limit for the size of the graph. In the vast majority of cases, only one property is detected per input question, so this algorithm is not even run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Graph Selection</head><p>The three separate detection steps produce more than one graph for complex questions, but testing every one of them on the ontology would take too much time. The rest of the steps in the pipeline are more complex because detected entities must be validated, while missing ones have to be inferred. All these actions require complex queries and the number of investigated graphs can be quite high. Also, the final results of each query are difficult to compare so it is hard to choose the best interpretation.</p><p>For all these reasons, QAnswer selects the highest scoring graph right after the matching steps and all the following steps will use only this graph. For each generated graph, a score has to be computed that reflects how well each match fits the text and how well the matches fit together. The first part is the sum of the scores for each matched entity. One problem is the fact that each type of property uses a different score formula because of the different matching methods applied. All these scores coming from different formulas are not trivially comparable because of the different range of the values, but also because some types of entities might be more important than others, depending on the question.</p><p>In the case of individuals, the only measure we can use is the importance of the entity in the DBpedia graph, which is computed as described in the previous sections. A logarithm is applied in order to make this score comparable to those for the other types. Also, the number of matched words increase the score, as it can be seen in the formula 2:</p><formula xml:id="formula_1" coords="8,180.41,623.30,300.18,9.71">S ind (I, match) = ln(1 + importance(I)) * nW ords(match)<label>(2)</label></formula><p>The same formula is applied for types, only by replacing the importance of an individual with the number of occurrences of a type in our database:</p><formula xml:id="formula_2" coords="9,162.44,141.39,318.16,9.71">S type (T, match) = ln(1 + occurences(T, match)) * nW ords(match)<label>(3)</label></formula><p>The property score is calculated as defined in formula 1. While the values for each pair of types in a property represents a probability, the sum over all existing pairs can produce results bigger than 1 for good matches, but usually lower than the scores for individuals or types. This is important because the certainty of a property match is lower than the one for the other types. Thus, we prefer to assign a lower importance to properties.</p><p>The graph score also contains bonuses for complete triples of (individual, property, individual) or (individual, property, type) that appear in the graph, because this account for a higher success rate. The final for the score is 4, where the triples are only the connected ones described above. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S(g) =</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">SPARQL generation</head><p>The SPARQL generation is the last step in the pipeline. This component will receive a query graph with all the possible entities set on the nodes and it has the job to generate a valid SPARQL query based on this graph. The process of generating the SPARQL query is done recursively from the root of the graph (the node with no incoming edges) downwards.</p><p>When a node is visited, it has access to the graph, the parent, the current SPARQL query and the triple constructed at the parent level.</p><p>At each step we take certain decisions of how to enrich the query based on the current triple that is being constructed, the current edge that has been visited and type of the current node.</p><p>Determining the answer type. In a QA system, the questions asked by the users can be of multiple types. They can ask for a list of things, what is the number of certain things, what is the date of an event, location questions etc.</p><p>The type of the question is determined by checking for patterns in the input. By default, we will list all the results of the SPARQL query. The other types of questions that we are checking for are the ones that need to count the number of results (e.g. :"How many . . . ?") or the ones that ask us to validate the query (e.g. "Is Berlin in Germany?").</p><p>An important part of the query generation process is accurately determining what the searched variable is. We are doing this process by analyzing the edges of the graph. Usually the searched element will be determined from the "nsubj" edges in the dependency graph. From the connected resources of this edge we look for edges that have a "det" (determiner) dependency connected to them. This highlights that the source connected to them is the one being searched.</p><p>The system was evaluated on the QALD-5 test corpus. Unlike other systems, ours did not use any training on the data so any differences between the results on the training and test data are probably due to the difficulty of the questions, and not the result of overfitting. However, any testing and changes of the system's parameters were performed on the training data and this may have caused overfitting on this training set. On the other hand, the results on the training set were slightly worse than those on the test set with about 0.27 global F1 score on test set compared to 0.30 global F1 on test set.</p><p>Table <ref type="table" coords="10,176.11,240.30,4.98,8.80" target="#tab_0">1</ref> presents the results of all the participating systems in the multilingual question answering over DBpedia task. Taking into account the global F-1 score, our system obtained the second best result after Xser, who outperformed all the other systems by far. Out of the 50 test questions, QAnswer only tried to answer those questions marked as "onlydbo" because possible expressions for types and properties were extracted only for the "dbpedia.org/ontology" ones. The system returns an answer for each of those questions, because it is hard to determine when an interpretation is incorrect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Questions Interpreted Incorrectly</head><p>The system might fail to produce the correct answer due to several reasons. Below, we present some of the problems identified after interpreting the results on the questions from the QALD-5 test dataset.</p><p>-"Which programming languages were influenced by Perl?"</p><p>Our system detected all the needed entities in the question, but generated a query with the wrong direction of the property influenced. This is expected to happen, since the property matching algorithm does not take into account the direction, which could be very clear in this case due to the word by.</p><p>-"Who killed John Lennon?"</p><p>The question was labeled with onlydbo, but the information needed to answer the question required the dbpedia.org/property/conviction property. Also, this question is very difficult to answer because the entity that needed to be detected was Death_of_John_Lennon, and not the much more probable John_Lennon.</p><p>-"Which artists were born on the same date as Rachel Stevens?" Although all the required entities were matched, our system is not yet able to answer questions where a comparison on dates is needed.</p><p>-"Who is the manager of Real Madrid?" Because of the long extraction process for properties, we have only searched for those properties that appeared in previous QALD datasets. The idea that those were the most important properties was not really true, as many new properties appeared in the training and test set. Those new properties cannot be detected by our system, so questions containing them, like the one above, were not interpreted correctly. We shall solve this problem in the next version of the system.</p><p>-"Give me the currency of China." Sometimes, the graph scoring formula indicated a wrong interpretation as the best one. In this case, currency was labeled as a type, instead of the property. In order to solve problems like this one, other ways of evaluating an interpretation must be found, or more than one graph should be evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We developed QAnswer -a question answering system that uses expressions extracted from Wikipedia to match entities in DBpedia. The solution can be generalized to other ontologies, but the properties extraction step must be executed again for the properties in these new ontologies. More, if an individual is not part of DBpedia, new text sources that describe it should be identified, since this means that it does not have a Wikipedia page either.</p><p>At the time the system was tested on the QALD-5 test dataset, only expressions for a small part of the properties in DBpedia were extracted. The process should be executed for other common properties too, so that a more relevant evaluation of our method can be performed. Also, the extraction process can be improved because a lot of irrelevant information appear in the extracted expressions, which may affect the results.</p><p>Methods for learning which graph interpretation is more plausible will be developed and tested in the future. Such methods could help us detect the correct type of an entity for a phrase, and not depend too much on graph scoring, which cannot reflect whether an interpretation makes sense or not.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="9,221.51,311.04,40.55,6.12;9,262.07,297.02,33.14,8.74;9,297.42,311.43,51.13,6.12;9,348.55,297.02,78.12,8.74"><head></head><label></label><figDesc>m matches S(m) + (x,y,z) triples S(x) * S(y) * S(z)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="10,136.16,319.72,302.96,84.42"><head>Table 1 :</head><label>1</label><figDesc>QALD-5 results</figDesc><table coords="10,136.16,335.05,302.96,69.09"><row><cell></cell><cell cols="6">Processed Right Partial Recall Precision F1 F1 Global</cell></row><row><cell>Xser (en)</cell><cell>42</cell><cell>26</cell><cell>7</cell><cell>0.72</cell><cell>0.74 0.73</cell><cell>0.63</cell></row><row><cell>QAnswer (en)</cell><cell>37</cell><cell>9</cell><cell>4</cell><cell>0.35</cell><cell>0.46 0.40</cell><cell>0.30</cell></row><row><cell>APEQ (en)</cell><cell>26</cell><cell>8</cell><cell>5</cell><cell>0.48</cell><cell>0.40 0.44</cell><cell>0.23</cell></row><row><cell>SemGraphQA (en)</cell><cell>31</cell><cell>7</cell><cell>3</cell><cell>0.32</cell><cell>0.31 0.31</cell><cell>0.20</cell></row><row><cell>YodaQA (en)</cell><cell>33</cell><cell>8</cell><cell>2</cell><cell>0.25</cell><cell>0.28 0.26</cell><cell>0.18</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="2,144.73,656.74,239.62,7.92"><p>http://greententacle.techfak.uni-bielefeld.de/~cunger/qald/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="3,144.73,623.87,200.85,7.92"><p>https://github.com/openlink/virtuoso-opensource</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2" coords="3,144.73,634.83,148.25,7.92"><p>http://downloads.dbpedia.org/2014/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3" coords="3,144.73,645.79,190.63,7.92"><p>http://nlp.stanford.edu/software/corenlp.shtml</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4" coords="3,144.73,656.74,203.91,7.92"><p>http://nlp.stanford.edu/software/CRF-NER.shtml</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work has been partly funded by the <rs type="programName">Sectorial Operational Programme Human Resources Development 2007-2013</rs> of the <rs type="funder">Romanian Ministry of European Funds</rs> through the <rs type="grantName">Financial Agreements</rs> <rs type="grantNumber">POSDRU/159/1.5/S/132397</rs> and by <rs type="grantNumber">POSDRU/155420 -PROSCIENCE</rs>. <rs type="person">Stefan Ruseti</rs> has also been awarded a student travel grant for <rs type="grantNumber">CLEF 2015</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_vr2vVab">
					<idno type="grant-number">POSDRU/159/1.5/S/132397</idno>
					<orgName type="grant-name">Financial Agreements</orgName>
					<orgName type="program" subtype="full">Sectorial Operational Programme Human Resources Development 2007-2013</orgName>
				</org>
				<org type="funding" xml:id="_pHry9tb">
					<idno type="grant-number">POSDRU/155420 -PROSCIENCE</idno>
				</org>
				<org type="funding" xml:id="_ahdWWKR">
					<idno type="grant-number">CLEF 2015</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,142.95,216.18,337.64,7.92;12,151.52,227.14,329.07,7.92;12,151.52,238.10,200.41,7.92" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,266.46,216.18,214.13,7.92;12,151.52,227.14,53.86,7.92">Algorithm 457: finding all cliques of an undirected graph</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kerbosch</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=362342.362367" />
	</analytic>
	<monogr>
		<title level="j" coord="12,217.56,227.14,133.08,7.92">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="575" to="577" />
			<date type="published" when="1973-09">Sep 1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.95,248.38,337.64,7.92;12,151.52,259.34,329.07,7.92;12,151.52,270.30,329.08,7.92;12,151.52,281.26,329.07,7.92;12,151.52,292.22,209.62,7.92" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,213.27,248.38,267.32,7.92;12,151.52,259.34,9.64,7.92">Discriminative training methods for hidden Markov models</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=1118693.1118694" />
	</analytic>
	<monogr>
		<title level="m" coord="12,193.37,259.34,287.22,7.92;12,151.52,270.30,208.64,7.92">Proceedings of the ACL-02 conference on Empirical methods in natural language processing -EMNLP &apos;02</title>
		<meeting>the ACL-02 conference on Empirical methods in natural language processing -EMNLP &apos;02<address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002-07">Jul 2002</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.95,302.50,337.63,7.92;12,151.52,313.46,329.07,7.92;12,151.52,324.42,295.72,7.92" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,403.95,302.50,76.64,7.92;12,151.52,313.46,171.12,7.92">Building Watson: An overview of the DeepQA project</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ferrucci</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<ptr target="http://www.aaai.org/ojs/index.php/aimagazine/article/viewArticle/2303" />
	</analytic>
	<monogr>
		<title level="j" coord="12,335.01,313.46,56.10,7.92">AI magazine</title>
		<imprint>
			<biblScope unit="page" from="59" to="79" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.95,334.71,337.64,7.92;12,151.52,345.67,264.06,7.92" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="12,290.14,334.71,190.45,7.92;12,151.52,345.67,227.72,7.92">Bootstrapping the Linked Data Web. 1st Workshop on Web Scale Knowledge Extraction @ ISWC 2011</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Ngonga Ngomo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.95,355.95,337.63,7.92;12,151.52,366.91,329.07,7.92;12,151.52,377.87,129.75,7.92" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,302.39,355.95,178.20,7.92;12,151.52,366.91,139.04,7.92">CASIA @ V2 : A MLN-based Question Answering System over Linked Data</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<idno>No. 61272332</idno>
	</analytic>
	<monogr>
		<title level="m" coord="12,315.14,366.91,143.84,7.92">CLEF 2014 Working Notes Papers</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1249" to="1259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.95,388.16,337.64,7.92;12,151.52,399.12,329.07,7.92;12,151.52,410.07,76.78,7.92" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,360.30,388.16,120.29,7.92;12,151.52,399.12,161.93,7.92">Answering Natural Language Questions via Phrasal Semantic Parsing</title>
		<author>
			<persName coords=""><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,335.92,399.12,140.40,7.92">CLEF 2014 Working Notes Papers</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="333" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.95,420.36,337.64,7.92;12,151.52,431.32,329.07,7.92;12,151.52,442.28,212.22,7.92" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,238.36,420.36,242.23,7.92;12,151.52,431.32,101.14,7.92">Binary Codes Capable of Correcting Deletions, Insertions and Reversals</title>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">I</forename><surname>Levenshtein</surname></persName>
		</author>
		<ptr target="http://adsabs.harvard.edu/abs/1966SPhD...10..707L" />
	</analytic>
	<monogr>
		<title level="j" coord="12,266.34,431.32,107.45,7.92">Soviet Physics Doklady</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">707</biblScope>
			<date type="published" when="1966-02">Feb 1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.95,452.56,337.63,7.92;12,151.52,463.52,329.07,7.92;12,151.52,474.48,293.36,7.92" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,333.14,452.56,147.45,7.92;12,151.52,463.52,100.73,7.92">Acquiring Relational Patterns from Wikipedia: A Case Study</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mahendra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wanzare</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bernardi</surname></persName>
		</author>
		<ptr target="http://disi.unitn.it/bernardi/Papers/ltc.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="12,259.67,463.52,220.92,7.92;12,151.52,474.48,27.93,7.92">Proceedings of the 5th Language and Technology Conference</title>
		<meeting>the 5th Language and Technology Conference</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="111" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.95,484.77,337.64,7.92;12,151.52,495.73,329.07,7.92" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="12,278.83,484.77,151.58,7.92">Stanford typed dependencies manual</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Marneffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://nlp.stanford.edu/downloads/dependencies_manual.pdf" />
		<imprint>
			<date type="published" when="2008-09">September 2008. 2008</date>
			<biblScope unit="page" from="1" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,506.01,337.98,7.92;12,151.52,516.97,329.07,7.92;12,151.52,527.93,312.54,7.92" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,418.71,506.01,61.89,7.92;12,151.52,516.97,167.83,7.92">Introduction to WordNet: An On-line Lexical Database *</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Beckwith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">J</forename><surname>Miller</surname></persName>
		</author>
		<ptr target="http://ijl.oxfordjournals.org/content/3/4/235.short" />
	</analytic>
	<monogr>
		<title level="j" coord="12,326.44,516.97,154.15,7.92">International Journal of Lexicography</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="235" to="244" />
			<date type="published" when="1990-01">Jan 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,538.22,337.98,7.92;12,151.52,549.17,263.71,7.92" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,249.18,538.22,81.23,7.92">On cliques in graphs</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Moser</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF02760024</idno>
		<ptr target="http://link.springer.com/10.1007/BF02760024" />
	</analytic>
	<monogr>
		<title level="j" coord="12,337.68,538.22,120.88,7.92">Israel Journal of Mathematics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="28" />
			<date type="published" when="1965-03">Mar 1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,559.46,337.98,7.92;12,151.52,570.42,329.07,7.92;12,151.52,581.38,300.56,7.92" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,348.09,559.46,132.50,7.92;12,151.52,570.42,155.00,7.92">PATTY: A Taxonomy of Relational Patterns with Semantic Types</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Nakashole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<ptr target="http://dblp.uni-trier.de/db/conf/emnlp/emnlp2012.html#NakasholeWS12" />
	</analytic>
	<monogr>
		<title level="m" coord="12,316.20,570.42,66.40,7.92">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1135" to="1145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,591.66,337.98,7.92;12,151.52,602.62,294.07,7.92" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,277.81,591.66,88.94,7.92">Markov logic networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10994-006-5833-1</idno>
		<ptr target="http://link.springer.com/10.1007/s10994-006-5833-1" />
	</analytic>
	<monogr>
		<title level="j" coord="12,374.03,591.66,72.30,7.92">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="107" to="136" />
			<date type="published" when="2006-01">Jan 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,612.91,337.98,7.92;12,151.52,623.87,329.07,7.92;12,151.52,634.83,329.07,7.92;12,151.52,645.79,329.08,7.92;12,151.52,656.74,209.62,7.92" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,409.70,612.91,70.89,7.92;12,151.52,623.87,256.10,7.92">Natural language question answering over RDF: a graph data driven approach</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2588555.2610525" />
	</analytic>
	<monogr>
		<title level="m" coord="12,432.53,623.87,48.06,7.92;12,151.52,634.83,329.07,7.92;12,151.52,645.79,52.60,7.92">Proceedings of the 2014 ACM SIGMOD international conference on Management of data -SIGMOD &apos;14</title>
		<meeting>the 2014 ACM SIGMOD international conference on Management of data -SIGMOD &apos;14<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2014-06">Jun 2014</date>
			<biblScope unit="page" from="313" to="324" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
