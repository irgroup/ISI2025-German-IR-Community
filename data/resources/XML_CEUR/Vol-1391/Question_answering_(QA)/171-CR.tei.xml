<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,144.98,152.67,305.39,12.64">Overview of CLEF QA Entrance Exams Task 2015</title>
				<funder ref="#_hbHsH6c #_UCZXEcr #_rbPXtKb #_ZCUrCAe">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,174.02,191.98,75.59,10.80"><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
							<email>alvarory@lsi.uned.es</email>
							<affiliation key="aff0">
								<orgName type="department">NLP&amp;IR group</orgName>
								<orgName type="institution">UNED</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,258.77,191.98,74.24,10.80"><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
							<email>anselmo@lsi.uned.es</email>
							<affiliation key="aff0">
								<orgName type="department">NLP&amp;IR group</orgName>
								<orgName type="institution">UNED</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,343.15,191.98,70.79,10.80"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
							<email>yusuke@nii.ac.jp</email>
							<affiliation key="aff1">
								<orgName type="institution">National Institute of Informatics</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,215.33,205.90,64.43,10.80"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
							<email>hovy@cmu.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,306.89,205.90,69.02,10.80"><forename type="first">Noriko</forename><surname>Kando</surname></persName>
							<email>kando@nii.ac.jp</email>
							<affiliation key="aff1">
								<orgName type="institution">National Institute of Informatics</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,144.98,152.67,305.39,12.64">Overview of CLEF QA Entrance Exams Task 2015</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E9148CC22B9E1F8CB4AD8FB205E18DDF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the Entrance Exams task at the CLEF QA Track 2015. Following the last two editions, the data set has been extracted from actual university entrance examinations including a variety of topics and question types. Systems receive a set of Multiple-Choice Reading Comprehension tests where the task is to select the correct answer among a finite set of candidates, according to the given text. Questions are designed originally for testing human examinees, rather than evaluating computer systems. Therefore, the data set challenges human ability to show their understanding of texts. Thus, questions and answers are lexically distant from their supporting excerpts in text, requiring not only a high degree of textual inference, but also the development of strategies for selecting the correct answer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>Following the 2013 and 2014 editions, the Entrance Exams task at CLEF QA Track 2015 is focused on solving Reading Comprehension (RC) tests of English examinations. Reading Comprehension tests are routinely used to assess the degree to which people comprehend what they read, so we work with the hypothesis that it is reasonable to use these tests to assess the degree to which a machine "comprehends" what it is reading. Despite the difficulty of the challenge, we believe we are building a real benchmark that will serve to measure real progress in the field during the next years.</p><p>With this goal in mind, CLEF and NTCIR started collaboration in 2013 around the idea of testing systems against University Entrance Exams, the same exams humans have to pass to enter University. The data set was prepared and distributed by NTCIR, while other organization efforts, including announcements, collecting and evaluating submissions, etc. were managed by UNED. The success of this coordination also owes to the standard data format and evaluation methodology followed in past editions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TASK DESCRIPTION</head><p>Participant systems are asked to read a given document and answer a set of questions. Questions are given in multiple-choice format, with several options from which a single answer must be selected. Systems have to answer questions by referring to "common sense knowledge" that high school students who aim to enter the university are expected to have. Another important difference is that we do not intend to restrict question types. Any type of reading comprehension questions in real entrance exams will be included in the test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DATA</head><p>Japanese University Entrance Exams include questions formulated at various levels of complexity and test a wide range of capabilities. The challenge of "Entrance Exams" aims at evaluating systems under the same conditions that humans are evaluated to enter the University.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sources</head><p>The data set is extracted from standardized English examinations for university admission in Japan. Exams are created by the Japanese National Center for University Entrance Examinations. Original examinations include various styles of questions, such as word filling, grammatical error recognition, sentence filling, etc. One of such styles is reading comprehension, where a test provides a text that describes some daily life situation, and questions about the text. As in the previous edition, we reduced the challenge to these Reading Comprehension exercises contained in the English exams.</p><p>For each examination, one text is given and some (between 4 and 8) questions about the given text are asked. Each question has four choices, with only one correct answer. For this year campaign, we reused as development data 24 examinations from previous campaigns, with a total number of 115 questions and 460 candidates. Besides, we provided a new test set of 19 documents, 89 questions and 356 candidate answers to be validated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Languages</head><p>Test data sets, originally in English, were manually translated into German, Russian, French, Spanish and Italian<ref type="foot" coords="3,365.83,182.81,4.02,7.24" target="#foot_0">1</ref> . They are parallel translations of texts, questions and candidate answers. All these collections represent a benchmark for evaluating systems in different languages.</p><p>In addition to the official data, we collected several unofficial translations for each language. These collections have the same meaning that the original collection, but they use different words, expressions, syntax, semantics and anaphora, which produce collections with a different level of difficulty. The study of results over these variations should offer useful conclusions about systems' performance and the main issues for current technologies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION</head><p>We obtain the score of each system comparing the answers of systems against the gold standard collection with annotations made by humans. This is an automatic evaluation where we do not need manual assessments.</p><p>Each test receives an evaluation score between 0 and 1 using c@1 <ref type="bibr" coords="3,124.70,451.45,12.87,10.80" target="#b0">[1]</ref>. This measure, used in previous CLEF QA Tracks, encourages systems to reduce the number of incorrect answers while maintaining the number of correct ones by leaving some questions unanswered. Systems received evaluation scores from two different perspectives:</p><p>1. At the question-answering level: correct answers are counted individually without grouping them 2. At the reading-test level: firstly we obtain scores for each reading test. Then, we consider that a system passes a test if the score is at least 0.5. Finally, we account for the number of passed tests. A system passes the task if it passes more than a half of tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>Table <ref type="table" coords="4,154.96,182.98,6.00,10.80" target="#tab_0">1</ref> enumerates the participating groups and their reference paper in CLEF 2015 Working Notes. Although the number of participant groups was the same than last year, they presented fewer systems (only 18 runs). Only LIMSI-CNRS has participated in the three editions, while two teams, CICNLP and SYNAPSE, took part also in the last edition and only one team (Synapse) has participated in a second language different than English (French). Results are summarized in Tables <ref type="table" coords="4,328.13,440.89,6.00,10.80" target="#tab_1">2</ref> and<ref type="table" coords="4,361.50,440.89,6.00,10.80" target="#tab_2">3</ref> for the QA and the Reading perspectives respectively.</p><p>Table <ref type="table" coords="4,184.33,468.51,6.00,10.80" target="#tab_1">2</ref> shows that only the two systems from Synapse <ref type="bibr" coords="4,429.70,468.51,14.06,10.80" target="#b1">[2]</ref> gave more correct answers than incorrect ones and obtained a c@1 score greater than 0.5. In fact, Synapse obtained also the best results in the previous edition. While French results remain similar, English results raise from a c@1score of 0.45 in the last edition, to a score of 0.58 in this edition. Furthermore, the LIMSI group has improved also its performance with respect to the previous edition, while CICNLP obtained similar scores.</p><p>Overall results were lower in this edition. This may mean that the current collection was more complex, but participants did not reported if they performed better over past collections. This is why we would find interesting the proposition of some baseline systems based on lexical and syntactic similarity able to offer reference scores for collections. Only one system (NTUNLG-1) decided to leave some questions unanswered, while several systems of two participants did it in the previous edition. In fact, this is why NTUNLG-1 was the only system with different c@1 and precision scores. This is because c@1 gives the same score that accuracy and precision if all the questions are answered.</p><p>Therefore, there have been fewer systems returning unanswered questions every year. However, the reduction of unanswered questions did not bring a reduction of incorrect answers. It seems that neither the campaign nor the evaluation measure have been able to promote this change in systems. Hence, we must think about new ways of promoting such change.</p><p>Synapse reported additional experiments where they left unanswered some questions and they increased precision over answered questions. However, they obtained fewer correct answers and lower c@1 scores. This is because the objective of c@1 is to acknowledge systems able to reduce incorrect answers while keeping the number of correct answers, but systems are not able to do so.</p><p>On the other hand, Table <ref type="table" coords="5,277.57,643.83,6.00,10.80" target="#tab_2">3</ref> shows results for the reading perspective. First column corresponds to systems run id, second column  0.4 0.2 0.6 0.25 0.43 0.25 0.5 0.25 0.5 0.75 0.17 LIMSI-3 0.31 6/19 0.5 0.25 0.5 0 0.17 0.33 0.4 0.67 0.4 0.6 0 0.25 0.29 0.5 0.25 0.5 0.17 0.25 0.17 LIMSI-4 0.31 4/19 0.25 0.25 0.17 0.25 0.17 0.33 0.6 0 0.2 0.4 0.6 0.75 0.43 0 0.75 0 0.33 0.25 0.17 Average 0.31 -0.24 0.29 0.25 0.31 0.38 0.31 0.20 0.37 0.32 0.31 0.30 0.42 0.30 0.26 0.50 0.36 0.26 0.36 0.33 cicnlp-8 0.3 6/19 0 0.25 0.17 0.75 0.33 0.67 0 0.67 0.6 0.2 0.4 0.25 0 0 0.5 0.25 0.17 0.25 0.67 cicnlp-2 0.29 5/19 0.25 0.25 0.17 0.25 0.5 0.33 0 0 0.4 0.2 0.2 0.5 0.43 0 0.5 0.5 0 0.25 0.67 NTUNLG-2 0.29 6/19 0.5 0 0.33 0.25 0.17 0 0 0 0.2 0.2 0.8 0.5 0.29 0.5 0.5 0 0.33 0.5 0.33 CoMiC-1 0.29 5/19 0.25 0.5 0.33 0.25 0.83 0.67 0.2 0.33 0.4 0.2 0 0 0.29 0 0.5 0.25 0.17 0.5 0 Median 0.29 -0.25 0.25 0.17 0.25 0.47 0.33 0.1 0.33 0.4 0.2 0.3 0.5 0.29 0.25 0.5 0.5 0.17 0.25 0.17 cicnlp-3 0.28 7/19 0 0.5 0.17 0.25 0.5 0 0 0.67 0 0.4 0.2 0.5 0.14 0.25 0.75 0.5 0.17 0.5 0.17 cicnlp-5 0.28 5/19 0.25 0.25 0.5 0.75 0.17 0 0.2 0.33 0.4 0.2 0 0 0.14 0.5 0.25 0.5 0.17 0.25 0.5 cicnlp-4 0.27 5/19 0.25 0.5 0.17 0.25 0.33 0 0 0.33 0.4 0 0 0.5 0.29 0.25 0.75 0.5 0 0.25 0.5 cicnlp-6 0.26 5/19 0.5 0.25 0 0.25 0.5 0 0 0.33 0.6 0.4 0 0.5 0.29 0.25 0.25 0.5 0 0.25 0.17 cicnlp-1 0.26 4/19 0.25 0.25 0.17 0.25 0.5 0.33 0 0.33 0.4 0 0.2 0.5 0.43 0 0.5 0.5 0 0.25 0.17 0 0.25 0.17 0.25 0.17 0.33 0.2 1 0 0.2 0 0.5 0.14 0.25 0.5 0.25 0.17 0 0.17 to the overall c@1 score obtained, third column shows the number of tests that the systems had passed if we considered a c@1 threshold of 0.5, and the rest of columns correspond to the c@1 value for each single test.</p><p>Under the reading perspective we say that a system passes the global exam if it passes a 50% or more tests. That is, if the system passes at least 10 reading tests. According to this requirement, only the two systems from Synapse passed the 2015 Entrance Exams task.</p><p>Although results in the QA perspective are worse than in the previous edition, results in the RC perspective are a little bit greater. In fact, the proportion of passed tests this year (84%) is higher than in the previous edition (75%).</p><p>We have also observed that the raking of system sometimes changes between the QA and the Reading perspective. For instance, system cicnlp-3 ranked fourth in the Reading perspective, while it ranked eleventh in the QA perspective. We observed also similar changes for other systems. We think this is because participants have focused more on the Reading perspective, creating systems with low results in some tests, but good results in other tests.</p><p>We think Tables <ref type="table" coords="7,242.21,412.21,6.00,10.80" target="#tab_3">4</ref> and<ref type="table" coords="7,276.04,412.21,4.51,10.80" target="#tab_4">5</ref>, which show the number of systems passing each test and the maximum score per test, offer a similar conclusion. We see in Tables <ref type="table" coords="7,250.97,439.81,6.00,10.80" target="#tab_3">4</ref> and<ref type="table" coords="7,280.36,439.81,6.00,10.80" target="#tab_4">5</ref> that the maximum scores remain similar or better this year with respect to the previous edition. Moreover, there are more systems passing a test despite the fact that we have fewer systems in this edition.</p><p>Tables <ref type="table" coords="7,188.66,495.03,6.00,10.80" target="#tab_3">4</ref> and<ref type="table" coords="7,219.48,495.03,6.00,10.80" target="#tab_4">5</ref> show also a different degree of difficulty for systems over each test. This difficulty mainly depends on the lexical gap between the text and the candidate answers. Besides, systems find also difficulties depending on different formulations of the same text, as Synapse showed last year <ref type="bibr" coords="7,255.89,550.23,14.08,10.80" target="#b4">[5]</ref> and we are studying now with different versions of the same collection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">SUMMARY OF SYSTEMS</head><p>In this Section we offer deeper details about systems from groups that sent a reference paper about their participation. The general architecture of participant systems includes the following components: (1) a preprocessing step for preparing texts for the next steps; (2) the retrieval of relevant passages in order to reduce the search space; (3) the creation of graph style structures from texts, questions and candidates; (4) the enrichment of structures including, for instance, background knowledge; (5) the comparison of structures as a way of finding the most probable answer; (6) the ranking of candidates with respect to the comparison score and; (7) the selection of the candidate with the best score. Most of systems follow this architecture, with slight changes at some levels or removing some steps.</p><p>The general architecture shows that systems relied on ranking methods instead of validation. In fact, only one system (NTUNLG-1) decided to leave unanswered some questions. Systems know that there is a correct answer, and they take the risk of returning always the candidate more similar to the text, no matter if this similarity is low. As we have pointed out above, this is not the expected behavior in the task, and we must think about new ways of promoting the desired change in such direction.</p><p>It is still not clear the impact of selecting relevant passages rather than working with the whole document. Systems working with passages do it as a way of reducing the amount of work in the following steps.</p><p>Unfortunately, participant groups did not report if they return some incorrect answers as a consequence of this selection. We think this might be an interesting study for other researchers. Anyway, it is clear that this step must be focused on recall rather than in precision.</p><p>Some participants prefer to work with a representation of texts and answers similar to graphs instead of raw text. We think these participants try to exploit the properties of such structures for representing connections between concepts, the inclusion of extra knowledge, etc.</p><p>Regarding the use of external knowledge, we think it is one of the main issues at this task. Reading comprehension texts contain a lot of implicit information that automatic systems might not be able to extract, as LIMSI reported <ref type="bibr" coords="9,252.06,301.81,12.86,10.80" target="#b2">[3]</ref>. However, the best performing systems, from Synapse, did not use any kind of external knowledge. We think current systems are still quite far from a proper way of representing, exploiting and using external knowledge in this task.</p><p>A more detailed analysis of each system showed that Synapse <ref type="bibr" coords="9,456.67,357.01,13.94,10.80" target="#b1">[2]</ref> built Clause Description Structures (CDS) structures, which are similar to graphs, of whole documents. They preferred not to include external knowledge from resources such as DBPedia or Wikipedia because they thought that the given text contained enough information for finding the correct candidate. They also removed candidates which did not match the expected answer type as a way of reducing the search space. Then, they compared CDSs from texts and candidates, measuring proximity and the number of common elements. Finally, they chose the candidate with the highest coefficient.</p><p>On the other hand, LIMSI <ref type="bibr" coords="9,285.29,495.03,14.06,10.80" target="#b2">[3]</ref> selected a set of passages in order to reduce the computation time. Next, they represented passages as graphs and enriched those graphs using external sources. They wanted to reduce the gap between the knowledge extracted from texts by humans and computers. Then, they recorded the changes required for passing from passages graphs to candidate graphs. Finally, they applied two classifiers, one for validation and the other one for rejection, using the set of changes as features. The selected candidate was the one with the highest final score according to equation finalScore = valida-tionScore -rejectionScore.</p><p>CoMIC <ref type="bibr" coords="9,193.10,633.03,14.06,10.80" target="#b3">[4]</ref> retrieved also a set of relevant passages. For this purpose, they took also in consideration that passages relevant to the first questions usually appears at the beginning of the text, while passages referring to the last questions appears at the bottom of the text. Then, they measured the similarity between relevant passages and candidates without using any intermediate graph structure. They accounted for vector-space model based measures, similarity measures using Word-Net, as well as syntactic and semantic similarity measures. Finally, they applied a Ranking SVM model to obtain the final answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>In the third edition of the task we expected a jump in performance in comparison with previous campaigns. However, we have seen similar results at the Question Answering perspective and slight improvements at the Reading Comprehension perspective. Only systems from Synapse could give more correct than incorrect answers. We think the current task is still very hard for current technologies and it is not clear what the bottleneck is. We know that there are several issues such as <ref type="bibr" coords="10,231.68,374.41,13.99,10.80" target="#b0">(1)</ref> the semantic gap between texts, questions and answers; (2) external knowledge management; etc.</p><p>Participants tried different approaches and offered some details about the right way for obtaining progress in this task, but it is not clear what the right direction is.</p><p>Anyway, the availability of the created resources and methodology provides a benchmark able to assess real progress in the field along future years.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,413.88,196.54,8.13,16.61;6,413.88,232.07,8.13,16.95;6,413.88,256.94,8.13,17.07;6,413.88,285.19,8.13,10.57;6,413.88,306.60,8.13,10.57;6,413.88,327.97,8.13,10.57;6,413.88,349.34,8.13,10.57;6,413.88,370.74,8.13,10.57;6,413.88,392.11,8.13,10.57;6,413.88,413.48,8.13,10.57;6,413.88,434.85,8.13,10.57;6,413.88,456.38,8.13,10.57;6,413.88,475.46,8.13,15.14;6,413.88,496.83,8.13,15.14;6,413.88,518.22,8.13,15.14;6,413.88,539.60,8.13,15.14;6,413.88,560.97,8.13,15.14;6,413.88,582.34,8.13,15.14;6,413.88,603.85,8.13,15.14;6,413.88,625.23,8.13,15.14;6,413.88,646.60,8.13,15.14;6,413.88,668.00,8.13,15.14;6,403.12,183.26,8.13,43.19;6,403.12,232.67,8.13,15.86;6,403.12,255.25,8.13,20.69;6,403.12,282.54,8.13,15.86;6,402.88,306.24,8.13,11.39;6,403.12,325.31,8.13,15.86;6,402.88,348.98,8.13,11.39;6,403.12,370.37,8.13,11.39;6,402.88,395.13,8.13,4.53;6,402.88,413.12,8.13,11.39;6,403.12,432.20,8.13,15.86;6,402.88,456.02,8.13,11.39;6,402.88,477.39,8.13,11.39;6,403.12,498.76,8.13,11.39;6,403.12,520.15,8.13,11.39;6,402.88,539.23,8.13,15.86;6,402.88,560.61,8.13,15.86;6,403.12,584.27,8.13,11.39;6,402.88,605.79,8.13,11.39;6,402.88,624.87,8.13,15.86;6,402.88,646.24,8.13,15.86;6,403.12,667.64,8.13,15.86;6,392.28,184.23,8.13,41.20;6,392.28,232.67,8.13,15.86;6,392.28,255.25,8.13,20.69;6,392.28,282.54,8.13,15.86;6,392.04,306.24,8.13,11.39;6,392.04,327.61,8.13,11.39;6,392.28,346.69,8.13,15.86;6,392.28,370.37,8.13,11.39;6,392.28,389.45,8.13,15.86;6,392.28,413.12,8.13,11.39;6,392.04,437.87,8.13,4.53;6,392.04,456.02,8.13,11.39;6,392.04,477.39,8.13,11.39;6,392.28,498.76,8.13,11.39;6,392.04,517.86,8.13,15.86;6,392.04,539.23,8.13,15.86;6,392.28,562.90,8.13,11.39;6,392.28,584.27,8.13,11.39;6,392.04,605.79,8.13,11.39;6,392.28,627.16,8.13,11.39;6,392.28,648.53,8.13,11.39;6,392.04,667.64,8.13,15.86;6,381.31,188.70,8.13,32.20;6,381.31,232.67,8.13,15.86;6,381.31,257.55,8.13,16.11;6,381.31,288.21,8.13,4.53;6,381.31,303.94,8.13,15.86;6,381.07,327.61,8.13,11.39;6,381.31,346.69,8.13,15.86;6,381.31,370.37,8.13,11.39;6,381.31,389.45,8.13,15.86;6,381.31,413.12,8.13,11.39;6,381.31,432.20,8.13,15.86;6,381.31,456.02,8.13,11.39;6,381.31,477.39,8.13,11.39;6,381.31,498.76,8.13,11.39;6,381.31,517.86,8.13,15.86;6,381.31,539.23,8.13,15.86;6,381.31,560.61,8.13,15.86;6,381.31,584.27,8.13,11.39;6,381.07,605.79,8.13,11.39;6,381.31,627.16,8.13,11.39;6,381.31,651.91,8.13,4.53;6,381.31,667.64,8.13,15.86;6,370.47,188.70,8.13,32.20;6,370.47,232.67,8.13,15.86;6,370.47,257.55,8.13,16.11;6,370.47,282.54,8.13,15.86;6,370.23,306.24,8.13,11.39;6,370.47,325.31,8.13,15.86;6,370.47,346.69,8.13,15.86;6,370.47,373.76,8.13,4.53;6,370.47,389.45,8.13,15.86;6,370.47,413.12,8.13,11.39;6,370.47,437.87,8.13,4.53"><head>Run c@ 1 1 0</head><label>11</label><figDesc>Pass T1 T2 T3 T4 T5 T6 T7 T8 T9 T10 T11 T12 T13 T14 T15 T16 T17 T18 T19 Synapse-En 0.58 16/19 0.25 0.5 0.33 0.5 0.5 1 0.8 0.67 0.6 0.8 0.4 0.5 0.57 0.75 0.5 0.5 0.67 0.75 0.67 Synapse-Fr 0.56 16/19 0.25 0.5 0.5 0.25 0.5 0.67 0.6 25 0.5 0.25 0.5 0.67 0.2 0.33 0.2 0.6 0.6 0.25 0.29 0.25 0.5 0.5 0.5 0 0.33 LIMSI-1 0.34 5/19 0.25 0.5 0.33 0.25 0 0.33 0.4 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,217.82,189.54,8.13,30.82;6,217.82,232.67,8.13,15.86;6,217.82,264.07,8.13,3.02;6,217.82,282.54,8.13,15.86;6,217.82,303.94,8.13,15.86;6,217.82,325.31,8.13,15.86;6,217.82,346.69,8.13,15.86;6,217.82,368.08,8.13,15.86;6,217.82,389.45,8.13,15.86;6,217.82,410.83,8.13,15.86;6,217.82,432.20,8.13,15.86;6,217.82,453.72,8.13,15.86;6,217.82,475.09,8.13,15.86;6,217.82,496.47,8.13,15.86;6,217.82,517.86,8.13,15.86;6,217.82,539.23,8.13,15.86;6,217.82,560.61,8.13,15.86;6,217.82,581.98,8.13,15.86;6,217.82,603.49,8.13,15.86;6,217.82,624.87,8.13,15.86;6,217.82,646.24,8.13,15.86;6,217.82,667.64,8.13,15.86;6,206.98,182.42,8.13,44.64;6,206.98,232.67,8.13,15.86;6,206.98,257.55,8.13,16.11;6,206.98,282.54,8.13,15.86;6,206.98,309.62,8.13,4.53;6,206.98,330.99,8.13,4.53;6,206.98,346.69,8.13,15.86;6,206.98,370.37,8.13,11.39;6,206.98,395.13,8.13,4.53;6,206.98,416.50,8.13,4.53;6,206.98,437.87,8.13,4.53;6,206.98,459.40,8.13,4.53;6,206.98,477.39,8.13,11.39;6,206.98,498.76,8.13,11.39;6,206.98,520.15,8.13,11.39;6,206.98,539.23,8.13,15.86;6,206.98,560.61,8.13,15.86;6,206.98,584.27,8.13,11.39;6,206.98,603.49,8.13,15.86;6,206.98,624.87,8.13,15.86;6,206.98,648.53,8.13,11.39;6,206.98,667.64,8.13,15.86;6,196.01,182.42,8.13,44.64;6,196.01,232.67,8.13,15.86;6,196.01,257.55,8.13,16.11;6,196.01,282.54,8.13,15.86;6,196.01,309.62,8.13,4.53;6,196.01,330.99,8.13,4.53;6,196.01,346.69,8.13,15.86;6,196.01,368.08,8.13,15.86;6,196.01,395.13,8.13,4.53;6,196.01,416.50,8.13,4.53;6,196.01,437.87,8.13,4.53;6,196.01,459.40,8.13,4.53;6,196.01,480.77,8.13,4.53;6,196.01,496.47,8.13,15.86;6,196.01,520.15,8.13,11.39;6,196.01,541.53,8.13,11.39;6,196.01,560.61,8.13,15.86;6,196.01,584.27,8.13,11.39;6,196.01,603.49,8.13,15.86;6,196.01,624.87,8.13,15.86;6,196.01,648.53,8.13,11.39;6,196.01,673.32,8.13,4.53;6,185.17,190.02,8.13,29.76;6,185.17,232.67,8.13,15.86;6,185.17,257.55,8.13,16.11"><head></head><label></label><figDesc>0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,124.70,291.62,344.39,132.03"><head>Table 1 .</head><label>1</label><figDesc>Participants and reference papers</figDesc><table coords="4,124.70,308.97,344.39,114.68"><row><cell>Group ID</cell><cell>Group Name</cell><cell>#runs</cell><cell>Reference paper</cell></row><row><cell>SYNAPSE</cell><cell>Synapse Développement, France</cell><cell>2</cell><cell>Laurent et al. 2015 [2]</cell></row><row><cell>NTUNLG</cell><cell>National Taiwan University, Taiwan</cell><cell>3</cell><cell>-</cell></row><row><cell></cell><cell>Centro de Investigación en</cell><cell>8</cell><cell></cell></row><row><cell>CICNLP</cell><cell>Computación</cell><cell></cell><cell>-</cell></row><row><cell></cell><cell>Instituto Politécnico Nacional, Mexico</cell><cell></cell><cell></cell></row><row><cell>CoMIC</cell><cell>Universitä Tübingen, Germany</cell><cell>1</cell><cell>Ziai 2015 [4]</cell></row><row><cell>LIMSI-CNRS</cell><cell>ILES -LIMSI, France</cell><cell>4</cell><cell>Gleize et al. 2015 [3]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,131.90,149.99,341.36,242.01"><head>Table 2 .</head><label>2</label><figDesc>Overall results for all runs, QA perspective</figDesc><table coords="5,131.90,167.15,341.36,224.85"><row><cell></cell><cell></cell><cell></cell><cell cols="3"># of questions ANSWERED</cell><cell># of questions</cell></row><row><cell>RUN NAME</cell><cell>C@1</cell><cell cols="3">RIGHT WRONG TOTAL</cell><cell>Prec.</cell><cell>UNANSWERED</cell></row><row><cell>Synapse-English</cell><cell>0.58</cell><cell>52</cell><cell>37</cell><cell>89</cell><cell>0.58</cell><cell>0</cell></row><row><cell>Synapse-French</cell><cell>0.56</cell><cell>50</cell><cell>39</cell><cell>89</cell><cell>0.56</cell><cell>0</cell></row><row><cell>LIMSI-2</cell><cell>0.36</cell><cell>32</cell><cell>57</cell><cell>89</cell><cell>0.36</cell><cell>0</cell></row><row><cell>LIMSI-1</cell><cell>0.34</cell><cell>30</cell><cell>59</cell><cell>89</cell><cell>0.34</cell><cell>0</cell></row><row><cell>LIMSI-3</cell><cell>0.31</cell><cell>28</cell><cell>61</cell><cell>89</cell><cell>0.31</cell><cell>0</cell></row><row><cell>LIMSI-4</cell><cell>0.31</cell><cell>28</cell><cell>61</cell><cell>89</cell><cell>0.31</cell><cell>0</cell></row><row><cell>cicnlp-8</cell><cell>0.3</cell><cell>27</cell><cell>62</cell><cell>89</cell><cell>0.3</cell><cell>0</cell></row><row><cell>cicnlp-2</cell><cell>0.29</cell><cell>26</cell><cell>63</cell><cell>89</cell><cell>0.29</cell><cell>0</cell></row><row><cell>NTUNLG-2</cell><cell>0.29</cell><cell>26</cell><cell>63</cell><cell>89</cell><cell>0.29</cell><cell>0</cell></row><row><cell>CoMiC-1</cell><cell>0.29</cell><cell>26</cell><cell>63</cell><cell>89</cell><cell>0.29</cell><cell>0</cell></row><row><cell>cicnlp-3</cell><cell>0.28</cell><cell>25</cell><cell>64</cell><cell>89</cell><cell>0.28</cell><cell>0</cell></row><row><cell>cicnlp-5</cell><cell>0.28</cell><cell>25</cell><cell>64</cell><cell>89</cell><cell>0.28</cell><cell>0</cell></row><row><cell>cicnlp-4</cell><cell>0.27</cell><cell>24</cell><cell>65</cell><cell>89</cell><cell>0.27</cell><cell>0</cell></row><row><cell>cicnlp-6</cell><cell>0.26</cell><cell>23</cell><cell>66</cell><cell>89</cell><cell>0.26</cell><cell>0</cell></row><row><cell>cicnlp-1</cell><cell>0.26</cell><cell>23</cell><cell>66</cell><cell>89</cell><cell>0.26</cell><cell>0</cell></row><row><cell>Random</cell><cell>0.25</cell><cell>22</cell><cell>67</cell><cell>89</cell><cell>0.25</cell><cell>0</cell></row><row><cell>NTUNLG-3</cell><cell>0.24</cell><cell>21</cell><cell>68</cell><cell>89</cell><cell>0.24</cell><cell>0</cell></row><row><cell>NTUNLG-1</cell><cell>0.22</cell><cell>17</cell><cell>57</cell><cell>74</cell><cell>0.23</cell><cell>15</cell></row><row><cell>cicnlp-7</cell><cell>0.21</cell><cell>19</cell><cell>70</cell><cell>89</cell><cell>0.21</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,196.34,149.99,202.44,8.10"><head>Table 3 .</head><label>3</label><figDesc>Overall results for all runs, reading perspective</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,124.94,149.99,345.38,61.11"><head>Table 4 .</head><label>4</label><figDesc>Number of runs (out of 18) that passed each test (from test 1 to test 10), and maximum c@1 score achieved per test.</figDesc><table coords="8,153.38,178.38,291.20,32.72"><row><cell cols="9">T1 T2 T3 T4 T5 T6 T7 T8 T9 T10</cell></row><row><cell># Runs pass 3</cell><cell>12</cell><cell>2</cell><cell>5</cell><cell>15 10</cell><cell>4</cell><cell>8</cell><cell>6</cell><cell>6</cell></row><row><cell cols="9">Max. score 0.50 0.75 0.57 0.75 0.75 0.50 0.67 0.75 0.83 0.50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,130.46,240.35,334.25,74.82"><head>Table 5 .</head><label>5</label><figDesc>Number of runs (out of 18) that passed each test (from test 11 to test 19), and maximum c@1 score achieved per test.</figDesc><table coords="8,164.06,282.45,269.12,32.72"><row><cell></cell><cell cols="9">T11 T12 T13 T14 T15 T16 T17 T18 T19</cell></row><row><cell># Runs pass</cell><cell>4</cell><cell>3</cell><cell>9</cell><cell>5</cell><cell>3</cell><cell>6</cell><cell>4</cell><cell>4</cell><cell>5</cell></row><row><cell cols="10">Max. score 0.50 0.75 0.83 1.00 0.80 1.00 0.60 0.80 0.80</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,129.98,686.23,285.82,8.10"><p>Development data was translated to the same languages in the previous edition.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENTS</head><p>The collaboration has been developed in the framework of <rs type="projectName">Todai Robot Project in Japan</rs>, and the <rs type="projectName">CHIST-ERA Readers project in Europe</rs> (<rs type="grantNumber">MINECO PCIN-2013-002-C02-01</rs>) and the <rs type="projectName">Voxpopuli</rs> project (<rs type="grantNumber">TIN2013-47090-C3-1-P</rs>). The <rs type="projectName">Todai Robot Project</rs> is a grand challenge headed by NII, and aims to develop an end-to-end AI system that can solve real entrance examinations of universities in Japan integrating heterogeneous AI technologies, such as natural language processing, situation understanding, math formula processing or vision processing.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_hbHsH6c">
					<orgName type="project" subtype="full">Todai Robot Project in Japan</orgName>
				</org>
				<org type="funded-project" xml:id="_UCZXEcr">
					<idno type="grant-number">MINECO PCIN-2013-002-C02-01</idno>
					<orgName type="project" subtype="full">CHIST-ERA Readers project in Europe</orgName>
				</org>
				<org type="funded-project" xml:id="_rbPXtKb">
					<idno type="grant-number">TIN2013-47090-C3-1-P</idno>
					<orgName type="project" subtype="full">Voxpopuli</orgName>
				</org>
				<org type="funded-project" xml:id="_ZCUrCAe">
					<orgName type="project" subtype="full">Todai Robot Project</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="11,128.47,176.98,341.96,10.80;11,142.70,190.78,327.76,10.80;11,142.70,204.58,328.03,10.80;11,142.70,218.38,298.73,10.80" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,327.77,176.98,142.65,10.80;11,142.70,190.78,64.16,10.80">A Simple Measure to Assess Non-response</title>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alvaro</forename><surname>Rodrigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,237.16,190.78,233.30,10.80;11,142.70,204.58,328.03,10.80;11,142.70,218.38,147.77,10.80">Proceedings of 49th Annual Meeting of the Association for Computational Linguistics -Human Language Technologies (ACL-HLT 2011)</title>
		<meeting>49th Annual Meeting of the Association for Computational Linguistics -Human Language Technologies (ACL-HLT 2011)<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,131.83,232.18,338.58,10.80;11,146.06,245.98,324.35,10.80;11,146.06,259.78,280.37,10.80" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,288.52,245.98,181.88,10.80;11,146.06,259.78,57.37,10.80">Reading Comprehension at Entrance Exams 2015</title>
		<author>
			<persName coords=""><forename type="first">Dominique</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Baptiste</forename><surname>Chardon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sophie</forename><surname>Negre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Camille</forename><surname>Pradel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Seguela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,211.85,259.78,128.87,10.80">CLEF 2015 Working Notes</title>
		<meeting><address><addrLine>Toulouse</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.56,273.58,337.99,10.80;11,146.06,287.41,324.53,10.80;11,146.06,301.21,182.31,10.80" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,306.62,273.58,163.93,10.80;11,146.06,287.41,286.31,10.80">LIMSI-CNRS@CLEF 2015: Tree Edit Beam Search for Multiple Choice Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Gleize</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brigitte</forename><surname>Grau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,441.19,287.41,29.40,10.80;11,146.06,301.21,96.60,10.80">CLEF 2015 Working Notes</title>
		<meeting><address><addrLine>Toulouse</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.56,315.01,338.00,10.80;11,146.06,328.81,324.68,10.80;11,146.06,342.61,77.67,10.80" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,210.58,315.01,259.98,10.80;11,146.06,328.81,177.87,10.80">CoMiC: Exploring Text Segmentation and Similarity in the English Entrance Exams Task</title>
		<author>
			<persName coords=""><forename type="first">Ramon</forename><surname>Ziai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,333.31,328.81,132.36,10.80">CLEF 2015 Working Notes</title>
		<meeting><address><addrLine>Toulouse</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,129.20,356.41,341.41,10.80;11,142.70,370.21,327.96,10.80;11,142.70,384.01,242.93,10.80" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,186.62,370.21,284.04,10.80;11,142.70,384.01,21.60,10.80">English run of Synapse Développement at Entrance Exams 2014</title>
		<author>
			<persName coords=""><forename type="first">Dominique</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Baptiste</forename><surname>Chardon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sophie</forename><surname>Negre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Seguela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,172.70,384.01,130.63,10.80">CLEF 2014 Working Notes</title>
		<meeting><address><addrLine>Sheffield</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
