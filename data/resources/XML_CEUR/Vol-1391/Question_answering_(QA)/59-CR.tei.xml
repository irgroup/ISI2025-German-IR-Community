<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,196.18,115.96,223.01,12.62;1,201.97,133.89,211.43,12.62">HPI question answering system in the BioASQ 2015 challenge</title>
				<funder>
					<orgName type="full">HPI Research School</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,273.02,171.56,64.84,8.74"><forename type="first">Mariana</forename><surname>Neves</surname></persName>
							<email>mariana.neves@hpi.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Hasso-Plattner-Institute at the University of Potsdam</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,196.18,115.96,223.01,12.62;1,201.97,133.89,211.43,12.62">HPI question answering system in the BioASQ 2015 challenge</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AC8DB61E5B77E7ABBEBB86E2341EA4C9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>question answering</term>
					<term>biomedicine</term>
					<term>passage retrieval</term>
					<term>document retrieval</term>
					<term>concept extraction</term>
					<term>in-memory database</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>I describe my participation on the 2015 edition of the BioASQ challenge in which I submitted results for the concept matching, document retrieval, passage retrieval, exact answer and ideal answer subtasks. My approach relies on a in-memory based database (IMDB) and its built-in text analysis features, as well as on PubMed for retrieving relevant citations, and on predefined ontologies and terminologies necessary for matching concepts to the questions. Although results are far below the ones obtained by other groups, I present an novel approach for answer extraction based on sentiment analysis.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>I describe my participation in the 2015 edition of the BioASQ challenge 1 <ref type="bibr" coords="1,470.08,408.52,10.52,8.74" target="#b5">[6]</ref> which took place in the scope of the CLEF initiative. This challenge aims to assess the current state of question answering systems and semantic indexing for biomedicine. The task 3b (Biomedical Semantic QA) is split in two phases and includes various sub-tasks such as concept mapping, information retrieval, question answering and text summarization. In phase A, participants receive a test set of questions along with their question type, i.e., "yes/no", "factoid", "list" or "summary". Participants have 24 hours to submit predictions for relevant concepts, documents, passages and RDF triplets. When phase A is over, the organizers make available the test set for phase B containing the same questions of phase A, along with gold-standard annotations for concepts, documents, passages and RDF triples. This time, participants have 24 hours to submit predictions for the exact and ideal answers (short summaries). Exact answers are only required for "yes/no", "factoid", "list", while ideal answers are expected to be returned for questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Architecture</head><p>I participated with a system developed on top of an in-memory database (IMDB) <ref type="bibr" coords="1,134.77,638.32,9.96,8.74" target="#b4">[5]</ref>, the SAP HANA database, which is similar to the approach that I used during 1 http://bioasq.org/ in the 2014 edition of the BioASQ challenge <ref type="bibr" coords="2,325.13,118.99,9.96,8.74" target="#b1">[2]</ref>. I participated in phases A and B of the task 3b of the 2015 edition of the BioASQ challenge and I have submitted predictions for potentially relevant concepts, documents, passages and answers.</p><p>Similar to previous QA systems <ref type="bibr" coords="2,291.38,167.83,9.96,8.74" target="#b3">[4]</ref>, my system is composed of the following components: (a) question processing for construction of a query from the question; (b) concept mapping for performing concept recognition on the question; (c) document and passage retrieval for ranking and retrieval of relevant PubMed documents and passages; (d) answer extraction for building the short and long (summaries) answers. Figure <ref type="figure" coords="2,264.79,227.61,4.98,8.74" target="#fig_0">1</ref> illustrates the architecture of the system and I describe the various steps in details below, including a short overview of the IMDB technology. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">In-memory database</head><p>The SAP HANA database relies on IMDB technology <ref type="bibr" coords="2,372.52,548.52,10.52,8.74" target="#b4">[5]</ref> for fast access of data directly from main memory, in contrast to approaches which process data from files that reside on disk space and requires loading data into main memory. It also includes lightweight compression, i.e., a data storage representation that consumes less space than its original format, and built-in parallelization. The SAP HANA database comes with built-in text analysis which includes language detection, sentence splitting, tokenization, stemming, part-of-speech tagging, named-entity recognition based on pre-compiled dictionaries, information extraction based on manually crafted rules, document indexing, approximate searching and sentiment analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Question processing</head><p>In this step, the system processes the questions using the Standford CoreNLP <ref type="bibr" coords="3,134.77,156.43,10.52,8.74" target="#b0">[1]</ref> for sentence splitting, tokenization, part-of-speech tagging and chunking. The system constructed two queries for each question by selecting their more meaningful tokens. The first approach consists in removing all tokens which match a stopword list<ref type="foot" coords="3,190.38,190.72,3.97,6.12" target="#foot_0">2</ref> and connecting them with the "OR", operator for more flexibility of the query. Both the document and passage retrieval steps as well as the answer extraction step made use of this high recall query for ranking documents and passages.</p><p>The second query aims on more precision and less recall and filters tokens further based on a list of the 5,000 most popular words of English<ref type="foot" coords="3,433.78,263.57,3.97,6.12" target="#foot_1">3</ref> and uses the "AND" operator for connecting words. Only the document retrieval step used this high precision query for ranking relevant documents from PubMed. For instance, for the question "What disease is mirtazapine predominantly used for?", "disease OR mirtazapine OR predominantly OR used" is the resulting high recall query and "mirtazapine AND predominantly" is a higher precision query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Concept mapping</head><p>The approach is the same that I used in the 2014 edition of the challenge <ref type="bibr" coords="3,467.30,397.80,9.96,8.74" target="#b1">[2]</ref>: I made use of the built-in named-entity recognition feature of the IMDB for mapping the questions to concepts from the five required terminologies and ontologies, which needed to be previously converted to dictionaries in an appropriate XML format. Given the dictionaries, the IMDB databases automatically matched terms to the words of the question, as illustrated in Figure <ref type="figure" coords="3,434.44,457.57,3.87,8.74" target="#fig_1">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Document and passage retrieval</head><p>The approach for retrieving relevant PubMed documents for each question is similar to the one described in my recently submitted paper <ref type="bibr" coords="4,401.70,151.19,9.96,8.74" target="#b2">[3]</ref>. It consisted in first posing the two generates queries to PubMed web services, retrieving up to 200 top ranked documents for each query and fetching the title and abstract for each PMID using the BioASQ web services. When querying PubMed, I restricted publication dates up to '2013/03/14' and I required citations to have an abstract available. This current approach differs from the one of my last year's participation <ref type="bibr" coords="4,194.11,222.92,10.52,8.74" target="#b1">[2]</ref> in terms that no I did not perform synonym expansion for the terms in the query, given the poor results obtained when relying on BioPortal for this purpose. Finally, titles and abstracts were inserted into a table in the IMDB. I retrieved passages using on the built-in information retrieval features available in the IMDB, which is based in approximated string similarity to match terms from the query to the words in the documents. The system proceeds ranks the passages (sentences) based on the TF-IDF metrics and I retrieve the top 10 sentences and corresponding documents as answers for the passage and document retrieval sub-tasks, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Answer extraction</head><p>I extracted both exact and ideal answers based on the gold-standard snippets that the organizers made available for phase B of task 3b. The process consisted in inserting the snippets into the IMDB database and I utilized built-in text analysis features for the extracting the answers, as described in details below for each question type.</p><p>Yes/No: Decision on either the answers "yes" or "no" was based on the sentiment analysis predictions provided by the IMDB. The assumption was that all snippets are somehow related to the question and that detection of sentiments in these passages could be used to distinguish between the two possible answers. Figure <ref type="figure" coords="4,166.20,500.64,4.98,8.74" target="#fig_2">3</ref> shows the sentiments which were detected for a certain question.</p><p>The IMDB returns 10 types of sentiments, namely "StrongPositiveSentiment"", "StrongPositiveEmoticon", "WeakPositiveSentiment", "WeakPositiveEmoticon", "StrongNegativeSentiment", "StrongNegativeEmoticon", "MajorProblem", "WeakNegativeSentiment", "WeakNegativeEmoticon" and "MinorProblem". I merged some of these sentiment types into coarser categories according to simples rules (Table <ref type="table" coords="4,206.75,572.43,3.87,8.74" target="#tab_0">1</ref>). The sentiments were first grouped into four coarse categories, i.e., "positiveStrong", "positiveWeak", "negativeStrong", "negativeWeak", and then into the three main sentiments "positive" or "negative". For the rules shown in Table <ref type="table" coords="4,174.49,608.30,3.87,8.74" target="#tab_0">1</ref>, I consider that the "positiveStrong" sentiment is stronger than the "negativeStrong" one, and therefore I assign the "positive" sentiment for such cases. Similarly, I consider "positiveWeak" weaker than "negativeWeak" when both are returned for the same question. Cases which did not match none of the rules for "positive" or "negative" sentiments are classified as "neutral". Final decision for the the answers "yes" or "no" was based on these three coarse sentiments. By default, I return the answer "no", unless I get "positive" or "neutral' as output from the above rules. Factoid and list: I extracted factoid and list answers based also on built-in predictions provided by our IMDB, more specifically, on the annotations of noun phrases and topics, as presented in Figure <ref type="figure" coords="5,320.08,608.30,3.87,8.74" target="#fig_3">4</ref>. Given that no semantic processing was performed neither for the question nor for the snippets, in oder to tag named entities and to identify the entity type of expected answer, I choose the five top answers based on the order returned by the IMDB. Summary: I also built summaries for the ideal answers based on the phrases which contain sentiments, as shown in Table <ref type="table" coords="6,334.61,362.28,3.87,8.74" target="#tab_4">6</ref>. The assumption was that such phrases are more informative and relevant than the ones in which no sentiments were found. My approach consisted in concatenating the sentences up to a limit of 200 words, as specified in the challenge's guidelines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results and discussion</head><p>I submitted results for all five batches of test questions for task 3b: (a) phase A, i.e., concept mapping and document and passage retrieval, and (b) phase B, i.e., exact and ideal answers. Different from previous editions of the BioASQ challenge, when participants were allowed to submit up to 100 entries per question for each of the required sub-tasks, whether documents, concepts or exact answers, this year's edition limited concepts, documents and passages up to 10 per question and factoid answers up to 5. I present below the results I obtained as published by the organizers in the BioASQ Web site<ref type="foot" coords="6,364.55,551.15,3.97,6.12" target="#foot_2">4</ref> . I do not show results for concept matching because the organizers seem not to have made them available yet.</p><p>Table <ref type="table" coords="6,178.45,588.91,4.98,8.74" target="#tab_1">3</ref> shows my results for document retrieval for each of the five test batches. As discussed in the methods section, I did not implement any specific approach for this task and documents were ranked based on the relevancy of the query to the passages and not to the documents (abstracts) themselves. In Table <ref type="table" coords="7,165.28,115.91,4.13,7.89">2</ref>. Phases related to sentiments found in the gold-standard snippets of the question "What disease is mirtazapine predominantly used for?". second-generation antidepressants (selective serotonin reuptake inhibitors, nefazodone, venlafaxine, and mirtazapine) in participants younger than 19 years with MDD, OCD, or non-OCD anxiety disorders. patients 65 years or older with major depression. A case report involving linezolid with citalopram and mirtazepine in the precipitation of serotonin syndrome in a critically ill bone marrow transplant patient is described in this article. In 26 patients with FMS who completed a 6-week open study with mirtazapine, 10 (38%) responded with a reduction of at least 40% of the initial levels of pain, fatigue and sleep disturbances (Samborski et al 2004). In general, drugs lacking strong cholinergic activity should be preferred. Drugs blocking serotonin 5-HT2A or 5-HT2C receptors should be preferred over those whose sedative property is caused by histamine receptor blockade only. this year's edition of the challenge, organizers required participants to submit up to 10 document, which is a hard assignment, given the millions of citations in PubMed. Indeed, results have been lower than the ones obtained by participants last year and it is unclear whether we (teams) performed better than the baseline systems as the organizers did not publish results for these systems yet. Table <ref type="table" coords="7,176.31,582.70,4.98,8.74" target="#tab_2">4</ref> shows my results for passage retrieval for each of the five test batches. Few groups participated in this task, in comparison to the number of submissions for the document retrieval task. A task which is already very complex has been made even more difficult this year by the limitation of providing up to only 10 top passages.</p><p>Finally, tables 5 and 6 shows the results I obtained for the exact and ideal answers in phase B of task 3b. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,237.34,463.76,140.68,7.89"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Architecture of the system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,134.77,588.24,345.83,7.89;3,134.77,599.22,141.92,7.86;3,145.63,491.22,324.10,82.25"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Screen-shot of the entities recognized for the question "What disease is mirtazapine predominantly used for?".</figDesc><graphic coords="3,145.63,491.22,324.10,82.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,134.77,237.81,345.82,7.89;5,134.77,248.79,196.11,7.86;5,150.88,115.83,313.60,107.20"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Screen-shot of the sentiments detected from the gold-standard snippets for the question "Is miR-126 involved in heart failure?".</figDesc><graphic coords="5,150.88,115.83,313.60,107.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,134.77,305.00,345.82,7.89;6,134.77,315.99,292.95,7.86;6,166.08,115.83,283.20,174.40"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.4. Screen-shot of the noun phrases and topics detected from the relevant snippets for the question "What disease is mirtazapine predominantly used for?".</figDesc><graphic coords="6,166.08,115.83,283.20,174.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,145.38,356.88,324.60,183.28"><head>Table 1 .</head><label>1</label><figDesc>Rules for merging fine-grained sentiments into coarser sentiments.</figDesc><table coords="5,145.38,377.68,324.60,162.48"><row><cell cols="2">coarse sentiment Rule</cell></row><row><cell cols="2">positiveStrong StrongPositiveSentiment OR StrongPositiveEmoticon</cell></row><row><cell>positiveWeak</cell><cell>WeakPositiveSentiment OR WeakPositiveEmoticon</cell></row><row><cell cols="2">negativeStrong StrongNegativeSentiment OR StrongNegativeEmoticon OR Ma-</cell></row><row><cell></cell><cell>jorProblem</cell></row><row><cell>negativeWeak</cell><cell>WeakNegativeSentiment OR WeakNegativeEmoticon OR Mi-</cell></row><row><cell></cell><cell>norProblem</cell></row><row><cell>positive</cell><cell>(positiveStrong OR positiveWeak) AND (NOT(negativeStrong)</cell></row><row><cell></cell><cell>AND NOT(negativeWeak))</cell></row><row><cell>positive</cell><cell>positiveStrong AND negativeStrong</cell></row><row><cell>positive</cell><cell>positiveStrong AND negativeWeak</cell></row><row><cell>negative</cell><cell>(negativeStrong OR negativeWeak) AND (NOT(positiveStrong)</cell></row><row><cell></cell><cell>AND NOT(positiveWeak))</cell></row><row><cell>negative</cell><cell>positiveWeak AND negativeStrong</cell></row><row><cell>negative</cell><cell>positiveWeak AND negativeWeak</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,134.77,444.78,345.83,94.81"><head>Table 3 .</head><label>3</label><figDesc>Results for document retrieval for the test set. The "Rank" column shows the position obtained by my system in relation to the total number of submissions.</figDesc><table coords="7,192.13,476.53,231.10,63.06"><row><cell cols="3">test batch Mean precision Recall F-measure MAP Rank</cell></row><row><cell>batch 1</cell><cell>0.1027</cell><cell>0.1250 0.0841 0.0464 17/18</cell></row><row><cell>batch 2</cell><cell>0.1164</cell><cell>0.1363 0.1009 0.0658 17/20</cell></row><row><cell>batch 3</cell><cell>0.1082</cell><cell>0.1139 0.0950 0.0634 17/20</cell></row><row><cell>batch 4</cell><cell>0.1354</cell><cell>0.1849 0.1283 0.0737 18/21</cell></row><row><cell>batch 5</cell><cell>0.1465</cell><cell>0.2810 0.1690 0.0700 16/19</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,134.77,147.67,345.83,94.81"><head>Table 4 .</head><label>4</label><figDesc>Results for passage retrieval for the test set. The "Rank" column shows the position obtained by my system in relation to the total number of submissions.</figDesc><table coords="8,192.96,179.43,229.44,63.06"><row><cell cols="3">test batch Mean precision Recall F-measure MAP Rank</cell></row><row><cell>batch 1</cell><cell>0.0545</cell><cell>0.0686 0.0501 0.0347 6/6</cell></row><row><cell>batch 2</cell><cell>0.0580</cell><cell>0.0493 0.0437 0.0355 7/7</cell></row><row><cell>batch 3</cell><cell>0.0542</cell><cell>0.0396 0.0391 0.0452 7/7</cell></row><row><cell>batch 4</cell><cell>0.0881</cell><cell>0.0981 0.0807 0.0624 8/8</cell></row><row><cell>batch 5</cell><cell>0.0859</cell><cell>0.1189 0.0883 0.0572 6/6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,134.77,325.97,372.36,114.99"><head>Table 5 .</head><label>5</label><figDesc>Results for exact answers for the test set. The "Rank" column shows the position obtained by my system in relation to the total number of submissions.</figDesc><table coords="8,136.16,366.94,370.96,74.02"><row><cell>test batch</cell><cell cols="7">Yes/No Accuracy Strict Acc. Lenient Acc. MRR Mean precision Recall F-measure Factoid List</cell><cell>Rank</cell></row><row><cell>batch 1</cell><cell>0.6667</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.0292</cell><cell cols="2">0.0603 0.0364</cell><cell>7/9</cell></row><row><cell>batch 2</cell><cell>0.5625</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.0714</cell><cell cols="3">0.0161 0.0262 10/12</cell></row><row><cell>batch 3</cell><cell>0.6207</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>7/14</cell></row><row><cell>batch 4</cell><cell>0.5600</cell><cell>0.0345</cell><cell>0.0345</cell><cell>0.0345</cell><cell>0.1522</cell><cell cols="3">0.0473 0.0689 10/12</cell></row><row><cell>batch 5</cell><cell>0.3571</cell><cell>0.0909</cell><cell>0.0909</cell><cell>0.0909</cell><cell>0.0625</cell><cell cols="3">0.0292 0.0397 14/14</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,134.77,524.44,345.83,94.81"><head>Table 6 .</head><label>6</label><figDesc>Results for ideal answers for the test set. The "Rank" column shows the position obtained by my system in relation to the total number of submissions.</figDesc><table coords="8,232.94,556.19,149.47,63.06"><row><cell cols="3">test batch Rouge-2 Rouge-SU4 Rank</cell></row><row><cell>batch 1</cell><cell>0.1884</cell><cell>0.2008 15/15</cell></row><row><cell>batch 2</cell><cell>0.2026</cell><cell>0.2227 18/18</cell></row><row><cell>batch 3</cell><cell>0.1934</cell><cell>0.2189 17/17</cell></row><row><cell>batch 4</cell><cell>0.2504</cell><cell>0.2724 16/18</cell></row><row><cell>batch 5</cell><cell>0.1694</cell><cell>0.1790 18/18</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="3,144.73,646.48,278.73,7.47"><p>http://www.textfixer.com/resources/common-english-words.txt</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="3,144.73,657.44,283.44,7.47"><p>https://www.englishclub.com/vocabulary/common-words-5000.htm</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="6,144.73,646.48,292.35,7.47;6,144.73,657.44,221.74,7.47"><p>http://participants-area.bioasq.org/results/3b/phaseA/;http:// participants-area.bioasq.org/results/3b/phaseB/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements MN would like to acknowledge funding from the <rs type="funder">HPI Research School</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,138.35,186.42,300.66,8.11" xml:id="b0">
	<monogr>
		<ptr target="http://nlp.stanford.edu/software/corenlp.shtml" />
		<title level="m" coord="9,146.91,186.42,68.66,7.86">Stanford core nlp</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,197.38,342.24,7.86;9,146.91,208.34,333.68,7.86;9,146.91,219.30,47.11,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,191.61,197.38,234.70,7.86">HPI in-memory-based database system in task 2b of bioasq</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Neves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,446.53,197.38,34.06,7.86;9,146.91,208.34,131.75,7.86">Working Notes for CLEF 2014 Conference</title>
		<meeting><address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">September 15-18, 2014. 2014</date>
			<biblScope unit="page" from="1337" to="1347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,230.26,342.24,7.86;9,146.91,241.22,231.54,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,191.50,230.26,289.09,7.86;9,146.91,241.22,11.14,7.86">In-memory database for passage retrieval in biomedical question answering</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Neves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,164.84,241.22,134.21,7.86">Journal Of Biomedical Semantics</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>submitted</note>
</biblStruct>

<biblStruct coords="9,138.35,252.18,342.25,7.86;9,146.91,263.78,310.69,7.47" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,231.85,252.18,120.80,7.86">Question answering for biology</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Leser</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S1046202314003491" />
	</analytic>
	<monogr>
		<title level="j" coord="9,358.90,252.18,34.86,7.86">Methods</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page" from="36" to="46" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,274.09,342.24,7.86;9,146.91,285.05,194.39,7.86" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="9,201.91,274.09,278.69,7.86;9,146.91,285.05,87.03,7.86">A Course in In-Memory Data Management: The Inner Mechanics of In-Memory Databases</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Plattner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>1st edn.</note>
</biblStruct>

<biblStruct coords="9,138.35,296.01,342.25,7.86;9,146.91,306.97,333.67,7.86;9,146.91,317.93,333.68,7.86;9,146.91,328.89,240.65,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,468.56,306.97,12.03,7.86;9,146.91,317.93,333.68,7.86;9,146.91,328.89,79.32,7.86">An overview of the bioasq large-scale biomedical semantic indexing and question answering competition</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Tsatsaronis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Balikas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Malakasiotis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Partalas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zschunke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Alvers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Polychronopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,233.52,328.89,81.85,7.86">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">138</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
