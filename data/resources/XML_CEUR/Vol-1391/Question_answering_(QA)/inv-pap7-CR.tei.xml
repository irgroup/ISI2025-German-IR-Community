<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,149.67,115.96,316.02,12.62;1,202.32,133.89,210.73,12.62">Results of the BioASQ tasks of the Question Answering Lab at CLEF 2015</title>
				<funder>
					<orgName type="full">Viseo and Atypon</orgName>
				</funder>
				<funder ref="#_QhCa4ym">
					<orgName type="full">NIH/NLM</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,155.40,171.56,72.30,8.74"><forename type="first">Georgios</forename><surname>Balikas</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Laboratoire d&apos; Informatique de Grenoble</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,238.25,171.56,79.34,8.74"><forename type="first">Aris</forename><surname>Kosmopoulos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NCSR &quot;Demokritos&quot;</orgName>
								<address>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,328.15,171.56,83.46,8.74"><forename type="first">Anastasia</forename><surname>Krithara</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NCSR &quot;Demokritos&quot;</orgName>
								<address>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,422.17,171.56,37.79,8.74;1,227.54,183.51,40.35,8.74"><forename type="first">Georgios</forename><surname>Paliouras</surname></persName>
							<email>paliourg@iit.demokritos.gr</email>
							<affiliation key="aff0">
								<orgName type="institution">NCSR &quot;Demokritos&quot;</orgName>
								<address>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,302.40,183.51,80.94,8.74"><forename type="first">Ioannis</forename><surname>Kakadiaris</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Houston</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,149.67,115.96,316.02,12.62;1,202.32,133.89,210.73,12.62">Results of the BioASQ tasks of the Question Answering Lab at CLEF 2015</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">77ACFC21C1992C668185C53FBF21224A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The goal of the BioASQ challenge is to push research towards highly precise biomedical information access systems. We aim to promote systems and approaches that are able to deal with the whole diversity of the Web, especially for, but not restricted to, the context of biomedicine. The third challenge consisted of two tasks: semantic indexing and question answering. 59 systems by 18 different teams participated in the semantic indexing task (Task 3a). The question answering task was further subdivided into two phases. 24 systems from 9 different teams participates in the annotation phase (Task 3b-phase A), while 26 systems of 10 different teams participated in the answer generation phase (Task 3b-phase B). Overall, the best systems were able to outperform the strong baselines provided by the organizers. In this paper, we present the data used during the challenge as well as the technologies which were used by the participants.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The aim of this paper is to present an overview of the BioASQ challenge in CLEF 2015. The overview provides information about:</p><p>1. the two BioASQ tasks of the Question Answering Lab at CLEF 2015, 2. the data provided during the BioASQ tasks, 3. the systems that participated in the challenge, according to the system descriptions that we have received; detailed descriptions of some of the systems are given in the lab proceedings which we cite, 4. evaluation results about the performance of the participating systems and compare them to dedicated baseline systems.</p><p>2 Overview of the Tasks</p><p>The challenge comprised two tasks: (1) a large-scale semantic indexing task (Task 3a) and (2) a question answering task (Task 3b). Information about the challenge and the nature of the data it provides is available at <ref type="bibr" coords="1,409.46,637.71,15.50,8.74" target="#b24">[21,</ref><ref type="bibr" coords="1,426.61,637.71,7.01,8.74" target="#b5">2]</ref>.</p><p>Large-scale semantic indexing. In Task 3a the goal is to classify documents from the MEDLINE 4 digital library unto concepts of the MeSH 5 hierarchy. Here, new MEDLINE articles that are not yet annotated are collected on a weekly basis. These articles are used as test sets for the evaluation of the participating systems. As soon as the annotations are available from the MEDLINE curators, the performance of each system is assessed using standard information retrieval measures as well as hierarchical ones. The winners of each batch are decided based on their performance in the Micro F-measure (MiF) from the family of flat measures <ref type="bibr" coords="2,194.97,214.64,14.61,8.74" target="#b25">[22]</ref>, and the Lowest Common Ancestor F-measure (LCA-F) from the family of hierarchical measures <ref type="bibr" coords="2,289.38,226.59,14.61,8.74" target="#b14">[11]</ref>. For completeness several other flat and hierarchical measures are reported <ref type="bibr" coords="2,287.80,238.55,9.96,8.74" target="#b6">[3]</ref>.</p><p>In order to provide an on-line and large-scale scenario, the task was divided into three independent batches. In each batch 5 test sets of biomedical articles were released following a pre-announced schedule. The test sets were released on a weekly basis (on Monday 17.00 CET) and the participants were asked to provide their system's answers within 21 hours. Figure <ref type="figure" coords="2,382.16,298.87,4.98,8.74">1</ref> gives an overview of the time plan of Task 3a. Biomedical semantic QA. The goal of Task 3b was to assess the performance of participating systems in different stages of the question answering process, ranging from the retrieval of relevant concepts and articles, to the generation of natural-language answers. Task 3b comprised two phases: In phase A, BioASQ released questions in English from benchmark datasets created by a group of biomedical experts. There were four types of question: "yes/no" questions, "factoid" questions,"list" questions and "summary" questions <ref type="bibr" coords="2,388.60,553.67,9.96,8.74" target="#b6">[3]</ref>. Participants were asked to respond with relevant concepts (from specific terminologies and ontologies), relevant articles (PubMed and PubMedCentral 6 articles), relevant snippets extracted from the relevant articles and relevant RDF triples (from specific ontologies). In phase B, the released questions were accompanied by the correct answers for a subset of the required elements of phase A; namely documents and snippets. <ref type="foot" coords="3,173.34,117.42,3.97,6.12" target="#foot_0">7</ref> The participants had to answer with exact answers as well as with paragraph-sized summaries in natural language (dubbed ideal answers).</p><p>The task was split into five independent batches (see Fig. <ref type="figure" coords="3,399.50,143.62,3.87,8.74" target="#fig_1">2</ref>). For each phase, the participants had 24 hours to submit their answers. We used well-known measures such as mean precision, mean recall, mean F-measure, mean average precision (MAP) and geometric MAP (GMAP) to evaluate the performance of the participants in Phase A. The winners were selected based on MAP. The evaluation in phase B was carried out manually by biomedical experts on the ideal answers provided by the systems. For the sake of completeness, ROUGE <ref type="bibr" coords="3,465.09,215.35,15.50,8.74" target="#b15">[12]</ref> was also reported. 3 Technology Overview of the Participating Systems</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task 3a</head><p>The systems that participated in the semantic indexing task of the BioASQ challenge adopted a variety of approaches based mostly on flat classification. In the rest of section we describe the participating systems and stress their key characteristics.</p><p>The NCBI system <ref type="bibr" coords="3,237.45,528.09,14.61,8.74">[14]</ref>, called MeSH Now, was contributed as a baseline system for the semantic indexing task of 2015. This allowed other participants to use its predictions, in order to improve their own results. The system is very similar to that developed by NCBI for the BioASQ2 challenge, based on the generic learning-to-rank approach presented in <ref type="bibr" coords="3,337.83,575.91,9.96,8.74" target="#b12">[9]</ref>. The main improvements were the addition of new training data from the third iteration of the challenge and the submission of two separate runs each week, one favoring high F 1 and one favoring high recall. Improvements were also done on the scalability of the system which now runs in parallel using a computer cluster.</p><p>The AUTH-Atypon system [16] also adopted a flat classification approach. The approach is based on binary linear SVM models for each class. A Meta-Labeler <ref type="bibr" coords="4,170.62,142.90,15.50,8.74" target="#b23">[20]</ref> is used to predict the number of classes that an instance should be assigned to. An ensemble of such classifiers, trained on variable training data sizes and different time periods, is then used in order to deal with the problem of Concept Drift.</p><p>A domain-independent k-nearest-neighbor approach is adopted by the IIIT team <ref type="bibr" coords="4,159.15,207.29,9.96,8.74">[1]</ref>. Initially the system uses k-NN in order to find the most relevant MeSH headings. Then a series of procedures are used, based on POS-tagging, IDF computation and SVM-rank in order to assign some extra classes to each test instance and improve the recall of the initial k-NN results. In the final step, tree-based classifiers (one versus all) are used (FastXML), which actually take in to account the hierarchical relations between the MeSH terms.</p><p>Another k-nearest-neighbor approach is that of USI <ref type="bibr" coords="4,377.12,283.63,9.96,8.74">[8]</ref>, which does not take into account the hierarchy. The authors claim that the method is generic since it does not take into account the domain or use any NLP, although they believe that an NLP module would boost their performance. Given an instance the system finds the k nearest instances in the training corpus and then uses the labels of these instances for annotating it by computing semantic similarities. During the challenge they experimented with various parameters of their system, such as the value of k and they also took into account the predictions of the baselines in order to improve their results.</p><p>The CoLe and UTAI <ref type="bibr" coords="4,251.10,395.84,15.50,8.74" target="#b21">[18]</ref> teams introduce a new approach, compared to their approach during the previous challenges. This year they use only conventional information retrieval tools, such as Lucene, combined with k-NN methods. The authors also experimented with several approaches of index term extraction ranging from simple to more complex ones requiring the use of NLP.</p><p>The ESIS* systems used the Lucene index in order to find useful features for each of the MeSH classes separately. In this direction, they selected words that co-occur often with a particular class, as well as the most common terms excluding stop words. The decision function follows an k-nearest-neighbor approach, where for each test instance and given the feature extraction process they find in the Lucene index the most common training examples that decide the class of the test instance. Intuitively, the probability of a class increases if a term that is strongly associated with it is present and decreases if a frequent term is absent.</p><p>The Fudan system <ref type="bibr" coords="4,233.81,560.48,15.50,8.74" target="#b20">[17]</ref> uses a learning to rank (LTR) method for predicting MeSH headings. The MeSHLabeler algorithm consists of two components. The first component, called MeSHRanker, returns an ordered list of MeSH headings for each test instance. The ranking is determined by a combination of (a) binary classifiers, one for each MeSH heading, (b) the most similar citations to the test instance, (c) pattern matching between the MeSH headings and the title of the abstract and (d) the prediction of the MTI system. The second component, called MeSHNumber, predicts the actual number of MeSH heading that must be assigned to each test instance.</p><p>Table <ref type="table" coords="5,176.19,118.99,4.98,8.74">1</ref> describes the principal technologies that were employed by the participating systems and whether a hierarchical or a flat approach has been adopted.</p><p>Table <ref type="table" coords="5,224.84,162.55,4.13,7.89">1</ref>. Technologies used by participants in Task 3a.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Team</head><p>Approach Technologies NCBI <ref type="bibr" coords="5,156.65,198.78,12.45,6.12">[14]</ref> flat k-NN, learning-to-rank AUTH-Atypon <ref type="bibr" coords="5,182.44,206.78,17.34,6.08" target="#b19">[16]</ref> flat SVMs, MetaLabeler <ref type="bibr" coords="5,371.19,206.75,11.77,6.12" target="#b23">[20]</ref>, Ensembles IIIT <ref type="bibr" coords="5,151.01,214.72,8.48,6.12">[1]</ref> hierarchical k-NN, POS-tagging, SVM-rank, FastXML USI <ref type="bibr" coords="5,149.68,222.69,8.48,6.12">[8]</ref> flat k-NN, semantic similarities, used Baseline CoLe and UTAI <ref type="bibr" coords="5,193.54,230.66,12.45,6.12" target="#b21">[18]</ref> flat k-NN, Lucene Fudan <ref type="bibr" coords="5,154.32,238.66,15.65,6.08" target="#b20">[17]</ref> flat Logistic regression, learning-to-rank, used Baseline</p><p>Baselines. Five systems have served as baseline systems for BioASQ task 3a. The first one, dubbed BioASQ Baseline, follows a simplistic unsupervised approach to the problem and is thus easy to beat. The rest of the systems are implementations of state-of-the-art methods: the Medical Text Indexer (MTI) and the MTI First Line Index <ref type="bibr" coords="5,273.65,335.27,15.50,8.74" target="#b13">[10]</ref> were developed and are maintained by the National Library of Medicine (NLM). <ref type="foot" coords="5,302.16,345.65,3.97,6.12" target="#foot_1">8</ref> They serve as classification systems for articles of MEDLINE and are actively used by the MEDLINE curators in order to assist them in the annotation process. Furthermore, MeSH Now BF and MeSH Now HR were developed by NCBI and were among the best-performing systems in the second edition of the BioASQ challenge <ref type="bibr" coords="5,383.27,395.05,14.61,8.74">[14]</ref>. Consequently, we expected these baselines to be hard to beat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Task 3b</head><p>As mentioned above, the second task of the challenge is further divided into two phases. In the first phase, where the goal is to annotate questions with relevant concepts, documents, snippets and RDF triples, 9 teams with 24 systems participated. In the second phase, where team are requested to submit exact and paragraph-sized answers for the questions, 10 teams with 26 different systems participated.</p><p>The OAQA system described in <ref type="bibr" coords="5,294.23,529.13,15.50,8.74" target="#b26">[23]</ref> focuses on learning to answer factoids and list questions. The participants trained three supervised models, using factoid and list questions of the previous editions of the task. The first is an answer type prediction model, the second assigns a score to each predicted answer while the third is a collective re-ranking model. Although the system also participated in phase A of Task 3b its performance was much better in the factoid and list questions of phase B.</p><p>In contrast, the USTB system <ref type="bibr" coords="5,284.94,612.87,15.50,8.74" target="#b28">[25]</ref> participated only in phase A of the challenge. This approach initially uses a sequential dependence model for document retrieval. It then uses Word Embeddings (specifically the Word2Vec tool) to rank the results and improve the document retrieval of the previous step. In the final step, biomedical concepts and corresponding RDF triples are extracted, using concept recognition tools, such as MetaMap and Banner.</p><p>Another system that focused on phase A is by the IIIT team and is described in <ref type="bibr" coords="6,147.54,168.66,14.61,8.74" target="#b27">[24]</ref>. The authors relied on the PubMed search engine to retrieve relevant documents. They then applied their own snippet extraction methods, which is based on the similarity of the top 10 sentences of the retrieved documents and the query.</p><p>The HPI system <ref type="bibr" coords="6,225.19,218.33,15.50,8.74" target="#b18">[15]</ref> participated in both phases of Task 3b. The system relies on in-memory based database technology, in order to map the given questions to concepts. The Stanford CoreNLP package is used for question tokenization and the BioASQ services are used for relevant document retrieval. The selection of snippets from the retrieved documents is performed using string similarity between terms of the question and words of the documents. Exact and ideal answers are both extracted using the gold-standard snippets that were provided to the participants.</p><p>The Fudan system <ref type="bibr" coords="6,233.64,315.82,15.50,8.74" target="#b20">[17]</ref> also participated in the second task of challenge. For phase A a language model is used in order to retrieve relevant documents. For snippet extraction, the retrieved documents are searched for query keywords by giving extra credit to terms that appear close to the query keywords. Regarding exact and ideal answers, the system is split into three main components: question analysis, candidate answer generation and candidate answer ranking.</p><p>In the system of ILSP and AUEB <ref type="bibr" coords="6,312.53,389.41,15.50,8.74" target="#b16">[13]</ref> a different approach for question answering is presented based on multi-document summarization from relevant documents. The system first uses an SVR in order to assign scores to each sentence of the relevant documents. The most relevant sentences are then combined to form an answer. In order to avoid redundancy, two main approaches are examined, the use of an ILP model and the use of a more greedy strategy. Several versions of the system were examined, which differ on the features and training data that was used.</p><p>The YodaQA system, described in <ref type="bibr" coords="6,297.46,486.90,9.96,8.74" target="#b7">[4]</ref>, is a pipeline question answering system that was altered in order to make it compatible with the BioASQ task. The system first extracts natural language features from the questions and then searches its knowledge base for existing answers. It then either directly provides these passages as answers or performs passage analysis in order to produce answers from the extracted texts. Each answer is evaluated using a logistic regression classifier and those with the highest scores are provided as a final answer. The initial system was designed to answer only factoid questions, so modifications were necessary in order to be able to answer list questions.</p><p>The final system is the SNUMedinfo described in <ref type="bibr" coords="6,374.42,596.34,9.96,8.74" target="#b8">[5]</ref>. Regarding Phase A, the system participated only in the document retrieval task. The approach was based on the Indri search engine <ref type="bibr" coords="6,281.22,620.25,15.50,8.74" target="#b22">[19]</ref> and the semantic concept-enriched model presented in <ref type="bibr" coords="6,192.46,632.21,9.96,8.74" target="#b9">[6]</ref>. In phase B, the system participated only in the ideal answer generation subtask, where it ranked each passage from the provided list, based on the unique keywords it contained. A set of m (parameter of the system) passages were selected, in rank order, by selecting only passages that contain a minimum proportion of new tokens compared the already selected ones.</p><p>Table <ref type="table" coords="7,177.20,142.90,4.98,8.74">2</ref> describes the principal technologies that were employed by the participating systems and in which phase (A and/or B) have participated.</p><p>Table <ref type="table" coords="7,224.58,186.35,4.13,7.89">2</ref>. Technologies used by participants in Task 3b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference</head><p>Phase Technologies</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OAQA [23]</head><p>A,B supervised learning, collective re-ranking model USTB <ref type="bibr" coords="7,158.99,230.54,12.45,6.12" target="#b28">[25]</ref> A Word Embeddings, MetaMap, Banner IIIT <ref type="bibr" coords="7,152.20,238.51,12.45,6.12" target="#b27">[24]</ref> A PubMed search engine, sentence similarity HPI <ref type="bibr" coords="7,151.56,246.48,12.45,6.12" target="#b18">[15]</ref> A, B Stanford CoreNLP, string similarity Fudan <ref type="bibr" coords="7,159.11,254.45,12.45,6.12" target="#b20">[17]</ref> A, B language model, word similarity, ranking ILSP-AUEB <ref type="bibr" coords="7,172.96,262.42,16.97,6.12" target="#b16">[13]</ref> A, B multi-document summarization, ILP model, greedy strategy YodaQA <ref type="bibr" coords="7,167.24,278.37,8.48,6.12" target="#b7">[4]</ref> A, B natural language features, logistic regression SNUMedinfo <ref type="bibr" coords="7,181.82,286.34,8.48,6.12" target="#b8">[5]</ref> A, B Indri search engine, semantic concept-enriched model</p><p>Baselines. The BioASQ baseline of Task 3b phase B is a system similar to <ref type="bibr" coords="7,134.77,352.54,14.61,8.74" target="#b16">[13]</ref>. It applies a multi-document summarization method using Integer Linear Programming and Support Vector Regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Task 3a</head><p>During the evaluation phase of the Task 3a, the participants submitted their results on a weekly basis to the online evaluation platform of the challenge. <ref type="foot" coords="7,456.25,447.24,3.97,6.12" target="#foot_2">9</ref> The evaluation period was divided into three batches containing 5 test sets each. 18 teams participated in the task with a total of 59 systems. Two training datasets were provided: the first contains 11,804,715 articles that cover 27,097 MeSH labels; the second is a subset containing 4,607,922 articles and covers 26,866 MeSH labels. The latter dataset focuses on the journals that appear also in the test sets. The uncompressed size of those training sets in text format is 19Gb and 7.4Gb respectively. Table <ref type="table" coords="7,267.27,532.50,4.98,8.74" target="#tab_0">3</ref> shows the number of articles in each test set of each batch of the challenge. Table <ref type="table" coords="7,178.33,556.41,4.98,8.74">4</ref> presents the correspondence of the system names in the BioASQ Participants Area Leaderboard for Task 3a and the system description submitted in the track's working notes. Systems that participated in less than 4 test sets in each batch are not reported in the results. <ref type="foot" coords="7,330.64,590.70,7.94,6.12" target="#foot_3">10</ref>According to <ref type="bibr" coords="7,209.37,604.23,10.52,8.74" target="#b10">[7]</ref> the appropriate way to compare multiple classification systems over multiple datasets is based on their average rank across all the datasets. Table <ref type="table" coords="8,163.47,361.08,4.13,7.89">4</ref>. Correspondence between the public names of the participating teams on the BioASQ Participants Area leaderboard and their submissions on the lab working notes.</p><p>On each dataset the system with the best performance gets rank 1.0, the second best rank 2.0 and so on. In case two or more systems tie, they all receive the average rank. Table <ref type="table" coords="8,223.05,523.63,4.98,8.74" target="#tab_1">5</ref> presents the average rank (according to MiF and LCA-F) of each system over all the test sets for the corresponding batches. Note, that the average ranks are calculated for the 4 best results of each system in the batch according to the rules of the challenge 11 . The best ranked system is highlighted with bold typeface. As it can be noticed, on all three batches and for both flat and hierarchical measures, the Fudan system <ref type="bibr" coords="8,333.98,583.41,15.50,8.74" target="#b20">[17]</ref> clearly outperforms other approaches. The AUTH-Atypon system <ref type="bibr" coords="8,302.13,595.36,15.49,8.74" target="#b19">[16]</ref> managed to score second in two out of three batches, while the MeSH-UK0 scored second in one of the batches. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Task 3b</head><p>Phase A. Table <ref type="table" coords="9,207.45,608.30,4.98,8.74" target="#tab_2">6</ref> presents the statistics of the test data that were provided to the participants. The evaluation included five test batches. For phase A of Task 3b the systems were allowed to submit up to 10 responses per question to any of the corresponding type of annotation; that is documents, concepts, snippets and RDF triples. For each of the categories we rank the systems according to the Mean Average Precision (MAP) measure <ref type="bibr" coords="10,315.34,555.14,9.96,8.74" target="#b6">[3]</ref>. The final ranking for each batch is calculated as the average of the individual rankings in the different categories. Tables <ref type="table" coords="10,165.77,579.05,4.98,8.74" target="#tab_3">7</ref> and<ref type="table" coords="10,192.79,579.05,4.98,8.74" target="#tab_4">8</ref> present the scores of the participating systems for document and snippet retrieval in the first batch of Phase A . <ref type="foot" coords="10,331.04,589.43,7.94,6.12" target="#foot_4">12</ref> Note that systems are allowed to participate in any or all four parts of the task e.g., SNUMedinfo* retrieved only</p><p>The third edition of the BioASQ challenge has led to a number of interesting results by the participating systems. Despite them being quite advanced systems, the baselines that we provided have been beaten by the best systems. Both tasks have attracted an increasing number of participants and the number of submissions to the workshop has also increase. Therefore, we believe that the third edition of the challenge has been another contribution towards better biomedical information systems.This encourages us to continue the effort and establish BioASQ as a reference point for research in the area. In future editions of the challenge, we aim to provide even more benchmark data derived from a community-driven acquisition process.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,239.82,432.92,135.71,7.89"><head>F e b r u a r y 0 2 FFig. 1 .</head><label>21</label><figDesc>Fig. 1. The time plan of Task 3a.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,134.77,359.65,345.82,7.89;3,134.77,370.63,20.53,7.86"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The time plan of Task 3b. The two phases for each batch ran in consecutive days.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,137.45,115.91,341.75,213.48"><head>Table 3 .</head><label>3</label><figDesc>Statistics on the test datasets of Task 3a.</figDesc><table coords="8,137.45,137.40,341.75,191.99"><row><cell>Batch</cell><cell>Articles</cell><cell>Annotated Articles</cell><cell>Labels per article</cell></row><row><cell>1</cell><cell>21014</cell><cell>14,145</cell><cell>13.03</cell></row><row><cell></cell><cell>4,435</cell><cell>3,338</cell><cell>13.27</cell></row><row><cell></cell><cell>3,638</cell><cell>2,906</cell><cell>13.29</cell></row><row><cell></cell><cell>2,153</cell><cell>1,625</cell><cell>13.27</cell></row><row><cell></cell><cell>5,725</cell><cell>4,223</cell><cell>13.10</cell></row><row><cell>Subtotal</cell><cell>36,965</cell><cell>26,237</cell><cell>13.12</cell></row><row><cell>2</cell><cell>3,617</cell><cell>2,634</cell><cell>12.60</cell></row><row><cell></cell><cell>4,725</cell><cell>3,020</cell><cell>12.97</cell></row><row><cell></cell><cell>4,861</cell><cell>3,342</cell><cell>13.41</cell></row><row><cell></cell><cell>2,902</cell><cell>2,254</cell><cell>12.89</cell></row><row><cell></cell><cell>4,059</cell><cell>2,911</cell><cell>12.67</cell></row><row><cell>Subtotal</cell><cell>20,164</cell><cell>14,161</cell><cell>12.93</cell></row><row><cell>3</cell><cell>3,902</cell><cell>2,937</cell><cell>13.40</cell></row><row><cell></cell><cell>4,027</cell><cell>2,822</cell><cell>13.49</cell></row><row><cell></cell><cell>3,162</cell><cell>2,116</cell><cell>13.29</cell></row><row><cell></cell><cell>3,621</cell><cell>2,299</cell><cell>13.56</cell></row><row><cell></cell><cell>3,842</cell><cell>2,362</cell><cell>12.82</cell></row><row><cell>Subtotal</cell><cell>18,554</cell><cell>12,536</cell><cell>13.32</cell></row><row><cell>Total</cell><cell>72,430</cell><cell>52,934</cell><cell>13.11</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,134.77,115.91,345.83,440.12"><head>Table 5 .</head><label>5</label><figDesc>Average ranks for each system across the batches of task 3a for the measures MiF and LCA-F. A dash (-) is used whenever the system participated in less than 4 times in the batch. Systems that didn't participate in the challenge regularly, i.e. they didn't submit results for at least four test sets in at least one of the three batches, were excluded from the table.</figDesc><table coords="9,134.77,181.24,344.43,374.80"><row><cell>System</cell><cell cols="2">Batch 1</cell><cell cols="2">Batch 2</cell><cell cols="2">Batch 3</cell></row><row><cell></cell><cell>MiF</cell><cell>LCA-F</cell><cell>MiF</cell><cell>LCA-F</cell><cell>MiF</cell><cell>LCA-F</cell></row><row><cell>auth1</cell><cell>7.5</cell><cell>7.0</cell><cell>10.5</cell><cell>8.5</cell><cell>10.0</cell><cell>8.0</cell></row><row><cell>qaiiit system 1</cell><cell>-</cell><cell>-</cell><cell>25.0</cell><cell>25.0</cell><cell>-</cell><cell>-</cell></row><row><cell>TextCategorisation5</cell><cell>8.5</cell><cell>9.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MeSH-UK2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>9.0</cell><cell>7.75</cell></row><row><cell>Dexstr system</cell><cell>-</cell><cell>-</cell><cell>24.25</cell><cell>23.5</cell><cell>-</cell><cell>-</cell></row><row><cell>USI 20 neighbours</cell><cell>15.25</cell><cell>13.0</cell><cell>15.25</cell><cell>14.75</cell><cell>16.75</cell><cell>16.75</cell></row><row><cell>iria-1</cell><cell>16.5</cell><cell>16.0</cell><cell>21.25</cell><cell>20.75</cell><cell>17.25</cell><cell>17.25</cell></row><row><cell>pseudo n-grams</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>27.25</cell><cell>27.0</cell></row><row><cell>iria-4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>24.25</cell><cell>24.5</cell></row><row><cell>auth2</cell><cell>7.5</cell><cell>8.75</cell><cell>7.0</cell><cell>9.0</cell><cell>7.0</cell><cell>7.5</cell></row><row><cell>test unibitri</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>20.0</cell><cell>20.5</cell></row><row><cell>auth3</cell><cell>4.25</cell><cell>3.75</cell><cell>5.25</cell><cell>6.5</cell><cell>5.0</cell><cell>4.75</cell></row><row><cell>it is a test submit</cell><cell>22.5</cell><cell>22.0</cell><cell>26.0</cell><cell>25.5</cell><cell>28.0</cell><cell>28.0</cell></row><row><cell>MeSHLabeler-3</cell><cell>2.25</cell><cell>3.25</cell><cell>1.0</cell><cell>1.0</cell><cell>2.25</cell><cell>2.5</cell></row><row><cell>MeSH-UK0</cell><cell>-</cell><cell>-</cell><cell>8.0</cell><cell>7.25</cell><cell>7.75</cell><cell>10.75</cell></row><row><cell>MeSHLabeler-1</cell><cell>2.5</cell><cell>1.75</cell><cell>2.5</cell><cell>3.0</cell><cell>2.0</cell><cell>2.0</cell></row><row><cell>fork-fork</cell><cell>17.75</cell><cell>18.0</cell><cell>16.75</cell><cell>17.75</cell><cell>-</cell><cell>-</cell></row><row><cell>TextCategorisation3</cell><cell>8.75</cell><cell>11.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MeSH-UK4</cell><cell>-</cell><cell>-</cell><cell>6.25</cell><cell>7.75</cell><cell>10.5</cell><cell>11.5</cell></row><row><cell>TextCategorisation1</cell><cell>11.25</cell><cell>12.75</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>testLee15</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>27.0</cell><cell>26.5</cell></row><row><cell>spoon-spoon</cell><cell>16.25</cell><cell>16.5</cell><cell>14.5</cell><cell>16.0</cell><cell>-</cell><cell>-</cell></row><row><cell>IMI-KOI</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>30.0</cell><cell>30.5</cell></row><row><cell>auth4</cell><cell>10.0</cell><cell>10.5</cell><cell>8.0</cell><cell>9.75</cell><cell>4.5</cell><cell>4.5</cell></row><row><cell>MeSHLabeler-4</cell><cell>1.0</cell><cell>2.25</cell><cell>2.75</cell><cell>2.5</cell><cell>2.5</cell><cell>3.5</cell></row><row><cell>MeSH-UK3</cell><cell>-</cell><cell>-</cell><cell>7.75</cell><cell>11.0</cell><cell>10.25</cell><cell>12.0</cell></row><row><cell>BioASQ Baseline</cell><cell>24.25</cell><cell>24.25</cell><cell>27.75</cell><cell>27.5</cell><cell>29.25</cell><cell>29.0</cell></row><row><cell>MeSHLabeler-2</cell><cell>4.5</cell><cell>3.75</cell><cell>1.75</cell><cell>1.75</cell><cell>3.0</cell><cell>2.0</cell></row><row><cell>Default MTI</cell><cell>12.0</cell><cell>9.5</cell><cell>14.0</cell><cell>13.0</cell><cell>15.75</cell><cell>13.75</cell></row><row><cell>MeSHLabeler</cell><cell>3.5</cell><cell>2.25</cell><cell>2.5</cell><cell>2.25</cell><cell>3.75</cell><cell>3.25</cell></row><row><cell>Abstract framework</cell><cell>17.5</cell><cell>18.75</cell><cell>17.25</cell><cell>18.0</cell><cell>19.5</cell><cell>20.25</cell></row><row><cell>iria-2</cell><cell>-</cell><cell>-</cell><cell>21.0</cell><cell>20.0</cell><cell>21.75</cell><cell>22.0</cell></row><row><cell>MeSH Now BF</cell><cell>8.25</cell><cell>7.75</cell><cell>11.25</cell><cell>7.75</cell><cell>13.0</cell><cell>9.75</cell></row><row><cell>MeSH Now HR</cell><cell>23.75</cell><cell>23.75</cell><cell>20.75</cell><cell>20.75</cell><cell>31.25</cell><cell>31.75</cell></row><row><cell>USI 10 neighbours</cell><cell>18.5</cell><cell>17.75</cell><cell>18.25</cell><cell>17.5</cell><cell>20.5</cell><cell>19.75</cell></row><row><cell>IMI-KOI R</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>31.25</cell><cell>30.75</cell></row><row><cell>iria-3</cell><cell>-</cell><cell>-</cell><cell>20.0</cell><cell>19.75</cell><cell>22.0</cell><cell>22.25</cell></row><row><cell>iria-mix</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>14.0</cell><cell>13.0</cell></row><row><cell>MTI First Line Index</cell><cell>16.0</cell><cell>12.75</cell><cell>16.0</cell><cell>15.25</cell><cell>18.5</cell><cell>18.0</cell></row><row><cell>USI baseline</cell><cell>6.25</cell><cell>6.0</cell><cell>11.5</cell><cell>9.25</cell><cell>14.5</cell><cell>13.5</cell></row><row><cell>TextCategorisation4</cell><cell>8.5</cell><cell>9.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>IIIT system 2</cell><cell>-</cell><cell>-</cell><cell>18.75</cell><cell>19.0</cell><cell>-</cell><cell>-</cell></row><row><cell>MeSH-UK1</cell><cell>-</cell><cell>-</cell><cell>4.5</cell><cell>4.5</cell><cell>9.25</cell><cell>9.75</cell></row><row><cell>TextCategorisation2</cell><cell>10.0</cell><cell>11.25</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="10,134.77,115.91,345.82,98.15"><head>Table 6 .</head><label>6</label><figDesc>Statistics on the test datasets of Task 3b. The numbers concerning the documents and snippets refer to averages.</figDesc><table coords="10,163.11,150.10,286.33,63.96"><row><cell cols="8">Batch Size # of documents # of snippets Yes/No List Factoid Summary</cell></row><row><cell>1</cell><cell>100</cell><cell>11.27</cell><cell>13.33</cell><cell>33</cell><cell>22</cell><cell>26</cell><cell>19</cell></row><row><cell>2</cell><cell>100</cell><cell>10.96</cell><cell>12.95</cell><cell>16</cell><cell>28</cell><cell>32</cell><cell>24</cell></row><row><cell>3</cell><cell>100</cell><cell>9.3</cell><cell>10.98</cell><cell>28</cell><cell>17</cell><cell>26</cell><cell>29</cell></row><row><cell>4</cell><cell>97</cell><cell>9.37</cell><cell>11.97</cell><cell>29</cell><cell>23</cell><cell>25</cell><cell>20</cell></row><row><cell>5</cell><cell>100</cell><cell>5.84</cell><cell>8.53</cell><cell>28</cell><cell>24</cell><cell>22</cell><cell>26</cell></row><row><cell cols="2">total 497</cell><cell>9.35</cell><cell>11.55</cell><cell>134</cell><cell>114</cell><cell>131</cell><cell>118</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,134.77,230.50,344.44,185.80"><head>Table 7 .</head><label>7</label><figDesc>Results for batch 1 for documents in phase A of Task3b.</figDesc><table coords="10,134.77,253.74,344.44,162.57"><row><cell>System</cell><cell>Mean</cell><cell>Mean</cell><cell>Mean</cell><cell>MAP</cell><cell>GMAP</cell></row><row><cell></cell><cell>Precision</cell><cell>Recall</cell><cell>F-measure</cell><cell></cell><cell></cell></row><row><cell>SNUMedinfo1</cell><cell>0.2430</cell><cell>0.3055</cell><cell>0.2220</cell><cell>0.1733</cell><cell>0.0117</cell></row><row><cell>SNUMedinfo2</cell><cell>0.2440</cell><cell>0.3076</cell><cell>0.2231</cell><cell>0.1731</cell><cell>0.0115</cell></row><row><cell>SNUMedinfo4</cell><cell>0.2420</cell><cell>0.3062</cell><cell>0.2220</cell><cell>0.1724</cell><cell>0.0117</cell></row><row><cell>fdu3</cell><cell>0.2320</cell><cell>0.3275</cell><cell>0.2232</cell><cell>0.1719</cell><cell>0.0071</cell></row><row><cell>fdu2</cell><cell>0.2290</cell><cell>0.3242</cell><cell>0.2201</cell><cell>0.1703</cell><cell>0.0066</cell></row><row><cell>SNUMedinfo3</cell><cell>0.2340</cell><cell>0.2900</cell><cell>0.2117</cell><cell>0.1695</cell><cell>0.0076</cell></row><row><cell>fdu4</cell><cell>0.2320</cell><cell>0.3290</cell><cell>0.2242</cell><cell>0.1695</cell><cell>0.0078</cell></row><row><cell>ustb prir3</cell><cell>0.2430</cell><cell>0.3092</cell><cell>0.2245</cell><cell>0.1687</cell><cell>0.0120</cell></row><row><cell>testtext</cell><cell>0.2410</cell><cell>0.3042</cell><cell>0.2226</cell><cell>0.1681</cell><cell>0.0124</cell></row><row><cell>ustb prir4</cell><cell>0.2430</cell><cell>0.3088</cell><cell>0.2241</cell><cell>0.1666</cell><cell>0.0105</cell></row><row><cell>ustb prir1</cell><cell>0.2370</cell><cell>0.3045</cell><cell>0.2194</cell><cell>0.1663</cell><cell>0.0105</cell></row><row><cell>fdu</cell><cell>0.2200</cell><cell>0.3045</cell><cell>0.2091</cell><cell>0.1590</cell><cell>0.0067</cell></row><row><cell>SNUMedinfo5</cell><cell>0.2240</cell><cell>0.2854</cell><cell>0.2050</cell><cell>0.1569</cell><cell>0.0070</cell></row><row><cell>qaiiit system 1</cell><cell>0.1957</cell><cell>0.1757</cell><cell>0.1559</cell><cell>0.1099</cell><cell>0.0006</cell></row><row><cell>fa1</cell><cell>0.1385</cell><cell>0.0888</cell><cell>0.0935</cell><cell>0.0489</cell><cell>0.0001</cell></row><row><cell>ilsp.aueb.1</cell><cell>0.1264</cell><cell>0.1103</cell><cell>0.0922</cell><cell>0.0485</cell><cell>0.0001</cell></row><row><cell>HPI-S2</cell><cell>0.1027</cell><cell>0.1250</cell><cell>0.0841</cell><cell>0.0464</cell><cell>0.0005</cell></row><row><cell>fdu5</cell><cell>0.0370</cell><cell>0.0314</cell><cell>0.0276</cell><cell>0.0138</cell><cell>0.0000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,134.77,433.74,344.44,90.16"><head>Table 8 .</head><label>8</label><figDesc>Results for batch 1 for snippets in phase A of Task3b.</figDesc><table coords="10,134.77,456.97,344.44,66.93"><row><cell>System</cell><cell>Mean</cell><cell>Mean</cell><cell>Mean</cell><cell>MAP</cell><cell>GMAP</cell></row><row><cell></cell><cell>Precision</cell><cell>Recall</cell><cell>F-measure</cell><cell></cell><cell></cell></row><row><cell>ustb prir3</cell><cell>0.0845</cell><cell>0.0967</cell><cell>0.0785</cell><cell>0.0570</cell><cell>0.0004</cell></row><row><cell>ustb prir1</cell><cell>0.0829</cell><cell>0.0970</cell><cell>0.0774</cell><cell>0.0546</cell><cell>0.0003</cell></row><row><cell>qaiiit system 1</cell><cell>0.0616</cell><cell>0.0697</cell><cell>0.0511</cell><cell>0.0545</cell><cell>0.0002</cell></row><row><cell>testtext</cell><cell>0.0887</cell><cell>0.0948</cell><cell>0.0797</cell><cell>0.0529</cell><cell>0.0004</cell></row><row><cell>ustb prir4</cell><cell>0.0772</cell><cell>0.0882</cell><cell>0.0706</cell><cell>0.0513</cell><cell>0.0003</cell></row><row><cell>HPI-S2</cell><cell>0.0545</cell><cell>0.0686</cell><cell>0.0501</cell><cell>0.0347</cell><cell>0.0002</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_0" coords="3,144.73,645.84,335.87,8.37;3,144.73,656.80,327.18,7.86"><p>In the first two editions of the BioASQ challenge, the datasets released for Phase B contained relevant articles, snippets, concepts and RDF triples for each question.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_1" coords="5,144.73,656.80,157.99,7.86"><p>http://ii.nlm.nih.gov/MTI/index.shtml</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_2" coords="7,144.73,635.53,169.96,7.47"><p>http://participants-area.bioasq.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_3" coords="7,144.73,645.84,335.86,7.86;7,144.73,656.80,209.04,7.86"><p>According to the rules of BioASQ, each system had to participate in at least 4 test sets of a batch in order to be able to win the batch.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_4" coords="10,144.73,623.92,335.86,7.86;10,144.73,634.88,335.87,8.37;10,144.73,645.84,335.87,7.86;10,144.73,656.80,214.32,7.86"><p>In contrast to the first two editions of the challenge, the biomedical experts of BioASQ were not asked to produce golden concepts and triples prior to the challenge. The ground truth for concepts and snippets will be constructed by the experts on the basis of the material provided by the systems.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The third edition of BioASQ is supported by a conference grant from the <rs type="funder">NIH/NLM</rs> (number <rs type="grantNumber">1R13LM012214-01</rs>) and sponsored by the companies <rs type="funder">Viseo and Atypon</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_QhCa4ym">
					<idno type="grant-number">1R13LM012214-01</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>documents. It is worth noting, that document retrieval for the given questions was the most popular aspect of the task; far fewer systems returned document snippets, concepts and RDF triples. The detailed results for Task 3b phase A can be found in http://participants-area.bioasq.org/results/3b/phaseA/.</p><p>Phase B. In phase B of Task 3b, the systems were asked to generate exact and ideal answers. The systems will be ranked according to the manual evaluation of ideal answers by the BioASQ experts <ref type="bibr" coords="11,323.20,202.53,9.96,8.74" target="#b6">[3]</ref>. For reasons of completeness we report also the results of the systems for the exact answers. In contrast to the previous editions of the BioASQ challenge, the test files of Phase B included only relevant documents and snippets for each question instead of relevant documents, snippets, concepts and RDF triples. As a result, the participating systems had less information available in order to construct the exact and the ideal answers.</p><p>Table <ref type="table" coords="11,177.07,274.27,4.98,8.74">9</ref> shows the results for the exact answers in the first batch of task 3b. For systems that didn't provide exact answers for a particular kind of question we use the dash symbol "-". The results of the other batches are available at http://participants-area.bioasq.org/results/3b/phaseB/. They are not reproduced here in the interest of space. From those results we can see that some of the systems are achieving a very high (&gt; 80% accuracy) performance in the yes/no questions. The performance in factoid and list questions is not as good, indicating that there is room for improvement. On the other hand, the performance on ideal answers has improved compared to the previous years <ref type="bibr" coords="11,467.31,369.91,9.96,8.74" target="#b5">[2]</ref>, which in combination with the increase of participation leads us to believe that a significant amount of effort was invested by the participants and that the task is gaining attention. It is to be noted that those conclusions are based only on the automated evaluation measures; the manual assessment was still in progress at the time of writing this document. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="8,227.19,408.26,110.68,6.12" xml:id="b0">
	<analytic>
		<title/>
		<author>
			<orgName type="collaboration" coords="8,227.19,408.26,50.96,6.12">MeSH Now HR</orgName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,285.48,408.26,52.39,6.12">MeSH Now BF</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,227.20,424.20,51.08,6.12" xml:id="b1">
	<monogr>
		<title level="m" coord="8,227.20,424.20,44.42,6.12">qaiiit system</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,227.20,432.17,252.00,6.12" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,227.20,432.17,67.34,6.12">Abstract framework</title>
	</analytic>
	<monogr>
		<title level="m" coord="8,300.67,432.17,178.52,6.12">USI 20 neighbours, USI baseline, USI 10 neighbours</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,227.19,440.14,18.25,6.12;8,134.77,448.11,12.45,6.12;8,227.19,448.11,53.61,6.12;12,134.77,352.79,62.94,10.52" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><surname>Iria</surname></persName>
		</author>
		<title level="m" coord="8,227.19,448.11,53.61,6.12;12,134.77,352.79,62.94,10.52">MeSHLabeler-* References</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,375.32,337.64,7.86;12,151.52,386.28,329.07,7.86;12,151.52,397.24,329.07,7.86;12,151.52,408.20,94.67,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,221.31,386.28,255.20,7.86">Extreme Classification of PubMed Articles usingMeSH Labels</title>
		<author>
			<persName coords=""><forename type="first">Kamineni</forename><surname>Avinash</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fatma</forename><surname>Nausheen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Das</forename><surname>Arpita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shrivastava</forename><surname>Manish</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chinnakotla</forename><surname>Manoj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,163.10,397.24,312.54,7.86">Working Notes for the Conference and Labs of the Evaluation Forum (CLEF)</title>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,418.66,337.64,7.86;12,151.52,429.62,329.07,7.86;12,151.52,440.58,329.07,7.86;12,151.52,451.54,138.70,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="12,307.09,429.62,173.49,7.86;12,151.52,440.58,329.07,7.86;12,151.52,451.54,50.33,7.86">Results of the bioasq track of the question answering lab at clef 2014. Results of the BioASQ Track of the Question Answering Lab at CLEF</title>
		<author>
			<persName coords=""><forename type="first">George</forename><surname>Balikas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ioannis</forename><surname>Partalas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Axel-Cyrille Ngonga</forename><surname>Ngomo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anastasia</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">George</forename><surname>Paliouras</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="1181" to="1193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,462.00,337.63,7.86;12,151.52,472.96,329.07,7.86;12,151.52,483.92,329.07,7.86;12,151.52,494.88,220.76,7.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="12,387.94,483.92,92.65,7.86;12,151.52,494.88,53.33,7.86">Evaluation Framework Specifications</title>
		<author>
			<persName coords=""><forename type="first">Georgios</forename><surname>Balikas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ioannis</forename><surname>Partalas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aris</forename><surname>Kosmopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sergios</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Prodromos</forename><surname>Malakasiotis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ioannis</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ion</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicolas</forename><surname>Baskiotis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thierry</forename><surname>Artieres</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="5" to="2013" />
		</imprint>
	</monogr>
	<note>Project deliverable D4</note>
</biblStruct>

<biblStruct coords="12,142.96,505.35,337.64,7.86;12,151.52,516.30,329.07,7.86;12,151.52,527.26,208.31,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,272.66,505.35,207.93,7.86;12,151.52,516.30,100.24,7.86">Biomedical Question Answering using the YodaQA System: Prototype Notes</title>
		<author>
			<persName coords=""><forename type="first">Petr</forename><surname>Baudis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jan</forename><surname>Sedivy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,273.68,516.30,206.91,7.86;12,151.52,527.26,105.62,7.86">Working Notes for the Conference and Labs of the Evaluation Forum (CLEF)</title>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,537.73,337.63,7.86;12,151.52,548.69,329.07,7.86;12,151.52,559.65,20.99,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,213.06,537.73,167.56,7.86">SNUMedinfo at CLEF QA track BioASQ</title>
		<author>
			<persName coords=""><forename type="first">Sungbin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,420.43,537.73,60.16,7.86;12,151.52,548.69,250.07,7.86">Working Notes for the Conference and Labs of the Evaluation Forum (CLEF)</title>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,570.11,337.64,7.86;12,151.52,581.07,329.07,7.86;12,151.52,592.03,201.98,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,151.52,581.07,325.50,7.86">Semantic concept-enriched dependence model for medical information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Sungbin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jinwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sooyoung</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Heechun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Youngho</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,151.52,592.03,133.47,7.86">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="18" to="27" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,602.50,337.63,7.86;12,151.52,613.46,213.39,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,220.04,602.50,256.75,7.86">Statistical Comparisons of Classifiers over Multiple Data Sets</title>
		<author>
			<persName coords=""><forename type="first">Janez</forename><surname>Demsar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,151.52,613.46,153.92,7.86">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,623.92,337.64,7.86;12,151.52,634.88,329.07,7.86;12,151.52,645.84,329.07,7.86;12,151.52,656.80,132.43,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,189.11,634.88,291.48,7.86;12,151.52,645.84,32.54,7.86">USI at BioASQ 2015: a semantic similarity-based approach for semantic indexing</title>
		<author>
			<persName coords=""><forename type="first">Nicolas</forename><surname>Fiorini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sylvie</forename><surname>Ranwez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sbastien</forename><surname>Harispe1</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jacky</forename><surname>Montmain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vincent</forename><surname>Ranwez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,203.27,645.84,277.32,7.86;12,151.52,656.80,29.74,7.86">Working Notes for the Conference and Labs of the Evaluation Forum (CLEF)</title>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,119.67,337.64,7.86;13,151.52,130.63,236.71,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="13,338.36,119.67,142.24,7.86;13,151.52,130.63,110.77,7.86">Recommending mesh terms for annotating biomedical articles</title>
		<author>
			<persName coords=""><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aurlie</forename><surname>Nvol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,269.85,130.63,27.39,7.86">JAMIA</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="660" to="667" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,141.60,337.98,7.86;13,151.52,152.56,329.07,7.86;13,151.52,163.52,329.07,7.86;13,151.52,174.48,40.70,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,466.64,141.60,13.95,7.86;13,151.52,152.56,212.55,7.86">Recent enhancements to the nlm medical text indexer</title>
		<author>
			<persName coords=""><forename type="first">Susan</forename><forename type="middle">C</forename><surname>Schmidt Alan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">Aronson</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mork</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,387.68,152.56,92.90,7.86;13,151.52,163.52,227.34,7.86">Working Notes for the Conference and Labs of the Evaluation Forum (CLEF)</title>
		<meeting><address><addrLine>Sheffied, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1180</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,185.45,337.97,7.86;13,151.52,196.41,329.07,7.86;13,151.52,207.36,228.35,7.86" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="13,229.13,196.41,251.46,7.86;13,151.52,207.36,105.63,7.86">Evaluation Measures for Hierarchical Classification: a unified view and novel approaches</title>
		<author>
			<persName coords=""><forename type="first">Aris</forename><surname>Kosmopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ioannis</forename><surname>Partalas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Georgios</forename><surname>Paliouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ion</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<idno>CoRR, abs/1306.6802</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,218.33,337.98,7.86;13,151.52,229.29,329.07,7.86;13,151.52,240.25,109.46,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="13,217.63,218.33,243.28,7.86">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName coords=""><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,151.52,229.29,278.58,7.86">Proceedings of the ACL workshop &apos;Text Summarization Branches Out</title>
		<meeting>the ACL workshop &apos;Text Summarization Branches Out<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,251.22,337.98,7.86;13,151.52,262.18,329.07,7.86;13,151.52,273.14,329.07,7.86;13,151.52,284.10,294.30,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="13,298.46,262.18,182.13,7.86;13,151.52,273.14,183.11,7.86">Biomedical question-focused multi-document summarization: ILSP and AUEB at BioASQ3</title>
		<author>
			<persName coords=""><forename type="first">Prodromos</forename><surname>Malakasiotis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emmanouil</forename><surname>Archontakis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ion</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dimitrios</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Harris</forename><surname>Papageorgiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,355.88,273.14,124.72,7.86;13,151.52,284.10,191.61,7.86">Working Notes for the Conference and Labs of the Evaluation Forum (CLEF)</title>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,295.07,337.97,7.86;13,151.52,306.02,329.07,7.86;13,151.52,316.98,208.31,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="13,274.41,295.07,206.18,7.86;13,151.52,306.02,96.43,7.86">NCBI at the 2015 BioASQ challenge task: Baseline results from MeSH Now</title>
		<author>
			<persName coords=""><forename type="first">Yuqing</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,271.79,306.02,208.80,7.86;13,151.52,316.98,105.62,7.86">Working Notes for the Conference and Labs of the Evaluation Forum (CLEF)</title>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,327.95,337.97,7.86;13,151.52,338.91,329.07,7.86;13,151.52,349.87,94.67,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="13,221.32,327.95,255.35,7.86">HPI question answering system in the BioASQ 2015 challenge</title>
		<author>
			<persName coords=""><forename type="first">Mariana</forename><surname>Neves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,163.10,338.91,312.54,7.86">Working Notes for the Conference and Labs of the Evaluation Forum (CLEF)</title>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,360.84,337.97,7.86;13,151.52,371.80,329.07,7.86;13,151.52,382.76,329.07,7.86;13,151.52,393.72,162.08,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="13,241.19,371.80,239.40,7.86;13,151.52,382.76,58.18,7.86">AUTH-Atypon at BioASQ 3: Large-Scale Semantic Indexing in Biomedicine</title>
		<author>
			<persName coords=""><forename type="first">Yannis</forename><surname>Papanikolaou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Grigorios</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manos</forename><surname>Laliotis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nikos</forename><surname>Markantonatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ioannis</forename><surname>Vlahavas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,230.16,382.76,250.43,7.86;13,151.52,393.72,59.39,7.86">Working Notes for the Conference and Labs of the Evaluation Forum (CLEF)</title>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,404.68,337.98,7.86;13,151.52,415.64,329.07,7.86;13,151.52,426.60,329.07,7.86;13,151.52,437.56,273.60,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="13,151.52,415.64,329.07,7.86;13,151.52,426.60,173.30,7.86">The Fudan participation in the 2015 BioASQ Challenge: Large-scale Biomedical Semantic Indexing and Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Shengwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ronghui</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhikai</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yanchun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shanfeng</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,344.03,426.60,136.56,7.86;13,151.52,437.56,170.91,7.86">Working Notes for the Conference and Labs of the Evaluation Forum (CLEF)</title>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,448.53,337.97,7.86;13,151.52,459.49,329.07,7.86;13,151.52,470.45,329.07,7.86;13,151.52,481.41,190.78,7.86" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="13,193.38,459.49,287.22,7.86;13,151.52,470.45,85.47,7.86">CoLe and UTAI at BioASQ 2015: experiments with similarity based descriptor assignment</title>
		<author>
			<persName coords=""><forename type="first">Francisco</forename><forename type="middle">J</forename><surname>Ribadas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luis</forename><forename type="middle">M</forename><surname>De Campos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Vctor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alfonso</forename><forename type="middle">E</forename><surname>Darriba1</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,256.03,470.45,224.56,7.86;13,151.52,481.41,88.09,7.86">Working Notes for the Conference and Labs of the Evaluation Forum (CLEF)</title>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,492.37,337.97,7.86;13,151.52,503.33,329.07,7.86;13,151.52,514.29,307.56,7.86" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="13,447.97,492.37,32.62,7.86;13,151.52,503.33,231.43,7.86">Indri: A language model-based search engine for complex queries</title>
		<author>
			<persName coords=""><forename type="first">Trevor</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Howard</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,406.64,503.33,73.95,7.86;13,151.52,514.29,192.90,7.86">Proceedings of the International Conference on Intelligent Analysis</title>
		<meeting>the International Conference on Intelligent Analysis</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,525.26,337.97,7.86;13,151.52,536.22,329.07,7.86;13,151.52,547.18,291.29,7.86" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="13,348.57,525.26,132.02,7.86;13,151.52,536.22,78.69,7.86">Large scale multi-label classification via metalabeler</title>
		<author>
			<persName coords=""><forename type="first">Lei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Suju</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vijay</forename><forename type="middle">K</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,249.04,536.22,231.55,7.86;13,151.52,547.18,83.62,7.86">Proceedings of the 18th international conference on World wide web, WWW &apos;09</title>
		<meeting>the 18th international conference on World wide web, WWW &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="211" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,558.15,337.98,7.86;13,151.52,569.11,329.07,7.86;13,151.52,580.07,329.07,7.86;13,151.52,591.02,329.07,7.86;13,151.52,601.98,126.91,7.86" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="13,347.87,580.07,132.73,7.86;13,151.52,591.02,295.77,7.86">An overview of the bioasq largescale biomedical semantic indexing and question answering competition</title>
		<author>
			<persName coords=""><forename type="first">George</forename><surname>Tsatsaronis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Georgios</forename><surname>Balikas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Prodromos</forename><surname>Malakasiotis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ioannis</forename><surname>Partalas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthias</forename><surname>Zschunke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dirk</forename><surname>Michael R Alvers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anastasia</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sergios</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dimitris</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Polychronopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,459.27,591.02,21.32,7.86;13,151.52,601.98,55.85,7.86">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">138</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,612.95,337.98,7.86;13,151.52,623.91,329.07,7.86;13,151.52,634.87,224.70,7.86" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="13,404.47,612.95,76.13,7.86;13,151.52,623.91,17.92,7.86">Mining Multi-label Data</title>
		<author>
			<persName coords=""><forename type="first">Grigorios</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ioannis</forename><surname>Katakis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ioannis</forename><surname>Vlahavas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,363.25,623.91,117.34,7.86;13,151.52,634.87,78.49,7.86">Data Mining and Knowledge Discovery Handbook</title>
		<editor>
			<persName><forename type="first">Oded</forename><surname>Maimon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lior</forename><surname>Rokach</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer US</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="667" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,645.84,337.98,7.86;13,151.52,656.80,329.07,7.86;14,151.52,119.67,329.07,7.86;14,151.52,130.63,53.40,7.86" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="13,445.11,645.84,35.48,7.86;13,151.52,656.80,285.07,7.86">Learning to Answer Biomedical Factoid and List Questions OAQA at BioASQ 3B</title>
		<author>
			<persName coords=""><forename type="first">Zi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Niloy</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiangyu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Di</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,455.28,656.80,25.31,7.86;14,151.52,119.67,283.48,7.86">Work-ing Notes for the Conference and Labs of the Evaluation Forum (CLEF)</title>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,141.59,337.98,7.86;14,151.52,152.55,329.07,7.86;14,151.52,163.51,329.07,7.86;14,151.52,174.47,53.40,7.86" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="14,151.52,152.55,275.69,7.86">BioASQ 3b Challange 2015: Bio-Medical Question Answering System</title>
		<author>
			<persName coords=""><forename type="first">Harish</forename><surname>Yenala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Avinash</forename><surname>Kamineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manish</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manoj</forename><surname>Chinnakotla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,446.35,152.55,34.25,7.86;14,151.52,163.51,282.02,7.86">Working Notes for the Conference and Labs of the Evaluation Forum (CLEF)</title>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,185.43,337.98,7.86;14,151.52,196.39,329.07,7.86;14,151.52,207.34,329.07,7.86;14,151.52,218.30,329.07,7.86;14,151.52,229.26,20.99,7.86" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="14,319.52,196.39,161.07,7.86;14,151.52,207.34,245.85,7.86">A generic retrieval system for biomedical literatures: USTB at BioASQ2015 Question Answering Task</title>
		<author>
			<persName coords=""><forename type="first">Zhi-Juan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tian-Tian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bo-Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chun-Hua</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shao-Hui</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fang</forename><surname>Xu-Cheng Yin</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,419.86,207.34,60.74,7.86;14,151.52,218.30,250.07,7.86">Working Notes for the Conference and Labs of the Evaluation Forum (CLEF)</title>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
