<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,133.86,152.87,327.46,12.58;1,207.96,170.87,179.25,12.58">ISOFT at QALD-5: Hybrid question answering system over linked data and text data</title>
				<funder ref="#_2w83QaU #_t4MxzNC">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,147.24,208.15,63.60,11.09"><forename type="first">Seonyeong</forename><surname>Park</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Pohang University of Science and Technology</orgName>
								<address>
									<settlement>Pohang</settlement>
									<region>Gyungbuk</region>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,217.45,208.15,66.81,11.09"><forename type="first">Soonchoul</forename><surname>Kwon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Pohang University of Science and Technology</orgName>
								<address>
									<settlement>Pohang</settlement>
									<region>Gyungbuk</region>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,292.15,208.15,60.89,11.09"><forename type="first">Byungsoo</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Pohang University of Science and Technology</orgName>
								<address>
									<settlement>Pohang</settlement>
									<region>Gyungbuk</region>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,372.41,208.15,75.56,11.09"><forename type="first">Gary</forename><forename type="middle">Geunbae</forename><surname>Lee</surname></persName>
							<email>gblee@postech.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Pohang University of Science and Technology</orgName>
								<address>
									<settlement>Pohang</settlement>
									<region>Gyungbuk</region>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,133.86,152.87,327.46,12.58;1,207.96,170.87,179.25,12.58">ISOFT at QALD-5: Hybrid question answering system over linked data and text data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5B24AC933053CA393ED1097B7E7073D5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Question answering</term>
					<term>Hybrid QA</term>
					<term>SPARQL</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We develop a question answering system over linked data and text data. We combine knowledgebase-based question answering (KBQA) approach and information retrieval based question answering (IRQA) approach to solve complex questions. To solve this kind of complex question using only knowledgebase and SPARQL query, we use various methods to translate natural language (NL) phrases in question to entities and properties in knowledgebase (KB). However, converting NL phrases to entities and properties in KB many times usually has low accuracy in most KBQA. To reduce the number of converting NL phrases to words in KB, we extract clues of answers using IRQA and generate one SPARQL based on the extracted clues and analyses of the question and the semantic answer type.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Question answering (QA) systems extract short and preprocessed answers to natural language questions. QA is the fundamental goal of information extraction. In the big data era, this property of QA is increasingly gaining importance. Two popular types of QAs are knowledgebase-based question answering (KBQA) and information retrievalbased question answering (IRQA).</p><p>Recently, structured and semantically-rich KBs have been released; examples include Yago <ref type="bibr" coords="1,173.89,564.19,10.64,11.09" target="#b0">[1]</ref>, DBpedia <ref type="bibr" coords="1,229.40,564.19,10.64,11.09" target="#b1">[2]</ref>, and Freebase <ref type="bibr" coords="1,302.30,564.19,10.64,11.09" target="#b2">[3]</ref>. As an increasing quantity of resource description framework (RDF) data are published in linked form, finding intuitive ways to access the data is becoming increasingly important. Several QAs use RDF data; examples include Aqualog <ref type="bibr" coords="1,228.65,600.19,10.64,11.09" target="#b3">[4]</ref>, template-based SPARQL learner (TBSL) <ref type="bibr" coords="1,420.13,600.19,11.70,11.09" target="#b4">[5]</ref> and Parasempre <ref type="bibr" coords="1,160.28,612.19,10.63,11.09" target="#b5">[6]</ref>. Because it is structured in linked form, the semantic web inference is possible <ref type="bibr" coords="1,146.24,624.19,10.64,11.09" target="#b6">[7]</ref>, and KB is curated and verified by human, KBQA can provide more accurate answers than IRQA. However KBQAs require that the user's phrase be translated to entities and properties that exist in a KB. Many previous works use PATTY <ref type="foot" coords="1,434.10,647.87,3.24,7.17" target="#foot_0">1</ref> , pattern based matching, and other methods, but it is still not sufficient to achieve high accuracy. Especially in complex question such as 'who is the architect of the tallest building in Japan?', system needs to map the predicate and generate queries in sequential way, and errors in mappings result in propagating errors.</p><p>To solve the problem, we combine IRQA approach and KBQA approach. We extract clues from the question using sequential phrase queries which are generated segmentation of the question using NLP tools such as chunking and dependency parsing. We define the answer clue as extracted entities to find final answer. We use multi-source tagged text database which is tagged with co-reference resolved and disambiguated form using Stanford co-reference resolution tool<ref type="foot" coords="2,317.88,256.31,3.24,7.17" target="#foot_1">2</ref> and DBPedia Spotlight<ref type="foot" coords="2,415.68,256.31,3.24,7.17" target="#foot_2">3</ref>  <ref type="bibr" coords="2,421.44,255.78,11.46,12.15" target="#b7">[8]</ref>. To generate query, we don't need to detect entity from every text in database in runtime because the entities have been already detected in document processing time. We concatenate next query and the answer of the first query and the next rightmost phrase. We repeat this process to find the answer of the given question. If we failed to find appropriate answer clue, we generate SPARQL query for one triple.</p><p>We use semantic similarity based on explicit semantic analysis (ESA) <ref type="bibr" coords="2,426.90,328.27,11.70,11.09" target="#b8">[9]</ref> to map predicates in the NL question to uniform resource identifiers (URIs) of properties in the KB. ESA converts target strings to semantic vectors that can convey their explicit meaning as weighted vectors of Wikipedia concepts, and calculating the similarity of two vectors reveals the semantic relatedness of the two strings from which the vectors were generated. We also increase the effectiveness of mapping the NL words to URIs by concatenating additional information to the predicate. If the property is related to arithmetic measurement (e.g. length or height), we map the predicate by pattern matching and rules.</p><p>After the final sequential phrase query is processed, we extract answer candidates and select the answer by using answer types and rules in the case of questions which requires to arithmetic comparison (e.g. 'highest mountain' and 'tallest building').</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.1</head><p>Overall system architecture Fig. <ref type="figure" coords="3,173.11,484.92,3.77,9.02">1</ref>. Overall proposed QA system: processes are described in the text</p><p>In our system (Fig <ref type="figure" coords="3,213.03,508.98,3.60,9.02">1</ref>), first we analyze question, extract the sequential phrase query and classify the semantic answer type (SAT). For example, when the question is 'who is the architect of the tallest building in Japan?', the question is divided into three phrases; 'is the architect', 'of the tallest building', and 'in Japan'. Because most given hybrid questions in the QALD-5 are quite long to find answer at one time. We first search the query containing 'the tallest building' and 'in Japan' from the multi-information tagged text data. Then, we extract entities such as 'Tokyo_Skytree', 'To-kyo_Tower' and so on. If the query contains comparative form such as 'deeper' or superlative form such as 'tallest', we map these indicator to properties in KB. To extract the height of each entities, we generate SPARQL query such as SELECT DISTINCT ?y WHERE { res:Tokyo_Skytree dbo:height ?y }. Then, we compare the heights among the entities to get the tallest entity. If we failed to find answer, we generate SPARQL query to get answer from the tallest entity and the property which is in the KB mapped from the dependent of main verb or main verb itself. The SPARQL query is as follow, SELECT DISTINCT ?y WHERE { res:Tokyo_Skytree dbo:architect ?y }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Basic Analysis</head><p>To find the correct answer, we should analyze the input of the system, the question, carefully and thoroughly in various aspects ( </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2. Basic analyses of a question</head><p>The QA system oriented techniques include Q2S analysis, LAT extraction and phrase extraction. The Q2S analysis is a rule-based analysis that recovers the corresponding declarative sentence from the interrogative or imperative sentence, the question. This analysis uses the result of the previous analyses, LAT, modal verbs, preceding preposition (e.g. "For whom does …"), usage of 'be' or 'do', and interrogative, to match the rules built for this system. The missing information which is asked through the question, which is called focus, is added to make a complete declarative sentence. LAT is a part of the question that limits the type of the answer, and gives a strong hint for SAT classification. LAT extraction is done along with Q2S analysis. Phrase extraction is to extract predicate phrase and prepositional phrase for query generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Query Generation</head><p>We have to generate Apache Lucene<ref type="foot" coords="5,284.46,305.87,3.24,7.17" target="#foot_6">7</ref> queries to find sentences that contain the answer of the question from multi-information tagged text database. Some questions in QALD task cannot be solved with a single query. For example, a question 'who is the architect of the tallest building in Japan?' should be queried twice; 'Tokyo Skytree' from 'the tallest building in Japan' and 'Nikken Sekkei' from 'the architect of Tokyo Skytree' (Fig <ref type="figure" coords="5,180.78,368.28,3.61,9.02">3</ref>).</p><p>We devise sequential phrase queries to answer such questions. A unit of queries is a prepositional phrase or a predicate phrase. We generate the first query with concatenating the two rightmost phrases and find the answer. The next query is concatenation of the answer of the first query and the next rightmost phrase. We repeat this process to find the answer of the given question. The result of query: "tallest building in Japan" have many entities. If we failed to generate query in rightmost phrase, we used chunker and generate sequential query including more than one named entity. We filter many entities and select one answer cue. We can know whether the sentence including answer clues is related to query or not. We measure cosine similarity, Jaccard similarity between answer clue sentence and question statement. Also, we check whether named entities in question are in the answer clue statements or not. If the question include polarity such as "tallest", we select the entities satisfy the condition. If we failed to find answer clue, we generate SPARQL query to get answer candidates. Fig. <ref type="figure" coords="6,245.82,307.38,3.76,9.02">3</ref>. Sequential phrase queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Semantic Answer Type Classification</head><p>Semantic answer type (SAT) is a very important feature in reducing wrong answer candidates. We can infer the SAT from the question before finding the answer candidates. For example, the answer of 'what did Bill Gates found?' can't be found before searching the database, but the SAT, 'ORGANIZATION' can be inferred from the question itself. Instead of other typesets built for SAT classification such as UIUC typeset <ref type="bibr" coords="6,138.40,415.21,15.37,11.09" target="#b9">[10]</ref>, we use open typeset from DBPedia because the DBPedia typeset covers most entities properly and it can be extended as more entities are added to Wikipedia.</p><p>To classify the SAT, we used features from previous analyses such as keywords and LAT. The instances of the entities are not used because type rather than instance of each entity is important. For example, SAT of 'what is the capital of France?' is more similar than that of 'what is the capital of Germany?' than 'who is the president of France?', even though the first and the third question shares the same NE, 'France'. Thus we replaced the NE instances to type of each NE for features. We used other features such as interrogative wh-word, predicate and its arguments.</p><p>We train the SAT classifier with libSVM 8 and train data from four years of previous QALD challenges. We achieve 71.42 % accuracy for 3-level type ontology and 84.62 % for 2-level type ontology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Multi-information Tagged Text Database</head><p>Even though most classical IR systems search the answer from plain text, we search the answer from multi-informational tagged text database. Texts on web are ambiguous and not structured, such as in 'Obama is the president of America. He is born in Hawaii.', where 'He' should be co-reference resolved to 'Obama', and 'Obama' should be disambiguated to 'Barack Obama'. We use Stanford coreference tool for co-reference resolution and DBPedia Spotlight for disambiguation. However, such processing is not a fast job at runtime: It takes more than three weeks to process all texts in Wikipedia. That is why we store such information along plain text in multi-information tagged text database. For each sentence in Wikipedia, we have stored 1. plain text, 2. tagged text with co-reference resolution and disambiguation information, 3. title from which Wikipedia page the sentence is, 4. PoS tagging, dependency parsing, and SRL result. We used Apache Lucene to store and index these attributes.</p><p>When the system throws a SPARQL query, the Apache Lucene search engine finds the related 'tagged texts'. Our system selects all NEs from all 'tagged texts' as the answer candidates. Of course most of them are irrelevant to the question, we use our SAT and answer selection module to prune out such answer candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">SPARQL query template generator</head><p>We detect words from each question to extract the appropriate SPARQL template.</p><p> Questions including arithmetic information ─ If the query contains a comparative word such as 'deeper' or a superlative word such as 'deepest', we map these indicators to properties in KB based on mapping rules such as PATTY. We generate SPARQL query to find the answer candidates for further comparison. If the question contains comparative word 'deeper', we select answer candidates those are 'deeper' than the criterion in the question. If the question contains superlative word 'deepest', we sort the answer candidates searched from the query and get the candidate which is the 'deepest'. We used polarity information of adjectives. If the adjectives "highest", we sorted entities in descent order, and the adjectives "lowest", we sorted entities in ascending order.  Yes/No questions ─ Using lexical information, we detect whether the question is a 'wh-question' or 'yes/no question'. If the question does not include 'wh-keywords' nor 'list-question keywords' (e.g., 'Give', 'List'), we regard the question as a 'yes/no question'.  Simple question ─ If the question is not including arithmetic information nor yes/no question, we generate SPARQL query for one triple. In this case, we map predicates to properties in KB using lexical matching. If we fail to map using lexical, we try to use semantic similarity same as our previous work <ref type="bibr" coords="7,334.75,566.83,15.33,11.09" target="#b10">[11]</ref>. The proposed system extract important words which is related to predicate meaning. For example, just verb such as "start" did not assign a high score to the desired predicate URI, but concatenating additional information which make up for meaning of predicate works well. The proposed system used ESA lib which converts strings to semantic weighted vectors of Wikipedia concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment</head><p>We use the QALD-5 hybrid question test dataset for task 1 to evaluate the proposed method and use multi-information tagged text database built from Wikipedia as the text database and DBpedia 3.10 (DBPedia 2014) as the KB. We mainly compare two approaches. One is without semantic answer type and the other uses sematic answer type to filter answer candidates. We follow the QALD measurement. Correct states for how many of questions were answered with an F-1 measure of 1. Partially correct specifies how many of the questions were answered with an F-1 measure strictly between 0 and 1. Recall, Precision report the measures with respect to the number of processed questions. F-1 Global reports the F-1 measure with respect to the total number of questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head><p>To evaluate the QA system, we use global precision, recall and F-measure (Table <ref type="table" coords="8,459.72,330.30,3.61,9.02" target="#tab_1">1</ref>).</p><p>With S_AT yield a higher F-measure than Without S_AT. For example, semantic answer type detector extract "City" as answer type when the sentence "In which city where Charlie Chaplin's half brothers born?" is processed, so answer candidates such as United Kingdom and England can be filtered. a. Semantic answer type: Place b. Query generation: man-made lake Australia, deeper 100 c. Success: extract answer clues of "man -made lake Australia". The proposed system compare the length of each named entities and check the more than one river is deeper than 100 meters. The proposed system find the river which is deeper than 100 meters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future work</head><p>To process the complex question with reducing error in mapping NL-predicate to KB property, we use both KBQA approach and IRQA approach. First, we search query from multi-information tagged text data. If the results are not appropriate or related to arithmetic, we generate SPARQL query and search KB. This combined approach works reduce error from mapping predicate in NL to predicate URI. Furthermore, our semantic answer type detector filter answer candidates. However, still many questions are answered wrong because of the failure of not finding relevant answer clue, mapping NL predicate to predicate URI and query segmentation. We will focus on extending query to find relevant answer clue well and mapping NL-predicate to predicate URI to improve QA system in the future work. Also, we developed semantic parsing and open information extraction for question processing well.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,308.23,450.67,162.35,11.10;4,124.68,462.67,345.96,11.09;4,124.68,474.67,346.18,11.09;4,124.68,486.67,345.96,11.09;4,124.68,498.67,138.56,11.09;4,136.02,510.67,334.80,11.09;4,124.68,522.67,345.82,11.09;4,124.68,534.67,345.86,11.09;4,124.68,546.67,288.60,11.09;4,413.16,546.35,3.24,7.17;4,418.92,546.67,51.78,11.09;4,124.68,558.67,345.99,11.09;4,124.68,570.67,345.99,11.09;4,124.68,582.67,345.92,11.09;4,124.68,594.67,14.49,11.09;4,139.14,594.35,3.24,7.17;4,144.90,594.67,325.72,11.09;4,124.68,606.67,36.19,11.09;4,160.80,606.35,3.24,7.17;4,164.04,606.67,306.58,11.09;4,124.68,618.67,70.03,11.09"><head>Fig 2 ).</head><label>2</label><figDesc>We use both statistical and rulebased approaches to analyze the questions. These analyses include ordinary NLP techniques such as tokenization and part of speech (PoS) tagging, and QA system oriented techniques such as question to statement (Q2S) analysis, lexical answer type (LAT) extraction, and SAT classification. The ordinary NLP techniques include tokenization, part of speech tagging, dependency parsing, keyword extraction, term extraction, and named entity (NE) extraction. This information is not only important features for SAT extraction and answer selection, but also a basis for further question processing. We use ClearNLP 4 for tokenization, PoS tagging and dependency parsing. Keyword extraction is simply removing stop words and a few functional words, such as 'the', 'of' and 'please', from the question. Term extraction is finding nouns and verbs and their synonyms which exist in Word-Net 5 dictionary. NE extraction uses Spotlight to map NEs in the question to entities in DBPedia 6 . The keywords, terms, and NEs are further used for SAT classification and query generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,154.02,640.21,316.61,11.09;9,172.02,652.21,90.51,11.09;9,190.02,664.21,142.77,11.09;10,190.02,148.21,280.61,11.09;10,208.02,160.21,49.74,11.09;10,190.02,172.21,280.64,11.09;10,208.02,184.21,53.65,11.09;10,154.02,196.21,252.90,11.09;10,190.02,208.21,132.15,11.09;10,190.02,220.21,225.11,11.09;10,190.02,232.21,267.77,11.09"><head>9 .</head><label>9</label><figDesc>Which movie by the Coen brothers stars John Turturro in the role of a New York City playwright? a. Semantic answer type: PLACE b. Query generation: role of a New York City playwright/Coen brother stars c. Fail: cannot find relevant answer clue in multi-tagged text database and KB. 10. Which of the volcanoes that erupted in 1550 is still active? a. Semantic answer type: place b. Query generation: volcanoes/erupted in 1550/active c. Fail: cannot generate appropriate query to extract answer clue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,126.96,411.67,343.69,273.05"><head>Table 1 .</head><label>1</label><figDesc>Evaluation results for the QALD-5 test dataset.</figDesc><table coords="8,126.96,425.17,343.69,259.55"><row><cell>Method</cell><cell cols="3">Total Correct Partially</cell><cell>Recall</cell><cell>Precision</cell><cell>Global</cell></row><row><cell></cell><cell></cell><cell cols="2">Correct</cell><cell></cell><cell cols="2">F-1 measure</cell></row><row><cell>Without S_AT</cell><cell>10</cell><cell>2</cell><cell>1</cell><cell>1.00</cell><cell>0.78</cell><cell>0.26</cell></row><row><cell>With S_AT</cell><cell>10</cell><cell>3</cell><cell>0</cell><cell>1.00</cell><cell>1.00</cell><cell>0.33</cell></row><row><cell cols="2">Error Case Analysis</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">1. Where was the "Father of Singapore" born?</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">a. Semantic answer type: PLACE</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">b. Query generation: "Father of Singapore" born/ Where</cell><cell></cell></row><row><cell></cell><cell cols="5">c. Fail reason: cannot find relevant answer clue</cell><cell></cell></row><row><cell cols="7">2. Which Secretary of State was significantly involved in the United States'</cell></row><row><cell cols="4">dominance of the Caribbean?</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">a. Semantic answer type: ADMINISTRATIVEREGION</cell><cell></cell></row><row><cell></cell><cell cols="6">b. Query generation: Unites' dominance of the Carbbean/in-</cell></row><row><cell></cell><cell cols="2">volve/Secretary</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="6">c. Which Secretary/of/State/was significantly involved in/the United</cell></row><row><cell></cell><cell cols="4">States' dominance of the Caribbean</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="6">d. Fail reason: cannot find relevant answer clue (both IR approach</cell></row><row><cell></cell><cell cols="2">and KB approach)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">3. Who is the architect of the tallest building in Japan?</cell><cell></cell></row><row><cell></cell><cell cols="4">a. Semantic answer type: Building</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,139.44,673.28,331.18,9.96;1,136.02,684.32,40.04,9.96"><p>http://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yagonaga/patty/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,129.96,673.28,154.32,9.96"><p>http://nlp.stanford.edu/projects/coref.shtml</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,129.96,684.32,196.69,9.96"><p>https://github.com/dbpedia-spotlight/dbpedia-spotlight</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,129.96,662.30,111.82,9.96"><p>https://github.com/clir/clearnlp</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="4,129.96,673.28,108.49,9.96"><p>https://wordnet.princeton.edu/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="4,129.96,684.32,85.03,9.96"><p>http://wiki.dbpedia.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="5,129.96,684.32,110.53,9.96"><p>https://lucene.apache.org/core/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>. This work was supported by the <rs type="programName">ICT R&amp;D program</rs> of <rs type="grantNumber">MSIP/IITP [R0101-15-0176</rs>, <rs type="projectName">Development of Core Technology for Human-like Self-taught Learning based on a Symbolic Approach] and ATC(Advanced Technology Center) Program -"Development of Conversational Q&amp;A Search Framework Based On Linked Data: Project</rs> No. <rs type="grantNumber">10048448</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_2w83QaU">
					<idno type="grant-number">MSIP/IITP [R0101-15-0176</idno>
					<orgName type="project" subtype="full">Development of Core Technology for Human-like Self-taught Learning based on a Symbolic Approach] and ATC(Advanced Technology Center) Program -&quot;Development of Conversational Q&amp;A Search Framework Based On Linked Data: Project</orgName>
					<orgName type="program" subtype="full">ICT R&amp;D program</orgName>
				</org>
				<org type="funding" xml:id="_t4MxzNC">
					<idno type="grant-number">10048448</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,132.68,556.22,337.81,9.96;10,141.66,567.20,328.98,9.96;10,141.66,578.18,60.27,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,373.99,556.22,96.49,9.96;10,141.66,567.20,37.64,9.96">Yago: a core of semantic knowledge</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,196.65,567.20,256.89,9.96">Proceedings of the 16th international conference on World Wide Web</title>
		<meeting>the 16th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007-05">2007, May</date>
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.68,589.22,337.93,9.96;10,141.66,600.20,328.94,9.96;10,141.66,611.18,83.80,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,182.11,600.20,284.71,9.96">DBpedia-a large-scale, multilingual knowledge base extracted from wikipedia</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Isele</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jentzsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kontokostas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">N</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">.</forename><forename type="middle">.</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,141.66,611.18,80.20,9.96">Semantic Web Journal</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.67,622.22,337.87,9.96;10,141.66,633.20,329.02,9.96;10,141.66,644.18,328.95,9.96;10,141.66,655.22,22.70,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,428.78,622.22,41.76,9.96;10,141.66,633.20,258.98,9.96">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,416.83,633.20,53.84,9.96;10,141.66,644.18,266.52,9.96">Proceedings of the 2008 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2008 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008-06">2008, June</date>
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.68,148.22,337.99,9.96;11,141.66,159.20,328.93,9.96;11,141.66,170.18,183.18,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,328.44,148.22,142.22,9.96;11,141.66,159.20,198.34,9.96">AquaLog: An ontology-driven question answering system for organizational semantic intranets</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Uren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Motta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pasin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,345.85,159.20,124.74,9.96;11,141.66,170.18,128.89,9.96">Web Semantics: Science, Services and Agents on the World Wide Web</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="72" to="105" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.68,181.22,337.96,9.96;11,141.66,192.20,328.94,9.96;11,141.66,203.18,261.03,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,195.73,192.20,188.44,9.96">Template-based question answering over RDF data</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Unger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bühmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Ngonga Ngomo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cimiano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,401.63,192.20,68.97,9.96;11,141.66,203.18,181.99,9.96">Proceedings of the 21st international conference on World Wide Web</title>
		<meeting>the 21st international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012-04">2012, April</date>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.68,214.22,337.94,9.96;11,141.66,225.20,94.26,9.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,281.45,214.22,127.84,9.96">Semantic parsing via paraphrasing</title>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,417.01,214.22,53.61,9.96;11,141.66,225.20,15.20,9.96">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.68,236.18,337.89,9.96;11,141.66,247.22,329.07,9.96;11,141.66,258.20,29.26,9.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,259.19,236.18,211.38,9.96;11,141.66,247.22,74.19,9.96">TRIPLE -A query, inference, and transformation language for the semantic web</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sintek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Decker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,232.87,247.22,105.92,9.96">Semantic Web -ISWC 2002</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="364" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.68,269.18,337.93,9.96;11,141.66,280.22,289.23,9.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,234.23,269.18,210.44,9.96">DBpedia spotlight: shedding light on the web of documents</title>
		<author>
			<persName coords=""><forename type="first">Pablo</forename><forename type="middle">N</forename><surname>Mendes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,455.10,269.18,15.50,9.96;11,141.66,280.22,237.65,9.96">Proceedings of the 7th International Conference on Semantic Systems</title>
		<meeting>the 7th International Conference on Semantic Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.68,291.20,337.90,9.96;11,141.66,302.18,287.52,9.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,326.10,291.20,144.47,9.96;11,141.66,302.18,159.86,9.96">Computing Semantic Relatedness Using Wikipedia-based Explicit Semantic Analysis</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,317.27,302.18,21.92,9.96">IJCAI</title>
		<imprint>
			<date type="published" when="2007-01">2007, January</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1606" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.30,313.22,338.29,9.96;11,141.66,324.20,328.84,9.96;11,141.66,335.18,38.86,9.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,250.05,313.22,220.54,9.96;11,141.66,324.20,23.36,9.96">Learning Question Classifiers: The Role of Semantic Information</title>
		<author>
			<persName coords=""><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,171.07,324.20,281.76,9.96">Proceedings of the 19th international conference on Computational linguistics</title>
		<meeting>the 19th international conference on Computational linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.30,346.22,338.29,9.96;11,141.66,357.20,195.69,9.96" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,299.16,346.22,171.43,9.96;11,141.66,357.20,156.04,9.96">ISOFT at QALD-4: Semantic similarity-based question answering system over linked data</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Shim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">G</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,313.15,357.20,19.36,9.96">CLEF</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
