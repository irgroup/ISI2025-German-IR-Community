<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,164.11,117.09,287.12,12.64;1,207.46,135.03,200.43,12.64">Learning to Answer Biomedical Factoid &amp; List Questions: OAQA at BioASQ 3B</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,162.07,172.86,29.81,8.76"><forename type="first">Zi</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Language Technologies Institute</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Avenue</addrLine>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,198.98,172.86,47.45,8.76"><forename type="first">Niloy</forename><surname>Gupta</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Language Technologies Institute</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Avenue</addrLine>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,253.40,172.86,50.30,8.76"><forename type="first">Xiangyu</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Language Technologies Institute</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Avenue</addrLine>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,310.69,172.86,22.23,8.76"><forename type="first">Di</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Language Technologies Institute</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Avenue</addrLine>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,340.30,172.86,40.18,8.76"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Language Technologies Institute</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Avenue</addrLine>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,404.50,172.86,48.80,8.76"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Language Technologies Institute</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Avenue</addrLine>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,164.11,117.09,287.12,12.64;1,207.46,135.03,200.43,12.64">Learning to Answer Biomedical Factoid &amp; List Questions: OAQA at BioASQ 3B</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F3D91915A330F3E4DE51E9F14C54BAC5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>biomedical question answering</term>
					<term>learning to answer questions</term>
					<term>type coercion</term>
					<term>answer scoring</term>
					<term>answer ranking</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the CMU OAQA system evaluated in the BioASQ 3B Question Answering track. We first present a three-layered architecture, and then describe the components integrated for exact answer generation and retrieval. Using over 400 factoid and list questions from past BioASQ 1B and 2B tasks as background knowledge, we focus on how to learn to answer questions using a gold standard dataset of question-answer pairs, using supervised models for answer type prediction and candidate answer scoring. On the three test sets where the system was evaluated (3, 4, and 5), the official evaluation results have shown that the system achieves an MRR of .1615, .5155, .2727 for factoid questions, and an F-measure of .0969, .3168, .1875 for list questions, respectively; five of these scores were the highest reported among all participating systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A number of shared tasks have been organized to evaluate the performance of biomedical question answering (QA) systems. For example, TREC Genomics QA <ref type="bibr" coords="1,435.36,470.44,11.62,8.76" target="#b6">[4]</ref> focused on genomics questions for a subdomain of biomedical research, and made available a relatively small set of factoid and list questions (a total of 64 questions from two years). Recently, the CLEF QA4MRE task <ref type="bibr" coords="1,285.39,506.30,16.60,8.76" target="#b13">[11]</ref> organized a pilot track on multiple choice QA for questions related to Alzheimer's disease. Compared to these shared tasks, the BioASQ challenge <ref type="bibr" coords="1,213.30,530.21,16.60,8.76" target="#b15">[13]</ref> covers a wider range of biomedical subdomains, and releases a larger topic set with much more detailed gold standard data, including relevant documents, snippets, concepts, triples and both exact and ideal answers.</p><p>This paper reports results for the last three batches of BioASQ phase 3B, and focuses on factoid and list QA for Phase B and Phase A. We describe both the system architecture and individual components. First, we adapted a leveraged a UIMA <ref type="foot" coords="1,473.29,588.45,3.49,6.13" target="#foot_0">1</ref> based three-layered architecture that was previously developed for biomedical QA tasks (TREC Genomics-style questions <ref type="bibr" coords="1,292.96,614.00,16.60,8.76" target="#b18">[16]</ref> and CLEF QA4MRE-style questions <ref type="bibr" coords="1,460.69,614.00,15.93,8.76" target="#b12">[10]</ref>) for the BioASQ challenge; the architecture consists of a generic component layer (BaseQA), a biomedical component layer (BioQA) and a BioASQ-specific component layer. Using the development set, we also investigated whether it is possible to design and train supervised models to answer factoid and list questions, without the use of manually-constructed rules or predefined templates. We utilized supervised models to merge answer scores obtained from various sources, a technique utilized by some systems in past years <ref type="bibr" coords="2,213.29,168.02,15.77,8.76" target="#b16">[14,</ref><ref type="bibr" coords="2,230.71,168.02,7.19,8.76" target="#b11">9]</ref>, and to predict likely answer type(s) from among the 133 semantic types in the UMLS Semantic Network.</p><p>Our hypothesis, described in the OAQA technical report <ref type="bibr" coords="2,381.48,191.93,11.62,8.76">[2]</ref> and motivated by recent success in building and optimizing a TREC Genomics-style QA system, is that informatics challenges like BioASQ are best met through careful design of a flexible and extensible architecture, coupled with continuous, incremental experimentation and optimization over various combinations of existing state-of-the-art components, rather than relying on a single "magic" component or single component combination. We leveraged an existing framework <ref type="bibr" coords="2,268.16,263.66,15.27,8.76" target="#b18">[16]</ref>, integrated commonly adopted components (e.g. MetaMap<ref type="foot" coords="2,173.50,273.98,3.49,6.13" target="#foot_1">2</ref> , ClearNLP<ref type="foot" coords="2,224.36,273.98,3.49,6.13" target="#foot_2">3</ref> , etc.) and extracted features for statistical machine learning. Over the 70 days of intensive development between April 2 to Jun 10, our experiment database has recorded 717 experiments. Among 669 successful experiments, there were 167 executing the training pipeline (177.5 topics per run on average), 422 executing the testing pipeline (24.1 topics per run on average) and 80 "dummy" runs used to cache service results (284.5 topics per run on average). The official evaluation results indicate that the system achieves MRR scores of .1615, .5155, and .2727 for factoid questions, and F-measure score of .0969, .3168, and .1875 for list questions; five of these results are the highest scores reported among all participating systems. The architecture frameworks and most of the components are currently available as open-source downloads, and we are planning to release the remaining components that are used in the system as open source software in the near future.</p><p>The goal of this working note is to describe the overall architecture and the components that are necessary in order to rebuild the system and reproduce the results from the open-source software.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Architecture</head><p>The three-layered architecture uses the UIMA ECD/CSE framework<ref type="foot" coords="2,416.48,496.50,3.49,6.13" target="#foot_3">4</ref>  <ref type="bibr" coords="2,424.25,498.13,10.79,8.76" target="#b5">[3,</ref><ref type="bibr" coords="2,436.69,498.13,11.83,8.76" target="#b18">16]</ref>, which extends the UIMA framework with a YAML <ref type="foot" coords="2,315.45,508.45,3.49,6.13" target="#foot_4">5</ref> -based language which supports formal, declarative descriptors for the space of system and component configurations to be explored during the optimization step. The CSE framework also provides evaluation APIs, experimental result persistence, and customized execution control flow to support automatic performance evaluation and optimization via scalable web services (UIMA-AS).</p><p>The first layer BaseQA 6 is designed for domain-independent QA components, and includes the basic input/output definition of a QA pipeline, intermediate data objects (such as answer type, question type, relevant passages, relevant concepts, etc.), QA evaluation components, and data processing components (e.g. LingPipe <ref type="foot" coords="3,476.61,118.56,3.49,6.13" target="#foot_6">7</ref>and Apache OpenNLP<ref type="foot" coords="3,227.66,130.51,3.49,6.13" target="#foot_7">8</ref> wrappers, Lucene<ref type="foot" coords="3,307.64,130.51,3.49,6.13" target="#foot_8">9</ref> -based passage retrieval component, Lib-Linear <ref type="foot" coords="3,160.77,142.47,6.97,6.13" target="#foot_9">10</ref> wrapper, and models applicable to generic English questions). Although the BioASQ task focuses on the biomedical domain, it is the first shared task on QA to combine four types of questions and evaluate both exact and ideal answers along with other relevant elements (e.g. triples), so many aspects of the existing BaseQA framework were extended to accommodate BioASQ application development. We modified the intermediate object and input/output object definition (UIMA type system) according to the task requirements. For example, we added two new attributes Begin/EndSection to each Passage type, and changed the Begin/EndPosition attributes to Begin/EndPositionInSection. We also provided a BioASQ-compatible JSON format reader and writer at the BaseQA level, which we believe can be widely used in various QA tasks beyond BioASQ. We also implemented evaluation methods according to the specific BioASQ evaluation requirements.</p><p>In the second layer (BioQA), we implemented biomedical resources that can be used in any biomedical QA task (outside the context of BioASQ), including UMLS Terminology Services (UTS) <ref type="foot" coords="3,252.81,311.31,6.97,6.13" target="#foot_10">11</ref> -based synonym expansion component, a MetaMap annotation component, etc. For the components that are included in the BaseQA layer, we also created a descriptor for the component at the BioQA level by overriding the model value with a path to the specific model tailored for biomedical domain, where applicable. For example, the ClearNLP wrapper, which is provided at the BaseQA level with the default general-en model specified in the descriptor, has a new descriptor for the bioinformatics-en model, trained on the CRAFT treebank, defined at the BioQA level. Although the training and testing processes are performed on the BioASQ development set, the derived models can also be used for other biomedical questions, so we also place the models and training components in the BioQA layer.</p><p>A few BioASQ-specific components were integrated in the third design layer; for example, GoPubMed services are only hosted for the purpose of the BioASQ challenge. The introduction of this task-specific layer will facilitate easy replacement of proprietary and restricted components when we adapt the system to other biomedical QA tasks or deploy the system as a real-world application. The end-to-end training and testing pipelines are also defined in this layer. The test descriptor used for Batch 5 in Phase B is shown in Listings 1.1 to 1.3. Similar to the resource-wrapper providers which we introduced for the TREC Genomics QA task <ref type="bibr" coords="3,356.73,517.65,15.27,8.76" target="#b18">[16]</ref>, we also created a caching layer, using Redis <ref type="foot" coords="3,208.01,527.97,6.97,6.13" target="#foot_11">12</ref> , for all outgoing GoPubMed service requests, along with a Java client for accessing either the official GoPubMed server or the caching server, specified by a properties file <ref type="foot" coords="3,223.94,551.88,6.97,6.13" target="#foot_12">13</ref> , which helps to reduce the workload of the official server and reduce experiment run-time when multiple developers are evaluating their components.</p><p>3 Factoid &amp; List Question Answering for Phase B Factoid and list QA tasks have similar topic distributions and linguistic structures, and each exact answer also uses a similar language representation. Accordingly we designed two supervised models that are shared by both question types: answer type prediction (described in Sect. 3.1) and candidate answer scoring (described in Sect. 3.3), which allows us to best leverage the training data. In addition, we introduce approaches for candidate answer generation in Sect. 3.2. In comparison to factoid questions, list questions require the system to return a list of exact answers, of the same type, which requires an answer pruning component for list question answering only, which is described in Sect. 3.4. The overall pipeline diagram is illustrated in Fig. <ref type="figure" coords="4,349.76,238.80,4.98,8.76">1</ref> in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Question and Answer Type Prediction</head><p>Previous work has studied how to define rules to extract a lexical answer type (or LAT) from questions to predict the answer type, e.g. IBM's Watson system <ref type="bibr" coords="4,421.10,298.66,10.58,8.76" target="#b7">[5]</ref>. Classification based approaches have been proposed to predict answer type from the question using syntactic and/or semantic features. Preparation of training data involves defining an answer type taxonomy manually or by leveraging existing ontologies (e.g. MUC), collecting training questions (e.g. TREC QA question set) and annotating gold standard answer type(s) <ref type="bibr" coords="4,195.09,358.44,10.79,8.76" target="#b8">[6,</ref><ref type="bibr" coords="4,207.54,358.44,7.19,8.76" target="#b10">8]</ref>. Weissenborn et al. <ref type="bibr" coords="4,295.31,358.44,16.60,8.76" target="#b16">[14]</ref> also define patterns for LAT extraction for BioASQ questions, and leverage the UMLS Semantic Network and map the LATs to the ontological hierarchy to obtain one of the UMLS semantic types as an "expected answer type", which is used for type coercion checking. We took advantage of these ideas and further incorporated the Q/A pairs in the BioASQ data set for training a multi-class answer type classifier that predicts candidate answer type(s).</p><p>Answer Type Definition. We introduce two additional question types: CHOICE and QUANTITY in addition to the UMLS semantic types. CHOICE questions are those that have candidate answers expressed explicitly in the question, e.g. "Is Rheumatoid Arthritis more common in men or women?". We treat CHOICE questions as a special case because the candidate answers can be directly extracted from the question, and no further answer type prediction is needed. Since there exist an unlimited number of quantitative values which cannot be all covered in the UMLS semantic network, we add the QUANTITY type to complement the existing qnco (Quantitative Concept) type.</p><p>Answer Type Extraction. To identify the gold standard labels for the existing Q/A pairs used for training, we apply UTS to retrieve the semantic types for each gold standard exact answer, where we first use the exact search type, and if no results are returned, we further relax the search type to words. Since UTS may return more than one concept type for each input concept, and each training question may contain more than one gold standard answer variant (these may be synonyms or answer concepts for list questions), the gold standard answer type is assigned as the most frequent concept type. If multiple concept types have the same number of occurrences for all gold standard answer variants, we keep all of them as the gold standard labels for the question.</p><p>We identified 82 out of the 406 questions which do not have a single gold standard answer variant for which UTS can provide a semantic type. There are three major reasons for this phenomenon. First, some answer concepts are not included in the UMLS No. Feature 1 the lemma form of each token 2 if the question begins with "do" or "be" 3 if the question contains a token "or" 4 if the question contains a quantity question phrase 5 the semantic type of each concept 6 a ⟨semantic type, dependency label⟩ pair, where we use the dependency label of the head token in the concept bearing phrase as the second element 7 also a ⟨semantic type, dependency label⟩ pair, where we use the dependency label of the head of the head token in the concept bearing phrase as the second element 8 the lemma form of the first child of the root in the parse tree that is a noun and has a dependency relation of dep semantic network. For example, the question "Which histone marks are deposited by Set7?" has two answers: "H4K20 monomethylation" and "H3K4 monomethylation", both of which cannot be mapped to a single semantic type. Second, some gold standard exact answers do not strictly follow the representation format. For example, the question "Which enzyme is deficient in Krabbe disease?" has a gold standard answer "Galactocerebrosidase is an enzyme that is deficient in . . . " In fact, "Galactocerebrosidase" alone should be the gold standard exact answer. Third, some questions (e.g. "Which is the most important prognosis sub-classification in Chronic Lymphocytic Leukemia?", with a gold standard answer "The mutational status of the IGHV genes."</p><p>) have an answer which is not a simple biomedical entity, and thus cannot be mapped to a single concept type. Finally, we obtained gold standard labels for the 324 remaining questions. Feature Extraction. We first apply the ClearNLP parser to annotate the tokens, part of speech tags, and dependency relations for the question (corresponding to Lines 21 -26 of Listing 1.1 in the Appendix) . We use three approaches to identify the concept mentions in the question. We first use the MetaMap service to identify the concepts and use UTS to retrieve variant names for each concept (Lines 27 -29). Only the first concept mapping with the confidence score returned from the service is used for each question. We also use a statistics-based LingPipe named entity recognizer (NER) (Lines 30 -32), where the label of the named entity that is assigned by LingPipe NER is used as the semantic type of the concept. We then consider all noun phrases in the question as candidate concepts. Therefore, we employ the OpenNLP chunker to detect all noun phrases (NPs) and prepositional phrases (PPs) from each question, and extract all NPs and all NP-PP-NP occurrences (Lines 33 -38). We then extract a number of linguistic and semantic features from the tokens and concepts, as detailed in Table <ref type="table" coords="5,424.66,572.63,3.74,8.76" target="#tab_0">1</ref>.</p><p>Classification. We use Logistic Regression from the LibLinear tool <ref type="bibr" coords="5,428.71,585.59,11.62,8.76" target="#b3">[1]</ref> to train a multi-class classifier, and use 10-fold cross prediction to predict a list of up to five most likely semantic labels for each question in the training set, which is used in the downstream training process (Lines 42 -44). The model can correctly identify answer types for most high-frequency sentence patterns, such as "which genes", but it may fail for low-frequency question patterns, where UTS may not be able to resolve ambiguous cases (e.g. AUS is identified as a country name without the context).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Candidate Answer Generation</head><p>We first use the same set of token and concept identification tools used for the question (described in Sec. 3.1) to annotate all the relevant snippets provided as input for Phase B (corresponding to Lines 51 -65 of Listing 1.1). We then integrate four components to generate candidate answers (corresponding to Lines 69 -71, and the component level descriptor is presented in Listing 1.2).</p><p>Concepts as Candidate Answers. We create a candidate answer using each concept identified by one of three concept identification approaches described in Sect. 3.1 (corresponding to Line 6 of Listing 1.2). In Batch 3, we also filtered out any concept mention that is exactly a stopword, a token or phrase in the question, or a concept that is also annotated in the question. We used a stopword list that combines the most 5,000 frequent English words and the list of Entrez (PubMed) stopwords.</p><p>CHOICE Questions (Line 4). We first identify the "or" token in the question, and then identify its head token, which is most likely the first option in the list of candidate answers. Next, we find all the children of the first option token in the parse tree that have a dependency relation of conj, which are considered to be alternative options. We see this approach works well on most CHOICE questions, but still has problems in a few special cases. First, if two options have different prefixes but the same suffix, the suffix may be discarded in the first option, e.g. "Is the long non-coding RNA malat-1 up or downregulated in cancer?". Another issue is that the head tokens can be semantically incomplete, such that a phrase which covers the head token should be used instead for the options; we expand the candidate answer using a minimal concept mention that covers the candidate answer occurrence (Line 7).</p><p>QUANTITY Questions. We identify all the tokens that have a POS tag of CD in all relevant snippets (Line 5). This approach can reliably produce a complete set of quantitative mentions. However, it does not give us a way to semantically interpret the extracted numbers. For example, it could correctly identify "20,687", " 24,500", etc. as candidate numbers, but does not have the ability to "summarize" the numbers and produce a single answer, e.g. "Between 20,000 and 25,000" as required. Similar to CHOICE questions, another limitation is that this method can only identify a single token as a candidate answer (e.g. "3.0") where semantically complete phrase (e.g. "3.0 mm") is preferred. We apply the same approach used for CHOICE questions to include the CD-bearing phrase as the candidate answer (Line 7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Candidate Answer Scoring</head><p>We predict a confidence score for each candidate answer (corresponding to Lines 75 -77 of Listing 1.1, and the component level descriptor is presented in Listing 1.3). In Batch 3, we use a simple multiplication method to combine the type coercion score and the occurrence count.</p><p>In Batches 4 &amp; 5, we define a feature space containing 11 groups of features, as shown in Table <ref type="table" coords="6,198.25,621.45,3.74,8.76" target="#tab_1">2</ref>, which extend the approach used by Weissenborn et al. <ref type="bibr" coords="6,428.45,621.45,15.27,8.76" target="#b16">[14]</ref>, and use Logistic Regression to learn the scoring function. We only use the questions with nonzero recall for training, where we assign "1" to each candidate answer variant if it is also contained in the gold standard answer set, and "0" otherwise. Since there are many For each candidate answer occurrence (CAO), the percentage of semantic types that are also among the top-k (k = 1, 3, and 5) predicted answer types. To accumulate the scores from multiple CAOs, we use "average", "maximum", "minimum", "non-zero ratio", "one ratio", and "boolean or". 6 CAO count. We use the number of CAOs for each answer variant and we also count the total number of tokens in all occurrences. 7 Name count. The number of distinct candidate answer names, which differs from CAO count; if two CAOs have the same text string, only one will count. 8 Avg. covered token count. Averaged number of tokens in each CAO. 9 Stopword count. For each CAO, we calculate the stop word percentage. We use the same stoplist as described in Section 3.2. We accumulate the scores from multiple CAOs using "average", "minimum", "one ratio", and "boolean or". 10 Token overlap count. For each CAO, we calculate the percentage of tokens that overlap with the question. We accumulate the scores from multiple CAOs using "average", "nonzero ratio", and "boolean or". 11 Concept overlap count. For each CAO, we calculate the percentage of covered concept mentions that overlap with the question. We accumulate the scores from multiple CAOs using "average", "non-zero ratio", and "boolean or". 12 Token proximity. For each CAO, we calculate the averaged distance to the nearest occurrence of each question word in the relevant snippet. We set a window size of 10, and if any question word falls out of the window, we use a fixed distance of 20. We also transform the distance to its negation and inverse, and accumulate the scores from multiple CAOs using "average", "maximum", "minimum", and "non-zero ratio". 13 Concept proximity. Similar to token proximity, we calculate the distance from each CAO to each question concept mention in the relevant snippet. 14 LAT count. For each CAO, we calculate the percentage of tokens that overlap with a LAT token (i.e. the 8th feature in Table <ref type="table" coords="7,277.98,432.51,3.24,7.88" target="#tab_0">1</ref>). We accumulate the scores from multiple CAOs using "average" and "non-zero ratio". 15 Parse proximity. Similar to token proximity, we use the distance in the parse tree, which is important for list questions, as answer bearing sentences may be in the form of "includes A, B, C, . . . ". more negative instances than positive instances, we assign to each negative instance a weight of #positive instances #negative instances .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Answer Pruning</head><p>In Batch 3, we used the factoid QA pipeline to produce answers for list questions without any pruning. In Batch 4, we used an absolute threshold to select only the answers that have a confidence score, predicted by the candidate answer scoring model, above threshold. Starting from Batch 5, instead of an absolute threshold for all questions, we use a relative threshold to filter the answers that have a confidence score above a percentage of the highest predicted score for the question (corresponding to Line 78 -80 of Listing 1.1). We tune the threshold on the development set.</p><p>In this section, we describe the approaches that are used for retrieval tasks in Phase A.</p><p>The pipeline diagram for Phase A is illustrated in Fig. <ref type="figure" coords="8,351.99,152.42,4.98,8.76" target="#fig_1">2</ref> in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Document Retrieval</head><p>Our approach is similar to what we have proposed in the TREC 2014 Web Track <ref type="bibr" coords="8,461.52,194.94,15.27,8.76" target="#b17">[15]</ref>, with some modifications made for better performance and efficiency.</p><p>Offline Indexing of Medline Baseline Corpus. We used Lucene to index a Medline baseline corpus using title, abstract and keywords fields, if available. We used the standard Lucene tokenizer combined with the Krovetz Stemmer, which is less aggressive compared to the Porter Stemmer. This is an important step, because many biomedical terms (in particular gene names) are not recognizable by stemmers, and the Porter stemmer is likely to truncate many of the words, causing increased confusion between the stemmed biomedical terms and common terms during search time. We also kept the stopwords in the index. The motivation is that since we only have the abstract text for the document, removing stopwords may result in less accurate field length statistics, thus affecting the performance of many language model based retrieval models.</p><p>Hierarchical Retrieval Architecture. The fact is that given a query, we have more retrievable documents than we can perform a deeper analysis for. However, to ensure better retrieval performance, in-depth analysis of the documents is necessary. Therefore, a hierarchical retrieval architecture is introduced here to find a good balance between performance and efficiency. In summary, each search task is processed by three stages:</p><p>1. Obtaining an affordable high recall candidate set. During the query time, we have removed all stopwords from the query, as they provide no useful information and will likely cause serious efficiency issues. We use the Dirichlet smoothing retrieval model implemented in Lucene to conduct this search. In our implementation, we consider only the top 10,000 ranked documents. 2. Precision oriented reranking. We incorporate the Negative Query Generation (NQG) model <ref type="bibr" coords="8,211.08,473.49,10.58,8.76" target="#b9">[7]</ref>, which utilizes a negative language model by assuming that all documents in the corpus are non-relevant, thus making more accurate adjustments to query term weights and relevance calculations. After re-ranking with NQG, we can now further cut down the candidate set by considering only the top 100 documents in the ranked list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Deep document feature extraction and learning to rank (LETOR). We use ranker</head><p>scores (e.g. BM25, Jelinek-Mercer smoothing, Dirichlet smoothing, Indri two-stage smoothing, NQG, etc), similarity scores (e.g. Jaccard coefficient and Dice coefficient, etc.), raw features (e.g. document length, vocabulary size, etc.), and customized features (e.g. harmonic means of the ranker scores across all fields, the distribution of the query terms across the documents, etc.). We simply score the K documents with a pre-trained LETOR model which was optimized for Precision@10. Here, we are using Random Forest, an ensemble method known for robustness against overfitting.</p><p>The details of the proposed document retrieval approach can be found in our previous work <ref type="bibr" coords="8,173.95,657.32,15.27,8.76" target="#b17">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3. Snippet Retrieval Features</head><p>No. Feature 1 BM25: We index all the candidate snippets using Lucene, and then use a query that contains not only words but also phrases and confidence scores of all the different query concepts returned by the MetaMap service. 2 Skip-bigram: Based on the dependency relations generated from the dependency parser for each question, we count the number of matched pairs and calculate the F-1 based skip-bigram score. 3 Textual alignment: Surface similarity of a snippet and a question. We also consider the relative order of the different words. 4 Some other question independent features, such as the length of the snippet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Snippet Retrieval</head><p>The snippet retrieval module analyzes the 10 most relevant documents returned from the upstream document retrieval component. We first identify the extent of a snippet and then apply a LETOR approach for snippet retrieval.</p><p>Candidate Snippets Generation. The definition of "snippet" is the original piece of text extracted from the document. In our initial study, we found that the distribution of snippet length in the gold standard answers is similar to that of sentence length. Therefore, we apply a sentence segmenter to split the snippets and define each sentence as a snippet candidate.</p><p>Feature Extraction and LETOR. We define four types of features for LETOR in Table <ref type="table" coords="9,159.14,396.94,3.74,8.76">3</ref>, and also apply the logistic regression classifier for scoring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Concept Retrieval</head><p>We first identify the text spans from each question and search these texts from various GoPubMed concept services. Since only a single list of concepts is returned, we also propose to merge and rank the concept lists returned from multiple sources.</p><p>Candidate Queryable Concept Generation. We use MetaMap to identify the UMLS concepts from the question, and our results indicate a significant improvement in recall. However, one of the major drawbacks of MetaMap is that it is poor at identifying gene and protein names. To overcome this issue, we use LingPipe NER with the model trained on the GeneTag corpus to recognize gene names to enrich the retrieved metathesaurus concepts. We then use the combination of tokens retrieved from the MetaMap service and the LingPipe NER to query various biomedical ontologies.</p><p>Concept Ranking and Merging. We create a ranking model that can rank the search results from different ontologies. We use the federated search approach <ref type="bibr" coords="9,461.52,574.95,15.27,8.76" target="#b14">[12]</ref>, which trains a relevance mapping logistic function that maps the relevance scores of each result from each ontology to a global relevance scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Triple Retrieval</head><p>Similar to concept retrieval, we rely on the BioASQ provided service to retrieve relevant triples. Therefore, our goal is to construct an effective query string. Beyond the baseline method that simply concatenates all the keywords from the concept retrieval result, we made three improvements:</p><p>-Append "[obj]" and "[sub]" identifiers to each keyword in the query string.</p><p>-Enumerate all letter case possibilities for keywords: lower case, upper case, and capitalized word. -Add all words in the original question to the keyword set while excluding the stop words and SQL reserved keywords.</p><p>The first improvement is to help the triple query server understand that most of our keywords are used as objects or subjects. This finding is intuitive through observation; since most of the words are nouns or adjectives, which are unlikely used as predicates in triples. The second improvement is based on an observation from examination of gold standard answers, where triple results indicate case-sensitivity during triple matching. Therefore, we need to include all casing variants to ensure that keywords are matched during triple retrieval. The third improvement ensures that we do not omit keywords from the original question, to make the query more robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results &amp; Analysis</head><p>We summarize the official evaluation results of document and snippet retrieval in Phase A and factoid and list QA in Phase B in Batches 3, 4, and 5 from the official evaluation portal in Table <ref type="table" coords="10,195.12,382.35,3.74,8.76">4</ref>.</p><p>Among all the systems that participated in Phase A evaluation, the performance of our document retrieval pipeline is scored at the bottom of the first tier. The absolute performance gaps between our pipeline and the system that is scored one place behind ours in Batches 3, 4, and 5 are measured as .0915, .0225, and .0869 respectively in terms of MAP, which are larger than those between our pipeline and the best performing system (.0435, .0204, and .0466 respectively).</p><p>Due to a relatively steep learning curve for the developers who have not had much experience with the system and the task, Phase A system used a different question analysis pipeline from the Phase B system, which had no concept retrieval module integrated and tested, which should expand each concept with synonyms. Therefore, we believe document and snippet retrieval evaluated in Phase A can be further improved by considering synonyms expanded using UTS during query formulation. Moreover, the snippets extracted by the latter snippet retrieval stage can be fed back to the search engine as an expanded query to harvest more relevant information; reinforcement learning can thus be utilized in this scenario.</p><p>For Phase B, we see that our system achieved five of six highest performance scores among all participating systems for factoid and list question answering in Batches 3, 4, and 5. We notice that the performance in Batch 4 is higher than in other batches, which we believe is because Batch 4 set contains more questions seeking for the types of answers that have occurred more frequently in the training set, e.g. gene, disease, etc.</p><p>To further understand what causes the error and how we may improve the system, we manually answer each factoid question in Batches 3, 4, and 5 using the gold standard snippets provided for the input of Phase B, and compare with the output of our system to label the error types (multiple types allowed) for each incorrectly answered question. We list the error categories and give definition and examples to each category in Table <ref type="table" coords="11,134.76,387.34,3.74,8.76" target="#tab_2">5</ref>, where we also show the occurrence of each error category in each test batch.</p><p>Based on the analysis, we believe a better concept identification model and concept type prediction model will make the hugest impact to the overall performance improvement. Moreover, we plan to conduct a thorough ablation study to estimate how much each component or feature contributes to the overall performance, as soon as we have the gold-standard outputs for the 3B dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper describes the CMU OAQA system evaluated in the BioASQ 3B Question Answering track. We first present a three-layered architecture, and then describe the components that have been integrated into the participating system for exact answer generation and retrieval. We also investigate how to learn to answer questions from such a large gold standard biomedical QA dataset, using an answer type prediction model and an answer scoring model. The official evaluation results show the effectiveness of the proposed approach in factoid and list QA.</p><p>Further work is necessary to improve the retrieval components in Phase A. We are also interested in investigating how to learn to answer yes/no questions and summary questions from existing Q/A pairs. We plan to integrate the system into the BioQUADS (biomedical decision support system) <ref type="bibr" coords="11,285.12,645.36,16.60,8.76" target="#b19">[17]</ref> to process biomedical complex decision processes represented in natural language. The highest ranked answer has a different concept type from the answer type that question asks for, which may be caused by a wrongly predicted answer type, an incorrect score combination equation from the score prediction model, or the concept identification module. Concept identification 4 4 2 Some answer variants are not identified as concepts or we can find little evidence from the relevant snippets for the concept. For example, for the question"Neurostimulation of nucleus is used for treatment of dystonia?", none of the components is able to identify "Bilateral globus pallidus internus (GPi)" as a concept and further candidate answer variant. Complex answer 2 2 5 The ideal answer is a complex phrase or sentence, rather than a single-entity concept, usually in response to the questions containing "effect", "role", "function", etc. For example, "executors/mediators of apoptosis" should be extracted to answer the question "What is the function of capspases?", but we only see "apoptosis" in the candidate answer list. Mistakenly use question phrase as answer 3 2 2 Although we design a scorer in the ranking module to identify whether each candidate answer co-occurs in the original question, which should lower the rank of those candidate answers, we still see some question phrase variants are chosen as the top answer. For example, the question "What is the effect of enamel matrix derivative on pulp regeneration" mentions a concept "enamel matrix derivative", but the system ranks its acronym "EMD" at the top. Tokenization 2 4 0 Tokenization module may fail if the concept contains punctuation marks, e.g. parentheses, colon, semicolon, etc, and/or numbers, as in the example "t(11;22)(q24:q12)". Definition question 2 0 1 The asker knows the terminology but asks for the definition, e.g. "What is Piebaldism?", or knows the properties and asks for terminology, e.g. "How are ultraconserved elements called when they form clusters?". We believe we need to introduce special question types and modules. Question type 1 0 1 Identification of QUANTITY and CHOICE questions may fail in some cases. For example, "Alpha-spectrin and beta-spectrin subunits form parallel or antiparallel heterodimers?" does not use "Do" at the beginning. Another example is that "risk" is a QUANTITY indicator in the question "What is the risk of developing acute myelogenous leukemia in Fanconi anemia?" Snippets that have no information 0 0 2 Some snippets do not contain any answer variant. For example, "What is the main role of Ctf4 in dna replication?" has a gold standard snippet "Ctf4 remains a central player in DNA replication". Relation concept identification 0 1 1 A relation concept refers to a verb or verbal adjective, e.g. "responsible" or "leading" that distinguishes the expected answer from other candidates that have the same concept type. Syntactic function 0 1 1</p><p>The key to answer the question is embedded in the syntactic structures of the relevant snippets. For example, in the snippet "Medicarpin, the major phytoalexin in alfalfa, ...", no explicit relation word is used between "Medicarpin" and "the major phytoalexin", but the syntactic structure clearly implies that the latter explains the former.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="14,134.76,463.14,345.83,8.37;14,134.76,474.27,31.12,7.88"><head>Fig. 1 . 16 -</head><label>116</label><figDesc>Fig. 1. Phase B pipeline diagram. † represents a provider that requires accessing external Web services.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="15,134.76,359.43,345.83,8.07;15,134.76,370.57,31.12,7.88"><head>Fig. 2 .-</head><label>2</label><figDesc>Fig. 2. Phase A pipeline diagram. ⋆ represents a provider that requires accessing BioASQ Web services.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,231.75,116.95,151.85,8.01"><head>Table 1 .</head><label>1</label><figDesc>Answer Type Prediction Features</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,136.67,116.95,215.66,38.52"><head>Table 2 .</head><label>2</label><figDesc>Answer Scorers</figDesc><table coords="7,136.67,136.22,71.92,19.24"><row><cell>Line Feature</cell></row><row><cell>5 Type coercion.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="12,139.75,130.96,337.22,51.30"><head>Table 5 .</head><label>5</label><figDesc>Error categories and occurrences for factoid questions in test batches 3, 4, and 5.</figDesc><table coords="12,139.75,157.53,51.89,7.88"><row><cell>Error category</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,658.87,129.11,6.72"><p>https://uima.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,144.73,614.18,145.25,6.72"><p>http://metamap.nlm.nih.gov/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,144.73,625.35,177.53,6.72"><p>https://github.com/clir/clearnlp/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,144.73,636.52,204.92,6.72"><p>https://github.com/oaqa/cse-framework/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="2,144.73,647.70,86.07,6.72"><p>http://yaml.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="2,144.73,658.87,166.77,6.72"><p>https://github.com/oaqa/baseqa/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="3,144.73,591.83,204.92,6.72"><p>http://alias-i.com/lingpipe/index.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="3,144.73,603.00,145.25,6.72"><p>https://opennlp.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8" coords="3,144.73,614.18,139.87,6.72"><p>https://lucene.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9" coords="3,144.73,625.35,236.70,8.44"><p>http://www.csie.ntu.edu.tw/ ˜cjlin/liblinear/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10" coords="3,144.73,636.52,177.53,6.72"><p>https://uts.nlm.nih.gov/home.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_11" coords="3,144.73,647.70,86.07,6.72"><p>http://redis.io/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_12" coords="3,144.73,658.87,248.45,6.72"><p>https://github.com/ziy/bioasq-gopubmed-client/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_13" coords="15,143.13,564.61,83.69,5.50"><p>inherit: ecd.phase</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. We thank <rs type="person">Qianru Zhu</rs> and <rs type="person">Avner Maiberg</rs> for their involvement in the early stages. We also thank <rs type="person">Ying Li</rs>, <rs type="person">Xing Yang</rs>, <rs type="person">Venus So</rs>, <rs type="person">James Cai</rs> and the other team members at <rs type="institution">Roche Innovation Center New York</rs> for their continued support of OAQA and biomedical question answering research and development.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="11,161.00,116.95,319.58,8.01;11,134.76,128.03,105.10,7.88;11,271.82,150.99,71.71,7.88;11,141.80,162.35,60.27,7.88;11,231.46,162.35,22.91,7.88;11,294.23,162.35,37.85,7.88;11,355.99,162.35,19.43,7.88;11,417.76,162.35,25.90,7.88;11,146.28,173.71,59.76,7.88;11,231.45,173.71,37.35,7.88;11,294.22,173.71,37.35,7.88;11,355.98,173.71,37.35,7.88;11,417.75,173.71,37.35,7.88;11,146.53,184.66,59.51,7.88;11,231.45,184.66,37.35,7.88;11,294.22,184.66,37.35,7.88;11,355.98,184.66,37.35,7.88;11,417.75,184.66,37.35,7.88;11,146.53,195.62,59.51,7.88;11,231.45,195.62,37.35,7.88;11,294.22,195.62,37.35,7.88;11,355.98,195.62,37.35,7.88;11,417.75,195.62,25.90,7.88" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="11,166.16,117.07,314.42,7.88;11,134.76,128.03,101.43,7.88">Ranks among systems (as of the manuscript completion) are shown in the parentheses</title>
		<idno>.3242 (15) .2311 (15) .1654 (15) .0136 (15) 4th .2144 (15) .3320 (15) .2263 (15) .1524</idno>
		<imprint/>
	</monogr>
	<note>Partial official evaluation result. Phase A: Document Batch Precision Recall F-measure MAP GMAP 3rd .2310 (15). (15) .0081 (14) 5th .2130 (15) .4474 (15) .2605 (15) .1569 (15) .0267</note>
</biblStruct>

<biblStruct coords="11,276.79,208.97,61.77,7.88;11,141.80,220.33,60.27,7.88;11,231.46,220.33,22.91,7.88;11,294.23,220.33,37.85,7.88;11,355.99,220.33,19.43,7.88;11,417.76,220.33,25.90,7.88;11,146.28,231.69,55.28,7.88;11,231.46,231.69,32.87,7.88;11,294.23,231.69,32.86,7.88;11,356.00,231.57,32.86,7.90;11,417.76,231.69,32.87,7.88;11,146.53,242.65,55.03,7.88;11,231.46,242.65,37.35,7.88;11,294.22,242.65,32.87,7.88;11,356.00,242.65,32.86,7.88;11,417.76,242.65,32.87,7.88;11,146.53,253.61,55.03,7.88;11,231.46,253.61,32.87,7.88;11,294.23,253.61,32.86,7.88;11,356.00,253.61,32.86,7.88;11,417.76,253.61,32.87,7.88" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="11,311.16,208.97,27.40,7.88;11,141.80,220.33,60.27,7.88;11,231.46,220.33,22.91,7.88;11,294.23,220.33,37.85,7.88;11,355.99,220.33,19.43,7.88;11,417.76,220.33,25.90,7.88;11,146.28,231.69,11.95,7.88">Snippet Batch Precision Recall F-measure MAP GMAP 3rd</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Phase</surname></persName>
		</author>
		<idno>.1133</idno>
		<imprint>
			<biblScope unit="page">1044</biblScope>
		</imprint>
	</monogr>
	<note>) .0891 (3) .0892 (1) .0013 (5) 4th .1418 (5) .1264 (10) .1153 (8) .0957 (5) .0027 (6) 5th .1472 (9) .1756 (9) .1391 (9) .1027 (9) .0040 (5)</note>
</biblStruct>

<biblStruct coords="11,263.73,266.36,87.89,7.88;11,141.80,283.20,20.92,7.88;11,230.04,277.72,26.76,7.88;11,391.86,277.72,13.95,7.88;11,168.70,288.67,123.53,7.88;11,324.11,288.67,33.37,7.88;11,375.92,288.67,22.91,7.88;11,427.73,288.67,37.85,7.88;11,146.28,299.91,55.28,8.01;11,220.50,299.91,32.87,7.90;11,272.30,299.91,32.87,7.90;11,324.11,300.03,32.87,7.88;11,375.92,299.91,32.87,7.90;11,427.73,300.03,32.86,7.88;11,146.53,310.87,55.03,8.01;11,220.50,310.87,32.87,7.90;11,272.30,310.87,32.87,7.90;11,324.11,310.87,32.87,7.90;11,375.92,310.87,32.87,7.90;11,427.72,310.87,32.87,7.90;11,146.53,321.83,55.03,8.01;11,220.50,321.83,32.87,7.90;11,272.30,321.83,32.87,7.90;11,324.11,321.83,32.87,7.90;11,375.92,321.95,32.87,7.88;11,427.73,321.83,32.86,7.90" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,297.59,266.36,54.03,7.88;11,141.80,283.20,20.92,7.88;11,230.04,277.72,26.76,7.88;11,391.86,277.72,13.95,7.88;11,168.70,288.67,34.17,7.88">Exact Answers Batch Factoid List Strict Acc</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Phase</surname></persName>
		</author>
		<idno>1615 (1) .0539 (8) .6933</idno>
	</analytic>
	<monogr>
		<title level="j" coord="11,220.50,288.67,71.74,7.88;11,324.11,288.67,33.37,7.88;11,375.92,288.67,22.91,7.88;11,427.73,288.67,37.85,7.88">Lenient Acc. MRR Precision Recall F-measure</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2308</biblScope>
		</imprint>
	</monogr>
	<note>(1) .0969 (7) 4th .4483 (1) .6207 (1) .5155 (1) .3836 (1) .3480 (1) .3168 (1) 5th .2273 (1) .3182 (1) .2727 (1) .1704 (1) .2573 (5) .1875 (1)</note>
</biblStruct>

<biblStruct coords="13,142.61,207.09,337.97,7.88;13,150.95,218.05,247.02,7.88" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,374.38,207.09,106.20,7.88;13,150.95,218.05,68.45,7.88">Liblinear: A library for large linear classification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><forename type="middle">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,224.91,218.05,73.95,7.88">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008-08">Aug. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,150.95,228.51,329.63,7.88;13,150.95,239.47,329.63,7.88;13,150.95,250.43,274.57,7.88" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="13,288.40,239.47,192.18,7.88;13,150.95,250.43,26.81,7.88">Towards the open advancement of question answering systems</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ferrucci</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ciccolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Duboue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Gondek</surname></persName>
		</author>
		<idno>RC24789</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="904" to="0093" />
			<pubPlace>W</pubPlace>
		</imprint>
		<respStmt>
			<orgName>IBM Research Division</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct coords="13,142.61,260.90,337.96,7.88;13,150.95,271.86,329.63,7.88;13,150.95,282.82,62.01,7.88" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,415.72,260.90,64.85,7.88;13,150.95,271.86,234.50,7.88">Cse framework: A uima-based distributed system for configuration space exploration</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Garduno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Maiberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Mccormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Nyberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,403.16,271.86,72.26,7.88">UIMA@GSCL&apos;2013</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="14" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,293.28,337.96,7.88" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,240.68,293.28,132.31,7.88">Trec genomics special issue overview</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,378.89,293.28,31.97,7.88">Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,303.75,337.97,7.88;13,150.95,314.71,329.63,7.88;13,150.95,325.67,23.90,7.88" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,209.44,314.71,157.03,7.88">Question analysis: How watson reads a clue</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lally</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">C</forename><surname>Mccord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Boguraev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Patwardhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Fodor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chu-Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,372.26,314.71,61.79,7.88">IBM J. Res. Dev</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2" to="3" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,336.14,283.84,7.88" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,211.97,336.14,101.71,7.88">Learning question classifiers</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,331.26,336.14,35.48,7.88">ACL&apos;2002</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,346.60,337.96,7.88;13,150.95,357.56,44.08,7.88" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,209.05,346.60,170.44,7.88">Query likelihood with negative query generation</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,396.74,346.60,41.91,7.88">CIKM&apos;2012</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1799" to="1803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,368.03,337.96,7.88;13,150.95,378.99,329.63,7.88;13,150.95,389.95,329.63,7.88;13,150.95,400.91,23.90,7.88" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,368.37,378.99,112.21,7.88;13,150.95,389.95,264.71,7.88">The javelin question-answering system at trec 2003: A multi-strategy approach with dynamic planning</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mitamura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Frederking</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Collins-Thompson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Hiyakumoto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Huttenhower</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Judy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,436.74,389.95,43.83,7.88">TREC&apos;2003</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,411.37,337.96,7.88;13,150.95,422.33,329.63,7.88;13,150.95,433.29,290.65,7.88" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="13,187.70,422.33,292.87,7.88;13,150.95,433.29,82.53,7.88">Ensemble approaches for large-scale multi-label classification and question answering in biomedicine</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Papanikolaou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Dimitriadis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Laliotis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Markantonatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Vlahavas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,251.81,433.29,42.84,7.88">CLEF&apos;2014</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1348" to="1360" />
		</imprint>
	</monogr>
	<note>Working Notes</note>
</biblStruct>

<biblStruct coords="13,142.23,443.76,338.34,7.88;13,150.95,454.72,329.63,7.88;13,150.95,465.68,128.53,7.88" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mitamura</surname></persName>
		</author>
		<title level="m" coord="13,318.99,443.76,161.59,7.88;13,150.95,454.72,312.97,7.88;13,150.95,465.68,42.84,7.88">Building an optimal question answering system automatically using configuration space exploration (cse) for qa4mre 2013 tasks</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>CLEF&apos;2013. Working Notes</note>
</biblStruct>

<biblStruct coords="13,142.23,474.19,338.35,9.83;13,150.95,487.10,329.63,7.88;13,150.95,498.06,210.75,7.88" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,244.22,487.10,236.35,7.88;13,150.95,498.06,64.73,7.88">Overview of qa4mre at clef 2012: Question answering for machine reading evaluation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Forner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Á</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">F E</forename><surname>Sutcliffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sporleder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Forascu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Benajiba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Osenova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,233.67,498.06,42.84,7.88">CLEF&apos;2012</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>Working Note</note>
</biblStruct>

<biblStruct coords="13,142.23,508.53,338.34,7.88;13,150.95,519.49,62.01,7.88" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="13,212.07,508.53,204.60,7.88">Modeling search engine effectiveness for federated search</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,434.01,508.53,42.34,7.88">SIGIR&apos;2005</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="83" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.23,529.95,338.35,7.88;13,150.95,540.91,329.63,7.88;13,150.95,551.87,329.63,7.88;13,150.95,562.83,99.60,7.88" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="13,388.04,540.91,92.54,7.88;13,150.95,551.87,278.60,7.88">An overview of the bioasq large-scale biomedical semantic indexing and question answering competition</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Tsatsaronis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Balikas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Malakasiotis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Partalas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zschunke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Alvers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Polychronopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,435.52,551.87,45.05,7.88;13,150.95,562.83,33.86,7.88">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">138</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.23,573.30,338.34,7.88;13,150.95,584.26,136.72,7.88" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="13,325.86,573.30,154.72,7.88;13,150.95,584.26,40.16,7.88">Answering factoid questions in the biomedical domain</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Tsatsaronis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schroeder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,209.72,584.26,51.81,7.88">BioASQ&apos;2013</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.23,594.72,294.74,7.88" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<title level="m" coord="13,217.94,594.72,192.89,7.88">Towards a simple and efficient web search framework</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.23,605.19,338.34,7.88;13,150.95,616.15,329.63,7.88;13,150.95,627.11,190.03,7.88" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="13,419.74,605.19,60.83,7.88;13,150.95,616.15,329.63,7.88;13,150.95,627.11,43.00,7.88">Building optimal information systems automatically: Configuration space exploration for biomedical information systems</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Garduno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Maiberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Mccormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Nyberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,212.22,627.11,41.91,7.88">CIKM&apos;2013</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1421" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.23,637.58,338.34,7.88;13,150.95,648.53,119.79,7.88;15,159.87,548.67,112.98,5.50;15,154.69,556.78,79.50,5.22" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="13,288.09,637.58,175.81,7.88">Quads: Question answering for decision support</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Nyberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,150.95,648.53,42.34,7.88">SIGIR&apos;2014</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="375" to="384" />
		</imprint>
	</monogr>
	<note>baseqa.quesanal. lexical-answer-type</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
