<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,159.89,115.96,295.59,12.62;1,185.73,133.89,243.90,12.62">Biomedical Question Answering using the YodaQA System: Prototype Notes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,246.55,171.56,51.83,8.74"><forename type="first">Petr</forename><surname>Baudiš</surname></persName>
							<email>baudipet@fel.cvut.cz</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Cybernetics</orgName>
								<orgName type="institution">Czech Technical University</orgName>
								<address>
									<addrLine>Technická 2</addrLine>
									<settlement>Praha</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,321.07,169.04,47.74,11.26"><forename type="first">Jan</forename><surname>Šedivý</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Cybernetics</orgName>
								<orgName type="institution">Czech Technical University</orgName>
								<address>
									<addrLine>Technická 2</addrLine>
									<settlement>Praha</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,159.89,115.96,295.59,12.62;1,185.73,133.89,243.90,12.62">Biomedical Question Answering using the YodaQA System: Prototype Notes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">037654F98B85B73F8F4E73618199562A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Question answering</term>
					<term>linked data</term>
					<term>natural language processing</term>
					<term>bioinformatics</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We briefly outline the YodaQA open domain question answering system and its initial adaptation to the Biomedical domain for the purposes of the BIOASQ challenge (question answering task 3b) on CLEF2015.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The YodaQA system for open domain factoid English question answering has been published recently. <ref type="bibr" coords="1,248.26,386.65,10.52,8.74" target="#b0">[1]</ref> [2] The system is a fully open source, modular pipeline inspired by the IBM Watson DeepQA system <ref type="bibr" coords="1,381.35,398.60,9.96,8.74" target="#b2">[3]</ref>. So far, the system has not been specialized for any particular domain, this represents the first such effort.</p><p>The BIOASQ challenge <ref type="bibr" coords="1,253.06,434.47,10.52,8.74" target="#b3">[4]</ref> aims at semantic indexing and question answering in the biomedical domain using a variety of knowledge bases from the given domain. The BIOASQ 2015 has tasks 3A and 3B, where 3A concerns semantic indexing and 3B is about question answering, where we participated.</p><p>The BIOASQ Task 3B is further split into phase A (information retrieval) and phase B (answer production); the phases are evaluated separately, with gold standard phase A results (documents, snippets and triples) available for phase B. Our system participated in the phase B evaluation.</p><p>The paper is structured as follows. In Sec. 2, we briefly outline the YodaQA system in its original form. In Sec. 3, we discuss the changes of the system for the biomedical domain. In Sec. 4, we review the system performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">YodaQA Summary</head><p>The YodaQA pipeline is implemented mainly in Java, using the Apache UIMA framework <ref type="bibr" coords="1,185.20,620.25,9.96,8.74" target="#b4">[5]</ref>. Detailed technical description of the pipeline is included in a technical report <ref type="bibr" coords="1,206.78,632.21,9.96,8.74" target="#b0">[1]</ref>.</p><p>The system maps an input question to ordered list of answer candidates in a pipeline fashion, encompassing the following stages:</p><p>-Question Analysis extracts natural language features from the input and produces in-system representations of the question. We currently build just a naive representation of the question as a bag-of-features. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">YodaQA Domain Adaptation</head><p>We made a variety of adjustments to fit our end-to-end pipeline to the BIOASQ task. The changes are available within the public open source code base (https: //github.com/brmson/yodaqa) in the d/clef2015-bioasq branch.</p><p>As a minor technical change, we enhanced our question analysis for imperative and otherwise specifically phrased questions which were uncommon in our TREC-based open domain dataset.</p><p>Our system is designed to answer just factoid questions, while the BIOASQ challenge also includes list and yes-no questions. For the list question, we simply use the top 5 answer candidates returned by the system. Since we implemented no text entailment algorithm yet and there was no easy way to skip yes-no questions, we simply use a fixed yes answer for all since it was significantly more prevalent in the training dataset. Similarly, our system is designed to return narrow answers, not sentencelength answers that include background and justification. Therefore, we always return empty string as the ideal answer and supply our answers as the exact answer. However, the BIOASQ definition of exact answers might be more stringent than ours, which simply requires that the gold standard answer is a sub-string of the produced answers answer (e.g. "the red color" would be acceptable for gold standard "red" in our scenario). Therefore, we modified our system to require exact matches during training, and disabled a heuristic in answer analysis which attempts to find a focus word in the answer and run analysis (like title lookup, type coercion) on it instead of the whole answer.</p><p>As we focus on the phase B of the question answering task, we do not perform an explicit primary search on questions and instead base answer production on the search result snippets (generated by phase A) supplied along with the question on program input. These snippets are equivalent to passages our primary search would produce. <ref type="foot" coords="3,231.46,165.24,3.97,6.12" target="#foot_0">1</ref>To further improve the accuracy of the system, we implemented an enhanced answer production strategy (inspired by the Jacana QA system) that approaches the problem of identifying the answer in a text passage in a way similar to named entity recognition: as a (token) sequence tagging (by begin-insideoutside labels) that uses the conditional random field model to predict labels. <ref type="bibr" coords="3,470.08,227.34,10.52,8.74" target="#b5">[6]</ref> However, we use a significantly simplified feature set: just part-of-speech tags, named entity labels and dependency labels as token sequence unigrams, bigrams and trigrams.</p><p>A crucial feature for scoring answers is information on successful type coercion. This involves identification of Lexical Answer Type (LAT) in the question (typically an easy task using a few fixed heuristics), production of a set of LATs describing the answer and the question-answer type coercion using straight string matching and Wordnet <ref type="bibr" coords="3,237.57,323.74,10.52,8.74" target="#b6">[7]</ref> hypernymy relations. In an open domain QA system, the most useful LAT source is looking up the answer as a Wikipedia article by title and using the category information. <ref type="foot" coords="3,300.63,346.08,3.97,6.12" target="#foot_1">2</ref> This may fail for specialized terminology of the biomedical domain, we also look up the answer in the GeneOntology <ref type="bibr" coords="3,470.08,359.61,10.52,8.74" target="#b8">[9]</ref> using the GOLR endpoint (as either of bioentity, bioentity label, bioentity name, synonym ) and using the type field as the answer LAT; this is successful for correct identification of answers like gene and protein names.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>To evaluate the performance of our system, we split the (randomly reshuffled) reference training dataset provided for the Task 3B to a local dev/train set (100 questions) and a test set (100 questions). <ref type="foot" coords="3,334.75,481.14,3.97,6.12" target="#foot_2">3</ref> For comparison, we also include baseline version performance<ref type="foot" coords="3,259.08,493.10,3.97,6.12" target="#foot_3">4</ref> on the "curated" factoid open domain dataset <ref type="bibr" coords="3,467.30,494.67,9.96,8.74" target="#b1">[2]</ref>. The results are summarized in Table <ref type="table" coords="3,298.02,506.63,3.87,8.74">1</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,140.99,142.90,339.60,212.56"><head></head><label></label><figDesc>The most important characterization of the question is a set of clues (keywords, keyphrases and concept clues that exactly match enwiki titles) and possible lexical answer types.-Answer Production generates a set of candidate answers based on the question, typically by performing a Primary Search in the knowledge bases according to the question clues and either directly using search results as candidate answers or filtering relevant passages from the result text (the Passage Extraction) and generating candidate answers from the filtered passages (the Passage Analysis). Answers are produced from text passages by a simple strategy of considering all named entities and noun phrases. -Answer Analysis generates various answer features based on detailed analysis. Most importantly, this concerns lexical type determination and coercion to question type. Other features include distance from clues in passages or text overlap with clues. -Answer Merging and Scoring consolidates the set of answers, removing duplicates and using a machine learned classifier (logistic regression) to score answers by their features.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,134.77,120.08,345.83,111.59"><head></head><label></label><figDesc>Benchmark results of various pipeline variants on the test split of the dataset. MRR is the Mean Reciprocal Rank |Q| • q∈Q 1/rq.</figDesc><table coords="4,134.77,120.08,275.12,98.47"><row><cell>Pipeline</cell><cell cols="3">Recall Accuracy-at-1 MRR</cell></row><row><cell>final</cell><cell>33.0%</cell><cell>10.0%</cell><cell>0.132</cell></row><row><cell cols="2">w/o GeneOntology 33.0%</cell><cell>8.0%</cell><cell>0.120</cell></row><row><cell>w/o G.O., CRF</cell><cell>33.0%</cell><cell>5.5%</cell><cell>0.114</cell></row><row><cell cols="2">final, ignoring yes/no q. 43.5%</cell><cell>10.1%</cell><cell>0.148</cell></row><row><cell>open domain</cell><cell>79.3%</cell><cell>32.6%</cell><cell>0.420</cell></row><row><cell>Fig. 1.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,144.73,602.00,293.67,7.86"><p>We did not participate in phase A due to a lack of resources on our side.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,144.73,612.96,299.46,8.12"><p>In practice, DBpedia<ref type="bibr" coords="3,232.04,612.96,9.73,7.86" target="#b7">[8]</ref> rdf:type ontology can be leveraged for this task.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,144.73,623.92,335.87,7.86;3,144.73,634.88,201.98,7.86"><p>The rest of the questions were unused; we opted for a smaller dataset as the Ge-neOntology access was causing quite a slow-down.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="3,137.50,644.07,3.65,5.24;3,144.73,645.84,335.86,7.86;3,144.73,656.80,86.75,7.86"><p><ref type="bibr" coords="3,137.50,644.07,3.65,5.24" target="#b3">4</ref> This performance is done using the open domain metric which permits gold standard substrings, see above.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="3,138.35,569.65,342.25,7.86;3,146.91,580.60,287.58,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="3,192.53,569.65,230.02,7.86">YodaQA: A Modular Question Answering System Pipeline</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Baudiš</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="3,442.33,569.65,38.27,7.86;3,146.91,580.60,287.58,7.86">POSTER 2015 -19th International Student Conference on Electrical Engineering</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="4,138.35,258.19,342.24,7.86;4,146.91,269.15,333.68,7.86;4,146.91,280.11,207.01,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,196.87,258.19,238.35,7.86">YodaQA: A Modular Question Answering System Pipeline</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Baudiš</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,459.35,258.19,21.25,7.86;4,146.91,269.15,242.16,7.86">Sixth International Conference of the CLEF Association, CLEF&apos;15</title>
		<meeting><address><addrLine>Toulouse</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">September 8-11, 2015. 9283. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,138.35,291.07,342.24,7.86;4,146.91,302.03,333.68,7.86;4,146.91,312.96,261.53,7.89" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,394.58,302.03,86.01,7.86;4,146.91,312.99,122.64,7.86">Building watson: An overview of the deepqa project</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ferrucci</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Gondek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Kalyanpur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lally</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Murdock</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Prager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,277.51,312.99,50.69,7.86">AI magazine</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="59" to="79" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,138.35,323.95,342.25,7.86;4,146.91,334.91,333.67,7.86;4,146.91,345.86,333.68,7.86;4,146.91,356.80,240.49,7.89" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,468.56,334.91,12.03,7.86;4,146.91,345.86,333.68,7.86;4,146.91,356.82,79.32,7.86">An overview of the bioasq large-scale biomedical semantic indexing and question answering competition</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Tsatsaronis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Balikas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Malakasiotis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Partalas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zschunke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Alvers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Polychronopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,234.53,356.82,81.85,7.86">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">138</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,138.35,367.78,342.24,7.86;4,146.91,378.71,333.68,7.89;4,146.91,389.70,106.52,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,247.05,367.78,233.54,7.86;4,146.91,378.74,227.18,7.86">UIMA: An architectural approach to unstructured information processing in the corporate research environment</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ferrucci</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,382.51,378.74,64.96,7.86">Nat. Lang. Eng</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="327" to="348" />
			<date type="published" when="2004-09">September 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,138.35,400.66,342.25,7.86;4,146.91,411.62,193.05,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="4,281.96,400.66,198.64,7.86;4,146.91,411.62,49.88,7.86">Answer extraction as sequence tagging with tree edit distance</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,218.90,411.62,51.36,7.86">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="858" to="867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,138.35,422.58,342.24,7.86;4,146.91,433.51,81.77,7.89" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="4,201.10,422.58,155.14,7.86">WordNet: a lexical database for english</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,363.37,422.58,117.21,7.86">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,138.35,444.49,342.24,7.86;4,146.91,455.45,333.68,7.86;4,146.91,466.41,258.90,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="4,338.96,455.45,141.63,7.86;4,146.91,466.41,164.45,7.86">Dbpedia-a large-scale, multilingual knowledge base extracted from wikipedia</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Isele</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jentzsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kontokostas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">N</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hellmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Morsey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Van Kleef</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,319.55,466.41,57.60,7.86">Semantic Web</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,138.35,477.37,342.24,7.86;4,146.91,488.30,152.39,7.89" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="4,253.22,477.37,165.78,7.86">Gene ontology consortium: going forward</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">O</forename><surname>Consortium</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,427.86,477.37,52.73,7.86;4,146.91,488.33,32.61,7.86">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="1049" to="D1056" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
