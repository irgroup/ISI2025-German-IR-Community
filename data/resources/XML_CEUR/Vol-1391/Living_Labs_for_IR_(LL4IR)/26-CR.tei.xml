<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,177.58,115.96,260.20,12.62;1,206.30,133.89,202.75,12.62">Historical Clicks for Product Search: GESIS at CLEF LL4IR 2015</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,199.72,171.56,63.54,8.74"><forename type="first">Philipp</forename><surname>Schaer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">GESIS -Leibniz Institute for the Social Sciences</orgName>
								<address>
									<postCode>50669</postCode>
									<settlement>Cologne</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,290.42,171.56,108.38,8.74"><forename type="first">Narges</forename><surname>Tavakolpoursaleh</surname></persName>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Computer Science / EIS</orgName>
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<postCode>53117</postCode>
									<settlement>Bonn</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,177.58,115.96,260.20,12.62;1,206.30,133.89,202.75,12.62">Historical Clicks for Product Search: GESIS at CLEF LL4IR 2015</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">397D0CC7354021A2F33C28B8278E24E7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Living Labs for Information Retrieval (LL4IR) lab was held for the first time at CLEF and GESIS participated in this pilot evaluation. We took part in the product search task and describe our system that is based on the Solr search engine and includes a re-reranking based on historical click data. This brief workshop note also includes some preliminary results, discussion and some lessons learned.</p><p>One of the remaining of formally three different tasks were the product search on the e-commerce site REGIO J ÁT ÉK. As previously noted in the Living Labs Challenge Report <ref type="bibr" coords="1,213.68,637.10,10.52,8.74" target="#b1">[2]</ref> this specific task introduces a range of different challenges,</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In 2015 the Living Labs for Information Retrieval initiative (LL4IR) for the first time organized a lab at the CLEF conference series <ref type="bibr" coords="1,369.94,378.53,9.96,8.74" target="#b4">[5]</ref>. This lab can be seen as a pilot evaluation lab or as stated by the organizers: "a first round". GESIS took part in this pilot to get a first hand experience with the lab's API <ref type="bibr" coords="1,450.60,402.44,10.52,8.74" target="#b0">[1]</ref> and the rather new evaluation methodology. The main focus was not on winning the implicit competition every evaluation campaign is, but to learn more about the procedures and the systems used. Since this lab had no direct predecessor we could not learn from previous results and best practices. Previous systems that we used in other CLEF labs, namely the CHiC lab on cultural heritage <ref type="bibr" coords="1,444.62,462.21,9.96,8.74" target="#b3">[4]</ref>, were from a totally different domain and could therefore not be directly applied to the use cases of LL4IR. So, the main objective of this pilot participation was to establish the retrieval environment and to surpass the obvious issues in the first place. After the initial three tasks were cut down to only two, we took part in the remaining task on product search using the REGIO J ÁT ÉK e-commerce site.</p><p>In the following paper we will present our approaches and preliminary results from their assessments.</p><p>2 Product Search with REGIO J ÁT ÉK issues, and possibilities. In the report some issues like "generally little amount of textual material associated with products (only name, description, and name of categories)" were noted. On the other hand additional information included in the available metadata were listed, among these were: (1) Historical click information for queries, (2) collection statistics, (3) product taxonomy, (4) product photos, (5) date/time when the product first became available, (6) historical click information for products, and (7) sales margins.</p><p>In our approach we decided to re-use the historical click data for products and a keyword-based relevance score derived from a Solr indexation of the available product metadata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Ranking Approach</head><p>We used a Solr search server (version 5.0.0) to index all available metadata provided by the API for every document related to the given queries. For each document (or more precisely for each product) we additionally stored the corresponding query number. This way we were able to retrieve all available candidate documents and rank them according to the Solr score based on the query string. Additionally we added the historical click rates as a weighting factor into the final ranking if this was available at query time.</p><p>Some query related documents had no term which can be matched with the query string and therefore we were not able to retrieve every query related document on a mere query string-based search. We had to add the query number to the query itself as a boolean query part to fix this issue and used Solr's query syntax and the default QParserPlugin<ref type="foot" coords="2,302.54,400.78,3.97,6.12" target="#foot_0">3</ref> . This syntax allows the use of boolean operators and of different boosting factors which were both used in the query formulation: qid:query[id]^0.0001 OR (qid:query[id]^0.0001 AND query[str])</p><p>Using this query string we got a Solr-ranked list of documents for each query which were then re-ranked using the historical click rates as outlined in algorithm 1. Basically it's a linear combination of a boosted search on the document id (field name docid) and the vector space-based relevance score of the query string. This is a typical "the rich are getting richer" approach where formally successful products are more likely to be once again ranked high in the result list. The approach was inspired by a presentation by Andrzej Bia lecki <ref type="bibr" coords="2,442.42,534.89,9.96,8.74" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Solr Configuration</head><p>As stated above we used a Solr installation. To keep the system simple, we used the original Solr configuration and imported the REGIO dump using the originally provided schema.xml and the configuration from table 1. We did not include any language specific configurations for stemmers or stop word lists, since the Hungarian Solr stemmer returned the same results as the generic stemmer. We used the following standard components for text general fields:</p><p>Algorithm 1: Re-ranking algorithm merging the Solr ranking score and the historical click rates.</p><p>Data: runs of production system correspond to the queries to products of REGIO JA TE Ḱ site Result: runs of our experimental system according to the document's fields and click-through rate for query in queries do The detailed indexation configuration of the given fields is listed in table 1. We used a very limited set of fields for the first round and were basically only searching in the title and description field. We changed this in the second round where we included all the available metadata in the search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Official Run</head><p>The results of the campaign were documented by giving numbers on the (1) impressions per query, (2) wins, losses, and ties calculated against the production system, and (3) the calculated outcome (#wins/(#wins+#losses). As noted in the documentation a win "is defined as the experimental system having more clicks on results assigned to it by Team Draft Interleaving than clicks on results assigned to the production system". This means that any value below 0.5 can be seen as a performance worse than the production system. Due to the problem of unavailable items in the shop the expected outcome had to be corrected to 0.28 as unavailable items were not filtered out for participating systems (for more details check the workshop overview paper <ref type="bibr" coords="3,323.70,632.05,10.30,8.74" target="#b4">[5]</ref>).</p><p>Our system received 523 impressions in the two weeks test period. This makes roughly 37.4 impressions per day and 1.6 impressions per hour. Although we don't have any comparable numbers we interpret these impression rates to be quite low. If we compare to the other teams we received the lowest number of impressions while for example system UiS-Mira received 202 more impressions in the same time period (725 impressions which is 38% more impressions than we got). This is not quite in line with the principle of giving fair impression rates between the different teams. Another thing regarding the impressions is the fact that different queries had very different impression rates (see figure <ref type="figure" coords="4,439.13,553.87,3.87,8.74" target="#fig_1">1</ref>). While some of them got more than 50 impressions others (actually 5) were not shown to the users at all.</p><p>Our approach did not perform very well due to some obvious misconfiguration and open issues of our implementation. In fact we provided the least efficient ranking compared to the other three participants <ref type="bibr" coords="4,389.07,620.25,9.96,8.74" target="#b4">[5]</ref>. We could achieve an outcome of 0.2685 by getting 40 wins vs. 109 losses and 374 ties. On the other hand no other participant was able to beat the baseline with an outcome rate of 0.4691. The best performing system received an outcome rate of 0.3413 (system UiS-Mira) and was able to be better than the expected outcome of 0.28 but below the simple baseline provided by the organizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Unofficial 2nd Round</head><p>We also took part in the 2nd evaluation round and adapted some parameters of the system. As there was a misconfiguration in the Solr system of round #1 we only searched the titles and description of products. We fixed this bug so that for round #2 we correctly indexed all available metadata fields. Another issue from round #1 was that not all 50 test topics were correctly calculated. We only used the historical click data for 1 test topic and 13 training topics. The other topics were just the standard Solr ranking without any click history boosting. We fixed this issue for round #2 where now all 50 topics are correctly calculated according to the described boosting approach. After we corrected these two points we observe a clear increase in the outcomes. The outcome increased to 0.4520 by getting 80 wins, 97 losses and 639 ties. Although the performace increase might be due to the fixes introduced by the organizers regarding unavailable items we could still see some positive effects: The performance of the other teams increased too but while we were the weakest team in round #1 we were now able to provide the second best system performance. We also outperformed the winning system from round #1. Nevertheless we (and no other system) was able to compete with the productive system.</p><p>Comparing the number of impressions we see a clear increase in queries that are above the 0.5 threshold and the baseline (13 queries each) and the impressions in total and per day are also increased. The issue of unbalanced impression rates stays the same for round #2 (see figure <ref type="figure" coords="5,310.25,656.12,3.87,8.74" target="#fig_1">1</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Lessons Learned, Open Issues, and Future Work</head><p>The first prominent issue that arose when processing the data collection was the Hungarian content. Since we don't know Hungarian we were not able to directly read or understand the content or queries and therefore had used a languageand content-agnostic approach. Although the different fields were documented<ref type="foot" coords="6,476.12,487.14,3.97,6.12" target="#foot_1">4</ref> the content was hidden behind the language barrier, except for obvious brand names like Barbie or He-Man.</p><p>It would have been really interesting and maybe useful to make use of further provided metadata like for example the classes of the two-level deep topical categorization system to which the products were assigned to. As we don't know more about this categorization system, expect for the ad-hoc translation <ref type="foot" coords="6,460.42,560.09,3.97,6.12" target="#foot_2">5</ref> we could only add the category names to the index and leave it with that.</p><p>A typical problem with real world systems was also present in the available queries: Real world users tend to use rather short queries. For the 100 available query strings only 15 had more than one word and only 2 had more than 2 words (R-q22: "bogyó és babóca" and R-q50: "my little pony"). The average word length per query was 1.17 and the average string length was 7.16 characters.</p><p>Another factor that we did not think of, although it was clearly stated in the documentation and in the key concepts<ref type="foot" coords="7,321.12,153.28,3.97,6.12" target="#foot_3">6</ref> , was the fact that no feedback data was available during the test phase. As this came to our mind way too late we were only able to include historical click data for some queries. Therefore the validity of our results from round #1 is weak as their are to few queries to really judge on the influence of the historical click data vs. live click data. We were not able to include new feedback data into our rankings after the official upload and the beginning of the test phase. All the uploaded rankings were "final" and only depend on historical clicks. While this is of course due to the experimental setup it is not truly a "living" component in the living lab environment (metaphorically spoken). On top of that not every document received clicks and therefore some documents are missing any hint of being relevant at all.</p><p>Last but not least we had to struggle with speed issues of the LL4IR platform itself. As mentioned in the workshop report of 2014 there are known issues on "scaling up with the number of participants and sites talking to the API simultaneously" <ref type="bibr" coords="7,184.63,322.23,9.96,8.74" target="#b1">[2]</ref>. Although they state that these bottlenecks have been identified and that they have started addressing these it still takes some time to correspond with the API. To get a feeling for the lack of the systems performance: The extraction of the final evaluation outcomes took roughly 45 minutes to extract 100 short JSON snippets not longer than in listing 1.1. Same is true for the generation of the list of available queries, result list and other data sets. The development team should thing about caching these kind of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>To sum it up, we succeeded in our primary objective of our participation which was to learn about the LL4IR API and the evaluation methodology. We could clearly improve our results from round #1 to round #2 and learned a lot during the campaign. We fixed some obvious configuration issues in our Solr system and were therefore desperately looking forward to the start of the second phase of the evaluation that started on 15 June 2015. As it turned out, these issues could be solved and the performance of the retrieval could be clearly improved. Although we are not able to simply compare round #1 and #2 due to the misconfiguration we can see the positive effects of the including of historical click data to boost on popular products.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,165.05,197.40,85.45,6.12;3,165.05,207.76,70.84,6.12;3,165.05,217.45,63.97,6.14;3,180.39,227.49,114.48,6.12;3,180.39,239.24,106.56,6.12;3,165.05,248.94,14.15,6.14;3,165.05,259.96,292.91,6.12;3,165.05,270.03,57.20,6.12;3,165.05,279.39,108.48,6.12;3,165.05,288.76,127.45,6.12;3,149.71,298.48,16.66,7.89;3,140.99,339.81,339.60,8.77;3,151.70,351.80,162.17,8.74;3,140.99,363.88,339.60,8.77;3,151.70,375.87,31.58,8.74;3,140.99,387.95,331.17,8.77"><head></head><label></label><figDesc>run = get doclist(query) ctr = get ctr query for doc in run do doc detail = get docDetail doc BuildSolrIndex doc detail,qid end myQuery = docid1ˆctr1 OR docid2ˆctr2 OR . . . OR docidnˆctrn OR qidˆ0.0001 AND query 'str' myRun = solr.search myQuery update runs key, myRun , feedbacks end -StandardTokenizerFactory: A general purpose tokenizer, which divides a string into tokens with various types. -StopFilterFactory: Words from the Solr included stopword lists are discarded. -LowerCaseFilterFactory: All letters are indexed and queried as lowercase.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,134.77,375.16,345.83,7.89;6,134.77,386.14,170.13,7.86"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Distribution of impressions per topic for the first and official CLEF round (blue) and the second unofficial test round (red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,134.77,115.91,345.83,320.04"><head>Table 1 .</head><label>1</label><figDesc>Solr field configuration for the available product metadata. Since different configurations for round #1 and #2 were used we also report in the usage of the fields for the two evaluation rounds.</figDesc><table coords="4,153.45,160.07,301.33,275.88"><row><cell>field</cell><cell>type</cell><cell>multiValued</cell><cell>round #1</cell><cell>round #2</cell></row><row><cell>title</cell><cell>text general</cell><cell></cell><cell></cell><cell></cell></row><row><cell>category</cell><cell>text general</cell><cell></cell><cell></cell><cell></cell></row><row><cell>content</cell><cell>text general</cell><cell></cell><cell></cell><cell></cell></row><row><cell>creation time</cell><cell>string</cell><cell></cell><cell></cell><cell></cell></row><row><cell>docid</cell><cell>text general</cell><cell></cell><cell></cell><cell></cell></row><row><cell>main category</cell><cell>text general</cell><cell></cell><cell></cell><cell></cell></row><row><cell>brand</cell><cell>text general</cell><cell></cell><cell></cell><cell></cell></row><row><cell>product name</cell><cell>text general</cell><cell></cell><cell></cell><cell></cell></row><row><cell>photos</cell><cell>string</cell><cell></cell><cell></cell><cell></cell></row><row><cell>short description</cell><cell>text general</cell><cell></cell><cell></cell><cell></cell></row><row><cell>description</cell><cell>text general</cell><cell></cell><cell></cell><cell></cell></row><row><cell>category id</cell><cell>string</cell><cell></cell><cell></cell><cell></cell></row><row><cell>main category id</cell><cell>string</cell><cell></cell><cell></cell><cell></cell></row><row><cell>bonus price</cell><cell>float</cell><cell></cell><cell></cell><cell></cell></row><row><cell>available</cell><cell>string</cell><cell></cell><cell></cell><cell></cell></row><row><cell>age min</cell><cell>string</cell><cell></cell><cell></cell><cell></cell></row><row><cell>age max</cell><cell>string</cell><cell></cell><cell></cell><cell></cell></row><row><cell>characters</cell><cell>string</cell><cell></cell><cell></cell><cell></cell></row><row><cell>queries</cell><cell>text general</cell><cell></cell><cell></cell><cell></cell></row><row><cell>gender</cell><cell>string</cell><cell></cell><cell></cell><cell></cell></row><row><cell>arrived</cell><cell>string</cell><cell></cell><cell></cell><cell></cell></row><row><cell>qid</cell><cell>string</cell><cell></cell><cell></cell><cell></cell></row><row><cell>characters</cell><cell>string</cell><cell></cell><cell></cell><cell></cell></row><row><cell>site id</cell><cell>string</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,188.03,115.91,235.71,100.86"><head>Table 2 .</head><label>2</label><figDesc>Some basic statistics on the results.</figDesc><table coords="5,333.16,138.15,90.58,7.86"><row><cell>round #1 round #2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,176.44,235.58,262.47,58.77"><head>Table 3 .</head><label>3</label><figDesc>Outcome, wins, losses and ties from round #1 and #2.</figDesc><table coords="5,188.03,259.57,224.60,34.78"><row><cell></cell><cell>outcome</cell><cell>wins</cell><cell>losses</cell><cell>ties</cell></row><row><cell>round #1</cell><cell>0.2685</cell><cell>40</cell><cell>109</cell><cell>374</cell></row><row><cell>round #2</cell><cell>0.4520</cell><cell>80</cell><cell>97</cell><cell>639</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="2,144.73,657.44,207.12,7.47"><p>https://wiki.apache.org/solr/SolrQuerySyntax</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="6,144.73,635.53,326.30,7.47"><p>http://doc.living-labs.net/en/latest/usecase-regio.html#usecase-regio</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2" coords="6,144.73,646.48,320.10,7.47;6,144.73,657.44,183.59,7.47"><p>https://translate.google.com/translate?sl=auto&amp;tl=en&amp;u=http\%3A\%2F\ %2Fwww.regiojatek.hu\%2Fkategoriak.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3" coords="7,144.73,657.44,297.56,7.47"><p>http://doc.living-labs.net/en/latest/guide-participant.html#key</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>Besides the previously (maybe unfair) mentioned complains about the system's and API's performance we are really impressed by it's functionality and stability. The online documentation and the support by the organizers were clear, direct and always helpful. Thank you.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Listing 1.1. Sample output of the outcome documentation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{</head><p>" outcomes ": [ { " impressions ": 36 , " losses ": 11 , " outcome ": "0.15384615384615385" , " qid ": "R -q60 " , " site_id ": " R " , " test_period ": { " end ": " Sat , 16 May 2015 00:00:00 -0000" , " name ": " CLEF LL4IR Round #1" , " start ": " Fri , 01 May 2015 00:00:00 -0000" } , " ties ": 23 , " type ": " test " , " wins ": 2 } ] }</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="8,138.35,400.93,342.24,7.86;8,146.91,411.89,333.68,7.86;8,146.91,422.85,333.68,8.12;8,146.91,434.45,311.68,7.47" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,275.85,400.93,200.77,7.86">Head first: Living labs for ad-hoc search evaluation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Schuth</surname></persName>
		</author>
		<ptr target="http://www.anneschuth.nl/wp-content/uploads/2014/08/cikm2014-lleval.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="8,160.81,411.89,319.78,7.86;8,146.91,422.85,142.84,7.86;8,358.25,422.85,38.59,7.86">Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management</title>
		<meeting>the 23rd ACM International Conference on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1815" to="1818" />
		</imprint>
	</monogr>
	<note>CIKM &apos;14</note>
</biblStruct>

<biblStruct coords="8,138.35,444.77,342.24,8.11;8,146.91,456.37,278.73,7.47" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Schuth</surname></persName>
		</author>
		<ptr target="http://living-labs.net/wp-content/uploads/2014/05/LLC_report.pdf" />
		<title level="m" coord="8,276.94,444.77,146.58,7.86">Living labs for ir challenge workshop</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,466.68,342.24,7.86;8,146.91,477.64,333.68,8.12;8,146.91,489.25,335.22,7.47" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bia Lecki</surname></persName>
		</author>
		<ptr target="http://de.slideshare.net/LucidImagination/bialecki-andrzej-clickthroughrelevancerankinginsolrlucidworksenterprise" />
		<title level="m" coord="8,207.48,466.68,273.11,7.86;8,146.91,477.64,85.37,7.86">Implementing click-through relevance ranking in solr and lucidworks enterprise</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,499.56,342.24,7.86;8,146.91,510.52,333.68,7.86;8,146.91,521.48,288.76,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,401.52,499.56,79.07,7.86;8,146.91,510.52,244.62,7.86">Dealing with sparse document and topic representations: Lab report for chic 2012</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Schaer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hienert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Sawitzki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Wira-Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lüke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,412.53,510.52,68.05,7.86;8,146.91,521.48,260.09,7.86">CLEF 2012 Labs and Workshop, Notebook Papers: CLEF/CHiC Workshop-Notes</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,532.44,342.24,7.86;8,146.91,543.40,333.68,7.86;8,146.91,554.36,333.68,7.86;8,146.91,565.31,38.17,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,275.93,532.44,204.67,7.86;8,146.91,543.40,101.60,7.86">Overview of the living labs for information retrieval evaluation (ll4ir) clef lab</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Schuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,291.98,543.40,188.61,7.86;8,146.91,554.36,70.35,7.86">CLEF 2015 -6th Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="8,225.03,554.36,169.71,7.86">Lecture Notes in Computer Science (LNCS</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015-09">2015. September 2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
