<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,167.73,115.96,279.89,12.62">LifeCLEF Bird Identification Task 2015</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,151.87,153.63,56.55,8.74"><forename type="first">Hervé</forename><surname>Goëau</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inria ZENITH team</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,218.98,153.63,56.56,8.74"><forename type="first">Hervé</forename><surname>Glotin</surname></persName>
							<email>glotin@univ-tln.fr</email>
							<affiliation key="aff1">
								<orgName type="laboratory">CNRS LSIS</orgName>
								<orgName type="institution" key="instit1">Aix Marseille Univ</orgName>
								<orgName type="institution" key="instit2">ENSAM</orgName>
								<orgName type="institution" key="instit3">Univ. Toulon</orgName>
								<orgName type="institution" key="instit4">Institut Univ. de France</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,286.09,153.63,90.66,8.74"><forename type="first">Willem-Pier</forename><surname>Vellinga</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Xeno-canto Foundation</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,387.31,153.63,68.94,8.74"><forename type="first">Robert</forename><surname>Planqué</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Xeno-canto Foundation</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,227.73,165.58,71.09,8.74"><forename type="first">Andreas</forename><surname>Rauber</surname></persName>
							<email>rauber@ifs.tuwien.ac.at</email>
							<affiliation key="aff3">
								<orgName type="institution">Vienna University of Technology</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,328.76,165.58,48.07,8.74"><forename type="first">Alexis</forename><surname>Joly</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inria ZENITH team</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">LIRMM</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,167.73,115.96,279.89,12.62">LifeCLEF Bird Identification Task 2015</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CB2B822B3A611FDFFB6299D45DB95130</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>LifeCLEF</term>
					<term>bird</term>
					<term>song</term>
					<term>call</term>
					<term>species</term>
					<term>retrieval</term>
					<term>audio</term>
					<term>collection</term>
					<term>identification</term>
					<term>fine-grained classification</term>
					<term>evaluation</term>
					<term>benchmark</term>
					<term>bioacoustics</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The LifeCLEF bird identification task provides a testbed for a system-oriented evaluation of 999 bird species identification. The main originality of this data is that it was specifically built through a citizen science initiative conducted by Xeno-Canto, an international social network of amateur and expert ornithologists. This makes the task closer to the conditions of a real-world application than previous, similar initiatives. This overview presents the resources and the assessments of the task, summarizes the retrieval approaches employed by the participating groups, and provides an analysis of the main evaluation results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Accurate knowledge of the identity, the geographic distribution and the evolution of bird species is essential for a sustainable development of humanity as well as for biodiversity conservation. Unfortunately, such basic information is often only partially available for professional stakeholders, teachers, scientists and citizens. In fact, it is often incomplete for ecosystems that possess the highest diversity, such as tropical regions. A noticeable cause and consequence of this sparse knowledge is that identifying birds is usually impossible for the general public, and often a difficult task for professionals like park rangers, ecology consultants, and of course, the ornithologists themselves. This "taxonomic gap" <ref type="bibr" coords="1,134.77,572.43,15.50,8.74" target="#b19">[21]</ref> was actually identified as one of the main ecological challenges to be solved during United Nations Conference in Rio de Janeiro, Brazil, in 1992.</p><p>The use of multimedia identification tools is considered to be one of the most promising solutions to help bridging this taxonomic gap <ref type="bibr" coords="1,384.69,608.30,14.61,8.74" target="#b13">[14]</ref>, <ref type="bibr" coords="1,406.59,608.30,9.96,8.74" target="#b7">[8]</ref>, <ref type="bibr" coords="1,423.51,608.30,9.96,8.74" target="#b5">[6]</ref>, <ref type="bibr" coords="1,440.42,608.30,14.61,8.74" target="#b18">[20]</ref>, <ref type="bibr" coords="1,462.32,608.30,14.61,8.74" target="#b17">[19]</ref>, <ref type="bibr" coords="1,134.77,620.25,14.61,8.74" target="#b11">[12]</ref>. With the recent advances in digital devices, network bandwidth and information storage capacities, the collection of multimedia data has indeed become an easy task. In parallel, the emergence of "citizen science" and social networking tools has fostered the creation of large and structured communities of nature observers (e.g. eBird <ref type="foot" coords="2,224.21,117.42,3.97,6.12" target="#foot_0">6</ref> , Xeno-canto <ref type="foot" coords="2,284.17,117.42,3.97,6.12" target="#foot_1">7</ref> , iSpot<ref type="foot" coords="2,321.30,117.42,3.97,6.12" target="#foot_2">8</ref> , etc.) that have started to produce outstanding collections of audio and/or visual records. Unfortunately, the performance of the state-of-the-art multimedia analysis techniques on such data is still not well understood and it is far from reaching the real world's requirements in terms of identification tools. Most existing studies or available tools typically identify a few tens of species with moderate accuracy whereas they should be scaled-up to take one, two or three orders of magnitude more, in terms of number of species.</p><p>The LifeCLEF Bird task proposes to evaluate one of these challenges [?] based on big and real-world data and defined in collaboration with biologists and environmental stakeholders so as to reflect realistic usage scenarios.</p><p>Using audio records rather than bird pictures is justified by current practices <ref type="bibr" coords="2,134.77,271.42,9.96,8.74" target="#b5">[6]</ref>, <ref type="bibr" coords="2,152.54,271.42,14.61,8.74" target="#b18">[20]</ref>, <ref type="bibr" coords="2,175.31,271.42,14.61,8.74" target="#b17">[19]</ref>, <ref type="bibr" coords="2,198.07,271.42,9.96,8.74" target="#b4">[5]</ref>. Birds are actually not easy to photograph; audio calls and songs have proven to be easier to collect and sufficiently species specific.</p><p>Only three notable previous worldwide initiatives on bird species identification based on their songs or calls have taken place, all three in 2013. The first one was the ICML4B bird challenge joint to the International Conference on Machine Learning in Atlanta, June 2013 <ref type="bibr" coords="2,319.15,335.68,9.96,8.74" target="#b1">[2]</ref>. It was initiated by the SABIOD MASTODONS CNRS group<ref type="foot" coords="2,260.11,346.06,3.97,6.12" target="#foot_3">9</ref> , the University of Toulon and the National Natural History Museum of Paris <ref type="bibr" coords="2,272.74,359.59,9.96,8.74" target="#b8">[9]</ref>. It included 35 species, and 76 participants submitted their 400 runs on the Kaggle interface. The second challenge was conducted by F. Brigs at MLSP 2013 workshop, with 15 species, and 79 participants in August 2013. The third challenge, and biggest in 2013, was organised by University of Toulon, SABIOD and Biotope <ref type="bibr" coords="2,351.54,407.41,9.96,8.74" target="#b3">[4]</ref>, with 80 species from the Provence, France. More than thirty teams participated, reaching 92% of average AUC. Descriptions of the best systems of ICML4B and NIPS4B bird identification challenges are given in the on-line books <ref type="bibr" coords="2,349.38,443.28,10.96,8.74" target="#b1">[2,</ref><ref type="bibr" coords="2,360.34,443.28,7.31,8.74" target="#b0">1]</ref> including, in some cases, references to useful scripts. In collaboration with the organizers of these previous challenges, BirdCLEF 2014 and 2015 go one step further by (i) significantly increasing the species number by almost an order of magnitude (ii) working on real-world data collected by hundreds of recordists (iii) moving to a more usage-driven and system-oriented benchmark by allowing the use of meta-data and defining information retrieval oriented metrics. Overall, the task is expected to be much more difficult than previous benchmarks because of the higher confusion risk between the classes, the higher background noise and the higher diversity in the acquisition conditions (devices, recordists uses, contexts diversity, etc.). It will therefore probably produce substantially lower scores and offer a better progression margin towards building real-world generalist identification tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset</head><p>The training and test data of the bird task is composed by audio recordings hosted on xeno-canto.org (XC). Xeno-canto is a web-based community of bird sound recordists worldwide with more than 2300 active contributors that have already collected more than 240,000 recordings of about 9330 species (may 2015). 999 species from Brazil are used in the BirdCLEF dataset. They represent the species of that country with the highest number of recordings on XC, totalling 33,862 recordings contributed by hundreds of users. The dataset has between 13 and 234 recordings per species, recorded by between 1 and 72 recordists. This dataset also contains the entire dataset from the 2014 BirdCLEF challenge <ref type="bibr" coords="3,462.33,235.60,14.61,8.74" target="#b9">[10]</ref>, which contained about 14,000 recordings from 501 species.</p><p>To avoid any bias in the evaluation related to the audio devices used, each audio file has been normalized to a constant bandwidth of 44.1 kHz and coded over 16 bits in .wav mono format (the right channel was selected by default). The conversion from the original Xeno-canto data set was done using ffmpeg, sox and matlab scripts. An optimized 16 Mel Filter Cepstrum Coefficients for bird identification (according to an extended benchmark <ref type="bibr" coords="3,370.23,319.29,10.79,8.74" target="#b6">[7]</ref>) have been computed with their first and second temporal derivatives on the whole set. They were used in the best systems run in ICML4B and NIPS4B challenges <ref type="bibr" coords="3,420.84,343.20,9.96,8.74" target="#b1">[2]</ref>, <ref type="bibr" coords="3,437.43,343.20,9.96,8.74" target="#b0">[1]</ref>, <ref type="bibr" coords="3,450.72,343.20,9.96,8.74" target="#b3">[4]</ref>, <ref type="bibr" coords="3,467.31,343.20,9.96,8.74" target="#b8">[9]</ref>.</p><p>Audio records are associated with various meta-data including the species of the most active singing bird, the species of the other birds audible in the background, the type of sound (call, song, alarm, flight, etc.), the date and location of the observations (from which rich statistics on species distribution can be derived), common names and collaborative quality ratings. All of them were produced collaboratively by the Xeno-canto community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task Description</head><p>Participants were asked to determine the species of the most active singing birds in each query file. The background noise can be used as any other meta-data, but it is forbidden to correlate the test set of the challenge with the original annotated Xeno-canto data base (or with any external content as many of them are circulating on the web). More precisely, the whole BirdCLEF dataset has been split in two parts, one for training (and/or indexing) and one for testing. The test set was built by randomly choosing 1/3 of the observations of each species whereas the remaining observations were kept in the reference training set. Recordings of the same species done by the same person the same day are considered as being part of the same observation and cannot be split across the test and training set. The xml files containing the meta-data of the query recordings were purged so as to erase the foreground and background species names (the ground truth), the vernacular names (common names of the birds) and the collaborative quality ratings (that would not be available at query stage in a real-world mobile application). Meta-data of the recordings in the training set are kept unaltered.</p><p>The groups participating to the task were asked to produce up to 4 runs containing a ranked list of the most probable species for each record of the test set. Each species had to be associated with a normalized score in the range [0, 1] reflecting the likelihood that this species was singing in the sample. For each submitted run, participants had to say if the run was performed fully automatically or with a human assistance in the processing of the queries, and if they used a method based on only audio analysis or with the use of the metadata. The metric used to compare the runs was the Mean Average Precision averaged across all queries. Since the audio records contain a main species and often some background species belonging to the set of 501 species in the training, we decided to use two metrics, one focusing on all species (MAP1) and a second one focusing only on the main species (MAP2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Participants and methods</head><p>137 research groups worldwide registered for the task and downloaded the data (from a total of 189 groups that registered for at least one of the three Life-CLEF tasks). This shows the high attractiveness of the challenge in both the multimedia community (presumably interested in several tasks) and in the audio and bioacoustics community (presumably registered only to the bird songs task). Finally, 6 of the registrants crossed the finish line by submitting runs and 5 of them submitted working notes explaining their runs in details. We list them hereafter in alphabetical order and give a brief overview of the techniques they used in their runs. We would like to point out that the LifeCLEF benchmark is a system-oriented evaluation and not a deep or fine evaluation of the underlying algorithms. Readers interested in the scientific and technical details of the implemented methods should refer to the LifeCLEF 2015 working notes or to the research papers of each participant (referenced below): CHIN. AC. SC., China, 3 runs: This participant attempted to experiment a baseline audio classification system based on the classification of Mel-bands representations and their scattering refinements <ref type="bibr" coords="4,350.83,523.35,10.51,8.74" target="#b2">[3]</ref> using a Gaussian Mixture Model. The first run used only MFCC features with 128 Gaussian mixtures, the second run used the scattering refinements with 32 Gaussian mixtures, the third run used the scattering refinements with 128 Gaussian mixtures.</p><p>Golem, Mexico, 3 runs <ref type="bibr" coords="4,259.47,584.36,16.80,8.77" target="#b14">[15]</ref>: This participant experimented a simple yet highly scalable system based on the classification of Mel-bands representations using a random forest. The extracted Mel bands per recording were actually pooled through simple statistics (i.e. mean, standard deviation, median and skewness), resulting in time-and space-efficient 320-dimensional features to be trained by the classifier.</p><p>Inria Zenith, France, 3 runs <ref type="bibr" coords="5,279.53,118.96,16.80,8.77" target="#b10">[11]</ref>: Inspired by recent works on fine-grained image classification, this group introduced a new match kernel based on the shared nearest neighbors of the low level audio features extracted at the frame level. To make such strategy scalable to the tens of millions of MFCC features extracted from the training set, they make use of high-dimensional hashing techniques coupled with an efficient approximate nearest neighbors search algorithm with controlled quality. Further improvements are obtained by (i) using a sliding window for the temporal pooling of the raw matches (ii) weighting each low level feature according to the semantic coherence of its nearest neighbors. The final classification was then completed thanks to a support vector machine trained on top of the resulting matching-based representations.</p><p>MNB TSA, Germany, 4 runs <ref type="bibr" coords="5,291.20,273.95,16.80,8.77" target="#b12">[13]</ref>: This participant combined two main categories of features for the classification: parametric acoustic features (see openSMILE Audio Statistics) and probabilities of species-specific spectrogram segments (see Segment-Probabilities). This second source of information, which performs the best, consists in extracting for each species, a set of representative segments from spectrogram images. These segments are then used to extract Segment-Probabilities for each file by calculating the maxima of the normalized cross-correlation between all segments and the target spectrogram image via template matching. Due to the very large amount of audio data not all files belonging to a certain species were used as a source for segmentation (i.e. only good quality files without background species were used). Additionally, to further reduce the computation time, the spectrogram images were downsmapled before computing the template matching. The classification problem was then formulated as a multi-label regression task completed by training ensembles of randomized decision trees with probabilistic outputs. The training was performed in two passes, one selecting a small subset of the most discriminant features, and one training the final classifiers on the selected features (Run 1). To further improve classification results a bagging approach was used consisting in calculating further Segment-Probabilities from additional segments and to combine them either by averaging (Run 2) or by blending (Run 3 and Run 4 with more blends).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QMUL, UK, 1 run [17]:</head><p>This group focused on unsupervised feature learning in order to learn regularities in spectro-temporal content without reference to the training labels and further help the classifier to generalise to further content of the same type. MFCC features and several temporal variants are first extracted from the audio signal after a median-based thresholding pre-processing. Extracted low level features were then reduced through PCA whitening and clustered via spherical k-means (and a two-layer variant of it) to build the vocabulary. During classification, MFCC features are pooled by projecting them on the vocabulary with different temporal pooling strategies. Final supervised classification is achieved thanks to a random forest classifier. This method is the subject of a full-length article which can be read at <ref type="bibr" coords="5,361.18,656.12,14.61,8.74" target="#b16">[18]</ref>. Details of the different parameters settings used in each run are detailed in the working note [?].</p><p>MARF, Canada, 4 runs : These participants mainly attempted to transpose a speech processing method they developed earlier to the birds case (Modular Audio Recognition Framework (MARF)'s API, <ref type="bibr" coords="6,348.51,166.81,14.76,8.74">[16]</ref>). The first run was using only 20 LPC coefficients as features and the Chebyshev distance. The second run was using only the meta-data features using the MARFCAT approach <ref type="bibr" coords="6,465.09,190.72,15.50,8.74">[16]</ref> to represent the XML meta-data as a wave form without pre-processing, and using 512-window FFT features and cosine similarity measure. The third run was a concatenation of Run 1 and Run 2. The fourth run used the same set up as Run 1 but split the training data by quality ratings attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Figure <ref type="figure" coords="6,166.70,291.64,4.98,8.74" target="#fig_0">1</ref> and table <ref type="table" coords="6,220.77,291.64,4.98,8.74" target="#tab_0">1</ref> show the scores obtained by all the runs for the two distinct measured Mean Average Precision (MAP) evaluation measures: MAP1 when considering only the foreground species of each test recording and MAP2 when considering additionally the species listed in the Background species field of the metadata. Note that different colors have been used to easily differentiate between the methods using the metadata from the purely audio-based methods. The main outcome of the evaluation is that the use of matching-based scores as high-dimensional features to be classified by supervised classifiers (as done by MNB TSA and INRIA ZENITH) provides the best results, with a Mean Average Precision up to 0.454 for the fourth run of the MNB TSA group. These approaches notably outperform the unsupervised feature learning framework of the QMUL group as well as the baseline method of the Golem group. The matching of all the audio recordings however remains a very time-consuming process that had to be carefully designed in order to process a large-scale dataset such as the one deployed within the challenge. The MNB TSA group notably reduced as much as possible the number of audio segments to be matched thanks to an effective audio pre-processing and segmentation framework. They also restricted the extraction of these segments to the files having the best quality according to the user ratings and that do not have background species. On the other side, the INRIA ZENITH group did not use any segmentation but attempted to speed-up the matching though the use of a hash-based approximate k-nearest neighbors search scheme (on top of MFCC features). The better performance of the MNB TSA runs shows that cleaning the audio segments vocabulary before applying the matching is clearly beneficial. But using a scalable knn-based matching as the one of the INRIA ZENITH runs could be a complementary way to speed up the matching phase. It is interesting to notice that the first run of the MNB TSA group is roughly the same method than the one they used within the BirdCLEF challenge of the previous year <ref type="bibr" coords="7,196.33,656.12,15.50,8.74" target="#b9">[10]</ref> and which achieved the best results (with a MAP1 equals to 0.511 vs. 0.424 this year). This shows that the impact of the increasing difficulty of the challenge (with twice the number of species) is far from negligible. The performance loss is notably not compensated by the bagging extension of the method which resulted in a MAP1 equals to 0.454 for MNB TSA run 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper presented the overview and the results of the first LifeCLEF bird identification challenge 2015. With a number of registrant exceeding hundred, it showed a high interest of the multimedia and the bio-acoustic communities in applying their technologies to real-world environmental data such as the ones collected by Xeno-canto. The main outcome of this evaluation is a snapshot of the performances of state-of-the-art techniques that will hopefully serve as a guideline for developers interested in building end-user applications. One important conclusion of the campaign is that the two best performing methods were based on matching approaches attempting to construct high-dimensional representations of the audio recordings based on their matching scores in a large vocabulary of audio segments. The results of the evaluation clearly show the superiority of these approaches in terms of effectiveness but also point out the underlying scalability issues in terms of efficiency. The increasing complexity of the challenge over the previous year in terms of the number species and items, notably conducted to a consistent loss of the raw identification performance despite the progress of the underlying methods. Considering that the number of bird species on earth is more than 10,000 and that the number of singing insects is even much larger, we believe it is important to continue working on such large-scale identification issues in the next years.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,134.77,352.72,345.82,7.89;7,134.77,363.70,345.82,7.86;7,134.77,374.66,243.20,7.86;7,134.77,115.83,345.83,211.15"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Official scores of the LifeCLEF Bird Identification challenge 2015. MAP2 is the Mean Average Precision averaged across all queries taking into account the Background species (while MAP1 considers only the foreground species).</figDesc><graphic coords="7,134.77,115.83,345.83,211.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,141.60,392.29,326.02,222.78"><head>Table 1 :</head><label>1</label><figDesc>Raw results of the LifeCLEF 2014 Bird Identification Task</figDesc><table coords="6,141.60,409.54,326.02,205.52"><row><cell>Run name</cell><cell>Type</cell><cell>MAP 1</cell><cell>MAP 2</cell></row><row><cell></cell><cell></cell><cell cols="2">(without Bg. Sp.) (with Bg Sp.)</cell></row><row><cell>MNB TSA Run 4</cell><cell>AUDIO</cell><cell>0.454</cell><cell>0.414</cell></row><row><cell>MNB TSA Run 3</cell><cell>AUDIO</cell><cell>0.442</cell><cell>0.411</cell></row><row><cell>MNB TSA Run 2</cell><cell>AUDIO</cell><cell>0.442</cell><cell>0.405</cell></row><row><cell>MNB TSA Run 1</cell><cell>AUDIO</cell><cell>0.424</cell><cell>0.388</cell></row><row><cell>INRIA ZENITH Run 2</cell><cell>AUDIO</cell><cell>0.334</cell><cell>0.291</cell></row><row><cell>QMUL Run 1</cell><cell>AUDIO</cell><cell>0.302</cell><cell>0.262</cell></row><row><cell>INRIA ZENITH Run 3</cell><cell>AUDIO</cell><cell>0.292</cell><cell>0.259</cell></row><row><cell>INRIA ZENITH Run 1</cell><cell>AUDIO</cell><cell>0.265</cell><cell>0.240</cell></row><row><cell>GOLEM Run 2</cell><cell>AUDIO</cell><cell>0.171</cell><cell>0.149</cell></row><row><cell>GOLEM Run 1</cell><cell>AUDIO</cell><cell>0.161</cell><cell>0.139</cell></row><row><cell>CHIN. AC. SC. Run 1</cell><cell>AUDIO</cell><cell>0.01</cell><cell>0.009</cell></row><row><cell>CHIN. AC. SC. Run 3</cell><cell>AUDIO</cell><cell>0.009</cell><cell>0.01</cell></row><row><cell>CHIN. AC. SC. Run 2</cell><cell>AUDIO</cell><cell>0.007</cell><cell>0.008</cell></row><row><cell>MARF Run 1</cell><cell>AUDIO</cell><cell>0.006</cell><cell>0.005</cell></row><row><cell>MARF Run 2</cell><cell>METADATA</cell><cell>0.003</cell><cell>0.002</cell></row><row><cell>MARF Run 3</cell><cell>AUDIO &amp; METADATA</cell><cell>0.005</cell><cell>0.005</cell></row><row><cell>MARF Run 4</cell><cell>AUDIO</cell><cell>0.000</cell><cell>0.000</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_0" coords="2,144.73,623.92,69.41,7.86"><p>http://ebird.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_1" coords="2,144.73,634.88,114.96,7.86"><p>http://www.xeno-canto.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_2" coords="2,144.73,645.84,196.49,7.86"><p>http://www.ispotnature.org/communities/global</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_3" coords="2,144.73,657.44,118.18,7.47"><p>http://sabiod.univ-tln.fr</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,142.96,481.43,337.64,7.86;8,151.52,492.39,329.07,8.11;8,151.52,503.99,14.12,7.47" xml:id="b0">
	<monogr>
		<ptr target="http://sabiod.univ-tln.fr/NIPS4B2013_book.pdf" />
		<title level="m" coord="8,151.53,481.43,329.07,7.86;8,151.52,492.39,96.59,7.86">Proc. of Neural Information Processing Scaled for Bioacoustics: from Neurons to Big Data, joint to NIPS</title>
		<meeting>of Neural Information essing Scaled for Bioacoustics: from Neurons to Big Data, joint to NIPS</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,514.31,337.63,7.86;8,151.52,525.27,243.55,8.12" xml:id="b1">
	<monogr>
		<ptr target="http://sabiod.univ-tln.fr/ICML4B2013_book.pdf" />
		<title level="m" coord="8,151.53,514.31,329.07,7.86">Proc. of the first workshop on Machine Learning for Bioacoustics, joint to ICML</title>
		<meeting>of the first workshop on Machine Learning for Bioacoustics, joint to ICML</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,536.23,337.64,7.86;8,151.52,547.19,60.92,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,240.60,536.23,174.72,7.86">Multiscale scattering for audio classification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Andén</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,435.45,536.23,29.56,7.86">ISMIR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="657" to="662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,558.16,337.63,7.86;8,151.52,569.12,329.07,7.86;8,151.52,580.07,329.07,8.12;8,151.52,591.68,37.66,7.47" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,278.18,558.16,161.82,7.86">Overview of the nips4b bird classification</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Dufour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<ptr target="http://sabiod.univ-tln.fr/NIPS4B2013_book.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="8,459.20,558.16,21.39,7.86;8,151.52,569.12,329.07,7.86;8,151.52,580.07,75.74,7.86">Proc. of Neural Information Processing Scaled for Bioacoustics: from Neurons to Big Data, joint to NIPS</title>
		<meeting>of Neural Information essing Scaled for Bioacoustics: from Neurons to Big Data, joint to NIPS</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="12" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,602.00,337.63,7.86;8,151.52,612.96,329.07,7.86;8,151.52,623.92,329.07,7.86;8,151.52,634.87,135.70,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,265.30,612.96,215.29,7.86;8,151.52,623.92,184.95,7.86">Acoustic classification of multiple simultaneous bird species: A multi-instance multi-label approach</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Briggs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><forename type="middle">Z</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Raich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Hadley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Hadley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Betts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,343.95,623.92,136.64,7.86;8,151.52,634.87,66.07,7.86">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="page">4640</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,645.84,337.64,7.86;8,151.52,656.80,329.07,7.86;9,151.52,119.67,329.07,7.86;9,151.52,130.63,43.91,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,333.77,645.84,146.82,7.86;8,151.52,656.80,141.77,7.86">Sensor network for the monitoring of ecosystem: Bird species recognition</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Roe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,315.16,656.80,165.44,7.86;9,151.52,119.67,46.01,7.86;9,231.50,119.67,191.14,7.86">Intelligent Sensors, Sensor Networks and Information</title>
		<imprint>
			<date type="published" when="2007-12">2007. Dec 2007</date>
			<biblScope unit="page" from="293" to="298" />
		</imprint>
	</monogr>
	<note>ISSNIP 2007. 3rd International Conference on</note>
</biblStruct>

<biblStruct coords="9,142.96,141.59,337.64,7.86;9,151.52,152.55,329.07,7.86;9,151.52,163.51,260.22,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,359.84,141.59,120.76,7.86;9,151.52,152.55,262.20,7.86">Clusterized mel filter cepstral coefficients and support vector machines for bird song idenfication</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Dufour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Artieres</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Giraudet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,433.70,152.55,46.90,7.86;9,151.52,163.51,174.53,7.86">Soundscape Semiotics -Localization and Categorization</title>
		<imprint>
			<publisher>Glotin</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,174.47,337.63,7.86;9,151.52,185.43,329.07,7.86;9,151.52,196.39,329.07,8.12;9,151.52,207.99,136.51,7.47" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,273.37,174.47,207.22,7.86;9,151.52,185.43,220.95,7.86">Automated species identification: why not? Philosophical Transactions of the Royal Society of London</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">J</forename><surname>Gaston</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>O'neill</surname></persName>
		</author>
		<ptr target="http://rstb.royalsocietypublishing.org/content/359/1444/655.abstract" />
	</analytic>
	<monogr>
		<title level="j" coord="9,381.44,185.43,99.16,7.86;9,151.52,196.39,21.05,7.86">Series B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">359</biblScope>
			<biblScope unit="issue">1444</biblScope>
			<biblScope unit="page" from="655" to="667" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,218.30,337.64,7.86;9,151.52,229.26,329.07,7.86;9,151.52,240.22,285.54,8.11" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,239.68,218.30,223.24,7.86">Overview of the 1st int&apos;l challenge on bird classification</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sueur</surname></persName>
		</author>
		<ptr target="http://sabiod.univ-tln.fr/ICML4B2013_book.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="9,151.52,229.26,323.72,7.86">Proc. of the first workshop on Machine Learning for Bioacoustics, joint to ICML</title>
		<meeting>of the first workshop on Machine Learning for Bioacoustics, joint to ICML</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="17" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,251.18,337.97,7.86;9,151.52,262.14,18.43,7.86" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="9,357.34,251.18,123.25,7.86">Lifeclef bird identification task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rauber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,273.10,337.97,7.86;9,151.52,284.06,329.07,7.86;9,151.52,295.02,56.60,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,289.55,273.10,191.04,7.86;9,151.52,284.06,173.01,7.86">Shared nearest neighbors match kernel for bird songs identification -lifeclef 2015 challenge</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Champ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Buisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,346.08,284.06,134.51,7.86;9,151.52,295.02,27.93,7.86">Working notes of CLEF 201 conference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,305.98,337.97,7.86;9,151.52,316.93,329.07,7.86;9,151.52,327.89,209.08,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,298.24,316.93,182.35,7.86;9,151.52,327.89,43.01,7.86">Interactive plant identification based on social image data</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Bakić</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Barbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Selmi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carré</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mouysset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Molino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,201.70,327.89,89.28,7.86">Ecological Informatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="22" to="34" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,338.85,337.98,7.86;9,151.52,349.81,328.74,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,205.58,338.85,275.01,7.86;9,151.52,349.81,115.92,7.86">Improved automatic bird identification through decision tree based feature selection and bagging</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,288.82,349.81,162.78,7.86">Working notes of CLEF 2015 conference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,360.77,337.98,7.86;9,151.52,371.73,329.07,7.86;9,151.52,382.69,216.83,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,407.32,360.77,73.27,7.86;9,151.52,371.73,215.91,7.86">Contour matching for a fish recognition and migration-monitoring system</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Schoenberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Shiozawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,387.79,371.73,45.22,7.86">Optics East</title>
		<imprint>
			<publisher>International Society for Optics and Photonics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,393.65,227.29,7.86;9,134.77,404.61,345.83,7.86;9,151.52,415.56,329.07,7.86;9,151.52,426.52,206.48,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="9,213.20,404.61,267.39,7.86;9,151.52,415.56,234.83,7.86">Study of best algorithm combinations for speech processing tasks in machine learning using median vs. mean clusters in marf</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Meza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Espino-Gamez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Solano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Villarreal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Mokhov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,406.95,415.56,73.64,7.86;9,151.52,426.52,104.41,7.86">Proceedings of the 2008 C 3 S 2 E conference</title>
		<meeting>the 2008 C 3 S 2 E conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="29" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,437.48,337.98,7.86;9,151.52,448.44,205.53,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="9,202.02,437.48,274.48,7.86">Birdclef 2015 submission: Unsupervised feature learning from audio</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Stowell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,165.60,448.44,162.78,7.86">Working notes of CLEF 2015 conference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,459.40,337.97,7.86;9,151.52,470.36,329.07,7.86;9,151.52,481.32,93.19,7.86" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="9,289.78,459.40,190.81,7.86;9,151.52,470.36,259.88,7.86">Automatic large-scale classification of bird sounds is strongly improved by unsupervised feature learning</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Stowell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.6524</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,142.62,492.28,337.97,7.86;9,151.52,503.24,207.34,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="9,394.42,492.28,86.17,7.86;9,151.52,503.24,60.09,7.86">A toolbox for animal call recognition</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Towsey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Planitz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nantes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wimmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Roe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,218.61,503.24,49.64,7.86">Bioacoustics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="125" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,514.19,337.97,7.86;9,151.52,525.15,329.07,7.86;9,151.52,536.11,216.90,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="9,371.30,514.19,109.29,7.86;9,151.52,525.15,272.73,7.86">Automated species recognition of antbirds in a mexican rainforest using hidden markov models</title>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">M</forename><surname>Trifa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Kirschel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">E</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">E</forename><surname>Vallejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,431.49,525.15,49.10,7.86;9,151.52,536.11,147.27,7.86">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page">2424</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,547.07,337.98,7.86;9,151.52,558.03,329.07,8.12;9,151.52,569.63,42.37,7.47" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="9,327.04,547.07,153.55,7.86">Taxonomy: Impediment or expedient?</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">D</forename><surname>Wheeler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">H</forename><surname>Raven</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">O</forename><surname>Wilson</surname></persName>
		</author>
		<ptr target="http://www.sciencemag.org/content/303/5656/285.short" />
	</analytic>
	<monogr>
		<title level="j" coord="9,151.52,558.03,29.19,7.86">Science</title>
		<imprint>
			<biblScope unit="volume">303</biblScope>
			<biblScope unit="issue">5656</biblScope>
			<biblScope unit="page">285</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
