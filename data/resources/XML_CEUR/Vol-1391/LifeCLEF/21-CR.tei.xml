<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,150.63,115.96,314.11,12.62;1,260.98,133.89,93.40,12.62">MICA at LifeCLEF 2015: Multi-organ Plant Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,160.45,171.56,47.83,8.74"><forename type="first">Thi-Lan</forename><surname>Le</surname></persName>
							<email>thi-lan.le@mica.edu.vn</email>
						</author>
						<author>
							<persName coords="1,216.08,171.56,82.21,8.74"><forename type="first">Nam-Duong</forename><surname>Duong</surname></persName>
						</author>
						<author>
							<persName coords="1,306.85,171.56,28.51,8.74"><forename type="first">Hai</forename><surname>Vu</surname></persName>
							<email>hai.vu@mica.edu.vn</email>
						</author>
						<author>
							<persName coords="1,363.03,171.56,91.87,8.74"><forename type="first">Thanh-Nhan</forename><surname>Nguyen</surname></persName>
							<email>thanh-nhan.nguyen@mica.edu.vn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">International Research Institute MICA HUST</orgName>
								<orgName type="institution" key="instit1">CNRS/UMI</orgName>
								<orgName type="institution" key="instit2">GRENOBLE</orgName>
								<orgName type="institution" key="instit3">INP</orgName>
								<address>
									<postCode>2954</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Hanoi University of Science and Technology Hanoi</orgName>
								<address>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,150.63,115.96,314.11,12.62;1,260.98,133.89,93.40,12.62">MICA at LifeCLEF 2015: Multi-organ Plant Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D701334D0FFDC5C7CF43E5CB5E346EC8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we describe the system including image preprocessing, feature descriptor extraction, classification method and fusion techniques that we applied for LifeCLEF 2015 -multi-organ plant identification task. In the preprocessing step, we apply relevant preprocessing techniques for each type of plants' organs based on the characteristic of the organs. For the feature descriptor, we propose to use kernel descriptor (KDES) with different types of kernel for all organs types. For flower and entire images, we combine KDES with HSV histogram. At the image level, we apply Support Vector Machine (SVM) as a classification method. Finally, we investigate different late fusion techniques in order to build the retrieved observation list.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Plant identification is a process that aims at matching a given specimen plant to a known taxon. This is a difficult and time consuming task even for the botanist experts. Recently, with the advanced research in computer vision community, a number of works have been proposed for plant identification based on images. However, most of these methods have been dedicated to one sole organ of the plants. The most widely used organs are leaf and flower. Since 2014, with the availability of dataset for multi-organ plant identification, the plant identification moves from image-centered to observation-centered <ref type="bibr" coords="1,361.05,536.57,9.96,8.74" target="#b0">[1]</ref>. The plant identification can be defined as a image retrieval problem with the input is a set of organ images of a query plant and the output is the ranked list of retrieved plants. In this paper, we present our work dedicated to LifeCLEF 2015 -multi-organ plant identification task <ref type="bibr" coords="1,216.48,584.39,9.96,8.74" target="#b1">[2]</ref>. Our previous work for leaf-based plant identification has shown that the KDES is a robust descriptor for leaf representation <ref type="bibr" coords="1,420.24,596.34,9.96,8.74" target="#b2">[3]</ref>. Therefore, in this work, in order to analyze the performance of KDES for others organs, we apply KDES as descriptor at image level for all types of organs. For the fusion techniques, we investigates three fusion techniques: BC (Borda Count), IRP (Inverse Rank Position) and WP (Weighted Probability). Finally, we propose to use IRP and WP in our runs. The remaining of the paper is organized as follows. In Section 2, we present in detail the methods applied in each step of our system including preprocessing, feature extraction and fusion techniques. The experimental results on both validation and testing sets are shown in Section 3. Section 4 gives some conclusions and future works. -Feature extraction: There are two main features extracted from organ images: KDES and HSV histogram. Since HSV histogram is well-known feature, in this paper, we describe only KDES. -Classification: We apply SVM as classification method at image level for all types of organs. -Fusion: we investigates three fusion techniques: BC (Borda Count), IRP (Inverse Rank Position) and WP (Weighted Probability).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview</head><p>Based on this sytem, we have created 3 runs to LifeCLEF 2015.</p><p>-Run 1: In this run, we employ KDES for all types of organs and IRP (Inverse Rank Position) for result fusion at observation level. -Run 2: The difference between Run 1 and Run 2 is that for flower and entire images, we combine HSV histogram with KDES. We also apply IRP for result fusion. -Run 3: This run is similar to Run 3. However, instead of using IRP, we employ WP (Weighted Probability).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Preprocessing Techniques</head><p>The preprocessing techniques aim to separate the regions-of-interest (ROI) from an image. Except the leaf scan images, most of images of the observations are captured from natural scenes. In ImageClef 2014 <ref type="bibr" coords="3,358.81,373.89,9.96,8.74" target="#b0">[1]</ref>, teams utilized a simple threshold method (such as Otsu threshold) in order to separate leaf regions for leaf-scan images. IBM-AU team deployed more complicated techniques (e.g., active contour-based, pre-determined size of ROI) to make a boundary box of flower, fruit, or leaf on complicated background, so on <ref type="bibr" coords="3,366.34,421.71,9.96,8.74" target="#b0">[1]</ref>. In this work, we deploy appropriate preprocessing techniques for each type of images as described below:</p><p>-Leaf on the complicated background: An Interactive segmentation method, which is based on Watershed algorithms as described in <ref type="bibr" coords="3,407.29,465.04,9.96,8.74" target="#b7">[8]</ref>, is applied to segment leaf from background regions; Moreover, size and main direction of the leave are normalized in a normalization procedure based on moment calculations <ref type="bibr" coords="3,206.82,500.90,9.96,8.74" target="#b7">[8]</ref>. Fig. <ref type="figure" coords="3,245.42,500.90,4.98,8.74">2</ref> shows some results of the preprocessing techniques applied on the leaf images with the complex background. Because this is an interactive technique, it requires an user's manipulation. It is time consuming when working with a large number of images. -Leaf-scan images: We adapt a saliency extraction method as described in <ref type="bibr" coords="3,470.07,548.62,10.52,8.74" target="#b3">[4]</ref> and a common segmentation technique (e.g., mean-shift algorithm). An segmented region is selected based on a condition that its corresponding saliency value is large enough. The connected-region techniques then are applied to connect the selected regions. The main flow works are expressed in Fig. <ref type="figure" coords="3,472.84,596.44,3.87,8.74">3</ref>. Because leaf-scan images contain only simple background, we obtain stable results with leaf-scan images. The main reasons are that saliency values of leaf regions are more significant from background ones. -Flower and fruit images: we apply same saliency-segmentation procedure as Leaf-scan image for selecting the ROIs on flower and fruit images; However,  -Stem images: We observe that stem covers almost regions on the captured image. Moreover, we take into account texture of the stem regions, then a simple procedure to select ROIs on the stem image is based on a filter technique. We apply a Hanning window on the stem image. The size and sigma of the Hanning window is pre-determined. We then crop stem regions using the filtered image. The crop procedure utilizes a pre-determined pad which is 15 pixels from image border for both dimensions. Fig. <ref type="figure" coords="5,445.46,523.15,4.98,8.74">5</ref> shows results of the ROI extracted on a stem image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Feature Extraction</head><p>Kernel descriptor (KDES) has been proposed firstly by Liefeng Bo et al. <ref type="bibr" coords="5,454.78,584.39,9.96,8.74" target="#b4">[5]</ref>. In our previous works <ref type="bibr" coords="5,221.78,596.34,9.96,8.74" target="#b7">[8]</ref>, <ref type="bibr" coords="5,238.82,596.34,9.96,8.74" target="#b2">[3]</ref>, KDES has been proved to be robust for leaf representation. In this work, we propose to use KDES for all types of organ images. KDES is extracted from images of the organs after preprocessing through 3 steps: pixel-level feature extraction, patch-level feature extraction and image-level feature extraction. We employ the same process to compute KDES as proposed by Liefeng Bo et al. <ref type="bibr" coords="5,211.21,656.12,9.96,8.74" target="#b5">[6]</ref>, <ref type="bibr" coords="5,228.04,656.12,9.96,8.74" target="#b4">[5]</ref>. However, we make the choice of pixel-level feature for m(z) and orientation θ(z). In <ref type="bibr" coords="6,266.01,506.10,9.96,8.74" target="#b4">[5]</ref>, the orientation θ(z) is defined as follows:</p><formula xml:id="formula_0" coords="6,250.19,527.31,230.40,8.74">θ(z) = [sin(θ(z) cos(θ(z))]<label>(1)</label></formula><p>Local binary patterns (LBP) is computed in the manner shown in Fig. <ref type="figure" coords="6,472.84,548.52,3.87,8.74" target="#fig_4">7</ref>. Each pixel is compared to each of its 8 neighbors. Where the neighborhood pixel's value is greater than the center pixel's value, the resulting LBP value is 1. Otherwise, it is 0. The result is an 8-digit binary number. Let denote the resulting binary 8-dimensional vector at pixel z by b(z), and denote the standard deviation of pixel values in the 3×3 neighborhood around z by s(z). b(z) and s(z) are treated as texture pixel level features. From the pixel-level feature, we extract the patch-level feature by generating patches from images and computing the approximate map based on match kernel. Corresponding to each type of pixellevel, we have a match kernel. After extracting patch-level feature, based on a pyramid structure, we can get the final feature vector for organ image. For organ-based plant classification, we apply multi-class SVM. At the end of this step, from one organ image as query, we have a list of ranked plants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Multi-organ Plant Identification</head><p>In our work, we investigate three different fusion techniques that are BC (Borda Count), IRP (Inverse Rank Position) and WP (Weighted Probability) <ref type="bibr" coords="7,438.91,353.91,9.96,8.74" target="#b6">[7]</ref>. These techniques are explained in the Eq. 2, Eq. 3 and Eq. 4.</p><formula xml:id="formula_1" coords="7,258.58,384.23,222.02,30.32">BC(i) = n j=1 rank(i, j)<label>(2)</label></formula><formula xml:id="formula_2" coords="7,247.32,426.55,233.27,63.23">IRP (i) = 1 n j=1 1 rank(i,j) (3) W P (i) = n j=1 w(j)rank(i, j)<label>(4)</label></formula><p>where n is the number of retrieved lists, rank(i, j) is rank of species i in list j th and w(j) is the weight of list j th 3 Experimental and Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Preparation</head><p>The training data of this year finally results in 27,907 plant-observations illustrated by 91,759 images while the test data results in 13,887 plant-observationqueries illustrated by 21,446 images <ref type="bibr" coords="7,292.00,608.30,9.96,8.74" target="#b1">[2]</ref>. The number of images for each organ in training and testing set is shown in Tab.1. We apply the proposed preprocessing techniques as described in Sec. 2.2 in organ images. In order to evaluate our system, we divide the training set into training and validation sets by taking randomly 1/5 observation for validation and the remaining for training. For the leaf image (with complex background), since we use interactive segmentation and this is time consuming, therefore, in order to reduce the work, if the leaf scan set contains images of the leaf of one plant, instead of using images from leaf training set, we use images from leaf scan training set for this plant. The number of images for each organ in our training and validation set is shown in Tab. 2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Working Environment</head><p>We implement our proposed system in C++ and Matlab and use two libraries: OpenCV and KDES (http://www.cs.washington.edu/robotics/projects/kdes/). In order to evaluate the performance of our system on validation sets, we implement the score image and score observation as described in <ref type="bibr" coords="8,396.12,457.36,9.96,8.74" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Obtained Results</head><p>This section presents the results on both datasets: validation and testing set.</p><p>For the testing set, we use the results released by the organizers of the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on Validation Set</head><p>We performs two experiments. In the first experiment, we use KDES for all organs (KDES with LBP kernel for stem and KDES with gradient kernel for the others). In the second experiment, for flower and entire images, we combine HSV histogram with KDES by using IRP as fusion technique as organ level.</p><p>The results at image level and observation level of two experiments are shown in Tab. 3, Tab. 4 and Tab. 5 and Tab. 6 respectively.</p><p>Besides the score image, we compute the accuracy at rank k.</p><formula xml:id="formula_3" coords="8,274.60,647.53,201.75,22.31">Accuracy = T N (<label>5</label></formula><formula xml:id="formula_4" coords="8,476.35,654.27,4.24,8.74">)</formula><p>where T is the true recognition and N is the number of queries. One image or observation is correctly recognized if the relevant plant is in the k first plants of the retrieved list. In our experiments, we compute accuracy at rank 1 and rank 10. From the results obtained with two experiments, we extract three observation. Firstly, the obtained results have shown that KDES is a good descriptor for Leaf and Leaf Scan images. The score at image level for Leaf Scan is 62.88 % and the accuracy at the rank 1 is 78.28%. The performance of KDES is reduced when applying for Leaf images. The reason is that in Leaf image set, there  are a number of compound leaf and we donot apply any specific technique for compound leaf. The KDES is not a good choice for the others types of organ. This shows that KDES is relatively good and distinctive feature for classify the classes with high intra similarity such as leaf. Secondly, the combination of HSV histogram and KDES improves slightly the performance for Flower and Entire images (from 20.63% to 22.55% for Flower and from 10.36% to 11.3% for Entire). This shows that the global feature such as histogram can not help to identify the plants by using Flower and Entire images. More robust and local features need to be investigated. Finally, we can see the performance of different fusion techniques. It shows that IRP and WP obtain better results than BC in both experiments. Based on the results of two experiments in validation dataset, we decide to submit three runs to LifeClef 2015 -multi-organ plant identification task. The characteristic of each run is described as follows:</p><p>-Run 1: In this run, we employ KDES for all types of organs and IRP (Inverse Rank Position) for result fusion at observation level. -Run 2: The difference between Run 1 and Run 2 is that for flower and entire images, we combine HSV histogram with KDES by using IRP. We also apply IRP for observation result fusion. -Run 3: This run is similar to Run 3. However, instead of using IRP, we employ WP (Weighted Probability).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on Test Set</head><p>The score image and score observation of our three runs on test set is shown in Fig. <ref type="figure" coords="11,285.71,482.81,4.98,8.74" target="#fig_5">8</ref> while the score for each type of organs is illustrated in Fig. <ref type="figure" coords="11,217.88,494.76,3.87,8.74" target="#fig_6">9</ref>. Our team is ranked at 5th place. We can see that Run 2 is slightly better than Run 1 and Run 3. This is consistent with the results obtained in the validation set. From the detailed score for each type of organs, we can see that KDES is relatively good in comparison with other descriptors used by others labs/teams. Our method for Leaf Scan obtains the second place. The score obtained for Leaf Scan is 0.737 while the score of the first place team is 0.766.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,197.27,440.91,220.81,7.89;2,134.77,250.35,345.93,175.79"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of our three runs for LifeCLEF 2015.</figDesc><graphic coords="2,134.77,250.35,345.93,175.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,134.77,374.84,345.82,7.89;4,134.77,385.83,345.83,7.86;4,170.35,156.52,275.43,203.18"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig. 2. Results of the segmented leaf from complex background. Upper row: original images; Lower row: corresponding the segmented leaf results using proposed techniques</figDesc><graphic coords="4,170.35,156.52,275.43,203.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,181.12,417.77,253.11,7.89;5,153.18,210.97,309.86,190.88"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The results of selected ROIs on flower and fruit images</figDesc><graphic coords="5,153.18,210.97,309.86,190.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,134.77,235.44,345.83,7.89;6,134.77,246.42,345.82,7.86;6,134.77,257.38,23.55,7.86;6,221.57,116.04,171.71,103.10"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Fig. 5. The result of selected ROIs on stem image. Left: original stem image; Right: Filtered image using a Hanning window. ROI is marked in yellow box on the filtered image</figDesc><graphic coords="6,221.57,116.04,171.71,103.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,234.17,225.41,147.01,7.89;7,182.65,116.28,250.70,93.60"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Local binary patterns (LBP)</figDesc><graphic coords="7,182.65,116.28,250.70,93.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="10,157.35,424.68,300.66,7.89;10,134.77,199.68,345.73,210.23"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Score image and score observation of our three runs on test set [2].</figDesc><graphic coords="10,134.77,199.68,345.73,210.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="11,134.77,350.25,345.83,7.89;11,134.77,361.23,26.67,7.86;11,134.77,116.08,345.73,219.40"><head>Fig. 9 .</head><label>9</label><figDesc>Fig.9. Detailed scores obtained for each type of plant organs of our three runs on test set.<ref type="bibr" coords="11,151.71,361.23,9.73,7.86" target="#b1">[2]</ref> </figDesc><graphic coords="11,134.77,116.08,345.73,219.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,163.16,207.75,285.96,51.38"><head>Table 1 .</head><label>1</label><figDesc>Training and testing sets provided by LifeCLEF2015</figDesc><table coords="8,163.16,228.53,285.96,30.60"><row><cell cols="4">Leaf Leaf scan Flower Fruit Stem Branch Entire</cell></row><row><cell cols="2">Training Set 13,367 12,605</cell><cell cols="2">28,225 7,720 5,476 8,130 16,235</cell></row><row><cell>Testing Set 2,690</cell><cell>221</cell><cell>8,327 1,423 584</cell><cell>2,088 6,113</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,158.92,305.36,294.44,51.38"><head>Table 2 .</head><label>2</label><figDesc>Training and validation sets used in our experiments</figDesc><table coords="8,158.92,326.13,294.44,30.61"><row><cell cols="4">Leaf Leaf scan Flower Fruit Stem Branch Entire</cell></row><row><cell>Training Set 15,220</cell><cell>9,787</cell><cell cols="2">22,945 6,356 4,485 6,542 13,031</cell></row><row><cell>Validation Set 1,814</cell><cell>2,610</cell><cell>5,280 1,364 994</cell><cell>1,588 3,204</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,161.80,184.15,291.75,119.12"><head>Table 3 .</head><label>3</label><figDesc>Results at image level of the first experiment on validation set</figDesc><table coords="9,210.88,204.92,190.52,98.35"><row><cell></cell><cell cols="3">Score (Image) Accuracy(%)</cell></row><row><cell></cell><cell></cell><cell cols="2">Rank 1 Rank 10</cell></row><row><cell>Leaf</cell><cell>32.90</cell><cell>24.26</cell><cell>46.36</cell></row><row><cell>Leaf Scan</cell><cell>62.88</cell><cell>78.28</cell><cell>92.76</cell></row><row><cell>Flower</cell><cell>20.63</cell><cell>10.95</cell><cell>24.62</cell></row><row><cell>Fruit</cell><cell>13.96</cell><cell>6.52</cell><cell>20.46</cell></row><row><cell>Stem</cell><cell>13.16</cell><cell>16.60</cell><cell>34.20</cell></row><row><cell>Branch</cell><cell>7.18</cell><cell>3.53</cell><cell>9.70</cell></row><row><cell>Entire</cell><cell>10.36</cell><cell>6.40</cell><cell>14.61</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,150.63,350.10,314.10,62.73"><head>Table 4 .</head><label>4</label><figDesc>Results at observation level of the first experiment on validation set</figDesc><table coords="9,225.46,370.87,161.36,41.96"><row><cell></cell><cell>BC IRP WP</cell></row><row><cell cols="2">Score (observation) 21.86 23.31 22.22</cell></row><row><cell>Rank 1</cell><cell>22.49 24.22 22.96</cell></row><row><cell>Rank 10</cell><cell>37.80 39.28 38.84</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,156.44,459.67,302.49,119.12"><head>Table 5 .</head><label>5</label><figDesc>Results at image level of the second experiment on validation set</figDesc><table coords="9,210.88,480.44,190.52,98.35"><row><cell></cell><cell cols="3">Score (Image) Accuracy(%)</cell></row><row><cell></cell><cell></cell><cell cols="2">Rank 1 Rank 10</cell></row><row><cell>Leaf</cell><cell>32.90</cell><cell>24.26</cell><cell>46.36</cell></row><row><cell>Leaf Scan</cell><cell>62.88</cell><cell>78.28</cell><cell>92.76</cell></row><row><cell>Flower</cell><cell>22.55</cell><cell>11.38</cell><cell>38.05</cell></row><row><cell>Fruit</cell><cell>13.96</cell><cell>6.52</cell><cell>20.46</cell></row><row><cell>Stem</cell><cell>13.16</cell><cell>16.60</cell><cell>34.20</cell></row><row><cell>Branch</cell><cell>7.18</cell><cell>3.53</cell><cell>9.70</cell></row><row><cell>Entire</cell><cell>11.30</cell><cell>6.62</cell><cell>17.51</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="10,145.27,115.91,324.83,62.74"><head>Table 6 .</head><label>6</label><figDesc>Results at observation level of the second experiment on validation set</figDesc><table coords="10,225.46,136.68,161.36,41.96"><row><cell></cell><cell>BC IRP WP</cell></row><row><cell cols="2">Score (observation) 21.75 23.27 22.36</cell></row><row><cell>Rank 1</cell><cell>22.31 23.29 22.83</cell></row><row><cell>Rank 10</cell><cell>38.75 39.51 39.95</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="4">Conclusions and Future works</head><p>In this paper, we have presented our proposed system for multi-organ plant identification. We have described in detail the proposed system and analyzed the results obtained on both validation and testing set. The obtained results with KDES for Leaf Scan are promising. However, the results are still limited for the others types of organs and multi-organ plant identification. In the future, we focus on compound leaf and descriptors for the other types of organs.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="12,138.35,186.42,342.24,7.86;12,146.91,197.38,333.68,7.86;12,146.91,208.34,333.68,7.86;12,146.91,219.30,164.59,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,414.18,186.42,66.41,7.86;12,146.91,197.38,74.82,7.86">LifeCLEF Plant Identification Task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Selmi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J-F</forename><surname>Molino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,437.46,197.38,43.13,7.86;12,146.91,208.34,227.67,7.86">CLEF2014 Working Notes. Working Notes for CLEF 2014 Conference</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Halvey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">;</forename><forename type="middle">W</forename><surname>Kraaij</surname></persName>
		</editor>
		<meeting><address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<publisher>CEUR-WS</publisher>
			<date type="published" when="2014-09-15">2014. September 15-18, 2014. 2014</date>
			<biblScope unit="page" from="598" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.35,230.26,342.24,7.86;12,146.91,241.22,108.61,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,280.55,230.26,146.24,7.86">LifeCLEF Plant Identification Task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Pierre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,455.89,230.26,24.70,7.86;12,146.91,241.22,75.75,7.86">CLEF working notes 2015</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.35,252.18,342.24,7.86;12,146.91,263.14,333.68,7.86;12,146.91,274.09,333.68,7.86;12,146.91,285.05,333.68,7.86;12,146.91,296.01,333.09,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,146.91,263.14,333.68,7.86;12,146.91,274.09,140.92,7.86">Complex background leaf-based plant identification method based on interactive segmentation and kernel descriptor</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">L</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">D</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">N</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">N</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,306.96,274.09,173.63,7.86;12,146.91,285.05,263.96,7.86;12,431.19,285.05,49.41,7.86;12,146.91,296.01,224.68,7.86">Proceeding of the 2nd International Workshop on Environmental Multimedia Retrieval 2015 (EMR 2015)</title>
		<meeting>eeding of the 2nd International Workshop on Environmental Multimedia Retrieval 2015 (EMR 2015)<address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Conjunction with ACM Conference on Multimedia Retrieval (ICMR)</note>
</biblStruct>

<biblStruct coords="12,138.35,306.97,342.24,7.86;12,146.91,317.93,333.68,7.86;12,146.91,328.89,110.58,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,369.32,306.97,111.28,7.86;12,146.91,317.93,55.74,7.86">Frequency-tuned salient region detection</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,221.78,317.93,258.82,7.86;12,146.91,328.89,31.16,7.86">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.35,339.85,342.24,7.86;12,146.91,350.81,297.50,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,249.69,339.85,167.06,7.86">Kernel Descriptors for Visual Recognition</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,435.53,339.85,45.06,7.86;12,146.91,350.81,248.99,7.86">the Annual Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.35,361.77,342.24,7.86;12,146.91,372.72,333.68,7.86;12,146.91,383.68,77.94,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,248.40,361.77,232.19,7.86;12,146.91,372.72,46.11,7.86">Efficient Match Kernel between Sets of Features for Visual Recognition</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,212.17,372.72,268.43,7.86;12,146.91,383.68,26.88,7.86">the Annual Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.35,394.64,342.24,7.86;12,146.91,405.60,207.88,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,184.82,394.64,295.77,7.86;12,146.91,405.60,51.16,7.86">Image Retrieval Based on Similarity Score Introduction : Image Similarity Computation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jovi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,205.79,405.60,79.80,7.86">Pattern Recognition</title>
		<imprint>
			<biblScope unit="page" from="461" to="470" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.35,416.56,342.24,7.86;12,146.91,427.52,333.68,7.86;12,146.91,438.48,333.68,7.86;12,146.91,449.44,25.60,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,311.16,416.56,169.43,7.86;12,146.91,427.52,232.74,7.86">Fully automatic leaf-based plant identification, application for Vietnamese medicinal plant search</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">L</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">N</forename><surname>Hoang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,399.00,427.52,81.59,7.86;12,146.91,438.48,256.15,7.86">the fifth symposium on Information and Communication Technology (SoICT 2014)</title>
		<meeting><address><addrLine>Hanoi, VietNam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
