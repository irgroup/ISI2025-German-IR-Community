<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,151.34,152.67,292.50,12.64;1,144.74,170.67,305.72,12.64">Improved Automatic Bird Identification through Decision Tree based Feature Selection and Bagging</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,268.01,230.22,59.04,8.96"><forename type="first">Mario</forename><surname>Lasseck</surname></persName>
							<email>mario.lasseck@mfn-berlin.de</email>
							<affiliation key="aff0">
								<orgName type="department">Animal Sound Archive Museum für Naturkunde</orgName>
								<address>
									<settlement>Berlin</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,151.34,152.67,292.50,12.64;1,144.74,170.67,305.72,12.64">Improved Automatic Bird Identification through Decision Tree based Feature Selection and Bagging</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A11A6D43991E86894D766F254C6D75B8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bird Identification</term>
					<term>Content Based Information Retrieval</term>
					<term>Template Matching</term>
					<term>Decision Tree based Feature Selection</term>
					<term>Bagging</term>
					<term>Blending</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a machine learning technique for bird species identification at large scale. It automatically identifies about a thousand different species in a large number of audio recordings and provides the basis for the winning solution to the LifeCLEF 2015 Bird Identification Task. To process the very large amounts of audio data and to achieve similar good results compared to previous identification challenges new methods e.g. downsampling of spectrogram images for faster feature extraction, advanced feature selection via decision tree based feature ranking and bootstrap aggregating using averaging and blending were tested and evaluated.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic identification of species from their sound can be a very useful computational tool for assessing biodiversity with many potential applications in ecology, bioacoustic monitoring and behavioral science <ref type="bibr" coords="1,316.65,522.59,10.84,8.96" target="#b0">[1]</ref>. Some examples of previous studies on species identification, especially of birds, are given in [2, <ref type="bibr" coords="1,386.44,534.59,7.37,8.96" target="#b1">3,</ref><ref type="bibr" coords="1,393.81,534.59,7.37,8.96" target="#b2">4,</ref><ref type="bibr" coords="1,401.18,534.59,7.37,8.96" target="#b3">5]</ref>. The approach towards automatic species identification presented here is a further development of ideas and methods already successfully applied in previous challenges. The NIPS4B 2013 Multi-label Bird Species Classification Challenge [6] hosted by Kaggle for example asked participants to identify 87 sound classes (songs, calls and instrumental sounds) of more than 60 different species in a large number of wildlife recordings. Last year the LifeClef2014 Bird Identification Task [7] challenged participants to identify 501 different species in almost 5000 audio recordings. This year the number of training and test files as well as the number of species to identify was increased once again. With almost a thousand species and over 33,000 audio files it is the largest computer-based bird identification challenge so far. A detailed description of the task, dataset and experimentation protocol can be found in <ref type="bibr" coords="1,380.14,666.62,10.76,8.96" target="#b4">[8]</ref>, <ref type="bibr" coords="1,399.19,666.62,15.34,8.96">[20]</ref>. The task is among others part of the LifeCLEF 2015 evaluation campaign <ref type="bibr" coords="1,376.19,678.62,11.93,8.96" target="#b5">[9,</ref><ref type="bibr" coords="1,388.12,678.62,11.93,8.96" target="#b6">10,</ref><ref type="bibr" coords="1,400.05,678.62,11.93,8.96" target="#b7">11]</ref>.</p><p>The features used for classification and methods to speed up feature extraction are introduced in section 2. In section 3 a two-pass training approach is proposed including feature ranking and selection. Bagging is used to improve classification results and different methods for aggregating model predictions are compared. Finally, submission results are evaluated in section 4 and briefly discussed in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Feature Engineering</head><p>There are two main categories of features used for classification: parametric acoustic features (see openSMILE Audio Statistics) and probabilities of species-specific spectrogram segments (see Segment-Probabilities). The feature sets are briefly described in the following sections. Similar features have been already successfully used in previous identification challenges and additional details can be found in <ref type="bibr" coords="2,414.53,303.21,16.06,8.96" target="#b8">[12,</ref><ref type="bibr" coords="2,430.59,303.21,12.04,8.96" target="#b9">13,</ref><ref type="bibr" coords="2,442.63,303.21,12.04,8.96" target="#b10">14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">openSMILE Audio Statistics</head><p>For each audio file a large number of acoustic features were extracted using the openSMILE Feature Extractor Tool <ref type="bibr" coords="2,270.92,365.25,15.49,8.96" target="#b11">[15]</ref>. The configuration file emo_large.conf, originally designed by Florian Eyben for emotion detection in human speech, was modified in several ways to better capture the characteristics of bird sounds. The changes relate primarily to the frame-wise calculated low-level descriptors (LLDs). For example the maximum frequency for pitch and Mel-spectrum was set to 11 kHz (instead of 500 Hz and 8 kHz). Also, the number of Mel Frequency Cepstral Coefficients (MFCC) was increased as well as the number of frequency bands for energy calculations. Furthermore, pitch-and spectral-related LLDs were added e.g. harmonics-tonoise ratio, raw F0, spectral skewness, kurtosis, entropy, variance and slope.</p><p>The all in all 73 LLDs consist of: </p><formula xml:id="formula_0" coords="2,124.70,505.33,4.58,9.05"></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Segment-Probabilities</head><p>The second source used for features are Segment-Probabilities (SegProbs). For each species a set of representative segments also referred to as region of interests (ROIs) or templates was extracted from spectrogram images representing the acoustic content of audio files. These segments were then used to calculate Segment-Probabilities for each target file by finding the maxima of the normalized cross-correlation <ref type="bibr" coords="3,437.23,457.65,16.79,8.96" target="#b12">[16]</ref> between all segments and the target spectrogram image via template matching. A more detailed description regarding preprocessing, segmentation of spectrogram images and extraction of Segment-Probabilities can be found in <ref type="bibr" coords="3,350.54,493.67,16.93,8.96" target="#b8">[12]</ref> and <ref type="bibr" coords="3,386.91,493.67,15.47,8.96" target="#b10">[14]</ref>.</p><p>Due to the very large amount of audio data, not all files belonging to a certain species were used as a source for segmentation. In a first session only short, good quality files (metadata: Quality = 1) without background species were selected for segment extraction. If the number of segments was smaller than a given threshold another file belonging to the same species was selected and so on. To ensure diversity and to capture the entire sound repertoire of a given species each file was chosen to belong to a different bird individual. To keep track of individuals an Individual-ID was assigned to each audio file. Two audio files of the same species were assigned the same Individual-ID if, according to the metadata, they were recorded by the same author on the same day. Individual-IDs were also used to accomplish a somewhat individualindependent species classification by creating "individual-independent" folds for cross-validation during training.</p><p>In the first session 262,232 segments were extracted from 2027 audio files of the training set with an average of 262 segments and 2 files (individuals) per species.</p><p>Fast Template Matching through prior Downsampling. When starting the template matching to collect Segment-Probabilities as described in <ref type="bibr" coords="4,379.92,198.18,16.82,8.96" target="#b8">[12]</ref> it quickly became apparent that sliding 262,232 templates over the spectrogram representation of all audio recordings was too time consuming (33,203 files in total). Even with modifications described in <ref type="bibr" coords="4,199.37,234.18,16.88,8.96" target="#b10">[14]</ref> it would have taken too long. Both methods apply a Gaussian blur on segments and target image before the actual template matching. This smoothing is a form of low-pass filtering to reduce detail. Interestingly, Gaussian smoothing is also used when reducing the size of an image. Before downsampling an image, it is common to apply a low-pass filter to ensure that spurious high-frequency information does not appear in the resampled image (aliasing). So if high-frequency information is discarded anyway why not apply a Gaussian blur and then downsample both template and target image by a factor 2 prior to the template matching? Together with other speed-related optimizations introduced in [14] (e.g. short-time Fourier transform (STFT) with only 50% overlap, restricting the template matching to a few pixels above and below the original vertical segment position along the frequency axes) this preprocessing reduces calculation time significantly while maintaining comparable results in finding maxima of segments within spectrograms. Proportions of spectrogram images and effects of low-pass filtering are visualized in Fig. <ref type="figure" coords="4,397.75,390.21,4.98,8.96" target="#fig_0">1</ref> for an audio file of about 8 seconds taken from the training set (MediaId: 83). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training, Feature Selection and Classification</head><p>Like in previous challenges classification was split up into several independent classification tasks by training one classifier per species (999 classes) following the binary relevance approach. For each species the classification problem was formulated as a multi-label regression task with the target function set to 1.0 for dominant species and 0.5 for all background species. For classification the scikit-learn library [17] was used (ExtraTreesRegressor) by training ensembles of randomized decision trees <ref type="bibr" coords="5,433.12,237.18,17.06,8.96" target="#b14">[18]</ref> with probabilistic outputs. Hyperparameter grid search was performed to improve classification results. For each classifier the following parameters and variations of treespecific hyperparameters were used during training:</p><p> number of "individual-independent" folds for cross-validation: 10  number of estimators (trees in the forest): 500  number of features to consider when looking for the best split: 10, 30  minimum number of samples required to split an internal node: 5, 10</p><p>Best hyperparameters were chosen separately for each species by evaluating the Area Under the Curve (AUC) on predictions of held-out training files. To have a more realistic estimation and to improve generalization, Individual-IDs were used to create "individual-independent" folds for cross-validation. This way, recordings of the same bird were either part of the training or the validation set but not both. For the best runs the probability of occurrence for that species was predicted in all test files and averaged during cross-validation.</p><p>Decision Tree based Feature Ranking and Selection. For both feature sets (SegProbs1 &amp; openSMILE) training was performed in two passes. During the first pass feature importances returned by the classifier were cumulated for each species during hyperparameter variation and saved for later feature ranking. The importance of a feature was computed as the total reduction of the mean squared error brought by that feature. During the second pass classifiers were trained again, but this time with only a limited number of features, starting with the most important ones. To determine the optimal number of SegProbs1 and openSMILE features for each species features were added in decreasing order of importance. For SegProbs1 the number of selected features considered for training was 10, 50, 100, 150 and 500 and for openSMILE 50, 100, 150, 500, 3000 and 8541 (no feature selection). In Fig. <ref type="figure" coords="5,449.50,577.31,4.98,8.96" target="#fig_1">2</ref> the frequency distribution regarding the optimal number of selected features (= best AUC per species) is given as bar chart for both feature sets. For Segment-Probabilities this means using the 50 most important features only is most likely better than using the most important 100, 150 or 500 features to identify a species. For openSMILE using 500 important features on average is better than using 3000 or all of them.  Again, only files belonging to individuals not processed before were chosen for further segmention. Segment-Probabilities calculated for last year's challenge were also reused as features (SegProbsOld). Properties of the different sets are summarized in Table <ref type="table" coords="7,160.31,186.18,3.78,8.96" target="#tab_1">1</ref>. To combine predictions of the different models two methods were tested: simple averaging and blending (also known as stacking or stacked generalization <ref type="bibr" coords="8,403.99,334.65,15.09,8.96" target="#b15">[19]</ref>). In case of blending the outputs of different classifiers were used as training data for another classifier to approximate the same target function. For this second level classifier an ordinary least squares linear regression model was trained to figure out the combining mechanism or weighting of the individual predictions.</p><p>Post-processing of Predictions for Submission. The predictions returned by the classifiers assign a score to each species within each audio file (probability of occurrence as real value). After blending some prediction values were not within the requested interval [0,1]. To deal with this, all negative values were clipped to zero. Additionally, all values greater 0.6 were replaced using a hyperbolic tangent function (tanh). By passing predictions to this non-linear transfer function, values were all kept below 1.0. This way ranking was preserved especially among the top ranks that are most important when evaluating the Mean Average Precision (MAP). In a final step predictions of species not part of last year's challenge were set to zero for all files marked with Year = 'BirdCLEF2014'. Figures <ref type="figure" coords="8,317.35,514.67,4.98,8.96">4</ref> and<ref type="figure" coords="8,343.03,514.67,4.98,8.96">5</ref> show the progress of classification results using simple averaging compared to blending for stepwise aggregating predictions from and within bootstrap data sets followed by post-processing. Results are presented via AUC statistics (summarized for all species as boxplots) and MAP statistics (evaluated over the entire training set) within the same figure. It is worth mentioning that AUC and MAP statistics are not perfectly correlated. Although averaging leads to better AUC statistics, blending yields much better MAP scores. Also, setting predictions of the 499 species new in 2015 to zero during postprocessing for BirdCLEF2014 files increases MAP for both averaging and blending if background species are included for evaluation, whereas AUC statistics are getting worse in both cases (far more outliers below 1.5×IQR). This can be explained by the fact that those species are not among the dominant species in BirdCLEF2014 files but they do may appear as background species.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Submission Results</head><p>In Table <ref type="table" coords="9,162.81,659.42,4.98,8.96" target="#tab_4">3</ref>  For the first run only SegProbs1 and openSMILE features were used for classification. Different models were trained on the entire training set with and without prior feature selection. The predictions of the models were than combined via blending followed by post-processing as described above. For the second run additional classifiers were trained on smaller subsets of the training files (C2_2014, C3_2015 and C4_Old). For each subset a distinct set of additional features (SegProbs2, SegProbs3 and SegProbs-Old) was used for training. For this run features from one subset were not part of any other subset. Again, feature selection was performed individually for each species and each bootstrap data set. Predictions of all subset models including the ones from the first run were aggregated via blending and post-processed. For the third and fourth run additional models were trained for each bootstrap data set including also selected features from other subsets. The difference however between these two runs is that averaging was used for run three whereas blending was used for the final and best performing fourth run to aggregate model predictions. In Fig. <ref type="figure" coords="10,375.85,470.75,4.98,8.96" target="#fig_5">6</ref> results for all submitted runs are visualized as combination of AUC boxplots and MAP scores. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Downsampling spectrogram images prior to template matching significantly reduces calculation time. Luckily, the here occurring loss of information can actually be considered as a feature, not a bug. Too much detail is rather disturbing and distracting when comparing call or song elements between different individuals of the same species. For some birds maybe even a downsampling factor greater than 2 produces equal or even better results? Faster template matching meansespecially when dealing with so many species and such large amounts of audio databeing able to extract more segments and calculate more Segment-Probabilities. But using more features does not necessarily lead to better identification results. Reducing the number of features to the most important ones has several advantages. Less but meaningful features improve prediction accuracy, reduce over-fitting and therefore increase generalization power. Besides faster training and prediction, a smaller set of features also reduces, once the model is trained, prediction time of new and unseen audio material due to faster feature extraction. Especially in case of Segment-Probabilities from now on only relevant segments need to be considered for template matching. Due to the large amount of data and limited training time the searching for the optimal number of features per species was not very extensive. Only a few manual selected candidates were evaluated. A finer grid search or a more sophisticated way to approach the true optimum, for example using a binary search algorithm, might have led to better results. When looking at the different MAP scores for training and test files in Fig. <ref type="figure" coords="11,440.35,417.21,4.98,8.96" target="#fig_5">6</ref> it becomes clear that most of the progress achieved by bagging on the training set is due to over-fitting. This could be partly explained by the fact that only for the first dataset C1_All an "individual-independent" training approach with accordingly selected folds was used whereas for all other subsets common stratified folds were used for crossvalidation. Another reason might be that all bootstrap data sets used similar features and equal classification methods and therefore model predictions were not independent or uncorrelated enough to significantly boost classification results when combining them. Nevertheless bagging increased average prediction scores also for test files and could clearly improve submission results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,128.06,635.20,339.10,8.10;4,131.78,646.24,331.63,8.10;4,186.14,657.19,222.89,8.10;4,124.70,435.40,345.90,191.10"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) Original spectrogram image: STFT with 75% overlap, (b) STFT with 50% overlap, (c) Image for template matching: Downsampling of (b) by factor 2, (d) Loss of information: Expansion of (c) to the size of the original spectrogram image</figDesc><graphic coords="4,124.70,435.40,345.90,191.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,130.34,277.94,334.51,8.10;6,185.90,288.86,223.36,8.10;6,125.40,147.40,170.59,121.85"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Absolute frequency distribution regarding the optimal number of selected features for left: Segment-Probabilities and right: openSMILE feature set</figDesc><graphic coords="6,125.40,147.40,170.59,121.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,208.85,360.09,261.70,8.96;6,124.70,372.09,345.89,8.96;6,124.70,384.09,345.81,8.96;6,124.70,396.09,345.65,8.96;6,124.70,408.09,201.22,8.96"><head>Figure 3</head><label>3</label><figDesc>gives an impression to what extent the above described feature selection method improves classification results over the entire training set. Each column in the boxplot summarizes the best possible cross-validated AUC score achieved for each species during hyperparameter grid search and the dotted line shows the improvement regarding the mean AUC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,129.74,582.28,335.66,8.10;6,141.02,593.32,313.31,8.10;6,125.40,441.25,170.33,132.35"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. AUC statistics: without (w/o FS) vs. with (w/ FS) feature selection for SegProbs1 and openSMILE features, left: without background species, right: with background species</figDesc><graphic coords="6,125.40,441.25,170.33,132.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="9,151.70,288.14,291.86,8.10;9,250.37,299.06,94.53,8.10;9,125.40,147.40,170.30,132.05"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. Progress of classification results: Aggregating predictions via Averaging, left: w/o BS, right: w/ BS</figDesc><graphic coords="9,125.40,147.40,170.30,132.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="10,157.82,670.15,279.48,8.10;10,125.52,521.90,170.30,132.90"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. AUC and MAP statistics of submitted runs, left: w/o BS, right: w/ BS</figDesc><graphic coords="10,125.52,521.90,170.30,132.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,124.70,209.99,346.00,196.26"><head>Table 1 .</head><label>1</label><figDesc>Properties of Segment-Probabilities feature sets (total number of features, average number of features per species, number of species covered) For all sets feature selection based on prior feature ranking was performed as described above. Finally, four data sets (C1_All, C2_2014, C3_2015 and C4_Old) were created for each species by combining different feature sets. Those subsets can be interpreted as bootstrap data sets where rows represent a subsampling of the training files and columns a subsampling of the feature space. The number of training files, test files and species associated with each data set are listed in the table below.</figDesc><table coords="7,124.70,239.63,345.97,70.08"><row><cell>Feature set</cell><cell># Features (Segments)</cell><cell># Features per Species (avg)</cell><cell># Species</cell></row><row><cell>SegProbs1</cell><cell>262,232</cell><cell>262</cell><cell>999</cell></row><row><cell>SegProbs2</cell><cell>224,852</cell><cell>557</cell><cell>404</cell></row><row><cell>SegProbs3</cell><cell>245,003</cell><cell>620</cell><cell>395</cell></row><row><cell>SegProbsOld</cell><cell>492,753</cell><cell>985</cell><cell>500</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,124.70,421.10,346.12,268.28"><head>Table 2 .</head><label>2</label><figDesc>Number of audio files and number species used in different bootstrap data setsThe bootstrap data sets were used to train somewhat independent predictive models. The predictions of those models were than combined which is also known as bootstrap aggregating or bagging. In bootstrap aggregating the subsets are usually randomly drawn but here the data sets were chosen from a pragmatic point of view with regards to BirdCLEF 2014 vs. 2015 data and extraction of additional features for species with classification results below a certain threshold. Besides faster training, this ensemble method helps to reduce variance and improves stability. For all bootstrap data sets different models were trained using different feature sets and feature set combinations:</figDesc><table coords="7,124.70,440.48,345.97,248.90"><row><cell></cell><cell></cell><cell cols="2">BirdCLEF2014</cell><cell></cell><cell cols="2">BirdCLEF2015</cell></row><row><cell>Data set</cell><cell cols="2">#Train Files #Test Files</cell><cell># Species</cell><cell cols="2">#Train Files #Test Files</cell><cell># Species</cell></row><row><cell>C1_All</cell><cell>9596</cell><cell>4299</cell><cell>500</cell><cell>15011</cell><cell>4297</cell><cell>499</cell></row><row><cell>C2_2014</cell><cell>7919</cell><cell>4299</cell><cell>404</cell><cell>3956</cell><cell>4297</cell><cell>-</cell></row><row><cell>C3_2015</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>8604</cell><cell>4297</cell><cell>395</cell></row><row><cell>C4_Old</cell><cell>9596</cell><cell>4299</cell><cell>500</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2"> Data subset 1: C1_All</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">o openSMILE without and with prior feature selection (w/o &amp; w/ FS)</cell><cell></cell></row><row><cell cols="3">o SegProbs1 (w/o &amp; w/ FS)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,124.70,310.65,331.49,8.96"><head>Combining the Outputs of Multiple Classifiers via Averaging and Blending.</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,124.70,659.42,345.92,20.96"><head>Table 3 .</head><label>3</label><figDesc>results of submitted runs are summarized using two evaluation statistics: mean of the Area Under the Curve (AUC) calculated per species and mean Average Precision (MAP) on the public training and the private test set. All four runs outperformed the results of the other participating teams[20]. Performance of submitted runs (without / with background species)</figDesc><table coords="10,166.22,215.70,264.97,71.51"><row><cell></cell><cell cols="2">Public Training Set</cell><cell>Private Test Set</cell></row><row><cell>Run</cell><cell>Mean AUC [%]</cell><cell>MAP [%]</cell><cell>MAP [%]</cell></row><row><cell>1</cell><cell>95.2 / 90.0</cell><cell>43.3 / 40.8</cell><cell>42.4 / 38.8</cell></row><row><cell>2</cell><cell>96.6 / 91.3</cell><cell>67.1 / 62.1</cell><cell>44.2 / 40.5</cell></row><row><cell>3</cell><cell>98.1 / 93.5</cell><cell>61.2 / 57.7</cell><cell>44.2 / 41.1</cell></row><row><cell>4</cell><cell>96.7 / 91.6</cell><cell>69.3 / 64.0</cell><cell>45.4 / 41.4</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. I would like to thank <rs type="person">Hervé Glotin</rs>, <rs type="person">Hervé Goëau</rs>, <rs type="person">Andreas Rauber</rs> and <rs type="person">Willem-Pier Vellinga</rs> for organizing this challenge, <rs type="person">Alexis Joly</rs> and <rs type="person">Henning Müller</rs> for coordination, the <rs type="institution">Xeno-Canto</rs> foundation for nature sounds for providing the audio data and the French projects Pl@ntNet (<rs type="institution">INRIA</rs>, <rs type="institution">CIRAD</rs>, <rs type="institution">Tela Botanica)</rs> and <rs type="institution">SABIOD Mastodons</rs> for supporting this task. I also want to thank <rs type="person">Dr. Karl-Heinz Frommolt</rs> for supporting my work and providing me with access to resources of the Animal Sound Archive [21].</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="12,127.03,173.99,343.51,8.10;12,136.10,185.03,330.32,8.10;12,123.62,196.07,347.24,8.10;12,136.10,206.99,289.06,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,320.11,173.99,150.43,8.10;12,136.10,185.03,41.60,8.10;12,221.30,196.07,249.55,8.10;12,136.10,206.99,115.06,8.10">Detecting bird sounds in a complex acoustic environment and application to bioacoustic monitoring</title>
		<author>
			<persName coords=""><forename type="first">K-H</forename><surname>Frommolt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bardeli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Clausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,183.46,185.03,282.96,8.10">Proc. of the int. expert meeting on IT-based detection of bioacoustical patterns</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Bardeli</surname></persName>
		</editor>
		<meeting>of the int. expert meeting on IT-based detection of bioacoustical patterns</meeting>
		<imprint>
			<date type="published" when="2008">2008. 2009</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1524" to="1534" />
		</imprint>
	</monogr>
	<note>Computational bioacoustics for assessing biodiversity</note>
</biblStruct>

<biblStruct coords="12,127.03,218.03,343.71,8.10;12,136.10,229.07,330.26,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,216.09,218.03,254.65,8.10;12,136.10,229.07,104.91,8.10">Acoustic classification of multiple simultaneous bird species: A multiinstance multi-label approach</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Briggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,247.22,229.07,178.34,8.10">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="page">4640</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,127.03,239.99,343.61,8.10;12,136.10,251.03,243.21,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,209.24,239.99,261.40,8.10;12,136.10,251.03,16.19,8.10">Automatic Classification of Taxon-Rich Community Recorded in the Wild</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Potamitis</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0096936</idno>
	</analytic>
	<monogr>
		<title level="j" coord="12,158.51,251.03,40.86,8.10">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">96936</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,127.03,262.07,343.61,8.10;12,136.10,272.99,237.02,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,267.29,262.07,203.34,8.10;12,136.10,272.99,184.96,8.10">Automatic large-scale classification of bird sounds is strongly improved by unsupervised feature learning</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Stowell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,326.77,272.99,19.57,8.10">PeerJ</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">488</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,127.03,306.02,343.66,8.10;12,136.10,317.06,130.55,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,339.79,306.02,130.90,8.10;12,136.10,317.06,16.30,8.10">LifeCLEF Bird Identification Task 2015</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rauber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,171.02,317.06,75.35,8.10">CLEF working notes</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,127.03,328.10,343.62,8.10;12,136.10,339.02,203.17,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,323.71,328.10,146.93,8.10;12,136.10,339.02,86.83,8.10">LifeCLEF 2015: multimedia life species identification challenges</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,241.13,339.02,77.78,8.10">Proceedings of CLEF</title>
		<meeting>CLEF</meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,126.64,350.06,343.88,8.10;12,136.10,361.10,334.59,8.10;12,136.10,372.02,108.32,8.10" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,346.71,350.06,119.50,8.10">CLEF 2015 Labs and Workshops</title>
		<ptr target="http://ceur-ws.org/Vol-1391/" />
	</analytic>
	<monogr>
		<title level="s" coord="12,136.10,361.10,234.29,8.10">Notebook Papers. CEUR Workshop Proceedings (CEUR-WS</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">San</forename><surname>Juan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename></persName>
		</editor>
		<idno type="ISSN">1613-0073</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,126.64,383.06,344.10,8.10;12,136.10,394.10,334.76,8.10;12,136.10,405.02,323.89,8.10" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,278.18,383.06,192.56,8.10;12,136.10,394.10,64.98,8.10">Experimental IR meets Multilinguality, Multimodality, and Interaction</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mothe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,206.94,394.10,239.26,8.10">Sixth International Conference of the CLEF Association, CLEF&apos;15</title>
		<title level="s" coord="12,243.84,405.02,68.56,8.10">Proceedings. LNCS</title>
		<meeting><address><addrLine>Toulouse; Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">September 8-11, 2015. 2015</date>
			<biblScope unit="volume">9283</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,126.64,416.06,344.07,8.10;12,136.10,427.10,334.29,8.10;12,136.10,438.02,312.10,8.10" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,210.41,416.06,260.30,8.10;12,136.10,427.10,96.03,8.10">Bird Song Classification in Field Recordings: Winning Solution for NIPS4B 2013 Competition</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,331.78,427.10,138.62,8.10;12,136.10,438.02,111.68,8.10">Proc. of int. symp. Neural Information Scaled for Bioacoustics, sabiod</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Glotin</surname></persName>
		</editor>
		<meeting>of int. symp. Neural Information Scaled for Bioacoustics, sabiod<address><addrLine>, Nevada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-12">2013. dec. 2013</date>
			<biblScope unit="page" from="176" to="181" />
		</imprint>
	</monogr>
	<note>.org/nips4b, joint to NIPS</note>
</biblStruct>

<biblStruct coords="12,126.64,449.06,344.12,8.10;12,136.10,460.10,93.55,8.10" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,204.77,449.06,193.84,8.10">Large-scale identification of birds in audio recordings</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,417.31,449.06,53.45,8.10;12,136.10,460.10,93.55,8.10">Working notes of CLEF 2014 conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,126.64,471.04,344.10,8.10;12,136.10,482.08,129.94,8.10" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,204.53,471.04,266.21,8.10;12,136.10,482.08,13.80,8.10">Towards Automatic Large-Scale Identification of Birds in Audio Recordings</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,167.90,482.08,77.78,8.10">Proceedings of CLEF</title>
		<meeting>CLEF</meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,126.64,493.12,344.12,8.10;12,136.10,504.04,334.47,8.10;12,136.10,515.08,334.50,8.10;12,136.10,526.12,110.95,8.10" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,319.39,493.12,151.37,8.10;12,136.10,504.04,193.84,8.10">Recent Developments in openSMILE, the Munich Open-Source Multimedia Feature Extractor</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<idno type="DOI">10.1145/2502081.2502224</idno>
	</analytic>
	<monogr>
		<title level="m" coord="12,352.27,504.04,113.43,8.10">Proc. ACM Multimedia (MM)</title>
		<meeting>ACM Multimedia (MM)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013-10">2013. October 2013</date>
			<biblScope unit="page" from="835" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,126.64,537.04,298.44,8.10;12,119.06,548.08,11.37,8.10" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,197.45,537.04,125.00,8.10">Fast Normalized Cross-Correlation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,328.45,537.04,96.63,8.10">Industrial Light and Magic</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,136.10,548.08,334.64,8.10;12,136.10,559.12,18.05,8.10" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,235.29,548.08,150.53,8.10">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,393.55,548.08,22.92,8.10">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,126.64,570.04,306.33,8.10" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,215.69,570.04,100.46,8.10">Extremely randomized trees</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Geurts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,321.65,570.04,64.63,8.10">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="42" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,126.64,581.08,273.80,8.10" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,209.45,581.08,80.35,8.10">Stacked generalization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">H</forename><surname>Wolpert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,295.73,581.08,59.69,8.10">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="241" to="259" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,126.64,603.04,227.48,8.10" xml:id="b16">
	<analytic>
		<title/>
		<ptr target="http://www.animalsoundarchive.org" />
	</analytic>
	<monogr>
		<title level="j" coord="12,136.10,603.04,81.79,8.10">Animal Sound Archive</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
