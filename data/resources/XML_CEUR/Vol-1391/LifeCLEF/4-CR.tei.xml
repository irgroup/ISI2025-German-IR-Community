<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,158.30,116.95,298.76,12.62;1,210.87,134.89,193.62,12.62">BirdCLEF 2015 submission: Unsupervised feature learning from audio</title>
				<funder ref="#_RVm95Kz">
					<orgName type="full">EPSRC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,281.46,172.84,52.44,8.74"><forename type="first">Dan</forename><surname>Stowell</surname></persName>
							<email>dan.stowell@qmul.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Digital Music</orgName>
								<orgName type="institution">Queen Mary University of London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,158.30,116.95,298.76,12.62;1,210.87,134.89,193.62,12.62">BirdCLEF 2015 submission: Unsupervised feature learning from audio</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1DD7C1C95A262CB685E14801E8D1A5F7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe our results submitted to BirdCLEF 2015 for classifying among 999 tropical bird species. Our test attained a MAP score of over 30% in the official results. This note is not a self-contained paper, since our system was largely the same as used in BirdCLEF 2014 and described in detail elsewhere. The method uses raw audio without segmentation and without using any auxiliary metadata. and successfully classifies among 999 bird categories.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The BirdCLEF 2015 challenge, as part of the LifeCLEF evaluation campaign <ref type="bibr" coords="1,467.31,346.53,9.96,8.74" target="#b0">[1]</ref>, challenged researchers to build systems which could classify audio files across 999 bird species encountered in South America.</p><p>For our participation we submitted a single run from our classifier based on unsupervised feature learning and random forest classifier. This was broadly as used in BirdCLEF 2014, and described in detail in <ref type="bibr" coords="1,364.27,406.59,9.96,8.74">[3]</ref>. We refer the reader to that paper for a full system description. We used a single instance of the twolayer unsupervised feature learning process. Figure <ref type="figure" coords="1,361.25,430.50,4.98,8.74" target="#fig_0">1</ref> illustrates the main steps involved in processing.</p><p>Differences from the 2014 system included:</p><p>in the downsampling step between the two feature-learning layers, we used L2 pooling rather than max-pooling, which gave a slight improvement; we reduced the size of our random forest to 100 due to memory constraints.</p><p>Our system is fully streaming except for the construction of the random forest; in future it would be interesting to use a streamed implementation such as [2].</p><p>Our unsupervised feature learning scales well with increasing data size: linearly, as described in the main paper. However, in our case, due to the compute resources available in the time leading up to the competition deadline we were not able to submit more than one run, nor to apply model averaging.</p><p>Our own tests using a two-fold split of the training data confirmed an observation that we made in [3]: adding more layers gives a benefit up to a certain limit, which appears to be related to the size of the available data set. In our tests (Figure <ref type="figure" coords="1,192.97,645.16,4.43,8.74" target="#fig_1">2</ref>) the available data appeared insufficient to support a three-layer variant, hence we submitted a two-layer run. For this 2015 challenge (across 999 bird species with 33,203 audio files) our final MAP score was 30.2% (considering only foreground species), and 26.2% (including background species). These results are a few percentage points lower than the results for the similar systems submitted to the 2014 challenge, as one might expect given that the number of species to identify had been increased from 501 to 999. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,134.77,361.43,345.82,7.89;2,134.77,372.42,96.15,7.86"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Summary of the classification workflow, here showing the case where single-layer feature learning is used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,134.77,345.42,345.82,7.89;3,134.77,356.41,331.70,7.86"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Evaluation using a two-fold crossvalidation split on the training data. The columns represent a single-layer run, three two-layer runs and one three-layer run.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to thank the people and projects which made available the data used for this research-the Xeno Canto website and its many volunteer contributors-as well as the SABIOD research project for instigating the contest, and the CLEF contest hosts.</p><p>This work was supported by <rs type="funder">EPSRC</rs> <rs type="grantName">Early Career Fellowship</rs> <rs type="grantNumber">EP/L020505/1</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_RVm95Kz">
					<idno type="grant-number">EP/L020505/1</idno>
					<orgName type="grant-name">Early Career Fellowship</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="2,138.35,635.88,342.24,7.86;2,146.91,646.84,333.68,7.86;2,146.91,658.44,132.80,7.47" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="2,369.87,635.88,110.72,7.86;2,146.91,646.84,20.56,7.86">CLEF 2015 Labs and Workshops</title>
		<ptr target="http://ceur-ws.org/Vol-1391/" />
	</analytic>
	<monogr>
		<title level="s" coord="2,175.91,646.84,231.07,7.86">Notebook Papers. CEUR Workshop Proceedings (CEUR</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>San Juan</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
