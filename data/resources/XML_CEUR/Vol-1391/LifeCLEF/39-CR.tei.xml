<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,174.42,116.95,266.52,12.62;1,190.11,134.89,235.14,12.62">Content Specific Feature Learning for Fine-Grained Plant Classification</title>
				<funder>
					<orgName type="full">Australian Government</orgName>
				</funder>
				<funder>
					<orgName type="full">Department of Communications</orgName>
				</funder>
				<funder ref="#_KFpNfb5 #_GAFPyHC">
					<orgName type="full">Australian Research Council</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,149.63,173.95,59.85,8.74;1,211.83,172.38,1.83,6.12"><forename type="first">Zongyuan</forename><surname>Ge</surname></persName>
							<email>z.ge@qut.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Australian Center for Robotic Vision</orgName>
								<orgName type="institution" key="instit1">Queensland University of Technology</orgName>
								<orgName type="institution" key="instit2">NICTA</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,222.07,173.95,60.41,8.74;1,284.83,172.38,1.83,6.12"><forename type="first">Chris</forename><surname>Mccool</surname></persName>
							<email>c.mccool@qut.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Australian Center for Robotic Vision</orgName>
								<orgName type="institution" key="instit1">Queensland University of Technology</orgName>
								<orgName type="institution" key="instit2">NICTA</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,295.07,173.95,79.82,8.74;1,377.24,172.38,1.36,6.12"><forename type="first">Conrad</forename><surname>Sanderson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Australian Center for Robotic Vision</orgName>
								<orgName type="institution" key="instit1">Queensland University of Technology</orgName>
								<orgName type="institution" key="instit2">NICTA</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,407.28,173.95,51.94,8.74;1,461.57,172.38,1.83,6.12"><forename type="first">Peter</forename><surname>Corke</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Australian Center for Robotic Vision</orgName>
								<orgName type="institution" key="instit1">Queensland University of Technology</orgName>
								<orgName type="institution" key="instit2">NICTA</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,174.42,116.95,266.52,12.62;1,190.11,134.89,235.14,12.62">Content Specific Feature Learning for Fine-Grained Plant Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A9432368130C416A9965F9CE5F5FD602</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>deep convolutional neural network</term>
					<term>plant classification</term>
					<term>subset feature learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present the plant classification system submitted by the QUT RV team to the LifeCLEF 2015 plant task. Our system learns a content specific feature for various plant parts such as branch, leaf, fruit, flower and stem. These features are learned using a deep convolutional neural network. Experiments on the LifeCLEF 2015 plant dataset show that the proposed method achieves good performance with a score of 0.633 on the test set.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Fine-grained image classification has received considerable attention recently with a particular emphasis on classifying various species of birds, dogs and plants <ref type="bibr" coords="1,165.59,463.03,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="1,178.10,463.03,7.75,8.74" target="#b2">3,</ref><ref type="bibr" coords="1,187.84,463.03,7.75,8.74" target="#b3">4,</ref><ref type="bibr" coords="1,197.58,463.03,11.62,8.74" target="#b10">11]</ref>. Fine-grained image classification is a challenging computer vision problem due to the small inter-class variation and large intra-class variation. Plant classification is a particularly important domain because of the implications for automating Agriculture as well as enabling robotic agents to detect and measure plant distribution and growth.</p><p>To evaluate the current performance of the state-of-the-art vision technology for plant recognition, the Plant Identification Task of the LifeCLEF challenge <ref type="bibr" coords="1,160.12,548.12,10.52,8.74" target="#b4">[5,</ref><ref type="bibr" coords="1,171.85,548.12,7.75,8.74" target="#b6">7]</ref> focuses on distinguishing 1000 herb, tree and fern species. This is an observation-centered task where several images from seven organs of a plant are related to one observation. There are seven organs, referred to as content types, and include images of the entire plant, branch, leaf, fruit, flower, stem or a leaf scan.</p><p>Inspired by <ref type="bibr" coords="1,200.00,609.29,9.96,8.74" target="#b3">[4]</ref>, we use a deep convolutional neural network (DCNN) approach and learn a separate DCNN for each content type. We combine the contentspecific feature with a generic DCNN feature, which is trained using all of the content types. This approach yields a highly accurate classification system with a score of 0.633 on the test set. For each test sample, a domain-generic (GCNN) and subset-specific (SCNN) feature is extracted. These two features are then concatenated to form a combined feature vector.</p><p>2 Our Approach</p><p>Our proposed system consists of two main parts. First, we perform transfer learning to learn a domain-generic feature termed as φ GCN N from all plants images (regardless of content type). Second, we manually cluster the dataset into subsets based on content type and learn a feature specific to each subset (φ SCN N ). For each image we extract both domain-generic (φ GCN N ) and subsetspecific (φ SCN N ) features, these features are obtained from layer 20, l 20 , of the deep network. The two feature vectors are then concatenated to form a single feature vector as shown in Figure <ref type="figure" coords="2,322.68,397.75,3.87,8.74">1</ref>. These features are then used to learn a multi-class linear SVM. Power and l 2 norm are applied independently for domain-generic feature and content specific feature prior to combining the feature vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Content Clustering</head><p>There are 7 pre-defined content types consisting of images from the entire plant, branch, leaf, fruit, flower, stem or a leaf scan. In both the training and testing phases all participants are allowed to use the indicated content. We make use of the content type to learn a DCNN that is fine-tuned (specialised) for a subset of the content types. However, because there is a limited number of images for each content type, we first group the most visually similar content types toghether. In particular, we define four subsets. The first subset conists of the the entire plant and branch content types, the second subset consists of the leaf and leaf scan content types, the third subset contains fruit and flower content types, and the fourth subset consists of the stem only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep Convolutional Neural Networks as Feature Representation</head><p>Krizhevsky et al. <ref type="bibr" coords="2,211.60,645.16,10.52,8.74" target="#b7">[8]</ref> recently achieved impressive performance on the ImageNet recognition task using CNNs, which were initially proposed by LeCun et al. <ref type="bibr" coords="2,470.08,657.11,10.52,8.74" target="#b8">[9]</ref> for hand written digit recognition. Since then CNNs have received considerable attention and in the Large-scale ImageNet Challenge 2014 (ILSVRC) the top five results were all produced using CNN-based systems <ref type="bibr" coords="3,381.26,143.90,14.61,8.74" target="#b9">[10]</ref>.</p><p>In this work we fine-tune a general model for the task of plant classification. The base model that we fine-tune is the best performing model from ILSVRC <ref type="bibr" coords="3,175.43,181.63,14.61,8.74" target="#b11">[12]</ref>, referred to as GoogLeNet. GoogLeNet is a very deep neural network model with 22 layers. It consists primarily of convolutional layers. We use the output of the last convolutional layer l 20 , after average pooling, to obtain our feature vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Domain Specific Feature Learning</head><p>Transfer learning has usually been applied by fine-tuning a general network, such as the network of Krizhevsky et al. <ref type="bibr" coords="3,324.86,297.95,9.96,8.74" target="#b7">[8]</ref>, to a specific task such as bird classification <ref type="bibr" coords="3,192.99,309.91,14.61,8.74" target="#b12">[13]</ref>.</p><p>Inspired by the findings of Zhang et al. <ref type="bibr" coords="3,318.27,323.73,15.50,8.74" target="#b12">[13]</ref> we learn a domain-generic DCNN for the task of plant classification. This is achieved by applying transfer learning on the parameters of the GoogLeNet model (learned from the large-scale Ima-geNet dataset) using all of the training data for the plant classification task. This new DCNN provides domain-generic features for the task of plant classification and is referred to as the domain-generic DCNN. The only difference between the pre-trained GoogLeNet model and the domain-generic DCNN is that the number of outputs for the last fully connected layer is changed to be 1, 000 which is the number of training classes available. For each image we can then obtain a domain-generic feature φ GCN N from the last convolutional layer l 20 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Subset Feature Learning as Content Specific Feature</head><p>A separate DCNN is learned for each of the K = 4 pre-defined subsets by finetuning the domain-specific model, described in Section 2.3. The aim is to learn features for each subset that will allow us to more easily differentiate visually similar content of plant species. As such, for each subset, we apply fine-tuning to the pre-trained GoogLeNet model. To train the k-th subset (Subset k ) we use the N k images assigned to this subset X k = [x 1 , ..., x N k ], with their corresponding class labels.</p><p>The only difference between these models and the pre-trained GoogLeNet model is that the number of outputs for the last fully connected layer, of each model, is set to the number of training classes in each subset. Transfer learning is then applied separately to each network using backpropogation and stochastic gradient descent (SGD). For each image belonging to the k-th subset a subset feature vector φ SCN N k is obtained by taking the output of the last convolutional layer l 20 .</p><p>In this section we present a comparative performance evaluation of our proposed method on a validation set and the defined test sets. The provided training dataset is split into two sets: roughly 10% of the total training data was used as a validation set and the rest is used for training the models. The split is based on observation id because final testing is also observation-based.</p><p>This results in 82,033 training images, including 21,746 for the branch and entire subset, 32,186 for fruit and flower subset, 23,234 for the leaf and leaf scan subset and 4,867 for the stem subset. The validation set consists of 9,725 images.</p><p>We use Caffe <ref type="bibr" coords="4,209.90,251.65,10.52,8.74" target="#b5">[6]</ref> for learning generic and subset specific features. The opensource package LibLinear <ref type="bibr" coords="4,248.46,263.60,10.52,8.74" target="#b1">[2]</ref> is used to train the multi-class linears SVMs. The SVM cost parameter C is set to 1 and all images are resized to 224 × 224.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Results on Validation Set</head><p>First we assess our proposed method on the validation set. We conducted three sets of experiments which examine the effectives of the domain-specific feature vector, the subset feature vector and the combination of these two feature vectors.</p><p>The results on the validation set, shown in Table <ref type="table" coords="4,374.41,373.43,3.87,8.74" target="#tab_0">1</ref>, demonstrate that the combination of these two feature vectors provides a considerable performance improvement. The combination of these two feature vectors achieves a mean accuracy of 66.6%. This is an absolute improvement of 6.5 percentage points over the domain-specific feature vector φ GCN N which achieves a mean accuracy of 60.1%. By comparison, the subset feature vector φ SCN N k achieves a mean accuracy of only 58.0%. We believe that the subset feature vector performs worse than the domain-specific feature vector because of the limited number of training images for each subset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results on Test Set</head><p>In this section, we present our submitted results for the LifeCLEF2015 plant challenge. We submitted three runs:</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,134.77,246.39,345.83,7.89;2,134.77,257.37,345.83,7.86;2,134.77,268.33,58.16,7.86;2,273.30,188.70,98.67,98.67"><head></head><label></label><figDesc>Fig.1. For each test sample, a domain-generic (GCNN) and subset-specific (SCNN) feature is extracted. These two features are then concatenated to form a combined feature vector.</figDesc><graphic coords="2,273.30,188.70,98.67,98.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="5,134.77,257.66,345.83,211.38"><head></head><label></label><figDesc></figDesc><graphic coords="5,134.77,257.66,345.83,211.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="6,134.77,116.83,345.83,211.38"><head></head><label></label><figDesc></figDesc><graphic coords="6,134.77,116.83,345.83,211.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,134.77,500.65,345.82,78.57"><head>Table 1 :</head><label>1</label><figDesc>Mean accuracy on the LifeCLEF 2015 Plant dataset of our proposed method. Annotated content information is used.</figDesc><table coords="4,218.79,534.20,174.71,45.03"><row><cell>Method</cell><cell>Mean Accuracy</cell></row><row><cell>Domain Specific Feature</cell><cell>60.1%</cell></row><row><cell>Content Specific Feature</cell><cell>58.0%</cell></row><row><cell>Combined</cell><cell>66.6%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The <rs type="institution">Australian Centre for Robotic Vision</rs> is supported by the <rs type="funder">Australian Research Council</rs> via the <rs type="programName">Centre of Excellence program</rs>. <rs type="institution">NICTA</rs> is funded by the <rs type="funder">Australian Government</rs> through the <rs type="funder">Department of Communications</rs>, as well as the <rs type="funder">Australian Research Council</rs> through the <rs type="programName">ICT Centre of Excellence program</rs>. We would also like to thank <rs type="person">Professor Chunhua Shen</rs> and <rs type="person">Dr. Lingqiao Liu</rs> for the fruitful conversations of this work.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_KFpNfb5">
					<orgName type="program" subtype="full">Centre of Excellence program</orgName>
				</org>
				<org type="funding" xml:id="_GAFPyHC">
					<orgName type="program" subtype="full">ICT Centre of Excellence program</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>the rank 1 score is submitted for each observation.</p><p>-RUN2 is the image retrieval task where we take the first 5 predictions.</p><p>-RUN3 is based on RUN 2 but we perform an additional softmax normalization for the first five predictions.</p><p>In Figure <ref type="figure" coords="5,196.00,189.23,4.98,8.74">2</ref> we present the overall performance for all of the competitors using the defined score metric. It can be seen that our best performing system is RUN 2 which achieved a score of 0.633. This is slightly worse than SNUMED INFO systems (RUN 4 and RUN 3). In Figure <ref type="figure" coords="5,194.34,538.56,4.98,8.74">3</ref> we present results for the image-based run. It can be seen that our proposed method provides competitive performance for both the imagebased and observation-based metrics. However, we do have a minor performance loss for the image-based result compared to the observation-based result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and Future Work</head><p>In this paper we presented a domain-specific feature learning and subset-specific feature learning system applied to the plant identification task of LifeCLEF 2015. For domain-specific feature learning, we have shown that it is possible to perform transfer learning from a DCNN pre-trained on the larger-scale ImangNet dataset. Furthermore, we have presented a subset feature learning system that is able to learn content specific features. This approach yields highly competitive performance with a score of 0.633 for this year's task.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="6,142.96,613.55,337.64,7.86;6,151.52,624.51,219.19,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,323.21,613.55,157.39,7.86;6,151.52,624.51,149.48,7.86">Symbiotic segmentation and part localization for fine-grained categorization</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,320.53,624.51,20.90,7.86">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,635.88,337.63,7.86;6,151.52,646.84,329.07,7.86;6,151.52,657.79,115.95,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,151.52,646.84,189.29,7.86">Liblinear: A library for large linear classification</title>
		<author>
			<persName coords=""><forename type="first">Rong-En</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiang-Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,347.27,646.84,133.33,7.86;6,151.52,657.79,33.44,7.86">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,120.67,337.63,7.86;7,151.52,131.63,329.07,7.86;7,151.52,142.59,189.59,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,228.09,131.63,191.35,7.86">Local alignments for fine-grained categorization</title>
		<author>
			<persName coords=""><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arnold</forename><forename type="middle">Wm</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tinne</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,428.00,131.63,52.60,7.86;7,151.52,142.59,112.22,7.86">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,153.55,337.63,7.86;7,151.52,164.51,329.07,7.86;7,151.52,175.46,20.99,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,453.91,153.55,26.68,7.86;7,151.52,164.51,181.69,7.86">Subset feature learning for fine-grained classification</title>
		<author>
			<persName coords=""><forename type="first">Zongyuan</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Mccool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Conrad</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Corke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,341.78,164.51,134.71,7.86">CVPR Workshop on Deep Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,186.42,337.63,7.86;7,151.52,197.38,168.63,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,349.43,186.42,131.16,7.86;7,151.52,197.38,16.79,7.86">Lifeclef plant identification task 2015</title>
		<author>
			<persName coords=""><forename type="first">Hervé</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexis</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><surname>Bonnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,188.12,197.38,103.70,7.86">CLEF working notes 2015</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,208.34,337.64,7.86;7,151.52,219.30,329.07,7.86;7,151.52,230.26,248.15,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="7,382.84,219.30,97.75,7.86;7,151.52,230.26,147.06,7.86">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName coords=""><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,241.22,337.63,7.86;7,151.52,252.18,329.07,7.86;7,151.52,263.14,329.07,7.86;7,151.52,274.09,45.44,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,451.79,252.18,28.80,7.86;7,151.52,263.14,218.31,7.86">Lifeclef 2015: multimedia life species identification challenges</title>
		<author>
			<persName coords=""><forename type="first">Alexis</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hervé</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hervé</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Concetto</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andreas</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Willem-Pier</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bob</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,393.88,263.14,86.71,7.86;7,151.52,274.09,17.10,7.86">Proceedings of CLEF 2015</title>
		<meeting>CLEF 2015</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,285.05,337.64,7.86;7,151.52,296.01,301.74,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,389.06,285.05,91.54,7.86;7,151.52,296.01,161.23,7.86">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,332.60,296.01,19.49,7.86">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,306.97,337.63,7.86;7,151.52,317.93,329.07,7.86;7,151.52,328.89,304.77,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,367.35,317.93,113.24,7.86;7,151.52,328.89,131.54,7.86">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName coords=""><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bernhard</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donnie</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wayne</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,291.07,328.89,79.78,7.86">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.62,339.85,337.98,7.86;7,151.52,350.81,329.07,7.86;7,151.52,361.77,329.07,7.86;7,151.52,372.73,20.99,7.86" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="7,151.52,361.77,192.68,7.86">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName coords=""><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0575</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,142.61,383.68,337.97,7.86;7,151.52,394.64,272.06,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,384.15,383.68,96.44,7.86;7,151.52,394.64,215.87,7.86">Confidence sets for finegrained categorization and plant species identification</title>
		<author>
			<persName coords=""><forename type="first">Asma</forename><surname>Rejeb Sfar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nozha</forename><surname>Boujemaa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,375.15,394.64,19.49,7.86">IJCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,405.60,337.98,7.86;7,151.52,416.56,329.07,7.86;7,151.52,427.52,199.46,7.86" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<title level="m" coord="7,456.47,416.56,24.13,7.86;7,151.52,427.52,98.94,7.86">Going deeper with convolutions</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,438.48,337.98,7.86;7,151.52,449.44,272.38,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,400.63,438.48,79.97,7.86;7,151.52,449.44,136.92,7.86">Part-based R-CNNs for fine-grained category detection</title>
		<author>
			<persName coords=""><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,307.99,449.44,23.05,7.86">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="834" to="849" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
