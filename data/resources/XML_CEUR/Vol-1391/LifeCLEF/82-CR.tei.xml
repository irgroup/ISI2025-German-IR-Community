<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,146.12,152.89,303.08,12.58;1,130.58,170.89,334.10,12.58;1,269.44,188.89,56.31,12.58">Plant identification with deep convolutional neural network: SNUMedinfo at LifeCLEF plant identification task 2015</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,270.04,228.26,55.31,9.02"><forename type="first">Sungbin</forename><surname>Choi</surname></persName>
							<email>wakeup06@empas.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,146.12,152.89,303.08,12.58;1,130.58,170.89,334.10,12.58;1,269.44,188.89,56.31,12.58">Plant identification with deep convolutional neural network: SNUMedinfo at LifeCLEF plant identification task 2015</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">02B8F3E2FE440B10B5F5EC174C9DF746</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image classification</term>
					<term>Deep convolutional neural network</term>
					<term>Goog-LeNet</term>
					<term>Borda-fuse</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our participation at the LifeCLEF Plant identification task 2015. Given various images of plant parts such as leaf, flower or stem, this task is about identification of plant species given multi-image observation query. We utilized GoogLeNet for individual image classification, and combined image classification results for plant identification per observation. Our approach achieved best performance in this task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper, we describe the participation of the SNUMedinfo team at the LifeCLEF Plant Identification task 2015. Each query is composed of multi-image observation, which represents individual plant observed the same day by a same person. Each observation has multiple image, taken from various parts of plant such as leaf, stem or flower. So this task is about identification of plant species given multi-image observation query. For a detailed introduction of the task, please see the overview paper of this task <ref type="bibr" coords="1,143.36,529.30,10.61,9.02" target="#b0">(1)</ref>.</p><p>In recent years, deep Convolutional Neural Network (CNN) has improved automatic image classification performance dramatically <ref type="bibr" coords="1,311.74,553.30,10.65,9.02" target="#b1">(2)</ref>. In this study, we experimented with GoogLeNet (3) which has shown effective performance in recent ImageNet Challenge <ref type="bibr" coords="1,124.70,577.30,10.65,9.02" target="#b3">(4)</ref>. Although LifeCLEF Plant identification task is about more fine-grained image classification compared to ImageNet's general object category classification, finetuning CNN pretrained on ImageNet dataset was very effective in performance. Our experimental methods are detailed in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>We applied CNN for individual image classification (Section 2.1). Then image classification results are combined to produce observation classification (Section 2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.1</head><p>Image We trained five separate CNNs <ref type="foot" coords="2,249.64,317.19,3.24,5.83" target="#foot_0">1</ref> . CNN output score is used to produce ranked list of relevant plant species. Five ranked list is combined into single ranking using Bordafuse method (5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Observation classification by combining image classification result</head><p>Each query observation is composed of multiple image. We combined image classification result from Section 2.1 using two different rank aggregation method.</p><p>(1) Borda-fuse method (2) Majority voting based method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>We submitted four different runs. Details of runs are summarized in the following table.  With regard to the rank aggregation methods used in observation classification, majority-voting based method showed slightly better performance compared to the Borda-fuse method, but the difference was negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">CNN finetuning from other task model</head><p>In Chen et al.'s experiments <ref type="bibr" coords="3,253.60,465.34,11.68,9.02" target="#b5">(6)</ref> in last year, CNN trained without finetuning from other external dataset showed inferior performance, compared to their advanced feature encoding method <ref type="bibr" coords="3,197.72,489.34,11.64,9.02" target="#b6">(7)</ref> based on SIFT and Color Moments features. But when CNN is finetuned from ImageNet pretrained GoogLeNet, it was very effective, even though plant identification is targeted for finer-grained image classification task between different plant species compared to the ImageNet's general object category classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Combining CNN output</head><p>From table 2, we could observe that training multiple CNN and combining their outputs improve classification performance. As also experimented in <ref type="bibr" coords="3,402.76,587.32,10.67,9.02" target="#b7">(8)</ref>, training and combining multiple CNN output method is considered to be effective to cope with CNN's variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training plant part-specific CNN</head><p>In this task, each image is tagged with plant part name (e.g., stem, flower). We also tried dividing training set images according to the tagged part and training CNN per each part separately. But in our preliminary experiments, these part-specific image trained CNNs mostly showed no performance gain (similar or slightly worse performance, compared to when no part-specific training is used). So we chose not to use tagged plant part information for CNN training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In LifeCLEF Plant identification task 2015', we applied GoogLeNet pretrained on ImageNet dataset for training by finetuning on the plant training set. Although task is more finer-grained image category classification compared to the ImageNet, and the number of plant species has doubled compared to the last year's plant task <ref type="bibr" coords="4,428.22,267.32,10.65,9.02" target="#b8">(9)</ref>, classification performance was very effective. Also, training multiple CNNs and combining their output improved classification performance further. In our future study, we will explore other CNN architectural design options and different classification result combination methodologies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,124.70,150.44,345.41,165.32"><head>classification using deep convolutional neural network Finetuning from GoogLeNet We</head><label></label><figDesc>utilized GoogLeNet for individual plant image classification. GoogLeNet incorporates Inception module with the intention of increasing network depth with computational efficiency. We randomly divided observations in LifeCLEF Plant identification training set into five-fold. Images from one fold is used as validation set, and images from other four fold is used as training set. Training CNN for plant identification started from GoogLeNet pretrained on ImageNet dataset. We finetuned CNN on plant identification training set (initial learn-</figDesc><table /><note coords="2,124.70,283.76,341.03,9.02;2,124.70,295.28,344.54,9.02;2,124.70,306.74,258.62,9.02"><p>ing rate 0.001; batch_size:120; number of iteration:100,000). Only horizontal mirroring (left-right flipping of image) and image random cropping (cropping 224 x 224 image out of 256 x 256 input image) is used for data augmentation.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,137.54,518.11,323.93,76.84"><head>Table 1 .</head><label>1</label><figDesc>Different setting of submitted runs</figDesc><table coords="2,137.54,535.90,323.93,59.06"><row><cell></cell><cell>Image classification</cell><cell>Observation classification</cell></row><row><cell>SNUMedinfo1</cell><cell>Only 1 CNN is used 2</cell><cell>Borda-fuse</cell></row><row><cell>SNUMedinfo2</cell><cell>Only 1 CNN is used</cell><cell>Majority voting based method</cell></row><row><cell>SNUMedinfo3</cell><cell>5 CNNs are used</cell><cell>Borda-fuse</cell></row><row><cell>SNUMedinfo4</cell><cell>5 CNNs are used</cell><cell>Majority voting based method</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,124.70,210.11,342.97,146.68"><head>Table 2 .</head><label>2</label><figDesc>Evaluation results of submitted runs</figDesc><table coords="3,124.70,227.90,342.97,128.90"><row><cell></cell><cell>Image classification</cell><cell>Observation classification</cell></row><row><cell></cell><cell>score</cell><cell>score</cell></row><row><cell>SNUMedinfo1</cell><cell>0.594</cell><cell>0.604</cell></row><row><cell>SNUMedinfo2</cell><cell>0.594</cell><cell>0.611</cell></row><row><cell>SNUMedinfo3</cell><cell>0.652</cell><cell>0.663</cell></row><row><cell>SNUMedinfo4</cell><cell>0.652</cell><cell>0.667</cell></row><row><cell cols="3">Performance was clearly better when five CNNs are combined for image classifica-</cell></row><row><cell cols="3">tion (SNUMedinfo3 and SNUMedinfo4), compared to when only one CNN is used</cell></row><row><cell cols="3">(SNUMedinfo1 and SNUMedinfo2). This is observed from both per image classifica-</cell></row><row><cell cols="2">tion score and per observation classification score.</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,136.04,653.23,334.45,8.10;2,136.04,664.21,334.30,8.10;2,136.04,675.19,37.75,8.10"><p>We arbitrarily determined number of CNN classifier for experiment as five. In this study, we tried to assess the effects on performance when more CNNs are trained and their results are combined.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,136.04,686.23,243.80,8.10"><p>Among five trained CNNs, only one CNN is used for classification.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,132.70,372.11,337.95,8.10;4,141.74,383.09,171.75,8.10" xml:id="b0">
	<analytic>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>And San Juan</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Clef</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="4,367.69,372.11,72.77,8.10">Labs and Workshops</title>
		<title level="s" coord="4,446.70,372.11,23.94,8.10;4,141.74,383.09,108.58,8.10">CEUR Workshop Proceedings (CEUR</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,132.70,394.07,337.76,8.10;4,141.74,405.11,295.62,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,315.20,394.07,155.26,8.10;4,141.74,405.11,78.77,8.10">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,226.71,405.11,184.25,8.10">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,132.70,416.11,337.84,8.10;4,141.74,427.09,175.10,8.10" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<idno>arXiv:14094842. 2014</idno>
		<title level="m" coord="4,384.51,416.11,86.02,8.10;4,141.74,427.09,32.21,8.10">Going deeper with convolutions</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="4,132.70,438.13,337.90,8.10;4,141.74,449.11,219.34,8.10" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="4,373.81,438.13,96.78,8.10;4,141.74,449.11,76.31,8.10">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<idno>arXiv:14090575. 2014</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="4,132.70,460.09,337.75,8.10;4,141.74,471.13,328.76,8.10;4,141.74,482.11,211.25,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,235.19,460.09,81.73,8.10">Models for metasearch</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montague</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,326.28,460.09,144.16,8.10;4,141.74,471.13,306.27,8.10">Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 24th annual international ACM SIGIR conference on Research and development in information retrieval<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">384007</biblScope>
			<biblScope unit="page" from="276" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,132.70,493.09,337.87,8.10;4,141.74,504.13,264.06,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="4,330.69,493.09,139.88,8.10;4,141.74,504.13,84.42,8.10">Ibm research australia at lifeclef2014: Plant identification task</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Abedini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Garnavi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,231.73,504.13,147.77,8.10">Working notes of CLEF 2014 conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,132.70,515.11,337.73,8.10;4,141.74,526.09,328.87,8.10;4,141.74,537.13,21.74,8.10" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="4,255.45,515.11,214.98,8.10;4,141.74,526.09,13.02,8.10">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Editors</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,335.32,526.09,99.24,8.10">CVPR&apos;07 IEEE Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,132.70,548.02,337.81,8.19;4,141.74,559.09,328.62,8.10;4,141.74,570.04,328.90,8.19;4,141.74,581.11,328.77,8.10;4,141.74,592.09,54.75,8.10" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="4,341.03,548.02,129.47,8.18;4,141.74,559.09,167.97,8.10">Mitosis Detection in Breast Cancer Histology Images with Deep Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cireşan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Giusti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,216.15,570.04,254.49,8.19;4,141.74,581.11,52.49,8.10">Medical Image Computing and Computer-Assisted Intervention -MICCAI 2013</title>
		<title level="s" coord="4,202.07,581.11,133.06,8.10">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Mori</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Sakuma</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Barillot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">8150</biblScope>
			<biblScope unit="page" from="411" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,132.70,603.13,337.91,8.10;4,141.74,614.11,328.83,8.10;4,141.74,625.09,230.53,8.10" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="4,422.28,603.13,48.32,8.10;4,141.74,614.11,64.41,8.10">Lifeclef plant identification task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Selmi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J-F</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Barthélémy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,230.42,614.11,240.15,8.10;4,141.74,625.09,14.99,8.10">CLEF2014 Working Notes Working Notes for CLEF 2014 Conference</title>
		<meeting><address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<publisher>CEUR-WS</publisher>
			<date type="published" when="2014-09-15">2014. September 15-18, 2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
