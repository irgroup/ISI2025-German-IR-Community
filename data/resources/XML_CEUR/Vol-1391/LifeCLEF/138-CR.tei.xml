<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,140.53,115.96,334.30,12.62;1,144.13,133.89,327.11,12.62">Shared nearest neighbors match kernel for bird songs identification -LifeCLEF 2015 challenge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,153.78,171.56,48.07,8.74"><forename type="first">Alexis</forename><surname>Joly</surname></persName>
							<email>alexis.joly@inria.frvalentin.leveau@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">LIRMM</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,212.41,171.56,70.16,8.74"><forename type="first">Valentin</forename><surname>Leveau</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">LIRMM</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Institut National de l&apos;Audiovisuel (INA)</orgName>
								<address>
									<settlement>Bry-sur-Marne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,299.46,171.56,61.03,8.74"><forename type="first">Julien</forename><surname>Champ</surname></persName>
							<email>julien.champ@inra.fr</email>
							<affiliation key="aff1">
								<orgName type="laboratory" key="lab1">Inra</orgName>
								<orgName type="laboratory" key="lab2">AMAP</orgName>
								<orgName type="institution">LIRMM</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,390.41,171.56,66.69,8.74"><forename type="first">Olivier</forename><surname>Buisson</surname></persName>
							<email>olivier.buisson@ina.fr</email>
							<affiliation key="aff2">
								<orgName type="institution">Institut National de l&apos;Audiovisuel (INA)</orgName>
								<address>
									<settlement>Bry-sur-Marne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,140.53,115.96,334.30,12.62;1,144.13,133.89,327.11,12.62">Shared nearest neighbors match kernel for bird songs identification -LifeCLEF 2015 challenge</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">047AA93898FF1555EC020B78014E40AD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a new fine-grained audio classification technique designed and experimented in the context of the LifeCLEF 2015 bird species identification challenge. Inspired by recent works on fine-grained image classification, we introduce a new match kernel based on the shared nearest neighbors of the low level audio features extracted at the frame level. To make such strategy scalable to the tens of millions of MFCC features extracted from the tens of thousands audio recordings of the training set, we used high-dimensional hashing techniques coupled with an efficient approximate nearest neighbors search algorithm with controlled quality. Further improvements are obtained by (i) using a sliding window for the temporal pooling of the raw matches (ii) weighting each low level feature according to the semantic coherence of its nearest neighbors. Results show the effectiveness of the proposed technique which ranked 2nd among the 7 research groups participating to the LifeCLEF bird challenge.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Building accurate knowledge of the identity, the geographic distribution and the evolution of living species is essential for a sustainable development of humanity as well as for biodiversity conservation. In this context, using multimedia identification tools is considered as one of the most promising solution to help bridging the taxonomic, i.e. the difficulty for common people to name observed living organisms and then produce or access to useful knowledge. The LifeCLEF <ref type="bibr" coords="1,134.77,560.48,15.50,8.74" target="#b9">[10]</ref> lab proposes to evaluate this challenge in the continuity of the image-based plant identification task was run within ImageCLEF the years before but with a broader scope (considering birds and fish in addition to plants and audio and video contents in addition to images). This paper particularly reports the participation of Inria ZENITH research group to the audio-based bird identification task. Inspired by some recent works on fine-grained image classification <ref type="bibr" coords="1,447.81,620.25,14.61,8.74" target="#b10">[12]</ref>, we introduce a new match kernel based on the shared nearest neighbors of the low level audio features extracted at the frame level. Section 2 describes the preliminary audio processing and features extraction steps. Section 3 then presents our new match kernel and the resulting explicit representations to be further classified thanks to a linear supervised classifier (section 4). Section 5 and ?? finally reports and discuss the results we obtained within the LifeCLEF 2015 challenge .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Pre-processing and features extraction</head><p>The dataset used for this challenge is composed of 33,203 audio recordings belonging to 999 bird species from Brazil area. As various recording devices are used, and because it is difficult to capture these sounds as birds are often far away from the recording devices, many recordings contains a lot of noise. To overcome this problem, we used SoX, the "Swiss Army knife of sound processing programs"<ref type="foot" coords="2,197.29,271.07,3.97,6.12" target="#foot_0">4</ref> . As a first step, we used the noisered specialised filter, to filter out noise from the audio, and then we reduce the length of large (i.e. &gt; 0.1s) silent passages from audio files to 0.1s. In order to obtain audio files with ideally no more noise but still enough signal, we tried removing as much noise as possible (using the noisered amount parameter) while guaranteeing that the resulting audio file was at least 20% the size of the initial audio record. After this pre-processing step, we used an open source software framework, marsyas <ref type="foot" coords="2,460.62,342.81,3.97,6.12" target="#foot_1">5</ref> , to extract MFCC features with parameters based on the provided audio features in the Birdclef task : MFCC are computed on windows of 11.6 ms, each 3.9 ms, and we additionally derive their speed resulting in 26-dimensional feature vectors (13+13) for each frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Shared Nearest Neighbors Match Kernel</head><p>We consider two recordings I x and I y represented by sets of 26-dimensional MFCC features X = {x} and Y = {y}. We then build on the normalized sum match kernel proposed by <ref type="bibr" coords="2,251.27,474.12,15.49,8.74" target="#b11">[13]</ref> to compare feature sets:</p><formula xml:id="formula_0" coords="2,208.39,493.21,272.21,24.79">K(X, Y ) = Φ(X) T Φ(Y ) = 1 |X| |Y | x y k(x, y)<label>(1)</label></formula><p>where k() is itself a Mercer kernel allowing to compare two individual local features x and y. In our case, k() is however not defined as a direct matching between x and y but rather as the degree of correlation of their matches in a large training set. Let denote as Z such a training set composed of N 26-dimensional MFCC feature vectors z. We introduce the following shared nearest neighbors (SNN) match kernel :</p><formula xml:id="formula_1" coords="2,218.03,608.65,262.56,24.79">KS(X, Y ) = 1 |X| |Y | x y z ϕx(z).ϕy(z)<label>(2)</label></formula><p>with ϕ x (z) a rank-based activation function given by :</p><formula xml:id="formula_2" coords="3,244.42,142.55,236.17,19.82">ϕx(z) = log(K) -log(rx(z)) log(K)<label>(3)</label></formula><p>where r x (z) : Z → R + is a ranking function returning the rank of an item z ∈ Z according to its distance to x and K is the maximum number of items returned by this ranking function. The distance itself could be a L 2 metric in the original feature space but, as we will see in section 3, we use in practice a more efficient Hamming embedding scheme. Whatever the distance used, the intuition of the SNN match kernel is that it counts the number of common neighbors in the neighborhood of x and in the one of y. The product k z (x, y) = ϕ x (z).ϕ y (z) is actually equal to one if z is the nearest neighbor of both x and y and close to zero if z is not in the top neighbors of either x or y. Using this shared nearest neighbors kernel instead of a more classical distance in the feature space has several justifications and advantages. First, sharedneighbors techniques are known to overcome several shortcomings of traditional metrics. They are notably less sensitive to the dimensionality curse, more robust to noisy data and more stable over unusual features distribution <ref type="bibr" coords="3,416.45,331.39,10.52,8.74" target="#b1">[2,</ref><ref type="bibr" coords="3,428.63,331.39,7.75,8.74" target="#b4">5]</ref> . Measuring the similarity between features by the degree to which their neighbourhoods in the training set resemble one another is actually a form of generative metric learning. Features belonging to dense clusters are actually more likely to share neighbors than uniformly distributed and isolated features. So that their contribution in the global kernel will be enhanced. Secondly, using an indirect matching rather than a direct one allows to have en explicit formulation of the embedded space Φ(X). By factorizing equation 3, it is actually easy to show that K S (X, Y ) = Φ S (X) T Φ S (Y ) with:</p><formula xml:id="formula_3" coords="3,244.59,448.20,236.00,29.15">ΦS(X) = N i=1 1 |X| x ϕx(zi). -→ ei (4)</formula><p>So that, the explicit N-dimensional feature vector Φ(X) representing each audio recording in the training set can be computed before training a simple linear classifier on top of them. This principle of this approach was already introduced in the intermediate matching kernel of <ref type="bibr" coords="3,304.95,524.61,10.52,8.74" target="#b0">[1]</ref> and further re-used in many methods including the NBNN kernel of <ref type="bibr" coords="3,270.34,536.57,14.61,8.74" target="#b13">[15]</ref>. Such methods did however rely on the distance between the features of the candidate object X and the ones of the training set Z so that they did not benefit from the nice properties of the SNN-kernel. Last but not least, one of the main advantage of the SNN match kernel is that is that it can be easily converted to a sparse representation as the ratio of the number of values close to zeros is very high. Only the features z lying in the top neighbors of both x and y lead to consistent component values. In practice, it is therefore sufficient to consider only the top-m neighbors of each feature x and y to get a good approximation of K(X, Y ). This allows using efficient nearest neighbors search techniques to construct the explicit representations Φ(X) and to use an efficient sparse encoding when training linear classifiers on top of them.</p><p>Temporal pooling of the raw SNN-based representations As elegant as the explicit representations Φ S (X) is, it does not conduct to good classification results in practice. It's very high dimensionality, equals to the number of features in the training set (often millions), actually leads to strong overfitting even when using L2 regularizers with high values of the regularization control parameter λ. It is therefore required to group the individual matches of the SNN kernel before deriving an effective explicit representation. In this work, we do focus on the temporal pooling of the raw matches rather than aggregating them in the feature space as done in many popular image representations such as BoW, Fisher Vectors or VLAD. We consequently loose some generalization capacity in the feature space compared to these methods but on the other side we strongly boost the locality, the interpretability and the discrimination of the trained audio patterns. Practically, our temporal pooling algorithm first aggregates the raw matches within a sliding window (centered around each frame) and then keep the max score over the whole record. More formally, we can reformulate our explicit formulation of Equation <ref type="formula" coords="4,242.86,310.28,4.98,8.74">4</ref>as:</p><formula xml:id="formula_4" coords="4,201.67,328.77,275.00,34.27">Φ w S (X) = M m=1   max t i ∈[1,Tm] t i +(w/2) t=t i -(w/2) x∈X ϕx(z m t )   . -→ em (<label>5</label></formula><formula xml:id="formula_5" coords="4,476.67,342.82,3.93,7.86">)</formula><p>where M is the number of audio recordings in the training set, T m the number of frames of the m-th recording and z m t the MFCC feature of the t-th frame of the m-th recording. The size w of the sliding window was trained by cross-validation and then fixed to w = 1000 frames (resulting in a sliding window of 3.9 seconds).</p><p>Approximate K-NN search scheme In practice, to speed up the computation of our SNN based representations, the ranking function r x (z) : Z → R + is implemented as an approximate nearest neighbors search algorithm based on hashing and probabilistic accesses in the hash table. It takes as input each query feature x of the audio recording I x to be described and returns a set of K approximated neighbors in Z with an approximated rank r x (z). The exact ranking function r x (z) is simply replaced by this approximated ranking function in all equations above. Note that the features z ∈ Z that are not returned in the top-K approximated nearest neighbors are simply removed from the SNN match kernels equations conducting to a considerable reduction of the computation time. Consequently, they are implicitly considered as having a rank-based activation function ϕ x (z) equal to zero which is a good approximation as their rank is supposed to be higher than K.</p><p>Let us now describe more precisely our approximate nearest neighbors indexing and search method. It first compresses the original feature vectors z ∈ Z into compact binary hash codes h(z) of length b. This is done by using RMMH <ref type="bibr" coords="4,134.77,656.12,9.96,8.74" target="#b7">[8]</ref>, a recent data-dependent hash function family, in order to embed the original feature vectors in compact binary hash codes of b = 128 bits (the parameter M of RMMH was fixed to M = 32). The distance between any two features x and z can then be efficiently approximated by the Hamming distance between their respective 128-length hash codes h(z) and h(x). According to our experiments, this hashing method provides in our context better performances than several other tested methods, including random projections or hamming embedding <ref type="bibr" coords="5,470.07,178.77,10.52,8.74" target="#b5">[6]</ref> (orthogonal random projections).</p><p>To avoid scanning the whole dataset, the hash codes h(z) derived from the local features of the training set Z are then indexed in a hash table whose keys are the t-length prefix of the hash codes h(z). At search time, the hash code h(x) of a query feature x is computed as well as its t-length prefix. We then use a probabilistic multi-probe search algorithm inspired by the one of <ref type="bibr" coords="5,415.20,265.30,10.52,8.74" target="#b6">[7]</ref> to select the buckets of the hash table that are the most likely to contain exact nearest neighbors. This is done by using a probabilistic search model that is trained offline on the exact m-nearest neighbors of M sampled features z ∈ Z. We however use a simpler search model than the one of <ref type="bibr" coords="5,299.06,313.12,9.96,8.74" target="#b6">[7]</ref>. We actually use a normal distribution with independent components parameterized by a single vector σ that is trained over the exact nearest neighbors of the training samples. At search time, we also use a slightly different probabilistic multi-probe algorithm trading stability for time. Instead of probing the buckets by decreasing probabilities, we rather use a greedy algorithm that computes the probability of neighboring buckets and select only the ones having a probability greater than a threshold ζ that is fixed over all queries. The value of ζ is trained offline on M training samples and their exact nearest neighbors so as to reach on average cumulative probability α over the visited buckets. In our experiments, we always used α = 0.80 meaning that on average we retrieve 80% of the exact nearest neighbors in the original feature space. Once the most probable buckets have been selected, the refinement step computes the Hamming distance between h(x) and the h(z)'s belonging to the selected buckets and keep only the top-m matches thanks to a max heap.</p><p>Weak semantic weighting As we are in the case of weakly annotated audio recordings with multiple classes (primary and secondary species) and highly cluttered contexts, we suggest improving our SNN match kernel by weighting the query features according to the semantic coherence of their k nearest neighbours. We therefore compute a discrimination score f (x) for all MFCC features x ∈ X of a given audio recording I X . A weak label l(x) is first estimated for each x as the most represented label within the k-nearest neighbors of x in the training set (actually the ones computed by the hash-based k-nn search method described in section 3). The semantic weight f (x) is then computed as the percentage of the k-nearest neighbors having the same weak label than the feature itself (i.e. the percentage of k-nearest neighbors whose label is equal to l(x)). Finally, our representation of a given audio recording I X becomes:</p><formula xml:id="formula_6" coords="6,190.13,126.92,290.46,34.27">Φ w S (X) = M m=1   max t i ∈[1,Tm] t i +(w/2) t=t i -(w/2) x∈X f (x).ϕx(z m t )   . -→ em (6)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Training and classification</head><p>To achieve an effective supervised classification task, we trained a linear discriminant model on top of our proposed SNN matching-based representations (cf. Equation <ref type="formula" coords="6,198.09,231.65,3.87,8.74" target="#formula_4">5</ref>). This requires first building the representations of all audio recordings in the training set and then in learning as many linear classifiers as the number of species in the training set. The resulting linear classifiers are of the form:</p><formula xml:id="formula_7" coords="6,245.59,277.58,124.17,12.69">h(Φ w S (X)) = ω T .Φ w S (X) + b</formula><p>so that they interestingly affect weights ω j to each audio recording in the training set according to its relevance for the targeted class (rather than affecting weights to the individual MFCC features as in the raw representation of Equation 4). In our experiments, we used a linear support vector machine for training these discriminant linear models. We more precisely used the LibLinear implementation of the scikit-learn library with a squared hinge loss function and a L 2 penalty. The C parameter of the SVM was fixed to C = 100.0 * weight(class) where weight(class) is a class-dependent weight that is automatically adjusted to be inversely proportional to the class frequency. Finally, the scores returned by the SVM are converted into probabilities using the following p-value test:</p><formula xml:id="formula_8" coords="6,189.85,441.10,218.68,23.70">P (class) = 1 2 1 + erf 1 (2) s(class) -µ(class) σ(class)</formula><p>where erf is the Gauss error function and µ(class) and σ(class) are respectively the mean and the standard deviation of the SVM score across the considered class. We will see in the experiments that this conversion provides a noticeable accuracy improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset and task</head><p>The LifeCLEF 2015 bird dataset <ref type="bibr" coords="6,282.35,603.37,10.52,8.74" target="#b2">[3]</ref> is built from the Xeno-canto collaborative database 6 involving at the time of writing more than 140k audio records covering 8700 bird species observed all around the world thanks to the active work of more than 1400 contributors. The subset of Xeno-canto data used for the 2015 edition of the task contains 33,203 audio recordings belonging to 999 bird species in Brazil area, i.e. the ones having the more recordings in Xeno-canto database. The dataset contains minimally 14 recordings per species and minimally 10 different recordists per species.Audio records are associated to various metadata such as the type of sound (call, song, alarm, flight, etc.), the date and localization of the observations (from which rich statistics on species distribution can be derived), some textual comments of the authors, multilingual common names and collaborative quality ratings (more details can be found in <ref type="bibr" coords="7,404.58,202.68,10.30,8.74" target="#b2">[3]</ref>). The task was evaluated as a bird species retrieval task. A part of the collection was delivered as a training set available a couple of months before the remaining data is delivered. The goal was to retrieve the singing species among the top-k returned for each of the undetermined observation of the test set. Participants were allowed to use any of the provided metadata complementary to the audio content but we did not in our own submissions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Submitted runs and results</head><p>We submitted three runs to be evaluated within the LifeCLEF 2015 challenge:</p><p>INRIA Zenith Run 1: This run was not based on the method described in this paper, but on our former instance-based classification method <ref type="bibr" coords="8,425.73,175.14,10.52,8.74" target="#b8">[9]</ref> evaluated within the 2014 BirdCLEF challenge <ref type="bibr" coords="8,302.75,187.10,9.96,8.74" target="#b3">[4]</ref>. This allows us measuring progresses between that former approach and the new one proposed in this paper. It basically relied on a very similar matching process than the one described in this paper but it did not train any supervised classifier on top of the resulting matching score. It actually only computed the top-30 most similar training records of each query and then used a simple vote on the labels of the retrieve records as classifier. It however included a pre-filtering of the training set that removed the less discriminant MFCC features from the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INRIA Zenith Run 2:</head><p>The new approach described in this paper.</p><p>INRIA Zenith Run 3: The same approach than Run 2 (i.e. the main contribution of that paper), but without the conversion of the SVM scores into probabilities (see section 4).</p><p>The results of the whole challenge, including our own results as well as the results of the other participating research groups, are reported in Figure <ref type="figure" coords="8,456.02,378.56,4.98,8.74" target="#fig_0">1</ref> and Table <ref type="table" coords="8,162.16,390.51,3.87,8.74" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Discussion and perspectives</head><p>Our system globally achieved very good performance and ranked as the second best one among the 7 participating research groups. Our best run, i.e. the one based on the method proposed in this paper, achieved a mAP of 0, 334 when considering only the primary species of each test recording. This is 3 points better than the state-of-the-art approach of the QMUL research group which makes use of unsupervised feature learning as described in <ref type="bibr" coords="8,390.68,500.70,15.50,8.74" target="#b12">[14]</ref> whereas we used classical MFCC features. Also, compared to the mAP of our first run (equals to 0.265), it shows that training discriminant models using our SNN match kernel is much more effective than using our former semantic pruning and instancebased classification approach. The weights learned by the SVM on the pooled matches actually compensate most of the bias involved by the heterogeneity of the noise level in the recordings and the heterogeneity of the recordings length. The intermediate performance of INRIA Zenith Run 3 shows, however, that the conversion of the SVM scores into probabilities plays an important role in the performance of Run2. Our interpretation of this phenomenon is related to the fact that the number of training records per species follows an heavily tailed distribution (as in most biodiversity data). The SVM scores are consequently boosted for the most populated species to the detriment of the less populated ones. Our p-value normalization allows compensating this bias by normalizing the distribution across all classes. Now, the performance of our approach is still much lower than the best performing system of MNB TSA which has a mAP equal to 0.453. Note that their approach is in essence not so far from ours as they also represent the audio recordings thanks to their matching score in a reference set of audio segments <ref type="bibr" coords="9,134.77,202.24,14.61,8.74">[11]</ref>. A major difference however is that they pre-compute a clean set of relevant audio segments whereas we use all the recordings of the training set as vocabulary. They notably consider only the audio recordings with the highest user ratings in the metadata, and, then extract only the segments that are likely to contain a bird song (thanks to bandwidth considerations). A second difference is that their matching is computed at the signal level whereas we are using MFCC features that might loose some important information. We believe that integrating these two additional paradigms within our framework could make it competitive with their approach. Investigating more in depth the semantic pruning strategy that we introduced in <ref type="bibr" coords="9,310.75,309.83,10.52,8.74" target="#b8">[9]</ref> but in the context of our new SNN match kernel might for instance be an effective way of further improving the quality of the reference set. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="9,134.77,609.88,345.83,7.89;9,134.77,620.84,285.19,7.89;9,134.77,388.18,338.91,206.93"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Official results of LifeCLEF 2015 Bird Task -Our runs are referred as INRIA Zenith Run 1, INRIA Zenith Run 2 and INRIA Zenith Run 3</figDesc><graphic coords="9,134.77,388.18,338.91,206.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,134.77,305.81,407.74,222.83"><head>Table 1 .</head><label>1</label><figDesc>Official results of LifeCLEF 2015 Bird Task -Our runs are referred as INRIA Zenith Run 1, INRIA Zenith Run 2 and INRIA Zenith Run 3</figDesc><table coords="7,136.16,305.81,406.35,200.97"><row><cell>Run Name</cell><cell cols="2">MAP 2(without Background Species) MAP 2 (with Background Species)</cell></row><row><cell>MNB TSA Run 4</cell><cell>0.454</cell><cell>0.414</cell></row><row><cell>MNB TSA Run 3</cell><cell>0.442</cell><cell>0.411</cell></row><row><cell>MNB TSA Run 2</cell><cell>0.442</cell><cell>0.405</cell></row><row><cell>MNB TSA Run 1</cell><cell>0.424</cell><cell>0.388</cell></row><row><cell>INRIA ZENITH Run 2</cell><cell>0.334</cell><cell>0.291</cell></row><row><cell>QMUL Run 1</cell><cell>0.302</cell><cell>0.262</cell></row><row><cell>INRIA ZENITH Run 3</cell><cell>0.292</cell><cell>0.259</cell></row><row><cell>INRIA ZENITH Run 1</cell><cell>0.265</cell><cell>0.240</cell></row><row><cell>GOLEM Run 2</cell><cell>0.171</cell><cell>0.149</cell></row><row><cell>GOLEM Run 1</cell><cell>0.161</cell><cell>0.139</cell></row><row><cell>CHIN. AC. SC. Run 1</cell><cell>0.01</cell><cell>0.009</cell></row><row><cell>CHIN. AC. SC. Run 3</cell><cell>0.009</cell><cell>0.01</cell></row><row><cell>CHIN. AC. SC. Run 2</cell><cell>0.007</cell><cell>0.008</cell></row><row><cell>MARF Run 1</cell><cell>0.006</cell><cell>0.005</cell></row><row><cell>MARF Run 2</cell><cell>0.003</cell><cell>0.002</cell></row><row><cell>MARF Run 3</cell><cell>0.005</cell><cell>0.005</cell></row><row><cell>MARF Run 4</cell><cell>0.000</cell><cell>0.000</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="2,144.73,645.84,109.21,7.86"><p>http://sox.sourceforge.net/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="2,144.73,656.80,83.58,7.86"><p>http://marsyas.info/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2" coords="6,144.73,656.80,114.96,7.86"><p>http://www.xeno-canto.org/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,142.96,142.59,337.64,7.86;10,151.52,153.55,329.07,7.86;10,151.52,164.51,298.90,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,327.79,142.59,152.80,7.86;10,151.52,153.55,81.88,7.86">The intermediate matching kernel for image local features</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Boughorbel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Tarel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,257.49,153.55,67.12,7.86;10,359.35,153.55,121.24,7.86;10,151.52,164.51,148.26,7.86">IJCNN&apos;05. Proceedings. 2005 IEEE International Joint Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="889" to="894" />
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct coords="10,142.96,175.46,337.63,7.86;10,151.52,186.42,329.07,7.86;10,151.52,197.38,329.07,7.86;10,151.52,208.34,60.92,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,308.99,175.46,171.59,7.86;10,151.52,186.42,117.47,7.86">A new shared nearest neighbor clustering algorithm and its applications</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ertoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Steinbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,288.63,186.42,191.96,7.86;10,151.52,197.38,308.46,7.86">Workshop on Clustering High Dimensional Data and its Applications at 2nd SIAM International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="105" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,219.30,337.64,7.86;10,151.52,230.26,219.38,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,397.71,219.30,82.88,7.86;10,151.52,230.26,64.20,7.86">Lifeclef bird identification task 2015</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rauber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,237.07,230.26,83.66,7.86">CLEF working notes</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">2015</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,241.22,337.63,7.86;10,151.52,252.18,18.43,7.86" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="10,357.34,241.22,123.25,7.86">Lifeclef bird identification task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rauber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,263.14,337.63,7.86;10,151.52,274.09,312.46,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,263.64,263.14,216.94,7.86;10,151.52,274.09,57.43,7.86">Clustering using a similarity measure based on shared near neighbors</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A</forename><surname>Jarvis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Patrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,216.13,274.09,138.81,7.86">Computers, IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1025" to="1034" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,285.05,337.64,7.86;10,151.52,296.01,329.07,7.86;10,151.52,306.97,25.60,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,292.20,285.05,188.39,7.86;10,151.52,296.01,146.45,7.86">Hamming embedding and weak geometric consistency for large scale image search</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,305.54,296.01,100.61,7.86">Computer Vision-ECCV</title>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="304" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,317.93,337.63,7.86;10,151.52,328.89,329.07,7.86;10,151.52,339.85,50.43,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,240.86,317.93,200.66,7.86">A posteriori multi-probe locality sensitive hashing</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Buisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,463.04,317.93,17.56,7.86;10,151.52,328.89,269.74,7.86">Proceedings of the 16th ACM international conference on Multimedia</title>
		<meeting>the 16th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="209" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,350.81,337.64,7.86;10,151.52,361.77,161.35,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,240.99,350.81,140.82,7.86">Random maximum margin hashing</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Buisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,403.40,350.81,52.15,7.86">CVPR. IEEE</title>
		<meeting><address><addrLine>United States</addrLine></address></meeting>
		<imprint>
			<publisher>Colorado springs</publisher>
			<date type="published" when="2011-06">Jun 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,372.72,337.63,7.86;10,151.52,383.68,270.87,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,295.21,372.72,185.38,7.86;10,151.52,383.68,177.73,7.86">Instance-based bird species identication with undiscriminant features pruning-lifeclef 2014</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Champ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Buisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,350.60,383.68,43.13,7.86">CLEF2014</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,394.64,337.97,7.86;10,151.52,405.60,329.07,7.86;10,151.52,416.56,40.24,7.86;10,134.77,427.52,345.82,7.86;10,151.52,438.48,154.32,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,276.16,405.60,204.44,7.86;10,151.52,416.56,40.24,7.86;10,134.77,427.52,7.85,7.86;10,205.97,427.52,217.70,7.86">Lifeclef 2014: multimedia life species identification challenges 11</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,446.53,427.52,34.06,7.86;10,151.52,438.48,125.64,7.86">Working notes of CLEF 2014 conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>Large-scale identification of birds in audio recordings</note>
</biblStruct>

<biblStruct coords="10,142.62,449.44,337.98,7.86;10,151.52,460.40,272.23,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,353.32,449.44,127.27,7.86;10,151.52,460.40,176.14,7.86">Kernelizing spatially consistent visual matches for fine-grained classification</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Leveau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Buisson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,348.38,460.40,25.20,7.86">ICMR</title>
		<imprint>
			<biblScope unit="volume">2015</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,471.35,337.98,7.86;10,151.52,482.31,329.07,7.86;10,151.52,493.27,178.57,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,187.74,471.35,229.91,7.86">Mercer kernels for object recognition with local features</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,440.12,471.35,40.47,7.86;10,151.52,482.31,125.46,7.86;10,307.83,482.31,46.25,7.86">Computer Vision and Pattern Recognition</title>
		<title level="s" coord="10,361.10,482.31,119.49,7.86;10,151.52,493.27,27.93,7.86">IEEE Computer Society Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="223" to="229" />
		</imprint>
	</monogr>
	<note>CVPR 2005</note>
</biblStruct>

<biblStruct coords="10,142.62,504.23,337.98,7.86;10,151.52,515.19,295.46,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,270.04,504.23,210.56,7.86;10,151.52,515.19,206.05,7.86">Automatic large-scale classification of bird sounds is strongly improved by unsupervised feature learning</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Stowell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,364.52,515.19,22.55,7.86">PeerJ</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">488</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,526.15,337.98,7.86;10,151.52,537.11,329.07,7.86;10,151.52,548.07,25.60,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,353.93,526.15,65.16,7.86">The nbnn kernel</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,440.12,526.15,40.47,7.86;10,151.52,537.11,58.59,7.86;10,242.63,537.11,128.03,7.86">IEEE International Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="1824" to="1831" />
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
