<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,150.62,152.89,294.01,12.58;1,125.96,170.89,343.24,12.58;1,256.84,188.89,81.63,12.58">Fish identification in underwater video with deep convolutional neural network: SNUMedinfo at LifeCLEF fish task 2015</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,270.04,228.26,55.31,9.02"><forename type="first">Sungbin</forename><surname>Choi</surname></persName>
							<email>wakeup06@empas.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,150.62,152.89,294.01,12.58;1,125.96,170.89,343.24,12.58;1,256.84,188.89,81.63,12.58">Fish identification in underwater video with deep convolutional neural network: SNUMedinfo at LifeCLEF fish task 2015</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4D144A90F23EEB2439FC9860803ADC11</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Object detection</term>
					<term>Image classification</term>
					<term>Deep convolutional neural network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our participation at the LifeCLEF Fish task 2015. The task is about video-based fish identification. Firstly, we applied foreground detection method with selective search to extract candidate fish object window. Then deep convolutional neural network is used to classify fish species per window. Classification results are post-processed to produce final identification output. Experimental results showed effective performance in spite of challenging task condition. Our approach achieved best performance in this task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper, we describe the participation of the SNUMedinfo team at the LifeCLEF Fish task 2015. The purpose of task is automatically counting separate fish species in video segments. Training data includes video clips with annotation and sample images of 15 fish species. For a detailed introduction of the task, please see the overview paper of this task <ref type="bibr" coords="1,171.14,516.34,10.61,9.02" target="#b0">(1)</ref>.</p><p>In recent years, deep Convolutional Neural Network (CNN) has improved automatic image classification performance dramatically <ref type="bibr" coords="1,311.74,540.34,10.65,9.02" target="#b1">(2)</ref>. In this study, we experimented with GoogLeNet (3) which has shown effective performance in a recent ImageNet Challenge (4). Firstly, we applied foreground detection method with selective search to extract candidate fish object window (Section 2.1). CNN is trained and used to identify fish species in candidate window (Section 2.2). Then CNN classification results are further refined to produce final identification output (Section 2.3). Our experimental methods are detailed in the next section.   Then, we applied selective search <ref type="bibr" coords="3,261.88,334.88,11.64,9.02" target="#b4">(5)</ref> to extract candidate fish object window. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Candidate fish object window extraction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Foreground detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training CNN</head><p>We utilized GoogLeNet for image classification. GoogLeNet incorporates Inception module with the intention of increasing network depth with computational efficiency. Training CNN for fish identification started from GoogLeNet pretrained on ImageNet dataset. We finetuned CNN on fish identification training set (initial learning rate 0.001; batch_size:40).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Post-processing classification results</head><p>Filtering CNN output within each video segment CNN classified results from Section 2.2 contains lots of image windows overlapped to each other, so we need to select best matching window for final output. Firstly, among all target positive windows, we selected maximum 20 windows having top classification score from CNN. Secondly, windows having IoU more than 0.3 is considered as duplicate, so it is removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Refining classification output by utilizing temporally connected video segment</head><p>Video segments are temporally connected, so existing fish object in previous frame is expected to be located within nearby region in next frame. Based on this idea, we applied following two rules. Rule 1 (Adding): If video segment (k-1) and (k+1) has target positive fish object window in nearby geographic location, but video segment (k) does not have target fish object window in that location, then fish is expected to be in segment (k) also. Rule 2 (Removing): If video segment (k) has target positive fish object, but both video segment (k-1) and (k+1) does not have target fish object window in nearby location, then it is expected that fish is expected to be not in segment (k).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>In fish task test set, 73 video clips are given. We submitted three different runs. In SNUMedinfo1 and SNUMedinfo2, assigned 10 video clips in training set and validation set is switched (Section 2.2). SNUMedinfo3 is same as SNUMedinfo1, but Filtering CNN output within each video segment step (Section 2.3) is not applied. Evaluation metric for this task was counting score, precision and normalized counting score (For a detailed introduction to these evaluation metric, please see the overview paper of this task). Counting score is calculated based on the difference between the number of occurrences in the submitted run and the ground truth. Precision is calculated as number of true positive divided by number of true positive plus false positive. Normalized counting score is calculated as multiplication of counting score with precision. Evaluation results on test set is described in following table. But generally, our overall fish identification performance was very effective in spite of challenging task conditions of varying video images in underwater scene. Our counting score approached near 0.9 and precision exceed 0.8 (Table <ref type="table" coords="5,398.97,280.28,3.62,9.02" target="#tab_0">1</ref>).</p><p>Our post-processing step utilizing temporal neighborhood segment (Section 2.3) clearly improved performance when we compare run SNUMedinfo1 (with temporal post-processing step) to SNUMedinfo3 (without temporal post-processing step).</p><p>Technically, this method is simple compared to more advanced techniques such as ( <ref type="formula" coords="5,128.25,337.76,3.55,9.02">6</ref>), but it was very helpful for improving precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This fish tasks deals with underwater video image, so it was more challenging than general image classification task and additional steps were needed for pre-processing and post-processing. We combined foreground detection method with selective search for candidate fish object window detection. Then CNN pretrained on other general object classification task is trained to classify fish species. Outputs from CNN classification results are further refined to produce final identification results. In our future work, we'll explore other methodological options to find more effective method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,227.90,362.57,139.45,8.10;2,124.70,385.28,340.14,9.02;2,124.70,396.74,337.13,9.02;2,124.70,408.26,339.94,9.02;2,201.30,200.88,203.94,153.00"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Video segment image example Firstly, we tried to identify background region per each video clip. If a video clip has S temporal segments, each pixel location has corresponding S pixel values. Per each pixel location in video clip, we took median value as background pixel value (Fig 2).</figDesc><graphic coords="2,201.30,200.88,203.94,153.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,233.06,592.63,129.18,8.10;2,124.70,615.28,318.21,9.02;2,124.70,626.80,341.50,9.02;2,124.70,638.26,87.75,9.02;2,194.88,429.42,205.50,154.20"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Background image example Pixels having pixel values different from this background more than predefined threshold, is considered as foreground pixel. Bilateral filter is applied to smooth foreground image (Fig 3).</figDesc><graphic coords="2,194.88,429.42,205.50,154.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,234.02,312.17,127.20,8.10;3,193.38,147.42,208.08,156.06"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Foreground image example</figDesc><graphic coords="3,193.38,147.42,208.08,156.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="3,124.70,365.00,12.53,9.02;3,153.08,365.00,109.50,9.02;3,124.70,395.00,133.85,9.02;3,124.70,406.34,332.18,9.02;3,124.70,417.88,335.29,9.02;3,124.70,429.40,56.99,9.02;3,134.72,440.86,256.92,9.02;3,134.72,452.38,142.47,9.02;3,124.70,463.90,341.91,9.02;3,124.70,475.36,342.98,9.02;3,124.70,486.88,331.06,9.02;3,124.70,498.40,337.24,9.02;3,124.70,509.86,338.04,9.02;3,124.70,521.28,39.73,9.11"><head></head><label></label><figDesc>for CNN In fish task training set, there are 20 video clips with bounding box annotation, and samples images of 15 considered species. We formulated training set and validation set as follows.Training set: Samples images of 15 fish species + 10 video clips Validation set: Other 10 video clips Per each video clip, among candidate fish object windows extracted from section 2.1, windows having intersection over union area (IoU) over 0.7 with ground truth bounding box annotation is considered as target fish species positive example. Candidate fish object windows having IoU less than 0.2 is considered as negative example (No fish inside window). So we have 16 labels for image classification (15 fish species + 'No fish')</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,137.00,530.11,322.67,76.30"><head>Table 1 .</head><label>1</label><figDesc>Evaluation results of submitted runsCompared to other image recognition task such as ImageNet or LifeCLEF Plant task, this task deals with low quality underwater video. So our experiments involved additional pre-processing and post-processing step besides deep convolutional neural network training for image recognition. To further analyze contributions of each step with regard to the final performance, we need to experiment with various combinations of method options. We postpone thorough analysis of each step to future study when test set ground truth becomes available.</figDesc><table coords="4,137.00,547.90,322.67,58.52"><row><cell></cell><cell>Counting score</cell><cell>Precision</cell><cell>Normalized</cell></row><row><cell></cell><cell></cell><cell></cell><cell>counting score</cell></row><row><cell>SNUMedinfo1</cell><cell>0.89</cell><cell>0.81</cell><cell>0.72</cell></row><row><cell>SNUMedinfo2</cell><cell>0.89</cell><cell>0.80</cell><cell>0.71</cell></row><row><cell>SNUMedinfo3</cell><cell>0.85</cell><cell>0.71</cell><cell>0.60</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,132.70,523.57,337.82,8.10;5,141.74,534.61,171.75,8.10" xml:id="b0">
	<analytic>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>And San Juan</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Clef</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="5,367.70,523.57,72.77,8.10">Labs and Workshops</title>
		<title level="s" coord="5,446.58,523.57,23.94,8.10;5,141.74,534.61,108.58,8.10">CEUR Workshop Proceedings (CEUR</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,132.70,545.59,337.76,8.10;5,141.74,556.57,295.74,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,315.20,545.59,155.26,8.10;5,141.74,556.57,78.77,8.10">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,226.71,556.57,184.36,8.10">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,132.70,567.61,337.69,8.10;5,141.74,578.59,175.10,8.10" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<idno>arXiv:14094842. 2014</idno>
		<title level="m" coord="5,384.36,567.61,86.02,8.10;5,141.74,578.59,32.21,8.10">Going deeper with convolutions</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="5,132.70,589.57,337.96,8.10;5,141.74,600.61,219.34,8.10" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="5,373.81,589.57,96.84,8.10;5,141.74,600.61,76.31,8.10">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<idno>arXiv:14090575. 2014</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="5,132.70,611.59,337.72,8.10;5,141.74,622.57,189.47,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,371.72,611.59,98.70,8.10;5,141.74,622.57,42.43,8.10">Selective Search for Object Recognition</title>
		<author>
			<persName coords=""><forename type="first">Jrr</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kea</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Awm</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,190.26,622.57,60.26,8.10">Int J Comput Vis</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,132.70,633.61,337.91,8.10;5,141.74,644.59,328.78,8.10;5,141.74,655.57,96.25,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,306.09,633.61,164.51,8.10;5,141.74,644.59,54.41,8.10">The Shape-Time Random Field for Semantic Video Labeling</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kae</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Marlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,201.87,644.59,179.37,8.10;5,407.68,644.59,62.84,8.10;5,141.74,655.57,7.67,8.10">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014-06">2014. 2014. June 2014</date>
			<biblScope unit="page" from="23" to="28" />
		</imprint>
	</monogr>
	<note>IEEE Conference on</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
