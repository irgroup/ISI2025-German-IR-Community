<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,145.79,115.96,323.77,12.62;1,243.93,133.89,127.49,12.62">Fine-tuning Deep Convolutional Networks for Plant Recognition</title>
				<funder>
					<orgName type="full">NVIDIA Corporation</orgName>
				</funder>
				<funder>
					<orgName type="full">BIGDATA SOLUTIONS S.A</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,175.97,172.01,67.44,8.74"><forename type="first">Angie</forename><forename type="middle">K</forename><surname>Reyes</surname></persName>
							<email>angreyes@uan.edu.co</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratory for Advanced Computational Science and Engineering Research</orgName>
								<orgName type="institution">Universidad Antonio Nariño</orgName>
								<address>
									<country key="CO">Colombia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,253.97,172.01,72.09,8.74"><forename type="first">Juan</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
							<email>juanc.caicedor@konradlorenz.edu.co</email>
							<affiliation key="aff1">
								<orgName type="institution">Fundación Universitaria Konrad Lorenz</orgName>
								<address>
									<country key="CO">Colombia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,355.99,172.01,78.92,8.74"><forename type="first">Jorge</forename><forename type="middle">E</forename><surname>Camargo</surname></persName>
							<email>jorgecamargo@uan.edu.co</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratory for Advanced Computational Science and Engineering Research</orgName>
								<orgName type="institution">Universidad Antonio Nariño</orgName>
								<address>
									<country key="CO">Colombia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,145.79,115.96,323.77,12.62;1,243.93,133.89,127.49,12.62">Fine-tuning Deep Convolutional Networks for Plant Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">847EF3801019439909E65255EA167D9A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Plant recognition</term>
					<term>deep learning</term>
					<term>convolutional neural networks</term>
					<term>image retrieval</term>
					<term>imageCLEF</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of the ECOUAN team in the LifeCLEF 2015 challenge. We used a deep learning approach in which the complete system was learned without hand-engineered components. We pre-trained a convolutional neural network using 1.8 million images and used a fine-tuning strategy to transfer learned recognition capabilities from general domains to the specific challenge of Plant Identification task. The classification accuracy obtained by our method outperformed the best result obtained in 2014. Our group obtained the 4th position among all teams and the 10th position among 18 runs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Systems that assist professionals to recognize the species and categories of plants are important. The ImageCLEF challenge includes a task for plant identification that brings together researchers to study the problem and propose and exchange ideas and methods.</p><p>As in any other image recognition task, plant identification relies on computational methods to extract discriminative features from images. Features have been traditionally hand-crafted or hand-engineered. However, a recent trend in machine learning has demonstrated that learned representations are more effective and efficient. The main advantage of representation learning is that algorithms automatically analyze large collections of images and identify features that can categorize images with minimum error. How can we adapt these strategies for plant identification.</p><p>In this work we propose to use deep Convolutional Neural Networks (CNNs) to extract features and classify images at the same time. The proposed CNN is pre-trained in a large collection of generic web images and fine-tuned to recognize plants with the highest possible accuracy. Our approach is an end-to-end learning strategy, with minimum assumptions about the contents of images.</p><p>Using our strategy we achieved 0,487 precision in the main challenge of Life-CLEF 2015 improving the best result obtained by IBM Research Australia in LifeCLEF 2014 <ref type="bibr" coords="2,205.70,142.90,14.61,8.74" target="#b9">[10]</ref>. We submitted one entry that obtained the 10th best run and we obtained the 4th position among all the team participants.</p><p>Several groups participated in the LifeCLEF 2014 challenge, with a variety of techniques and results. These strategies used by the 3 best teams were: <ref type="bibr" coords="2,467.86,183.26,12.73,8.74" target="#b0">(1)</ref> The IBM Research Australia team <ref type="bibr" coords="2,294.75,195.22,15.50,8.74" target="#b9">[10]</ref> proposed a method based on the extraction of visual features (Fisher Vector) and linear classifiers as classification strategy. They also followed a Deep Learning approach in one of the runs; <ref type="bibr" coords="2,467.86,219.13,12.73,8.74" target="#b1">(2)</ref> The INRIA Pl@ntNet team <ref type="bibr" coords="2,260.08,231.08,10.52,8.74" target="#b5">[6]</ref> proposed a fusion strategy that combines a set of local descriptors to represent images and KNN as classification strategy; and</p><p>(3) The BME TMIT team <ref type="bibr" coords="2,252.45,254.99,10.52,8.74" target="#b3">[4]</ref> proposed a strategy based on dense SIFT for feature detection and description, and a Gaussian Mixture Model based on Fisher Vector. They used the C-support vector classification algorithm as classification method.</p><p>The rest of this paper is organized as follows: section 2 presents the plant identification task; section 3 describes the details of our approach; section 4 presents conducted experiments and obtained results; and section 5, concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Plant Identification Task</head><p>The Plant Identification task is oriented to recognize the species of a plant given an observation. An observation is a set of images (1 to 5) that capture the appearance of the plant from different perspectives or points of view. These points of view include entire plant, branch, leaf, fruit, flower, stem or leaf scan. It is important to note that images of an observation were acquired by the same person the same day. Images of an observation belong to one of 1,000 possible species (last year the total number of species were 500).</p><p>The plant identification task was based on the Pl@ntView dataset. It focuses on 1,000 herb, tree and fern species centered on France and neighboring countries, which contains 113,205 pictures. Contrary to previous plant identification challenges, queries are not defined as single images but as plant observations. That is to say, a query is composed of 1 to 5 images belonging to the same plant species.</p><p>Each participating team was allowed to submit up to 4 runs built from different methods. Semi-supervised and interactive approaches, particularly for segmenting leaves from the background, were allowed but were compared independently from fully automatic methods. Any human assistance in the processing of the test queries had therefore to be signaled in the submitted runs.</p><p>We designed a plant identification system using deep learning at its core. The proposed system is learned end-to-end, without hand-engineered components. This section presents the computational details of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Deep Convolutional Networks</head><p>A Convolutional Neural Network (CNN) is a stack of non-linear transformation functions that are learned from data. CNNs were originally proposed in the 1980's for digit recognition <ref type="bibr" coords="3,256.50,237.65,9.96,8.74" target="#b8">[9]</ref>, and have been recently revisited for large scale recognition problems. The success of modern CNNs relies on several factors that include: availability of large datasets, more computing power and new ideas and algorithms. Among the most successful ideas that make CNNs a powerful tool for image recognition nowadays is the concept of deep architectures <ref type="bibr" coords="3,432.54,285.47,10.52,8.74" target="#b7">[8,</ref><ref type="bibr" coords="3,444.72,285.47,11.62,8.74" target="#b10">11]</ref>. A deep CNN has multiple layers that progressively compute features from input images. The deep architecture proposed by Krishevsky et al. <ref type="bibr" coords="3,434.11,536.57,10.52,8.74" target="#b7">[8]</ref> demonstrated the power of deep CNNs for the first time in a large scale image classification setting. There are mainly two types of layers in this network: convolutional layers and fully connected layers. Convolutional layers may be understood as banks of filters that transform an input image into another image, highlighting specific patterns. On the other hand, fully connected layers take a vector as input and produce another vector as output. Figure <ref type="figure" coords="3,361.63,608.30,4.98,8.74" target="#fig_0">1</ref> illustrates the concept of convolutional and fully connected layers.</p><p>In our work, we use a CNN following the architecture proposed by Krishevsky et al. <ref type="bibr" coords="3,159.71,644.16,10.52,8.74" target="#b7">[8]</ref> which has 5 convolutional layers and 2 fully-connected layers. An additional prediction layer is added to the top of the network to obtain classification scores given the learned image representation. This network is illustrated in Figure <ref type="figure" coords="4,166.86,130.95,3.87,8.74" target="#fig_1">2</ref>. The entire network is learned from data using the back-propagation algorithm. We pre-train this network on the ILSVRC 2012 dataset <ref type="bibr" coords="4,437.88,142.90,9.96,8.74" target="#b1">[2]</ref>, which consists of 1.2 million images of 1,000 different categories. In this way, the network is initialized with a powerful and generic set of features that can recognize a variety of objects in images with low error rate. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fine-tuning the CNN</head><p>We initialized the CNN to recognize 1,000 categories of generic objects that are part of the ImageNet hierarchy following the procedure described in the previous section. Then, we proceed to finetune the network for the Plant Identification task.</p><p>Fine-tuning a network is a procedure based on the concept of transfer learning <ref type="bibr" coords="4,134.77,523.73,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="4,146.95,523.73,7.01,8.74" target="#b2">3]</ref>. We start training a CNN to learn features for a broad domain with a classification function targeted at minimizing error in that domain. Then, we replace the classification function and optimize the network again to minimize error in another, more specific domain. Under this setting, we are transferring the features and the parameters of the network from the broad domain to the specific one.</p><p>The classification function in the original CNN is a softmax classifier that computes the probability of 1,000 classes of the ImageNet dataset. To start the fine-tuning procedure, we remove this softmax classifier and initialize a new one with random values. The new softmax classifier is trained from scratch using the back-propagation algorithm with data from the Plant Identification task, which also has 1,000 different categories. In order to start the back-propagation algorithm for fine-tuning, it is key to set the learning rates of each layer appropriately. The classification layer, i.e., the new softmax classifier, needs a large learning rate because it has been just initialized with random values. The rest of the layers need a relatively small learning rate because we want to preserve the parameters of the previous network to transfer that knowledge into the new network. However, notice that the learning rate is not set to zero in the rest of the layers: they will be optimized again at a slower pace.</p><p>In our experiments we set the learning rate of the top classification layer to 10, while leaving the learning rate of all the other seven layers to 0.1. We run the back-propagation algorithm for 50,000 iterations, which optimizes the network parameters using stochastic gradient descent (SGD). Figure <ref type="figure" coords="5,445.88,514.23,4.98,8.74" target="#fig_2">3</ref> shows how the precision of classifying single images improves with more iterations. Our implementation is based on the open source Deep Learning library Caffe <ref type="bibr" coords="5,134.77,550.10,9.96,8.74" target="#b6">[7]</ref>, and we run the experiments using a NVIDIA Titan Z GPU (5,760 cores and 12 GB of RAM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Prediction on Observations</head><p>An observation in the Plant Identification task is a set of 3 to 5 images on average in the training set. The system is required to produce a single prediction given these images, and the fine-tuned CNN discussed before is able to make predictions for a single input image. Therefore, we need to aggregate predictions from multiple images to produce a single output for the whole observation.</p><p>We used a simple combination strategy. The output of the fine-tuned network is a 1,000-dimensional vector with probability values of all categories, and the input is a single image. To combine the predictions of all images in an observation we compute the sum of all predicted vectors. This can be understood as a voting scheme, as we aggregate evidence from multiple images in a single vector without any further processing step. Finally, we assign the category to the input observation by identifying the class with maximum score in the aggregated vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training and Test Datasets</head><p>We used the ILSVRC 2012 dataset to pre-train the proposed CNN. This dataset is composed of 1.2 million images of 1,000 different object categories. To finetune the network, we used the released training set of LifeCLEF 2015 <ref type="bibr" coords="6,439.04,272.94,9.96,8.74" target="#b4">[5]</ref>, which has 91,759 images distributed in 13,887 plant-observation-queries with examples of 1,000 species that include trees, herbs, and ferns, among others.</p><p>The test set is composed of 21,446 images organized in 13,887 observations: an average of 1.5 images per observation. Images in the training and test sets may have been taken from 7 different views. These 7 views are: entire plant, branch, flower, fruit, leaf, stem and scans. An observation can have one of many of these views taken by the same person. Table <ref type="table" coords="6,161.38,544.02,4.98,8.74" target="#tab_0">1</ref> presents the distribution of images in the training and test sets according to viewpoint, and Figure <ref type="figure" coords="6,246.18,555.97,4.98,8.74" target="#fig_3">4</ref> shows some example pictures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>We submit one run with the results of the proposed system. We only trained and fine-tuned one network with the architecture described in previous sections. To adjust the hyper-parameters of the network, we separated a validation set with 1% of the training images. After identifying the correct parameters in the validation set, we fine-tuned the network again with all images.  The results of our experiment are presented in Figure <ref type="figure" coords="8,395.70,118.99,3.87,8.74" target="#fig_4">5</ref>, which shows the accuracy of our method in each of the 7 possible views of a plant. The proposed system can categorize images of flowers and leaf scans with higher accuracy than the rest of the views. Our system obtained an average precision of 0.486 when classifying single images in the test set.</p><p>Table <ref type="table" coords="8,176.88,178.77,4.98,8.74" target="#tab_1">2</ref> compares our result with results of other participating groups in the challenge. This table presents the best experiment of each group with the scores obtained for single image classification and full observation identification. Our run was placed 10 among 18 total experiments, obtaining the 4th place for our group in the competition.</p><p>Notice that our experiment involved no special prior knowledge about plants or hand-engineered methods tailored for plant identification. We learned an image classification system end-to-end, using only publicly available training data. Our result is significantly better than the next result in the table, obtaining 42% better accuracy. Also, our result is 20% worse than the result above, explained mainly by the absence of plant specific strategies in our pipeline. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>This paper presented a system for plant identification based on Deep Convolutional Neural Networks. The proposed training strategy allows the system to learn all layers end-to-end from data, without involving techniques that are specific to plants. We believe that involving more domain knowledge in the design of the system can be beneficial to improve accuracy. x The fine-tuning strategy demonstrated to be a good solution to transfer learned recognition capabilities from general domains to the specific challenge of Plant Identification task. This is useful to take advantage of big visual data available on the Internet, and then transfer general recognition abilities to specific domains. In our future work we plan to evaluate deeper architectures of the CNN and adaptations of domain specific techniques to improve performance even further.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,134.77,449.02,345.83,7.89;3,134.77,460.01,345.83,7.86;3,134.77,470.97,345.82,7.86;3,134.77,481.93,345.82,7.86;3,134.77,316.96,345.83,117.30"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Illustration of the types of layers in a CNN. A convolutional layer is a bank of n learned filters that are applied to an input image. The output is another image with the response of the n filters. A fully-connected layer takes a vector as input and transforms it with a non-linear function with parameters in the corresponding connection matrix.</figDesc><graphic coords="3,134.77,316.96,345.83,117.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,134.77,327.89,345.83,7.89;4,134.77,338.88,345.83,7.86;4,134.77,349.83,345.83,7.86;4,134.77,360.79,345.83,7.86;4,134.77,371.75,155.67,7.86;4,134.77,211.96,345.84,101.16"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Architecture of the CNN used in this work: 5 convolutional layers (conv), 2 fully-connected (fc) layers and 1 classification layer (softmax). Colored squares show the size of the output in the corresponding layer. Small squares with a downwards arrow indicate the size of filters. Diagonal arrows indicate the number of channels in the output of each convolutional layer.</figDesc><graphic coords="4,134.77,211.96,345.84,101.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,134.77,318.90,345.82,7.89;5,134.77,329.88,345.83,7.86;5,134.77,340.84,90.97,7.86;5,203.93,115.84,207.50,188.29"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Evolution of image classification accuracy in a validation set during the finetuning process. Accuracy improves quickly during the first iterations and stabilizes after 40,000 iterations.</figDesc><graphic coords="5,203.93,115.84,207.50,188.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,164.26,269.77,286.83,7.89;7,134.77,173.61,345.83,81.39"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Example images of the seven views taken from the training set.</figDesc><graphic coords="7,134.77,173.61,345.83,81.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,161.73,587.29,291.90,7.89;7,186.64,411.14,242.08,161.39"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Accuracy of our result in the test set by each of the seven views.</figDesc><graphic coords="7,186.64,411.14,242.08,161.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,157.00,351.39,301.35,119.83"><head>Table 1 .</head><label>1</label><figDesc>Distribution of images in the Plant dataset for training and test.</figDesc><table coords="6,240.05,372.16,133.86,99.06"><row><cell>View</cell><cell>Training</cell><cell>Test</cell></row><row><cell>Branch</cell><cell>8,130</cell><cell>2,088</cell></row><row><cell>Entire</cell><cell>16,235</cell><cell>6,113</cell></row><row><cell>Flower</cell><cell>28,225</cell><cell>8,327</cell></row><row><cell>Fruit</cell><cell>7,720</cell><cell>1,423</cell></row><row><cell>Leaf</cell><cell>13,367</cell><cell>2,690</cell></row><row><cell>Stem</cell><cell>5,476</cell><cell>584</cell></row><row><cell>Scans</cell><cell>12,605</cell><cell>221</cell></row><row><cell>Total</cell><cell>91,758</cell><cell>21,446</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,134.77,329.35,345.83,130.39"><head>Table 2 .</head><label>2</label><figDesc>Best result of each participating group in the 2015 challenge (accuracy). Our experiment was number 10 out of 18 submitted runs, leaving our group in the 4th place in the competition.</figDesc><table coords="8,170.88,372.04,272.20,87.70"><row><cell cols="4">Research group Run name Single Image Observation</cell></row><row><cell>SNUMED INFO</cell><cell>Run 4</cell><cell>0,652</cell><cell>0,667</cell></row><row><cell>QUT RV</cell><cell>Run 3</cell><cell>0,590</cell><cell>0,624</cell></row><row><cell>INRIA ZENTH</cell><cell>Run 1</cell><cell>0,581</cell><cell>0,609</cell></row><row><cell>ECOUAN (ours)</cell><cell>Run 1</cell><cell>0,486</cell><cell>0,487</cell></row><row><cell>MICA</cell><cell>Run 2</cell><cell>0,194</cell><cell>0,209</cell></row><row><cell>SABANCI</cell><cell>Run 1</cell><cell>0,153</cell><cell>0,160</cell></row><row><cell>UAIC</cell><cell>Run 1</cell><cell>0,013</cell><cell>0,013</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6">Acknowledgements</head><p>We gratefully acknowledge the support of <rs type="funder">NVIDIA Corporation</rs> with the donation of the Titan Z GPU used in this paper. This research was partially funded by <rs type="funder">BIGDATA SOLUTIONS S.A</rs>.S.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,142.96,222.29,337.63,7.86;9,151.52,233.25,329.07,7.86;9,151.52,244.21,48.64,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,198.02,222.29,278.70,7.86">Deep learning of representations for unsupervised and transfer learning</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,151.52,233.25,282.50,7.86">Unsupervised and Transfer Learning Challenges in Machine Learning</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,255.17,337.63,7.86;9,151.52,266.12,329.07,7.86;9,151.52,277.08,246.28,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,387.38,255.17,93.21,7.86;9,151.52,266.12,109.79,7.86">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,282.66,266.12,169.69,7.86;9,151.52,277.08,123.28,7.86">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference</note>
</biblStruct>

<biblStruct coords="9,142.96,288.04,337.63,7.86;9,151.52,299.00,329.07,7.86;9,151.52,309.96,128.82,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="9,151.52,299.00,300.08,7.86">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.1531</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,142.96,320.92,337.63,7.86;9,151.52,331.88,329.07,7.86;9,151.52,342.84,77.69,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,272.01,320.92,208.58,7.86;9,151.52,331.88,120.28,7.86">Viepoints combined classification method in imagebased plant identification task</title>
		<author>
			<persName coords=""><forename type="first">Gbor</forename><surname>Szcs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dvid</forename><surname>Papp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,292.71,331.88,110.19,7.86">Working notes ImageCLEF</title>
		<imprint>
			<publisher>ImageCLEF</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="763" to="770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,353.80,337.64,7.86;9,151.52,364.75,111.17,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,283.15,353.80,148.48,7.86">Lifeclef plant identication task 2015</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J</forename><surname>Herve Goau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,455.89,353.80,24.70,7.86;9,151.52,364.75,54.51,7.86">CLEF working notes</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,375.71,337.63,7.86;9,151.52,386.67,328.29,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,273.23,375.71,207.36,7.86;9,151.52,386.67,38.44,7.86">Plantnet participation at lifeclef2014 plant identificatin task</title>
		<author>
			<persName coords=""><forename type="first">Herv</forename><surname>Goau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexis</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,210.96,386.67,110.27,7.86">Working notes ImageCLEF</title>
		<imprint>
			<publisher>ImageCLEF</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="724" to="737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,397.63,337.63,7.86;9,151.52,408.59,329.07,7.86;9,151.52,419.55,329.07,7.86;9,151.52,430.51,50.43,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,237.79,408.59,238.19,7.86">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,165.05,419.55,258.44,7.86">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,441.47,337.63,7.86;9,151.52,452.43,329.07,7.86;9,151.52,463.38,86.01,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,328.09,441.47,152.50,7.86;9,151.52,452.43,103.94,7.86">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,275.64,452.43,200.74,7.86">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,474.34,337.63,7.86;9,151.52,485.30,329.07,7.86;9,151.52,496.26,136.69,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,204.14,485.30,243.06,7.86">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,453.69,485.30,26.90,7.86;9,151.52,496.26,50.69,7.86">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,507.22,337.97,7.86;9,151.52,518.18,329.07,7.86;9,151.52,529.14,25.60,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,306.98,507.22,173.61,7.86;9,151.52,518.18,70.61,7.86">Ibm research australia at lifeclef2014: Plant identification task</title>
		<author>
			<persName coords=""><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mani</forename><surname>Abedini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">G X L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,242.45,518.18,109.59,7.86">Working notes ImageCLEF</title>
		<imprint>
			<publisher>ImageCLEF</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="693" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,540.10,337.98,7.86;9,151.52,551.06,329.07,7.86;9,151.52,562.01,93.19,7.86" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<title level="m" coord="9,284.72,551.06,128.81,7.86">Going deeper with convolutions</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
