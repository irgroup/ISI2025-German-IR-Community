<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,164.42,115.96,286.53,12.62">LifeCLEF Plant Identification Task 2015</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,199.25,153.66,56.55,8.74"><forename type="first">Hervé</forename><surname>Goëau</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inria ZENITH team</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,266.36,153.66,60.95,8.74"><forename type="first">Pierre</forename><surname>Bonnet</surname></persName>
							<email>pierre.bonnet@cirad.fr</email>
							<affiliation key="aff2">
								<orgName type="laboratory">CIRAD</orgName>
								<orgName type="institution">UMR AMAP</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,357.23,153.66,48.07,8.74"><forename type="first">Alexis</forename><surname>Joly</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inria ZENITH team</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">LIRMM</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,164.42,115.96,286.53,12.62">LifeCLEF Plant Identification Task 2015</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8017360574398E6E2F1A0A36BA7F5AC2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>LifeCLEF</term>
					<term>plant</term>
					<term>leaves</term>
					<term>leaf</term>
					<term>flower</term>
					<term>fruit</term>
					<term>bark</term>
					<term>stem</term>
					<term>branch</term>
					<term>species</term>
					<term>retrieval</term>
					<term>images</term>
					<term>collection</term>
					<term>species identification</term>
					<term>citizen-science</term>
					<term>fine-grained classification</term>
					<term>evaluation</term>
					<term>benchmark</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The LifeCLEF plant identification challenge aims at evaluating plant identification methods and systems at a very large scale, close to the conditions of a real-world biodiversity monitoring scenario. The 2015 evaluation was actually conducted on a set of more than 100K images illustrating 1000 plant species living in West Europe. The main originality of this dataset is that it was built through a large-scale participatory sensing plateform initiated in 2011 and which now involves tens of thousands of contributors. This overview presents more precisely the resources and assessments of the challenge, summarizes the approaches and systems employed by the participating research groups, and provides an analysis of the main outcomes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image-based approaches are nowadays considered to be one of the most promising solution to help bridging the botanical taxonomic gap, as discussed in <ref type="bibr" coords="1,453.43,461.03,15.50,8.74" target="#b20">[21]</ref> or <ref type="bibr" coords="1,134.77,472.98,15.50,8.74" target="#b15">[16]</ref> for instance. We therefore see an increasing interest in this trans-disciplinary challenge in the multimedia community (e.g. in <ref type="bibr" coords="1,343.23,484.94,14.61,8.74" target="#b13">[14]</ref>, <ref type="bibr" coords="1,364.67,484.94,9.96,8.74" target="#b5">[6]</ref>, <ref type="bibr" coords="1,381.14,484.94,14.61,8.74" target="#b18">[19]</ref>, <ref type="bibr" coords="1,402.57,484.94,14.61,8.74" target="#b23">[24]</ref>, <ref type="bibr" coords="1,424.01,484.94,14.61,8.74" target="#b14">[15]</ref>, <ref type="bibr" coords="1,445.46,484.94,10.30,8.74" target="#b1">[2]</ref>). Beyond the raw identification performances achievable by state-of-the-art computer vision algorithms, the visual search approach offers much more efficient and interactive ways of browsing large floras than standard field guides or online web catalogs. Smartphone applications relying on such image-based identification services are particularly promising for setting-up massive ecological monitoring systems, involving hundreds of thousands of contributors, with different levels of expertise, and at a very low cost.</p><p>Noticeable progress in this way was achieved by several projects and apps like LeafSnap 4 <ref type="bibr" coords="1,200.63,592.57,14.61,8.74" target="#b20">[21]</ref>, PlantNet 5 , 6 <ref type="bibr" coords="1,276.22,592.57,14.61,8.74" target="#b15">[16]</ref>, or Folia 7 . But as promising as these applications are, their performances are however still far from the requirements of a real-world social-based ecological surveillance scenario. Allowing the mass of citizens to produce accurate plant observations requires to equip them with much more accurate identification tools. Measuring and boosting the performances of content-based identification tools is therefore crucial. This was precisely the goal of the ImageCLEF<ref type="foot" coords="2,215.54,165.24,3.97,6.12" target="#foot_0">8</ref> plant identification task organized since 2011 in the context of the worldwide evaluation forum CLEF <ref type="foot" coords="2,316.29,177.20,3.97,6.12" target="#foot_1">9</ref> (see <ref type="bibr" coords="2,341.15,178.77,14.61,8.74" target="#b11">[12]</ref>, <ref type="bibr" coords="2,363.13,178.77,14.61,8.74" target="#b12">[13]</ref>, <ref type="bibr" coords="2,385.12,178.77,15.50,8.74" target="#b16">[17]</ref> and <ref type="bibr" coords="2,424.11,178.77,15.50,8.74" target="#b17">[18]</ref> for more details).</p><p>Contrary to previous evaluations reported in the literature, the key objective of the PlantCLEF challenge has always been to build a realistic task close to real-world conditions (with many different contributors, cameras, areas, periods of the year, individual plants, etc.). This was initially achieved through a citizen science initiative that began 5 years ago, in the context of the Pl@ntNet project, in order to boost the production of plant images in close collaboration with the Tela Botanica social network. The evaluation dataset was enriched every year with new contributions and progressively diversified with different input feeds (annotation and cleaning of older data, contributions made through Pl@ntNet mobile applications). The plant task of LifeCLEF 2015 was directly in the continuity of this effort. Main novelties compared to the last year were:</p><p>the doubling of the number species, i.e. 1000 species instead of 500 the possibility to use external training data at the condition that the experiment is entirely re-producible</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset</head><p>More precisely, PlantCLEF 2015 dataset is composed of 113,205 pictures belonging to 41,794 observations of 1000 species of trees, herbs and ferns living in Western European regions. This data was collected by 8,960 distinct contributors. Each picture belongs to one and only one of the 7 types of views reported in the meta-data (entire plant, fruit, leaf, flower, stem, branch, leaf scan) and is associated with a single plant observation identifier allowing to link it with the other pictures of the same individual plant (observed the same day by the same person). It is noticeable that most image-based identification methods and evaluation data proposed in the past were so far based on leaf images (e.g. in <ref type="bibr" coords="2,134.77,530.17,14.61,8.74" target="#b20">[21]</ref>, <ref type="bibr" coords="2,156.37,530.17,9.96,8.74" target="#b2">[3]</ref>, <ref type="bibr" coords="2,173.00,530.17,10.52,8.74" target="#b5">[6]</ref> or in the more recent methods evaluated in <ref type="bibr" coords="2,378.94,530.17,14.76,8.74" target="#b12">[13]</ref>). Only few of them were focused on flower's images as in <ref type="bibr" coords="2,301.80,542.12,15.50,8.74" target="#b24">[25]</ref> or <ref type="bibr" coords="2,333.58,542.12,9.96,8.74" target="#b0">[1]</ref>. Leaves are far from being the only discriminant visual key between species but, due to their shape and size, they have the advantage to be easily observed, captured and described. More diverse parts of the plants however have to be considered for accurate identification, especially because it is not possible for many plant to see their leaves all over the year. An originality of PlantCLEF dataset is that its "social nature" makes it closer to the conditions of a real-world identification scenario: (i) images of the Fig. <ref type="figure" coords="3,154.40,408.84,4.13,7.89">1</ref>. Examples of PlantCLEF pictures with decreasing averaged users ratings for the different types of views same species are coming from distinct plants living in distinct areas, (ii) pictures are taken by different users that might not used the same protocol of image acquisition, (iii) pictures are taken at different periods in the year. Each image of the dataset is associated with contextual meta-data (author, date, locality name, plant id) and social data (user ratings on image quality, collaboratively validated taxon name, vernacular name) provided in a structured xml file. The gps geo-localization and device settings are available only for some of the images. Table <ref type="table" coords="3,162.30,538.05,4.98,8.74" target="#tab_0">1</ref> gives some examples of pictures with decreasing averaged users ratings for the different types of views. Note that the users of the specialized social network creating these ratings (Tela Botanica) are explicitly asked to rate the images according to their plant identification ability and their accordance to the pre-defined acquisition protocol for each view type. This is not an aesthetic or general interest judgement as in most social image sharing sites.</p><p>To sum up each image is associated with the followings meta-data:</p><p>-ObservationId: the plant observation ID from which several pictures can be associated -FileName</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task Description</head><p>The challenge was evaluated as a plant species retrieval task based on multiimage plant observation queries. The goal was to retrieve the correct plant species among the top results of a ranked list of species returned by the evaluated system. Contrary to previous plant identification benchmarks, queries were not defined as single images but as plant observations, meaning a set of one to several images depicting the same individual plant, observed by the same person, the same day, with the same device. Each image of a query observation is associated with a single view type (entire plant, branch, leaf, fruit, flower, stem or leaf scan) and with contextual meta-data (data, location, author).</p><p>The whole PlantCLEF dataset was split in two parts, one for training (and/or indexing) and one for testing. All observations with pictures used in the previous plant identification tasks were directly integrated in the training dataset. Then for the new observations and pictures, in order to guarantee that most of the time each species contained more images in the training dataset than in the test dataset, we used a constrained random rule for putting with priority observations with more distinct organs and views in the training dataset. The test set was built by choosing 1/2 of the observations of each species with this constrained random rule, whereas the remaining observations were kept in the reference training set. Thus, 1/3 of the pictures are in the test dataset (see Table <ref type="table" coords="5,397.11,142.90,4.98,8.74" target="#tab_0">1</ref> for more detailed stats). The xml files containing the meta-data of the query images were purged so as to erase the taxon names (the ground truth) and the image quality ratings (that would not be available at query stage in a real-world application). Metadata of the observations in the training set are kept unaltered. As a novelty this year, participants to the challenge were allowed to use external training data at the condition that (i) the experiment is entirely reproducible, i.e. that the used external resource is clearly referenced and accessible to any other research group in the world, (ii) participants submit at least one run without external training data so that we can study the contribution of such resources, (iii) the additional resource does not contain any of the test observations. It was in particular strictly forbidden to crawl training data from the following domain names: http://ds.plantnet-project.org/ http://www.tela-botanica.org http://identify.plantnet-project.org http://publish.plantnet-project.org/ http://www.gbif.org/ In practice, each candidate system was evaluated through the submission of a run, i.e. a file containing a set of ranked lists of species (each list corresponding to one query observation and being sorted according to the confidence score of the system in the suggested species). Each participating group was allowed to submit up to 4 runs built from different methods. The metric used to evaluate the submitted runs is an extension of the mean reciprocal rank <ref type="bibr" coords="5,418.28,536.75,15.50,8.74" target="#b28">[29]</ref> classically used in information retrieval. The difference is that it is based on a two-stage averaging rather than a flat averaging such as:</p><formula xml:id="formula_0" coords="5,258.36,579.34,222.23,30.31">S = 1 U U u=1 1 P u Pu p=1 1 r u,p<label>(1)</label></formula><p>where U is the number of users (within the test set), P u the number of individual plants observed by the u-th user (within the test set), r u,p is the rank of the correct species within the ranked list of species returned by the evaluated system (for the p-th observation of the u-th user). Note that if the correct species does not appear in the returned list, its rank r u,p is considered as infinite. Overall, the proposed metric allows compensating the long-tail distribution effects occurring in social data. In most social networks, few people actually produce huge quantities of data whereas a vast majority of users (the long tail) produce much less data. If, for instance, only one person did collect an important percentage of the images, the classical mean reciprocal rank over a random set of queries would be strongly influenced by the images of that user to the detriment of the users who only contributed with few pictures. This is a problem for several reasons: (i) the persons who produce the more data are usually the most expert ones but not the most representative of the potential users of the automatic identification tools. (ii) The large number of the images they produce makes the classification of their observations easier because they tend to follow the same protocol for all their observations (same device, same position of the plant in the images, etc.) (iii) The images they produce are also usually of better quality so that their classification is even easier.</p><p>A secondary metric was used to evaluate complementary (but not mandatory) runs providing species prediction at the image level (and not at the observation level). The evaluation metric in that is expressed as:</p><formula xml:id="formula_1" coords="6,233.52,343.01,247.07,31.28">S = 1 U U u=1 1 P u Pu p=1 1 N u,p Nu,p n=1 1 r u,p,n<label>(2)</label></formula><p>where U is the number of users, P u the number of individual plants observed by the u-th user, N u,p the number of pictures of the p-th plant observation of the u-th user, r u,p,n is the rank of the correct species within the ranked list of images returned by the evaluated system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Participants and methods</head><p>123 research groups worldwide registered to LifeCLEF plant challenge 2015 in order to download the dataset. Among this large raw audience, 7 research groups succeeded in submitting runs on time and 6 of them submitted a technical report describing in details their system. Participants were mainly academics, specialized in computer vision, machine learning and multimedia information retrieval. We list below the participants and give a brief overview of the techniques used in their runs. We remind here that LifeCLEF benchmark is a system-oriented evaluation and not a deep or fine evaluation of the underlying algorithms. Readers interested by the scientific and technical details of any of these methods should refer to the LifeCLEF 2015 working notes of each participant (referenced below):</p><p>EcoUAN (1 run) <ref type="bibr" coords="6,222.68,608.27,16.80,8.77" target="#b26">[27]</ref>, Colombia. This participant used a deep learning approach based on a Convolutional Neural Network (CNN). They used the CNN architecture introduced in <ref type="bibr" coords="6,252.31,632.21,15.50,8.74" target="#b19">[20]</ref> and that was pre-trained using the popular Im-ageNet image collection <ref type="bibr" coords="6,242.46,644.16,14.61,8.74" target="#b9">[10]</ref>. A tuning process was conducted to train the last layer using PlantCLEF training set. The classification at the observation level was done using a sum pooling mechanism based on the individual images classification.</p><p>INRIA-ZENITH (3 runs) <ref type="bibr" coords="7,269.30,166.78,11.46,8.77" target="#b6">[7]</ref>, France. MICA (3 runs) <ref type="bibr" coords="7,214.09,358.07,16.80,8.77" target="#b21">[22]</ref>, VietNam. This participant used different hand-crafted visual features for the different view types and trained support vector machines for the classification. The hand-crafted visual features mainly differ in the way the main region of interest is selected before extracting the features:</p><p>-For leaf scans, fruit and flower images: automatic selection of a region of interest by using salient features and mean-shift algorithms. -For leaf images: Segmenting the leaf region by using a watershed algorithm with manual inner/outer markers -For stem images: Select stem regions by applying a Hanning filter with a pre-determined window size.</p><p>The feature extraction step in itself is based on kernel descriptors, namely a gradient kernel for the leafscan, fruit, flower, leaf, entire and branch view type, and a LBP kernel for the stem view type. The late fusion of the SVM classifiers of each view type is based on the sum of the inverse rank position in each ranked list of species. The second run, (run 2) differs from the first one in that it uses complementary HSV histogram features for the flower and entire view types.</p><p>The third run (Run 3) differs from Run 2 in the fact that it uses an alternative fusion strategy based on a weighted probability combination.</p><p>QUT RV (3 runs), <ref type="bibr" coords="7,229.70,608.27,16.80,8.77" target="#b10">[11]</ref>, Australia. This group mainly based his experiment on the use of the GoogLeNet convolutional neural network <ref type="bibr" coords="7,398.65,620.25,15.50,8.74" target="#b27">[28]</ref> pre-trained on ImageNet dataset. The 3 runs only differ on the strategy used to fuse the classification results of each image of a query observation (sum pooling in Run 1, softmax in Run 2, normalization &amp; softmax in Run3).</p><p>Sabanki-Okan (3 runs) <ref type="bibr" coords="8,257.17,143.27,16.80,8.77" target="#b22">[23]</ref>, Turkey. This group focused its experiment on the evaluation of PCANet <ref type="bibr" coords="8,267.52,155.25,9.96,8.74" target="#b7">[8]</ref>, a very simple yet efficient deep learning network for image classification which comprises only the very basic data processing components: cascaded principal component analysis (PCA), binary hashing, and block-wise histograms. The original method of <ref type="bibr" coords="8,370.10,191.12,10.52,8.74" target="#b7">[8]</ref> was only modified to handle unaligned images. In Run 1, the PCANet is used alone, without using any additional metadata. In Run 2, the date field of the metadata was used to post-process the results of the PCANet. Finally, Run 3 was a trial to combine more classical hand-crafted features for some of the organs (actually SIFT-based VLAD features for Fruit/Leaf/Stem/Branch) with the PCANet approach for the Flower and Entire categories (no meta data used).</p><p>SNUMED (4 runs), <ref type="bibr" coords="8,238.67,299.08,11.46,8.77" target="#b8">[9]</ref>, Korea. As the QUT RV and the INRIA ZENITH research groups, the SNUMED group mainly based his experiment on the use of the GoogLeNet convolutional neural network <ref type="bibr" coords="8,351.44,323.02,15.50,8.74" target="#b27">[28]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Main task</head><p>The following graphic 2 and table <ref type="table" coords="8,285.89,560.48,4.98,8.74" target="#tab_2">2</ref> show the scores obtained on the main task (i.e. at the observation level). It is noticeable that the top-9 runs which perform the best were based on the GoogLeNet <ref type="bibr" coords="8,307.31,584.39,15.50,8.74" target="#b27">[28]</ref> convolutional neural network which clearly confirms the supremacy of deep learning approaches over hand-crafted features as well as the benefit of training deeper architecture thanks to the improved utilization of the computing resources inside the network. The score's deviations between these 9 runs are however still interesting (actually 10 points of mAP between the worst and the best one). A first source of improvement was the fusion strategy allowing to combine the classification results at the image  level into classification scores at the observation level. In this regard, the best performing algorithm was a SoftMax function <ref type="bibr" coords="10,338.74,130.95,10.52,8.74" target="#b3">[4]</ref> as shown by the performance of QUT RV Run 2 compared to INRIA ZENITH run1 based on max pooling, or SNUMED INFO run1 based on a Borda count, or QUT RV run1 based on a sum pooling. The other source of improvement, which allowed the SNUMED group to get the best results, was to use a bootstrap aggregating (bagging) strategy <ref type="bibr" coords="10,134.77,190.72,10.52,8.74" target="#b4">[5]</ref> to improve the stability and the accuracy of the GoogLeNet Convolutional Neural Network. In SNUMED INFO Run 3 and SNUMED INFO Run 4, they actually randomly partitioned the PlantCLEF training set into five-fold so as to train 5 complementary CNN classifiers. Bagging is a well known strategy for reducing variance and avoiding overfitting, in particular in the case of decision trees, but it is interesting to see that it is also very effective in the case on deep learning.</p><p>The second best approach that did not rely on deep learning (i.e. INRIA ZENITH run 2) was to use the Fisher Vector model <ref type="bibr" coords="10,316.13,286.37,15.50,8.74" target="#b25">[26]</ref> on top of a variety of hand-crafted visual features and then to train a multi-class supervised linear classifier through logistic regression. It is here important to note that this method does not make use of any additional training data other than the one provided in the benchmark (contrary to the CNN's that were all previously trained on the large-scale ImageNet dataset). Within the 2014 PlantCLEF challenge <ref type="bibr" coords="10,395.12,346.14,14.61,8.74" target="#b17">[18]</ref>, in which using external training data was not allowed, the Fisher Vector approach was performing the best, even compared to CNN's. But still, the huge performance gap confirms that learning visual features with deep learning is much more effective than sticking on hand-crafted visual features. Interestingly, the third run of the INRIA ZENITH team was based on a fusion of the fisher vector run and the GoogLeNet one which allows assessing in which measure the two approaches are complementary or not. The results show that the performance of the fused run was not better than the GoogLeNet alone. This indicates that the hand-crafted visual features encoded in the fisher vectors did not bring sufficient additional information to be captured by the fusion model (based on Bayesian inference).</p><p>A last interesting outcome that can be derived from the raw results of the task is the relative low performance achieved by the runs of the SABANCI research group which were actually based on the recent PCANet method <ref type="bibr" coords="10,425.93,501.56,9.96,8.74" target="#b7">[8]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Complementary results on images</head><p>The following graphic 3 presents the scores obtained by the additional imagelevel runs provided by the participants. In order to evaluate the benefit of the combination of the test images from the same observation, the graphic compares the pairs of run files on images and on observations produced with the same method.</p><p>Fig. <ref type="figure" coords="11,154.40,521.38,4.13,7.89">3</ref>. Comparison of the methods: before and after combining the prediction for each image from a same plant observation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Complementary results on images detailed by organs</head><p>The following graphic 4 below show the detailed scores obtained for each type of organs. Remember that we use a specific metric weighted by authors and plants, and not by sub-categories, explaining why the score on images not detailed is not the mean of the 7 scores of these sub-categories. Like during the previous Plant Identification Task, this detailed analysis shows that LeafScan and the Flower views are far away the most effective for identifying plant species, followed by the Fruit view, the Leaf view, the Entire view and the Branch view. On the other side, the stem view (or bark view when speaking about trees) is the less informative one particularly when noticing that the number of species represented in that view, and thus the confusion risk, was lower than for the other organs. Interestingly, the hand-crafted visual features of the MICA group perform very well on the Leaf Scan category, with an identification score better than most of the runs based on the GoogLeNet CNN. This shows the relevance of the leaf normalization strategy they used as well as the effectiveness of the gradient kernel for this type of view. For professional use cases in which taking the time to scan the leaf might not be an issue, this method is a serious alternative to the use of the CNN which requires much more resources and training data. This paper presented the overview and the results of the LifeCLEF 2015 plant identification challenge following the four previous ones conducted within CLEF evaluation forum. The main novelty compared to the previous year was the possibility to use external training data in addition to the specific training set provided within the testbed. The first objective of this novelty was clearly to encourage the deployment of transfer learning methods, and in particular of deep convolutional neural networks in order to evaluate their ability to identify plant species at a large-scale. In this regard, the results show that such transfer learning approaches clearly outperform previous approaches based on hand-crafted visual features, aggregation models and linear classifiers. The results are as impressive as a 0, 784 identification score on the flower category. Now, the second objective of opening the training data was to encourage the integration of new plant data (and not only of the popular generlist dataset ImageNet), particularly for populating the long tail of the less populated species which is an important challenge in terms of biodiversity. Unfortunately, none of the participants addressed this issue. More generally, we believe that collecting and building appropriate training data is becoming one of the most central problem for solving definitely the taxonomic gap problem.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="8,371.22,323.02,109.37,8.74;8,134.77,334.98,345.82,8.74;8,134.77,346.93,345.82,8.74;8,134.77,358.89,345.83,8.74;8,134.77,370.84,345.82,8.74;8,134.77,382.80,345.83,8.74;8,134.77,394.75,345.82,8.74;8,134.77,406.71,261.43,8.74;8,134.77,419.03,345.83,8.77;8,134.77,431.01,345.83,8.74;8,134.77,442.97,345.82,8.74;8,134.77,454.92,345.82,8.74;8,134.77,466.88,264.09,8.74"><head></head><label></label><figDesc>pre-trained on ImageNet dataset. In SNUMED INFO Run 1 and SNUMED INFO Run 2 they fine-tuned a single network across all the whole PlantCLEF 2015 dataset. In SNUMED INFO Run 3 and SNUMED INFO Run 4, they used a different training strategy consisting in randomly partitioning the PlantCLEF training set into five-fold so as to obtain 5 complementary CNN classifier whose combination is supposed to be more stable. The scores at the observation level were obtained by combining the image classification results with the Borda-fuse method. UAIC (1 run), [], Romania. This participant used a content-based image search engine (Lucene Image Retrieval Library []) to retrieve the most similar images of each query image and then apply a two-stage instance-based classifier returning the top-10 most populated species for each image and then the top-10 most populated across all the images of a query observation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,164.82,641.18,285.71,7.89;9,134.77,404.07,345.83,211.38"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Official results of the LifeCLEF 2014 Plant Identification Task.</figDesc><graphic coords="9,134.77,404.07,345.83,211.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="12,194.27,461.53,226.83,7.89;12,134.77,225.05,345.84,210.75"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Results detailed for each type of image category.</figDesc><graphic coords="12,134.77,225.05,345.84,210.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="3,134.96,115.84,342.35,278.23"><head></head><label></label><figDesc></figDesc><graphic coords="3,134.96,115.84,342.35,278.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="11,134.77,284.27,345.83,211.38"><head></head><label></label><figDesc></figDesc><graphic coords="11,134.77,284.27,345.83,211.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,136.11,221.45,343.13,61.13"><head>Table 1 :</head><label>1</label><figDesc>Detailed numbers of images of the LifeCLEF 2015 Plant Task dataset Total Branch Entire Flower Fruit Leaf LeafScan Stem Train 91,759 8,130 16,235 28,225 7,720 13,367 5,476 12,605 Test 21,446 2,088 2,983 6,113 8,327 1,423 696 935 All 113,205 10,218 19,218 34,438 16,047 14,790 6,172 13,540</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,134.77,166.81,345.83,188.07"><head></head><label></label><figDesc>This research group experimented two popular families of classification techniques, i.e. convolutional neural networks (CNN) on one side and fisher vectors-based discriminant models on the other side. More precisely, the run entitled INRIA ZENITH Run 1 was based on the GoogLeNet CNN as described in<ref type="bibr" coords="7,318.58,214.64,15.50,8.74" target="#b27">[28]</ref> (pre-trained on the popular Im-ageNet dataset). A single network was trained for all types of view and the fusion of the images of a given observation was performed through a Max pooling. The FV representation used in INRIA ZENITH Run 2 was built from a Gaussian Mixture Model (GMM) of 128 visual words computed on top of different hand-crafted visual features that were previsouly reduced thanks to a Principal Component Analysis (PCA). The classifier trained on top of the FV representations was a logistic regression, which was preferred over a Support Vectors Machine because it directly outputs probabilities which facilitate fusion purposes. INRIA-ZENITH Run 3 was based on a fusion of Run 1 and Run2 using a Bayesian inference framework making use of the confusion matrix of each classifier trained by cross-validation.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,134.77,129.80,345.83,236.67"><head>Table 2 :</head><label>2</label><figDesc>Results of the LifeCLEF 2015 Plant Identification Task. Column "Keywords" attempts to give the main idea of the method used in each run.</figDesc><table coords="9,168.43,160.94,272.35,205.52"><row><cell>Run name</cell><cell>Key-words</cell><cell>Score</cell></row><row><cell>SNUMED INFO run4</cell><cell>5-fold GoogLeNet Borda+</cell><cell>0,667</cell></row><row><cell>SNUMED INFO run3</cell><cell>5-fold GoogLeNet Borda</cell><cell>0,663</cell></row><row><cell>QUT RV run2</cell><cell>GoogLeNet SoftMax</cell><cell>0,633</cell></row><row><cell>QUT RV run3</cell><cell>GoogLeNet Norm &amp; SoftMax</cell><cell>0,624</cell></row><row><cell>SNUMED INFO run2</cell><cell>GoogLeNet Borda+</cell><cell>0,611</cell></row><row><cell>INRIA ZENITH run1</cell><cell>GoogLeNet Max Pool.</cell><cell>0,609</cell></row><row><cell>SNUMED INFO run1</cell><cell>GoogLeNet Borda</cell><cell>0,604</cell></row><row><cell cols="3">INRIA ZENITH run3 Fusion GoogLeNet &amp; Fisher Vectors 0,592</cell></row><row><cell>QUT RV run1</cell><cell>GoogLeNet Sum Pool.</cell><cell>0,563</cell></row><row><cell>ECOUAN run1</cell><cell>CNN Sum Pool.</cell><cell>0,487</cell></row><row><cell cols="3">INRIA ZENITH run2 hand-crafted features + Fisher Vectors 0,300</cell></row><row><cell>MICA run2</cell><cell>Hand-crafted feat. + SVM</cell><cell>0,209</cell></row><row><cell>MICA run1</cell><cell>Hand-crafted feat. + SVM</cell><cell>0,203</cell></row><row><cell>MICA run3</cell><cell>Hand-crafted feat. + SVM</cell><cell>0,203</cell></row><row><cell>SABANCI run2</cell><cell>PCANet (not pretrained)</cell><cell>0,162</cell></row><row><cell>SABANCI run1</cell><cell>PCANet (not pretrained)</cell><cell>0,160</cell></row><row><cell>SABANCI run3</cell><cell>PCANet (not pretrained)</cell><cell>0,158</cell></row><row><cell>UAIC run1</cell><cell>CBIR (LIRE)</cell><cell>0,013</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,134.77,501.56,345.83,140.24"><head></head><label></label><figDesc>. PCANet is a very simple deep learning network which comprises only basic data processing components, i.e. cascaded principal component analysis (PCA), binary hashing, and block-wise histograms. The learned visual features are claimed by the authors to be on par with the state of the art features, either prefixed, highly hand-crafted or carefully learned (by DNNs). The results of our challenge do not confirm this assertion. All the runs of SABANCI did notably have lower performances than the hand-crafted visual features used by MICA runs or INRIA ZENITH Run 2, and much lower performances than the features learned by all other deep learning methods. This conclusion should however be mitigated by the fact that the PCANet of SABANCI was only trained on PlantCLEF data and on a large-scale external data such as ImageNet. Complementary experiments in this way should therefore be conducted to really conclude on the competitiveness of this simple deep learning technique.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_0" coords="2,144.73,646.48,117.68,7.47"><p>http://www.imageclef.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_1" coords="2,144.73,657.44,141.71,7.47"><p>http://www.clef-initiative.eu/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2" coords="11,149.71,572.43,330.88,8.74;11,134.77,584.39,345.82,8.74;11,134.77,596.34,345.83,8.74;11,134.77,608.30,345.82,8.74;11,134.77,620.25,345.83,8.74;11,134.77,632.21,345.82,8.74;11,134.77,644.16,345.82,8.74;11,134.77,656.12,111.58,8.74"><p>Basically, for each method, we can observe an improvement by combining the different views of the same plant observation. This has to be related to the fact that observing different plant organs is the current practice of botanists, who most of the time can't identify a species with only one picture on only one organ. However, we can say that the improvement are not so much high: we guess that there is a room of improvement here, basically with more images and may be with new methods of fusions dealing with this specific problem of multi-image and multi-organ problem.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="13,142.96,410.80,337.63,7.86;13,151.52,421.76,329.07,7.86;13,151.52,432.72,329.07,8.11;13,151.52,444.33,164.76,7.47" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="13,376.99,410.80,103.60,7.86;13,151.52,421.76,199.05,7.86">Development and deployment of a large-scale flower recognition mobile app</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Shpecht</surname></persName>
		</author>
		<ptr target="http://www.nec-labs.com/research/information/infoAM_website/pdfs/MobileFlora.pdf" />
		<imprint>
			<date type="published" when="2012-12">December 2012</date>
		</imprint>
	</monogr>
	<note type="report_type">NEC Labs America Technical Report</note>
</biblStruct>

<biblStruct coords="13,142.96,455.25,337.64,7.86;13,151.52,466.21,307.29,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="13,261.82,455.25,214.85,7.86">Morphological features for leaf based plant recognition</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Aptoula</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Yanikoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,165.60,466.21,150.70,7.86">Proc. IEEE Int. Conf. Image Process</title>
		<meeting>IEEE Int. Conf. Image ess<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,477.78,337.63,7.86;13,151.52,488.74,329.07,7.86;13,151.52,499.70,162.30,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,324.59,477.78,156.00,7.86;13,151.52,488.74,96.13,7.86">Plant leaf identification based on volumetric fractal dimension</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Backes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">M</forename><surname>Bruno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,254.89,488.74,225.70,7.86;13,151.52,499.70,62.47,7.86">International Journal of Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1145" to="1160" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,511.27,337.63,7.86;13,151.52,522.23,68.62,7.86" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m" coord="13,240.56,511.27,170.18,7.86">Pattern recognition and machine learning</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,533.80,303.47,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,205.30,533.80,74.42,7.86">Bagging predictors</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,286.67,533.80,69.14,7.86">Machine learning</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,545.37,337.63,7.86;13,151.52,556.33,329.07,7.86;13,151.52,567.29,127.48,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,363.59,545.37,117.01,7.86;13,151.52,556.33,168.19,7.86">A parametric active polygon for leaf segmentation and shape estimation</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cerutti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Tougne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vacavant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Coquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,339.50,556.33,141.09,7.86;13,151.52,567.29,42.85,7.86">International Symposium on Visual Computing</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="202" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,578.86,337.64,7.86;13,151.52,589.82,329.07,7.86;13,151.52,600.78,229.59,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,338.89,578.86,141.70,7.86;13,151.52,589.82,329.07,7.86">A comparative study of fine-grained classification methods in the context of the lifeclef plant identification challenge</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Champ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,189.66,600.78,162.78,7.86">Working notes of CLEF 2015 conference</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,612.35,337.63,7.86;13,151.52,623.31,323.60,7.86" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.3606</idno>
		<title level="m" coord="13,386.71,612.35,93.88,7.86;13,151.52,623.31,166.09,7.86">Pcanet: A simple deep learning baseline for image classification?</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,142.96,634.88,337.64,7.86;13,151.52,645.84,329.07,7.86;13,151.52,656.80,25.60,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,189.11,634.88,291.49,7.86;13,151.52,645.84,151.31,7.86">Plant identification with deep convolutional neural network: Snumedinfo at lifeclef plant identification task 2015</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,322.36,645.84,158.23,7.86">Working notes of CLEF 2015 conference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,119.67,337.97,7.86;14,151.52,130.63,329.07,7.86;14,151.52,141.59,246.28,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="14,387.38,119.67,93.21,7.86;14,151.52,130.63,109.79,7.86">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,282.66,130.63,169.69,7.86;14,151.52,141.59,123.28,7.86">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference</note>
</biblStruct>

<biblStruct coords="14,142.62,151.91,337.98,7.86;14,151.52,162.87,285.75,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="14,280.50,151.91,200.09,7.86;14,151.52,162.87,73.60,7.86">Content specific feature learning for fine-grained plant classification</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Mccool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Corke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,245.82,162.87,162.78,7.86">Working notes of CLEF 2015 conference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,173.20,337.98,7.86;14,151.52,184.15,329.07,7.86;14,151.52,195.11,176.37,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="14,302.55,184.15,178.04,7.86;14,151.52,195.11,43.05,7.86">The ImageCLEF 2011 plant images classification task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Barthélémy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Birnbaum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mouysset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,215.57,195.11,83.65,7.86">CLEF working notes</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,205.44,337.97,7.86;14,151.52,216.39,329.07,7.86;14,151.52,227.35,25.60,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="14,205.46,216.39,172.67,7.86">The imageclef 2012 plant identification task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Barthelemy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Molino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,398.04,216.39,82.55,7.86">CLEF working notes</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,237.68,337.98,7.86;14,151.52,248.63,329.07,7.86;14,151.52,259.59,292.99,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="14,333.18,248.63,147.42,7.86;14,151.52,259.59,114.29,7.86">Visual-based plant species identification from crowdsourced data</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Selmi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mouysset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Joyeux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Birnbaum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bathelemy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,287.05,259.59,128.79,7.86">ACM conference on Multimedia</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,269.92,337.98,7.86;14,151.52,280.87,329.07,7.86;14,151.52,291.83,194.25,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="14,351.53,269.92,129.07,7.86;14,151.52,280.87,124.04,7.86">Shape oriented feature selection for tomato plant identification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hazra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Hazra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,283.31,280.87,197.29,7.86;14,151.52,291.83,102.09,7.86">International Journal of Computer Applications Technology and Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">449</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,302.16,337.97,7.86;14,151.52,313.11,329.07,7.86;14,151.52,324.07,209.08,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="14,298.24,313.11,182.35,7.86;14,151.52,324.07,43.01,7.86">Interactive plant identification based on social image data</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Bakić</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Barbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Selmi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carré</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mouysset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Molino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,201.70,324.07,89.28,7.86">Ecological Informatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="22" to="34" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,334.40,337.98,7.86;14,151.52,345.35,329.07,7.86;14,151.52,356.31,329.07,8.12;14,151.52,367.92,127.59,7.47" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="14,167.77,345.35,180.80,7.86">The imageclef plant identification task 2013</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Bakic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Barthélémy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
		<ptr target="http://hal.inria.fr/hal-00908934" />
	</analytic>
	<monogr>
		<title level="m" coord="14,372.22,345.35,108.37,7.86;14,151.52,356.31,161.23,7.86">International workshop on Multimedia analysis for ecological data</title>
		<meeting><address><addrLine>Barcelone, Espagne</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10">Oct 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,377.60,337.97,7.86;14,151.52,388.55,329.07,7.86;14,151.52,399.51,196.47,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="14,276.16,388.55,204.44,7.86;14,151.52,399.51,38.91,7.86">Lifeclef 2014: multimedia life species identification challenges</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,211.47,399.51,107.85,7.86">Proceedings of CLEF 2014</title>
		<meeting>CLEF 2014</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,409.84,337.98,7.86;14,151.52,420.79,261.89,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="14,304.35,409.84,176.24,7.86;14,151.52,420.79,62.35,7.86">Plant image retrieval using color, shape and texture features</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kebapci</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Yanikoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Unal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,220.74,420.79,92.84,7.86">The Computer Journal</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1475" to="1490" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,431.12,337.98,7.86;14,151.52,442.08,329.07,7.86;14,151.52,453.03,86.01,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="14,328.09,431.12,152.50,7.86;14,151.52,442.08,103.94,7.86">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,275.64,442.08,200.74,7.86">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,463.36,337.98,7.86;14,151.52,474.32,329.07,7.86;14,151.52,485.27,325.61,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="14,215.91,474.32,264.68,7.86;14,151.52,485.27,51.37,7.86">Leafsnap: A computer vision system for automatic plant species identification</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">J</forename><surname>Kress</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">C</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">V B</forename><surname>Soares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,223.71,485.27,169.57,7.86">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="502" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,495.60,337.97,7.86;14,151.52,506.56,263.64,7.86" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="14,325.02,495.60,155.57,7.86;14,151.52,506.56,51.37,7.86">Mica at lifeclef 2015: Multi-organ plant identification</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">L</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">N</forename><surname>Dng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">N</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,223.71,506.56,162.78,7.86">Working notes of CLEF 2015 conference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,516.88,337.98,7.86;14,151.52,527.84,329.07,7.86;14,151.52,538.80,119.70,7.86" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="14,446.29,516.88,34.30,7.86;14,151.52,527.84,238.97,7.86">Sabanciokan system in lifeclef 2015 plant identification competition</title>
		<author>
			<persName coords=""><forename type="first">Berrin</forename><surname>Mostafa Mehdipour Ghazi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A O M</forename><surname>Yanikoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">C</forename><surname>Ozdemir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,411.88,527.84,68.71,7.86;14,151.52,538.80,91.03,7.86">Working notes of CLEF 2015 conference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,549.12,337.98,7.86;14,151.52,560.08,329.07,7.86;14,151.52,571.04,186.00,7.86" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="14,343.84,549.12,136.76,7.86;14,151.52,560.08,184.87,7.86">Advanced shape context for plant species identification using leaf image retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mouine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Verroust-Blondet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,356.62,560.08,123.98,7.86;14,151.52,571.04,97.14,7.86">ACM International Conference on Multimedia Retrieval</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,581.36,337.98,7.86;14,151.52,592.32,329.07,7.86;14,151.52,603.28,124.92,7.86" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="14,279.21,581.36,201.38,7.86;14,151.52,592.32,53.96,7.86">Automated flower classification over a large number of classes</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,228.32,592.32,252.27,7.86;14,151.52,603.28,40.95,7.86">Indian Conference on Computer Vision, Graphics and Image Processing</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,613.60,337.97,7.86;14,151.52,624.56,329.07,7.86;14,151.52,635.52,25.60,7.86" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="14,324.74,613.60,155.85,7.86;14,151.52,624.56,97.40,7.86">Improving the fisher kernel for largescale image classification</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,268.54,624.56,118.81,7.86">Computer Vision-ECCV 2010</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="143" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,645.84,337.97,7.86;14,151.52,656.80,319.81,7.86" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="14,333.89,645.84,146.70,7.86;14,151.52,656.80,107.28,7.86">Fine-tuning deep convolutional networks for plant recognition</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">E</forename><surname>Camargo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,279.89,656.80,162.78,7.86">Working notes of CLEF 2015 conference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,119.67,337.98,7.86;15,151.52,130.63,329.07,7.86;15,151.52,141.59,93.19,7.86" xml:id="b27">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<title level="m" coord="15,284.72,130.63,128.81,7.86">Going deeper with convolutions</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,142.62,152.55,337.97,7.86;15,151.52,163.51,99.84,7.86" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="15,250.01,152.55,178.04,7.86">The trec-8 question answering track report</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,451.66,152.55,28.93,7.86">TREC</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="77" to="82" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
