<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,141.44,115.96,332.47,12.62;1,258.30,133.89,92.53,12.62">UNED Online Reputation Monitoring Team at RepLab 2013</title>
				<funder ref="#_YDETQJH">
					<orgName type="full">European Community</orgName>
				</funder>
				<funder ref="#_StGNX7J">
					<orgName type="full">ESF</orgName>
				</funder>
				<funder>
					<orgName type="full">Regional Government of Madrid</orgName>
				</funder>
				<funder ref="#_vEHsp8W">
					<orgName type="full">FPI</orgName>
				</funder>
				<funder ref="#_XFSzdqD">
					<orgName type="full">Spanish Ministry of Science and Innovation (Holopedia Project</orgName>
				</funder>
				<funder ref="#_fdkF2ra">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_dnQVB2E">
					<orgName type="full">Spanish Ministry of Education</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,138.98,172.94,65.08,8.74"><forename type="first">Damiano</forename><surname>Spina</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UNED NLP &amp; IR Group Juan del Rosal</orgName>
								<address>
									<postCode>16, 28040</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,211.89,172.94,114.85,8.74"><forename type="first">Jorge</forename><surname>Carrillo-De-Albornoz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UNED NLP &amp; IR Group Juan del Rosal</orgName>
								<address>
									<postCode>16, 28040</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,334.47,172.94,65.09,8.74"><forename type="first">Tamara</forename><surname>Martín</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UNED NLP &amp; IR Group Juan del Rosal</orgName>
								<address>
									<postCode>16, 28040</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,407.58,172.94,63.58,8.74"><forename type="first">Enrique</forename><surname>Amigó</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UNED NLP &amp; IR Group Juan del Rosal</orgName>
								<address>
									<postCode>16, 28040</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,230.79,184.89,57.97,8.74"><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UNED NLP &amp; IR Group Juan del Rosal</orgName>
								<address>
									<postCode>16, 28040</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,316.23,184.89,68.34,8.74"><forename type="first">Fernando</forename><surname>Giner</surname></persName>
							<email>fginer3@alumno.uned.es</email>
							<affiliation key="aff0">
								<orgName type="institution">UNED NLP &amp; IR Group Juan del Rosal</orgName>
								<address>
									<postCode>16, 28040</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,141.44,115.96,332.47,12.62;1,258.30,133.89,92.53,12.62">UNED Online Reputation Monitoring Team at RepLab 2013</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8315B1E4842F71683B749B801151FF57</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the UNED's Online Reputation Monitoring Team participation at RepLab 2013 <ref type="bibr" coords="1,338.90,326.05,9.22,7.86" target="#b2">[3]</ref>. Several approaches were tested: first, an instance-based learning approach that uses Heterogeneity Based Ranking to combine seven different similarity measures was applied for all the subtasks. The filtering subtask was also tackled by automatically discovering filter keywords: those whose presence in a tweet reliably confirm (positive keywords) or discard (negative keywords) that the tweet refers to the company <ref type="bibr" coords="1,290.21,391.80,13.52,7.86" target="#b15">[16]</ref>. Different approaches have been submitted for the topic detection subtask: agglomerative clustering over wikified tweets, co-occurrence term clustering [10] and an LDA-based model that uses temporal information. Finally, the polarity subtask was tackled by following the approach presented in [14] to generate domain specific semantic graphs in order to automatically expand the general purpose lexicon SentiSense <ref type="bibr" coords="1,240.35,457.55,9.22,7.86" target="#b8">[9]</ref>. We next use the domain specific sub-lexicons to classify tweets according to their reputational polarity, following the emotional concept-based system for sentiment analysis presented in <ref type="bibr" coords="1,423.86,479.47,9.22,7.86" target="#b7">[8]</ref>. We corroborated that using entity-level training data improves the filtering step. Additionally, the proposed approaches to detect topics obtained the highest scores in the official evaluation, showing that they are promising directions to address the problem. In the reputational polarity task, our results suggest that a deeper analysis should be done in order to correctly identify the main differences between the Reputational Polarity task and traditional Sentiment Analysis tasks. A final remark is that the overall performance of a monitoring system in RepLab 2013 highly depends on the performance of the initial filtering step.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper describes the UNED Online Reputation Monitoring Team participation in RepLab 2013, which is focused on organizing and classifying tweet streams associated with an entity of interest, in order to facilitate to the analysts the labor of monitoring the reputation of an entity in Twitter. We have participated in four of the five subtasks proposed in the evaluation campaign: filtering unrelated tweets, classifying tweets according to its reputational polarity, detecting topics discussed in tweets and the full monitoring task.</p><p>The RepLab 2013 collection consists of 61 entities from four different domains: automotive, banking, university and music. Each of the entities has an associated set of around 750 manually annotated tweets for training and 1,500 tweets for testing. Tweets are written in English and Spanish, following the same (unbalanced) language distribution as in Twitter. Crawling was performed during the period from the 1st June 2012 till the 31st Dec 2012 using the entitys canonical name as query (e.g. "BMW"), and training/test datasets correspond to different time ranges. The corpus also comprises additional background tweets for each entity (up to 50,000, with a large variability across entities).</p><p>We have applied a range of different approaches to each of the tasks, plus a horizontal approach for all of them. The filtering task has been tackled with keyword recognition based techniques learned from each entity. Polarity has been addressed with semantic graphs for domain-specific affective lexicon adaptation. In the case of topic detection, a revisited version of our three algorithms presented in RepLab 2012 <ref type="bibr" coords="2,234.86,394.25,15.50,8.74" target="#b9">[10]</ref> have been tested again in this edition. The horizontal approach consists of an extended instance-based learning method over the training corpus in which the similarity is measured over multiple tweet extensions (author tweets, external link, etc.).</p><p>The paper is organized as follows. We describe the approaches and the results for the RepLab 2013 subtasks: filtering in Section 2, polarity in Section 3 and topic detection in Section 4. Then, we analyze the results for the full monitoring task in Section 5. We conclude in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Filtering Subtask</head><p>The filtering task in RepLab 2013 is oriented to disambiguate tweets in order to discard those that do not refer to the company (i.e., related vs. unrelated). Here we describe the two different approaches we have tested to tackle this problem: instance-based learning over heterogeneity based ranking and filter keywords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Instance-based Learning over Heterogeneity Based Ranking</head><p>The instance-based learning approach that we have tested is similar to the official RepLab 2013 baseline, where each tweet inherits the (manually annotated) tags from the most similar tweet in the training corpus of the same entity. The difference is that, instead of using Jaccard distance to compute tweet similarity as the baseline does, we have employed and combined multiple similarity measures. Our measures expand the tweets with different sources, and then apply cosine similarity. We have used the following sources to expand tweets: (i) the hashtags in the tweets, (ii) the content of the URLs linked in the tweets, (iii) the rest of tweets published by the same author and (iv) several parts of the wikipedia entries associated with words in the tweet (e.g. title, wikipedia category, etc). As one word can be associated with multiple Wikipedia entries, we have used commonness probability <ref type="bibr" coords="3,240.56,202.68,15.50,8.74" target="#b10">[11]</ref> for disambiguation (described in Section 4.2).</p><p>In order to combine all similarity measures, we have employed an unsupervised method called Heterogeneity Based Ranking (HBR) <ref type="bibr" coords="3,400.60,226.59,9.96,8.74" target="#b4">[5]</ref>. Basically, this method considers the heterogeneity (a notion of diversity or disimilarity) of the set of measures that corroborate that two texts are more similar to each other than other pair of texts. It consists of the following steps: first, for each tweet we consider the most similar tweets in the training corpus according to the Jaccard distance. Then, for each measure, we re-rank all these distances obtaining 100 similarity instances for each measure. Then, we apply the unsupervised ranking fusion algorithm HBR to combine all the rankings. Finally, we take the tag from the winner tweet. When we have no information to compute a given measure (e.g. a tweet with no external links), we assign the average similarity between the tweets in the corpus that do contain this information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Filter Keywords Strategy</head><p>The filter keywords strategy has proved to be a competitive approach to company name disambiguation in Twitter <ref type="bibr" coords="3,282.10,407.88,14.61,8.74" target="#b15">[16]</ref>. A positive/negative filter keyword is an expression that, if present in a tweet, indicates a high probability that the tweet is related/unrelated to the company.</p><p>Here we explain the automatic classification approach, similar to <ref type="bibr" coords="3,428.21,443.75,14.61,8.74" target="#b15">[16]</ref>, that we have tested on the RepLab 2013 Filtering subtask. At a glance, it consists of two steps: first, filter keywords are discovered by using machine learning algorithms (keyword discovery); second, tweets containing positive/negative filter keywords are used to feed a model that classifies the uncovered tweets (tweet classification). Keyword Discovery. Given the tweet stream of an entity and its representative pages (i.e., the homepage and the English and Spanish Wikipedia pages), each term is represented by features that take into account the company's website, Wikipedia, Open Directory Project (ODP) and the RepLab 2013 collection itself <ref type="foot" coords="3,155.58,567.69,3.97,6.12" target="#foot_0">1</ref> .</p><p>Three families of features are defined:</p><p>-Collection-based features: This features are defined to capture differences between keywords and skip terms. In this approach, we added two additional specificity-based features, pseudo-document TF.IDF and KLD <ref type="bibr" coords="3,421.92,625.04,15.50,8.74" target="#b14">[15,</ref><ref type="bibr" coords="3,437.42,625.04,11.62,8.74" target="#b9">10]</ref>, to the existing set of collection-based features described in <ref type="bibr" coords="3,380.62,637.00,14.61,8.74" target="#b15">[16]</ref>.</p><p>-Web-based features: This features should discriminate between positive and negative filter keywords. These features are: term frequency on the representative pages (homepage and the English and Spanish Wikipedia pages), as well as term occurrence in relevant search results in Wikipedia and in ODP.</p><p>-Features expanded by co-occurrence: We applied the co-occurrence expansion described in <ref type="bibr" coords="4,253.68,190.83,15.50,8.74" target="#b15">[16]</ref> to all the features enumerated above.</p><p>Then, terms in the training set are labeled as positive/negative/skip by considering the precision of the tweets covered by the term:</p><p>-If 85% of the tweets containing the term are RELATED, the term is considered as a positive filter keyword; -if 85% of the tweets are UNRELATED then the term is labeled as a negative filter keyword; -in other case, the term is labeled as a skip term.</p><p>Finally, the labeled instances described above are used to feed a positive-negativeskip classifier. We combine two classifiers: positive versus others and negative versus others, using the confidence thresholds learned by the classifiers (i.e., those used by default to decide the final label of each instance). Terms which are simultaneously under/over both thresholds are tagged as skip terms.</p><p>Tweet Classification. After classifying the terms, tweets containing at least one of those terms labeled as positive (negative) keywords are straightforwardly classified as related (unrelated), respectively As classified keywords are unlikely to cover all the tweets, we use a standard bootstrapping method to annotate the uncovered tweets. Tweets are represented as Bag-of-Words (produced after tokenization, lowercase and stop word removal) and term occurrence is used as weighting function; finally, a supervised machine learning algorithm is used to classify the tweets.</p><p>The tweet classification process has been carried out at the entity level, that is, for each entity we use the tweets retrieved by the keywords as seed, only using the training set of the entity, in order to classify automatically the remaining tweets of the entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Submitted Runs</head><p>Table <ref type="table" coords="4,162.32,572.43,4.98,8.74" target="#tab_0">1</ref> provides an overview of the filtering runs, showing the type of training data used in each of them. Run UNED ORM filtering 1 is the instance-based learning over HBR, that works at the entity level as described in 2.1. Runs UNED_ORM_filtering_3, UNED_ORM_filtering_4 and UNED_ORM_filtering_5 follow the filter keyword approach described in Section 2.2, considering different training data to build the model used in the keyword classification step: UNED_ORM_filtering_3 joins all the terms from the different entities in the training dataset to build a single model, while UNED_ORM_filtering_4 and UNED_ORM_filtering_5 build a specific model at the entity level. The difference between them is that, while UNED_ORM_filtering_5 only considers entityspecific data, UNED_ORM_filtering_4 uses exactly the complementary. The latter run simulates the semi-supervised scenario in which the system does not use any previously annotated data about the target entity (like in previous evaluation campaigns <ref type="bibr" coords="5,210.21,178.77,10.67,8.74" target="#b1">[2,</ref><ref type="bibr" coords="5,220.88,178.77,7.12,8.74" target="#b3">4]</ref>). Since in this edition of RepLab, annotated data about the target entity is available, using this data instead of filter keywords to feed the tweet classification step will give us an idea of the performance that we can reach. For each entity, the run UNED_ORM_filtering_2 uses the tweets from the training data to learn the model that will directly classify all the tweets in the test data. In all the filter keywords' runs (from 2 to 5), tweets are tokenized using a Twitter-specific tokenizer <ref type="bibr" coords="5,260.81,413.04,14.61,8.74" target="#b12">[13]</ref>, frequent words are removed using both English and Spanish stop word lists, and terms occurring less than 5 times in the collection are discarded. The machine learning algorithm used in all the experiments was Naïve Bayes, using the implementation provided by the Rapidminer toolkit <ref type="bibr" coords="5,166.87,460.86,14.61,8.74" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Results</head><p>Table <ref type="table" coords="5,161.27,522.29,4.98,8.74" target="#tab_1">2</ref> reports the scores obtained for the evaluation metrics used in the filtering subtask: Accuracy, Reliability (R), Sensitivity (S) and F 1 (R, S). For each of the runs, the position on the official F 1 (R, S) RepLab rank is also shown.</p><p>Even if the results obtained in terms of accuracy are competitive, the scores according to Reliability and Sensitivity measures evidence the need for a better understanding of the problem. Comparing the results obtained with the baseline, only the run UNED ORM filtering 2 outperforms it in terms of accuracy and F 1 (R, S), which proves that there is still room for improvement.</p><p>As expected, runs that use previously annotated data from the entity are significantly better than semi-supervised approaches. A deeper analysis of the machine learning steps is needed to understand the actual limitations of the filter keywords approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Polarity Subtask</head><p>Most of the approaches that aim to detect polarity in texts make use of affective lexicons in order to identify opinions, sentiments or emotions. The main drawback of affective lexicons' development is the manual effort needed to generate good quality resources. Besides, these resources should be designed for a general purpose, not taking into account the peculiarities of a specific domain. The RepLab 2013 dataset is a perfect evaluation framework to study and analyze automatic methods for domain adaptation of affective lexicons. Sentiment Analysis (SA) and Reputational Polarity (RP) are close but different tasks. While the first is mainly focused on identifying subjective content in product/services reviews, the second one aims to measure if a text has positive or negative implications for a company's reputation. This definition includes the well known, but less studied in SA, polar facts. Statements such as "Report: HSBC allowed money laundering that likely funded terror, drugs, ..." are clearly negative when analyzing the reputation of HSBC company, even though no opinion is expressed. The effect of polar facts can be reasonable biased using a domain-specific affective lexicon. In the example, a system using a lexicon that attaches to the words money laundering, terror or drugs a negative polarity or emotion will correctly classify this fact.</p><p>Within this premise our aim in this approach is to evaluate if the use of semantic graphs and word sense disambiguation <ref type="bibr" coords="6,336.21,499.09,15.50,8.74" target="#b13">[14]</ref> will help to generate domainspecific affective lexicons. Following this idea we generate domain specific semantic graphs in order to expand the existing lexicon SentiSense <ref type="bibr" coords="6,418.66,523.00,9.96,8.74" target="#b8">[9]</ref>. To classify tweets with reputational polarity we have used the emotional concept-based system for sentiment analysis presented in <ref type="bibr" coords="6,308.46,546.91,10.52,8.74" target="#b7">[8]</ref> and adapted it to work with English and Spanish texts simultaneously <ref type="bibr" coords="6,283.59,558.87,9.96,8.74" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Semantic Graphs for Domain-Specific Affective Lexicon Adaptation</head><p>Our hypothesis is that using a semantic graph, where the nodes represent concepts and the edges represent semantic relations between concepts, the emotional meaning of previously labeled concepts can be spread to other concepts not labeled and that are strongly related in meaning. Even if this technique can be applied to a general purpose dataset, it seems that a domain specific dataset will produce better results due to the similarity in vocabulary of the different documents. To this end, we followed the approach presented in <ref type="bibr" coords="7,415.73,142.90,14.61,8.74" target="#b13">[14]</ref>, where the main points are the use of a word sense disambiguation algorithm to properly obtain the correct sense of each word and the use of different measures of text similarity and WordNet relations between concepts to generate a semantic graph of a document. This graph is next used to identify the different topics that are dealt with in the document and to extract the most representative sentences in the document to generate an automatic summary. Instead of manually labeling the initial seed of concepts with emotional meaning, we use the SentiSense affective lexicon as a seed.</p><p>The method proposed for domain adaption of affective lexicons consists of three steps:</p><p>-Concept Identification. In order to determine the appropriate sense of each word in WordNet we use the UKB algorithm <ref type="bibr" coords="7,373.80,294.28,10.52,8.74" target="#b0">[1]</ref> that is available both for English and Spanish. According to this, in this step each tweet in the dataset is represented as WordNet concepts. In this step also all hypernyms of such concepts are retrieved in order to enrich the graph generation. Note that only nouns, verbs, adjectives and adverbs are used to generate the graph. Also, a list of stop words has been used to remove non-relevant words. -Graph generation. We have analyzed different relations between concepts as studied in <ref type="bibr" coords="7,212.85,377.62,15.50,8.74" target="#b13">[14,</ref><ref type="bibr" coords="7,228.35,377.62,7.75,8.74" target="#b6">7]</ref> in order to determine which are suitable to propagate the emotional meaning to closely related concepts. As the dataset consists of tweets and the number of words per tweet is 10 words average, using only semantic relations from WordNet produces very unconnected graph. For this reason, we have also included a co-occurrence relation that links concepts which co-occur between 3 to 10 times in the corpus. Our experiments with the training sets reveal that the co-occurrence relation and the WordNet relations of hypernymy, antonymy and derived from are the most appropriate for spreading the emotional meaning between concepts. We use similar weights as the proposed in <ref type="bibr" coords="7,276.98,485.22,15.50,8.74" target="#b13">[14,</ref><ref type="bibr" coords="7,292.48,485.22,7.75,8.74" target="#b6">7]</ref> to ponder the different relations in the graph. To this end, we weight with 1.0 each pair of concepts related by the WordNet pointer derived from and with -1.0 for the antonymy one. Following the idea that a hypernym is a generalization of a concept, the weight assigned to these relations follow the equation 1. Finally, we weight with 1.0 each co-occurrence of two concepts.</p><formula xml:id="formula_0" coords="7,229.84,565.20,250.75,9.65">weight(C i ; E j ) = 1/(depth(hyper i ) + 1)<label>(1)</label></formula><p>-Propagating emotions to new concepts. Finally, in this step the semantic graph is used to extend the emotional categories to new concepts not labeled in SentiSense. To this end, a concept C i not previously labeled in SentiSense is labeled as follows:</p><p>• For all the incoming links that represent relations of co-occurrence, hypernymy and derived from, and that connect C i with concepts of a same It is important to note that these weights are domain-specific, since are derived from the domain semantic graph.</p><p>• The concept is finally labeled with multiple emotional categories, which weights are normalized at the concept level, so that the sum of the weights of all the emotional categories associated to the concept is 1.0. Figure <ref type="figure" coords="8,200.07,517.59,4.98,8.74" target="#fig_0">1</ref> shows an example of an expanded concept.</p><p>We have tested different approximations to generate the domain-specific semantic graphs using the training set of RepLab 2013. Our first approximation used all entities of the same domain to generate the graph and to adapt the affective lexicon of SentiSense. We evaluated different approaches: using just related tweets of the training set, using all tweets of the test set, and using both related tweets of the training set and all tweets of the test set. We have also tested generating graphs at the entity level, that is to say, generating a graph for each entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Emotional concept-based system for Sentiment Analysis</head><p>The resulting domain-specific affective lexicons are used to identify emotions in the tweets of RepLab 2013 dataset using the emotional concept-based system for sentiment analysis presented in <ref type="bibr" coords="9,289.18,161.01,10.96,8.74" target="#b7">[8,</ref><ref type="bibr" coords="9,300.14,161.01,7.31,8.74" target="#b6">7]</ref> and described here for clarification:</p><p>-Pre-processing: POS Tagging and Concept Identification. The objective of the first step is to translate each text to its conceptual representation in order to work at the concept level in the next steps and avoid word ambiguity. To this aim, the input text is split into sentences and the tokens are tagged with their POS. With this information, the system next maps each token to its appropriate WordNet concept using the UKB algorithm <ref type="bibr" coords="9,452.32,239.97,9.96,8.74" target="#b0">[1]</ref>. -Emotion Identification. Once the concepts are identified, the next step maps each WordNet synset to its corresponding emotional category in the SentiSense affective lexicon, if any. In this steps the different generated domain specific lexicons generated using the semantic graph approach are used. -Post-processing: Negation and Intensifiers. In this step, the system has to detect and solve the effect of negations and intensifiers over the emotions discovered in the previous step. This process is important, since these linguistic modifiers can change the polarity and intensity of the emotional meaning of the text.To this end, our system first identifies the presence of modifiers using a list of common negation and intensification tokens. In such a list, each intensifier is assigned a value that represents its weight or strength. The scope of each modifier is determined using the syntax tree of the sentence in which the modifier arises. We assume as scope all descendant leaf nodes of the common ancestor between the modifier and the word immediately after it, and to the right of the modifier. However, this process may introduce errors in special cases, such as subordinate sentences or those containing punctuation marks. In order to avoid this, our method includes a set of rules to delimit the scope in such cases. These rules are based on specific tokens that usually mark the beginning of a different clause (e.g., because, until, why, which, etc.). Since some of these delimiters are ambiguous, their POS is used to disambiguate them. Once the modifiers and their scope are identified, the system solves their effect over the emotions that they affect in the text. The effect of negation is addressed by substituting the emotions assigned to the concepts by their antonyms. In the case of the intensifiers, the concepts that fall into the scope of an intensifier are tagged with the corresponding percentage weight in order to increase or diminish the intensity of the emotions assigned to the concepts. -Classification. In the last step, all the information generated in the previous steps is used to translate each text into a Vector of Emotional Intensities (VEI), which will be the input to a machine learning algorithm. The VEI is a vector of 14 positions, each of them representing one of the emotional categories of the SentiSense affective lexicon. The values of the vector are generated as follows:</p><p>• For each concept, C i , labeled with an emotional category, E j , the weight of the concept for that emotional category, weight(C i ; E j ), is set to 1.0.</p><p>• If no emotional category was found for the concept, and it was assigned the category of its first labeled hypernym, hyper i , then the weight of the concept is computed as:</p><formula xml:id="formula_1" coords="10,238.31,165.35,242.28,9.65">weight(C i ; E j ) = 1/(depth(hyper i ) + 1)<label>(2)</label></formula><p>• If the concept is affected by a negation and the antonym emotional category, E a nton j , was used to label the concept, then the weight of the concept is multiplied by α = 0.6. This value has been empirically determined in previous studies. It is worth mentioning that the experiments have shown that α values below 0.5 decrease performance sharply, while it drops gradually for values above 0.6. • If the concept is affected by an intensifier, then the weight of the concept is increased/decreased by the intensifier percentage, as shown in Equation <ref type="formula" coords="10,211.67,283.96,3.87,8.74">3</ref>.</p><p>weight(C i ; E j ) = weight(C i ; E j ) * (100 + intensif ier percentage)/100</p><p>(3) • Finally, the position in the VEI of the emotional category assigned to the concept is incremented by the weight previously calculated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>The polarity detection task in RepLab 2013 consists of classifying tweets as positive, negative or neutral. It is important to notice that the polarity is oriented to reputation. That is, an emotionally negative tweet is not necessarily negative from the reputation point of view (i.e. "I'm sad. Michael Jackson is dead"). Note that tweets that are unrelated according to human assessors are not considered in the evaluation.</p><p>For the individual evaluation of the reputational polarity task, performance is evaluated in terms of accuracy (% of correctly annotated cases) over related tweets. Reliability (R) and sensitivity (S) are also included for comparison purposes <ref type="bibr" coords="10,161.62,503.37,9.96,8.74" target="#b5">[6]</ref>. Overall scores (accuracy, R, S, F(R,S)) are computed as the average of individual scores per entity, assigning the same weight to all entities.</p><p>As the system for reputational polarity is a supervised method, we have tested different machine learning algorithms in order to determine the best classifier for the task. In our experiments the logistic regression model (Logistic) as implemented in Weka with default parameters has obtained the best results. As previously mentioned we tested different approaches for generating the domainspecific semantic graph, however we only could submit the runs using the graph generated using related tweets of the training set and at the domain level. So, the runs submitted are:</p><p>-UNED ORM polarity 1: This run uses the approach described in section 2.1, which is similar to the baseline but using different text similarity measures and the Heterogeneity Based Ranking approach to combine them.</p><p>-UNED ORM polarity 2: This run uses the system presented in <ref type="bibr" coords="11,423.62,118.99,10.52,8.74" target="#b6">[7]</ref> trained at the entity level. -UNED ORM polarity 3: This run uses the system presented in <ref type="bibr" coords="11,423.62,142.77,10.52,8.74" target="#b6">[7]</ref> trained at the entity level, but using a balanced training instead of the whole training set. -UNED ORM polarity 4: This run uses the system presented in <ref type="bibr" coords="11,423.62,178.49,10.52,8.74" target="#b6">[7]</ref> trained at the entity level but using the domain-specific SentiSense lexicon generated with the graph resulted of all tweets related of the training set of the same domain.</p><p>-UNED ORM polarity 5: This run uses the same system as UNED ORM polarity 4, but using a balanced training set instead of the whole training set. As shown in Table <ref type="table" coords="11,229.25,440.92,3.87,8.74" target="#tab_2">3</ref>.3, the best performing configuration (UNED_ORM_polarity_ 2) is that which has been trained at the entity level and that uses the original SentiSense lexicon (without expanding it using semantic graphs). However, the difference with the (UNED_ORM_polarity_4) system is not significant. Both approaches perform better than the baseline in terms of accuracy, while their performance drops when evaluating with reliability and sensitivity measures. Even if a similar approach has been tested achieving promising results in <ref type="bibr" coords="11,434.14,512.66,9.96,8.74" target="#b7">[8]</ref>, our results obtained in the RepLab 2013 dataset suggest that a deeper analysis should be done in order to correctly understand the task. Our intuition is that analyzing reputational polarity in Twitter slightly differs from analyzing reviews from films or news. First, the text in tweets contains multiple errors and misspellings, so the error introduced in the linguistic process (POS analysis, parsing, word sense disambiguation, etc.) is bigger than in well structured texts such as news or reviews. Second, we found that most of the vocabulary used contains positive emotional meaning, so this positivity is propagated to other concepts with this methods obtaining vectors of emotions with mostly positive emotions, which is translated in an over learning in the positive class. Finally, as in the filtering subtask, combining multiple similarity measures for instance-based learning slightly outperforms the baseline that only considering Jaccard similarity over the terms. This subtask consists of grouping entity related tweets according to topics. The organization does not provide any set of predefined topics. Given that the training corpus corresponds with a previous time range, new topics can appear in the test set. It is assumed that each tweet belong to one topic. In the following sections we describe the different approaches proposed for the topic detection subtask: LDA-based clustering, tweet wikified clustering and term clustering. Finally, we show the results obtained by the runs submitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">LDA-based Clustering</head><p>We use an LDA-based model in order to obtain the topics of a collection of tweets. It is an unsupervised machine learning technique which uncovers information about latent topics across the corpora. The model is inspired by the TwitterLDA model <ref type="bibr" coords="12,223.69,300.31,15.50,8.74" target="#b17">[18]</ref> and TOT model <ref type="bibr" coords="12,320.40,300.31,14.61,8.74" target="#b16">[17]</ref>. TwitterLDA is an author-topic model that is based in the assumptions that there is a set of topics K in Twitter, each represented by a word distribution and each user has her topic interests modeled by a distribution over the topics. In this model a single topic is assigned for an entire tweet. In the TOT model, each topic is associated with a continuous distribution over timestamps, and for each generated document, the mixture distribution over topics is influenced by both word co-occurrences and the document's timestamp.</p><p>For the monitoring scenario there are two important characteristics that are present in TwitterLDA and TOT: tweets are about one single topic and each topic has a time distribution. We use this features in an LDA-based model with the following assumptions: (i) there is a set of topics K in the collection of tweets of an entity, each represented by a word distribution and a continuous distribution over time; and (ii) when a tweet is written, the author first chooses a topic based on a topic distribution for the entity.</p><p>Then a bag of words is chosen one by one based on the topic. However, not all words in a tweet are closely related to the topic of that tweet; some are background words commonly used in tweets on different topics. Therefore, for each word in a tweet, the author first decides whether it is a background word or a topic word and then chooses the word from the respective word distribution of the entity.</p><p>The process is described as follows:</p><formula xml:id="formula_2" coords="13,155.91,141.03,210.42,118.11">1. Draw θ d ∼ Dir(α), φ B ∼ Dir(β), π ∼ Dir(γ) 2. For each topic z = 1, ..., K (a) draw φ z ∼ Dir(β) 3. For each tweet d = 1, ..., D (a) draw a topic z d ∼ M ulti(θ d ) (b) draw a timestamp t d ∼ Beta(ψ z d ) (c) for each word i = 1, ..., N d i. draw y di ∼ Bernoulli(π) ii. if y di = 0 : draw w di ∼ M ulti(φ B ) if y di = 1 : draw w di ∼ M ulti(φ z d )</formula><p>where: φ z d denotes the word distribution for topic z; ψ z d is the time distribution for the topic z; φ B the word distribution for background words and π denotes a Bernoulli distribution that governs the choice between background words and topic words. After applying the model, a topic is represented as a vector of probabilities over the space of words and a single topic is assigned for an entire tweet. We employ Gibss sampling to perform approximate inference. The graphical model is shown in Figure <ref type="figure" coords="13,290.88,342.61,3.87,8.74" target="#fig_1">2</ref>. The system also relies on transfer learning by contextualizing the target tweets with a large set of unlabeled "background" tweets that help improving the clustering. We include background tweets together with target tweets in the LDA model, and we set the total number of clusters. In practice, this means that the system can adapt to find the right number of clusters for the target data, overcoming one of the limitations of using LDA-based approaches (the need of establishing a priori the number of clusters).</p><p>We train the LDA-based model simultaneously over all the tweets in the target set and the background and fix the target number of clusters (as required by LDA) for the whole set. Note that the LDA-model labels each tweet with only one topic, so we will have a non-overlapping clustering. Once we have obtained the clustering, we extract all clusters that contain at least one target tweet, and remove non-target tweets from those clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Wikified Tweet Clustering</head><p>This approach relies on the hypothesis that tweets sharing concepts or entities defined in a knowledge base -such as Wikipedia-are more likely to be talking about the same topic than tweets with none or less concepts in common.</p><p>Tweet Wikification. We use an entity linking approach to gather Wikipedia entries that are semantically related to a tweet. To this end, the COMMON-NESS probability <ref type="bibr" coords="14,213.29,300.14,14.61,8.74" target="#b10">[11]</ref>, based on the intra-Wikipedia hyperlinks, is used to select the most probable entity for each of the longest n-grams that were linked to Wikipedia articles. The approach computes the probability of a concept/entity c been the target of a link with anchor text q in Wikipedia by:</p><formula xml:id="formula_3" coords="14,238.50,353.81,131.20,24.72">Commonness(c, q) = |L q,c | c |L q,c</formula><p>| where L q,c denotes the set of all links with anchor text q and target c.</p><p>We used both Spanish and English Wikipedia dumps. Spanish Wikipedia articles are then translated to the corresponding English Wikipedia article by following the inter-lingual links, using the Wikimedia API<ref type="foot" coords="14,387.98,419.98,3.97,6.12" target="#foot_1">2</ref> .</p><p>Tweet Clustering. After tweets are wikified, the Jaccard similarity between the sets of entities linked to the tweets is used to group them together: given two tweets d 1 and d 2 represented by the set of Wikipedia entities C 1 and C 2 respectively, if Jaccard(C 1 , C 2 ) &gt; th, then d 1 and d 2 are grouped together to the same cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Term Clustering</head><p>Let us assume that each topic related to an entity can be represented with a set of keywords, that allow to the expert to understand what the topic is about. Considering this, we define a two-step algorithm that tries to (i) identify the terminology of each topic -by clustering the terms-and (ii) assigning tweets to the identified clusters.</p><p>We use Hierarchical Agglomerative Clustering (HAC) to build the term clustering. As similarity function, we use the confidence score returned by a classifier that, given a pair of co-occurrent terms, guesses whether both terms belong to the terminology of the same topic or not.</p><p>Co-occurrence pair representation. We used different families of features to represent each of the co-occurring pairs:</p><p>-Term features: Features that describe each of the terms of the co-occurrence pair. These are: term occurrence, normalized frequency, pseudo-document TF.IDF and KL-Divergence <ref type="bibr" coords="15,274.60,175.93,14.61,8.74" target="#b14">[15]</ref>. These features were computed in two ways: (i) considering only tweets on the labeled corpus, and (ii) considering tweets in both the labeled and background corpus. Features based on the tweets meta-data where each term occurs are: Shannon's entropy of named users, URLs, hashtags and authors in the tweets where the term occurs.</p><p>-Content-based pair term features: Features that consider both terms of the co-occurrence pair, such as Levenshtein's distance between terms, normalized frequency of co-occurrences, Jaccard similarity between occurrences of each of the terms. -Meta-data-based pair term features: Jaccard similarity and Shannon's entropy of named users, URLs, hashtags and authors between tweets where both terms co-occur. -Time-aware features: Features based on the date of the creation of the tweets where the terms co-occurs. Features computed are median, minimum, maximum, mean, standard deviation, Shannon's entropy and Jaccard similarity. These features were computed considering four different time intervals: milliseconds, minutes, hours and days.</p><p>In our classification model each instance corresponds to a pair of co-occurrent terms t, t in the entity stream of tweets. In order to learn the model, we extract training instances from the RepLab 2013 training dataset, considering the following labeling function:</p><formula xml:id="formula_4" coords="15,177.82,462.45,60.30,8.74">label ( t, t ) =</formula><p>clean if max j Precision(C t∩t , L j ) &gt; 0.9 noisy in other case where C t∩t is the set of tweets where terms t and t co-occurs and L is the set of topics in the goldstandard, and</p><formula xml:id="formula_5" coords="15,243.66,521.28,126.84,23.23">Precision(C i , L j ) = |C i ∩ L j | |C i |</formula><p>After this process, term pairs with 90% of the tweets belonging to the same cluster in the goldstandard (i.e., purity=0.9) are considered clean pairs. Otherwise, terms with less purity are labeled as noisy pairs. Hierarchical Agglomerative Term Clustering. Using all the co-occurrence pair instances in the training set, we build a single binary classifier that will be applied to all the entities in the test set. Then, the confidence of a co-occurrence pair belonging to the clean class is used to build a similarity matrix between terms. A Hierarchical Agglomerative Clustering is then applied to cluster the terms, using the previously built similarity matrix. After building the agglomerative clustering, a cut-off threshold based on the number of possible merges is used to return the final term clustering solution.</p><p>Tweet clustering. The second step of this algorithm consists on assigning tweets to the identified term clusters. Each tweet is assigned to the cluster with highest Jaccard similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Submitted Runs</head><p>A total of seven runs have been submitted for the topic detection subtask. Table 4 summarizes the approaches and the parameters used for each of the runs. UNED_ORM_topic_det_1 consist of applying instance-based learning over HBR, analogously as described in Section 2.1. UNED_ORM_topic_det_2 is the wikified tweet clustering approach, using the threshold for the Jaccard similarity of th = 0.2. This threshold has been empirically optimized using the training dataset<ref type="foot" coords="16,184.30,319.91,3.97,6.12" target="#foot_2">3</ref> . UNED_ORM_topic_det_3,UNED_ORM_topic_det_4 and UNED_ORM_ topic_det_5 use the term clustering approach using different machine learning algorithms to combine the features (Naïve Bayes and Logistic Regression) and different thresholds applied to the HAC. A threshold of 0.30 indicates that the final clustering considered is the one produced when the 30% of all possible merges are applied. In all the term clustering runs, the merges are carried out by the mean linkage criterion. Again, only terms after stopword removal and with occurrence greater than five are considered. Finally, UNED_ORM_topic_det_6 and UNED_ORM_topic_det_7 use the LDA-based clustering approach, where the transfer learning step is carried out considering background tweets from the entity with a max. of 10,000 tweets or from other randomly selected entity with 10,000 tweets, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results</head><p>Table <ref type="table" coords="16,161.99,504.96,4.98,8.74" target="#tab_4">5</ref> shows the results obtained by the submitted runs in the topic detection subtask and compared to the baselines. The table reports scores for the metrics Reliability (R), Sensitivity (S) and F 1 -measure of R and S, F 1 (R, S). It also reports the position of the runs in the ranking provided by the organizers. As a clustering task, Reliability (R) corresponds to BCubed Precision and Sensitivity (S) corresponds to BCubed Recall <ref type="bibr" coords="16,287.14,564.73,9.96,8.74" target="#b5">[6]</ref>.</p><p>In general, all the approaches are in the top of the rank, performing significantly better than the baselines. The wikified tweet clustering gets the highest score in all the metrics. Term clustering approach seems to performs similarly when changing the HAC cut-off threshold or the machine learning algorithm. On the other hand, the LDA-based clustering approach performs better when the background collection used for transfer learning contain tweets from different entities/test cases (in this run is ensures to have a collection of 10,000 tweets as background). Finally, as in the other subtasks, combining multiple similarity measures for instance-based learning slightly outperforms the baseline that only consider Jaccard similarity over the terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Full Monitoring Task</head><p>The full monitoring task combines the three subtasks that are typically addresses in a monitoring process: first, the tweets that are not related to the entity of interest are filtered out (filtering); then, the related tweets are clustered by topics (topic detection) and finally, those topics are ranked by priority (priority).</p><p>Next we describe the submitted runs for the full monitoring task, as well as the obtained results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Submitted Runs</head><p>For the full monitoring task, we submitted 8 different runs in total. The runs combine different approaches for each of the three subtasks, considering the subsystem as a pipeline (e.g., topic detection is carried upon the tweets labeled as related after the filtering step). Table <ref type="table" coords="18,323.79,172.54,4.98,8.74">6</ref> shows the different combinations submitted. For instance, the run UNED_ORM_full_task_1 consists of applying instance-based learning over HBR for the three subtasks. Run UNED_ORM_full_ task_2 uses the straightforward tweet classification filtering system, the wikified tweet clustering techniques to detect topics over the related tweets and finally the baseline provided by the organizers for the priority subtask (instance-based learning over Jaccard distance).</p><p>Table <ref type="table" coords="18,194.62,275.19,4.13,7.89">6</ref>. Overview of the runs submitted for the full monitoring task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>Table <ref type="table" coords="18,161.23,461.36,4.98,8.74" target="#tab_5">7</ref> shows the macro-averaged F 1 scores and the official rank of the eight runs submitted. The full monitoring task has been evaluated by means of a harmonic mean (F measure) over the six Reliability and Sensitivity measures (R and S for filtering, priority and topic detection).</p><p>In general, we can see that the overall F 1 -measure significantly drops when the filtering subsystem is worse (UNED_ORM_full_task_8,UNED_ORM_full_task_ 3,UNED_ORM_full_task_5). The best results are obtained by using a simple filtering approach -an entity-specific tweet classification algorithm based on Bagof-Words-with the wikified tweet clustering approach for detecting topics, and using the instance-based learning baseline for the priority subtask. Furthermore, considering any of our topic detection approaches perform equally in terms of the full task evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We have described and discussed here the systems submitted by UNED-ORM group to the RepLab 2013 evaluation campaign. We have participated in three of the subtasks (filtering, topic detection and polarity), as well as in the full monitoring task.</p><p>The filtering subtask turned out to be crucial for the overall performance of a monitoring system. An in-depth analysis of the results is needed to understand the limitations of our filter keyword approach -which was initially developed for semi-supervised scenarios, not for a fully supervised scenario as in RepLab 2013.</p><p>In the reputational polarity task, an approach to generate semantic graphs for domain-specific affective lexicon adaptation has been tested. Even if a similar approach has been previously tested achieving promising performance in a traditional Sentiment Analysis task using news and reviews as input data, our results in the RepLab 2013 seem to be less competitive and deserve further analysis. In particular, more analysis on the differences between Reputational Polarity and traditional sentiment analysis is needed.</p><p>Three of our topic detection approaches (LDA-based clustering, wikified tweet clustering and term clustering) perform competitively with respect to other RepLab submissions. Still, the room for improvement is large, and it is not easy to assess how much of the problem can be solved automatically.</p><p>Finally, in terms of instance-based learning, the results suggest that extending tweets with associated contents (tags, external links, Wikipedia entries) does not provide useful signals to improve performance, at least in our current setting.</p><p>Future work will focus on the analysis of the results and the optimization of the different subsystems for the monitoring task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="8,179.15,125.87,257.06,7.89;8,169.35,134.76,276.65,220.97"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example of propagating emotions to new concepts step</figDesc><graphic coords="8,169.35,134.76,276.65,220.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="13,257.21,384.12,100.93,7.89;13,203.93,393.00,207.51,165.44"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Graphical model</figDesc><graphic coords="13,203.93,393.00,207.51,165.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="18,230.21,299.15,40.08,7.89;18,299.66,299.15,73.88,7.89;18,391.32,299.15,36.63,7.89;18,138.55,315.79,338.26,7.47;18,138.55,326.10,284.57,8.12;18,138.55,337.06,284.57,8.12;18,138.55,348.02,284.57,8.12;18,138.55,358.98,284.57,8.11;18,138.55,369.93,123.46,8.11;18,299.66,369.93,123.46,8.11;18,138.55,380.89,123.46,8.12;18,299.66,380.89,123.46,8.12;18,138.55,392.50,338.26,7.47"><head></head><label></label><figDesc>Filtering Topic Detection Priority UNED ORM full task 1 UNED ORM filt 1 UNED ORM topic det 1 UNED ORM priority 1 UNED ORM full task 2 UNED ORM filt 2 UNED ORM topic det 2 baseline UNED ORM full task 3 UNED ORM filt 3 UNED ORM topic det 2 baseline UNED ORM full task 4 UNED ORM filt 2 UNED ORM topic det 3 baseline UNED ORM full task 5 UNED ORM filt 3 UNED ORM topic det 3 baseline UNED ORM full task 6 baseline UNED ORM topic det 6 baseline UNED ORM full task 7 baseline UNED ORM topic det 7 baseline UNED ORM full task 8 UNED ORM filt 3 UNED ORM topic det 2 UNED ORM priority 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,137.78,270.43,336.72,96.70"><head>Table 1 .</head><label>1</label><figDesc>Overview over the runs submitted for the filtering subtask.</figDesc><table coords="5,137.78,294.41,336.72,72.71"><row><cell></cell><cell></cell><cell>training data</cell></row><row><cell></cell><cell>entity</cell><cell>all</cell><cell>leave-entity-out</cell></row><row><cell></cell><cell>(supervised)</cell><cell cols="2">(supervised) (semi-supervised)</cell></row><row><cell cols="2">instance-based learning + HBR UNED ORM filt 1</cell><cell></cell></row><row><cell>filter keywords</cell><cell cols="3">UNED ORM filt 5 UNED ORM filt 3 UNED ORM filt 4</cell></row><row><cell cols="2">filter keywords (tweet classif. only) UNED ORM filt 2</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,138.75,115.91,337.86,102.85"><head>Table 2 .</head><label>2</label><figDesc>Results of the runs submitted for the filtering subtask.</figDesc><table coords="6,138.75,139.87,337.86,78.90"><row><cell>Run</cell><cell cols="5">Accuracy Reliability (R) Sensitivity (S) F1(R, S) Rank</cell></row><row><cell cols="2">UNED ORM filtering 2 0.8587</cell><cell>0.4254</cell><cell>0.3840</cell><cell cols="2">0.3382 19</cell></row><row><cell>baseline</cell><cell>0.8714</cell><cell>0.4902</cell><cell>0.3200</cell><cell>0.3255</cell><cell>21</cell></row><row><cell cols="2">UNED ORM filtering 1 0.8733</cell><cell>0.4732</cell><cell>0.3272</cell><cell>0.3019</cell><cell>27</cell></row><row><cell cols="2">UNED ORM filtering 5 0.8423</cell><cell>0.6742</cell><cell>0.2584</cell><cell>0.2539</cell><cell>42</cell></row><row><cell cols="2">UNED ORM filtering 4 0.5020</cell><cell>0.1696</cell><cell>0.2870</cell><cell>0.1429</cell><cell>61</cell></row><row><cell cols="2">UNED ORM filtering 3 0.5026</cell><cell>0.1713</cell><cell>0.2869</cell><cell>0.1428</cell><cell>62</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="11,154.74,276.76,305.89,124.55"><head>Table 3 .</head><label>3</label><figDesc>Results of the runs submitted for the polarity subtask.</figDesc><table coords="11,154.74,317.39,298.81,83.93"><row><cell>Best system</cell><cell>0.6859</cell><cell>0.4765</cell><cell>0.3360</cell><cell>0.3770</cell><cell>1</cell></row><row><cell cols="2">UNED ORM polarity 2 0.6164</cell><cell>0.3551</cell><cell>0.1049</cell><cell>0.1472</cell><cell>21</cell></row><row><cell cols="2">UNED ORM polarity 4 0.6153</cell><cell>0.3325</cell><cell>0.1100</cell><cell>0.1445</cell><cell>22</cell></row><row><cell cols="2">UNED ORM polarity 1 0.5872</cell><cell>0.3164</cell><cell>0.2906</cell><cell>0.2984</cell><cell>26</cell></row><row><cell>BASELINE</cell><cell>0.5840</cell><cell>0.3151</cell><cell>0.2899</cell><cell>0.2973</cell><cell>28</cell></row><row><cell cols="2">UNED ORM polarity 3 0.5821</cell><cell>0.3388</cell><cell>0.1172</cell><cell>0.1618</cell><cell>31</cell></row><row><cell>All positives</cell><cell>0.5774</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>36</cell></row><row><cell cols="2">UNED ORM polarity 5 0.5746</cell><cell>0.3244</cell><cell>0.1423</cell><cell>0.1761</cell><cell>37</cell></row></table><note coords="11,154.74,301.41,18.14,7.01;11,234.42,301.41,226.21,7.73"><p><p>Run</p>Accuracy Reliability (R) Sensitivity (S) F 1 (R, S) Rank</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="17,136.16,115.91,343.70,168.96"><head>Table 4 .</head><label>4</label><figDesc>Overview over the runs submitted for the topic detection subtask.</figDesc><table coords="17,136.16,139.87,343.70,145.00"><row><cell>Approach</cell><cell>Parameters</cell><cell>Run</cell></row><row><cell>instance-based learning + HBR</cell><cell></cell><cell>UNED ORM topic det 1</cell></row><row><cell>wikified tweet clustering</cell><cell cols="2">Jaccard sim. threshold= 0.2 UNED ORM topic det 2</cell></row><row><cell>term clustering</cell><cell>combination=N.Bayes,</cell><cell>UNED ORM topic det 3</cell></row><row><cell></cell><cell>HAC threshold= 0.3</cell><cell></cell></row><row><cell>term clustering</cell><cell>combination=N.Bayes,</cell><cell>UNED ORM topic det 4</cell></row><row><cell></cell><cell>HAC threshold= 0.7</cell><cell></cell></row><row><cell>term clustering</cell><cell cols="2">combination=Log.regression, UNED ORM topic det 5</cell></row><row><cell></cell><cell>HAC threshold= 0.7</cell><cell></cell></row><row><cell>LDA-based clustering</cell><cell>background tweets</cell><cell>UNED ORM topic det 6</cell></row><row><cell></cell><cell>only from the entity</cell><cell></cell></row><row><cell>LDA-based clustering</cell><cell>background tweets also</cell><cell>UNED ORM topic det 7</cell></row><row><cell></cell><cell>from other entities</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="17,161.17,305.77,291.15,146.44"><head>Table 5 .</head><label>5</label><figDesc>Results of the runs submitted for the topic detection subtask.</figDesc><table coords="17,161.17,329.73,289.94,122.48"><row><cell>Topic Detection</cell><cell cols="4">Reliability (R) Sensitivity (S) F1(R, S) Rank</cell></row><row><cell>UNED ORM topic det 2</cell><cell>0.4624</cell><cell>0.3246</cell><cell>0.3252</cell><cell>1</cell></row><row><cell>UNED ORM topic det 7</cell><cell>0.2976</cell><cell>0.2223</cell><cell>0.2407</cell><cell>5</cell></row><row><cell>UNED ORM topic det 3</cell><cell>0.4168</cell><cell>0.2120</cell><cell>0.2311</cell><cell>7</cell></row><row><cell>UNED ORM topic det 4</cell><cell>0.4162</cell><cell>0.2116</cell><cell>0.2299</cell><cell>8</cell></row><row><cell>UNED ORM topic det 5</cell><cell>0.4162</cell><cell>0.2116</cell><cell>0.2299</cell><cell>9</cell></row><row><cell>UNED ORM topic det 6</cell><cell>0.3415</cell><cell>0.1642</cell><cell>0.2099</cell><cell>16</cell></row><row><cell>UNED ORM topic det 1</cell><cell>0.1536</cell><cell>0.2186</cell><cell>0.1745</cell><cell>21</cell></row><row><cell>baseline</cell><cell>0.1525</cell><cell>0.2173</cell><cell>0.1735</cell><cell>22</cell></row><row><cell>all-in-one</cell><cell>0.0678</cell><cell>1.0000</cell><cell>0.1210</cell><cell>33</cell></row><row><cell>one-in-one</cell><cell>1.0000</cell><cell>0.0386</cell><cell>0.0693</cell><cell>34</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="19,169.84,115.91,275.68,124.77"><head>Table 7 .</head><label>7</label><figDesc>Results of the runs submitted for the full monitoring task.</figDesc><table coords="19,225.93,139.87,163.50,100.81"><row><cell>Run</cell><cell>F1 Rank</cell></row><row><cell>UNED ORM full task 2</cell><cell>0.1923 1</cell></row><row><cell>UNED ORM full task 7</cell><cell>0.1802 2</cell></row><row><cell>UNED ORM full task 4</cell><cell>0.1736 3</cell></row><row><cell>UNED ORM full task 6</cell><cell>0.1716 4</cell></row><row><cell>UNED ORM full task 1</cell><cell>0.1577 13</cell></row><row><cell>UNED ORM full task 8</cell><cell>0.1226 14</cell></row><row><cell>UNED ORM full task 3</cell><cell>0.1128 15</cell></row><row><cell>UNED ORM full task 5</cell><cell>0.1080 16</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,144.73,656.80,230.58,7.86"><p>See<ref type="bibr" coords="3,161.12,656.80,14.34,7.86" target="#b15">[16]</ref> for details about how the features are computed.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="14,144.73,657.44,207.12,7.47"><p>http://www.mediawiki.org/wiki/API:Properties</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="16,144.73,656.80,235.70,7.86"><p>Note that this is the only supervision used on this system.</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>This research was partially supported by the <rs type="funder">Spanish Ministry of Education</rs> (<rs type="grantName">FPU</rs> grant nr <rs type="grantNumber">AP2009-0507</rs> and <rs type="funder">FPI</rs> grant nr <rs type="grantNumber">BES-2011-044328</rs>), the <rs type="funder">Spanish Ministry of Science and Innovation (Holopedia Project</rs>, <rs type="grantNumber">TIN2010-21128-C02</rs>), the <rs type="funder">Regional Government of Madrid</rs> and the <rs type="funder">ESF</rs> under <rs type="projectName">MA2VICMR</rs> (<rs type="grantNumber">S2009/TIC-1542</rs>) and the <rs type="funder">European Community</rs>'s <rs type="programName">FP7 Programme</rs> under grant agreement nr <rs type="grantNumber">288024</rs> (<rs type="projectName">LiMo</rs><rs type="grantNumber">-SINe</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_dnQVB2E">
					<idno type="grant-number">AP2009-0507</idno>
					<orgName type="grant-name">FPU</orgName>
				</org>
				<org type="funding" xml:id="_vEHsp8W">
					<idno type="grant-number">BES-2011-044328</idno>
				</org>
				<org type="funding" xml:id="_XFSzdqD">
					<idno type="grant-number">TIN2010-21128-C02</idno>
				</org>
				<org type="funded-project" xml:id="_StGNX7J">
					<idno type="grant-number">S2009/TIC-1542</idno>
					<orgName type="project" subtype="full">MA2VICMR</orgName>
				</org>
				<org type="funded-project" xml:id="_YDETQJH">
					<idno type="grant-number">288024</idno>
					<orgName type="project" subtype="full">LiMo</orgName>
					<orgName type="program" subtype="full">FP7 Programme</orgName>
				</org>
				<org type="funding" xml:id="_fdkF2ra">
					<idno type="grant-number">-SINe</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="19,142.96,590.26,337.64,7.86;19,151.52,601.22,329.07,7.86;19,151.52,612.17,329.07,7.86;19,151.52,623.13,25.60,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="19,242.65,590.26,219.02,7.86">Personalizing pagerank for word sense disambiguation</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Soroa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,151.52,601.22,329.07,7.86;19,151.52,612.17,105.76,7.86">Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 12th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="33" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.96,634.88,337.64,7.86;19,151.52,645.84,329.07,7.86;19,151.52,656.80,206.58,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="19,415.45,634.88,65.14,7.86;19,151.52,645.84,282.85,7.86">WePS-3 Evaluation Campaign: Overview of the Online Reputation Management Task</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amigó</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Artiles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Spina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Corujo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,455.90,645.84,24.70,7.86;19,151.52,656.80,177.90,7.86">CLEF 2010 Labs and Workshops Notebook Papers</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,142.96,119.67,337.64,7.86;20,151.52,130.63,329.07,7.86;20,151.52,141.59,310.69,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="20,299.79,130.63,180.80,7.86;20,151.52,141.59,133.82,7.86">Overview of RepLab 2013: Evaluating Online Reputation Management Systems</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amigó</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carrillo-De-Albornoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Chugur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Corujo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Martín</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Meij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Spina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,306.88,141.59,109.25,7.86">CLEF 2013 Working Notes</title>
		<imprint>
			<date type="published" when="2013-09">Sep 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,142.96,152.50,337.63,7.86;20,151.52,163.45,329.07,7.86;20,151.52,174.41,159.24,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="20,396.82,152.50,83.78,7.86;20,151.52,163.45,236.87,7.86">Overview of RepLab 2012: Evaluating Online Reputation Management Systems</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amigó</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Corujo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Meij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,411.06,163.45,69.52,7.86;20,151.52,174.41,130.57,7.86">CLEF 2012 Labs and Workshop Notebook Papers</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,142.96,185.32,337.63,7.86;20,151.52,196.28,329.07,7.86;20,151.52,207.23,166.06,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="20,352.49,185.32,128.10,7.86;20,151.52,196.28,149.01,7.86">Uned: Improving text similarity measures without human assessments</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amigó</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gimenez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,327.37,196.28,153.22,7.86;20,151.52,207.23,166.06,7.86">SEM 2012: The First Joint Conference on Lexical and Computational Semantics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="20,142.96,218.14,337.63,7.86;20,151.52,229.10,231.79,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="20,298.57,218.14,182.02,7.86;20,151.52,229.10,76.30,7.86">A General Evaluation Measure for Document Organization Tasks</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amigó</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,249.17,229.10,108.93,7.86">Proceedings of SIGIR 2013</title>
		<meeting>SIGIR 2013</meeting>
		<imprint>
			<date>Jul.</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,142.96,240.00,337.63,7.86;20,151.52,250.96,329.07,7.86;20,151.52,261.92,181.43,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="20,341.87,240.00,138.72,7.86;20,151.52,250.96,259.40,7.86">Using an emotion-based model and sentiment analysis techniques to classify polarity for reputation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carrillo-De-Albornoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Chugur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amigó</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,433.64,250.96,46.95,7.86;20,151.52,261.92,152.76,7.86">CLEF 2012 Labs and Workshop Notebook Papers</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,142.96,272.82,337.63,7.86;20,151.52,283.78,329.07,7.86;20,151.52,294.74,329.07,7.86;20,151.52,305.70,42.49,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="20,347.48,272.82,133.11,7.86;20,151.52,283.78,181.21,7.86">A hybrid approach to emotional sentence polarity and intensity classification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carrillo-De-Albornoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gervás</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,355.72,283.78,124.87,7.86;20,151.52,294.74,286.39,7.86">Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL&apos;10)</title>
		<meeting>the Fourteenth Conference on Computational Natural Language Learning (CoNLL&apos;10)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="153" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,142.96,316.60,337.63,7.86;20,151.52,327.56,329.07,7.86;20,151.52,338.52,329.07,7.86;20,151.52,349.48,25.60,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="20,355.47,316.60,125.13,7.86;20,151.52,327.56,224.50,7.86">Sentisense: An easily scalable concept-based affective lexicon for sentiment analysis</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carrillo-De-Albornoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gervas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,401.56,327.56,79.03,7.86;20,151.52,338.52,329.07,7.86">Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC&apos;12)</title>
		<meeting>the Eight International Conference on Language Resources and Evaluation (LREC&apos;12)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,142.62,360.38,337.98,7.86;20,151.52,371.34,287.99,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="20,372.92,360.38,107.67,7.86;20,151.52,371.34,45.12,7.86">Uned at replab 2012: Monitoring task</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Martín-Wanton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Spina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amigó</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,217.64,371.34,188.46,7.86">CLEF (Online Working Notes/Labs/Workshop</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,142.62,382.24,337.97,7.86;20,151.52,393.20,329.07,7.86;20,151.52,404.16,56.31,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="20,313.14,382.24,149.10,7.86">Adding semantics to microblog posts</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Meij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Weerkamp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,151.52,393.20,329.07,7.86;20,151.52,404.16,27.65,7.86">Proceedings of the fifth ACM international conference on Web search and data mining</title>
		<meeting>the fifth ACM international conference on Web search and data mining</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,142.62,415.07,337.98,7.86;20,151.52,426.02,329.07,7.86;20,151.52,436.98,302.19,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="20,406.50,415.07,74.09,7.86;20,151.52,426.02,158.07,7.86">YALE: Rapid prototyping for complex data mining tasks</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Mierswa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wurst</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Klinkenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Scholz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Euler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,330.64,426.02,149.95,7.86;20,151.52,436.98,273.52,7.86">SIGKDD&apos;06: Proceedings of the 12th International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,142.62,447.89,337.98,7.86;20,151.52,458.85,265.77,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="20,307.52,447.89,173.07,7.86;20,151.52,458.85,102.86,7.86">Tweetmotif: Exploratory search and topic summarization for twitter</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krieger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,261.20,458.85,94.66,7.86">Proceedings of ICWSM</title>
		<meeting>ICWSM</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,142.62,469.75,337.97,7.86;20,151.52,480.71,329.07,7.86;20,151.52,491.67,56.31,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="20,234.48,469.75,246.11,7.86;20,151.52,480.71,156.23,7.86">Using semantic graphs and word sense disambiguation techniques to improve text summarization</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Diaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="20,316.40,480.71,148.20,7.86">Procesamiento de Lenguaje Natural</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="97" to="105" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,142.62,502.57,337.97,7.86;20,151.52,513.53,329.07,7.86;20,151.52,524.49,329.07,7.86;20,151.52,535.45,25.60,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="20,411.62,502.57,68.97,7.86;20,151.52,513.53,106.75,7.86">Identifying entity aspects in microblog posts</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Spina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Meij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Oghina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Breuss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,280.58,513.53,200.02,7.86;20,151.52,524.49,329.07,7.86">SIGIR &apos;12: Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,142.62,546.35,337.98,7.86;20,151.52,557.31,329.07,7.86;20,151.52,568.27,25.60,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="20,293.81,546.35,186.78,7.86;20,151.52,557.31,101.49,7.86">Discovering filter keywords for company name disambiguation in twitter</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Spina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amigó</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="20,259.93,557.31,138.33,7.86">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4986" to="5003" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,142.62,579.17,337.98,7.86;20,151.52,590.13,329.07,7.86;20,151.52,601.09,329.07,7.86;20,151.52,612.05,67.06,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="20,254.21,579.17,226.38,7.86;20,151.52,590.13,53.41,7.86">Topics over time: a non-markov continuous-time model of topical trends</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,224.91,590.13,255.69,7.86;20,151.52,601.09,161.06,7.86;20,371.68,601.09,34.39,7.86">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="424" to="433" />
		</imprint>
	</monogr>
	<note>KDD &apos;06</note>
</biblStruct>

<biblStruct coords="20,142.62,622.95,337.98,7.86;20,151.52,633.91,329.07,7.86;20,151.52,644.87,207.85,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="20,436.03,622.95,44.56,7.86;20,151.52,633.91,193.48,7.86">Comparing twitter and traditional media using topic models</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">P</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,366.18,633.91,94.02,7.86">Proceedings of ECIR&apos;11</title>
		<meeting>ECIR&apos;11<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="338" to="349" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
