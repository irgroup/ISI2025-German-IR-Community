<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,153.96,115.96,307.48,12.62;1,145.97,133.89,323.44,12.62;1,167.45,151.82,280.55,12.62">CIRG IRGDISCO at RepLab2013 Filtering Task: Use of Wikipedia&apos;s Graph Structure for Entity Name Disambiguation in Tweets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,151.87,189.80,109.69,8.74"><forename type="first">Muhammad</forename><surname>Atif Qureshi</surname></persName>
							<email>muhammad.qureshi@nuigalway.ie</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Computational Intelligence Research Group</orgName>
								<orgName type="institution">National University of Ireland Galway</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Informatics, Systems and Communication</orgName>
								<orgName type="laboratory">Information Retrieval Lab</orgName>
								<orgName type="institution">University of Milan Bicocca</orgName>
								<address>
									<settlement>Milan</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,278.93,189.80,79.54,8.74"><forename type="first">Arjumand</forename><surname>Younus</surname></persName>
							<email>arjumand.younus@nuigalway.ie</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Computational Intelligence Research Group</orgName>
								<orgName type="institution">National University of Ireland Galway</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Informatics, Systems and Communication</orgName>
								<orgName type="laboratory">Information Retrieval Lab</orgName>
								<orgName type="institution">University of Milan Bicocca</orgName>
								<address>
									<settlement>Milan</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,375.83,189.80,53.88,8.74"><forename type="first">Daniel</forename><surname>Abril</surname></persName>
							<email>dabril@iiia.csic.es</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Institut d&apos;Investigacio en Intel-ligencia Artificial</orgName>
								<orgName type="institution" key="instit1">Consejo Superior de Investigaciones Cientificas</orgName>
								<orgName type="institution" key="instit2">Campus UAB</orgName>
								<address>
									<settlement>Bellateraa</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,440.24,189.80,23.24,8.74;1,236.98,201.76,45.57,8.74"><forename type="first">Colm</forename><surname>O'riordan</surname></persName>
							<email>colm.oriordan@nuigalway.ie</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Computational Intelligence Research Group</orgName>
								<orgName type="institution">National University of Ireland Galway</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,312.47,201.76,61.48,8.74"><forename type="first">Gabriella</forename><surname>Pasi</surname></persName>
							<email>pasi@disco.unimib.it</email>
							<affiliation key="aff1">
								<orgName type="department">Informatics, Systems and Communication</orgName>
								<orgName type="laboratory">Information Retrieval Lab</orgName>
								<orgName type="institution">University of Milan Bicocca</orgName>
								<address>
									<settlement>Milan</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,153.96,115.96,307.48,12.62;1,145.97,133.89,323.44,12.62;1,167.45,151.82,280.55,12.62">CIRG IRGDISCO at RepLab2013 Filtering Task: Use of Wikipedia&apos;s Graph Structure for Entity Name Disambiguation in Tweets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">20F106B7E8E49A59094D29ADD1641F26</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Social media repositories serve as a significant source of evidence when extracting information related to the reputation of a particular entity (e.g., a particular politician, singer or company). Reputation management experts manually mine the social media repositories (in particular Twitter) for monitoring the reputation of a particular entity. Recently, the online reputation management evaluation campaign known as RepLab at CLEF has turned attention to devising computational methods for facilitating reputation management experts. A quite significant research challenge related to the above issue is to disambiguate tweets with respect to entity names. In fact, finding if a particular tweet is relevant or irrelevant to a particular entity is an important task not satisfactorily solved yet; to address this issue in this paper we use "context phrases" in a tweet and Wikipedia disambiguated articles for a particular entity in an SVM classifier that utilizes features extracted from the Wikipedia graph structure i.e., links into Wikipedia articles and links from Wikipedia articles. Additionally we also use features derived from term-specificity and term-collocation features derived from the Wikipedia article of an entity under investigation. The experimental evaluations do not show a significant improvement over the baseline and other systems outperform our approach; however, manual inspection of feature sets and training data demonstrates the proposed Wikipedia graph-based features may show a promising outcome when used in combination with sophisticated learning algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Companies are increasingly making use of social media for broadening their reach and enhancing their marketing. At the same time social media users excessively voice out their opinions about various entities (e.g. musicians, movies, companies) <ref type="bibr" coords="2,158.07,130.95,9.97,8.74" target="#b4">[5]</ref>. This has given birth to a new area within the marketing domain known as "online reputation management" whereby automated methods for monitoring reputation of entities are essential requiring novel computational algorithms to facilitate the work of reputation management experts <ref type="bibr" coords="2,366.60,166.82,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="2,378.78,166.82,7.01,8.74" target="#b2">3]</ref>. This paper describes our experience in devising a completely automated algorithm for dealing with the "entity name disambiguation" challenge in the context of RepLab2013 filtering task <ref type="bibr" coords="2,171.33,202.68,10.52,8.74" target="#b1">[2]</ref> where we are given a set of entities and for each entity a set of tweets, which contain some tweets relevant to the entity and some irrelevant ones.</p><p>Our approach consists in making use of the knowledge encoded within the Wikipedia graph structure for entity name disambiguation in tweets. We utilize the Wikipedia disambiguation pages for an entity to determine the amount of disambiguation within a particular tweet while at the same time proposing a technique on top of Wikipedia graph structure to determine context in a tweet <ref type="foot" coords="2,476.12,272.84,3.97,6.12" target="#foot_0">4</ref>Although the experimental results do not show a striking performance over the baseline, we argue that the use of Wikipedia graph structure for entity name disambiguation in tweets is a promising direction to pursue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There has been an increasing interest in research on applying natural language processing techniques to tweets over the past few years. However, in spite of the immense significance of extracting commercially useful information from tweets, the amount of research dedicated to company name disambiguation in tweets is very limited. The only two serious efforts which have been undertaken to stimulate this research task are represented by the WePS online reputation management evaluation campaign at CLEF 2010 <ref type="bibr" coords="2,350.75,435.81,9.97,8.74" target="#b0">[1]</ref>, and by the RepLab online reputation management evaluation campaign at CLEF 2012 <ref type="bibr" coords="2,400.08,447.76,9.97,8.74" target="#b2">[3]</ref>.</p><p>The best two teams in the WePS online reputation management evaluation campaign were LSIR-EPFL <ref type="bibr" coords="2,257.63,471.67,10.52,8.74" target="#b6">[7]</ref> and ITC-UT <ref type="bibr" coords="2,329.57,471.67,9.97,8.74" target="#b7">[8]</ref>. The LSIR-EPFL system builds profiles for each company relying on external resources such as WordNet or the company homepage in addition to a manual list of keywords for the company and the most frequent unrelated senses for the company name. The profiles are then used for extraction of tweet-specific features for use in an SVM classifier. The ITC-UT system is based on a two-step algorithm. In the first step, the algorithm categorizes queries by predicting the class of each company ("organization-like names" or "general-word like names") using a Naive Bayes classifier with six binary features (for example, is the query an acronym?, is the query an entry of a dictionary? etc.). They use thresholds manually set by looking at the training data results for this categorization. The second step consists in categorizing the tweets using a set of heuristics. Despite showing promising results, the two systems LSIR-EPFL and ITC-UT indicate heavy reliance on manual selection of both terms and thresholds for the company name disambiguation task.</p><p>During the RepLab2012 online reputation management evaluation campaign, the best performing team relied on hand-coded rules <ref type="bibr" coords="3,373.46,130.95,10.52,8.74" target="#b2">[3]</ref> for the filtering task.</p><p>Here we have defined a completely automatic algorithm for this task that relies on Wikipedia graph structure as an external knowledge resource of evidence. The method is unique in that it does not require any sort of manual keywords or hand-coded rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Background</head><p>The underlying filtering algorithm makes use of the encyclopedic structure in Wikipedia; more specifically the knowledge encoded in Wikipedia's graph structure is utilized for the classification of tweets as relevant or irrelevant with respect to a particular entity. Wikipedia is organized into categories in a taxonomy-like<ref type="foot" coords="3,476.12,286.30,3.97,6.12" target="#foot_1">5</ref> structure (see Figure <ref type="figure" coords="3,230.01,299.83,3.88,8.74">2</ref>). Each Wikipedia category can have an arbitrary number of subcategories as well as being mentioned inside an arbitrary number of supercategories (e.g., category C 4 in Figure <ref type="figure" coords="3,332.54,323.74,4.98,8.74" target="#fig_0">1</ref> is a subcategory of C 2 and C 3 , and a supercategory of C 5 , C 6 and C 7 .) Furthermore, in Wikipedia each article can belong to an arbitrary number of categories, where each category is a kind of semantic tag for that article <ref type="bibr" coords="3,293.29,359.61,14.62,8.74" target="#b9">[10]</ref>. As an example, in Figure <ref type="figure" coords="3,427.45,359.61,3.88,8.74">2</ref>, article A 1 belongs to categories C 1 and C 10 , article A 2 belongs to categories C 3 and C 4 , while article A 3 belongs to categories C 4 and C 7 . It can be seen that the articles and Wikipedia Category Graph are interlinked and our algorithm makes use of these interlinks for the task of entity name disambiguation within tweets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Wikipedia-Based Feature Set</head><p>Our proposed approach involves a two-step method for entity name disambiguation. In the first step we determine the context phrases within a tweet using an approach similar to Meij et al. <ref type="bibr" coords="3,285.87,479.50,9.97,8.74" target="#b5">[6]</ref>. In the second step we use the link structure of Wikipedia to extract a rich feature set which enables us to perform the disambiguation task.</p><p>Context phrase extraction is performed by the generation of possible n-grams within phrase chunks of a tweet <ref type="foot" coords="3,276.60,525.74,3.97,6.12" target="#foot_2">6</ref> . Similar to the technique in <ref type="bibr" coords="3,411.31,527.32,10.52,8.74" target="#b5">[6]</ref> <ref type="foot" coords="3,421.80,525.74,3.97,6.12" target="#foot_3">7</ref> we then reduce candidate phrases extracted from a tweet to those that have a match in Wikipedia article titles. The reduced set of phrases extracted from a tweet are referred to as ContextPhrases. As mentioned in Section 3.1 a significant aspect of our proposed approach is the Wikipedia graph structure; more specifically links between categories and  <ref type="table" coords="5,328.47,378.86,4.98,8.74" target="#tab_0">1</ref> is a boolean feature that reflects a mapping between the numerical feature to articles in ARelated entity . This mapping is constructed after taking average scores across all context phrases in a tweet and chosing the disambiguated Wikipedia article with highest score; if the mapping is to an entity in ARelated entity we chose the value of this feature as "1" and "0" otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Additional Features</head><p>We also use five additional features and these were obtained from our system <ref type="bibr" coords="5,470.10,497.25,10.52,8.74" target="#b8">[9]</ref> used for the last RepLab online reputation management evaluation campaign in 2012 <ref type="bibr" coords="5,158.48,521.16,9.97,8.74" target="#b2">[3]</ref>. The technique described in <ref type="bibr" coords="5,298.49,521.16,10.52,8.74" target="#b8">[9]</ref> is a two-pass approach where the first pass uses term specificity scores of concept terms (i.e., terms in infoboxes corresponding to the Wikipedia article of the entity, proper nouns "NNP" appearing in the Wikipedia article of the entity<ref type="foot" coords="5,303.15,555.45,3.97,6.12" target="#foot_5">9</ref> ), and the second pass utilizes a score propagation mechanism where terms co-located with concept terms are assigned a new score for re-computation of a score for each tweet. Further, the following additional scores were also used in our submission for RepLab2012 where our team was the second best amongst the participating teams: Note that the score of the first pass, score of the second pass, POS tag of company name in the tweet, URL occurring in the tweet and hashtag occuring in the tweet are used as features in our system for RepLab2013.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Machine Learning and Experimental Runs</head><p>Using the feature sets described in Section 3.2 and 3.3, we train a support vector machine over the training data and then use it to predict labels for the test data. We perform six machine learning runs as follows:</p><p>1. For the first run, we use all features i.e. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We performed our experiments by using the data set provided by the organizers of RepLab 2013 <ref type="bibr" coords="7,210.74,596.35,9.97,8.74" target="#b1">[2]</ref>. In this data set 61 entities were provided, and for each entity at least 2200 tweets were collected: the first 700 constituted the training set, and the rest served as the test set. Furthermore, the entities are grouped into categories based on their type and the four types distributed as part of RepLab2013 are as follows: 1) automatives, 2) banking, 3) universities, and 4) music.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Measures</head><p>The measures used to the evaluation purposes are Reliability and Sensitivity, which are described in detail in <ref type="bibr" coords="8,275.26,152.21,9.97,8.74" target="#b3">[4]</ref>. In the case of filtering, the measures of Reliability and Sensitivity are equivalent to the product of precision scores over positive and negative classes (reliability) and the product of recall scores (sensitivity). The property that makes them particularly suitable for the filtering problem is that they are strict with respect to standard measures, i.e., a high value according to Reliability and Sensitivity implies a high value in all standard measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>Table <ref type="table" coords="8,161.70,276.41,4.98,8.74" target="#tab_1">2</ref> presents a snapshot of the official results for the filtering task of RepLab 2013, where CIRG IRDISCO is the name of our team. As can be seen from Table <ref type="table" coords="8,134.76,300.32,3.88,8.74" target="#tab_1">2</ref>, out of a total of 11 participating teams in RepLab2013 filtering task 6 teams outperform our best run along with the baseline system. We believe this to be a consequence of a considerably high amount of skewness in the training set of tweets. Most of the tweets contained a high percentage of related tweets which affects the performance of learning algorithms such as support vector machines.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,154.88,311.72,305.53,7.89"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Wikipedia Category Graph Structure along with Wikipedia Articles</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,134.76,395.43,345.77,259.27"><head>Table 1 .</head><label>1</label><figDesc>Rich feature set for entity name disambiguation in tweets on top of Wikiped Article Link Structure articles and within articles are used as the fundamental building block for extraction of Wikipedia graph-specific features. At the first level, we use the parent Wikipedia article for the entity under investigation 8 and extract its parent categories PC entity from which we manually chose categories related to the entity under investigation. Sub-categories are then extracted from PC entity up to a hop count of two; finally all articles belonging to these sub-categories are marked as being related to the entity under investigation and we refer to the set of these articles as ARelated entity .The final step consists of constructing an information table of Wikipediabased features as follows:-We extract the disambiguation pages for the entity under investigation and the context phrases extracted in the first step. For each of these we then find the sets of Wikipedia articles in inlinks, outlinks, and inlinks+outlinks. More specifically for each disambiguated Wikipedia article for the entity say e</figDesc><table coords="4,165.64,395.43,284.07,227.44"><row><cell>Feature</cell><cell>Description</cell></row><row><cell>Intersection duplication</cell><cell>No. of intersections between inlinks,</cell></row><row><cell></cell><cell>outlinks and inlinks+outlinks sets</cell></row><row><cell></cell><cell>of e and p without removing dupli-</cell></row><row><cell></cell><cell>cated articles</cell></row><row><cell cols="2">NormalizedIntersection duplication No. of intersections between inlinks,</cell></row><row><cell></cell><cell>outlinks and inlinks+outlinks sets</cell></row><row><cell></cell><cell>of e and p without removing du-</cell></row><row><cell></cell><cell>plicated articles and normalized by</cell></row><row><cell></cell><cell>total number of articles in the sets</cell></row><row><cell>Intersection noduplication</cell><cell>No. of intersections between inlinks,</cell></row><row><cell></cell><cell>outlinks and inlinks+outlinks sets</cell></row><row><cell></cell><cell>of e and p after removing dupli-</cell></row><row><cell></cell><cell>cated articles</cell></row><row><cell cols="2">NormalizedIntersection noduplication No. of intersections between inlinks,</cell></row><row><cell></cell><cell>outlinks and inlinks+outlinks sets</cell></row><row><cell></cell><cell>of e and p without removing du-</cell></row><row><cell></cell><cell>plicated articles and normalized by</cell></row><row><cell></cell><cell>total number of articles in the sets</cell></row><row><cell>Ratio inlink:outlink</cell><cell>Ratio between articles in inlinks to</cell></row><row><cell></cell><cell>articles in outlinks</cell></row></table><note coords="5,156.28,295.80,324.33,9.65;5,151.70,307.75,109.78,8.74;5,158.67,319.53,268.13,9.65;5,158.67,331.40,284.61,9.65;5,158.67,343.26,339.03,9.66;5,140.99,355.01,339.60,8.77;5,151.70,366.99,147.40,8.74;5,140.99,378.83,183.70,8.77"><p>d and each context phrase p in set ContextPhrases, we extract the following sets of Wikipedia articles • Wikipedia articles linking to e d and p referred to as inlinks • Wikipedia articles linking from e d and p referred to as outlinks • Wikipedia articles linking to and from e d and p referred to as inlinks+outlinks -Using information in sets inlinks, outlinks and inlinks+outlinks the features shown in Table 1 are constructed. -Corresponding to each feature in Table</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,201.75,176.65,209.15,423.58"><head>Table 2 .</head><label>2</label><figDesc>Results of Filtering Task of RepLab 2013</figDesc><table coords="6,201.75,197.46,208.78,402.77"><row><cell>Team</cell><cell cols="3">Reliability Sensitivity F(R,S)</cell></row><row><cell>popstar 2</cell><cell>0.729</cell><cell>0.451</cell><cell>0.489</cell></row><row><cell>popstar 3</cell><cell>0.764</cell><cell>0.440</cell><cell>0.480</cell></row><row><cell>popstar 7</cell><cell>0.759</cell><cell>0.428</cell><cell>0.470</cell></row><row><cell>popstar 8</cell><cell>0.589</cell><cell>0.444</cell><cell>0.448</cell></row><row><cell>SZTE NLP 7</cell><cell>0.599</cell><cell>0.444</cell><cell>0.439</cell></row><row><cell>SZTE NLP 10</cell><cell>0.547</cell><cell>0.428</cell><cell>0.407</cell></row><row><cell>SZTE NLP 5</cell><cell>0.508</cell><cell>0.429</cell><cell>0.3911</cell></row><row><cell>SZTE NLP 1</cell><cell>0.491</cell><cell>0.429</cell><cell>0.3910</cell></row><row><cell>SZTE NLP 8</cell><cell>0.507</cell><cell>0.428</cell><cell>0.3893</cell></row><row><cell>SZTE NLP 4</cell><cell>0.480</cell><cell>0.429</cell><cell>0.3889</cell></row><row><cell>SZTE NLP 6</cell><cell>0.517</cell><cell>0.428</cell><cell>0.3886</cell></row><row><cell>SZTE NLP 3</cell><cell>0.496</cell><cell>0.425</cell><cell>0.3882</cell></row><row><cell>SZTE NLP 2</cell><cell>0.492</cell><cell>0.426</cell><cell>0.387</cell></row><row><cell>lia 1</cell><cell>0.658</cell><cell>0.357</cell><cell>0.381</cell></row><row><cell>SZTE NLP 9</cell><cell>0.519</cell><cell>0.416</cell><cell>0.380</cell></row><row><cell>UAMCLYR 4</cell><cell>0.555</cell><cell>0.401</cell><cell>0.379</cell></row><row><cell>UAMCLYR 1</cell><cell>0.631</cell><cell>0.400</cell><cell>0.375</cell></row><row><cell>lia 6</cell><cell>0.619</cell><cell>0.331</cell><cell>0.341</cell></row><row><cell>UNED ORM 2</cell><cell>0.425</cell><cell>0.384</cell><cell>0.338</cell></row><row><cell>BASELINE</cell><cell>0.490</cell><cell>0.320</cell><cell>0.326</cell></row><row><cell>UAMCLYR 3</cell><cell>0.697</cell><cell>0.303</cell><cell>0.322</cell></row><row><cell>Daedalus 1</cell><cell>0.353</cell><cell>0.448</cell><cell>0.321</cell></row><row><cell>Daedalus 3</cell><cell>0.349</cell><cell>0.443</cell><cell>0.318</cell></row><row><cell>lia 10</cell><cell>0.680</cell><cell>0.291</cell><cell>0.312</cell></row><row><cell>lia 9</cell><cell>0.680</cell><cell>0.282</cell><cell>0.302</cell></row><row><cell>UNED ORM 2</cell><cell>0.473</cell><cell>0.327</cell><cell>0.3018</cell></row><row><cell>Daedalus 4</cell><cell>0.302</cell><cell>0.474</cell><cell>0.297</cell></row><row><cell>Daedalus 2</cell><cell>0.299</cell><cell>0.479</cell><cell>0.2963</cell></row><row><cell>lia 8</cell><cell>0.687</cell><cell>0.266</cell><cell>0.2962</cell></row><row><cell>UAMCLYR 2</cell><cell>0.573</cell><cell>0.313</cell><cell>0.292</cell></row><row><cell>lia 5</cell><cell>0.489</cell><cell>0.310</cell><cell>0.289</cell></row><row><cell>lia 4</cell><cell>0.489</cell><cell>0.310</cell><cell>0.289</cell></row><row><cell>UAMCLYR 5</cell><cell>0.569</cell><cell>0.307</cell><cell>0.286</cell></row><row><cell>popstar 5</cell><cell>0.521</cell><cell>0.267</cell><cell>0.282</cell></row><row><cell cols="2">CIRG IRDISCO 4 0.341</cell><cell>0.329</cell><cell>0.2724</cell></row><row><cell>lia 2</cell><cell>0.423</cell><cell>0.331</cell><cell>0.2720</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,134.76,299.89,345.91,241.03"><head></head><label></label><figDesc>both Wikipedia graph-based features and additional score-based features of Section 3.2 and 3.3 whilst training SVM per entity 2. For the second run, we use only Wikipedia graph-based features of Section 3.2 whilst training SVM per entity 3. For the third run, we use only the score-based features of Section 3.3 whilst training SVM per entity 4. For the fourth run, we use all features i.e. both Wikipedia graph-based features and additional score-based features of Section 3.2 and 3.3 whilst training SVM per categories i.e. combining all tweets related to a a particular category into one training and one test set 5. For the fifth run, we use only Wikipedia graph-based features of Section 3.2 whilst training SVM per categories i.e. combining all tweets related to a a particular category into one training and one test set 6. For the sixth run, we use only the score-based features of Section 3.3 whilst training SVM per categories i.e. combining all tweets related to a a particular category into one training and one test set</figDesc><table coords="7,134.76,530.40,146.99,10.52"><row><cell>4 Experimental Results</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="2,144.73,645.84,335.78,7.86;2,144.73,656.80,68.14,7.86"><p>This is a huge research challenge within itself given the huge noise and less amount of text in tweets.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="3,144.73,580.09,335.78,7.86;3,144.73,591.05,160.05,7.86"><p>We say taxonomy-like because it is not strictly hierarchical due to the presence of cycles in the Wikipedia category graph.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2" coords="3,144.73,602.01,335.84,7.86;3,144.73,612.97,335.79,7.86;3,144.73,623.93,335.79,7.86;3,144.73,634.88,30.06,7.86"><p>We do not perform n-gram generation for the complete tweet but instead treat a tweet as a composition of phrase chunks with boundaries such as commas, semicolons, sentence terminators etc. along with other tweet-specific markers such as @, RT etc.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3" coords="3,137.50,644.07,3.65,5.24;3,144.73,645.84,335.80,7.86;3,144.73,656.80,74.35,7.86"><p><ref type="bibr" coords="3,137.50,644.07,3.65,5.24" target="#b6">7</ref> We differ in that we do not apply supervised machine learning for reduction of candidate phrases.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_4" coords="5,144.73,623.93,335.80,7.86;5,144.73,634.88,19.24,7.86"><p>The parent Wikipedia article for each entity is given as part of the dataset for this task.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_5" coords="5,144.73,645.84,335.81,7.86;5,144.73,656.80,39.17,7.86"><p>These are obtained after applying Stanford POS tagger to the Wikipedia article of the entity</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5">Future Work</head><p>Despite the unfavorable outcome of the RepLab2013 filtering task for our runs, we see significant value in the graph-based features mined from Wikipedia article inlinks and outlinks. Manual inspection of feature set shows an obvious difference of inlink and outlink intersections for the related and non-related tweets. As future work we aim to investigate the value of Wikipedia graph-based features when used in combination with sophisticated learning algorithms.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="8,142.95,524.50,337.54,7.86;8,151.52,535.46,329.08,7.86;8,151.52,546.42,329.08,7.86;8,151.52,557.37,77.33,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,427.50,524.50,52.99,7.86;8,151.52,535.46,294.72,7.86">WePS3 Evaluation Campaign: Overview of the On-line Reputation Management Task</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amigó</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Artiles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Spina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Corujo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,466.01,535.46,14.59,7.86;8,151.52,546.42,324.79,7.86">2nd Web People Search Evaluation Workshop (WePS 2010), CLEF 2010 Conference</title>
		<meeting><address><addrLine>Padova Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.95,568.60,337.54,7.86;8,151.52,579.56,328.95,7.86;8,151.52,590.52,329.07,7.86;8,151.52,601.48,302.88,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,304.20,579.56,176.27,7.86;8,151.52,590.52,122.30,7.86">Overview of replab 2013: Evaluating online reputation monitoring systems</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carrillo De Albornoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Chugur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Corujo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Meij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Spina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,294.97,590.52,185.62,7.86;8,151.52,601.48,87.10,7.86">Fourth International Conference of the CLEF initiative, CLEF 2013</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Springer LNCS</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.95,612.70,337.51,7.86;8,151.52,623.66,329.08,7.86;8,151.52,634.62,135.93,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,403.07,612.70,77.39,7.86;8,151.52,623.66,224.79,7.86">Overview of replab 2012: Evaluating online reputation management systems</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amigó</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Corujo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Meij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,395.46,623.66,85.14,7.86;8,151.52,634.62,107.64,7.86">CLEF 2012 Labs and Workshop Notebook Papers</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.95,645.84,337.57,7.86;8,151.52,656.80,241.48,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,313.36,645.84,167.15,7.86;8,151.52,656.80,99.58,7.86">A General Evaluation Measure for Document Organization Tasks</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amigó</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,270.92,656.80,95.67,7.86">Proceedings SIGIR 2013</title>
		<meeting>SIGIR 2013</meeting>
		<imprint>
			<date>July</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.95,119.68,337.54,7.86;9,151.52,130.64,329.07,7.86;9,151.52,141.60,171.29,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,334.35,119.68,146.14,7.86;9,151.52,130.64,265.12,7.86">Exploring the value of online reviews to organizations: Implications for revenue forecasting and planning</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dellarocas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">F</forename><surname>Awad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><forename type="middle">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,435.39,130.64,45.20,7.86;9,151.52,141.60,69.52,7.86">MANAGE-MENT SCIENCE</title>
		<imprint>
			<biblScope unit="page" from="1407" to="1424" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.95,152.55,337.53,7.86;9,151.52,163.51,329.06,7.86;9,151.52,174.47,284.93,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,326.91,152.55,149.67,7.86">Adding semantics to microblog posts</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Meij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Weerkamp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,163.17,163.51,317.41,7.86;9,151.52,174.47,77.27,7.86">Proceedings of the fifth ACM international conference on Web search and data mining, WSDM &apos;12</title>
		<meeting>the fifth ACM international conference on Web search and data mining, WSDM &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="563" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.95,185.43,337.56,7.86;9,151.52,196.39,329.07,7.86;9,151.52,207.35,329.04,7.86;9,151.52,218.31,137.98,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,313.17,185.43,167.34,7.86;9,151.52,196.39,160.14,7.86">What have fruits to do with technology?: the case of orange, blackberry and apple</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">R</forename><surname>Yerva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Miklós</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Aberer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,330.62,196.39,149.97,7.86;9,151.52,207.35,252.82,7.86">Proceedings of the International Conference on Web Intelligence, Mining and Semantics, WIMS &apos;11</title>
		<meeting>the International Conference on Web Intelligence, Mining and Semantics, WIMS &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="48" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.95,229.27,337.58,7.86;9,151.52,240.23,227.91,7.86" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="9,354.63,229.27,125.90,7.86;9,151.52,240.23,223.04,7.86">Itc-ut: Tweet categorization by query categorization for on-line reputation management</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Matsushima</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.95,251.19,337.55,7.86;9,151.52,262.14,329.08,7.86;9,151.52,273.10,151.91,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,316.73,251.19,163.77,7.86;9,151.52,262.14,258.95,7.86">Cirgdisco at replab2012 filtering task: A two-pass approach for company name disambiguation in tweets</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Younus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>O'riordan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Pasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,433.16,262.14,47.44,7.86;9,151.52,273.10,123.14,7.86">CLEF 2012 Evaluation Labs and Workshop</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,284.06,337.89,7.86;9,151.52,295.02,329.06,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,266.43,284.06,214.07,7.86;9,151.52,295.02,48.95,7.86">Analysis of the Wikipedia Category Graph for NLP Applications</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zesch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,218.96,295.02,232.64,7.86">Proceedings of the TextGraphs-2 Workshop (NAACL-HLT)</title>
		<meeting>the TextGraphs-2 Workshop (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
