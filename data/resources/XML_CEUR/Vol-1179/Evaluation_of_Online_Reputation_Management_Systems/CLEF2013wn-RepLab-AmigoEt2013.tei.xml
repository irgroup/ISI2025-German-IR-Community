<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,173.94,115.96,267.47,12.62;1,168.49,133.89,278.37,12.62">Overview of RepLab 2013: Evaluating Online Reputation Monitoring Systems</title>
				<funder ref="#_ccsvPkR">
					<orgName type="full">European Community</orgName>
				</funder>
				<funder ref="#_fMuGnxX">
					<orgName type="full">Royal Dutch Academy of Sciences (KNAW)</orgName>
				</funder>
				<funder ref="#_eGTsMvS">
					<orgName type="full">Netherlands eScience Center</orgName>
				</funder>
				<funder ref="#_kVPZQzM">
					<orgName type="full">Spanish Ministry of Education</orgName>
				</funder>
				<funder ref="#_AQ2K93f">
					<orgName type="full">Spanish Ministry of Science and Innovation (Holopedia Project</orgName>
				</funder>
				<funder ref="#_4dXdNHE">
					<orgName type="full">FPI</orgName>
				</funder>
				<funder ref="#_32q47pE">
					<orgName type="full">Regional Government of Madrid</orgName>
				</funder>
				<funder ref="#_mSEMP5g #_CGgqpwq #_kAgzJGS">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_VFrbZTs #_k3g3th6 #_HrpX8gm">
					<orgName type="full">Netherlands Organisation for Scientific Research (NWO)</orgName>
				</funder>
				<funder ref="#_DHNmUhP">
					<orgName type="full">Center for Creation, Content and Technology</orgName>
					<orgName type="abbreviated">CCCT</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,137.84,171.56,66.03,8.74"><forename type="first">Enrique</forename><surname>Amigó</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UNED NLP &amp; IR Group Juan del Rosal</orgName>
								<address>
									<addrLine>16</addrLine>
									<postCode>28040</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,214.43,171.56,116.48,8.74"><forename type="first">Jorge</forename><surname>Carrillo De Albornoz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UNED NLP &amp; IR Group Juan del Rosal</orgName>
								<address>
									<addrLine>16</addrLine>
									<postCode>28040</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,341.47,171.56,56.52,8.74"><forename type="first">Irina</forename><surname>Chugur</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UNED NLP &amp; IR Group Juan del Rosal</orgName>
								<address>
									<addrLine>16</addrLine>
									<postCode>28040</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,408.53,171.56,61.74,8.74"><forename type="first">Adolfo</forename><surname>Corujo</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Llorente</orgName>
								<address>
									<addrLine>Cuenca Lagasca</addrLine>
									<postCode>88. 28001</postCode>
									<settlement>Madrid</settlement>
									<country>Spain, http</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">llorenteycuenca.com</orgName>
								<address>
									<addrLine>3 Yahoo! Research Diagonal 177</addrLine>
									<postCode>08018</postCode>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,152.60,183.51,59.98,8.74"><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UNED NLP &amp; IR Group Juan del Rosal</orgName>
								<address>
									<addrLine>16</addrLine>
									<postCode>28040</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,223.14,183.51,67.03,8.74"><forename type="first">Tamara</forename><surname>Martín</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UNED NLP &amp; IR Group Juan del Rosal</orgName>
								<address>
									<addrLine>16</addrLine>
									<postCode>28040</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,300.72,183.51,48.87,8.74"><forename type="first">Edgar</forename><surname>Meij</surname></persName>
						</author>
						<author>
							<persName coords="1,360.15,183.51,75.99,8.74"><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">ISLA</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<addrLine>Science Park 904</addrLine>
									<postCode>1098 XH</postCode>
									<settlement>Amsterdam</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,272.03,195.47,66.83,8.74"><forename type="first">Damiano</forename><surname>Spina</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UNED NLP &amp; IR Group Juan del Rosal</orgName>
								<address>
									<addrLine>16</addrLine>
									<postCode>28040</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,173.94,115.96,267.47,12.62;1,168.49,133.89,278.37,12.62">Overview of RepLab 2013: Evaluating Online Reputation Monitoring Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1D2E8FA6420104A29CC8EFCF6A5E3CF6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>RepLab</term>
					<term>Reputation Management</term>
					<term>Evaluation Methodologies and Metrics</term>
					<term>Test Collections</term>
					<term>Text Clustering</term>
					<term>Sentiment Analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We summarize the goals, organization, and results of the second RepLab competitive evaluation campaign for Online Reputation Management systems (RepLab 2013). RepLab 2013 focuses on the process of monitoring the reputation of companies and individuals, and asks participating systems to annotate different types of information on tweets containing the names of several companies. First, tweets have to be classified as related or unrelated to the entity; relevant tweets have to be classified according to their polarity for reputation (Does the content of the tweet have positive or negative implications for the reputation of the entity?), clustered in coherent topics, and clusters have to be ranked according to their priority (potential reputation problems had to come first). The gold standard consists of more than 140,000 tweets annotated by a group of trained annotators supervised and monitored by reputation experts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In a world of online networked information, where its control has moved to users and consumers, every move of a company and every act of a public figure are subject, at all times, to the scrutiny of a powerful global audience. While traditional reputation analysis is mostly manual, online media one allow to process, understand and aggregate large streams of facts and opinions about a company or individual. In this context, natural language processing plays a key, enabling role and we are witnessing an unprecedented demand for text mining software for ORM. Although opinion mining has made significant advances in recent years, most of the work has focused on products. However, mining and interpreting opinions about companies and individuals is, in general, a much harder and less understood problem, since unlike products or services, opinions about people and organizations cannot be structured around any fixed set of features or aspects, requiring a more complex modeling of these entities.</p><p>RepLab is an initiative promoted by the EU project LiMoSINe<ref type="foot" coords="2,424.37,177.69,3.97,6.12" target="#foot_0">5</ref> which aims at enabling research on reputation management as a "living lab": a series of evaluation campaigns in which task design and evaluation are jointly carried out by researchers and the target user communities (reputation management experts). Like its first edition in 2012 <ref type="bibr" coords="2,300.31,227.09,9.96,8.74" target="#b1">[2]</ref>, RepLab 2013 has been organized as a CLEF lab, and the results of the exercise are discussed at CLEF 2013 in Valencia, Spain, on 23-26th September.</p><p>RepLab 2013 has been focused on the task of monitoring the reputation of entities (companies, organizations, celebrities, etc.) on Twitter. The monitoring task for analysts consists of searching the stream of tweets for potential mentions to the entity, filtering those that do refer to the entity, detecting topics (i.e., clustering tweets by subject) and ranking them based on the degree to which they are potential reputation alerts (i.e., issues that may have a substantial impact on the reputation of the entity, and must be handled by reputation management experts).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task Definition</head><p>Following the outline given above, the RepLab 2013 task is defined as (multilingual) topic detection combined with priority ranking of the topics, as input for reputation monitoring experts. The detection of polarity for reputation (does the tweet have negative/positive implications for the reputation of the entity?) is an essential step to assign priority, and is evaluated as a standalone subtask.</p><p>Participants were welcome to present systems that attempt the full monitoring task (filtering + topic detection + topic ranking) or modules that contribute only partially to solve the problem. Subtasks that are explicitly considered in RepLab 2013 are:</p><p>-Filtering. Systems are asked to determine which tweets are related to the entity and which are not. For instance, distinguishing between tweets that contain the word "Stanford" referring to the University of Stanford and filtering out tweets about Stanford as a place. Manual annotations are provided with two possible values: related/unrelated. -Polarity for reputation classification. The goal is to decide if the tweet content has positive or negative implications for the company's reputation. Manual annotations are: positive/negative/neutral.</p><p>-Topic detection. Systems are asked to cluster related tweets about the entity by topic with the objective of grouping together tweets referring to the same subject/event/conversation. -Priority assignment. The full task involves detecting the relative priority of topics. So as to be able to evaluate priority independently from the clustering task, we will evaluate the subtask of predicting the priority of the cluster a tweet belongs to.</p><p>A substantial difference between RepLab 2013 and its first edition in 2012 is that, in 2013, the training and test entities are the same, and therefore conventional machine learning techniques are readily applicable. RepLab 2013 models a scenario where reputation experts are constantly tracking and annotating information about a client (entity), and therefore it is likely to have manual annotations for data related to the entity of interest. RepLab 2012, on the other hand, modeled the scenario of a web application that can be used by anyone, at any time, using any entity name as keyword. In that case, training material was referred to entities other than those in the training set. In RepLab 2013 it was possible to present systems that address only filtering, only polarity identification, only topic detection or only priority assignment. Another difference with 2012 is that in its second edition, the RepLab organization provided baseline components for all of the four subtasks. This way any participant was able to participate in the full task regardless of his particular contribution or expertise.</p><p>Some relevant details on the polarity for reputation and topic detection tasks follow. Polarity for reputation is substantially different from standard sentiment analysis. First, when analyzing polarity for reputation, both facts and opinions have to be considered. For instance, "Barclays plans additional job cuts in the next two years" is a fact with negative implications for reputation. Therefore, systems will not be explicitly asked to classify tweets as factual vs. opinionated: the goal is to find polarity for reputation, that is, what implications a piece of information might have on the reputation of a given entity, regardless of whether the content is opinionated or not. Second, negative sentiments do not always imply negative polarity for reputation and vice versa. For instance, "R.I.P. Michael Jackson. We'll miss you" has a negative associated sentiment (sadness, deep sorrow), but a positive implication for the reputation of Michael Jackson. And the other way around, a tweet such as "I LIKE IT..... NEXT...MITT ROMNEY...Man sentenced for hiding millions in Swiss bank account," has a positive sentiment (joy about a sentence) but has a negative implication for the reputation of Mitt Romney.</p><p>As for the topic detection + topic ranking process, a three-valued classification was applied to assess the priority of each entity-related topic: alert (the topic deserves immediate attention of reputation managers), mildly relevant (the topic contributes to the reputation of the entity but does not require immediate attention) and unimportant (the topic can be neglected from a reputation management perspective). Some of the factors that play a role in the priority assessments are:</p><p>-Polarity. Topics with polarity (and, in particular, with negative polarity, where action is needed) usually have more priority. -Centrality. A high priority topic is very likely to have the company as the main focus of the content. -User's authority. A topic promoted by an influential (for example, in terms of the number of followers or the expertise) user has better chances of receiving high priority.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Baselines</head><p>The baseline approach consists of tagging tweets (in the test set) with the same tags of the closear tweet in the (entity) training set according to the Jaccard word distance. The baseline is, therefore, a simple version of memory-based learning. We have selected this approach for several reasons: (i) it is easy to understand; (ii) it can be applied to every subtask in RepLab 2013; (iii) it keeps the coherence between tasks: if a tweet is annotated as non-related, it will not receive any priority or topic tag; (iv) it exploits the training data set per entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Evaluation Measures</head><p>All subtasks consist of tagging single tweets according to their relatedness, priority, polarity or topic. However, each one corresponds to a particular artificial intelligence problem: binary classification (relatedness), three-level classification (polarity and priority), clustering (topic detection), and their concatenation (full task). A common feature for all tasks is that the classes, levels or clusters can be unbalanced. This entails challenges for the definition of our evaluation methodology. First, in classification tasks, a non informative system (i.e., all tweets to the same class) can achieve high scores without providing useful information. Second, in three-level classification tasks, a system could sort tweets correctly without a perfect correspondence between predicted and true tags. Third, an unbalanced cluster distribution across entities produces an important trade-off between precision/recall oriented evaluation metrics (precision or cluster entropy versus recall or class entropy) and that makes the measure combination function crucial for system ranking.</p><p>In evaluation, there is a hidden trade-off between interpretability and strictness. For instance, Accuracy is easy to interpret: it simply reports how frequently the system makes the correct decision. However, it is also easy to be cheated under unbalanced test sets. For instance, returning all tweets in the same class, cluster or level, may have high accuracy if the set is unbalanced. Other measures based on information theory are more strict when penalizing non informative outputs, but at the cost of interpretability. In this evaluation campaign we employ Accuracy as a high interpretable measure, and the combination of Reliability and Sensitivity (R&amp;S) as a strict and theory grounded measure <ref type="bibr" coords="4,414.74,620.25,9.96,8.74" target="#b3">[4]</ref>.</p><p>Basically, R&amp;S assumes that any organization task consists of a bag of relationships between documents. In our tasks, two documents are related if they have different priority, polarity or relatedness level, or when they appear in the same cluster. In brief, R&amp;S computes the precision and recall of relationships produced by the systems with respect to the goldstandard. In order to avoid the quadratic efect of document pairwise, R&amp;S is computed for each document relationships and averaged in a second step. Reliability and Sensitivity are computed as, being I the set of tweets considered in the evaluation:</p><formula xml:id="formula_0" coords="5,193.54,188.71,228.28,49.47">R(system) = Avg i∈I R(i) S(system) = Avg i∈I S(i) R(i) = P j∈I (rel gold (i, j) = rel sys (i, j)|rel sys (i, j)) S(i) = P j∈I (rel gold (i, j) = rel sys (i, j)|rel gold (i, j)),</formula><p>where rel gold (i, j) represents that i has a higher or lower polarity, priority or relatedness than i, or that i and j belong to the same cluster. Rel sys (i, j) is analogous but applied to the system output.</p><p>R&amp;S has three main strengths. First, it can be applied to ranking, filtering, organization by levels and grouping tasks. This matches all the RepLab 2013 tasks. In addition, it gives the possibility to evaluate the full task as a whole. Second, it covers simultaneously the desirable formal properties satisfied by other measures in each particular task <ref type="bibr" coords="5,283.47,330.14,9.96,8.74" target="#b3">[4]</ref>. Third, according to experimental results that we corroborate with RepLab 2013 data, R&amp;S is strict with respect to other measures: a high score according to R&amp;S ensures a high score according to any traditional measure. In other words, a low score according to one particular traditional measure produces a low R&amp;S score, even when the system is rewarded by other measures.</p><p>R and S are combined with the F measure, i.e., a weighted harmonic mean of R and S. This combining function is grounded in measure theory and satisfies a set of desirable constraints. One of the most useful is that a low score according to one of the two measures strongly penalizes the combined score. However, specially in clustering tasks, the F measure is seriously affected by the relative weight of partial measures (the α parameter). In order to solve this, we complement the evaluation results with the Unanimous Improvement Ratio, which has been proved to be the only weighting independent combining criterion <ref type="bibr" coords="5,445.49,485.56,9.96,8.74" target="#b2">[3]</ref>. UIR is computed over the test cases (entities in RepLab) in which all measures corroborates a difference between runs. Let S 1 and S 2 be two runs and N &gt;∀ (S 1 , S 2 ) the amount of test cases for which S 1 improves S 2 for all measures, then:</p><formula xml:id="formula_1" coords="5,214.17,541.77,185.83,16.39">U IR(S 1 , S 2 ) = N &gt;∀ (S 1 , S 2 ) -N &gt;∀ (S 2 , S 1 )</formula><p>Amount of cases</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset</head><p>RepLab 2013 uses Twitter data in English and Spanish. The balance between both languages depends on the availability of data for each of the entities included in the dataset. The collection comprises tweets about 61 entities from four domains: automotive, banking, universities and music. The domain selection was done to offer a variety of scenarios for reputation studies. To this aim we included entities whose reputation largely relies on their products (automotive), entities for which transparency and ethical side of their activity are the most decisive reputation factors (banking), entities for which the reputation of which depends on a very broad and intangible set of products (universities) and, finally, entities where the reputation is based almost equally on their products and personal qualities (music bands and artists). Table <ref type="table" coords="6,385.32,178.77,4.98,8.74" target="#tab_0">1</ref> summarizes the description of the corpus, as well as the number of tweets for both training and test sets, and the distribution by language.</p><p>Crawling was performed from 1 June, 2012 until 31 Dec, 2012 using each entity's canonical name as query. For each entity, at least 2,200 tweets were collected: the first 700 were reserved for the training set and the last 1,500 for the test collection. This distribution was set in this way to obtain a temporal separation (ideally of several months) between the training and test data. The corpus also comprises additional background tweets for each entity (up to 50,000, with a large variability across entities). These are the remaining tweets situated between the training (earlier tweets) and test material (the latest tweets) in the timeline. These data sets were manually labelled by thirteen annotators who were trained, guided and constantly monitored by experts in ORM. Each tweet is annotated as follows:</p><p>-RELATED/UNRELATED: the tweet is/is not about the entity.</p><p>-POSITIVE/NEUTRAL/NEGATIVE: the information contained in the tweet has positive, neutral or negative implications for the entity's reputation. -Identifier of the topic cluster the tweet has been assigned to.</p><p>-ALERT/MILDLY IMPORTANT/UNIMPORTANT: the priority of the topic cluster the tweet belongs to. Finally, Table <ref type="table" coords="7,214.84,523.33,4.98,8.74" target="#tab_5">5</ref> summarizes the distributions of tweets in priority classes. The less representative class is alert, with 4,780 tweets classified as a possible reputation alert in the whole corpus. Mildly Important has 56,578 tweets and Unimportant receives 48,992 tweets.</p><p>In order to determine inter-annotator agreement we perform two different experiments. First, 14 entities (4 automotive, 3 banking, 3 universities, 4 music) have been labeled by two annotators. This subset contains 31,381 tweets that represent 22% of the RepLab 2013 dataset covering all domains. Second, three annotators labeled 3 entities of the automotive domain. Table <ref type="table" coords="7,445.75,620.25,4.98,8.74" target="#tab_6">6</ref> shows the results of the first experiment of agreement using percentage of agreement and Kappa metrics (both Cohen and Fleiss) for filtering, polarity and priority detection tasks, and F measure of Reliability and Sensitivity for topic detection task. As can be observed, the percentage of agreement for the filtering subtask is near 100%, while taking into account the class distribution with the kappa metrics the inter agreement between annotator decreases. The values obtained for reputation polarity in terms of percentage of agreement are quite similar to other studies over sentiment analysis task. As in the filtering subtask, the value obtained with kappa in the reputation polarity subtask decrease with respect of percentage of agreement. For the topic detection subtask, we do not compute inter agreement between annotators for the whole RepLab 2013 dataset. This is due to the organization of the labeling process. The annotators consider the training and test set as two different sets, so cannot group tweets of both sets.</p><p>The agreement for the topic detection task is higher than expected, taking into account the complexity of the subtask.</p><p>As expected, the results obtained in the experiment with three annotators are lower. As can be seen in Table <ref type="table" coords="8,265.25,584.39,3.87,8.74" target="#tab_7">7</ref>, the inter agreement for the filtering task is quite similar to that obtained in the experiment with two annotators, while the results for the reputational polarity decrease considerably in all metrics. Concerning the topic detection subtask, the table shows the average of F measure over all combinations between annotators. Notably, this task is the one with a lower decrease with respect to the experiment with two annotators, even if this subtask depends on the organization behavior of the annotators. Similarly to the previous experiments of two annotators, as the training and test are considered as two sets by the annotator, the topic detection inter agreement for the whole RepLab 2013 dataset is not computed. Finally, the values obtained for the priority task for three annotators decrease more than for topic detection comparing with the previous experiment, but are still similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Participation</head><p>44 groups signed up for RepLab 2013, although only 15 of them submitted runs to the official evaluation.<ref type="foot" coords="9,240.28,435.87,3.97,6.12" target="#foot_3">6</ref> This year the task was defined in such a way that using the baselines provided by the organizers, every group, besides participating in a concrete subtask, could submit its system to the full task. Nevertheless, only 4 systems explicitly used this possibility.<ref type="foot" coords="9,312.27,471.74,3.97,6.12" target="#foot_4">7</ref> Overall, 5 groups participated in the topic detection subtask, 11 in the reputation polarity classification subtask, 14 in the filtering subtask and 4 in the priority assignment subtask. Below we list the participants and briefly describe the approaches used by each group. Table <ref type="table" coords="9,134.77,521.13,4.98,8.74" target="#tab_8">8</ref> shows the acronyms and affiliations of the research groups that took part in RepLab 2013.</p><p>CIRGDISCO participated in the filtering subtask. They exploited "context phrases" found in tweets and Wikipedia disambiguated articles for a particular entity in an SVM classifier that utilizes features extracted from the Wikipedia graph structure, i.e. incoming and outcoming links from and to Wikipedia articles. They used, in addition, features derived from term-specificity and termcollocation features derived from the Wikipedia article of the analysed entity. Daedalus submitted specific runs for the filtering and polarity subtasks, apart from the full task. Their approach to the filtering subtask is based on the use of linguistic processing modules to detect and disambiguate named entities at several levels. The 4 submitted runs are defined by a combination of morphosyntactic-based vs. semantic disambiguation and a case sensitive/insensitive processing of the tweets. On the other hand, the polarity classification uses a lexicon-based approach to sentiment analysis, improved with a full syntactic analysis and detection of negation and polarity modifiers, which also provides the polarity at entity level.</p><p>DIUE applied a supervised Machine Learning (ML) approach for the polarity classification subtask. The Python NLTK has been used for preprocessing, including file parsing, text analysis and feature extraction. The best run combines bag-of-words with a set of 18 features related to presence of the polarized term, negation before the polarized expression, as well as entity reference based on sentiment lexicons and shallow text analysis.</p><p>GAVKTH used its commercially available system for the filtering and reputation polarity subtasks. The system, designed for large scale analysis of streaming text and measuring the public attitude towards targets of interest, has been used with no adjustment for the specific subtasks. The basic approach relies on distributional semantics represented in a semantic space by means of a patented implementation of the Random Indexing processing framework.</p><p>LIA applied a large variety of ML methods mainly based on exploiting tweet contents to filtering, polarity classification, topic detection, and priority assignment. In several experiments some metadata were added and a fewer number of runs incorporated external information by using provided links to Wikipedia and entities' official web sites. NLP&amp;IR GROUP UNED focused on addressing filtering and reputation polarity classification using an IR method. Viewing these two subtasks as the same problem, i.e. finding the most relevant class to annotate a given tweet, a classical IR approach was applied, using the tweet content as query against an index with the models of the classes used to annotate tweets. The classes were modelled by means of the Kullback Leibler Divergence (KLD), in order to extract their most representative terminology. For topic detection, instead of a clustering based technique, this group resorted to Formal Concept Analysis (FCA) to represent the contents in a lattice structure. Topics were extracted from the lattice using a FCA concept, stability.</p><p>popstar participated in the filtering and reputation polarity classification subtasks. For filtering, these researchers explored different learning algorithms considering a variety of features describing the relationship between an entity and a tweet, such as text, keyword similarity scores between entities metadata and tweets, the Freebase entity graph and Wikipedia.</p><p>REINA used classical systems for the similarity matrix and community detection techniques for topic detection. No distinction was made between languages of the tweets, doing a uniform lexical analysis of all tweets, applying a simple sstemmer and removing the words with less than 4 characters. Additionally, the discarded emoticons were considered as well as hashtags and some entities terms. The urls shared by two tweets were deemed as another important feature of the tweets, assuming this is indicative of topic similarity.</p><p>SZTE NLP presented a system to tackle the filtering and reputation polarity classification subtasks using supervised ML techniques. Several Twitter specific text preprocessing and features engineering methods were applied. Besides supervised methods, they also experimented with incorporating clustering information.</p><p>UAMCLYR adopted Distributional Term Representations (DTR) to tackle the filtering and reputation polarity classification subtasks. Terms were represented by means of contextual information given by the term co-occurrence statistics.</p><p>For topic detection and priority assignment, these researchers explored clustering and classification methods as well as term selection techniques working with two settings: single tweets and tweets extended with derived posts.</p><p>UNED ORM submitted runs to the full task and all the subtasks testing several approaches. First, Instance-based learning using Heterogeneity Based Ranking to combine seven different similarity measures was applied to all the subtasks. The filtering subtask was also tackled by automatically discovering positive and negative filter keywords, i.e. terms present in a tweet that reliably predict the relatedness or non-relatedness of the message to the analysed entity. The topic detection subtask was attempted with three approaches: agglomerative clustering over Wikified tweets, co-occurrence term clustering and an LDA-based model that uses temporal information. Finally, the polarity subtask was tackled by generating domain specific semantic graphs in order to automatically expand the general purpose lexicon SentiSense.</p><p>UNED-READERS* applied an unsupervised knowledge-based approach to filter relevant tweets for a given entity. The method exploits a new way of contextualizing entity names from relatively large collections of texts using probabilistic signature models, i.e., discrete probability distributions of words lexically related to the knowledge or topic underlying the set of entities in background text collections. The contextualization is intended to recover relevant information about the entity, particularly, lexically related words, from background knowledge.</p><p>UNEDTECNALIA submitted a filtering algorithm that takes advantage of the Web of Data in order to create a context for every entity. The semantic context of the analysed entities is generated by querying different data sources (modelled by a set of ontologies) provided by the Linked Open Data Cloud. The extracted context is then compared to the terms contained in the tweet.</p><p>UVA UNED, a collaborative participation of UvA and UNED, focused on applying an active learning approach to the filtering subtask. It consisted of exploiting features based on the detected semantics in the tweet (using Entity Linking with Wikipedia), as well as tweet-inherent features such as hashtags and usernames. The tweets manually inspected during the active learning process were at most 1% of the test data.</p><p>volvam participated in polarity classification and applied one supervised and two unsupervised approaches, combining ML and lexicon-based techniques with an emotional concept model. These methods had been properly adapted to English and Spanish depending on the resources available for each language. The first, unsupervised, approach made use of fuzzy lexicons in order to catch informal variants that are common in Twitter texts. The supervised method extended the first approach with ML techniques and an emotion concept model, while the last one also employed ML but incorporating the bag-of-concepts approach using SenticNet common-sense affective knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Polarity</head><p>Polarity has been evaluated according to Accuracy and R&amp;S. Only entity-related tweets in the test set have been assessed. In order to keep evaluation independent from the filtering task, we do not penalize polarity annotations made on nonrelated tweets. That is, only related tweets are considered in the Accuracy and R&amp;S computation. The related tweets without system response are penalized.</p><p>The system results, sorted by accuracy are shown in Table <ref type="table" coords="13,390.24,397.68,3.87,8.74" target="#tab_9">9</ref>. The table includes only the best system, according to R&amp;S or Accuracy, for each team. The second column contains the ratio of tweets for which the output gives results. The majority class is the dataset is "POSITIVE". The baseline approach appears in the middle of the ranking. SZTE and POPSTAR teams improve, in general, most systems according to both accuracy and R&amp;S. Note that some systems achieve a low accuracy (below the baseline) but with competitive R&amp;S. As R&amp;S only looks at the relative ordering between tweets (rather than the  actual tags), a possible reason is that, while many tags are not correct, the ordinal polarity relationship between them is correct. Figure <ref type="figure" coords="14,410.23,445.82,4.98,8.74">1</ref> illustrates the correspondence between Accuracy and R&amp;S. Note that a high R&amp;S tends to be associated with a high accuracy.</p><p>Another important aspect of polarity detection for ORM is the ability to predict the average polarity of an entity with respect to other entities. To evaluate this ability, we have computed the Pearson correlation between the average estimated and real polarity levels across entities. <ref type="foot" coords="14,328.98,515.98,3.97,6.12" target="#foot_5">8</ref> An interesting result is that some approaches are able to estimate the average polarity reputation for an entity with a 0.9 correlation with the ground truth.</p><p>Finally, Figure <ref type="figure" coords="14,215.97,553.42,4.98,8.74">2</ref> shows the correlation between Accuracy scores over English versus Spanish tweets. In most cases there is a high correspondence. The accuracy for Spanish seems to be upper bounded by the accuracy over English tweets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Filtering</head><p>In this task, tweets must be classified as related or unrelated to the entity of interest. R&amp;S in filtering tasks (two levels) correspond with the products of precision in both classes and the product or recall scores respectively. Table <ref type="table" coords="15,134.77,425.12,9.96,8.74" target="#tab_10">10</ref> shows the Accuracy and R&amp;S results for the filtering task. Again, we have included only the best run according to Accuracy or R&amp;S for each team. Most tweets are related (77%). As in the polarity tasks, the baseline approach appears in the middle of the ranking for both R&amp;S and Accuracy. Figure <ref type="figure" coords="15,428.46,460.99,4.98,8.74" target="#fig_1">3</ref> shows the correspondence between Accuracy and R&amp;S. As in the polarity task, a high R&amp;S score ensures a high Accuracy score. As in the polarity task, there are no important differences in system scores when considering the Spanish vs. English tweets. There is a 0.94 Pearson Correlation) between scores over both kind of tweets. In general, the top scores are much higher than in RepLab 2012; this is explained by the fact that in this new dataset the training and test entities are the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Priority</head><p>The Priority task consists of classifying tweets into three levels. Reliability represents the ratio of correct priority relationships per tweet, while Sensitivity represents the ratio of captured relationships per tweet. In this case, as well as in polarity, only the related tweets (according to assessors) are considered in the evaluation process. Table <ref type="table" coords="15,245.69,644.16,9.96,8.74" target="#tab_11">11</ref> shows the results. Only the best Accuracy and R&amp;S score per team is included. Not all systems have annotated all tweets (see the  last column). The best run achieves a high score for both R&amp;S and Accuracy measures. The baseline approach is improved substantially for both measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Topic Detection</head><p>Topic detection is a clustering task which has been evaluated according to R&amp;S, which correspond with the popular measures Bcubed precision and Recall <ref type="bibr" coords="16,467.31,548.52,9.96,8.74" target="#b0">[1]</ref>. Table <ref type="table" coords="16,163.00,560.48,9.96,8.74" target="#tab_12">12</ref> displays the results. Only the best F measure is considered for each team. Figure <ref type="figure" coords="16,194.74,572.43,4.98,8.74" target="#fig_2">4</ref> shows that there is an important trade-off between R and S in this task. In these circumstances, the F measure weighted with α = 0.5 rewards the runs located in the diagonal axis. But this choice of α is, to some extent, arbitrary. For this reason, we check the evaluation results according to UIR (see previous section). UIR is a complementary measure that indicates to what extent run improvements are sensitive to variations in the measure weighting scheme (i.e. in α). Table <ref type="table" coords="16,212.75,644.16,9.96,8.74" target="#tab_13">13</ref> shows for all runs, the other runs which are improved by the first with UIR≥ 0, 2. This implies that there is a difference higher than 0.2  between the cases in which the first run improves the other for R and S and vice versa. Interestingly, although UAMCLYR 7 is not the best system in the F α=0.5 ranking, it improves robustly a great amount of runs. Some team runs like LIA are not comparable to each other. Probably, they have different grouping thresholds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Full Task</head><p>The full task joins filtering, priority and topic detection tasks. The use of R&amp;S allows us to apply the same evaluation criterion to all subtasks and therefore, to combine all of them. It is possible to apply R&amp;S directly over the set of relationships (priority, filtering and clustering) but then the most frequent binary relationships dominate the evaluation results (in our case, priority relationships would dominate). We decided to use a weighted harmonic mean (F measure) of the six Reliability and Sensitivity measures corresponding to the three subtasks embedded in the full task. In cases of empty partial outputs, we have completed runs with the baseline approach as specified in the guidelines.</p><p>Table <ref type="table" coords="18,177.34,548.52,9.96,8.74" target="#tab_14">14</ref> shows the team ranking in terms of F. However, this evaluation is highly sensitive to the relative importance of measures in the combining function. For this reason, we have also computed UIR between each pair of runs. Here we consider as an unanimous improvement of system A over system B to those test cases (entities) for which all the six measures are better for A than for B. Results of the UIR analysis are shown in Table <ref type="table" coords="18,337.43,608.30,8.49,8.74" target="#tab_15">15</ref>. The third and fourth columns represent how many entities one run improves or is improved by the other. It only includes those run pairs for which UIR is bigger than 0.2. As the table shows, actually, runs from different teams are not comparable to each other: improvements in F are dependent on the relative weighting scheme. However, there are a number of significant improvements (in terms of UIR) between runs from the same teams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>Perhaps the main outcome of RepLab 2013 is its dataset, which comprises more than 142,000 tweets in two languages with four types of high-quality manual annotations, covering all essential aspects of the reputation monitoring process. We expect this dataset to become a useful resource for researchers not only in the field of reputation management, but also for researchers in Information Retrieval and Natural Language Processing in general. Just to give an example, the topics (tweet clusters) together with their relative ranking can be directly mapped into a test collection to evaluate search with diversity algorithms over Twitter. Compared to RepLab 2012, availability of training data for the entities in the test set naturally improves system results and also allows for a more straight-forward application of machine learning techniques. But the tasks themselves are still far from solved; even with plenty of entity-specific training material the RepLab tasks-polarity, topic detection, and ranking-have proved challenging for state-of-the-art systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="13,134.77,655.03,155.62,7.89;13,305.94,655.03,150.97,7.89;13,134.77,513.28,179.99,126.98"><head>Fig. 1 . 2 .</head><label>12</label><figDesc>Fig. 1. Polarity: Accuracy versus R&amp;S Fig. 2. Polarity: Accuracy EN vs ES.</figDesc><graphic coords="13,134.77,513.28,179.99,126.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="16,204.03,246.95,207.30,7.89;16,217.68,115.83,180.00,116.35"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Accuracy versus R&amp;S in the Filtering Task.</figDesc><graphic coords="16,217.68,115.83,180.00,116.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="17,182.42,462.93,250.50,7.89;17,217.68,289.67,180.00,158.50"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Reliability vs. Sensitivity in the Topic Detection Task.</figDesc><graphic coords="17,217.68,289.67,180.00,158.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,136.16,327.69,343.03,47.81"><head>Table 1 .</head><label>1</label><figDesc>RepLab 2013 dataset.</figDesc><table coords="6,136.16,351.65,343.03,23.85"><row><cell></cell><cell cols="3">All Autom. Banking Univers. Music/Artist</cell></row><row><cell>Entities</cell><cell>61</cell><cell>20</cell><cell>11</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,134.77,572.43,345.83,92.42"><head>Table 2</head><label>2</label><figDesc>shows statistics about the filtering subtask. The collection contains 110,352 tweets related with the entities, out of which 34,882 are in the training set and 75,470 are in the test set. The 32,175 unrelated tweets of the dataset are distributed as follows: 10,797 tweets in the training set and 21,378 in the test set. The table also shows the distributions by domain.Table3shows the distribution of polarity classes in the RepLab 2013 dataset. The RepLab 2013 dataset contains 63,442 tweets classified as positive by the annotator, 30,493 classified as neutral and 16,415 classified as negative. The</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,136.16,115.91,343.04,112.61"><head>Table 2 .</head><label>2</label><figDesc>RepLab 2013 dataset for the Filtering Task.</figDesc><table coords="7,136.16,139.87,343.04,88.65"><row><cell></cell><cell cols="5">All Autom. Banking Univers. Music/Artist</cell></row><row><cell>Training No. Related</cell><cell cols="2">34,882 11,356</cell><cell>5,753</cell><cell>3,412</cell><cell>14,361</cell></row><row><cell>Training No. Unrelated</cell><cell>10,797</cell><cell>3,767</cell><cell>2,021</cell><cell>3,548</cell><cell>1,461</cell></row><row><cell>Test No. Related</cell><cell cols="2">75,470 24,415</cell><cell>12,053</cell><cell>7,715</cell><cell>31,287</cell></row><row><cell>Test No. Unrelated</cell><cell>21,378</cell><cell>7,370</cell><cell>4,568</cell><cell>7,229</cell><cell>2,211</cell></row><row><cell>Total No. Related</cell><cell cols="2">110,352 35,771</cell><cell>17,806</cell><cell>11,127</cell><cell>45,648</cell></row><row><cell>Total No. Unrelated</cell><cell cols="2">32,175 11,137</cell><cell>6,589</cell><cell>10,777</cell><cell>3,672</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,134.77,247.18,345.83,271.65"><head>Table 3 .</head><label>3</label><figDesc>RepLab 2013 dataset for the Polarity Task.Table 4 displays the number of topics per set as well as the average number of tweets per topic, which is 17.77 for the whole collection but goes from 14.40 in the training set to 21.14 in the test set. The training set contains 3,813 different topics, the test set 5,757 different topics, for a total of 9,570 different topics in the RepLab 2013 dataset.</figDesc><table coords="7,134.77,271.14,345.83,186.64"><row><cell></cell><cell cols="5">All Autom. Banking Univers. Music/Artist</cell></row><row><cell>Training No. Positive</cell><cell>19,718</cell><cell>5,749</cell><cell>2,195</cell><cell>2,286</cell><cell>9,488</cell></row><row><cell>Training No. Neutral</cell><cell>9,753</cell><cell>4,616</cell><cell>767</cell><cell>894</cell><cell>3,476</cell></row><row><cell>Training No. Negative</cell><cell>5,409</cell><cell>991</cell><cell>2,791</cell><cell>232</cell><cell>1,395</cell></row><row><cell>Test No. Positive</cell><cell cols="2">43,724 24,415</cell><cell>12,053</cell><cell>7,715</cell><cell>31,287</cell></row><row><cell>Test No. Neutral</cell><cell>20,740</cell><cell>9,512</cell><cell>1,407</cell><cell>2,443</cell><cell>7,378</cell></row><row><cell>Test No. Negative</cell><cell>11,006</cell><cell>2,101</cell><cell>4,994</cell><cell>820</cell><cell>3,091</cell></row><row><cell>Total No. Positive</cell><cell cols="2">63,442 12,802</cell><cell>5,652</cell><cell>4,452</cell><cell>20,818</cell></row><row><cell>Total No. Neutral</cell><cell cols="2">30,493 14,128</cell><cell>2,174</cell><cell>3,337</cell><cell>10,854</cell></row><row><cell>Total No. Negative</cell><cell>16,415</cell><cell>3,092</cell><cell>7,785</cell><cell>1,052</cell><cell>4,486</cell></row><row><cell cols="6">distribution in the training set is 19,718 tweets classified as positive, 9,753 as</cell></row><row><cell cols="6">neutral and 5,409 as negative, while the test set contains 63,442 positive tweets,</cell></row><row><cell cols="3">30,493 neutral tweets and 16,415 negatives.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,136.16,115.91,343.04,112.61"><head>Table 4 .</head><label>4</label><figDesc>RepLab 2013 dataset for the Topic Detection Task.</figDesc><table coords="8,136.16,139.87,343.04,88.65"><row><cell></cell><cell cols="5">All Autom. Banking Univers. Music/Artist</cell></row><row><cell>Training No. Topics</cell><cell>3,813</cell><cell>1,389</cell><cell>831</cell><cell>503</cell><cell>1,090</cell></row><row><cell cols="2">Training Avg. No. Tweets/Topic 14.40</cell><cell>12.36</cell><cell>11.35</cell><cell>17.57</cell><cell>16.53</cell></row><row><cell>Test No. Topics</cell><cell>5,757</cell><cell>1,959</cell><cell>1,121</cell><cell>1,035</cell><cell>1,642</cell></row><row><cell>Test Avg. No. Tweets/Topic</cell><cell>21.14</cell><cell>18.42</cell><cell>18.95</cell><cell>21.78</cell><cell>24.74</cell></row><row><cell>Total No. Topics</cell><cell>9,570</cell><cell>3,348</cell><cell>1,952</cell><cell>1,538</cell><cell>2,732</cell></row><row><cell>Total Avg. No. Tweets/Topic</cell><cell>17.77</cell><cell>15.39</cell><cell>15.15</cell><cell>19.67</cell><cell>20.64</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,136.16,248.20,343.04,145.48"><head>Table 5 .</head><label>5</label><figDesc>RepLab 2013 dataset for the Priority Detection Task.</figDesc><table coords="8,136.16,272.16,343.04,121.53"><row><cell></cell><cell cols="5">All Autom. Banking Univers. Music/Artist</cell></row><row><cell>Training No. Alert</cell><cell>1,540</cell><cell>226</cell><cell>841</cell><cell>88</cell><cell>385</cell></row><row><cell cols="2">Training No. Mildly Important 17,961</cell><cell>5,388</cell><cell>2,509</cell><cell>1,949</cell><cell>8,115</cell></row><row><cell>Training No. Unimportant</cell><cell>15,379</cell><cell>5,742</cell><cell>2,403</cell><cell>1,375</cell><cell>5,859</cell></row><row><cell>Test No. Alert</cell><cell>3,240</cell><cell>483</cell><cell>2,195</cell><cell>102</cell><cell>460</cell></row><row><cell>Test No. Mildly Important</cell><cell cols="2">38,617 10,967</cell><cell>5,429</cell><cell>4,441</cell><cell>17,780</cell></row><row><cell>Test No. Unimportant</cell><cell cols="2">33,613 12,965</cell><cell>4,429</cell><cell>3,172</cell><cell>13,047</cell></row><row><cell>Total No. Alert</cell><cell>4,780</cell><cell>709</cell><cell>3,036</cell><cell>190</cell><cell>845</cell></row><row><cell>Total No. Mildly Important</cell><cell cols="2">56,578 16,355</cell><cell>7,938</cell><cell>6,390</cell><cell>25,895</cell></row><row><cell>Total No. Unimportant</cell><cell cols="2">48,992 18,707</cell><cell>6,832</cell><cell>4,547</cell><cell>18,906</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,136.16,115.91,343.04,178.36"><head>Table 6 .</head><label>6</label><figDesc>RepLab 2013 agreement: analysis of 14 entities labeled by two annotators.</figDesc><table coords="9,136.16,139.87,343.04,154.40"><row><cell></cell><cell cols="4">% Agreement Cohen κ Fleiss κ F1(R, S)</cell></row><row><cell>Training Filtering</cell><cell>94.80</cell><cell>70.01</cell><cell>68.84</cell><cell>-</cell></row><row><cell>Training Polarity</cell><cell>68.27</cell><cell>41.04</cell><cell>38.93</cell><cell>-</cell></row><row><cell>Training Topic Detection</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>49.59</cell></row><row><cell>Training Priority Detection</cell><cell>58.41</cell><cell>23.96</cell><cell>15.96</cell><cell>-</cell></row><row><cell>Test Filtering</cell><cell>96.46</cell><cell>68.00</cell><cell>67.86</cell><cell>-</cell></row><row><cell>Test Polarity</cell><cell>68.81</cell><cell>42.26</cell><cell>39.92</cell><cell>-</cell></row><row><cell>Test Topic Detection</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>48.07</cell></row><row><cell>Test Priority Detection</cell><cell>60.88</cell><cell>29.29</cell><cell>20.91</cell><cell>-</cell></row><row><cell>Total Filtering</cell><cell>95.94</cell><cell>66.69</cell><cell>66.35</cell><cell>-</cell></row><row><cell>Total Polarity</cell><cell>68.59</cell><cell>41.93</cell><cell>39.79</cell><cell>-</cell></row><row><cell>Total Topic Detection</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Total Priority Detection</cell><cell>60.07</cell><cell>28.04</cell><cell>20.24</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="10,136.16,115.91,343.04,178.36"><head>Table 7 .</head><label>7</label><figDesc>RepLab 2013 agreement analysis of 3 entities labeled by three annotators.</figDesc><table coords="10,136.16,139.87,343.04,154.40"><row><cell></cell><cell cols="3">% Agreement Fleiss κ Average(F1(R, S))</cell></row><row><cell>Training Filtering</cell><cell>92.46</cell><cell>56.63</cell><cell>-</cell></row><row><cell>Training Polarity</cell><cell>48.81</cell><cell>36.75</cell><cell>-p</cell></row><row><cell>Training Topic Detection</cell><cell>-</cell><cell>-</cell><cell>48.11</cell></row><row><cell>Training Priority Detection</cell><cell>46.89</cell><cell>27.23</cell><cell>-</cell></row><row><cell>Test Filtering</cell><cell>91.54</cell><cell>59.60</cell><cell>-</cell></row><row><cell>Test Polarity</cell><cell>51.98</cell><cell>39.11</cell><cell>-</cell></row><row><cell>Test Topic Detection</cell><cell>-</cell><cell>-</cell><cell>51.33</cell></row><row><cell>Test Priority Detection</cell><cell>53.93</cell><cell>36.04</cell><cell>-</cell></row><row><cell>Total Filtering</cell><cell>91.83</cell><cell>59.59</cell><cell>-</cell></row><row><cell>Total Polarity</cell><cell>51.03</cell><cell>38.59</cell><cell>-</cell></row><row><cell>Total Topic Detection</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Total Priority Detection</cell><cell>51.72</cell><cell>33.38</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="11,136.16,115.91,343.04,278.77"><head>Table 8 .</head><label>8</label><figDesc>List of participants: acronyms and affiliation.</figDesc><table coords="11,136.16,139.90,343.04,254.78"><row><cell>Acronym</cell><cell>Affiliation</cell><cell>Country</cell></row><row><cell>CIRGDISCO</cell><cell>National University of Ireland,</cell><cell>Ireland</cell></row><row><cell></cell><cell>Galway</cell><cell></cell></row><row><cell>Daedalus</cell><cell>Daedalus, S.A.</cell><cell>Spain</cell></row><row><cell>DIUE</cell><cell>Universidade de Évora</cell><cell>Portugal</cell></row><row><cell>GAVKTH</cell><cell>Gavagai</cell><cell>Sweden</cell></row><row><cell>IE</cell><cell>National University of Singapore</cell><cell>Singapore</cell></row><row><cell>LIA</cell><cell>University of Avignon</cell><cell>France</cell></row><row><cell cols="2">NLP&amp;IR GROUP UNED UNED</cell><cell>Spain</cell></row><row><cell>POPSTAR</cell><cell>Universidade Porto</cell><cell>Portugal</cell></row><row><cell>REINA</cell><cell>Reina Research Group,</cell><cell>Spain</cell></row><row><cell></cell><cell>University of Salamanca</cell><cell></cell></row><row><cell>SZTE NLP</cell><cell>University of Szeged</cell><cell>Hungary</cell></row><row><cell>UAMCLYR</cell><cell cols="2">Universidad Autónoma Metropolitana Mexico</cell></row><row><cell></cell><cell>Cuajimalpa</cell><cell></cell></row><row><cell>UNED ORM</cell><cell>UNED</cell><cell>Spain</cell></row><row><cell>UNED-READERS*</cell><cell>UNED</cell><cell>Spain</cell></row><row><cell>UNEDTECNALIA</cell><cell>Tecnalia Research And Innovation,</cell><cell>Spain</cell></row><row><cell></cell><cell>UNED</cell><cell></cell></row><row><cell>UVA UNED</cell><cell>University of Amsterdam,</cell><cell>The Netherlands,</cell></row><row><cell></cell><cell>UNED</cell><cell>Spain</cell></row><row><cell>volvam</cell><cell>Volvam Analytics and</cell><cell>Ireland,</cell></row><row><cell></cell><cell>University of Alicante</cell><cell>Spain</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="14,134.77,115.91,345.83,288.91"><head>Table 9 .</head><label>9</label><figDesc>Accuracy, ratio of processed tweets, correlation at entity level, Reliability and Sensitivity for polarity task.</figDesc><table coords="14,136.16,150.86,343.04,253.96"><row><cell>Run</cell><cell cols="4">Acc. Processed Tweet Corr. Ent. R S</cell><cell>F</cell></row><row><cell></cell><cell></cell><cell>ratio</cell><cell>Level</cell><cell></cell></row><row><cell>SZTE NLP 8</cell><cell>0.69</cell><cell>1.00</cell><cell>0.88</cell><cell cols="2">0.48 0.34 0.38</cell></row><row><cell>LIA 7</cell><cell>0.65</cell><cell>1.00</cell><cell>0.82</cell><cell cols="2">0.50 0.15 0.19</cell></row><row><cell>POPSTAR 5</cell><cell>0.64</cell><cell>0.98</cell><cell>0.89</cell><cell cols="2">0.43 0.34 0.37</cell></row><row><cell>UAMCLYR 2</cell><cell>0.62</cell><cell>1.00</cell><cell>0.82</cell><cell cols="2">0.38 0.27 0.29</cell></row><row><cell>UNED ORM 2</cell><cell>0.62</cell><cell>1.00</cell><cell>0.70</cell><cell cols="2">0.36 0.10 0.15</cell></row><row><cell>LIA 3</cell><cell>0.60</cell><cell>1.00</cell><cell>0.64</cell><cell cols="2">0.37 0.27 0.29</cell></row><row><cell>UNED ORM 1</cell><cell>0.59</cell><cell>1.00</cell><cell>0.87</cell><cell cols="2">0.32 0.29 0.30</cell></row><row><cell>Baseline</cell><cell>0.58</cell><cell>1.00</cell><cell>0.87</cell><cell cols="2">0.32 0.29 0.30</cell></row><row><cell>NLP IR UNED 1</cell><cell>0.58</cell><cell>1.00</cell><cell>0.79</cell><cell cols="2">0.33 0.31 0.32</cell></row><row><cell>UAMCLYR 05</cell><cell>0.58</cell><cell>1.00</cell><cell>0.78</cell><cell cols="2">0.33 0.29 0.30</cell></row><row><cell>IE 6</cell><cell>0.58</cell><cell>1.00</cell><cell>0.22</cell><cell cols="2">0.94 0.00 0.00</cell></row><row><cell>ALL POSITIVE</cell><cell>0.58</cell><cell>1.00</cell><cell>0.00</cell><cell cols="2">1.00 0.00 0.00</cell></row><row><cell>DIUE 1</cell><cell>0.55</cell><cell>1.00</cell><cell>0.21</cell><cell cols="2">0.33 0.22 0.25</cell></row><row><cell>VOLVAM 3</cell><cell>0.54</cell><cell>1.00</cell><cell>0.36</cell><cell cols="2">0.32 0.23 0.26</cell></row><row><cell>IE 5</cell><cell>0.52</cell><cell>1.00</cell><cell>0.18</cell><cell cols="2">0.29 0.22 0.21</cell></row><row><cell>Daedalus 3</cell><cell>0.44</cell><cell>1.00</cell><cell>0.52</cell><cell cols="2">0.31 0.40 0.34</cell></row><row><cell>volvam 2</cell><cell>0.41</cell><cell>1.00</cell><cell>0.38</cell><cell cols="2">0.31 0.39 0.34</cell></row><row><cell>GAVKTH 6</cell><cell>0.37</cell><cell>0.98</cell><cell>0.49</cell><cell cols="2">0.30 0.21 0.24</cell></row><row><cell>ALL NEUTRAL</cell><cell>0.27</cell><cell>1.00</cell><cell>0.00</cell><cell cols="2">1.00 0.00 0.00</cell></row><row><cell>GAVKTH 2</cell><cell>0.26</cell><cell>0.82</cell><cell>0.21</cell><cell cols="2">0.37 0.21 0.27</cell></row><row><cell>ALL NEGATIVE</cell><cell>0.15</cell><cell>1.00</cell><cell>0.00</cell><cell cols="2">1.00 0.00 0.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="15,136.16,115.91,343.04,266.99"><head>Table 10 .</head><label>10</label><figDesc>Results for the Filtering Subtask.</figDesc><table coords="15,136.16,139.90,17.02,7.86"><row><cell>Run</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="16,136.16,276.80,343.04,168.36"><head>Table 11 .</head><label>11</label><figDesc>Accuracy, Reliability and Sensitivity Results for the Priority Subtask.</figDesc><table coords="16,435.94,300.78,43.26,7.86"><row><cell>Amount of</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="17,136.16,115.91,343.04,157.40"><head>Table 12 .</head><label>12</label><figDesc>Reliability and Sensitivity in the Topic Detection Task.</figDesc><table coords="17,455.55,139.90,22.14,7.86"><row><cell>Ratio</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="18,136.16,115.91,343.04,343.70"><head>Table 13 .</head><label>13</label><figDesc>UIR analysis for the Topic Detection Task.</figDesc><table coords="18,136.16,139.90,343.04,319.71"><row><cell></cell><cell>Improves</cell><cell></cell><cell></cell><cell>Number of</cell></row><row><cell>RUN</cell><cell>runs</cell><cell></cell><cell></cell><cell>improved</cell></row><row><cell></cell><cell>UIR ≥ 0.2</cell><cell></cell><cell></cell><cell>runs</cell></row><row><cell>UAMCLYR 07</cell><cell cols="3">UAMCLYR 1,2,3,4,5,6 LIA 2,3,4 REINA 1</cell><cell>12</cell></row><row><cell></cell><cell cols="2">BASELINE UNED ORM 1</cell><cell></cell><cell></cell></row><row><cell>UNED ORM 2</cell><cell cols="3">LIA 1,2,3,4 UNED ORM 1,3,4,5,6,7 BASELINE</cell><cell>11</cell></row><row><cell>REINA 2</cell><cell>LIA 1,2,3,4</cell><cell>BASELINE</cell><cell>UAMCLYR 4</cell><cell>9</cell></row><row><cell></cell><cell cols="2">UNED ORM 1,6,7</cell><cell></cell><cell></cell></row><row><cell>UAMCLYR 8</cell><cell>LIA 2,4</cell><cell>UAMCLYR 1,2,3,4</cell><cell>BASELINE</cell><cell>8</cell></row><row><cell></cell><cell>UNED ORM 1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>UNED ORM 4</cell><cell cols="2">BASELINE UNED ORM 1,6 LIA 1,4</cell><cell></cell><cell>5</cell></row><row><cell>UNED ORM 5</cell><cell cols="2">BASELINE UNED ORM 1,6 LIA 1,4</cell><cell></cell><cell>5</cell></row><row><cell>REINA 1</cell><cell cols="3">UAMCLYR 3,4 BASELINE UNED ORM 1</cell><cell>4</cell></row><row><cell>UNED ORM 3</cell><cell cols="3">BASELINE UNED ORM 1 LIA 1 UNED ORM 6</cell><cell>4</cell></row><row><cell>UNED ORM 7</cell><cell cols="2">LIA 2,4 BASELINE UNED ORM 1</cell><cell></cell><cell>4</cell></row><row><cell>UAMCLYR 6</cell><cell cols="3">BASELINE UAMCLYR 4 UNED ORM 1</cell><cell>3</cell></row><row><cell>UAMCLYR 3</cell><cell cols="3">BASELINE UAMCLYR 4 UNED ORM 1</cell><cell>3</cell></row><row><cell cols="3">NLP IR UNED 10 NLP IR UNED 3,4,5</cell><cell></cell><cell>3</cell></row><row><cell>UAMCLYR 5</cell><cell cols="3">BASELINE UAMCLYR 04 UNED ORM 1</cell><cell>3</cell></row><row><cell cols="3">NLP IR UNED 8 NLP IR UNED 3,4,5</cell><cell></cell><cell>3</cell></row><row><cell cols="3">NLP IR UNED 9 NLP IR UNED 3,4,5</cell><cell></cell><cell>3</cell></row><row><cell>LIA 2</cell><cell cols="2">BASELINE UNED ORM 1</cell><cell></cell><cell>2</cell></row><row><cell>LIA 3</cell><cell cols="2">BASELINE UNED ORM 1</cell><cell></cell><cell>2</cell></row><row><cell>LIA 4</cell><cell cols="2">BASELINE UNED ORM 1</cell><cell></cell><cell>2</cell></row><row><cell>UNED ORM 6</cell><cell cols="2">BASELINE UNED ORM 1</cell><cell></cell><cell>2</cell></row><row><cell>UAMCLYR 01</cell><cell>UAMCLYR 02</cell><cell></cell><cell></cell><cell>1</cell></row><row><cell>UAMCLYR 04</cell><cell>BASELINE</cell><cell></cell><cell></cell><cell>1</cell></row><row><cell cols="3">NLP IR UNED 6 NLP IR UNED 4</cell><cell></cell><cell>1</cell></row><row><cell cols="3">NLP IR UNED 7 NLP IR UNED 4</cell><cell></cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" coords="19,248.61,115.91,118.13,144.70"><head>Table 14 .</head><label>14</label><figDesc>Full Task Results.</figDesc><table coords="19,250.77,138.15,113.82,122.45"><row><cell>Run</cell><cell>F measure</cell></row><row><cell cols="2">UNED ORM 2 0.19</cell></row><row><cell cols="2">UNED ORM 7 0.18</cell></row><row><cell cols="2">UNED ORM 4 0.17</cell></row><row><cell cols="2">UNED ORM 6 0.17</cell></row><row><cell cols="2">DAEDALUS 1..8 0.16</cell></row><row><cell cols="2">UNED ORM 1 0.16</cell></row><row><cell cols="2">UNED ORM 8 0.12</cell></row><row><cell cols="2">UNED ORM 3 0.11</cell></row><row><cell cols="2">UNED ORM 5 0.11</cell></row><row><cell cols="2">SZTE NLP 1..10 0.03</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15" coords="19,209.50,275.97,196.35,179.32"><head>Table 15 .</head><label>15</label><figDesc>UIR Analysis for the Full Task.</figDesc><table coords="19,209.50,299.95,196.35,155.33"><row><cell>Run 1</cell><cell>Run 2</cell><cell cols="3">Imp. Is imp. UIR</cell></row><row><cell cols="3">UNED ORM 2 UNED ORM 4 24</cell><cell>1</cell><cell>0.38</cell></row><row><cell cols="3">UNED ORM 2 UNED ORM 6 15</cell><cell>0</cell><cell>0.25</cell></row><row><cell cols="3">UNED ORM 3 UNED ORM 5 14</cell><cell>1</cell><cell>0.21</cell></row><row><cell>SZTE 7</cell><cell>SZTE 4</cell><cell>44</cell><cell cols="2">15 0.47</cell></row><row><cell>SZTE 7</cell><cell>SZTE 3</cell><cell>43</cell><cell cols="2">15 0.46</cell></row><row><cell>SZTE 7</cell><cell>SZTE 6</cell><cell>44</cell><cell cols="2">17 0.44</cell></row><row><cell>SZTE 7</cell><cell>SZTE 1</cell><cell>42</cell><cell cols="2">17 0.41</cell></row><row><cell>SZTE 7</cell><cell>SZTE 2</cell><cell>40</cell><cell cols="2">15 0.41</cell></row><row><cell>SZTE 7</cell><cell>SZTE 5</cell><cell>43</cell><cell cols="2">19 0.39</cell></row><row><cell>SZTE 7</cell><cell>SZTE 9</cell><cell>40</cell><cell cols="2">18 0.36</cell></row><row><cell>SZTE 7</cell><cell>SZTE 8</cell><cell>37</cell><cell cols="2">17 0.33</cell></row><row><cell>SZTE 7</cell><cell>SZTE 10</cell><cell>35</cell><cell cols="2">18 0.28</cell></row><row><cell>SZTE 10</cell><cell>SZTE 9</cell><cell>37</cell><cell cols="2">22 0.25</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_0" coords="2,144.73,656.80,127.56,7.86"><p>http://www.limosine-project.eu</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_1" coords="6,469.98,367.63,9.22,7.86;6,136.16,378.59,82.76,7.86;6,269.44,378.59,63.51,7.86;6,353.09,378.59,20.99,7.86;6,394.55,378.59,20.99,7.86;6,453.60,378.59,25.60,7.86;6,136.16,389.55,65.90,7.86;6,269.44,389.55,63.51,7.86;6,348.47,389.55,25.60,7.86;6,389.94,389.55,25.59,7.86;6,453.59,389.55,25.60,7.86;6,136.16,400.51,69.94,7.86;6,264.83,400.51,68.12,7.86;6,348.48,400.51,25.60,7.86;6,389.94,400.51,25.60,7.86;6,453.60,400.51,25.60,7.86;6,136.16,411.47,61.87,7.86"><p>20 Training No. Tweets 45,679 15,123 7,774 6,960 15,822 Test No. Tweets 96,848 31,785 16,621 14,944 33,498 Total No. Tweets 142,527 46,908 24,395 21,904 49,320 No. Tweets EN</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2" coords="6,264.83,411.47,68.12,7.86;6,348.48,411.47,25.60,7.86;6,389.94,411.47,25.60,7.86;6,453.60,411.47,25.60,7.86;6,136.16,422.43,60.08,7.86;6,269.44,422.43,25.60,7.86;6,311.96,422.43,20.99,7.86;6,353.08,422.43,20.99,7.86;6,394.54,422.43,20.99,7.86;6,453.60,422.43,25.60,7.86"><p>113,544 38,614 16,305 20,342 38,283 No. Tweets ES 28,983 8,294 8,090 1,562 11,037</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3" coords="9,144.73,634.88,335.86,7.86;9,144.73,645.84,335.86,7.86"><p>One additional group sent their results two days after the deadline, and their runs are reported here as "unofficial." An asterisk in tables indicates an unofficial result.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4" coords="9,144.73,656.80,212.93,7.86"><p>Daedalus, GAVKTH, SZTE NLP, and UNED ORM.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5" coords="14,144.73,656.80,324.59,7.86"><p>For the correlation computation, we assign 0, 1 and 2 for each class respectively.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements This research was partially supported by the <rs type="funder">European Community</rs>'s <rs type="programName">FP7 Programme</rs> under grant agreement nrs <rs type="grantNumber">258191</rs> (<rs type="programName">PROMISE Network of Excellence</rs>) and <rs type="grantNumber">288024</rs> (LiMoSINe), the <rs type="programName">ESF Research Network Program ELIAS</rs>, the <rs type="funder">Spanish Ministry of Education</rs> (<rs type="grantName">FPU</rs> grant <rs type="grantNumber">AP2009-0507</rs> and <rs type="funder">FPI</rs> grant <rs type="grantNumber">BES-2011-044328</rs>), the <rs type="funder">Spanish Ministry of Science and Innovation (Holopedia Project</rs>, <rs type="grantNumber">TIN2010-21128-C02</rs>), and the <rs type="funder">Regional Government of Madrid</rs> under <rs type="projectName">MA2-VICMR</rs> (<rs type="grantNumber">S2009/TIC-1542</rs>), the <rs type="funder">Netherlands Organisation for Scientific Research (NWO)</rs> under project nrs <rs type="grantNumber">640.004.802</rs>, <rs type="grantNumber">727.011.005</rs>, <rs type="grantNumber">612.001.116</rs>, <rs type="grantNumber">HOR-11-10</rs>, the <rs type="funder">Center for Creation, Content and Technology (CCCT)</rs>, the <rs type="projectName">QuaMerdes</rs> project funded by the <rs type="programName">CLARIN-nl program</rs>, the <rs type="projectName">TROVe</rs> project funded by the <rs type="programName">CLAR-IAH program</rs>, the <rs type="programName">Dutch national program</rs> <rs type="projectName">COMMIT</rs>, the <rs type="projectName">Elite Network Shifts</rs> project funded by the <rs type="funder">Royal Dutch Academy of Sciences (KNAW)</rs>, the <rs type="funder">Netherlands eScience Center</rs> under project number <rs type="grantNumber">027.012.105</rs> and the <rs type="programName">Yahoo! Faculty Research and Engagement Program</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ccsvPkR">
					<idno type="grant-number">258191</idno>
					<orgName type="program" subtype="full">FP7 Programme</orgName>
				</org>
				<org type="funding" xml:id="_mSEMP5g">
					<idno type="grant-number">288024</idno>
					<orgName type="program" subtype="full">PROMISE Network of Excellence</orgName>
				</org>
				<org type="funding" xml:id="_kVPZQzM">
					<idno type="grant-number">AP2009-0507</idno>
					<orgName type="grant-name">FPU</orgName>
					<orgName type="program" subtype="full">ESF Research Network Program ELIAS</orgName>
				</org>
				<org type="funding" xml:id="_4dXdNHE">
					<idno type="grant-number">BES-2011-044328</idno>
				</org>
				<org type="funding" xml:id="_AQ2K93f">
					<idno type="grant-number">TIN2010-21128-C02</idno>
				</org>
				<org type="funded-project" xml:id="_32q47pE">
					<idno type="grant-number">S2009/TIC-1542</idno>
					<orgName type="project" subtype="full">MA2-VICMR</orgName>
				</org>
				<org type="funding" xml:id="_VFrbZTs">
					<idno type="grant-number">640.004.802</idno>
				</org>
				<org type="funding" xml:id="_k3g3th6">
					<idno type="grant-number">727.011.005</idno>
				</org>
				<org type="funding" xml:id="_HrpX8gm">
					<idno type="grant-number">612.001.116</idno>
				</org>
				<org type="funded-project" xml:id="_DHNmUhP">
					<idno type="grant-number">HOR-11-10</idno>
					<orgName type="project" subtype="full">QuaMerdes</orgName>
					<orgName type="program" subtype="full">CLARIN-nl program</orgName>
				</org>
				<org type="funded-project" xml:id="_CGgqpwq">
					<orgName type="project" subtype="full">TROVe</orgName>
					<orgName type="program" subtype="full">CLAR-IAH program</orgName>
				</org>
				<org type="funded-project" xml:id="_kAgzJGS">
					<orgName type="project" subtype="full">COMMIT</orgName>
					<orgName type="program" subtype="full">Dutch national program</orgName>
				</org>
				<org type="funded-project" xml:id="_fMuGnxX">
					<orgName type="project" subtype="full">Elite Network Shifts</orgName>
				</org>
				<org type="funding" xml:id="_eGTsMvS">
					<idno type="grant-number">027.012.105</idno>
					<orgName type="program" subtype="full">Yahoo! Faculty Research and Engagement Program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="20,138.35,415.56,342.25,7.86;20,146.91,426.52,333.68,7.86;20,146.91,437.48,42.50,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="20,337.02,415.56,143.58,7.86;20,146.91,426.52,189.41,7.86">A comparison of extrinsic clustering evaluation metrics based on formal constraints</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amigó</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Artiles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="20,343.83,426.52,87.51,7.86">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="461" to="486" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,138.35,448.44,342.24,7.86;20,146.91,459.40,333.68,7.86;20,146.91,470.36,159.24,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="20,396.11,448.44,84.48,7.86;20,146.91,459.40,239.42,7.86">Overview of RepLab 2012: Evaluating Online Reputation Management Systems</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amigó</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Corujo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Meij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,410.04,459.40,70.55,7.86;20,146.91,470.36,130.57,7.86">CLEF 2012 Labs and Workshop Notebook Papers</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,138.35,481.32,342.24,7.86;20,146.91,492.28,333.68,7.86;20,146.91,503.24,224.22,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="20,343.87,481.32,136.72,7.86;20,146.91,492.28,295.86,7.86">Combining evaluation metrics via the unanimous improvement ratio and its application to clustering tasks</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amigó</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Artiles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="20,450.24,492.28,30.35,7.86;20,146.91,503.24,133.60,7.86">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="689" to="718" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,138.35,514.19,342.24,7.86;20,146.91,525.15,248.82,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="20,296.47,514.19,184.12,7.86;20,146.91,525.15,76.30,7.86">A General Evaluation Measure for Document Organization Tasks</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amigó</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,244.57,525.15,108.93,7.86">Proceedings of SIGIR 2013</title>
		<meeting>SIGIR 2013</meeting>
		<imprint>
			<date type="published" when="2013-07">jul 2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
