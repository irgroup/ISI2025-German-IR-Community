<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,140.54,115.96,334.29,12.62;1,156.37,133.89,302.62,12.62">In search of reputation assessment: experiences with polarity classification in RepLab 2013</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,285.69,171.56,43.97,8.74"><forename type="first">José</forename><surname>Saias</surname></persName>
							<email>jsaias@uevora.pt</email>
							<affiliation key="aff0">
								<orgName type="department">Departamento de Informática</orgName>
								<orgName type="institution">ECT Universidade de Évora</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,140.54,115.96,334.29,12.62;1,156.37,133.89,302.62,12.62">In search of reputation assessment: experiences with polarity classification in RepLab 2013</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">02DC0832B0E15C5D2CBA93054F4CA2A9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The diue system uses a supervised Machine Learning approach for the polarity classification subtask of RepLab. We used the Python NLTK for preprocessing, including file parsing, text analysis and feature extraction. Our best solution is a mixed strategy, combining bag-of-words with a limited set of features based on sentiment lexicons and superficial text analysis. This system begins by applying tokenization and lemmatization. Then each tweet content is analyzed and 18 features are obtained, related to presence of polarized term, negation before polarized expression and entity reference. For the first run, the learning and classification were performed with the Decision Tree algorithm, from the NLTK framework. In the second run, we used a pipeline of classifiers. The first classifier applies Naive Bayes in a bag-of-words feature model, with the 1500 most frequent words in the training set. The second classifier used the features from the first run plus another feature with the result from the previous classifier. Our system's best result had 0.54694 Accuracy and 0.31506 in F measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This article describes the participation of a group from the Department of Computer Science, at the University of Évora, in the RepLab track of the 2013 edition of Cross Language Evaluation Forum (CLEF). RepLab<ref type="foot" coords="1,374.81,547.00,3.97,6.12" target="#foot_0">1</ref> is a competitive evaluation exercise for online reputation management systems, organized as a CLEF lab activity<ref type="foot" coords="1,184.30,570.91,3.97,6.12" target="#foot_1">2</ref> . In this challenge, the reputation processing subtasks are:</p><p>1. tweet filtering: distinguish the tweets that are related to the entity from those who are not; 2. reputation polarity classification: detect if a tweet has a positive, negative or neutral impact on the entity reputation;</p><p>3. tweet clustering per entity related topic; 4. priority detection.</p><p>Systems can participate in the full monitoring task, with the combined results of the four subtasks, or present partial solutions to the global task, providing results for one or more subtasks.</p><p>In this first participation, we focused our attention on the polarity classification subtask, because this seems to be a key task in reputation analysis. We have a recent work in the area of sentiment analysis in social media <ref type="bibr" coords="2,413.43,212.50,9.96,8.74" target="#b0">[1]</ref>. Polarity for reputation is different from standard sentiment analysis for two reasons. Firstly, an objective text, without sentiment, may still affect an entity's reputation. And on the other hand, sometimes the polarity of the expressed sentiment may be contrary to the resulting polarity for the reputation of the target entity. Given these differences, we have designed the diue system, with a supervised Machine Learning approach for classifying the reputation polarity, as described in section 3. The following section presents some recent related work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In the previous edition of RepLab about 10 systems participated in subtask polarity classification for reputation. Most systems rely on a sentiment polarity based approach, adapted for the reputation task <ref type="bibr" coords="2,348.88,392.47,9.96,8.74" target="#b1">[2]</ref>.</p><p>DAEDALUS system <ref type="bibr" coords="2,226.10,404.42,10.52,8.74" target="#b2">[3]</ref> has a model with rules and annotated resources for sentiment analysis. It applies an aggregation algorithm to calculate the polarity value based on the individual text segments polarity values. Morphosyntactic analysis is performed, for lemmatize, divide the text and detect negation. The approach from FBM/Yahoo! system <ref type="bibr" coords="2,298.06,452.24,10.52,8.74" target="#b3">[4]</ref> relies on lexicon-based techniques and Support Vector Machines classifiers. The UNED system <ref type="bibr" coords="2,385.49,464.20,10.52,8.74" target="#b4">[5]</ref> adapts an existing emotional concept-based system for sentiment analysis to determine polarity for entity reputation. Its approach includes the detection of negation and intensifiers, in order to deal with the effect of subordinate sentences.</p><p>The ILPS system <ref type="bibr" coords="2,214.64,512.02,10.52,8.74" target="#b5">[6]</ref> classifies the polarity of a tweet based on the observation of the reactions to that tweet, such as replies and retweets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Experiments</head><p>The reputation processing is done on data from Twitter, in English or Spanish. Systems received a corpus of tweets in both languages, arranged in sets for each of the 61 entities <ref type="bibr" coords="2,210.78,620.25,9.96,8.74" target="#b6">[7]</ref>. Due to the Twitter's terms of service, the provided corpus did not include the content of tweets, but only the identifier codes, for each system then make its own reading.</p><p>Obtaining the tweets was a setback in our participation. The normal download API imposes a maximum number of hits per hour, being very time consuming. Because of our naivete, we did not anticipate the difficulties of fetching all tweets, and when we completed the process, we had only 24 hours to the end of the official submission period. This left little room for studying the data.</p><p>For each entity, it was given its name, the domain which the entity belongs to, and URL addresses of their homepage and Wikipedia entries, in English and in Spanish. Our system did not use the contents of the homepages nor Wikipedia. Additional background tweets for each entity, and external links mentioned in the tweets were also provided for the participating systems, but we lacked the time to prepare that preprocessing step.</p><p>The diue system uses a supervised Machine Learning approach for the polarity classification subtask of RepLab. As mentioned in section 1, we developed a recent work <ref type="bibr" coords="3,181.26,262.46,10.52,8.74" target="#b0">[1]</ref> on Sentiment Analysis in Twitter. Despite differences in polarity for reputation, the data structure and some initial treatment to apply to tweet text are identical. So we decided to use part of the previous procedure, adding features related to the entity reference and its reputation implication.</p><p>For the initial entity file handling and parsing, for the text analysis and feature extraction, and also to manage the output format, we used Python and the Natural Language Toolkit (NLTK), a framework with resources and programming libraries suitable for linguistic processing <ref type="bibr" coords="3,316.14,346.14,10.52,8.74" target="#b7">[8,</ref><ref type="bibr" coords="3,328.32,346.14,7.01,8.74" target="#b8">9]</ref>. Tweet text processing started with tokenization, in which the splitting was white space or punctuation based. Lemmatization was then applied through the NLTK WordNet Lemmatizer. Here began the differences in relation to language. This lemmatization would help only tweets in English, because it was not applied similar functionality for Spanish.</p><p>To help determine the polarity direction in some terms of the text, our system uses three sentiment lexicons for English terms, and another hand-built resource with 100 words in Spanish. AFINN <ref type="bibr" coords="3,287.74,441.78,15.50,8.74" target="#b9">[10]</ref> is a sentiment lexicon containing English words manually labeled by Finn Årup Nielsen, from 2009 to 2011. Words were rated between minus five (negative) and plus five (positive). SentiWordNet <ref type="bibr" coords="3,465.09,465.69,15.50,8.74" target="#b10">[11]</ref> is a lexical resource for opinion mining that assigns sentiment scores to each synset of WordNet<ref type="foot" coords="3,216.37,488.03,3.97,6.12" target="#foot_2">3</ref> . We apply a threshold, disregarding terms whose score absolute value is less than 0.3. By doing this, we look for sharper polarities, or greater confidence in the direction of polarity. The third English sentiment lexicon derived from Bing Liu's work <ref type="bibr" coords="3,286.83,525.47,15.50,8.74" target="#b11">[12]</ref> on online customer reviews of products.</p><p>After tokenization and lemmatization, each tweet content is analyzed for extracting the features to use in machine learning. In the first run, we decided not to use a bag-of-words model. Instead, we chose a more restricted set of 18 features involving:</p><p>presence of polarized term, using sentiment lexicons; negation before polarized expression; polarized term before entity reference; polarized term after entity reference; negation before entity reference;</p><p>entity reference followed by negation and polarized term.</p><p>Each of the above represents a group of features. The presence of polarized term is checked for all sentiment lexicons, generating a pair of boolean features for each, to signal the presence of an expression with negative polarity and the presence of a positive expression. The system also creates an overall sentiment value feature, determined by consulting all those lexicons and adding 1 or -1, for each polarized term in the tweet, according to the term polarity. The features involving the entity reference try to capture differences that the learning algorithm can then associate to positive or negative impact on reputation. The learning and classification were performed with the Decision Tree algorithm, from the NLTK framework. Each tweet in the training set is annotated with RE-LATED/UNRELATED (the tweet is/is not about the entity, for the filtering subtask) and POSITIVE/NEUTRAL/NEGATIVE to train the polarity classification.</p><p>When training our model, the system discards tweets not having the RELATED annotation, because these have no interest for the subtask.</p><p>In preliminary experiments, the accuracy returned by NLTK matched with the result obtained with the evaluation script provided by the organization for use in development phase. This accuracy was around 58%, so we generated the first run over the test data.</p><p>In the second run, we used a pipeline of classifiers. The first classifier applies Naive Bayes in a bag-of-words feature model, with the 1500 most frequent words in the training set. The second classifier used the features from the first run, plus one more feature with the result from the former classifier. In this second run, some errors were also corrected in the extraction of features. This was the case of the overall sentiment value calculation, which needed sometimes to invert the polarity of the values, when the source expression was affected by the negation. A small lemmatization related bug was also fixed.</p><p>For the last run, a few terms were introduced in the Spanish sentiment lexicon, and the overall sentiment value feature was turned off in the first classifier features.</p><p>At the end of the competition, the systems were given extra time to finish ongoing experiments, and also receive the assessment on those latter unofficial runs.</p><p>Our second and third runs were submitted during this extra period. Given the short time and the delays in downloading the tweets, our system still got 0.995 for the ratio of tweets in the goldstandard that have been processed. Next section describes the evaluation metrics and the results for the submitted runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>The systems involved in the polarity for reputation classification task are evaluated according to Accuracy, Reliability and Sensitivity. These latter two measures have already been used in RepLab 2012, and are described in <ref type="bibr" coords="4,434.62,644.16,14.61,8.74" target="#b12">[13]</ref>. Table <ref type="table" coords="4,134.77,656.12,4.98,8.74" target="#tab_0">1</ref> has the result of evaluating the three runs for the diue system. In the second column we can see the Accuracy, as the proportion of cases where the system guesses the right polarity class. The F column shows the balanced F measure combining Reliability and Sensitivity. The value shown in these four columns is the average for all entities. Pearson correlation, in the last column, is calculated between average polarity of entities according to the system versus the gold standard.</p><p>The last two runs are marked with * because they were submitted in the extra period, and thus were not considered as official runs in competition, despite being assessed.</p><p>The Accuracy is practically the same in all three cases, but better in the second run, with 0.54694. Reliability is higher in the first run, with a little difference. Sensitivity, F and Pearson correlation make clear the difference between the first run and the other two using the classifier pipeline, all having the best result in run 2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>If we looked only to the Accuracy values, we would eventually say that the runs have equal results, with 54% accuracy. But the results of each run are substantially different, in particular from the first to the other two runs. In the first run the system assigned the neutral polarity to 9804 tweets, while in the second run that number rose to 18586. Run 1 had about 13000 more positive tweets than run 2 and 3. The pipelined classifier brought the bag-of-words model to complement the previous model and to compensate some scarcity in that feature set. This is noticed in the F and Pearson correlation values evolution.</p><p>Let us now compare our modest results with the best systems in competition <ref type="bibr" coords="5,470.08,560.48,10.52,8.74" target="#b6">[7]</ref> in the same subtask. The best value of our system accuracy is 0.54694, while the best system managed 0.68596 (and it seems to have had no problems downloading the tweets, having 100% processed tweets). Considering the F measure, the best official result was 0.38166, and the average was 0.22672. Our system official run had 0.25467, and for the second run we got 0.31506. At first, we thought that the existence of two languages would be a bigger problem. Writing used in tweets is very informal and full of typos. In Spanish tweets can also appear emoticons and may even arise expressions in English that are commonly used. Therefore certain results could be achieved, even with the base system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>This was our first experience in RepLab challenge. Our system is not yet ready to the full reputation monitoring task. We have dedicated our efforts to the polarity classification subtask. Our best solution is a mixed strategy, combining bag-of-words with a limited set of features based on sentiment lexicons and superficial analysis of text.</p><p>If we repeated the process, we would have started downloading the tweets earlier, in order to have the time for experiences and analysis, and to choose the most appropriate feature set for this kind of data and purpose.</p><p>For future work, we highlight the importance of strengthening the resources of language support for Spanish, including lemmatization, and sentiment lexicon.</p><p>In the bag-of-words, we used only the 1500 most frequent words in the training set. Maybe we should increase the number of words/features. We consider NLTK very effective in text processing. For the future, however, we consider using another tool for machine learning, supporting more classification algorithms in the same friendly way, but, at the same time, allowing a greater degree of configuration.</p><p>Regardless of the results obtained by our system, we consider that the participation in this challenge was very positive, by its competitive spirit, the large-scale evaluation, and the sharing of new ideas in the treatment of reputation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,150.92,305.90,325.85,65.08"><head>Table 1 .</head><label>1</label><figDesc>Evaluation of polarity subtask results for diue system</figDesc><table coords="5,150.92,305.90,325.85,43.95"><row><cell></cell><cell cols="3">Accuracy Reliability Sensitivity</cell><cell>F</cell><cell>Pearson correlation</cell></row><row><cell>1</cell><cell>0.54688</cell><cell>0.33303</cell><cell>0.21516</cell><cell>0.25467</cell><cell>0.21398</cell></row><row><cell>2*</cell><cell>0.54694</cell><cell>0.32923</cell><cell>0.31620</cell><cell>0.31506</cell><cell>0.64769</cell></row><row><cell>3*</cell><cell>0.54603</cell><cell>0.32734</cell><cell>0.31470</cell><cell>0.31343</cell><cell>0.64572</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,645.84,205.21,7.86"><p>http://www.limosine-project.eu/events/replab2013</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,144.73,656.80,215.29,7.86"><p>http://clef2013.org/index.php?page=Pages/labs.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,144.73,656.80,123.44,7.86"><p>http://wordnet.princeton.edu/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,142.96,481.43,337.63,7.86;6,151.52,492.39,329.07,7.86;6,151.52,503.35,329.07,7.86;6,151.52,514.31,329.07,7.86;6,151.52,525.27,329.07,7.86;6,151.52,536.23,32.36,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,291.74,481.43,188.85,7.86;6,151.52,492.39,207.91,7.86">senti.ue-en: an approach for informally written short texts in semeval-2013 sentiment analysis task</title>
		<author>
			<persName coords=""><forename type="first">José</forename><surname>Saias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hilário</forename><surname>Fernandes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,380.87,492.39,99.72,7.86;6,151.52,503.35,201.34,7.86;6,407.21,503.35,73.39,7.86;6,151.52,514.31,299.14,7.86">Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013)</title>
		<meeting>the Seventh International Workshop on Semantic Evaluation (SemEval 2013)<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-06">June 2013</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="508" to="512" />
		</imprint>
	</monogr>
	<note>Second Joint Conference on Lexical and Computational Semantics (*SEM)</note>
</biblStruct>

<biblStruct coords="6,142.96,547.19,337.64,7.86;6,151.52,558.15,329.07,7.86;6,151.52,569.11,219.61,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,151.52,558.15,312.25,7.86">Overview of RepLab 2012: Evaluating online reputation management systems</title>
		<author>
			<persName coords=""><forename type="first">Enrique</forename><surname>Amigó</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adolfo</forename><surname>Corujo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Edgar</forename><surname>Meij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,151.52,569.11,190.96,7.86">CLEF (Online Working Notes/Labs/Workshop)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,580.07,337.64,7.86;6,151.52,591.03,329.07,7.86;6,151.52,601.99,201.42,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="6,296.01,591.03,184.58,7.86;6,151.52,601.99,111.91,7.86">Daedalus at replab 2012: Polarity classification and filtering on twitter data</title>
		<author>
			<persName coords=""><forename type="first">Julio</forename><surname>Villena-Román</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sara</forename><surname>Lana-Serrano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cristina</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Janine</forename><surname>García-Morera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">José</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">González</forename><surname>Cristóbal</surname></persName>
		</author>
		<editor>Forner et al.</editor>
		<imprint>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,612.96,337.64,7.86;6,151.52,623.92,134.58,7.86" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="6,421.47,612.96,59.13,7.86;6,151.52,623.92,25.11,7.86">Fbm-yahoo! at replab</title>
		<author>
			<persName coords=""><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Chenlo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jordi</forename><surname>Atserias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carlos</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roi</forename><surname>Blanco</surname></persName>
		</author>
		<editor>Forner et al.</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,634.88,337.64,7.86;6,151.52,645.84,329.07,7.86;6,151.52,656.80,81.32,7.86" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="6,406.88,634.88,73.72,7.86;6,151.52,645.84,325.02,7.86">Using an emotionbased model and sentiment analysis techniques to classify polarity for reputation</title>
		<author>
			<persName coords=""><forename type="first">Jorge</forename><surname>Carrillo De Albornoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Irina</forename><surname>Chugur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Enrique</forename><surname>Amigó</surname></persName>
		</author>
		<editor>Forner et al.</editor>
		<imprint>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,119.67,337.64,7.86;7,151.52,130.63,129.97,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="7,404.72,119.67,75.87,7.86;7,151.52,130.63,40.52,7.86">From sentiment to reputation</title>
		<author>
			<persName coords=""><forename type="first">Maria-Hendrike</forename><surname>Peetz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anne</forename><surname>Schuth</surname></persName>
		</author>
		<editor>Forner et al.</editor>
		<imprint>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,141.59,337.64,7.86;7,151.52,152.55,329.07,7.86;7,151.52,163.51,329.07,7.86;7,151.52,174.47,329.07,7.86;7,151.52,185.43,173.61,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,151.52,163.51,308.64,7.86">Overview of replab 2013: Evaluating online reputation monitoring systems</title>
		<author>
			<persName coords=""><forename type="first">Enrique</forename><surname>Amigó</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jorge</forename><surname>Carrillo De Albornoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Irina</forename><surname>Chugur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adolfo</forename><surname>Corujo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tamara</forename><surname>Martín</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Edgar</forename><surname>Meij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Damiano</forename><surname>Spina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,151.52,174.47,325.01,7.86">Fourth International Conference of the CLEF initiative -CLEF 2013 Proceedings</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Springer LNCS</publisher>
			<date type="published" when="2013-09">Sep 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,196.39,337.64,7.86;7,151.52,207.34,329.07,7.86;7,151.52,218.30,329.07,7.86;7,151.52,229.26,275.07,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,282.25,196.39,134.18,7.86">Nltk: the natural language toolkit</title>
		<author>
			<persName coords=""><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,434.71,196.39,45.88,7.86;7,151.52,207.34,329.07,7.86;7,151.52,218.30,212.13,7.86;7,414.72,218.30,62.29,7.86">Proceedings of the ACL-02 Workshop on Effective tools and methodologies for teaching natural language processing and computational linguistics</title>
		<meeting>the ACL-02 Workshop on Effective tools and methodologies for teaching natural language processing and computational linguistics<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="63" to="70" />
		</imprint>
	</monogr>
	<note>ETMTNLP &apos;02</note>
</biblStruct>

<biblStruct coords="7,142.96,240.22,337.63,7.86;7,151.52,251.18,20.99,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="7,211.87,240.22,191.49,7.86">Python Text Processing with NLTK 2.0 Cookbook</title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Perkins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Packt Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.62,260.55,337.98,9.45;7,151.52,273.10,329.07,7.86;7,151.52,284.06,74.80,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,231.00,262.14,249.59,7.86;7,151.52,273.10,53.30,7.86">A new ANEW: Evaluation of a word list for sentiment analysis in microblogs</title>
		<author>
			<persName coords=""><surname>Finn Årup Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,228.81,273.10,246.50,7.86">1st Workshop on Making Sense of Microposts (#MSM2011)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="93" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.62,295.02,337.98,7.86;7,151.52,305.98,329.07,7.86;7,151.52,316.93,329.07,7.86;7,151.52,327.89,329.07,7.86;7,151.52,338.85,329.07,7.86;7,151.52,349.81,329.07,7.86;7,151.52,360.77,35.45,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,395.26,295.02,85.33,7.86;7,151.52,305.98,272.42,7.86">Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining</title>
		<author>
			<persName coords=""><forename type="first">Stefano</forename><surname>Baccianella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,443.99,305.98,36.60,7.86;7,151.52,316.93,115.25,7.86;7,434.71,327.89,45.88,7.86;7,151.52,338.85,329.07,7.86;7,151.52,349.81,42.87,7.86">Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC&apos;10)</title>
		<editor>
			<persName><forename type="first">Khalid</forename><surname>Choukri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bente</forename><surname>Maegaard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Joseph</forename><surname>Mariani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jan</forename><surname>Odijk</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stelios</forename><surname>Piperidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mike</forename><surname>Rosner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Daniel</forename><surname>Tapias</surname></persName>
		</editor>
		<meeting>the Seventh International Conference on Language Resources and Evaluation (LREC&apos;10)<address><addrLine>Valletta, Malta</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association (ELRA</publisher>
			<date type="published" when="2010-05">May 2010</date>
		</imprint>
	</monogr>
	<note>Nicoletta Calzolari (Conference Chair)</note>
</biblStruct>

<biblStruct coords="7,142.62,371.73,337.98,7.86;7,151.52,382.69,329.07,7.86;7,151.52,393.65,135.72,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,193.23,371.73,258.75,7.86">Opinion observer: Analyzing and comparing opinions on the web</title>
		<author>
			<persName coords=""><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,151.52,382.69,324.01,7.86">WWW &apos;05: Proceedings of the 14th international conference on World Wide Web</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="342" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,404.61,337.97,7.86;7,151.52,415.56,292.20,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,361.32,404.61,119.27,7.86;7,151.52,415.56,128.42,7.86">A general evaluation measure for document organization tasks</title>
		<author>
			<persName coords=""><forename type="first">Enrique</forename><surname>Amigó</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Felisa</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,299.36,415.56,95.67,7.86">Proceedings SIGIR 2013</title>
		<meeting>SIGIR 2013</meeting>
		<imprint>
			<date type="published" when="2013-07">July 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,426.52,337.98,7.86;7,151.52,437.48,329.07,7.86;7,151.52,448.44,73.67,7.86" xml:id="b13">
	<monogr>
		<title level="m" coord="7,433.81,426.52,46.78,7.86;7,151.52,437.48,225.19,7.86">CLEF 2012 Evaluation Labs and Workshop, Online Working Notes</title>
		<editor>
			<persName><forename type="first">Pamela</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jussi</forename><surname>Karlgren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Christa</forename><surname>Womser-Hacker</surname></persName>
		</editor>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">September 17-20, 2012, 2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
