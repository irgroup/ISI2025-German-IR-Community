<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,221.72,116.90,171.92,12.90;1,181.41,134.83,252.54,12.90">A HMM-based Approach to Question Answering against Linked Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,186.75,172.87,70.69,8.64"><forename type="first">Cristina</forename><surname>Giannone</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Enterprise Engineering</orgName>
								<orgName type="institution">University of Rome Tor Vergata</orgName>
								<address>
									<settlement>Roma</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,264.51,172.87,82.63,8.64"><forename type="first">Valentina</forename><surname>Bellomaria</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Enterprise Engineering</orgName>
								<orgName type="institution">University of Rome Tor Vergata</orgName>
								<address>
									<settlement>Roma</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,370.76,172.87,57.84,8.64"><forename type="first">Roberto</forename><surname>Basili</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Enterprise Engineering</orgName>
								<orgName type="institution">University of Rome Tor Vergata</orgName>
								<address>
									<settlement>Roma</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,221.72,116.90,171.92,12.90;1,181.41,134.83,252.54,12.90">A HMM-based Approach to Question Answering against Linked Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">82A9546C00F50CE1FB76D63589264996</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a QA system enabling NL questions against Linked Data, designed and adopted by the Tor Vergata University AI group in the QALD-3 evaluation. The system integrates lexical semantic modeling and statistical inference within a complex architecture that decomposes the NL interpretation task into a cascade of three different stages: (1) The selection of key ontological information from the question (i.e. predicate, arguments and properties), (2) the location of such salient information in the ontology through the joint disambiguation of the different candidates and (3) the compilation of the final SPARQL query. This architecture characterizes a novel approach for the task and exploits a graphical model (i.e. an Hidden Markov Model) to select the proper ontological triples according to the graph nature of RDF. In particular, for each query an HMM model is produced whose Viterbi solution is the comprehensive joint disambiguation across the sentence elements. The combination of these approaches achieved interesting results in the QALD competition. The RTV is in fact within the group of participants performing slightly below the best system, but with smaller requirements and on significantly poorer input information.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Language is the most powerful media for acquiring, communicating and sharing knowledge. It has been optimized through centuries of use, successes and failures. Although the Web of Data claims for machine readable standards, natural language is still the preferred query language for naive users, early adopters and even experts in some knowledge domains. Question answering is thus the crucial bottleneck for a truly and universal adoption of Open Linked Data as a knowledge sharing paradigm and practice.</p><p>In general, approaches for question answering range between rule-based (and strongly deductive) systems, whose expressivity is harmonic with the knowledge representation standards in the Web and whose precision is optimal, to shallow, basically lexicalist approaches very close to the bag-of-words practices in document retrieval processes. In the first family of systems, we could mention at least Swoogle <ref type="bibr" coords="1,402.49,585.70,11.62,8.64" target="#b2">[3]</ref> or Sindice <ref type="bibr" coords="1,461.50,585.70,15.27,8.64" target="#b12">[13]</ref>, where entity search exploiting the Linked Data constraints are formulated and high level of precision are in general achieved. Shortcomings of these approaches come from the naive (user's) dictionaries that can be very different from the data dictionary. In <ref type="bibr" coords="1,456.80,621.57,16.60,8.64" target="#b14">[15]</ref> a system that produces a SPARQL template that directly mirrors the internal structure of the question and then instantiates the template, using statistical entity identification and predicate detection is proposed. It relies on a linguistic analysis and adopts DRT over parse trees through the extension of the Pythia system <ref type="bibr" coords="2,349.97,120.31,15.27,8.64" target="#b15">[16]</ref>. By applying deep linguistic analysis Pythia is demanding w.r.t. the lexical and grammatical knowledge needed to cope with complex questions in heterogeneous domains. A general approach to question answering over Linked Data is described in PowerAqua <ref type="bibr" coords="2,362.94,156.17,10.58,8.64" target="#b7">[8]</ref>, that makes light assumptions on the ontology vocabulary or schema and emphasizes the combination of large data sets through filtering and ranking heuristics. The weaker linguistic component in Poweraqua makes the treatment of complex questions difficult.</p><p>An interesting vocabulary-independent approach is attempted in <ref type="bibr" coords="2,416.57,204.50,11.62,8.64" target="#b4">[5]</ref> which combines entity search and lexical similarity metrics to compute semantic relatedness and apply spreading activation onto RDF graphs. This work shares with the approach above presented a lexicalist perspective that rely on a strong model of lexical semantic information to solve most of the ambiguity and uncertainty problems arising in the interpretation of the question. A similar combination of statistical inference and logic-based representation is adopted in approaches focused on Semantic Parsing (e.g. <ref type="bibr" coords="2,438.24,276.23,11.20,8.64" target="#b1">[2]</ref>) where graphical models are used to converge towards the correct interpretation of a sentence in a predicate logic form, as well as in <ref type="bibr" coords="2,299.16,300.14,16.60,8.64" target="#b11">[12]</ref> where a HMM is employed in order to obtain the right RDF subgraph for a natural language query, here the observed data and the background knowledge is used for the HMM parameter estimation.</p><p>In line with the latter, in this paper we present a novel contribution to the above research line which combines symbolic reasoning over the semantic constraints on the underlying Open Data repository and statistical inference. This latter is useful to manage the ambiguity introduced by natural language within a complex process for the interpretation of the question, that integrates distributional semantic models of lexical information and probabilistic inference. In this way, we aim at solving at least two problems. The first one is the localization and retrieval of ontological elements evoked by a question without relying on strict hypothesis on the resource vocabulary. Second, we jointly solve the different ambiguities arising in the interpretation by integrating question grammatical structures and ontology information. The idea is to map the different inferences into a generative graphical model, i.e. an Hidden Markov Model of the question. It works as a bridge between the linguistic and the RDF structures, i.e the syntactic dependency graph of the question on the one side and the full paths in the RDF graph, on the other. The rest of the paper is organized as follow: Section 2 presents an overview of the systems architecture and introduces the HMM notions. Section 3.1 discusses in more detail the modeling of the input question through a Hidden Markov Model, and the resulting RDF sub-graph obtained. The process of mapping the HMM output into the SPARL Template is discussed in Section 3.4. Finally, in Section 4, a first analysis of the QALD-3 results are discussed with some final considerations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Architectural Overview</head><p>The overall architecture of the RTV 1 system is shown in Figure <ref type="figure" coords="2,391.02,612.51,4.98,8.64" target="#fig_0">1</ref> where the core components of the system are within the main box and interacts with auxiliary preexisting modules and resources. The treatment of a sentence as carried out by the core system can be summarized as follows. First an analysis of the syntactic (dependency) graph is carried out where proper nouns related to DBpedia resources are detected and classified in the so-called HMM initialization stage. In the HMM all the observations about key entities or classes are selected from the question, as they refer to ontological elements, i.e. mentions to entities, literals and relations (or properties). In the HMM modeling stage, the states, emissions and transitions of the Markov chain are defined. States corresponds to the RDF elements retrieved by a question fragment from DBpedia: these elements may correspond to resources, classes or relations. The HMM best sequence of states is computed in the decoding module, here a disambiguation process is imposed exploiting the combination of statistical and ontological constraints obtaining a state sequence which can be mapped into a RDF sub graph.</p><p>The process, triggered by a grammatical analysis of the input question, foresees three main components deal with the initialization, modeling and decoding of the HMM and a final stage to compile the resulting SPARQL query. A detailed description of such modules is given in Section 3. Besides a syntactic parser, i.e. Chaos <ref type="bibr" coords="3,298.29,601.04,11.62,8.64" target="#b0">[1]</ref> whose result is a chunk-based dependency graph, a number of external resources are made available:</p><p>-A wordspace in line with distributional semantic methods <ref type="bibr" coords="3,379.44,633.52,15.77,8.64" target="#b10">[11,</ref><ref type="bibr" coords="3,396.87,633.52,12.45,8.64" target="#b9">10,</ref><ref type="bibr" coords="3,410.99,633.52,8.30,8.64" target="#b8">9]</ref> is derived from the Wikipedia corpus. It provides lexical entries in form of real-valued vectors for a large dictionary including Named Entities as well as multiword expressions. Lex-ical similarity metrics is thus made available between word pairs, modeled in the space as cosine similarity. -Lucene<ref type="foot" coords="4,181.02,142.66,3.49,6.05" target="#foot_0">2</ref> has been applied as a retrieval tool for DBpedia concepts. DBpedia resources are retrieved (and properly ranked) as pseudo-documents, whereas grammatically meaningful fragments of the question can used as queries. Notice how resources or fragments can be mapped into the above wordspace <ref type="foot" coords="4,409.17,178.52,3.49,6.05" target="#foot_1">3</ref> . Their similarity can be thus applied to rank the retrieved candidates in a semantically meaningful manner; -DBpedia labels are also exploited, as they provide an extensive catalogue of Proper Nouns made available to the Chaos parser <ref type="bibr" coords="4,323.31,228.12,10.58,8.64" target="#b0">[1]</ref>. Given an input question, the parser carried out the recognition of most resource names (as possibly ambiguous Named Entities) as well as of the syntactic dependencies they are involved in.</p><p>3 An HMM-based QA system over Linked Data</p><p>Our objective is the modeling the interpretation of the natural language query q into the ontology domain D through the following set of HMM random variables:</p><p>-The observation set L = {l 1 , ..., l m }, in our case the observation set is computed over the set of symbols of the sentence q, the set of States O = {o 1 , ..., o n }, that models the ontological resources ∈ D candidate to be a suitable interpretation for the observation set L, the emission matrix E : L × O → [0, 1] models the probability that an observation l evokes the state o, i.e., the corresponding ontological resource, the transition matrix T : O × O → [0, 1] models the transition probability from the state o i the the state o j , these probability model the reachability of the state o j from the state o i . They help explaining the semantics of relationships between hidden states, i.e. inferential steps that correspond to the transversal of RDF graphs along specific paths.</p><p>Both emissions and transition probabilities are estimated on the fly, i.e. against the incoming sentence, just before applying the traditional decoding algorithms (i.e. Viterbi) to the resulting HMM. The outcome of the decoding stage is a full RDF path, that corresponds to one interpretation of classes (such as Person), instances as well as relations.</p><p>In the next subsections each step of the HMM modeling is discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Initializing the HMM graph</head><p>The sentence segmentation for the generation of the observation set L is driven by the syntactical structure of the sentence q. The HMM initialization process proceeds by selecting the root of the dependency graph and navigating all the syntactic relations, by thus enumerating the entire set of predicate expressions (e.g. die in the sentence in Figure <ref type="figure" coords="5,174.90,355.22,4.15,8.64" target="#fig_1">2</ref>) as well as their direct arguments (such as Where and Abraham Lincoln).</p><p>The generated set of HMM observations is thus bipartite into two sets: nodes (that corresponds to nodes or literals in the RDF graph) and relations (i.e. links in the RDF graph), alternating in the Markov chain.</p><p>The resulting sets of nodes and links gives rise to one chain with the precedence assigned to the focus of the question (i.e. the place introduced by Where) used as initial node). The resulting observation chain for the sentence Where did Abraham Lincoln die? is shown at the bottom of Fig. <ref type="figure" coords="5,285.20,439.02,4.98,8.64" target="#fig_1">2</ref> in which Where corresponds to the first node and the rest of the chain links together the other different nodes and relations. Notice how relations (e.g. die) alternate with elements but all of them align into a sequence. Moreover, proper nouns (e.g. Abraham Lincoln) are specifically treated as the Chaos parser lexicon has been enriched in the experiments with the catalogue derived from the values of the field label in DBpedia elements.</p><p>The analysis of the sentence ends when all grammatically relevant elements (basically nouns, verbs and adjective) give rise to an observation in the targeted Markov chain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Modeling question semantics through states, transitions and emissions: the HMM modeling stage</head><p>During the HMM initialization, key elements in the sentence are detected and their associated ontological elements are located in the ontology. Key elements correspond to linguistic expressions defined as observations. In this way, a sequence corresponding to a Markov chain is generated. In the interpretation process targeted here, the HMM is designed as a generative model of the question. The model provides statistical evidence about the way a question is generated as a request against RDF resources that form a graph. In order to fully define the HMM, we still need to make the set of observations L correspond to states O, and determine the corresponding transition and emission probabilities. The following modeling choices must be carried out:</p><p>-How to map linguistic expressions, such as proper nouns (e.g. Where, Abraham Lincoln) or predicate structures (e.g. die), to ontological elements such as DBpedia instances (e.g. possibly ambiguous Wikipedia pages) or relations such as deathDate -How to model the notion of emissions E, that characterize the relationship between hidden states and observable linguistic structures -How to model the notion of transitions T that help explaining the semantics of relationships between hidden states, i.e. inferential steps that correspond to the transversal of RDF graphs along specific paths</p><p>In this work, we mapped DBpedia resources, classes as well as relations or properties into HMM states: they justify (as emissions) the observable linguistic structures, i.e. key entities of the question. While observations correspond to an open set of linguistic symbols W , by relying on a large scale semantic lexicon developed through corpus analysis, we will be able to map them into ontology element set D. Transitions corresponds to links between RDF elements and have a clear ontological status determined by DBpedia. Notice how probabilities equal to 0 can be used to constraint the semantics of a transition and model DBpedia links. On the other hand, more plausible interpretations receive higher probabilities, according to the context of the question.</p><p>Estimating emissions probabilities through distributional semantics An emission connects a linguistic expression l to an ontological element o and its probability in the HMM is p(l|o)</p><p>The population of the state space and the estimation of p(l|o) is defined through an information retrieval process. First, the linguistic expression l related to a fragment of the dependency graph is generated. Ontological elements o for different linguistic expressions l are also different, and they must be precisely located in the ontology:</p><p>when instances are involved (as proper nouns are detected in the question), first the query is executed against a search engine devoted to index the different DBpedia instances; the index is built over the long abstracts of the resource made available by DBpedia. The query corresponds to the expression l itself (e.g. Abraham Lincoln) and a number of resources (in which l is more or less explicitly mentioned) are retrieved. As linguistic expressions are represented in vectors (as in <ref type="bibr" coords="6,444.00,570.24,14.94,8.64" target="#b9">[10]</ref>), after retrieval, a further reranking stage is applied. Every returned candidate o also consists in a linguistic expression and a vector o, whose similarity with the vector l corresponding to l, is used as the final ranking score: candidates o that are semantically more related to the entire question, i.e. their vector in the space is closer to the vector l, are given higher priority. More in detail the following probability estimate is used p(l|o i ) =</p><formula xml:id="formula_0" coords="6,266.39,639.25,56.82,16.75">αi+η1 k j=1 αj +(k+1)η1</formula><p>where the factors α i represent the similarity score between the linguistic expression l and the ontological resource (i.e. DBpedia entity) o i , as it is measured in the word space: details of the estimation are given in the next subsection.</p><p>when a common noun is involved (as it does not allow to retrieve any specific resource), a corresponding class is searched for. Classes o in DBpedia are indexed beforehand: the expressions found in the label field of the concept o correspond to a vector representation based on the description field of the corresponding DBpedia resource. If a nominal chunk is extracted from a question, ontological classes o can be retrieved if their vector are more similar to l than a given threshold 4 in the space. For every noun l, multiple classes are retrieved in general corresponding to the DBpedia classes available closer in the vector space. The estimation of the p(l|o) is again based on the similarity measure between l and o. More in detail the following probability estimate is used p(l|o i ) = βi+η2 k j=1 βj +(k+1)η2 where the β i factors are the cosine similarity estimates of the semantic similarity between the vector representation of the linguistic expression l and the vector related to the ontological object o i and η 2 is a further smoothing factor that enables an empty value (state) as a literal (i.e. non related to any ontological notion) interpretation of l . Notice how given an expression l we retrieve in the wordspace the k closer ontological classes o 1 , ..., o k , in order not to generalize too much: the probability estimates increases for more similar o's and decreases according to the decrements of the cosine measure.</p><p>-Relationship labels are treated similarly to classes. DBpedia relations form a closed dictionary that can be indexed in the word space beforehand. Infact, their lexical labels, such as in the case of spouse for the spouse relation, are indexed through their word vectors. The relationship dictionary thus formed is used to retrieve the relationships involved in a question. First, all the ontological relations in the dictionary that are linked to someone of the entities evoked in the previous step (i.e. the key entities of the question already derived from DBpedia) are collected: this set is the target relationship set, T RS. They involve several relations or properties admissible as they linked with the key entities detected in the question. Moreover, every relationship is triggered in the question from a linguistic expression l (such as married). Its DBpedia counterpart must be found in the T RS set. First, l is mapped to a vector l through its name (such as in the case of die that is represented by the vector of die) or through a more complex pattern 5 . When l and T RS are given, the relationships rel i in the dictionary that are in T RS and close enough to l are retrieved. They are individually mapped to different states corresponding to relations known in DBpedia, and ranked according to their descending semantic relatedness (die vs. deathPlace) in the word space. Notice that complex class labels (e.g. deathDate) can be also processed through a distributional semantic model: while co-occurrence vectors are obtained from the corpus for the individual words that compose the label, they can be linearly combined to represent complex com- 4 A threshold of 0.8 on the cosine similarity is imposed in the settings. 5 Notice that if a DBpedia relation corresponds to a complex linguistic expressions, i.e.</p><p>writtenBy), the distributional analysis can be carried out for the entire pattern (i.e. written by) as it occurs in the corpus. In all cases thus gives rise to a unique vector for the entire pattern.</p><p>binations. Technically, the estimate for the emission probability for a generic rel i is: p(l|o i ) = σi o j ∈T RS σj where l is the linguistic expression for a relation, o j belongs to the target relationship set T RS and σ are cosine estimates of the lexical similarity between l and o j . No smoothing factor is here applied, so that poorly similar relationship in T RS can be retrieved from l and no external relation (or literal relational expression) is admitted for l.</p><p>-When no relationship can be found at the previous stage, the system must deal with complex linguistic relational expressions, denoted by cl. A further attempt is carried out to map these expressions to the relations and properties of individuals extending their linguistic processing. As the target ontological relation in this case is unknown (i.e. the set of DBpedia properties are out of the closed dictionary of relationships delivered with DBpedia<ref type="foot" coords="8,301.12,254.66,3.49,6.05" target="#foot_2">6</ref> , parsing is carried out over such an complex cl, in order to extract its linguistic head, denoted by h cl . The vector h cl corresponding to h cl is selected from the word space used as a representative of the multiword expression. Similarly to the previous step, it is used as a query to retrieve the involved ontological relationships from the target relationship set T RS. All relations or properties closer than a given threshold to h are added to the set of as potential states for the underlying observation (i.e. cl). Also in this case, the probability estimate is:</p><formula xml:id="formula_1" coords="8,193.33,337.65,143.98,17.10">p(cl|o i ) = p(h cl |o i ) = σi o j ∈T RS σj</formula><p>where cl is the complex linguistic expression, o j still belongs to the target relationship set T RS and σ j are the cosine estimates of the lexical similarity between the expression head h cl and o j . Again, no smoothing is here applied.</p><p>At the end of the above phase the entire lattice structure is built, where columns (i.e. states) are fullfilled and emission and transition probabilities are estimated. The vector space treatment of the probabilities is discussed in the next section.</p><p>Probability Estimation in the HMM and distributional lexical similarity As we defined in the previous section, a word space is derived for representing: (1) the lexicon for individual words as they are used in the DBpedia corpus; (2) the ontological classes and entities through the linguistic expressions corresponding to their labels; (3) the ontological relations and properties as they appear in the DBpedia closed dictionary; (4) unknown ontological relations, occurring in a specific target relation set, i.e. originating by the RDF triples regarding one or more key entities retrieved for a question, but missing from the DBpedia relationship catalogues. In all this case, an ontological element o i is made corresponding to a word vector, hereafter denoted by o i : for example spouse correspond to the vector spouse. In this work we use the distributional behavior of the word as an hint on the relationship semantics, and this distinguishes our HMM based approach either from the work by <ref type="bibr" coords="8,270.78,588.53,11.62,8.64" target="#b4">[5]</ref> and <ref type="bibr" coords="8,301.77,588.53,15.27,8.64" target="#b11">[12]</ref>.</p><p>We captured the distributional behavior of words in a word space similarly to <ref type="bibr" coords="8,461.50,600.90,15.27,8.64" target="#b9">[10]</ref>. The similarity function applied for the retrieval and ranking of different concepts o i is based upon a distributional analysis <ref type="bibr" coords="8,283.61,624.81,10.79,8.64" target="#b8">[9]</ref>: each concept o i corresponds to a set of textual contexts in which its corresponding lexical expression l appears (Distributional Hypothesis, <ref type="bibr" coords="9,183.77,120.31,10.45,8.64" target="#b6">[7]</ref>). In our work, contexts are still words (or features) w that appear in a nwindow around a target expression l. Such a space models a generic notion of semantic relatedness: two words close in the space are likely to be either in paradigmatic or syntagmatic relation, as discussed in <ref type="bibr" coords="9,270.63,156.17,15.27,8.64" target="#b9">[10]</ref>. Weights are given to co-occurrences between l and its features w through point-wise mutual information, as discussed in <ref type="bibr" coords="9,428.97,168.13,15.27,8.64" target="#b13">[14]</ref>. Finally, the resulting word-by-features context matrix M is decomposed through Singular Value Decomposition (SVD) <ref type="bibr" coords="9,227.78,192.04,11.62,8.64" target="#b5">[6]</ref> into the product of three new matrices: U , S, and V so that S is diagonal and</p><formula xml:id="formula_2" coords="9,207.18,202.10,214.23,11.22">M = U SV T . M is approximated by M k = U k S k V T</formula><p>k in which only the first k columns of U and V are used, and only the first k greatest singular values are considered. This approximation supplies a way to project a generic expression l (being it or not corresponding to an ontological concept o i ) into the k-dimensional space using W = U k S 1/2 k , where each row corresponds to the representation vectors l. Therefore, given an expression l and an ontological element o i , the similarity function σ is estimated as the cosine similarity between the corresponding projections l, o i , i.e</p><formula xml:id="formula_3" coords="9,268.54,294.10,71.61,23.26">σ(l, o i ) = l • o i l o i</formula><p>The application of the above semantic relatedness measure to the transitions in an example sentence is shown in Fig. <ref type="figure" coords="9,251.59,337.31,3.88,8.64" target="#fig_2">3</ref>: only states that have a not null semantic similarity with an observation are given a numerical score, where scores are mapped in probabilities as in Sect. 3.2.</p><p>In QALD-2103, we applied the above method to the DBpedia corpus, made of all the textual descriptions associated with more than 3.77 million of entities in version 3.8. More specifically, to build the matrix M , POS tagging is first applied to build rows with pairs lemma, ::POS , or lemma :: P OS in brief. The contexts around these items are the columns of M and co-occurrences are computed within windows of size [-5, +5] around the items l. This allows to better capture syntactic properties of individual l. The most frequent 20,000 lemmas (i.e. l) are selected along with their 20k contexts (i.e. features w). The entries of M are the point-wise mutual information between them as computed in the entire corpus. The SVD reduction is then applied to M, with a dimensionality cut of k = 250.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">HMM decoding for semantic disambiguation</head><p>In Fig. <ref type="figure" coords="9,166.03,525.93,4.98,8.64" target="#fig_2">3</ref> the lattice corresponding to the sentence Where did Abraham Lincoln die? is shown. Emission probabilities are shown as nodes scores, while the different set of states for the three individual observations are reported. Transition probabilities are not made explicit. Their treatment is very basic in the current system. First we select transitions that are compatible with the range constraints of the DBpedia data: only transitions whose relations (or properties) are semantically compatible with the links in the DBpedia RDF graph receive a non zero probabilities. The graph is strongly simplified in this way although a fully connected HMM is obtained in most of the cases. Then the probability of the transitions are normalized through a maximum entropy perspective: if n nodes o 1 , ..., o n are reachable from a given state ôt , as they satisfy all semantic constraints, then the transition probability p(o i |ô t ) = 1/n for each i = 1, ..., n. In this way, every consistent transition is equally likely for the system.</p><p>The decoding stage, often called Viterbi <ref type="bibr" coords="10,319.53,120.31,11.62,8.64" target="#b3">[4]</ref> decoding, corresponds to the selection of the most likely state sequence (i.e. RDF path) able to justify the key semantic elements of the question. It provides thus the selection of a suitable RDF path (i.e. a subgraph) from which a unique SPARQL query can be easily derived. Notice how likelihood here implies the satisfaction of most semantic constraints as well as the maximization of the overall semantic relatedness, and well captures the need of fuzzy reasoning over the semantic ambiguity introduced linguistically.</p><p>In view of increasing robustness, also a form of smoothing is applied as in <ref type="bibr" coords="10,442.78,212.38,15.27,8.64" target="#b11">[12]</ref>. One or more special states are artificially added to every obervation related to a key entity. In order to account for possible mistakes in the retrieval process, we introduce a default state corresponding to a literal interpretation of a linguistic symbol: although it does not give rise to any constraint in the target SPARQL query, the default state is used to account for errors or lack of information in the source RDF data. In Figure <ref type="figure" coords="10,441.66,272.16,3.74,8.64" target="#fig_2">3</ref>, we can see two default states related to position one (Where) and three (Abraham Lincoln): for example, it can be seen as a way to model also the interpretation of Abraham Lincoln as a simple string and not an individual. These empty interpretations cumulate a small probability proportional to η as introduced in 3.2: these values have been empirically defined and no specific estimation has been applied onto the available training data in QUALD. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">SPARQL query compilation</head><p>In this module the resulting SPARQL query is generated starting from the RDF path obtained from Viterbi. Here, in order to obtain a consistent SPARQL query, the resulting RDF path is validated wrt the domain and range constraints imposed by the relational object in the RDF graph. For instance,for the HMM trellis in Figure <ref type="figure" coords="11,421.28,176.17,4.98,8.64" target="#fig_2">3</ref> the resulting graph is ?x dbo:deathPlace res:Abraham Lincoln . While the single elements are a correct interpretation for the NL input question, the resulting graph has the argument constraint order reversed for dbo:deathPlace. Here the correct order for the graph is computed, and the SPARQL query is built selecting the right query form. For the QALD evaluation we exploited the input feature answertype to select the right SPARQL form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and discussion</head><p>The RTV system has demonstrated a non-trivial performance, as reported in Table <ref type="table" coords="11,473.11,303.83,3.74,8.64" target="#tab_0">1</ref>. Its third place is mainly due to its poor coverage of the set of sentences with a good balance between precision an recall. Given that, except for the named entity dictionary made available to the parser, the system does not rely on any resource specific dictionary, technique or heuristics this result is very promising. The method is fully portable as language processing and a general parsing technology, i.e. Chaos <ref type="bibr" coords="11,418.56,363.61,10.58,8.64" target="#b0">[1]</ref>, is adopted. Moreover, fully general representations, methods and assumptions are employed. The core idea to model the disambiguation task as a side effect of large scale indexing of the reference ontology combined with a generative model of semantics is convincing and effective. The probabilistic approach, i.e. HMM, has been applied straightforwardly, and the estimates of its parameters have been fully automated. No tuning of the different model and system parameters has been carried out on the provided training set in QUALD, mostly for lack of time. The obtained results were really the very early outcomes also for the Tor Vergata team. The adopted model appears very promising especially for its portability across other domains as well as other languages. On the one side emission probabilities model the plausibility of interpretations (i.e. resources, relations or properties) as semantically related to observed syntactic structures (i.e. proper nouns but also verbs or nominal compounds). On the other side transition probabilities model the semantic relatedness across individual structures retrieved through distributional lexical semantic metrics. Finally, the Viterbi decoding guarantees a straight joint disambiguation process that acts on the global level of the sentence, and robustly accounts for individual interpretations interacting against the underlying syntactic structures.</p><p>A specific error analysis aiming at assessing the contribution of the individual stages in the processing chain has been carried out. In about 70 partially (or mistakenly) processed sentences, we observed that missing relations in the HMM is the major source of error (about 50%). In these cases the system was unable to lexically translate the underlying relation (e.g. admittancedate ): these were made available by the involved resources but it was not possible to properly locate it in the question (basically, unresolved anaphoric references or implicit arguments). An example is the case of a prepositional phrase such as "(... book) by Kerouac (...)" that elliptically refers to the predicate "writing a book". Given the absence of an explicit lexical reference to the predicate "writing", no retrieval of the proper relation writtenBy could be obtained, so that the HMM was inconsistently generated.</p><p>In a lower percentage of errors (below 20%), the mistake is generated by the Viterbi decoding phase, that fails to properly disambiguate the entities (i.e. select the proper state sequence) even if a well formed and consistent trellis was available: the output path produced by Viterbi was including wrong resources, thus preventing to compile the correct SPARQL query. In a small number of cases, the output of Viterbi is not properly connected to any resource (as the smoothing foreseen for a literal interpretation of a symbol in the sentence is selected for too many observations), and no semantic constraint was available to build the SPARQL query. These queries were simply rejected.</p><p>In future work, we will optimize parameters (e.g. the acceptance threshold imposed to the probability attached to the "best" Viterbi path) as these have not been studied in enough detail. Moreover, the current initialization of the HMM, basically dependent on the dependency graph, makes use of no semantics, e.g. no access to the DBpedia resources is done during the HMM initialization stage. We hypothesize here large improvement for methods that directly exploit ontological resources to initialize the HMM: this would allow to improve the treatment of relations as well as to early prune inconsistent interpretations. In the QUALD meeting, while we assume to bring a larger experimental evidence, and a quantitative discussion about the main weaknesses mentioned above, we will report of the system application on RDF data different from the QUALD data, in order to better generalize the system assessment.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,208.92,566.63,195.28,8.12;3,170.05,319.45,275.26,232.44"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The overall architecture of the RTV QA system</figDesc><graphic coords="3,170.05,319.45,275.26,232.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,134.77,311.57,345.82,8.12;5,134.77,322.72,32.62,7.73;5,172.68,116.83,270.00,180.00"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The analysis of the dependency graph for an example sentence: Where did Abraham Lincoln die?</figDesc><graphic coords="5,172.68,116.83,270.00,180.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="10,168.71,624.73,277.93,8.12;10,149.48,391.59,316.40,218.40"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The lattice for the example question: Where did Abraham Lincoln die?</figDesc><graphic coords="10,149.48,391.59,316.40,218.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="11,170.90,490.59,273.55,105.21"><head>Table 1 .</head><label>1</label><figDesc>The QALD-3 results for the RTV system over the DBpedia test set.</figDesc><table coords="11,176.81,521.87,261.74,73.93"><row><cell></cell><cell cols="7">Total Processed Right Partially Recall Precision F-mesure</cell></row><row><cell cols="2">squall2sparql 99</cell><cell>96</cell><cell>77</cell><cell>13</cell><cell>0.85</cell><cell>0.89</cell><cell>0.87</cell></row><row><cell>CASIA</cell><cell>99</cell><cell>52</cell><cell>29</cell><cell>8</cell><cell>0.36</cell><cell>0.35</cell><cell>0.36</cell></row><row><cell cols="2">Scalewelis 99</cell><cell>70</cell><cell>1</cell><cell>38</cell><cell>0.33</cell><cell>0.33</cell><cell>0.33</cell></row><row><cell>RTV</cell><cell>99</cell><cell>55</cell><cell>30</cell><cell>4</cell><cell>0.34</cell><cell>0.32</cell><cell>0.33</cell></row><row><cell>Intui2</cell><cell>99</cell><cell>99</cell><cell>28</cell><cell>4</cell><cell>0.31</cell><cell>0.31</cell><cell>0.31</cell></row><row><cell>SWIP</cell><cell>99</cell><cell>21</cell><cell>14</cell><cell>2</cell><cell>0.15</cell><cell>0.16</cell><cell>0.16</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="4,144.73,636.79,161.40,6.31"><p>http://lucene.apache.org/core/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="4,144.73,647.12,335.86,7.77;4,144.73,658.08,114.55,7.77"><p>Short texts are usually expressed by the vector that is the linear combination of vectors corresponding to the involved words.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2" coords="8,144.73,658.08,334.73,7.77"><p>for the set of DBpedia relations we refer to the file mappingbased properties en.nt</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="12,142.61,613.67,337.98,7.77;12,150.95,624.63,87.16,7.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,250.40,613.67,164.67,7.77">Parsing engineering and empirical robustness</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Basili</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">M</forename><surname>Zanzotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,421.82,613.67,58.77,7.77">Nat. Lang. Eng</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="97" to="120" />
			<date type="published" when="2002-06">Jun 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,636.16,337.98,7.77;12,150.95,647.12,329.64,7.77;12,150.95,658.92,135.00,6.31" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="12,258.63,636.16,221.96,7.77;12,150.95,647.12,85.83,7.77">Learning to interpret natural language navigation instructions from observations</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<ptr target="http://www.cs.utexas.edu/users/ai-lab/?chen:aaai11" />
		<imprint>
			<date type="published" when="2011-08">August 2011</date>
			<biblScope unit="page" from="859" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,120.96,337.98,7.77;13,150.95,131.92,329.64,7.77;13,150.95,142.88,329.64,7.77;13,150.95,153.83,67.38,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,164.61,131.92,229.27,7.77">Swoogle: A search and metadata engine for the semantic web</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Finin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Joel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Reddivari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,425.85,131.92,54.74,7.77;13,150.95,142.88,276.30,7.77">Proceedings of the Thirteenth ACM Conference on Information and Knowledge Management</title>
		<meeting>the Thirteenth ACM Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="652" to="659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,164.79,307.32,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,202.16,164.79,75.23,7.77">The viterbi algorithm</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">D</forename><surname>Forney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,283.34,164.79,63.25,7.77">Proc. of the IEEE</title>
		<meeting>of the IEEE</meeting>
		<imprint>
			<date type="published" when="1973-03">March 1973</date>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="268" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,175.75,337.98,7.77;13,150.95,186.71,329.64,7.77;13,150.95,197.67,329.64,7.77;13,150.95,208.63,329.64,7.77;13,150.95,220.43,166.78,6.31" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,420.34,175.75,60.25,7.77;13,150.95,186.71,243.90,7.77">Querying linked data using semantic relatedness: a vocabulary independent approach</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A G</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>O'riain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Curry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A C P</forename><surname>Da Silva</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2026011.2026017" />
	</analytic>
	<monogr>
		<title level="m" coord="13,413.53,186.71,67.07,7.77;13,150.95,197.67,312.07,7.77;13,178.90,208.63,33.78,7.77">Proceedings of the 16th international conference on Natural language processing and information systems</title>
		<meeting>the 16th international conference on Natural language processing and information systems<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="40" to="51" />
		</imprint>
	</monogr>
	<note>NLDB&apos;11</note>
</biblStruct>

<biblStruct coords="13,142.61,230.55,337.98,7.77;13,150.95,241.51,329.64,7.77;13,150.95,252.46,283.18,7.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,233.83,230.55,222.54,7.77">Calculating the singular values and pseudo-inverse of a matrix</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kahan</surname></persName>
		</author>
		<ptr target="http://www.jstor.org/stable/2949777" />
	</analytic>
	<monogr>
		<title level="j" coord="13,462.34,230.55,18.25,7.77;13,150.95,241.51,329.64,7.77">Journal of the Society for Industrial and Applied Mathematics: Series B, Numerical Analysis</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="205" to="224" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,263.42,337.98,7.77;13,150.95,274.38,144.83,7.77" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,191.01,263.42,82.48,7.77">Distributional structure</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Harris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,395.64,263.42,84.95,7.77;13,150.95,274.38,26.35,7.77">The Philosophy of Linguistics</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Katz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Fodor</surname></persName>
		</editor>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,285.34,337.98,7.77;13,150.95,296.30,205.22,7.77" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,406.87,285.34,73.72,7.77;13,150.95,296.30,86.62,7.77">Scaling up questionanswering to linked data</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sabou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">S</forename><surname>Uren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Motta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>D'aquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,255.46,296.30,27.50,7.77">EKAW</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="193" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,307.26,337.98,7.77;13,150.95,318.22,110.33,7.77" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,229.85,307.26,208.34,7.77">Dependency-based construction of semantic space models</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Pado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,444.72,307.26,35.87,7.77;13,150.95,318.22,62.52,7.77">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.24,329.18,302.76,7.77" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sahlgren</surname></persName>
		</author>
		<title level="m" coord="13,202.26,329.18,83.90,7.77">The Word-Space Model</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>Stockholm University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct coords="13,142.24,340.14,338.36,7.77;13,150.95,351.09,15.69,7.77" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,197.40,340.14,40.47,7.77">Word space</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,418.39,340.14,25.10,7.77">NIPS 5</title>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hanson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Cowan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="895" to="902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.24,362.05,338.35,7.77;13,150.95,373.01,329.64,7.77;13,150.95,383.97,247.37,7.77" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="13,302.29,362.05,178.30,7.77;13,150.95,373.01,59.93,7.77">Keyword-driven resource disambiguation over rdf knowledge bases</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shekarpour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C N</forename><surname>Ngomo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="13,431.93,373.01,48.66,7.77;13,150.95,383.97,96.85,7.77">JIST. Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Takeda</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Qu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Mizoguchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Kitamura</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">7774</biblScope>
			<biblScope unit="page" from="159" to="174" />
			<date type="published" when="2012">2012</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.24,394.93,338.35,7.77;13,150.95,405.89,266.22,7.77" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="13,294.82,394.93,158.65,7.77">Sindice.com: Weaving the open linked data</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Tummarello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Delbru</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Oren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,150.95,405.89,240.07,7.77">Proceedings of the International Semantic Web Conference (ISWC</title>
		<meeting>the International Semantic Web Conference (ISWC</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.24,416.85,338.35,7.77;13,150.95,427.81,317.16,7.77" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,242.54,416.85,234.29,7.77">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">D</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pantel</surname></persName>
		</author>
		<idno type="DOI">10.1613/jair.2934</idno>
	</analytic>
	<monogr>
		<title level="j" coord="13,150.95,427.81,144.42,7.77">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">141</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.24,438.77,338.36,7.77;13,150.95,449.72,256.64,7.77" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="13,444.37,438.77,36.22,7.77;13,150.95,449.72,138.73,7.77">Templatebased question answering over rdf data</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Unger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bühmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C N</forename><surname>Ngomo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cimiano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,307.56,449.72,26.81,7.77">WWW</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.24,460.68,338.35,7.77;13,150.95,469.69,329.63,9.73;13,150.95,482.60,329.64,7.77;13,150.95,493.56,248.20,7.77" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="13,239.73,460.68,240.86,7.77;13,150.95,471.64,153.58,7.77">Pythia: Compositional Meaning Construction for Ontology-Based Question Answering on the Semantic Web</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Unger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cimiano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,174.05,482.60,199.65,7.77">Natural Language Processing and Information Systems</title>
		<title level="s" coord="13,380.66,482.60,99.94,7.77;13,150.95,493.56,26.36,7.77">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Mu Ãoz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Montoyo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>M Ã C Tais</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin / Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">6716</biblScope>
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
