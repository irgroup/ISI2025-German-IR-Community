<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,147.96,115.75,319.48,12.90;1,189.60,133.75,236.16,12.90">Incorporating Statistical Topic Models in the retrieval of healthcare documents</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,205.20,170.85,115.88,9.96"><forename type="first">Karla</forename><forename type="middle">L</forename><surname>Caballero Barajas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California Santa Cruz</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,348.00,170.85,51.30,9.96"><forename type="first">Ram</forename><surname>Akella</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California Santa Cruz</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of California Berkeley</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,147.96,115.75,319.48,12.90;1,189.60,133.75,236.16,12.90">Incorporating Statistical Topic Models in the retrieval of healthcare documents</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DFA1724598BA502EC87A611CF91011F3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a framework based on Statistical Topics Models, Language Models, Information Extraction, and Ontology Analysis to retrieve healthcare related documents for the CLEF eHealth 2013 Task 3. In this framework we add global information based on latent topics from the documents to improve the document retrieval. We perform six different experiments which consist of a baseline and six variants of the model. Preliminary results show that the use of Language Models with a bag of words scheme results better estimates. However model tunning in the Topic Based model is required to achieve optimal results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In these notes we describe the experimental framework presented in the CLEF eHealth 2013 Task 3 which consist of retrieving relevant health care documents. Our main approach consist of providing a solution that does not require any manual input besides the query.</p><p>To achieve this goal, we captured global context from the documents in a unsupervised form using Statistical Topic Models. Our main hypothesis is that semantic content may not be correctly represented with single terms alone. In addition, we present a variant of the model where we extract noun phrases and incorporate them in the query. We claim that using the words independently may change the meaning of the query specially when the user is searching documents from a particular disease that is composed by two or more terms. For instance, cardiac disease might not be correctly detected as a noun phrase and the independent term disease would largely increase the documents retrieved with other types of diseases.</p><p>This paper is divided as follows: Section 2 shows the methodology that we follow and how we obtain the different components of the propose model. In section 3 we describe the steps we follow to pre-process the corpus, and we describe the different experiments that we performed. Finally the results and conclusion are discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>In this section we describe the methodology that we use to perform the retrieval of the documents as well as the different components used to run the experiments, such as the extraction of topics using Statistical Topic Models and the noun phrases extraction. In addition, we detail the incorporation and processing of the discharge summaries in the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Statistical Topic Modeling</head><p>Statistical Topic Models is an unsupervised learning technique that allows us to extract latent topics from a corpus of documents. The fundamental idea is that these topics provide a global context, which can not be achieved using only independent words as in the standard bag-of-words modeling. For the current problem, we use Latent Dirichlet Allocation (LDA) <ref type="bibr" coords="2,359.88,240.21,9.99,9.96" target="#b0">[1]</ref>. In this model, each document is defined as a mixture of topics and each topic is defined as a mixture of words with a given probability. The most dominant topics in the document are those with highest estimated probability. Similarly, the higher the probability the more important the word is in the topic. All these probabilities are estimated during the model fitting process, which is usually performed using Markov Chain Monte Carlo (MCMC) techniques such as collapsed Gibbs sampling. In this model, we choose the prior distribution parameters that optimizes the augmented likelihood that is defined as follows:</p><formula xml:id="formula_0" coords="2,243.48,359.97,237.14,9.96">p(w, z|α, β) = p(w|z, β)p(z|α)<label>(1)</label></formula><p>Where and w is the word, z is the topic label and α and β are the parameters of the prior distribution for the topic mixture and vocabulary mixture respectively. To fit the LDA model with the corpus of healthcare documents we need to select the number of topics. The two parameter vectors that are fitted in the model are the prior distribution vectors for the probability of topics in a document, and the global probability of words given a topic. With these parameters we obtain the topic mixture for each document and save it as metadata that is used in the document retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Noun Phrases Extraction</head><p>Noun phrases provide relevant information about diseases and treatments usually. This is because several diseases are often identified by two or more terms. In order to select accurately the noun phrases, we use the CTAKES <ref type="bibr" coords="2,422.64,535.89,10.56,9.96" target="#b3">[4]</ref> extraction tool that uses medical domain ontology such as SNOMED <ref type="bibr" coords="2,387.36,547.77,9.99,9.96">[5]</ref>. We extract all the nouns from the set of discharge summaries of the MIMIC II <ref type="bibr" coords="2,402.12,559.77,10.56,9.96" target="#b2">[3]</ref> data. By using this dataset, we are able to find a typical set of clinical nouns used in discharge summaries.</p><p>We select the extracted nouns with two or more words since these nouns are not detected by the bag-of-words model. Note that some of the resulting noun phrases are combinations of two of more shorter phrases. We select only those with the smallest number of terms. Then, we analyze if a long noun phrase can be decomposed as a combination of the shortest noun phrases. If this is the case then we ignore this phrase. If this is not the case, we keep the full noun phrase. We remove the phrases which do not include any medical content. To determine if the medical content of the phrases, we compare the extracted nouns with the SNOMED ontologies in order to keep the ones inside the ontology. We obtain 3, 075 noun phrases using 10, 000 discharge summaries from patients with different diseases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Discharge Summaries</head><p>We include the discharge summaries information in three of the six experiments we perform. However, when we reviewed the content of the discharge summaries, some of them have little of not relevance with the query. To overcome this problem. We only take into account those paragraphs with significant relation to the query.</p><p>We determine this relationship in a unsupervised form by comparing the query terms with the content of the paragraph. We found that only a small percentage of the discharge summaries contains information related to the query, which ranges from 1 to 3 paragraphs, compared to the average discharge summary length of 10 to 20 paragraphs. In some cases only a sentence in the entire summary has a relationship with the query. The extracted information from these summaries is combined with the orignal query in order to create a single expanded query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Retrieval Method</head><p>We use Language Models to retrieve the documents. In this framework, we estimate the probability that the query is generated by the document. This probability is represented by the following formula:</p><formula xml:id="formula_1" coords="3,258.84,463.96,221.79,30.35">P (Q|θ D ) = m i p(q i |D)<label>(2)</label></formula><p>where P (q i |D) is the probability that the query term is generated by the document, and this is defined as:</p><formula xml:id="formula_2" coords="3,266.16,539.57,214.47,23.20">P (q i |D) = c(q i , D) |D|<label>(3)</label></formula><p>here c(q i , D) is the frequency of the query word in the document and |D| is the total number of terms in the document. In this method, we might find a zero probability for P (q i |D) when we have a term in the query that is not inside the document. To overcome this problem, we incorporate a smooth factor in the probability P (q i |D) that is defined as follows:</p><formula xml:id="formula_3" coords="3,225.72,644.37,254.91,23.52">P (q i |D) = λ c(q i |D) |D| + (1 -λ)P (q i |C)<label>(4)</label></formula><p>where P (q i |C) is the probability that the query word is in the collection. The value of λ ranges from [0 -1]. For the experiments, we define this value to be 0.5. We include the topic information by using the framework introduced in [7], which consist of adding P lda (q i |D, θ D , φ) to the existing language model. This addition is defined as follows:</p><formula xml:id="formula_4" coords="4,218.88,198.76,261.74,30.23">P lda (q i |D, θ D , φ) = K z=1 p(w|z, φ)p(z|θ, D)<label>(5)</label></formula><p>where K is the number of topics, φ is the posterior probability estimate of the word mixture for each topic, and θ the topic mixture for the document. Then the language model is defined as follows:</p><formula xml:id="formula_5" coords="4,134.76,284.85,345.87,34.56">P (q i |D) = λ( N d µ + N d c(q i |D) |D| + (1 - N d µ + N d )P (q i |C)) + (1 -λ)(P lda (q i |D, θ D , φ)) (6)</formula><p>The value of µ is defined as 1 and N d is the number of documents. The value of λ is defined to be 0.6. The noun phrases are included in the model as any other word. Consequently, the frequency of those phrases in a document is used to represent the documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset Preprocessing</head><p>We use the corpus of medical-related documents provided by the Khresmoi project. Details of the dataset are found in <ref type="bibr" coords="4,337.44,451.05,9.99,9.96">[6]</ref>. We pre-processed the documents in the following manner: First we extract the text from the document by removing the html tags and headers. Then, we remove special and foreign characters by converting all the characters in UTF8 format. With this step we ensure that all the words are in English format. Subsequently we remove all the numbers from the document.</p><p>After the data has been cleaned, we indexed all the documents using Indri index engine from Lemur Project <ref type="bibr" coords="4,285.36,535.05,9.99,9.96" target="#b1">[2]</ref>. Here we perform stemming of the words using the Krovetz stemmer. In addition, common words are removed in the indexing step.</p><p>We have 1628500 documents and a vocabulary size of 98031 after removing documents with less than 20 terms, removing stop words and stemming when we use the bag of words approach. When we use the noun phrases, we replace the noun phrase found in the document with a term code used as a word, and then we index the documents. With this variant we have a vocabulary size of 99538 and 1628500 documents Once we have indexed all the data, we fit the model using 10% of the documents in the corpus and 75 topics. We select those documents randomly. This allow us to model the corpus more accurate. Once we estimate the prior distribution parameters, we extract the topic content for the remaining documents by sampling the labels for each word in the document using Gibbs Sampling and the prior distribution of the word mixture. Once we have estimated the label for each word, we calculate the topic mixture for each document and save it a metadata in the document. Therefore we have two sets of topic mixtures: one only taking into account the single terms and other that takes into account the noun phrases.</p><p>We analyze the queries by parsing them using the constructed index and we remove the stop words. If we use noun phrases, we first replace them with a term code and then we parsed with the index from the document and remove the stop words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Description of the Experiments</head><p>We submit six different runnings to the task: one baseline and five variants of the model. In this section we describe each of the variants of the model.</p><p>We use as baseline the language models described in equation 4 with only the single terms, and without any noun phrases. This method is considered to be the baseline for the other variants of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variants of the model</head><p>We have 5 variants of the model:</p><p>1. Including the information from the Discharge summaries and noun phrases using Probabilistic Topic Modeling and Language Models defined in equation 6. 2. We use noun phrases with the queries that contain the information of the Discharge summaries and using only Language Models 3. Language Models and the information extracted from Discharge Summaries without any noun phrases 4. Language models with noun phrases and topic information using only the queries 5. Language models with noun phrases without any information of the discharge summaries</p><p>The main idea is to test how the different components contribute to the global result. All the variants are fast in the retrieval process. The main constrain is the offline process that they require.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>In this section we will discuss the results of all the variants of the model that we run. Table <ref type="table" coords="5,197.76,631.53,4.98,9.96" target="#tab_0">1</ref> shows the results of the Precision at 10 (P@10), Mean Average Precision (MAP) and the Normalised Discounted Cumulative Gain (NDCG) for the baseline and the five variants of the model As we can observe the baseline results in better results than the variants. This can be explained by the nature of the query. In this case the use of noun phrases (Variant 5) may help in some queries but in others affect the result. We observe that the use of Topics does not improve the performance of the method, which may be cause because the topics may add noise to the query instead of clarifying the query. This phenomena could be caused by the number of topics, which are not sufficient to describe correctly the corpus. Other explanation is that some results retrieved by the method may not be labeled. This may result in a poor performance of the methods.</p><p>Figure <ref type="figure" coords="6,180.72,365.61,4.98,9.96" target="#fig_0">1</ref> shows the precision results for all the queries using the best method (baseline). Here we can observe that the precision of some queries is negative. This means that the baseline have lower performance that the median of the other groups. However some other queries has a comparable performance of the best groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper we presented a method to retrieve relevant health care related document. We believe that our approach have promising results. Further research and model tunning may be required in order to make our approach competitive and suitable to be applied in the healthcare context. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,194.76,367.68,225.82,8.97;7,134.76,115.81,337.90,237.59"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Precision at 10 for each query inside the test set</figDesc><graphic coords="7,134.76,115.81,337.90,237.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,134.76,115.32,345.76,105.09"><head>Table 1 .</head><label>1</label><figDesc>Mean Performance Results of the base model ant the variants of the model for the test set</figDesc><table coords="6,198.24,145.32,187.11,75.09"><row><cell>Model P@10</cell><cell>MAP</cell><cell>NDCG</cell></row><row><cell>Baseline 0.4040</cell><cell>0.2666</cell><cell>0.3637</cell></row><row><cell>Variant 1 0.0600</cell><cell>0.0178</cell><cell>0.0548</cell></row><row><cell>Variant 2 0.1920</cell><cell>0.1590</cell><cell>0.1765</cell></row><row><cell>Variant 3 0.2320</cell><cell>0.1634</cell><cell>0.2062</cell></row><row><cell>Variant 4 0.0580</cell><cell>0.0197</cell><cell>0.0549</cell></row><row><cell>Variant 5 0.3640</cell><cell>0.2270</cell><cell>0.3281</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,138.32,567.48,342.31,8.97;6,146.88,578.52,111.16,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,284.52,567.48,105.11,8.97">Latent dirichlet allocation</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,400.20,567.78,80.43,8.26;6,146.88,578.82,33.31,8.26">Journal of Machine Learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.32,589.80,342.20,8.97;6,146.88,600.72,20.80,8.97" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="6,403.20,589.80,73.46,8.97">The lemur project</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts and Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.32,612.00,342.26,8.97;6,146.88,623.04,333.65,8.97;6,146.88,633.96,149.20,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,305.64,612.00,174.94,8.97;6,146.88,623.04,248.05,8.97">MIMIC II: a massive temporal icu patient database to support research in intelligent patient monitoring</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Lieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">G</forename><surname>Mark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,403.92,623.34,76.61,8.26;6,146.88,634.26,26.28,8.26">Computers in Cardiology</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="641" to="644" />
			<date type="published" when="2002-09">September 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.32,645.24,342.35,8.97;6,146.88,656.16,333.82,8.97" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="6,408.24,656.16,72.46,8.97">Mayo clinical text</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Guergana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><forename type="middle">J</forename><surname>Savova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Philip</forename><forename type="middle">V</forename><surname>Masanz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiaping</forename><surname>Ogren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sunghwan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karin</forename><forename type="middle">C</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><forename type="middle">G</forename><surname>Kipper-Schuler</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Chute</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
