<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,151.59,115.96,312.17,12.62;1,202.16,133.89,211.04,12.62">Lucene, MetaMap, and Language Modeling: OHSU at CLEF eHealth 2013</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,211.91,171.64,65.20,8.74"><forename type="first">Steven</forename><surname>Bedrick</surname></persName>
							<email>bedricks@ohsu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Spoken Language Understanding</orgName>
								<orgName type="institution">Oregon Health &amp; Science University</orgName>
								<address>
									<settlement>Portland</settlement>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,299.79,171.64,103.65,8.74"><forename type="first">Golnar</forename><surname>Sheikshabbafghi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Spoken Language Understanding</orgName>
								<orgName type="institution">Oregon Health &amp; Science University</orgName>
								<address>
									<settlement>Portland</settlement>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,151.59,115.96,312.17,12.62;1,202.16,133.89,211.04,12.62">Lucene, MetaMap, and Language Modeling: OHSU at CLEF eHealth 2013</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AB34E3E2E36BD3135F5C8615B03EB760</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Lucene</term>
					<term>MetaMap</term>
					<term>language model</term>
					<term>skip-grams</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Oregon Health &amp; Science University team's participation in task #3 ("addressing patients' medical questions") of this year's eHealth CLEF campaign included submissions from two different retrieval systems. The first was a traditional, Lucene-based system modified from one used in previous years' TREC-med campaigns; the second was a novel system that used statistical language modeling techniques to perform text retrieval. Since 2013 was the first year of our participation in this campaign, our focus was on familiarizing ourselves with working on a corpus of web text, as well as putting together a proof-of-concept implementation of a language-model retrieval system. We submitted three runs in total; one from the novel system, and two from our Lucene-based system, one of which made use of the National Library of Medicine's MetaMap tool to perform query expansion. In general, our runs did not perform particularly well, although there were several topics for which our language model-based retrieval system produced the best P@10. Future work will focus on pre-indexing text normalization as well as a more sophisticated approach to query parsing.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Most research into medical information retrieval can be categorized into one of two broad and fuzzy groupings. The first ("type A") concerns itself with the information needs of clinicians, and focuses on searching specialized databases in response to specific and well-informed topics. The second category ("type B") deals with so-called "consumer" information needs: searches conducted by non-medical users (often patients or family members) over the Internet. Earlier medical IR evaluation campaigns have tended to focus on the first of these two groupings.</p><p>For example, for several years TREC included a medical track in which participants built and tested search systems designed to index and query electronic health records in order to identify patients matching particular textual descriptions (e.g., "Patients admitted to the hospital with end-stage chronic disease who are offered hospice care") <ref type="bibr" coords="1,263.44,644.16,11.96,8.74" target="#b0">[1]</ref>. Without in any way diminishing the substantial difficulties inherent in performing high-quality open-class IR over a clinical database, we believe that it is safe to say that this family of medical IR system enjoys several significant advantages over the aforementioned second family of medical IR system (systems designed for the lay population, and intended to work on a general web corpus).</p><p>A system designed for indexing and querying a clinical data repository of some kind has the advantage of a (relatively speaking) predictable data schema, as well as (again, relatively) a finite amount of content-both in terms of amount (i.e., how many unique records) as well as kind. After all, while there may be a very large number of types of pathology report, that number is generally both knowable and tractable. A search system designed for consumers and the open Web, however, must be able to handle an essentially infinite variety of input documents and user queries, and must do so with far less context (about both its corpus as well as its users<ref type="foot" coords="2,260.57,262.49,3.97,6.12" target="#foot_0">1</ref> ) than do its more constrained cousins.</p><p>The third task of this year's ShARe/CLEF eHealth track features just such a search scenario. The task is described in full elsewhere <ref type="bibr" coords="2,372.63,289.58,12.54,8.74" target="#b2">[3]</ref>; as such, we will keep our description brief. The task was an open-query document search over a corpus consisting of ≈1.6 million web pages ostensibly containing health-related information. The topics consisted of quasi-natural-language phrases (e.g. "is there a connection between multiple sclerosis and dysplasia in oesophagus") that represented the sorts of queries that actual patients might enter into a search engine such as Google.</p><p>Our group submitted runs derived from two different retrieval systems (described in section 2). While our results were-with one or two exceptions-not particularly impressive (see Section 3), we feel that we have laid a solid technical foundation for next year's CLEF campaign. Furthermore, our experience highlight several important differences between "type A" and "type B" medical retrieval systems in terms of how best to use external resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>We submitted runs from two separate systems. The first system was s very traditional IR system based on the Apache Lucene<ref type="foot" coords="2,356.48,516.90,3.97,6.12" target="#foot_1">2</ref> open-source toolkit. It was essentially a spiritual successor to the system used by the OHSU team for the 2011 and 2012 TREC medical tracks <ref type="bibr" coords="2,292.74,542.39,11.81,8.74" target="#b3">[4]</ref>. The second system was a novel system that uses techniques from statistical language modeling to perform retrieval. <ref type="foot" coords="2,476.12,552.77,3.97,6.12" target="#foot_2">3</ref>The two systems were quite different in terms of their operation, and we will describe each in turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Traditional System</head><p>The first of our two systems followed a very traditional architecture for a text retrieval system, in that it featured a standard inverted index paired with vectorspace retrieval model. As mentioned above, we used the Lucene open-source IR toolkit (version 4.2.1) to build the system, with no major modifications from its out-of-the-box configuration. Our intent was to develop a system to use as a reasonable baseline, and Lucene let us accomplish this with minimal difficulty. Lucene also provided us with robust index creation tools as well as a rich query language. This baseline Lucene system used the default Lucene StandardAnalyzer query processor. Our system had an alternative query processing mode, which we used for one of our additional runs. This mode makes use of our existing MetaMap-based query parser used in previous years' TREC campaigns (see <ref type="bibr" coords="3,463.43,271.30,10.30,8.74" target="#b3">[4]</ref>), described in greater detail below.</p><p>Indexing Due to the large size of the corpus, building the Lucene index of the documents proved to be a non-trivial task. We chose to use a 500-node Hadoop<ref type="foot" coords="3,476.12,324.26,3.97,6.12" target="#foot_3">4</ref> cluster to facilitate the process. Hadoop is an open-source implementation of the Map/Reduce pattern, which was first popularized by Google as a way of easily parallelizing certain computing tasks. A complete description of Map/Reduce is beyond the scope of this document (see <ref type="bibr" coords="3,331.18,373.65,96.78,8.74">Dean &amp; Ghemawat[6]</ref> for a more thorough description); in short, a Map/Reduce program splits a task into two steps, "map" and "reduce." In the map step, each input document is processed in parallel and is transformed into a set of key/value pairs {&lt; k i , v i &gt;, &lt; k j , v j &gt; , ...} . In the reduce step, the key/value pairs are aggregated by their key into sets of the form {&lt; k i , {v i1 , v i2 , ...} &gt;, &lt; k j , {v j1 , v j2 , ...} &gt;}, and each key's associated values are processed together. The ultimate point of this approach is that each execution of the map step can be run completely independently from any others, and a similar amount of parallelization can often be achieved in the reduce phase.</p><p>The canonical example is a distributed word-counting operation. In this case, the mapper would take as input a document, and emit for each token in the document a key/value pair in which the key is the token and the value is the number of times that that token appeared in that document. The reducer, then, would take as its input a single token along with a set of counts (one from each input document that contained that token); its job would be to sum the counts and emit the total number of occurrences of that token in the corpus. Because each document can be counted independently of the others, and each token's occurrence counts can be summed independently of any other tokens' counts, a word-counting program using this approach can benefit greatly from a parallel computing environment.</p><p>The Map/Reduce model lends itself extremely well to the creation of inverted indices. Consider the most trivial case, in which the map step emits terms and postings as keys and values, and the reduce step produces posting lists. For our purposes, we used Map/Reduce to produce a Lucene index. Each mapper processed a subset of the entire corpus, and produced a single index shard; the shards were then run through the Lucene API's index-merging tools to produce a single, large index. The final index size was approximately 7.5 gigabytes, which proved to be well within Lucene's capabilities. We indexed both the document titles as well as their bodies, after stripping the bodies of their HTML tags.</p><p>MetaMap As mentioned above, our baseline system uses Lucene's default StandardAnalyzer to process free-text queries. During previous years' TREC campaigns, we developed a query parser that uses the National Library of Medicine's (NLM) MetaMap tool <ref type="bibr" coords="4,283.44,253.54,11.74,8.74" target="#b6">[7]</ref> tool to attempt to identify query terms that are "medically-relevant." MetaMap uses a variety of NLP techniques to map unstructured text to concepts from the Unified Medical Language System (UMLS) Metathesaurus <ref type="bibr" coords="4,233.35,289.40,13.65,8.74" target="#b7">[8]</ref>.</p><p>Our query parser has a variety of operational modes, including several that perform query expansion by including sibling entry terms for any concepts matched from the NLM's Medical Subject Headings (MeSH) indexing system. For a complete description of the operation of this part of our system, consult our 2012 TREC Medical Track working notes paper <ref type="bibr" coords="4,344.10,349.18,12.65,8.74" target="#b3">[4]</ref>. In short, our query parser takes unstructured text as input, and in an unsupervised manner produces a (sometimes complex, and often suboptimal) query in Lucene's syntax, making use of various Boolean operators as appropriate.</p><p>In the simplest operation mode, our query parser uses MetaMap to analyze the free-text queries and identify any biomedical concepts. From there, we use the UMLS to identify possible synonyms for these concepts, and link those together using Boolean "OR." Each term group is then linked using Boolean "AND." The parser contains several "stop-word" lists-sets of CUIs or UMLS Semantic Types that it ignore, and not include in its final queries. Typically, CUIs or Semantic Types end up on this list due to being overly common in either queries or in documents (e.g., "Patients").</p><p>this approach worked reasonably well when used in previous years' campaigns, which involved querying electronic medical records for specific patient profiles. We were unsure as to how well it would work when querying the less formally-written and similarly less-focused content found in the present task, but decided that it was worth trying.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Language Model System</head><p>In addition to our traditional baseline system, we submitted a run from an entirely novel retrieval system. This system used techniques borrowed from statistical language modeling to attempt to identify documents that were statistically similar to the queries. Conceptually, we are using the language model to tell us, for each document, the probability of that document generating the query. More "relevant" documents should, in principle, have a higher probability than less "relevant" documents.</p><p>Slightly more formally, and using notation from [9], our approach computes a language model M d for each document in the corpus d. Then, for any given query q, we use the language models to compute P (q|M d ) for each document d, and can rank the documents in descending order of this probability.</p><p>This approach presents us with some interesting experimental possibilities. There are a wide variety of different text normalization approaches, model smoothing techniques, scoring functions, etc. that we wish to experiment with (see Section 4).</p><p>Text Normalization In any language modeling task, the decision of how to normalize one's input is critical. We were somewhat aggressive in our normalization approach. We first dropped all non-ASCII letters, since the task a priori only involved English-language queries over English-language documents. After alphabetical pruning, we compiled a list of more than 3 million tokens, most of which were not actual words but were instead fragments, numbers, etc. etc.</p><p>We then computed document frequencies for each token in order to compile a list of "stop tokens." We were primarily concerned with removing overly common tokens. By manually examining the relative frequencies of tokens, we set a document frequency threshold of 700,000-that is, we considered tokens appearing in more than that many documents to be "too common," and excluded them from both the documents as well as the queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language Model</head><p>The language modeling approach we used was very similar to a standard bigram language model with an absolute discounting backoff scheme. <ref type="foot" coords="5,168.31,416.57,3.97,6.12" target="#foot_4">5</ref> However, instead of using strict bigrams, we instead counted tokens that co-occurred within an 11-word window (i.e., word pairs that had a maximum distance of ten words). In other words, if two tokens occurred relatively near to each other, they were counted as a bigram. Furthermore, the distance calculation took place after pruning stop words, so the two tokens could potentially have been more than ten tokens apart in the original document.</p><p>In the language modeling world, these are referred to as skip-grams, and are often used to allow a language model to capture additional information about the contents of a text. <ref type="foot" coords="5,211.58,512.23,3.97,6.12" target="#foot_5">6</ref> In our case, we chose to use skip-grams in part to compensate for the large difference in length between the documents and the queries, which can cause problems relating to model sparsity. By allowing this "slop" we are effectively increasing the number of possible bigrams from each query, thereby increasing the number of chances for that query's bigrams to occur in the perdocument language models.</p><p>Scoring &amp; Ranking Documents Given a normalized query and a normalized set of documents, we compute the log-probability for all pseudo-bigram pairs Run P@10 MAP nDCG@10 Traditional (Baseline) 0.2300 0.0953 0.2436 Lang. Model 0.2600 0.0999 0.2344 Baseline w/ MetaMap 0.1620 0.0816 0.1706 Table <ref type="table" coords="6,162.74,161.64,4.13,7.89">1</ref>. Official results, including precision at rank 10 (P@10), mean average precision (MAP), and normalized discounted cumulative gain at rank 10 (nDCG@10).</p><p>present in the query occurring in each document, just as one would using a standard bigram language model when estimating the log-probability of a sentence. However, in our case, we are not strictly calculating the log-probability of the query vs. each document. Because of our adjusted bigram scheme, each bigram may potentially be counted multiple times depending on how the documents' tokens are arranged; as such, the actual probabilities we calculate are somewhat smaller than they would be in a traditional bigram scheme. However, as we are calculating it in the same way across documents and queries, the probabilities themselves are directly comparable with one another.</p><p>As such, in order to rank the documents in order of "relevance," we need simply to rank the documents in descending order of probability (i.e., the documents with the highest probabilities for a given query are ranked highest in the result list). There are any number of different ways to choose a threshold at which to cut off results; we chose a fairly robust method in which we used the histogram of calculated probabilities for a given query to choose a reasonable threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>Unfortunately, our runs did not perform particularly well this year. Generally speaking, all three of our runs performed below the median in terms of P@10, and none achieved an overall mean average precision score higher than 0.1. However, there were several topics for which we achieved adequate performance, and there were even several topics for which one or the other of our runs appeared to have the best P@10. Our official results can be seen in Table <ref type="table" coords="6,380.10,544.26,3.87,8.74">1</ref>.</p><p>As is often the case in these kinds of evaluation campaigns, we observed a large amount of variation in system performance across topics (see Figure <ref type="figure" coords="6,468.97,570.68,3.87,8.74">1</ref>). We also noted a similarly large amount of variation in the number of documents judged relevant for each topic, with one run having zero relevant documents, seven more having fewer than ten, and one topic apparently having 610 relevant documents. <ref type="foot" coords="6,184.08,616.92,3.97,6.12" target="#foot_6">7</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Traditional System</head><p>Our baseline system-essentially plain-vanilla Lucene, without external annotations of any sort-did not perform particularly well. In terms of bpref, there were only three topics for which it achieved a score higher than 0.5, and there was a great deal of variability among the remaining topics (see Figure <ref type="figure" coords="7,455.52,175.08,3.87,8.74">1</ref>). In terms of P@10, its performance was similarly dismal. There were 14 topics for which its P@10 was 0.0; one topic (qtest29, "what is prognosis and the treatment for aortic stenosis") came in at 0.8, with a small handful coming in above 0.5. Most of the topics had P@10 scores of between 0.1 and 0.4.</p><p>In addition to our baseline system, we also submitted a run from a system that used MetaMap to perform query expansion (see Section 2.1). In previous evaluation campaigns, this approach had often resulted in improved performance. In this task, the picture was somewhat more complex. Often, rather than retrieving more potentially-useful results, the query expansion (which made rather aggressive use of Boolean query operators) would result in fewer results being retrieved (often many fewer, down to and including zero).</p><p>Sometimes, as in the case of qtest27, this would result in improved precision (in this case, from a P@10 of 0.0 in the baseline run to 0.6 in the MetaMap run). More often, however, this would result in a decrease in precision, simply due to the query's Boolean criteria preventing more than a handful of articles from being retrieved.</p><p>Overall, there were 17 topics for which the MetaMap query expansion hurt the system's performance in terms of bpref, and 18 for which it had no effect. That leaves 15 for which the query expansion improved bpref (see Table <ref type="table" coords="7,468.97,402.41,3.87,8.74" target="#tab_1">2</ref>). In three of these cases (qtest7, qtest27, and qtest46), the MetaMap run's non-zero bpref represented an improvement over a baseline bpref of 0.0. The remaining cases of improvement fell into two categories. The first consisted of several runs that featured relatively modest improvements (Metamap:Baseline bpref ratios ranging from ≈ 1.06-≈ 1.70). The second consisted of a small number of runs that experienced a much more dramatic improvement, with bpref ratios ranging from ≈ 4.40 to ≈ 5.15.</p><p>Looking at the final processed form of the queries that experienced the most performance improvement, the thing that stands out is that they are simpler and more parsimonious than they were before being processed by MetaMap. This was not a universal result of being run through our MetaMap processor; quite often, the final queries would be quite large and complex, with multiple Boolean clauses. In these cases of notable improvement, it seems as though MetaMap acted as something of a filter, stripping out un-necessary and noisy terms and replacing them with a small number of salient terms. In our future work, we plan to investigate this phenomenon further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Language Model System</head><p>Strictly speaking, our language model system (LMS) did not perform particularly well. There were several topics for which we did not retrieve very many results, 0.0 0.1 0.2 0.3 0.4 0.5 0.6 bpref by topic, Traditional System Fig. <ref type="figure" coords="8,154.40,327.66,4.13,7.89">1</ref>. bpref score by topic, "Traditional" (baseline) run. Note the extreme betweentopic variability in system performance; note also the large number of topics for which the scored bpref was 0.0. period-much less many relevant results. Overall, our LMS performed even less consistently than did the baseline system (see Figure <ref type="figure" coords="8,371.12,396.49,3.87,8.74">2</ref>). That said, there were several runs for which the LMS outperformed the baseline system by a wide margin.</p><p>For example, for qtest49, the LMS:Baseline bpref ratio was 3.69, representing a very notable increase in performance. All in all, there were 19 topics for which the LMS outperformed our baseline system in terms of bpref, and 7 for which there was no difference in performance. Five of the topics with improved performance saw at least a doubling of bpref as compared to the baseline, and one more came very close to double (bpref ratio of ≈ 1.97). <ref type="foot" coords="8,394.80,491.05,3.97,6.12" target="#foot_7">8</ref> In terms of P@10, there were six topics for which our LMS run appears to have achieved the best score. In one of these topics (qtest12), we appear to have outperformed the median by a fairly significant margin; the other five topics (qtest20 qtest31, qtest42, qtest43, and qtest47) appear to have been more challenging, as the degree to which we outperformed the median was smaller.</p><p>One unexpected attribute of the LMS was that diagnosing performance issues was quite difficult. With our baseline system, we can easily examine our generated queries, and can also directly inspect our index. With the LMS, however, things are much more opaque. As such, it is not easy for us to say why it did particularly well or badly on any given topic. Future work will explore ways to improve this state of affairs.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions &amp; Future Work</head><p>Our results in this year's evaluation campaign were somewhat disappointing. However, we have now established two different solid baseline systems, and have also demonstrated that our existing query expansion system is not completely useless in a more general medically-themed retrieval task. Clearly, however, there is much for us to do in preparation for next year's evaluation campaign. Our future work will consist of work on several fronts. The first and most important next step will be to improve our indexing strategy. One issue that our baseline system ran into during development was that many documents in the corpus contained extraneous text 9 that led to numerous false positives. We will be working on ways to detect content zones within the page such that we only index the truly relevant text from each document. We also plan on experimenting with various other approaches, such as indexing anchor or heading text separately from the rest of the document body, and boosting their value in the search index.</p><p>A second area of work will be in improving our MetaMap-based query parser and expansion system. Right now, it is only using a subset of the vocabularies in the UMLS, which we believe limited its ability to successfully map concepts from the queries into UMLS CUIs. Furthermore, in its present form, the parser is relatively inflexible and makes overly-aggressive use of Boolean operators. This led to over-constrained queries as well as queries containing duplicate terms. We hope to improve this state of affairs.</p><p>A third area will be to explore ways to make use of syntactic data derived from the queries themselves to better understand what the queries are actually asking about. This could include constituent or dependency parsing, part-of-9 Sometimes in the form of page navigation elements, other times from intentional keyword frequency manipulation on the part of the page's authors-e.g., a hidden div filled with a smörgåsbord of medical terms unrelated to the ostensible topic of the page. bpref by topic, Language Model Fig. <ref type="figure" coords="10,154.40,329.82,4.13,7.89">2</ref>. bpref score by topic, "Language Model" run. As compared with the baseline system (see Figure <ref type="figure" coords="10,230.79,340.81,3.58,7.86">1</ref>), the language model system was less consistent but higherperforming.</p><p>speech tagging, and so on. We will also experiment with performing this sort of analysis on the documents as well.</p><p>Our language model retrieval system has many possible avenues for improvement. Besides making use of the aforementioned syntactic data, we plan to experiment with different skip-ngram window widths as well as some of the algorithm's other tuning parameters. This sort of optimization work was difficult to do during the development phase of this campaign due to our lack of supervised training data; our hope is that, with access to the qrels from this year's campaign, we will be able to improve our approach.</p><p>We are excited to have participated in this year's evaluation campaign, and eagerly await next year's. Although we were disappointed by our systems' performance, we see a lot of potential in our fundamental approach, and look forward to the opportunity to develop our ideas further.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,134.77,244.83,345.82,18.85"><head>Table 2 .</head><label>2</label><figDesc>bpref scores for topics that benefited from the MetaMap query expansion described in section 2.1.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.73,602.00,335.86,7.86;2,144.73,612.96,335.87,7.86;2,144.73,623.92,89.93,7.86"><p>While the information literacy of clinicians certainly varies<ref type="bibr" coords="2,381.53,602.00,10.54,7.86" target="#b1">[2]</ref>, it is safe to say that this variability is smaller than that found among the users of a general-purpose medical search engine.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,144.73,634.88,99.36,7.86"><p>http://lucene.apache.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,144.73,645.84,335.86,7.86;2,144.73,656.80,144.96,7.86"><p>See Chapter 12 of Manning, et al.'s "Introduction to Information Retrieval" for an overview of the general approach<ref type="bibr" coords="2,273.39,656.80,12.23,7.86" target="#b4">[5]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="3,144.73,656.80,103.70,7.86"><p>http://hadoop.apache.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="5,144.73,634.88,215.50,7.86"><p>We used an absolute discount parameter value of 0.5.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="5,144.73,645.84,335.87,7.86;5,144.73,656.80,122.71,7.86"><p>For more details, two good places to start would be the work of Siu &amp; Ostendorf<ref type="bibr" coords="5,463.95,645.84,16.64,7.86" target="#b9">[10]</ref> and that of Guthrie, et al.<ref type="bibr" coords="5,250.80,656.80,13.31,7.86" target="#b10">[11]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="6,144.73,645.84,335.87,8.12;6,144.73,656.80,62.48,7.86"><p>This would be topic qtest19, "is abdominal pain due to helicobacter pylori a symptom of cancer."</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="8,144.73,645.84,335.86,7.86;8,144.73,656.80,194.21,8.12"><p>On the other hand, there were 25 topics for which the LMS exhibited decreased bpref performance as compared to the baseline.</p></note>
		</body>
		<back>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>qtest2 qtest3 qtest4 qtest5 qtest6 qtest7 qtest8 qtest9 qtest10 qtest11 qtest12 qtest13 qtest14 qtest15 qtest16 qtest17 qtest18 qtest19 qtest20 qtest21 qtest22 qtest23 qtest24 qtest25 qtest26 qtest27 qtest28 qtest29 qtest30 qtest31 qtest32 qtest33 qtest34 qtest35 qtest36 qtest37 qtest38 qtest39 qtest40 qtest41 qtest42 qtest43 qtest44 qtest45 qtest46 qtest47 qtest48 qtest49 qtest50</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="10,142.96,580.66,337.63,7.86;10,151.52,591.62,303.55,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,253.95,580.66,189.62,7.86">Overview of the trec 2012 medical records track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,464.71,580.66,15.88,7.86;10,151.52,591.62,270.78,7.86">The Twenty-First Text REtrieval Conference Proceedings (TREC 2012)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,602.29,337.64,7.86;10,151.52,613.25,329.07,7.86;10,151.52,624.21,329.07,7.86;10,151.52,635.14,102.68,7.89" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,305.72,613.25,174.87,7.86;10,151.52,624.21,248.03,7.86">Factors associated with success in searching MEDLINE and applying evidence to answer clinical questions</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">R</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">K</forename><surname>Crabtree</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">H</forename><surname>Hickam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Sacherek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">P</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Tidmarsh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Mosbaek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kraemer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,407.48,624.21,73.11,7.86;10,151.52,635.17,23.14,7.86">J Am Med Inform Assoc</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="283" to="293" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,645.84,337.64,7.86;10,151.52,656.80,329.07,7.86;11,151.52,119.67,329.07,7.86;11,151.52,130.63,245.64,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,151.52,119.67,230.80,7.86">Overview of the ShARe/CLEF eHealth Evaluation Lab</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Salanterä</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Velupillai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">W</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Savova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Elhadad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mowery</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Leveling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,430.61,119.67,45.78,7.86">CLEF 2013</title>
		<title level="s" coord="11,151.52,130.63,170.30,7.86">Lecture Notes in Computer Science (LNCS</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,141.59,337.63,7.86;11,151.52,152.55,329.07,7.86;11,151.52,163.51,329.07,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,354.39,141.59,126.20,7.86;11,151.52,152.55,323.34,7.86">Identifying patients for clinical studies from electronic health records: TREC 2012 medical records track at OHSU</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Edinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,164.77,163.51,279.79,7.86">The Twenty-First Text REtrieval Conference Proceedings (TREC 2012</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,174.47,337.63,7.86;11,151.52,185.43,230.75,7.86" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="11,327.07,174.47,149.64,7.86">Introduction to Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,196.39,337.63,7.86;11,151.52,207.34,329.07,7.86;11,151.52,218.30,113.36,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,254.72,196.39,222.21,7.86">Mapreduce: simplified data processing on large clusters</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,165.40,207.34,310.79,7.86">OSDI&apos;04: Sixth Symposium on Operating System Design and Implementation</title>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,229.26,337.64,7.86;11,151.52,240.20,254.11,7.89" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,269.08,229.26,211.51,7.86;11,151.52,240.22,61.37,7.86">An overview of metamap: historical perspective and recent advances</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Aronson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">M</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,221.22,240.22,99.57,7.86">J Am Med Inform Assoc</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="229" to="236" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,251.18,337.63,7.86;11,151.52,262.11,307.32,7.89" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,221.08,251.18,259.50,7.86;11,151.52,262.14,62.44,7.86">The unified medical language system (umls): integrating biomedical terminology</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bodenreider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,222.18,262.14,72.42,7.86">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="267" to="270" />
			<date type="published" when="2004-01">Jan 2004</date>
		</imprint>
	</monogr>
	<note>Database issue</note>
</biblStruct>

<biblStruct coords="11,142.96,273.10,337.63,7.86;11,151.52,284.06,220.29,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,329.85,273.10,147.18,7.86">Introduction to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,164.32,284.06,207.49,7.86">chapter Language Models for Information Retrieval</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,295.02,337.98,7.86;11,151.52,305.95,329.07,7.89;11,151.52,316.93,51.70,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,248.05,295.02,232.55,7.86;11,151.52,305.98,74.49,7.86">Variable n-grams and extensions for conversational speech language modeling</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Siu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ostendorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,238.26,305.98,120.83,7.86">Speech and Audio Processing</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="63" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,327.89,337.97,7.86;11,151.52,338.85,329.07,7.86;11,151.52,349.81,209.39,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,391.96,327.89,88.63,7.86;11,151.52,338.85,60.98,7.86">A closer look at skipgram modelling</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Guthrie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Allison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Guthrie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wilks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,234.45,338.85,246.14,7.86;11,151.52,349.81,159.10,7.86">Proceedings of the 5th international Conference on Language Resources and Evaluation (LREC-2006)</title>
		<meeting>the 5th international Conference on Language Resources and Evaluation (LREC-2006)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
