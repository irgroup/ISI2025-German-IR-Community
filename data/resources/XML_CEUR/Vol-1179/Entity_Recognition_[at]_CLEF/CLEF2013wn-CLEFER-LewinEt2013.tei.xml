<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,138.35,116.95,338.67,12.62;1,232.14,134.89,151.07,12.62">Deriving an English Biomedical Silver Standard Corpus for CLEF-ER</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,232.71,172.56,43.58,8.74"><forename type="first">Ian</forename><surname>Lewin</surname></persName>
							<email>ian.lewin@linguamatics.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Linguamatics Ltd</orgName>
								<address>
									<addrLine>324 Science Park, Milton Road</addrLine>
									<postCode>CB4 0WG</postCode>
									<settlement>Cambridge</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,303.46,172.56,74.73,8.74"><forename type="first">Simon</forename><surname>Clematide</surname></persName>
							<email>simon.clematide@uzh.ch</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Zurich</orgName>
								<address>
									<addrLine>Binzmühlestr. 14</addrLine>
									<postCode>8050</postCode>
									<settlement>Zürich</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,138.35,116.95,338.67,12.62;1,232.14,134.89,151.07,12.62">Deriving an English Biomedical Silver Standard Corpus for CLEF-ER</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">456CB4F6AC731BEE8C45DBF5A9B6D424</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>annotation</term>
					<term>silver standard</term>
					<term>challenge preparation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe the automatic harmonization method used for building the English Silver Standard annotation supplied as a data source for the multilingual CLEF-ER named entity recognition challenge. The use of an automatic Silver Standard is designed to remove the need for a costly and time-consuming expert annotation. The final voting threshold of 3 for the harmonization of 6 different annotations from the project partners kept 45% of all available concept centroids. On average, 19% (SD 14%) of the original annotations are removed. 97.8% of the partner annotations that go into the Silver Standard Corpus have exactly the same boundaries as their harmonized representations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>CLEF-ER <ref type="foot" coords="1,178.91,436.43,3.97,6.12" target="#foot_0">3</ref> , a shared task on multilingual annotation of biomedical named entities is part of the EU ICT project "Multilingual Annotation of Named Entities and Terminology Resources Acquisition" (MANTRA) <ref type="foot" coords="1,375.94,460.34,3.97,6.12" target="#foot_1">4</ref> and is hosted by the "Conference and Labs of the Evaluation Forum"<ref type="foot" coords="1,346.44,472.29,3.97,6.12" target="#foot_2">5</ref> (CLEF).</p><p>The CLEF-ER challenge asks participants to find a wide range of biomedical named entities in non-English documents. As a possible (though not absolutely necessary) aid, a set of parallel English language documents is supplied which already contains entity markup. The entities are selected from the Unified Medical Language System (UMLS) <ref type="bibr" coords="1,266.74,533.64,9.96,8.74" target="#b0">[1]</ref>. In general, there are more synonyms in UMLS for English than for other languages, and so we expect the English markup to be helpful in finding non-English synonyms not already known to UMLS.</p><p>In principle, the challenge can be tackled in a variety of ways. For example, one might search the non-English document for the phrase which best translates the entity markup in the corresponding English document. Alternatively, one might perform entity recognition in the non-English document and then attempt to correlate entities across the pair of multi-lingual documents.</p><p>The English markup supplied for the CLEF-ER challenge was generated automatically using a Silver Standard methodology in order to harmonize six different annotations from the project partners. This paper explains the process used to generate the Silver Standard and the issues raised during its construction. In the following sections, we first outline the task requirements for the CLEF-ER English silver standard annotation. Next, we discuss how the Silver Standard methodology from the predecessor project "Collaborative Annotation of a Large Scale Biomedical Corpus" (CALBC <ref type="bibr" coords="2,289.94,203.68,10.79,8.74" target="#b1">[2]</ref>) was adapted to the new scenario. Finally we present some results and our conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">CLEF-ER requirements</head><p>The CLEF-ER task scenario imposes a number of requirements which makes the collection of manual and/or Gold Standard annotations (i.e. verified by a reliable procedure incorporating expert opinion) particularly onerous.</p><p>1. The concepts to be annotated are (a) very large in number (b) highly specialist (c) highly diverse in nature, even though they all fit under the broad heading of 'biomedical'. It is unlikely any one individual will be an expert in all of the included concepts. 2. The document set to be marked up is (a) very large in size (b) specialist (c) highly diverse, ranging from extracts of scientific papers, to drug labels to claims in patent documents. 3. The task requires assignment of concept identifiers (sometimes called "normalization" or "grounding") and not just the identification of names in text, perhaps with an indication of their semantic type.</p><p>The concepts are taken from UMLS and are (all) the members of the following specified semantic groups: anatomy, chemicals, drugs, devices, disorders, geographic areas, living beings, objects, phenomena and physiology. The sources of the concepts are MeSH (Medical Subject Headings), MedDRA (Medical Dictionary for Regulatory Activities) and SNOMED-CT (Systemized Nomenclature of Human and Veterinary Medicine). There are 531,466 concepts in total. The concept identifiers to be assigned are UMLS Concept Unique Identifiers (or CUIs).</p><p>The document set includes nearly 1.6m sentences (15.7m words) from scientific articles (source: titles of Medline abstracts) 6 , 364k sentences (5m words) from drug label documentaion (source: European Medicines Agency) 7 and 120k claims (6m words) from patent documents (source: IFI claims) 8 . endogenous TGF-beta-specific complex TGF-beta endogenous TGF-beta TGF-beta-specific complex  One additional requirement which clearly distinguishes the CLEF-ER entity recognition task from other similar challenges is that the recognition is to be performed in a different language from that of the supplied marked up data. This has the advantage, for current purposes, that the precise boundaries of the entities supplied in the English source data are not of critical importance. No challenge submissions will be evaluated against them. Nevertheless, the assignment of reasonable boundaries still remains important to the challenge not least as a possible locus for the use of machine translation technology in finding correlates in other languages. Furthermore, it is important that good precision is obtained in the assignment of CUIs.</p><p>These requirements all clearly speak to the use of an automatically derived silver standard for the English annotation. Automatic annotation enables one to cover large amounts of data and, by suitably combining the results of several systems, to avoid precision errors introduced by the inevitable idiosyncracies of any one system. Consequently, we decided to adapt and re-use the centroid harmonization methodology, deployed in a previous large-scale annotation challenge: the CALBC challenge (Collaborative Annotation of a Large Scale Biomedical Corpus).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The centroid harmonization of alternative annotations</head><p>Figure <ref type="figure" coords="3,167.51,513.07,4.98,8.74" target="#fig_1">1</ref> shows four human expert (BioCreative Gold Standard) annotations over the string endogenous TGF-beta-specific complex.</p><p>The centroid annotation algorithm, developed initially for the CALBC project, reads in markup from several different annotators and generates a single harmonized annotation which represents both the common heart of the set of annotations and its boundary distribution. The inputs can be Gold Standard as in Figure <ref type="figure" coords="3,166.20,585.09,4.98,8.74" target="#fig_1">1</ref> or imperfect automatic annotations.</p><p>First, text is tokenized at the character level and (ignoring spaces) votes are counted over pairs of adjacent inter-entity characters in the mark-up. Figure <ref type="figure" coords="3,134.77,621.25,4.98,8.74" target="#fig_1">1</ref> also shows the inter-entity character counts. For example, only two of the markups consider that the transition e-n at the start of endogenous falls within an entity. All four consider that T -G do. The focus on inter-entity pairs, rather than single characters, simply ensures that boundaries are valued when two Visceral &lt;e b='l:0:2,l:9:3,r:0:5'&gt; adipose tissue&lt;/e&gt;is particularly responsive to somatropin Fig. <ref type="figure" coords="4,221.63,149.29,4.13,7.89">2</ref>. Centroid plus boundary distribution markup different entity names happen to be immediately adjacent to each other in a markup. Over the course of a whole text, the number of votes will mostly be zero, punctuated by occasional bursts of wave-like variation. The centroids are the substrings over character pairs that are peaks (or local maxima) in a burst of votes. In Figure <ref type="figure" coords="4,218.28,250.44,3.87,8.74" target="#fig_1">1</ref>, TGF-beta is the centroid.</p><p>The inter-entity character votes also define the boundary distribution around the centroid. We define a boundary whenever the number of votes changes. Its value is the difference in votes. So, the centroid TGF-beta has a possible left boundary before T and receives a value of 2 (the difference between 4 and 2) and another before endogenous, which also receives 2 (the difference between 0 and 2). Therefore, these alternative boundaries are equally preferred. There is however no boundary immediately after specific.</p><p>With respect to Gold Standard inputs, the advantages of the centroid representation lie first in its perspicuous representation of variability and secondly in its use as something which candidate annotations can be evaluated against (see <ref type="bibr" coords="4,154.79,391.59,10.52,8.74" target="#b2">[3]</ref> for details) with a clear semantics and scoring method in which equally preferred alternatives are equally scored and more preferred alternatives score more highly than less preferred ones.</p><p>When the inputs are less than perfect (the result of automatic annotators rather than human experts), the result is a harmonized Silver Standard. Figure <ref type="figure" coords="4,134.77,456.19,4.98,8.74">2</ref> shows one centroid uncovered by applying the algorithm to the annotations of CLEF-ER data generated by the five Mantra project partners. In this case, the centroid itself is simply the most highly voted common substring adipose tissue but the boundary distribution shows the alternative boundaries. 'l:0:2" represents the left boundary at exactly the centroid left boundary (position 0) for which two votes were cast. "l:9:3" represents the boundary 9 characters to the left with three votes. "r:0:5" shows that all five votes agreed that the centroid's right boundary is exactly (position 0) the right boundary of the entity.</p><p>One advantage of a centroid Silver Standard is that one easily tailors it for precision or recall (for example, by throwing away centroids or boundaries with very few votes). It should be noted that the centroid heart, adipose tissue in no way represents the correct annotation, or even the best annotation. In the evaluation scheme of <ref type="bibr" coords="4,259.54,621.25,10.52,8.74" target="#b2">[3]</ref> it is simply the string which a candidate annotation must at least cover in order to be true-positive; and a candidate which also included Visceral would in fact score proportionately more because more standards-contributing annotators agree that it should.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Adapting centroids for CLEF-ER</head><p>Although we considered giving CLEF-ER participants the maximally informative centroid representation, it was at least equally desirable to provide the simplest possible representation. We did not wish to discourage participation. There was also no guarantee that the distributional information could be usefully exploited by participants, nor were we ourselves planning to exploit it in evaluation. Therefore, for clarity and simplicity, we decided to offer individual entity markup in a classical format, rather than a distributional format.</p><p>First, we apply a threshold to centroids so that only centroids with at least three votes percolate through to the Silver Standard. Higher thresholds would generate a higher precision harmonization but experiment showed that a three vote threshold gave good recall without sacrificing too much in precision.</p><p>Secondly, in the context of CLEF-ER, we decided that, where harmonization offered a distribution over boundaries, it would be more useful to participants to offer the widest possible boundary, again subject to a threshold, rather than the most popular boundary. In this way, the greatest amount of lexical content would be included in the marked up entities. Consequently, for each centroid, we calculate an extended centroid, or e-centroid, which has the greatest leftmost (rightmost) boundary with votes above the boundary threshold. The e-centroid for Figure <ref type="figure" coords="5,182.34,358.14,4.98,8.74">2</ref> would therefore have a leftmost boundary at position 9, since the leftmost boundary receives the most votes of any left boundary. In this case, the boundary also coincides with the most popular boundary.</p><p>Thirdly, we considered the assignment of CUIs to e-centroids. Since e-centroids result from a harmonization of several voting systems, this is not entirely trivial. In the vast majority of cases, the text stretch of the e-centroid does match the text stretch of at least one of the voting systems, in which case those CUIs are assigned. In cases where the string extent of no contributing annotation exactly matched an e-centroid, we experimented with a voting system over CUIs. It turned out however that such cases were nearly always the result of unusual combinations of errors from different contributing systems. Therefore, rather than try to assign a CUI, we used the absence of agreement between the e-centroid and any voting system as a filter on the silver standard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The granularity of harmonization</head><p>Finally we re-considered the issue of which annotations to harmonize.</p><p>In the CALBC challenge, the task had been to determine typed mentions, i.e. the string extents of entities of certain specified semantic groups. Thus, when evaluating challenge submissions over the title Bacterial eye infection in neonates, a prospective study in a neonatal unit looking for disease and anatomy markup, it makes sense to evaluate against one centroid centred on infection, but preferably extending to the left and another centred on eye and preferably not extending at all. In order to generate these centroids, we run the centroid algorithm once for all annotations of type disease and independently for all annotations of type anatomy.</p><p>&lt;e grp='ANAT' cui='C1563740'&gt;Visceral adipose tissue&lt;/e&gt;is particularly &lt;e grp='DISO' cui='C1273518'&gt;responsive to&lt;/e&gt;&lt;e grp='CHEM' cui='C0376560'&gt;somatropin&lt;/e&gt; Fig. <ref type="figure" coords="6,217.16,160.25,4.13,7.89">3</ref>. One annotation contributing to silver standard</p><p>The Silver Standard CLEF-ER English data is supplied as "hints" on what entities might be found in the non-English text and, since we are not supplying a distribution, it could be misleading to suggest that eye infection might be found, when a different disease bacterial eye infection can also be found.</p><p>The centroid algorithm in itself is perfectly neutral over the range of its inputs, though its outputs will make semantic sense only if the inputs are all annotations of the same semantic sort. The sort however can be semantic groups, or types or even individual CUIs, with the deciding factor being only a) the intended use of the output centroids b) the possibility of data sparsity in a too fine-grained harmonization. For example, if one harmonizes at the level of CUI and some systems annotate only the longest match and others only the shortest match, there is a clear danger that the minimum thresholds will not be met in either case.</p><p>We therefore experimented with both types of harmonization. The result was that the gain in the number of entities obtained outweighed the (only small) sparsity issues that resulted, especially as our thresholding was being set to low values (3 votes or more). Therefore, harmonization by CUI was our preferred option and final choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Examples</head><p>To illustrate the sort of work that harmonization carries out, Figure <ref type="figure" coords="6,446.03,477.79,4.98,8.74">3</ref> shows the annotations generated by one of the contributing Mantra project partners. One of the other four partners agreed exactly with the annotation over Visceral adipose tissue, and another also agreed with the string extent. Consequently, this annotation is propagated to the silver standard, and in fact it would be regardless of whether harmonization were carried out by group or by CUI. Two of the four partners did not support this annotation although they did annotate the substring adipose tissue and agreed amongst themselves on the CUI for it. Consequently, in harmonization by CUI, this annotation also propagates to the silver standard. In harmonization by group, this annotation would be lost, at least if the "widest boundaries which meet the threshold" is used as the criterion for selecting from the distribution. No other partner agreed with the DISO annotation of Figure <ref type="figure" coords="6,218.15,621.25,4.98,8.74">3</ref> so this annotation is not replicated in the silver standard. All partners agreed with the string extent of the annotation over somatropin but the three other CUI-assigning annotations supported a different CUI. In harmonization by CUI, therefore, this annotation does not propagate, although another annotation with the same string extent (and with the other CUI) does propagate.</p><p>In Table <ref type="table" coords="7,188.97,320.15,4.98,8.74" target="#tab_0">1</ref> we give a quantitative evaluation of all 6 partner annotations and show how many of these annotations find their way into a harmonized corpus of a given voting threshold. As explained in the last section, there are a few annotations that are lost even at a voting threshold of 1. Partner P2 and P4 annotated relatively few CUIs, however, most of these annotations had a broad support and contributed substantially more CUIs for the harmonized corpora. Table <ref type="table" coords="7,162.18,391.88,4.98,8.74" target="#tab_0">1</ref> shows also that our final voting threshold of 3 removes on average 19% (SD 14%) of all partner annotations.</p><p>A slightly different view on the effects of voting thresholds on our data is presented in Table <ref type="table" coords="7,219.05,428.86,3.87,8.74" target="#tab_1">2</ref>. For each corpus, we evaluate the loss of CUI annotations against a voting threshold of 1. In total, this amounts to 16 million centroids. The stock of centroids having support from all partners (voting threshold 6) is quite low, i.e. the common set of annotations is 17% of the whole set of annotations available from all partners that go into the SSC. For the preparation of the final SSC for the challenge, we inspected in detail the most frequent annotations from the voting thresholds 2 and 3. Raising the threshold from 2 to 3 filtered a substantial amount of noise. Raising the threshold to 4 would have been viable too. However, threshold 3 was more inline with the strategy to stick to a reasonably high recall harmonization.</p><p>In Table <ref type="table" coords="7,187.75,549.52,4.98,8.74" target="#tab_2">3</ref> we investigate the effect of boundary thresholds with respect to the original partner annotations for the final voting threshold of 3. Two questions are addressed here. How many annotations from partners that are covered by a e-centroid retain their original boundaries? How many of them get shortened or extended by different e-centroid boundary thresholds? Between 96.4% and 97.8% of the partner annotations are represented by e-centroids with exactly the same boundaries. For obvious reasons, a boundary threshold of 1 can only extend partner annotations. A threshold of 3 leads to a shorter e-centroid representation for the majority of inexact boundary matches. A boundary threshold of 2 represents a balanced strategy and was the setting for the final Silver Standard Corpus. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>The CLEF-ER challenge is an interestingly new variant on the classic "named entity recognition" task. In order to generate good quality English marked up data to be provided as part of the challenge, an automatic annotation method was required. The centroid harmonization method has proved a good basis for building a Silver Standard suitable for the CLEF-ER challenge, even in the circumstance where its distributional nature is not to be exploited in a direct evaluation.</p><p>On average, about 81% of all partner annotations are represented by the harmonized annotations using the final voting threshold of 3. This amounts to 45% of all centroids that could have been built by a voting threshold of 1.</p><p>For a voting threshold of 3, 97.8% of the partner annotations which go into the Silver Standard Corpus have exactly the same boundaries as their e-centroid representations. A boundary threshold of 2 extends a reasonable amount of the remaining cases with divergent boundaries. This work is funded by EU Research Grant agreement 296410.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,154.99,174.17,303.84,8.11;3,154.99,185.13,303.84,8.12"><head>.</head><label></label><figDesc>. . t h e e n d o g e n o u s T G F b e t a s p e c i f i c c o .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,155.71,206.07,303.94,7.89"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. BioCreative alternatives (id:534680) &amp; inter-entity character counts</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,134.77,116.91,345.83,138.65"><head>Table 1 .</head><label>1</label><figDesc>Absolute number of annotated CUIs provided by the project partners and the percentage of these annotations that are part of harmonized corpora produced with different voting thresholds.</figDesc><table coords="7,172.26,159.23,270.84,96.33"><row><cell>Partner</cell><cell></cell><cell></cell><cell cols="3">Voting Threshold</cell><cell></cell><cell>Mean</cell></row><row><cell></cell><cell>Annotations</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell></row><row><cell>P1</cell><cell cols="7">6,346,873 99.6% 86.0% 80.0% 72.6% 65.1% 43.8% 78.2%</cell></row><row><cell>P2</cell><cell cols="7">5,278,663 97.6% 95.9% 90.5% 86.4% 76.7% 52.6% 85.7%</cell></row><row><cell>P3</cell><cell cols="7">8,595,019 99.6% 86.5% 78.9% 65.8% 55.2% 32.3% 74.0%</cell></row><row><cell>P4</cell><cell cols="7">5,986,506 99.4% 96.8% 89.6% 77.2% 46.6% 46.5% 79.4%</cell></row><row><cell>P5</cell><cell cols="7">13,701,552 99.0% 59.7% 51.3% 42.7% 35.8% 20.9% 58.5%</cell></row><row><cell>P6</cell><cell cols="7">6,482,321 99.5% 97.7% 95.2% 85.5% 72.6% 42.9% 84.8%</cell></row><row><cell>Mean</cell><cell cols="7">7,731,822 99.1% 87.1% 80.9% 71.7% 58.7% 39.8% 76.8%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,134.77,116.91,345.83,127.29"><head>Table 2 .</head><label>2</label><figDesc>Number of annotated CUIs in the corpora by applying different voting thresholds. There were 6 partner annotations available for the creation of the harmonized corpora.</figDesc><table coords="8,153.90,159.23,307.56,84.97"><row><cell>Voting</cell><cell>EMEA</cell><cell></cell><cell>Medline</cell><cell></cell><cell>Patent</cell><cell></cell><cell cols="2">All Corpora</cell></row><row><cell>Threshold</cell><cell>abs</cell><cell>rel</cell><cell>abs</cell><cell>rel</cell><cell>abs</cell><cell>rel</cell><cell>abs</cell><cell>rel</cell></row><row><cell>1</cell><cell cols="8">2,640,837 100% 11,388,401 100% 2,210,797 100% 16,240,035 100%</cell></row><row><cell>2</cell><cell cols="8">1,393,936 53% 5,964,424 52% 1,290,346 58% 8,648,706 53%</cell></row><row><cell>3</cell><cell cols="8">1,147,152 43% 5,043,464 44% 1,087,133 49% 7,277,749 45%</cell></row><row><cell>4</cell><cell cols="8">963,835 36% 4,140,564 36% 838,000 38% 5,942,399 37%</cell></row><row><cell>5</cell><cell cols="8">792,271 30% 3,359,090 29% 691,322 31% 4,842,683 30%</cell></row><row><cell>6</cell><cell cols="8">460,444 17% 1,941,024 17% 375,547 17% 2,777,015 17%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,134.77,268.20,345.83,94.91"><head>Table 3 .</head><label>3</label><figDesc>Effect of different boundary thresholds (B) on e-centroids (voting threshold 3) compared to the original partner annotations. Label == means exact boundary match on both sides, +-means cases where the partner's left boundary was extended (+) to the left and some material at the end of the partner annotation was shortened(-). ,790 382,792 122,973 121,210 113,015 58,731 3,634 8,795 6,170 35,609,110 3 34,958,525 119,111 103,316 298,907 128,489 105,444 2,876 4,652 7,304 35,728,624</figDesc><table coords="8,139.18,321.97,337.00,30.18"><row><cell>B</cell><cell>==</cell><cell>=+</cell><cell>+=</cell><cell>=-</cell><cell>-=</cell><cell>+-</cell><cell>++</cell><cell>-+</cell><cell>--</cell><cell>Total</cell></row><row><cell cols="4">1 34,053,526 774,553 475,395</cell><cell></cell><cell></cell><cell cols="3">709 27,550 1,036</cell><cell></cell><cell>35,332,769</cell></row><row><cell>2 34,791</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="1,144.73,636.52,104.06,7.47"><p>http://www.CLEF-ER.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="1,144.73,647.48,132.30,7.47"><p>http://www.mantra-project.eu</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2" coords="1,144.73,658.44,137.01,7.47"><p>http://www.clef-initiative.eu</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3" coords="2,144.73,636.52,127.10,7.47"><p>http://www.ncbi.nlm.nih.gov</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4" coords="2,144.73,647.48,131.81,7.47"><p>http://www.ema.europa.eu/ema</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5" coords="2,144.73,658.44,94.15,7.47"><p>http://ificlaims.com</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,138.35,646.84,342.25,7.86;8,146.91,657.79,333.68,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,222.19,646.84,258.41,7.86;8,146.91,657.79,90.90,7.86">The Unified Medical Language System (UMLS): integrating biomedical terminology</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bodenreider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,244.31,657.79,92.25,7.86">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="267" to="270" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>Database-Issue</note>
</biblStruct>

<biblStruct coords="9,138.35,120.67,342.24,7.86;9,146.91,131.63,333.67,7.86;9,146.91,142.59,225.36,8.12" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,273.49,120.67,207.10,7.86;9,146.91,131.63,153.79,7.86">Assessment of NER solutions against the first and second CALBC Silver Standard Corpus</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Rebholz-Schuhmann</surname></persName>
		</author>
		<ptr target="http://www.jbiomedsem.com/content/2/S5/S11" />
	</analytic>
	<monogr>
		<title level="j" coord="9,307.27,131.63,128.70,7.86">Journal of Biomedical Semantics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2011">11/2011 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,153.55,342.24,7.86;9,146.91,164.51,333.67,7.86;9,146.91,175.46,333.67,7.86;9,146.91,186.42,159.44,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,336.61,153.55,143.98,7.86;9,146.91,164.51,81.80,7.86">Centroids: Gold standards with distributional variation</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Lewin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kafkas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Rebholz-Schuhmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,252.41,164.51,228.18,7.86;9,146.91,175.46,333.67,7.86;9,146.91,186.42,69.09,7.86">Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC&apos;12). European Language Resources Association (ELRA)</title>
		<meeting>the Eighth International Conference on Language Resources and Evaluation (LREC&apos;12). European Language Resources Association (ELRA)<address><addrLine>Istanbul</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-05">May 2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
