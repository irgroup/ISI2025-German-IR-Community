<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,153.62,116.95,308.11,12.62;1,242.51,134.89,130.34,12.62">Towards Knowledge-enriched Cross-Lingual Answer Validation</title>
				<funder ref="#_sxH6kSy">
					<orgName type="full">MOLTO European</orgName>
				</funder>
				<funder ref="#_bc5zfZB">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,222.03,172.74,68.92,8.74"><forename type="first">Valentin</forename><surname>Zhikov</surname></persName>
							<email>valentin.zhikov@ontotext.com</email>
							<affiliation key="aff0">
								<orgName type="department">Ontotext AD</orgName>
								<address>
									<addrLine>Polygraphia Office Center fl. 4, 47 A Tsarigradsko Shosse</addrLine>
									<postCode>1504</postCode>
									<settlement>Sofia</settlement>
									<country key="BG">Bulgaria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,318.09,172.74,70.77,8.74"><forename type="first">Georgi</forename><surname>Georgiev</surname></persName>
							<email>georgiev@ontotext.com</email>
							<affiliation key="aff0">
								<orgName type="department">Ontotext AD</orgName>
								<address>
									<addrLine>Polygraphia Office Center fl. 4, 47 A Tsarigradsko Shosse</addrLine>
									<postCode>1504</postCode>
									<settlement>Sofia</settlement>
									<country key="BG">Bulgaria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,153.62,116.95,308.11,12.62;1,242.51,134.89,130.34,12.62">Towards Knowledge-enriched Cross-Lingual Answer Validation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A063F01C0968A04AFDF74B3E483030AF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>answer validation, approximate matching</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our baseline approach from the 2012 year includes three language-independent methods for the task of answer validation. All methods are based on a scoring mechanism that reflects the degree of similarity between the question-answer pairs and the supporting text. We evaluate the proposed methods when using various string similarity metrics, such as exact matching, Levenshtein, Jaro and Jaro-Winkler. In addition to this baseline approach, we take advantage of the multilingual QA4MRE dataset, and devise an ensemble method, which chooses the answer indicated as correct by the largest number of analyses of the individual translations. Finally, we present a language-augmented method that enriches the questions and answers with paraphrases obtained by means of machine translation. We show that all of the described approaches achieve a significant improvement over the random baseline, and that both majority voting and language augmentation lead to superior accuracy as compared with the original method. However, the addition of some knowledge-based components in year 2013 plus the complexity of the datasets led to decrease in overall accuracy for Bulgarian language.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Question answering (QA) is a difficult problem situated at the intersection of several domains, including natural language processing and knowledge representation <ref type="bibr" coords="1,165.08,549.52,9.96,8.74" target="#b0">[1]</ref>. A subproblem of question answering is the answer validation task, which consists of deciding whether a given answer is correct or not, based on a text collection. The problem of answer validation remains challenging, the state-of-the-art performance being not larger than 60% accuracy <ref type="bibr" coords="1,428.50,585.39,9.96,8.74" target="#b1">[2]</ref>, whereas the human performance is around 80% <ref type="bibr" coords="1,306.79,597.34,9.96,8.74" target="#b1">[2]</ref>. In the frames of the QA4MRE competition at CLEF, many approaches for answer validation have been proposed. The techniques employed include part-of-speech tagging, named entity recognition, syntactic transformations, semantic role labeling, logical representations, theorem provers and others. Many QA systems make use of external knowledge resources such as encyclopedia, ontologies, gazetteers, thesauri, etc. An optimal combination between these approaches and resources is necessary in order to provide with a performant system.</p><p>Identifying paraphrases of the question and answer in the supporting text helps locating the sentences containing their correct answer. In order to obtain paraphrases, semantic and syntactic resources have been used <ref type="bibr" coords="2,417.74,168.01,9.96,8.74" target="#b5">[6]</ref>, <ref type="bibr" coords="2,435.65,168.01,9.96,8.74" target="#b6">[7]</ref>, <ref type="bibr" coords="2,453.56,168.01,9.96,8.74" target="#b7">[8]</ref>. In this article, we describe our current system, which builds on machine translation techniques for generating paraphrases, by translating text to another (dissimilar) language and then back to the source language. Our experience with statistical machine translation (by our involvement into the MOLTO European project<ref type="foot" coords="2,472.25,214.26,3.97,6.12" target="#foot_0">1</ref> ) shows that the resulting text is not identical with the initial text, but often contains synonymous paraphrases.</p><p>This year we also rely on additional preprocessing strategies as well as on some lexical resources, such as synonymy dictionary as well as paraphrases, extracted from Wikipedia. We focused on Bulgarian question answering task only, preforming 8 runs. As it is discussed in the next sections, the results drop in accuracy in comparison with the previous year's ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>We present here three methods for answer validation: an overlap-based algorithm (denoted by OV), a language augmented approach which builds on top of the overlap approach (called LAM-OV) and an ensemble model based on majority voting called voting overlap (V-OV). We will make use of the following simple notations: the questions are denoted by Q(1), ..., Q(n), the answers pertaining to question i are denoted as A(i, 1), ..., A(i, 5) and the supporting text is called T .</p><p>We note that both our algorithms always indicate the best scoring answer as the correct answer (according to our scoring scheme) and never leave a question unanswered. Also, our approaches are entirely based on the supporting, but this time we consider also some additional knowledge sources).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The OV method</head><p>The OV algorithm performs two steps: first, a filtering approach selects only the sentences from the supporting text that are similar to both the question and the answers. Then, the pairs (answer, supporting sentence) that yield highest similarity are returned.</p><p>More precisely, for each question Q(i), the OV algorithm performs the following steps: first, it compares the lexical overlap between all concatenated questionanswer pairs {⟨Q(i), A(i, 1)⟩, ..., ⟨Q(i), A(i, 5)⟩}, and all sentences of the supporting text s 1 , ..., s |T | ∈ T . The overlap is computed using a function δ ϕ of some similarity measure between text snippets ϕ. We will discuss the scoring function δ ϕ and our choices for ϕ later in this section. We proceed by computing a relevance score:</p><formula xml:id="formula_0" coords="3,186.20,142.49,242.95,9.65">ρ(s k ) = max j=1,...,5 δ ϕ (⟨Q(i), A(i, j)⟩, s k ), k ∈ {1, ..., |T |}</formula><p>and retain the top-scoring l sentences for futher analysis, concatenating them into a single long string. Hence, for each question Q(i), a text extract S(i) results. These extracts combine the sentences that are most relevant to any of the given question-answer combinations.</p><p>Then, the OV algorithm ranks the answers A(i, 1), ..., A(i, 5) in decreasing order by their similarity to the text in S(i). The pair with largest similarity δ ϕ (A(i, j), S(i)) gives the winning answer A(i, j) to the question Q(i).</p><p>The number l and the similarity measure ϕ are parameters of the OV method. In our experiments, we tried several values of l ∈ {1, 2, 3, 4, 5} and several similarity measures ϕ. Specifically, for two text snippets (e.g. sentences, represented as bag-of-words), a target t 0 and an arbitrary t, the similarity between t and the target t 0 is defined as follows:</p><formula xml:id="formula_1" coords="3,224.95,304.29,165.45,32.65">δ ϕ (t 0 , t) = ∑ |t0| i=1 max |t| j=1 ϕ(t 0 (i), t(j)) |t 0 | ,</formula><p>where ϕ corresponds to a distance measure between two words. In our experiments, ϕ is either exact matching, Levenshtein <ref type="bibr" coords="3,343.27,360.47,9.96,8.74" target="#b2">[3]</ref>, Jaro <ref type="bibr" coords="3,382.71,360.47,10.52,8.74" target="#b3">[4]</ref> or Jaro-Winkler <ref type="bibr" coords="3,470.09,360.47,10.52,8.74" target="#b4">[5]</ref> similarity. For the final models, we selected the values of l and ϕ that gave best results on the corpus from CLEF2011. (Pseudo-code for the described algorithm is available in Appendix A.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The V-OV method</head><p>The V-OV approach that we present is an ensemble method. For a specific question, each of the models based on the parallel corpora vote for the correct answer choice. The answer that gathers most votes is indicated as correct.</p><p>The assumption that we make is that some answers are easier to validate in some languages and more difficult in others. However, this approach heavily relies on the parallelism of the corpora in different languages, in the sense that the sentences forming the supporting text, the questions and the answers must carry the same information, and the questions and answers must follow the same order. Also, the prediction of the correct answer is identical, irrespective of the target language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">The LAM-OV method</head><p>The LAM-OV method uses automated translation as a means of enriching the text with paraphrases and synonyms prior to executing the answer selection algorithm, in order to improve its performance. Specifically, for each target language, we transform the questions and answers by successive translations into intermediate languages. For example, in order to obtain several (synonymous)</p><p>paraphrases in Bulgarian, we translate the question and answers from the Bulgarian corpus into other languages (English, German, Swedish, Arabic) and then back to Bulgarian. Thus, the answers that contain paraphrases of the support text have a higher chance of being matched. We used the online Google Translate<ref type="foot" coords="4,150.82,166.24,3.97,6.12" target="#foot_1">2</ref> service for obtaining translations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Preprocessing</head><p>Before the algorithms are applied we perform the following preprocessing steps. All questions, answers and supporting text are converted to lower-case. Next, possible abbreviations are discovered via a regular expression that looks for recurring sequences comprising letters and periods without any white-space characters in between, and the period symbols are deleted from the matched sequences. Also, we added several rules that instruct the algorithm to ignore several common abbreviations of the type 'years' (ã.), 'millions' (ìèë.), 'billions' (áèë.), etc. by eliminating the period character in such cases. The supporting text is then segmented into sentences by splitting the transformed strings at each remaining period symbol. All text undergoes one more phase of preprocessing, through which symbols other than numbers and letters are replaced with whitespace characters (we use a common mask for all languages apart from Arabic, for which our system is not directly applicable). Eventually, we tokenize each sentence using the resulting white-space subsequences as a delimiter. This year we also added stemming as a preprocessing step, a synonymy lexicon and compiled lexicons from Wikipedia. We used stemming in processing English questions and answers for getting a better generalization on the abstract semantic level. Note that the used Wikipedia resource contains not only typical paraphrases in the sense of synonyms, but also extensions of abbreviations, hyperonymic relations, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results from year 2012</head><p>For the QA4MRE competition at CLEF2012 we submitted a total of 10 models. A summary can be found in Table <ref type="table" coords="4,292.97,517.19,3.87,8.74" target="#tab_0">1</ref>. We submitted models based on the OV method for 6 of the languages included in the competition. Performance figures are presented in the last column of Table <ref type="table" coords="4,325.40,541.10,3.87,8.74" target="#tab_0">1</ref>. The performance is around 0.30 (accuracy), with larger values for Italian, Romanian and English and worse results for Bulgarian, German and Spanish. A similar trend was observed when applying the algorithms to the reading tests included in the CLEF2011 dataset. More details on the performance of the OV algorithm are given in Table <ref type="table" coords="4,455.57,588.92,3.87,8.74" target="#tab_1">2</ref>. We show how the results vary with the choice of parameters l and ϕ, on two corpora (from 2011 and 2012) and for two of the languages (Bulgarian and English). We used the 2011 corpus for selecting the optimal parameters, specifically l = 3 and ϕ = ϕ ExactM atching . These values maximized the mean accuracy of the system The values of the similarity δ ϕ are E (exact match), L (Levenshtein), J (Jaro) and J-W (Jaro-Winkler). calculated against the reading tests in all supported languages. In Table <ref type="table" coords="5,455.43,378.74,3.87,8.74" target="#tab_1">2</ref>, the accuracy corresponding to these parameters is marked (0.38).</p><p>The performance of the voting algorithm V-OV (0.29) is superior than that of the OV algorithm for several languages, including Bulgarian, Spanish and German, but worse for English, Italian and Romanian (Table <ref type="table" coords="5,401.53,434.24,3.87,8.74" target="#tab_0">1</ref>). The poor score is the consequence of the lack of parallelism between the corpora, meaning that the reading tasks, questions and answers were arranged in different order in the 2012 corpus available at submission time. We repeated our experiments against the synchronized dataset released after the system submission and found out that a simple ensemble voting scheme that excludes the worst-performing systems (Spanish and German languages, according to the results for the 2011 corpus) would have achieved an accuracy of 0.38. We report this number in this manuscript as the best result that we have obtained against the CLEF2012 dataset.</p><p>We applied the LAM-OV approach only to the Bulgarian corpus. In order to enrich the questions and answers with paraphrases, we translated the original corpus to several other languages and then back to Bulgarian. We performed three such experiments, where the intermediate languages were: i ) English, ii ) German and iii ) Swedish followed by Arabic. For cases i ) and ii ), we obtained 0.29 accuracy. In the case iii ), the accuracy reached 0.31. In all cases, we improve the OV baseline. We carried out an additional experiment in which we concatenated all translations (from German, English and Swedish/Arabic) to the original. The performance of the model was 0.31.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Current results</head><p>This year we performed tests only on Bulgarian data, using LAM-OV approach only and adding of new pre-processing steps. The performed runs were 8. They are presented in Table <ref type="table" coords="6,235.05,174.27,3.87,8.74" target="#tab_2">3</ref>. All the models include stemming and synonymy enrichment. Additionally, the first four models use paraphrases from Wikipedia. The results show, however, that these models perform under the baseline and results from year 2012. The next four models do not rely on paraphrases. They additionally have the following specific features:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ID Method Language</head><p>1. 05 model: looks up the answers in 1 sentence, which has the highest score; it does not give an answer, if it is not sure about it. 2. 06 model: looks up the answers in 1 sentence, which has the highest score; it always gives an answer. 3. 07 model: looks the answers in top 3 sentences, which have the highest score; it does not give an answer, if it is not sure about it. 4. 08 model: looks up the answers in top 3 sentences, which have the highest score; it always gives an answer.</p><p>It seems that the best score is achieved by the systems which look up in top 3 sentences, which means -in a wider context; and irrespectively of their confidence behaviour. It should be noted that the addition of a knowledge-rich, but somewhat noisy resources, such as the Wikipedia-derived lexicon, performs worse than the models without it. In general, however, all the results are bellow the last year's ones. This might be interpreted as follows: the canonical lexiconbased synonymy is not enough for handling the question-answer paraphrases. The addition of stemming seems suitable for the English part of data, but not so helful for the generation of Bulgarian pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>The OV algorithm is a very simple and generic approach, which can be applied to most of the languages included in the QA4MRE dataset without any supplementary resources. Its generality comes at the price of modest performance, although the accuracy is significantly larger than a random baseline of 0.20 (which picks the correct answer uniformly at random among the choices).</p><p>The OV approach essentially searches for common words between supporting text and question/answers using an approximate string matching paradigm. Interestingly, we found that metrics like Levenstein, Jaro, and Jaro-Winkler, which reflect the differences between words, were not better than exact matching with respect to system performance. We expected that approximate matching would have a similar effect to applying a lemmatizer, with the advantage of language independence. However, the experiments did not support our expectations.</p><p>One of the reasons why our overlap-based method did not perform very well in 2012 lied in its inability to address more complex textual inferences, such as synonymy, paraphrases, nominalization/verbalization, etc. (Refer to <ref type="bibr" coords="7,453.74,331.00,10.52,8.74" target="#b1">[2]</ref> for more information regarding the use of specific means of expression.) Our error analysis revealed that a large fraction of the errors were indeed attributable to paraphrasing. In 2012 paper, we presented the language-augmented method as a cheap and fast, albeit not highly accurate, approach to obtaining paraphrases. The approach is based on bidirectional machine translation (to the target and then back to the source language) performed using Google Translate. We rely on the statistical variance of the automated translator, which, if applied several times with different intermediate languages, is likely to output a rich set of synonyms and paraphrases. We also believe that the more different the intermediate language is with the target language, the more likely it is to obtain paraphrases. This year we added some lexical knowledge to face the paraphrase variety in natural language. However, the results remained below the baseline and last year's accuracy values.</p><p>Below we list three classes of issues addressed by the language-augmented technique in 2012.</p><p>The first one is the generation of synonyms, in a form suitable for exact matching. For instance, we have been able to generate the term "states" from a sentence/answer pair containing the closely related term "countries" (originally: "ñòðàíè" and "äúðaeàâè", in Bulgarian). Other examples include: "American"/"U.S." ("àìåðèêàíñêîòî" / "íà ÑÀÙ"), pairs of interchangeable Bulgarian terms for "industry" ("ïðîìèøëåíîñò" / "èíäóñòðèÿ"), "electricity" ("åëåêòðè÷åñòâî" / "òîê"), etc.</p><p>The second one is the generation of paraphrases, such as "÷àñò îò Àôðèêà, þaeíî îò Ñàõàðà" and "÷àñò íà Àôðèêà íà þã îò Ñàõàðà" (two expressions roughly translated as "a part of Africa to the south of Sahara"). Albeit the phrases generated in this way are not always gramatically correct, this class of transformations has the advantage of providing a more varied set of word forms given a term from the source text, and thus can improve the recall of matching during the candidate scoring phase.</p><p>Last, we observed issues related to the alternative representations of numerical values. For instance, the correct answer to the question "For how long has Rebecca Lolosoli been working with MADRE?" (reading test 4, question 9, synchronized gold standard dataset) has been provided in both a numerical and lexical forms, accross the various translations of the dataset. As Google Translate can interchange numerical values with their string representation in some cases (that seem to depend on the particular choice of a language pair), the language-augmented method can be regarded as a simple ad-hoc approach for resolving this kind of issues.</p><p>Presently, we consider the best scoring sentences from the text as likely to contain the answer, based on the assumption that the answer is indeed contained in the provided supporting text. However, in a general setting, some or all of the best scoring sentences might not be 'good enough', in the sense that their overlap with the question/answers text is very low. Introducing a minimal threshold parameter that eliminates sentences with too small overlap can for example result in unanswered questions -a choice which is encouraged by the evaluation system at the QA4MRE challenge. Also, it would allow for efficient scanning of very large collections of text, in addition to the corpus provided. Choosing a minimal threshold for text similarity can be for example done by comparing the distributions of the similarities between question/true answer and sentences containing answer and the rest of the similarities (false answers, arbitrary text, etc).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we have described the array of algorithms that constitute our consequent submission to the QA4MRE at CLEF2012 and CLEF2013 competition. The reported results reveal that our basic algorithm outperforms the random baseline irrespective of the language of the analyzed textual content, without resorting to any side resources nor language-specific tools.</p><p>We have shown that the results of the basic system can be improved significantly by incorporating a mechanism for majority voting based on the analysis of the individual translations included in the data collection. Also, we have shown that bidirectional statistical machine translation can introduce some amount of variation in the corpus that allows for improved overlap-based approaches. Last, we tried our system in 2013 with the addition of stemming and synonymy/paraphrases addition. The results, however, remained below the last year's experiments.</p><p>We could extend our unsupervised and language-independent approach by incorporating some importance-based weighting scheme (such as tf*idf) into the score computation mechanism, in order to boost the scores of answers containing terms of high relevance within the context of a concrete article. Similarly, an instantiation of the language-augmented approach that enriches the queries and answers with semantically close terms, extracted by some clustering technique from the background collection, could also lead to a better performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,134.76,118.75,339.03,196.65"><head>Table 1 .</head><label>1</label><figDesc>Experiments submitted. Description of the method is given in the second column. Last column indicates the accuracy of the model.</figDesc><table coords="5,136.56,118.75,337.23,140.49"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>EN</cell><cell cols="2">BG</cell></row><row><cell cols="4">ID Method Language Perf 01 OV Bulgarian 0.28</cell><cell></cell><cell>@ l @ @ ϕ E 1 0.36 0.26 0.26 0.28 -L J J-W E</cell><cell>L -</cell><cell cols="2">J J-W --</cell></row><row><cell>02</cell><cell>OV</cell><cell cols="2">English 0.31</cell><cell>2011</cell><cell>2 0.36 0.3 0.3 0.26 -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>03</cell><cell>OV</cell><cell>Italian</cell><cell>0.35</cell><cell></cell><cell>3 0.38 0.3 0.32 0.28 -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>08</cell><cell>OV</cell><cell cols="2">Romanian 0.34</cell><cell></cell><cell>4 0.36 0.30 0.31 0.31 -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>09</cell><cell>OV</cell><cell cols="2">German 0.28</cell><cell></cell><cell>5 0.36 0.31 0.32 0.32 -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>10</cell><cell>OV</cell><cell cols="2">Spanish 0.28</cell><cell></cell><cell cols="4">1 0.31 0.34 0.31 0.32 0.3 0.3 0.3 0.27</cell></row><row><cell cols="4">04 V-OV Bulgarian 0.29</cell><cell></cell><cell cols="4">2 0.33 0.34 0.31 0.29 0.27 0.24 0.29 0.29</cell></row><row><cell cols="2">05 V-OV</cell><cell cols="2">English 0.29</cell><cell>2012</cell><cell cols="4">3 0.31 0.33 0.33 0.31 0.28 0.28 0.27 0.29</cell></row><row><cell cols="2">06 V-OV</cell><cell>Italian</cell><cell>0.29</cell><cell></cell><cell cols="4">4 0.28 0.31 0.31 0.31 0.29 0.29 0.26 0.29</cell></row><row><cell cols="4">07 LAM-OV Bulgarian 0.30</cell><cell></cell><cell cols="4">5 0.27 0.31 0.33 0.31 0.28 0.31 0.25 0.29</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,267.62,263.38,207.49,51.73"><head>Table 2 .</head><label>2</label><figDesc>Performance of the OV model for English and Bulgarian. Results for the corpora from 2011 and 2012 are shown. Values corresponding to parameters l and ϕ are presented, optimal values being indicated by the marked cell from the 2011 corpus.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,134.76,210.18,345.82,122.03"><head>Table 3 .</head><label>3</label><figDesc>Experiments submitted. Description of the method is given in the second column. Last column indicates the accuracy of the model.</figDesc><table coords="6,247.08,210.18,121.64,98.75"><row><cell>Perf</cell></row><row><cell>01 LAM-OV Bulgarian 0.18</cell></row><row><cell>02 LAM-OV Bulgarian 0.18</cell></row><row><cell>03 LAM-OV Bulgarian 0.18</cell></row><row><cell>04 LAM-OV Bulgarian 0.18</cell></row><row><cell>05 LAM-OV Bulgarian 0.22</cell></row><row><cell>06 LAM-OV Bulgarian 0.23</cell></row><row><cell>07 LAM-OV Bulgarian 0.24</cell></row><row><cell>08 LAM-OV Bulgarian 0.24</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.73,657.80,122.38,7.86"><p>http://www.molto-project.eu/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,144.73,657.80,115.77,7.86"><p>http://translate.google.com/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was partially supported by the <rs type="funder">MOLTO European</rs> project (<rs type="grantNumber">FP7-ICT-247914</rs>). <rs type="projectName">Ontotext</rs> AD is a part of the <rs type="projectName">MOLTO</rs> consortium.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_sxH6kSy">
					<idno type="grant-number">FP7-ICT-247914</idno>
					<orgName type="project" subtype="full">Ontotext</orgName>
				</org>
				<org type="funded-project" xml:id="_bc5zfZB">
					<orgName type="project" subtype="full">MOLTO</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A</head><p>The OV Algorithm program OV (l, phi) {Assume given the three components:</p><p>-supporting text (T) -questions set Q(1), ..., Q(n) -corresponding multiple answers A(i, j), i=1..n, j=1..m} Preprocessing Trim spaces from T, Q and A; Apply lowercase conversion to T, Q and A; Remove "." from abbreviation-like strings; #matched using regex Segment into sentences T, Q and A, using the "." delimiter; Apply sentence tokenization based on white space characters;</p><p>Identifying the correct answer for each question Q(i), i=1..n Remove first word of Q(i) for each sentence S(k), k=1..length_in_sentences(T) for each answer A(i, j), j=1..m V(j) := Concatenate Q(i) and A(i,j); score(S(k)) := max(score(S(k), delta_phi(V(j) and S(k)))) endfor sort S by score(S(k)) in descending order R(i) = S(1) + ... + S(l) # concatenate highest-ranking l sentences endfor highestSimilarity := -Inf; for each answer A(i, j), j=1..m s := delta(A(i, j) and R(i)); if s &gt; highestSimilarity bestAnswer := A(i, j); highestSimilarity := s; endif endfor endfor Output bestAnswer for Q(i); endfor end.</p><p>The OV Algorithm. The basic algorithm that underlies all of the described methods. Described in detail in section 2.1.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="9,138.34,252.90,342.23,10.13;9,146.91,266.13,267.05,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,337.65,255.17,142.93,7.86;9,146.91,266.13,85.15,7.86">Testing the Reasoning for Question Answering Validation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Á</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,239.14,266.13,84.22,7.86">J. Log. and Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="459" to="474" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.34,274.82,342.24,10.13;9,146.91,288.05,333.67,7.86;9,146.91,299.01,282.28,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,161.15,288.05,319.43,7.86;9,146.91,299.01,41.78,7.86">Overview of QA4MRE at CLEF 2011: Question Answering for Machine Reading Evaluation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Forner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Á</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sutcliffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Forascu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sporleder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,195.92,299.01,200.32,7.86">CLEF 2011 Labs and Workshop Notebook Papers</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.34,309.96,342.24,7.86;9,146.91,320.92,214.48,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,221.79,309.96,258.79,7.86;9,146.91,320.92,25.65,7.86">Binary codes capable of correcting deletions, insertions, and reversals</title>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">I</forename><surname>Levenshtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,179.29,320.92,91.67,7.86">Soviet Physics Doklady</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="70" to="710" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.34,331.88,342.24,7.86;9,146.91,342.84,333.67,7.86;9,146.91,353.80,25.60,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,200.80,331.88,279.78,7.86;9,146.91,342.84,71.25,7.86">Advances in record linkage methodology as applied to the 1985 census of Tampa Florida</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Jaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,226.24,342.84,177.25,7.86">Journal of the American Statistical Society</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">406</biblScope>
			<biblScope unit="page" from="414" to="420" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.34,364.76,342.23,7.86;9,146.91,375.72,333.67,7.86;9,146.91,386.68,271.13,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,218.94,364.76,261.64,7.86;9,146.91,375.72,158.35,7.86">String Comparator Metrics and Enhanced Decision Rules in the Fellegi-Sunter Model of Record Linkage</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">E</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,312.68,375.72,167.90,7.86;9,146.91,386.68,203.11,7.86">Proceedings of the Section on Survey Research Methods (American Statistical Association)</title>
		<meeting>the Section on Survey Research Methods (American Statistical Association)</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="354" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.34,397.64,342.23,7.86;9,146.91,408.59,333.67,7.86;9,146.91,419.55,117.75,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,244.36,397.64,181.39,7.86">DIRT -discovery of inference rules from text</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,432.51,397.64,48.07,7.86;9,146.91,408.59,333.67,7.86;9,146.91,419.55,46.87,7.86">Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the seventh ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="323" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.34,430.51,342.24,7.86;9,146.91,441.47,333.67,7.86;9,146.91,452.43,333.67,7.86;9,146.91,463.39,28.16,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,346.85,430.51,133.74,7.86;9,146.91,441.47,135.83,7.86">Information fusion in the context of multi-document summarization</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">R</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,290.71,441.47,189.86,7.86;9,146.91,452.43,294.08,7.86">Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics</title>
		<meeting>the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="550" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.34,474.35,342.24,7.86;9,146.91,485.31,119.45,7.86" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="9,224.53,474.35,256.05,7.86;9,146.91,485.31,37.01,7.86">Determining similarity and inferring relations in a lexical knowledge base</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">D</forename><surname>Richardson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
