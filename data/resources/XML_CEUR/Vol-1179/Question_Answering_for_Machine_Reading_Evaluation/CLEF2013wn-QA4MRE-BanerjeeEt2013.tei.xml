<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,154.43,151.87,314.45,13.13;1,227.46,169.39,157.06,13.13">Multiple Choice Question (MCQ) Answering System for Entrance Examination</title>
				<funder ref="#_JZudNaC">
					<orgName type="full">Department of Electronics and Information Technology (DeitY), Ministry of Communications &amp; Information Technology</orgName>
					<orgName type="abbreviated">MCIT</orgName>
				</funder>
				<funder ref="#_3Yj84jG">
					<orgName type="full">DST India-CONACYT Mexico</orgName>
				</funder>
				<funder>
					<orgName type="full">DST, Government of India</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,205.82,207.91,73.52,9.11"><forename type="first">Somnath</forename><surname>Banerjee</surname></persName>
							<email>s.banerjee1980@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Jadavpur University</orgName>
								<address>
									<postCode>700032</postCode>
									<settlement>Kolkata</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,287.41,207.91,60.78,9.11"><forename type="first">Pinaki</forename><surname>Bhaskar</surname></persName>
							<email>pinaki.bhaskar@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Jadavpur University</orgName>
								<address>
									<postCode>700032</postCode>
									<settlement>Kolkata</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,356.23,207.91,55.74,9.11"><forename type="first">Partha</forename><surname>Pakray</surname></persName>
							<email>parthapakray@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Jadavpur University</orgName>
								<address>
									<postCode>700032</postCode>
									<settlement>Kolkata</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,213.01,219.43,90.75,9.11"><forename type="first">Sivaji</forename><surname>Bandyopadhyay</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Jadavpur University</orgName>
								<address>
									<postCode>700032</postCode>
									<settlement>Kolkata</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,328.77,219.43,78.54,9.11"><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
							<email>gelbukh@gelbukh.com</email>
							<affiliation key="aff1">
								<orgName type="department">Center for Computing Research</orgName>
								<orgName type="institution">National Polytechnic Institute</orgName>
								<address>
									<settlement>Mexico City</settlement>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,154.43,151.87,314.45,13.13;1,227.46,169.39,157.06,13.13">Multiple Choice Question (MCQ) Answering System for Entrance Examination</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">66452555472BFD1F1A88DA5BE6BE5B9A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Question Answering technique</term>
					<term>QA4MRE Data Sets</term>
					<term>Named Entity</term>
					<term>Textual Entailment</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The article presents the experiments carried out as part of the participation in the pilot task of QA4MRE@CLEF 2013. In the developed system, we have first generated answer pattern by combining the question and each answer option to form the Hypothesis (H). Stop words and interrogative word are removed from each H and query words are identified to retrieve the most relevant sentences from the associated document using Lucene. Relevant sentences are retrieved from the associated document based on the TF-IDF of the matching query words along with n-gram overlap of the sentence with the H. Each retrieved sentence defines the Text T. Each T-H pair is assigned a ranking score that works on textual entailment principle. A matching score is automatically assigned to each answer options based on the matching. A parallel procedure also generates the possible answer patterns from given questions and answer options. Each sentence in the associated document is assigned an inference score with respect to each answer pattern. Evaluated inference score for each answer option is added with the matching score. The answer option that receives the highest selection score is identified as the most relevant option and selected as the answer to the given question.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The main objective of QA4MRE <ref type="bibr" coords="1,260.81,580.63,11.61,9.11" target="#b2">[3]</ref> is to develop a methodology for evaluating Machine Reading systems through Question Answering and Reading Comprehension Tests. Machine Reading task obtains an in-depth understanding of just one or a small number of texts. The task focuses on the reading of single documents and identification of the correct answer to a question from a set of possible answer options. The identification of the correct answer requires various kinds of inference and the consideration of previously acquired background knowledge. Ad-hoc collections of background knowledge have been provided for each of the topics in all the languages involved in the exercise so that all participating systems work on the same background knowledge. Texts have been included from a diverse range of sources, e.g. newspapers, newswire, web, blogs, Wikipedia entries.</p><p>Answer Validation (AV) is the task of deciding for given a question and an answer from a QA system, whether the answer is correct or not and it was defined as a problem of RTE in order to promote a deeper analysis in Question Answering <ref type="bibr" coords="2,392.37,242.47,10.59,9.11" target="#b2">[3]</ref>. Answer Validation Exercise (AVE) is a task introduced in the QA@CLEF competition. AVE task is aimed at developing systems that decide whether the answer of a Question Answering system is correct or not. There were three AVE competitions AVE 2006 <ref type="bibr" coords="2,390.86,277.03,10.59,9.11" target="#b3">[4]</ref>, AVE 2007 <ref type="bibr" coords="2,457.41,277.03,11.61,9.11" target="#b4">[5]</ref> and AVE 2008 <ref type="bibr" coords="2,171.81,288.55,10.59,9.11" target="#b5">[6]</ref>. AVE systems receive a set of triplets (Question, Answer and Supporting Text) and return a judgment of "SELECTED", "VALIDATED" or "REJECTED" for each triplet.</p><p>Section 2 describes the corpus statistics. Section 3 describes the system architecture. The experiments carried out on test data sets are discussed in Section 4 along with the results. The conclusions are drawn in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Corpus Statistics</head><p>Like main task, this pilot task focuses on the reading of single documents and the identification of the answers to a set of questions about information that is stated or implied in the text. Questions are in the form of multiple choices, each having five options, and only one correct answer. The detection of correct answers is specifically designed to require various kinds of inference.</p><p>The Entrance Exams 2013 test set will be composed of reading comprehension tests taken from the Japanese Center Test, which is a nation-wide achievement test for Japanese university admissions: Each reading test will consist of one single document, with 5 questions and a set of five choices per question. So, there will be in total:</p><p>• -9 reading test documents</p><p>• -46 questions (5 questions for each document except document 2 with 6 questions) • -184 choices/options (4 for each question) Test documents, questions, and options were made available in English. Participating systems will be required to answer these 45 questions by choosing in each case one answer from the five alternatives. There will always be one and only one correct option.</p><p>Systems will also have the chance to leave some questions unanswered if they are not confident about the correctness of their response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Machine Reading System Architecture</head><p>The architecture of machine reading system is described in Figure <ref type="figure" coords="3,406.98,226.87,4.92,9.11" target="#fig_0">1</ref> and the proposed architecture is made up of two main modules. Each of these modules is now being described in subsequent subsections. XML parser. The given XML corpus has been parsed using XML parser. The XML parser extracts the document and associated questions. After parsing, the documents and the associated questions are extracted from the given XML documents and stored in the system.</p><p>Named Entity (NE) Identification. For each question, system must identify the correct answer among the proposed alternative answer options. Each generated answer pattern corresponding to a question is compared with each sentence in the document to assign an inference score. The score assignment module requires that the named entities in each sentence and in each answer pattern are identified. The CRF-based Stanford Named Entity Tagger<ref type="foot" coords="4,152.88,331.22,3.00,5.56" target="#foot_0">1</ref> (NE Tagger) has been used to identify and mark the named entities in the documents and queries. The tagged documents and queries are passed to the lexical inference sub-module. Anaphora Resolution. It has been observed that resolving the anaphors in the sentences in the documents improves the inference score of the sentence with respect to each associated answer option. To resolve the anaphora BART<ref type="foot" coords="4,389.52,404.90,3.00,5.56" target="#foot_1">2</ref> (Beautiful Anaphora Resolution Toolkit) has been used in the present task. BART performs automatic coreference resolution, including all necessary preprocessing steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Question Processing Module</head><p>This module responsible for deciding answers or not answers to a question. We do not answer comparative questions e.g., "How was Mary different from Susan?". Table <ref type="table" coords="4,478.51,496.15,8.87,9.11" target="#tab_0">-1</ref> shows the question types that we have answered. At first, stop words and interrogatives have been removed from question text to build query terms (QT). Then, an answer pattern is built by (QT, OPTION T ) pair; where OPTION T refers the T-th answer option. Each answer pattern is considered as a hypothesis H. So, five hypotheses have been built for each question. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Answering Module</head><p>Answering module is responsible for calculating the score of the each answer option.</p><p>Answer Validation. The corpus is in XML format. All the XML test data has been parsed before indexing using our XML Parser. The XML Parser extracts the sentences from the document. After parsing the documents, they are indexed using Lucene, an open source full text search tool.</p><p>Query Word Identification and Sentence Retrieval. After indexing has been done, the queries have to be processed to retrieve relevant sentences from the associated documents.</p><p>Each answer pattern or query is processed to identify the query words for submission to Lucene. Each hypothesis has been submitted to Lucene after removing stop words (using the stop word list<ref type="foot" coords="5,194.64,420.02,3.00,5.56" target="#foot_2">3</ref> ). The remaining words are identified as the query words. Query words may appear in inflected forms in the question. For English, standard Porter Stemming algorithm <ref type="foot" coords="5,163.44,443.06,3.00,5.56" target="#foot_3">4</ref> has been used to stem the query words. After searching using Lucene, a set of sentences in ranked order are retrieved. First of all, all query words are fired with AND operator. If at least one sentence is retrieved using the query with AND operator then the query is removed from the query list and need not be searched again. The rest of the queries are fired again with OR operator. OR searching retrieves at least one sentence for each query. Now, the top ranked relevant ten sentences for each query are considered for further processing In case of AND search only the top ranked sentence is considered. Sentence retrieval is the most crucial part of this system. We take only the top ranked relevant sentences assuming that these are the most relevant sentences in the associated document for the question from which the query has been generated.</p><p>Each retrieved sentence is considered as the Text (T) and is paired with each generated hypothesis (H). Each T-H pair identified for each answer option corresponding to a question is now assigned a score based on the NER module, Textual Entailment module, Chunking module, Syntactic Similarity module and Question Type module.</p><p>NER Module, It is based on the detection and matching of Named Entities (NEs) <ref type="bibr" coords="6,464.60,188.95,11.61,9.11" target="#b8">[9]</ref> in the Retrieved Sentence (T) -generated Hypothesis (H) pair. Once the NEs of the hypothesis and the text have been detected, the next step is to determine the number of NEs in the hypothesis that match in the corresponding retrieved sentence. The measure NE_Match is defined as NE_Match = number of common NEs between T and H/Number of NEs in Hypothesis.</p><p>If the value of NE_Match is 1, i.e., 100% of the NEs in the hypothesis match in the text, then the T-H pair is considered as an entailment. The T-H pair is assigned the value "1", otherwise, the pair is assigned the value "0".</p><p>Textual Entailment Module (TE), This TE module <ref type="bibr" coords="6,328.52,308.95,11.61,9.11" target="#b7">[8]</ref> is based on three types of matching, i.e., WordNet based Unigram Match and Bigram Match and Skip-bigram Match. a) WordNet based Unigram Match: In this method, the various unigrams in the hypothesis for each Retrieved Sentence (T) -generated Hypothesis (H) pair are checked for their presence in the retrieved text. WordNet synsets are identified for each of the unmatched unigrams in the hypothesis. If any synset for the H unigram match with any synset of a word in the T then the hypothesis unigram is considered as a successful WordNet based unigram match. If the value of Wordnet_Unigram_Match is 0.75 or more, i.e., 75% or more unigrams in the H match either directly or through WordNet synonyms, then the T-H pair is considered as an entailment. The T-H pair is then assigned the value "1", otherwise, the pair is assigned the value "0". b) Bigram Match: Each bigram in the hypothesis is searched for a match in the corresponding text part. The measure Bigram_Match is calculated as the fraction of the hypothesis bigrams that match in the corresponding text, i.e., Bigram_Match=(Total number of matched bigrams in a T-H pair /Number of hypothesis bigrams). If the value of Bigram_Match is 0.5 or more, i.e., 50% or more bigrams in the H match in the corresponding T, then the T-H pair is considered as an entailment. The T-H pair is then assigned the value "1", otherwise, the pair is assigned the value "0". c) Skip-grams: A skip-gram is any combination of n words in the order as they appear in a sentence, allowing arbitrary gaps. In the present work, only 1-skip-bigrams are considered where 1-skip-bigrams are bigrams with one word gap between two words in a sentence. The measure 1-skip_bigram_Match is defined as 1_skip_bigram_Match = skip_gram(T,H) / n, where skip_gram(T,H) refers to the number of common 1-skip-bigrams (pair of words in order with one word gap) found in T and H and n is the number of 1-skip-bigrams in the hypothesis H. If the value of 1_skip_bigram_Match is 0.5 or more, then the T-H pair is considered as an entailment. The text-hypothesis pair is then assigned the value "1", otherwise, the pair is assigned the value "0".</p><p>Chunk Module, The question sentences are pre-processed using Stanford dependency parser. The words along with their part of speech (POS) information are passed through a Conditional Random Field (CRF) based chunker <ref type="bibr" coords="7,322.75,173.11,16.61,9.11" target="#b10">[11]</ref> to extract phrase level chunks of the questions. A rule-based module is developed to identify the chunk boundaries. The question-retrieved text pairs that achieve the maximum weight are identified and the corresponding answers are tagged as "1". The question-retrieved text pair that receives a zero weight is tagged as "0". Syntactic Similarity Module, This module is based on the Stanford dependency parser <ref type="bibr" coords="7,473.29,246.79,10.59,9.11" target="#b8">[9]</ref>, which normalizes data from the corpus of text and hypothesis pairs, accomplishes the dependency analysis and creates appropriate structures.</p><p>Matching Module. After dependency relations are identified for both the retrieved sentence and the hypothesis in each pair, the hypothesis relations are compared with the retrieved text relations. The different features that are compared are noted below. In all the comparisons, a matching score of 1 is considered when the complete dependency relations along with all of its arguments match in both the retrieved sentence and the hypothesis. In case of a partial match for a dependency relation, a matching score of 0.5 is assumed. a. Subject-Verb Comparison: The system compares hypothesis subject and verb with retrieved sentence subject and verb that are identified through the nsubj and nsubjpass dependency relations. A matching score of 1 is assigned in case of a complete match. Otherwise, the system considers the following matching process. b. WordNet Based Subject-Verb Comparison: If the corresponding hypothesis and sentence subjects do match in the subject-verb comparison, but the verbs do not match, then the WordNet distance between the hypothesis and the sentence is compared. If the value of the WordNet distance is less than 0.5, indicating a closeness of the corresponding verbs, then a match is considered and a matching score of 0.5 is assigned. Otherwise, the subject-subject comparison process is applied. c. Subject-Subject Comparison: The system compares hypothesis subject with sentence subject. If a match is found, a score of 0.5 is assigned to the match. d. Object-Verb Comparison. The system compares hypothesis object and verb with retrieved sentence object and verb that are identified through dobj dependency relation. In case of a match, a matching score of 0.5 is assigned. e. WordNet Based Object-Verb Comparison: The system compares hypothesis object with text object. If a match is found then the verb corresponding to the hypothesis object with retrieved sentence object's verb is compared. If the two verbs do not match then the WordNet distance between the two verbs is calculated. If the value of WordNet distance is below 0.5 then a matching score of 0.5 is assigned. f. Cross Subject-Object Comparison: The system compares hypothesis subject and verb with retrieved sentence object and verb or hypothesis object and verb with retrieved sentence subject and verb. In case of a match, a matching score of 0.5 is assigned.</p><p>g. Number Comparison: The system compares numbers along with units in the hypothesis with similar numbers along with units in the retrieved sentence. Units are first compared and if they match then the corresponding numbers are compared. In case of a match, a matching score of 1 is assigned. h. Noun Comparison: The system compares hypothesis noun words with retrieved sentence noun words that are identified through nn dependency relation. In case of a match, a matching score of 1 is assigned. i. Prepositional Phrase Comparison:</p><p>The system compares the prepositional dependency relations in the hypothesis with the corresponding relations in the retrieved sentence and then checks for the noun words that are arguments of the relation. In case of a match, a matching score of 1 is assigned. j. Determiner Comparison: The system compares the determiner in the hypothesis and in the retrieved sentence that are identified through det relation. In case of a match, a matching score of 1 is assigned. k. Other relation Comparison: Besides the above relations that are compared, all other remaining relations are compared verbatim in the hypothesis and in the retrieved sentence. In case of a match, a matching score of 1 is assigned. API for WordNet Searching RiWordnet<ref type="foot" coords="8,298.08,345.86,3.00,5.56" target="#foot_4">5</ref> provides Java applications with the ability to retrieve data from the WordNet database.</p><p>Each of the matches through the above comparisons is assigned some weight.</p><p>Inference Score Module. In this module, we have got the weight from Named Entity Recognition (NER) Module, Textual Entailment (TE) Module, Question Type Analysis Module, Chunk Boundary and Syntactic Similarity Module. Each sentence in the associated document is assigned an inference score with respect to each (QT, OPTION T ) pair.</p><p>Answer Pattern Generation for Inference Score. Each question has five answer options and the task is to identify the best answer to the question from an associated document. Each question in the system is identified as the (question, document) pair represented as {q i , d_id} where i=1…5. There are 5 questions corresponding to each document. Each answer option is represented in the system as {d_id, q_id i , a_id j }, where, d_id=document id, q_id i = i th query, where i=1…5, a_id j = j th answer option, where j=1…5. Each query frame is defined in the system as (DOC, QT, OPTION T ) where, DOC= Give Document to be used for verifying answer options QT= Query Term, is a list of words after removing the stop words and interrogative word from the given question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OPTION T = T-th answer option</head><p>Scoring Assignment. This module takes query frame as input and returns score as output. The algorithm InferenceScore describes the scoring procedure. Algorithm InferenceScore (sentence, QT, OPTION T )</p><p>Step If (keywordmatched = = total keywords -1) then Score =1</p><p>Step 5: Return score</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>End</head><p>Answer Ranking Module. Now, for each given answer option a score is calculated and the answer option with highest score is taken as correct answer for the given query. The algorithm RankigAnswerOption describes the option selection procedure. For each sentence S i € Sentences and answer option q j € Q Where, j=1…5 A ji =AnswerScore(S i , QT, OPTION)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>End For</head><p>Step 3: [Assign score to each option] For answer pattern q j € Q AQ i =maximum evaluated score for {S 1 ,S 2 ,…..S n };</p><p>Where AQ i is the score of i th option End For</p><p>Step 4: [ Applying Matching Score(M score )] For each answer option AQ j € AQ AQ j =InferenceScore(AQ j ) + M score</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>End For</head><p>Step 5: [Select the answer option] correct_option= index of maximum AQ={ AQ 1, AQ 2, AQ 3, AQ 4, AQ 5 } END</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>The main measure used in this evaluation campaign is c@1, which is defined in equation 1.</p><p>(1) where, n R : the number of correctly answered questions, n U : number of unanswered questions n: the total number of questions Afterwards, these c@1 scores can be aggregated at topic and global levels in order to obtain the following values: § Median, average and standard deviation of c@1 scores at test level, grouped by topic, § Overall median, average and standard deviation of c@1 values at test level.</p><p>The median c@1 has been provided under the consideration that it can be more informative at reading level than average values. This is because median is less affected by outliers than average, and therefore, it offers more information about the ability of a system to understand a text.</p><p>This approach allows us to evaluate systems in a similar way to the manner new language learners are graded. Median: 0.00 -Average: 0.23 -Standard Deviation: 0.31 -calculated over c@1 of all 9 reading tests Topict_id='1'-EntranceExams Median: 0.28 -Average: 0.40 -Standard Deviation: 0.31 -calculated over the c@1 of the four reading tests -c@1 measure for reading-test r_id '1' = (0+4(0/5))/5 = 0.00 -c@1 measure for reading-test r_id '2' = (1+3(1/6))/6 = 0.25 -c@1 measure for reading-test r_id '3' = (1+1(1/5))/5 = 0.24 -c@1 measure for reading-test r_id '4' = (3+1(3/5))/5 = 0.72 -c@1 measure for reading-test r_id '5' = (1+2(1/5))/5 = 0.28 -c@1 measure for reading-test r_id '6' = (2+3(2/5))/5 = 0.64 -c@1 measure for reading-test r_id '7' = (0+4(0/5))/5 = 0.00 -c@1 measure for reading-test r_id '8' = (2+3(2/5))/5 = 0.64 -c@1 measure for reading-test r_id '9' = (3+2(3/5))/5 = 0.84</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation at question-answering level:</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>The question answering system has been developed as part of the participation in the QA4MRE pilot track as part of the CLEF 2013 evaluation campaign. The overall system has been evaluated using the evaluation metrics provided as part of the QA4MRE 2013 pilot track. It has been observed from evaluation results that our proposed model works very well on the reading test -4,6,8,9. And the system performs very poor to handle reading test-1,7. As the questions of type comparative have not been answered, it affects the evaluation results. But, the overall evaluation results are satisfactory. Future works will be motivated towards improving the performance of the system</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,250.47,606.14,111.03,9.28;3,125.28,258.96,361.20,332.88"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: System Architecture</figDesc><graphic coords="3,125.28,258.96,361.20,332.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,170.28,225.81,69.11,9.21;9,185.00,243.19,36.67,9.11;9,188.50,260.47,216.79,9.11;9,150.00,277.65,156.26,9.21;9,306.37,282.32,3.34,5.50;9,309.71,277.84,99.08,9.02;9,191.00,295.10,70.16,9.28;9,261.28,299.68,4.00,5.66;9,265.28,295.10,114.91,9.28;9,200.00,312.55,38.32,9.11;9,200.00,329.83,43.80,9.11;9,152.50,347.01,165.97,9.21;9,318.57,351.68,3.34,5.50;9,321.91,347.20,3.83,9.02;9,191.00,364.22,124.30,9.28;9,315.42,368.80,4.00,5.66;9,206.00,381.67,153.24,9.11;9,221.00,399.19,178.10,9.11;9,221.00,416.47,166.11,9.11;9,155.00,433.65,223.22,9.21"><head>1 : 1 goto step 5 Step 3 : 1 Step 4 :</head><label>115314</label><figDesc>/ count no of matched keyword Step 2: [Check whether (QT, OPTION T ) matches in a sentence] If (QT, OPTION T ) matches in a sentence then Score = [Check each keyword in OPTION T ] For each keyword in OPTION T If keyword matches in a sentence then Score = score + 1 / (number of keywords -1) Keywordmatched = keywordmatched + [Check whether all the keywords have matched]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="11,188.54,626.30,234.96,9.28;11,148.08,510.72,315.84,113.28"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Pie Chart Representation of Evaluation at QA level</figDesc><graphic coords="11,148.08,510.72,315.84,113.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,229.32,149.61,155.23,86.85"><head>Table 1 .</head><label>1</label><figDesc>Answered Question Type.</figDesc><table coords="5,229.32,172.89,155.23,63.57"><row><cell>Word/Phrase</cell><cell>Question Type</cell></row><row><cell>What happened</cell><cell>FACT</cell></row><row><cell>What did</cell><cell>REASON</cell></row><row><cell>What</cell><cell>OBJECT</cell></row><row><cell>How</cell><cell>WAY/MANNER</cell></row><row><cell>Why</cell><cell>REASON</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,192.10,184.65,227.79,9.39"><head>Table 2 .</head><label>2</label><figDesc>Algorithm InferenceScore (Sentence, QT, OPTION T )</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="10,151.92,149.61,254.18,79.65"><head>Table 3 .</head><label>3</label><figDesc>Algorithm SelectAnswerOption (Answer Set)</figDesc><table coords="10,151.92,172.70,244.30,56.56"><row><cell>Algorithm SelectAnswerOption(answer set)</cell></row><row><cell>Step 1: [Initialization]</cell></row><row><cell>correct_option= ∞ // not answered</cell></row><row><cell>Step 2: [Calculate score for each sentence]</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,129.81,625.44,138.58,8.22"><p>http://nlp.stanford.edu/ner/index.shtml</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,129.81,636.24,95.39,8.22"><p>http://www.bart-coref.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="5,129.81,625.44,158.55,8.22"><p>http://members.unine.ch/jacques.savoy/clef/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="5,132.06,636.24,179.65,8.22"><p>http://tartarus.org/~martin/PorterStemmer/java.txt</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="8,133.06,636.24,228.97,8.22"><p>http://www.rednoise.org/rita/wordnet/documentation/index.htm</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. We acknowledge the support of the <rs type="funder">DST India-CONACYT Mexico</rs> project "<rs type="projectName">Answer Validation through Textual Entailment</rs>" funded by <rs type="funder">DST, Government of India</rs> and the <rs type="funder">Department of Electronics and Information Technology (DeitY), Ministry of Communications &amp; Information Technology (MCIT), Government of India</rs> funded project "<rs type="projectName">Development of Cross Lingual Information Access (CLIA) System Phase II</rs>".</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_3Yj84jG">
					<orgName type="project" subtype="full">Answer Validation through Textual Entailment</orgName>
				</org>
				<org type="funded-project" xml:id="_JZudNaC">
					<orgName type="project" subtype="full">Development of Cross Lingual Information Access (CLIA) System Phase II</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="13,127.92,261.36,359.48,8.22;13,135.91,271.68,351.49,8.22;13,135.91,282.24,351.59,8.22;13,135.91,292.56,230.94,8.22" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="13,370.63,271.68,116.77,8.22;13,135.91,282.24,220.40,8.22">Overview of ResPubliQA 2009: Question Answering Evaluation over European Legislation</title>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pamela</forename><surname>Forner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Sutcliffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Corina</forename><surname>Forăscu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iñaki</forename><surname>Alegria</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicolas</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Petya</forename><surname>Osenova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,375.29,282.24,112.21,8.22;13,135.91,292.56,55.57,8.22">Working Notes for the CLEF 2009 Workshop</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-10-02">30 September-2 October, 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,127.92,302.88,359.32,8.22;13,135.91,313.44,351.61,8.22;13,135.91,323.76,351.52,8.22;13,135.91,334.08,20.22,8.22" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="13,166.03,313.44,321.49,8.22;13,135.91,323.76,39.15,8.22">Overview of ResPubliQA 2010: Question Answering Evaluation over European Legislation</title>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pamela</forename><surname>Forner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Sutcliffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Corina</forename><surname>Forăscu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cristina</forename><surname>Mota</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,193.56,323.76,172.10,8.22">Working Notes for the CLEF 2010 Workshop</title>
		<meeting><address><addrLine>Padua, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-09">September 2010</date>
			<biblScope unit="page" from="20" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,127.92,344.40,359.40,8.22;13,135.91,354.96,351.50,8.22;13,135.91,365.28,247.42,8.22" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="13,246.10,354.96,241.31,8.22;13,135.91,365.28,198.63,8.22">Overview of QA4MRE at CLEF 2011: Question Answering for Machine Reading Evaluation, Working Notes of CLEF</title>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pamela</forename><surname>Forner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Sutcliffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Corina</forename><surname>Forascu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Caroline</forename><surname>Sporleder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,127.92,375.60,359.57,8.22;13,135.91,386.16,137.98,8.22" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,307.44,375.60,157.30,8.22">Overview of the answer validation exercise</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Á</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,135.91,386.16,89.18,8.22">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2006">2006. 2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,127.92,396.48,359.56,8.22;13,135.91,406.80,103.73,8.22" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,265.71,396.48,164.21,8.22">Overview of the Answer Validation Exercise</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Á</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,455.54,396.48,31.93,8.22;13,135.91,406.80,54.93,8.22">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2007">2007. 2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,127.92,417.36,359.58,8.22;13,135.91,427.68,105.97,8.22" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,269.81,417.36,159.58,8.22">Overview of the answer validation exercise</title>
		<author>
			<persName coords=""><forename type="first">Á</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,455.56,417.36,31.94,8.22;13,135.91,427.68,54.95,8.22">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2008">2008. 2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,127.92,438.00,359.51,8.22;13,135.91,448.32,351.50,8.22;13,135.91,458.88,253.46,8.22" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,174.62,448.32,258.05,8.22">JU_CSE_TE: System Description QA@CLEF 2010 -ResPubliQA</title>
		<author>
			<persName coords=""><forename type="first">Partha</forename><surname>Pakray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pinaki</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Santanu</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dipankar</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sivaji</forename><surname>Bandyopadhyay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,442.24,448.32,45.16,8.22;13,135.91,458.88,196.17,8.22">CLEF 2010 Workshop on Multiple Language Question Answering</title>
		<meeting><address><addrLine>MLQA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,127.92,469.20,359.32,8.22;13,135.91,479.48,351.52,7.38;13,135.91,489.32,90.62,7.38" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,304.60,469.64,159.67,7.38">Answer Validation using Textual Entailment</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pakray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bandyopadhyay</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-19437-5_29</idno>
	</analytic>
	<monogr>
		<title level="m" coord="13,472.19,469.64,15.06,7.38;13,135.91,479.48,28.28,7.38">12th CICLing</title>
		<title level="s" coord="13,170.59,479.48,120.73,7.38">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2011">2011. 2011. 2011</date>
			<biblScope unit="volume">6609</biblScope>
			<biblScope unit="page" from="353" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,127.92,499.20,359.47,8.22;13,135.91,509.52,220.70,8.22" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,277.82,499.20,149.02,8.22">The Second Release of the RASP System</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Watson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,443.46,499.20,43.93,8.22;13,135.91,509.52,217.01,8.22">Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions</title>
		<meeting>the COLING/ACL 2006 Interactive Presentation Sessions</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.04,519.84,355.37,8.22;13,135.91,530.40,351.41,8.22;13,135.91,540.72,188.70,8.22" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,447.48,519.84,39.93,8.22;13,135.91,530.40,211.78,8.22">Generating Typed Dependency Parses from Phrase Structure Parses</title>
		<author>
			<persName coords=""><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,367.08,530.40,120.24,8.22;13,135.91,540.72,162.44,8.22">5th International Conference on Language Resources and Evaluation (LREC)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.04,551.04,317.55,8.22" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,205.35,551.04,157.09,8.22">CRFChunker: CRF English Phrase Chunker</title>
		<author>
			<persName coords=""><forename type="first">Xuan-Hieu</forename><surname>Phan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,368.87,551.04,31.92,8.22">PACLIC</title>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.04,561.60,355.36,8.22;13,135.91,571.92,351.28,8.22;13,135.91,582.24,351.46,8.22;13,135.91,592.80,23.97,8.22" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="13,417.64,561.60,69.76,8.22;13,135.91,571.92,273.85,8.22">A Hybrid Question Answering System based on Information Retrieval and Answer Validation</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pakray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bandyopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,429.68,571.92,57.51,8.22;13,135.91,582.24,300.89,8.22">the proceedings of Question Answering for Machine Reading Evaluation (QA4MRE) at CLEF 2011</title>
		<meeting><address><addrLine>Amsterdam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.04,603.12,355.43,8.22;13,135.91,613.44,351.51,8.22;13,135.91,623.76,278.99,8.22" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="13,455.03,603.12,32.44,8.22;13,135.91,613.44,168.43,8.22">Question Answering System for QA4MRE@CLEF2012</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pakray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bandyopadhyay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,326.45,613.44,160.97,8.22;13,135.91,623.76,201.70,8.22">the proceedings of Question Answering for Machine Reading Evaluation (QA4MRE) at CLEF 2012</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
