<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,145.56,152.87,304.13,12.58">Overview of QA4MRE 2013 Entrance Exams Task</title>
				<funder ref="#_v7Ex4wG #_SWuqXwX #_gTp3yFX">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,130.50,192.26,74.34,10.80"><forename type="first">Anselmo</forename><surname>Pe√±as</surname></persName>
							<email>anselmo@lsi.uned.es</email>
							<affiliation key="aff0">
								<orgName type="department">NLP&amp;IR group</orgName>
								<orgName type="institution">UNED</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,214.86,192.26,70.98,10.80"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
							<email>yusuke@nii.ac.jp</email>
							<affiliation key="aff1">
								<orgName type="institution">National Institute of Informatics</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,295.86,192.26,64.26,10.80"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
							<email>hovy@cmu.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,370.08,192.26,70.31,10.80"><forename type="first">Pamela</forename><surname>Forner</surname></persName>
							<email>forner@celct.it</email>
						</author>
						<author>
							<persName coords="1,261.12,206.06,69.00,10.80"><forename type="first">Noriko</forename><surname>Kando</surname></persName>
							<email>kando@nii.ac.jp</email>
							<affiliation key="aff1">
								<orgName type="institution">National Institute of Informatics</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,145.56,152.87,304.13,12.58">Overview of QA4MRE 2013 Entrance Exams Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B741E426828F56B74EC15E0B0EB3CB1C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the Question Answering for Machine Reading (QA4MRE) Entrance Exams at the 2013 Cross Language Evaluation Forum. The data set of this task is extracted from actual university entrance examinations as-is, and therefore includes a variety of topics in daily life. Another unique feature of the Entrance Exams task is that questions are designed originally for testing human examinees, rather than evaluating computer systems. Therefore, the data set is expected to have a natural distribution of human ability for reading and understanding texts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>The Entrance Exams task at CLEF 2013 QA4MRE is focused on solving Reading Comprehension tests of English examinations. Reading Comprehension tests are routinely used to assess the degree to which people comprehend what they read, so we work with the hypothesis that it is reasonable to use these tests to assess the degree to which a machine "comprehends" what it is reading.</p><p>In QA4MRE, tests are usually made in an artificial way by organizers, in order to test properly systems performance on a controlled set of question types and a defined level of inference.</p><p>In such scenarios, the question arises how the performance of systems on artificial tests compares to their performance when confronted with real human tests. We believe that finding a real benchmark able to test real systems performance over the time offers great value to assess real progress in the field along the future years.</p><p>With this goal in mind, CLEF and NTCIR started collaboration around the idea of testing systems against University Entrance Exams, the same exams humans have to pass to enter University. The data set was prepared and distributed by NTCIR, while other organization ef-forts, including announcements, collecting and evaluating submissions, etc. were managed by CLEF. This style of the organization reduced the workload of each side, since the NTCIR side is already familiar with the contents of the data and its copyright issues, while the CLEF side has already established other organization processes such as submission management and evaluation. The success of this coordination also owes to the standard data format and evaluation methodology, which were also adopted for this pilot task. The next round of this task is expected to be organized in a similar manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">TASK DESCRIPTION</head><p>The form of the task is essentially the same as the QA4MRE Main Task. Participant systems are asked to read a given document and answer questions. Questions are given in multiple-choice format, with several options from which a single answer must be selected.</p><p>A crucial difference from the other QA4MRE tasks is that background text collections are not provided. Systems have to answer questions by referring to "common sense knowledge" that high school students who aim to enter the university are expected to have. Another important difference is that we do not intend to restrict question types. Any types of reading comprehension questions in real entrance exams will be included in the test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DATA</head><p>Japanese University Entrance Exams include questions formulated at various levels of complexity and test a wide range of capabilities. The challenge of "Entrance Exams" aims at evaluating systems under the same conditions that humans are evaluated to enter the University. In this first campaign we reduced the challenge to Reading Comprehension exercises contained in the English exams. The data set is extracted from standardized English examinations for university admission in Japan. Exams are created by the Japanese National Center for University Admissions Tests.</p><p>Original examinations include various styles of questions, such as word filling, grammatical error recognition, sentence filling, etc.</p><p>One of such styles is reading comprehension; a test provides a text that describes some daily life situation, and questions about the text are asked. Since this type of questions is suitable for the QA4MRE lab, we extracted questions of this type automatically from XML files of the examination data, and converted the XML annotations to fit the standard format of QA4MRE.</p><p>For each examination, one text is given, and five questions on the given text are asked. Each question has four choices. For this year campaign, we selected 10 examinations, one of which was delivered as development data while the others were provided as final test data. That is, we provided 9 documents, 46 questions<ref type="foot" coords="3,328.74,257.89,3.99,7.18" target="#foot_0">1</ref> and 184 choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION</head><p>Scoring of the output produced by participant systems was performed automatically by comparing the answers of systems against the gold standard collection with annotations made by humans. No manual assessment was performed. Each test receives an evaluation score between 0 and 1 using c@1 <ref type="bibr" coords="3,124.68,388.40,12.75,10.80" target="#b0">[1]</ref>. This measure, used in previous CLEF QA Tracks, encourages systems to reduce the number of incorrect answers while maintaining the number of correct ones by leaving some questions unanswered. Systems received evaluation scores from two different perspectives:</p><p>1. At the question-answering level: correct answers are counted individually without grouping them 2. At the reading-test level: figures both for each reading test as a whole are given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>During registration, 27 different groups showed interest in the task. Out of them, 10 groups fulfilled the data agreements, and finally, only 5 teams submitted runs. Despite their interest in the task, some groups expressed that the difficulty of the tests exceeded the current state of the art in the field and decided not to participate. Table <ref type="table" coords="3,405.86,618.80,6.00,10.80" target="#tab_0">1</ref> enumerates the participating groups and their reference paper in CLEF 2013 Working Notes. Results are summarized in Tables <ref type="table" coords="4,293.64,264.56,6.00,10.80" target="#tab_1">2</ref> and<ref type="table" coords="4,324.55,264.56,6.00,10.80" target="#tab_2">3</ref> for the QA and for Reading perspectives respectively. According to Table <ref type="table" coords="4,348.53,278.36,4.50,10.80" target="#tab_1">2</ref>, the system with higher score (jucs <ref type="bibr" coords="4,180.40,292.10,13.51,10.80" target="#b2">[3]</ref>) is the one that answered incorrectly less questions. It is also the unique system that answered more questions correctly than incorrectly, finding a better balance with leaving some questions unanswered. This indicates that their modules to detect whether they have enough evidence about the correctness of the answer are working pretty well. Table <ref type="table" coords="4,183.65,585.68,6.00,10.80" target="#tab_2">3</ref> shows results under the reading perspective. First column corresponds to systems run id, second column to the overall c@1 obtained, third column shows the number of tests that the systems have passed if we consider the threshold of 0.5, and the rest of columns correspond to the c@1 value for each particular test. 0.25 0/9 0.24 0.39 0.28 0.28 0.24 0.28 0.32 0.00 0.00 RANDOM 0.25 -0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 NIIJ-2 0.24 1/9 0.20 0.67 0.40 0.20 0.00 0.00 0.20 0.00 0.40 lims-cnrs-1 0.24 0/9 0.40 0.17 0.20 0.00 0.20 0.40 0.40 0.20 0.40 MEDIAN 0.24 -0.21 0.32 0.28 0.20 0.10 0.34 0.26 0.10 0.24 NIIJ-1 0.22 0/9 0.28 0.58 0.28 0.28 0.00 0.00 0.00 0.00 0.28 nara 0.22 1/9 0.22 0.00 0.40 0.20 0.00 0.20 0.20 0.60 0.20 lims-cnrs-2 0.20 0/9 0.20 0.17 0.00 0.20 0.40 0.40 0.40 0.00 0.00 cmu 0.10 0/9 0.00 0.19 0.00 0.00 0.00 0.00 0.20 0.00 0.40 JUCS <ref type="bibr" coords="5,156.08,374.96,14.00,10.80" target="#b2">[3]</ref> report very good results using a system based on Textual Entailment and answer ranking. One particularity of this system is that it only answered 23 questions out of the 46. From these 13 were right and 10 wrong. This strategy is rewarded by c@1, since that provides partial credit when no answer is given instead of an incorrect one. It is worth noticing the difference in score among different tests. In particular, authors report that the difference depends on the type of questions of tests 1 and 7.</p><p>The NIIJ system [2] also performed above average and random baseline. It is also based on Textual Entailment after combining relevant sentences, questions, and answers. In their case, the best run answered all questions, being 16 correct answers and 30 incorrect ones.</p><p>Results also show that systems based only on statistical analysis of words alone can't perform the kind of inferences required to solve the tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>The dataset together with results suggest something very interesting: the need to develop strategies to reject answers more than strategies to accept answers. In one hand, the dataset shows that in some cases, the way to select the correct answer is by discarding the other candidates.</p><p>In the other hand, most systems still select more incorrect answers than correct ones, while a measure of progress in systems development is, precisely, the reduction in selecting wrong answers. The Entrance Exams task shows that Question Answering is a task far from being solved. This is true even for the simplified scenario where only one text is given and a set of options are provided as candidate answers to the question.</p><p>Results also show that systems based only on statistical analysis of words alone can't perform the kind of inferences required to solve the tests. In other words, that systems based only on textual similarity can't address the challenge.</p><p>Finally, we think that Entrance Exams provides a real benchmark able to assess real progress in the field along future years.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,124.68,150.09,328.08,97.18"><head>Table 1 .</head><label>1</label><figDesc>Participants and reference papers</figDesc><table coords="4,124.68,167.22,328.08,80.06"><row><cell>NIIJ</cell><cell>National Institute of Informatics, Japan</cell><cell>Li et al. 2013 [2]</cell></row><row><cell>JUCS</cell><cell>Jadavpur University, India</cell><cell>Banerjee et al. 2013 [3]</cell></row><row><cell>NARA</cell><cell>Nara Institute of Science and Technology,</cell><cell>Arthur et al. 2013 [4]</cell></row><row><cell></cell><cell>Japan</cell><cell></cell></row><row><cell>CMU</cell><cell>Carnegie Mellon University, United States</cell><cell>-</cell></row><row><cell>LIMS-</cell><cell>ILES -LIMSI, France</cell><cell>-</cell></row><row><cell>CNRS</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,126.06,386.73,315.26,181.66"><head>Table 2 .</head><label>2</label><figDesc>Overall results for all runs, QA perspective</figDesc><table coords="4,126.06,404.04,315.26,164.36"><row><cell cols="2">RUN NAME C@1</cell><cell></cell><cell># of questions ANSWERED</cell><cell></cell><cell># of questions UNANSWERED</cell></row><row><cell></cell><cell></cell><cell cols="3">RIGHT WRONG Total</cell><cell></cell></row><row><cell>jucs</cell><cell>0.42</cell><cell>13</cell><cell>10</cell><cell>23</cell><cell>23</cell></row><row><cell>NIIJ-3</cell><cell>035</cell><cell>16</cell><cell>30</cell><cell>46</cell><cell>0</cell></row><row><cell>NIIJ-5</cell><cell>0.33</cell><cell>15</cell><cell>31</cell><cell>46</cell><cell>0</cell></row><row><cell>NIIJ-4</cell><cell>0.25</cell><cell>8</cell><cell>19</cell><cell>27</cell><cell>19</cell></row><row><cell>Random</cell><cell>0.25</cell><cell>12</cell><cell>34</cell><cell>46</cell><cell>0</cell></row><row><cell>NIIJ-2</cell><cell>0.24</cell><cell>11</cell><cell>35</cell><cell>46</cell><cell>0</cell></row><row><cell cols="2">lims-cnrs-1 0.24</cell><cell>11</cell><cell>35</cell><cell>46</cell><cell>0</cell></row><row><cell>NIIJ-1</cell><cell>0.22</cell><cell>7</cell><cell>17</cell><cell>24</cell><cell>22</cell></row><row><cell>nara</cell><cell>0.22</cell><cell>10</cell><cell>35</cell><cell>45</cell><cell>1</cell></row><row><cell cols="2">lims-cnrs-2 0.20</cell><cell>9</cell><cell>37</cell><cell>46</cell><cell>0</cell></row><row><cell>cmu</cell><cell>0.10</cell><cell>4</cell><cell>33</cell><cell>37</cell><cell>9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,124.68,150.09,346.01,99.12"><head>Table 3 .</head><label>3</label><figDesc>Overall results for all runs, reading perspective</figDesc><table coords="5,124.68,166.89,346.01,82.32"><row><cell>Run</cell><cell>Over-</cell><cell>Pass T1</cell><cell>T2</cell><cell>T3</cell><cell>T4</cell><cell>T5</cell><cell>T6</cell><cell>T7</cell><cell>T8</cell><cell>T9</cell></row><row><cell></cell><cell>all</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>jucs</cell><cell>0.42</cell><cell cols="9">4/9 0.00 0.25 0.24 0.72 0.28 0.64 0.00 0.64 0.84</cell></row><row><cell>NIIJ-3</cell><cell>0.35</cell><cell cols="9">3/9 0.40 0.50 0.60 0.20 0.20 0.40 0.60 0.20 0.00</cell></row><row><cell>NIIJ-5</cell><cell>0.33</cell><cell cols="9">2/9 0.20 0.67 0.40 0.20 0.00 0.40 0.60 0.20 0.20</cell></row><row><cell>AVERAGE</cell><cell>0.26</cell><cell cols="9">-0.21 0.36 0.28 0.23 0.13 0.27 0.29 0.18 0.27</cell></row><row><cell>NIIJ-4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,129.96,686.19,247.86,8.10"><p>One test document was accompanied with 6 questions exceptionally.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENTS</head><p>The collaboration has been developed in the framework of <rs type="projectName">Todai Robot Project in Japan</rs>, and the <rs type="projectName">CHIST-ERA Readers project in Europe</rs>. The <rs type="projectName">Todai Robot Project</rs> is a grand challenge headed by NII, and aims to develop an end-to-end AI system that can solve real entrance examinations of universities in Japan integrating heterogeneous AI technologies, such as natural language processing, situation understanding, math formula processing or vision processing.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_v7Ex4wG">
					<orgName type="project" subtype="full">Todai Robot Project in Japan</orgName>
				</org>
				<org type="funded-project" xml:id="_SWuqXwX">
					<orgName type="project" subtype="full">CHIST-ERA Readers project in Europe</orgName>
				</org>
				<org type="funded-project" xml:id="_gTp3yFX">
					<orgName type="project" subtype="full">Todai Robot Project</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,128.45,502.40,331.43,10.80;6,142.68,516.20,292.26,10.80;6,142.68,530.06,300.04,10.80;6,142.68,543.80,298.68,10.80" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,322.00,502.40,137.88,10.80;6,142.68,516.20,64.30,10.80">A Simple Measure to Assess Non-response</title>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Pe√±as</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alvaro</forename><surname>Rodrigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,228.33,516.20,206.61,10.80;6,142.68,530.06,300.04,10.80;6,142.68,543.80,147.85,10.80">Proceedings of 49th Annual Meeting of the Association for Computational Linguistics -Human Language Technologies (ACL-HLT 2011)</title>
		<meeting>49th Annual Meeting of the Association for Computational Linguistics -Human Language Technologies (ACL-HLT 2011)<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,129.18,557.60,340.45,10.80;6,142.68,571.40,290.60,10.80;6,142.68,585.20,216.32,10.80" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,185.35,571.40,247.92,10.80;6,142.68,585.20,44.80,10.80">Question Answering System for Entrance Exams in QA4MRE</title>
		<author>
			<persName coords=""><forename type="first">Xinjian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tian</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">T</forename><surname>Ngan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yusuke</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Akiko</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,198.00,585.20,128.98,10.80">CLEF 2013 Working Notes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,129.18,599.00,287.79,10.80;6,142.68,612.80,326.42,10.80;6,142.68,626.60,318.67,10.80;6,142.68,640.40,101.66,10.80" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,344.38,612.80,124.72,10.80;6,142.68,626.60,254.14,10.80">Multiple Choice Question (MCQ) Answering System for Entrance Examination</title>
		<author>
			<persName coords=""><forename type="first">Somnath</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pinaki</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Partha</forename><surname>Pakray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sivaji</forename><surname>Bandyopadhyay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,405.06,626.60,56.29,10.80;6,142.68,640.40,69.62,10.80">CLEF 2013 Working Notes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,129.18,654.20,325.72,10.80;6,142.68,668.00,325.64,10.80;6,142.68,681.80,161.00,10.80" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,236.96,668.00,226.10,10.80">NAIST at the CLEF 2013 QA4MRE Pilot Task</title>
		<author>
			<persName coords=""><forename type="first">Philip</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sakriani</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tomoki</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,142.68,681.80,128.98,10.80">CLEF 2013 Working Notes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
