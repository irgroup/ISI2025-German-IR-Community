<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,135.50,116.95,344.34,12.62;1,263.10,134.89,89.15,12.62">Question Answering System for Entrance Exams in QA4MRE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,141.40,172.58,43.26,8.74"><forename type="first">Xinjian</forename><surname>Li</surname></persName>
							<email>lixinjian@nii.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Informatics</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,191.90,172.58,39.26,8.74"><forename type="first">Tian</forename><surname>Ran</surname></persName>
							<email>tianran@nii.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Informatics</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,239.65,172.58,79.18,8.74"><forename type="first">Ngan</forename><forename type="middle">L T</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Informatics</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,327.24,172.58,59.06,8.74"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
							<email>yusuke@nii.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Informatics</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,413.92,172.58,60.07,8.74"><forename type="first">Akiko</forename><surname>Aizawa</surname></persName>
							<email>aizawa@nii.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Informatics</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,135.50,116.95,344.34,12.62;1,263.10,134.89,89.15,12.62">Question Answering System for Entrance Exams in QA4MRE</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">615480D458D7DC20CC033691FBB175CA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Question Answering Systems</term>
					<term>Coreference Resolution</term>
					<term>Recognizing Textual Entailment</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our question answering system for Entrance Exams, which is a pilot task of the Question Answering for Machine Reading Evaluation at Conference and Labs of the Evaluation Forum (CLEF) 2013. We conducted experiments in which participants were provided with documents and multiple-choice questions. Their goals was to select one answer or leave it unanswered for each question. In our system, we developed a component to detect all story characters in the documents and tag all personal pronouns using coreference resolution. For each question, we extracted related sentences and combined them with candidate answers to create inputs for a Recognizing Textual Entailment (RTE) component. The answers were then selected based on the confidence scores from the Recognizing Textual Entailment component. We submitted five runs in the task and the run that ranked highest obtained a c@1 score of 0.35, which outperformed the baseline c@1 score of 0.25.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Question Answering for Machine Reading Evaluation (QA4MRE) is a lab that has been offered in Conference and Labs of the Evaluation Forum (CLEF) since 2011. It is an exercise to develop a methodology for evaluating machine reading systems through Question Answering and Reading Comprehension Tests <ref type="bibr" coords="1,467.31,525.59,9.96,8.74" target="#b0">[1]</ref>. In the lab, participants are provided with several documents and questions, the answer for each question is to be stated or implied in the document. The goal for participants is to develop a system to extract corresponding knowledge from the documents, thereby solving questions with them. The main task is composed of four topics, Aids, Climate Change, Music and Society, and Alzheimer. Besides the main task, QA4MRE at CLEF 2013 also offers two pilot tasks; Machine Reading of Biomedical Texts about Alzheimer's Disease and Entrance Exams. Our discussion mainly focuses on our participation in Entrance Exams.</p><p>Entrances Exams is a new task for evaluating machine reading systems by solving problems from Japanese university entrance exams. Similar tasks using Japanese entrance exams were also held in NTCIR RITE <ref type="bibr" coords="1,388.50,657.12,9.96,8.74" target="#b3">[4]</ref>. The challenge for this task is to test systems in the same situation in which high school students are evaluated. While a diverse range of background knowledge, such as Wikipedia entries, are available to all participants in other tasks, no external sources are provided in this task; therefore, systems are expected to make use of a high-school level of common sense. Only reading comprehension exercises were included in QA4MRE at CLEF 2013. Other types of exercises will be used in the future.</p><p>The rest of this paper is composed as follows: Section 2 describes the details of system's architecture. Section3 presents the results of our participation and their evaluations, and Section 4 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Architecture</head><p>In this section, we describe the detailed architecture of our system in the Entrance Exams task. Although the task was offered for the first time in the lab, the basic approaches do not significantly differ from other tasks. We referred to studies attending other tasks in previous QA4MRE tasks to develop our system <ref type="bibr" coords="2,134.76,313.13,10.20,8.74" target="#b1">[2]</ref>[3]. Our system consists of three components, as shown in Figure <ref type="figure" coords="2,432.10,313.13,3.87,8.74" target="#fig_0">1</ref>. The first component is the Character Resolver, which detects all story characters appearing in the documents and applies coreference resolution to personal pronouns. The second component is the Sentence Extractor. Questions are classified into several types in this component then related sentences are extracted for each question from the document. The last component is Recognizing Textual Entailment (RTE), in which we calculate the most likely answer for each question. The following subsections describe the details of these three components. Besides these three main components, a component was also developed to process documents and questions with a parser <ref type="bibr" coords="3,353.94,191.73,9.96,8.74" target="#b4">[5]</ref>. Since this is just a simple preprocessor, we do not discuss it in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Character Resolver</head><p>Given an actual exam-data from QA4MRE, we found the documents for entrance examinations were mainly composed of stories and related questions. We also found that most of the questions were connected with actions of story characters in these documents. To answer each question correctly, therefore, it is important to detect which character is responsible for the focused upon action. The Character Resolver was developed to detect all story characters and mentions of them in the documents, including coreferential mentions. To achieve this, we divided the task into two small processes as shown in the following pseudo code. The first one is for detecting all nouns for characters and to merge them into groups if their mentioned characters are identical. In the second process, we unify personal pronouns into the same person groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pseudo Code for Character Resolver</head><p>Initialize personDict to an empty hash table <ref type="table" coords="3,134.76,442.64,52.31,8.30">Initialize</ref>  In Process 1, we combine a prepared words list with the Name Entity Recognizer from Stanford <ref type="bibr" coords="4,234.71,286.88,9.96,8.74" target="#b5">[6]</ref>. First, the documents are divided into sentences and each sentence is divided into words. Then we use the Name Entity Recognizer to find proper names in the documents. Since the Name Entity Recognizer only detects nouns that seem to be names, it fails to detect common nouns that indicate general characters such as "teachers" and "mother". To address this problem, we prepare a list consisting of character reference nouns and search the documents to determine whether these words appear. Each word or name in the text is then assigned a person ID for ease of management. In most cases, several detected nouns might denote the same person, which would cause bad influence in the Recognizing Textual Entailment component. Therefore, we also clustered some words with the same meaning such as "father" and "dad". For example, "mother" and "mom" in the following text are tagged as the same person.</p><p>Her &lt;coref id="2"&gt; mother &lt;/coref&gt; must have heard the front door close. &lt;coref id="1"&gt; Christine &lt;/coref&gt; went in and sat on the sofa. "How was your exam, dear?", her &lt;coref id="2"&gt; mom &lt;/coref&gt; asked.</p><p>After character detection, we classify personal pronouns in the documents and questions. We develop a tool for this task by just following simple rules such as plural and male/female. The most useful feature of this tool for this task is that we can to some extent successfully tag words like "I" and "you" from conversations in the text. General coreference resolution tools do not perform well in this respect, but this feature is important in the Entrance Exams task because conversations might occupy a large percentage of the text and it would be useful to tag these pronoun words. After this application, we generate a tagged text like the following conversation.</p><p>The &lt;coref id="3"&gt; professor &lt;/coref&gt; looked at &lt;coref id="2"&gt; me &lt;/coref&gt; and smiled. "Oh! &lt;coref id="2"&gt; You &lt;/coref&gt; speak Japanese very well."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sentence Extractor</head><p>Since the final goal with our system is to select the correct answer based on the output from the Recognizing Textual Entailment component. Both Text and Hypothesis should be generated as input to it. The "Text entails Hypothesis" means that a person would determine the Hypothesis as almost correct after reading the Text. In this component, we extract related sentences from documents. This is achieved by giving sentences relevant scores for each question. Sentences ranked highest are those we should extract.</p><p>Before the extraction process, all questions are classified into six types based on the interrogative as the following table shows. The classification indicates which sentences we should extract. We can prepare a list of keywords for each question type. The sentences that contain these expressions are more likely the sentences we need. For instance, related sentences corresponding to "Why" questions could possibly include a word such as "because". Preparing all expressions is impossible, so we mainly depend on the Name Entity Recognizer to find expressions for LOCATION, PERSON and DATE. We also does not prepare a list for questions such as "What" questions, because it is too vague to prepare a list of relevant words. For each keyword we add five point to the relevant score of sentences containing them. Then, we measure the similarity between questions and sentences. First, each word in documents and questions is assigned a tf-idf weight which we calculated from the Wikipedia corpus in advance. Second, the following features are combined to add as the relevant score.</p><p>a Word similarity. For each sentence, we calculate its similarity with a question. This similarity is determined by word similarity using WordNet. This feature would add at most 20 points to each sentence. b Dependency similarity. In the preprocess for documents and questions, dependencies are also generated by the parser <ref type="bibr" coords="5,362.91,633.21,9.96,8.74" target="#b7">[8]</ref>. One point is added as the relevant score if the same dependency appears in both a sentence and a question.</p><p>c Character reference. Every story character in the documents is assigned an ID number in the previous component. We add five points to sentences if the same person is referred.</p><p>With scores computed, all sentences are ranked for each question and the sentence that has the highest score is extracted. Except for the highest one, the sentences surrounding the highest one might also contain important information for the question. Therefore, we extracted five sentences in our experiments, the highest one and four sentences surrounding it. These sentences were used as the Text in our Recognizing Textual Entailment system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Recognizing Textual Entailment</head><p>Our Recognizing Textual Entailment component uses semantically annotated dependency parses of sentences as logical representations, utilizing an inference engine to perform logical inferences on such representation. The knowledge it uses in the inference process is obtained from synonym/antonym/hypernym relations in WordNet. Furthermore, it also uses an abduction component to generate alignments between small pieces of the Text/Hypothesis pair (T/H pair), which corresponds to may-be-missing knowledge that can make the logical inference process go further. These alignments are selected and evaluated by some rough similarity measure, for which we chose the distributional similarity calculated from the Google n-gram corpus. A final score for each T/H pair is given by a classifier which uses the evaluation of alignments and their contribution to the inference process as features, together with shallow ones such as word overlap. The classifier is trained on the PASCAL RTE dataset.</p><p>With the Text obtained in the previous component, we generated four T/H pairs for each question. The Hypothesis can be extracted easily from the candidate answers. These pairs are inputs to our Recognizing Textual Entailment component. We selected the highest pair as the output for our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results and Evaluations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Test Set and Evaluation Measure</head><p>The test data for evaluation in the Entrance Exams shared task were taken from the Japanese university entrance examination. Nine test documents were provided in the task. Eight contained five questions each, and the other contained six questions. Four candidate choices were shown for each question. The following is an example question with its four answer options.</p><p>-Question. Where did the author's mother sit when one of her children was away? -Answer 1. She didn't change her chair. -Answer 2. She moved her own chair next to Dad's. -Answer 3. She moved to an empty chair on the side.</p><p>-Answer 4. She sat opposite to Dad.</p><p>The task evaluates each participating system by giving them a score between 0 and 1. The measure is called c@1 and was used in previous QA4MRE tasks. Systems might obtain higher scores if they leave questions unanswered when they may possibly be wrong. The measure is defined as follows.</p><formula xml:id="formula_0" coords="7,259.43,193.61,221.16,22.31">C@1 = 1 n (n r + n u n r n )<label>(1)</label></formula><p>where:</p><p>n: the total number of questions n r : the number of correctly answered questions n u : the number of unanswered questions</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>We submitted five runs, NII1 to NII5, in the task. NII1, NII3 and NII5 answered all given questions, while NII2 and NII4 left several questions unanswered based on the score from the Recognizing Textual Entailment component. NII1 and NII2 generated the Text by using the Sentence Extractor, but NII3 and NII4 generated them just by using all the documents. NII5 combined results from NII1 and NII3. The evaluation results are listed in Table <ref type="table" coords="7,385.72,361.75,3.87,8.74" target="#tab_2">2</ref>. From this table, NII3 had the highest score. It answered all 46 questions and 16 of them were answered correctly. Except for NII3 and NII5, other runs had scores similar to the random baseline, which is a 0.25 C@1 score. Besides the main C@1 score, we also applied McNemar's test to find whether the difference between baseline and NII3 or NII5 runs would be regarded as significant enough, but we found the p-value was still not sufficient in this case. We expected NII1 or NII5 to have the best scores before the evaluation since we expected the Sentence Extractor to remove noise in the documents and contribute to accuracy of the Recognizing Textual Entailment component. It turned out that the Sentence Extractor component became bottlenecked in NII1. Compared with NII1, NII3 acquired detailed information from the entire document, which resulted in a good c@1 score. We analyze error reasons for the Sentence Extractor in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Error Analysis</head><p>We carried out an error analysis for our system. We described in the previous section that our system architecture consists of three components, and each one possibly affected the final results. The Character Resolver performed well because most mentions for story characters seemed to have been tagged correctly. The main reason is that there were few story characters in the documents, so it was easy to tag them even just based on simple features. The major causes for errors in this task came from the Sentence Extractor component and Recognizing Textual Entailment component. Two kinds of errors need to be taken into account regarding the Sentence Extractor. The first type is that the component extracted wrong sentences because there was not enough information in the question. One example is shown as follows.</p><p>-Question: What was the purpose of the wooden cat? -Extracted sentences: When the touch was repeated a moment later, she whispered to her husband about it , but he only replied that people do not bother you at the opera on purpose. (and other four sentences)</p><p>-Correct sentences: Thieves had stolen the jewels and were going to pass them over to a woman. In order to identify her, they placed the cat in the booth and told her to pick it up.</p><p>In the example, very few words are contained in the question. As stated in the previous section, our approach extracts sentences mainly depending on the similarity between sentences and questions, so we give the extracted sentences a very high score if they contain the same words appearing in the question, i.e., "purpose". In this case, the correct sentences also contains the same word as in the question, i.e., "cat". Since we give each word a tf-idf weight, a common word like cat contributes very little here. Half of the Sentence Extractor errors belong to this type. If we can deal with abstract words such as "purpose", we might obtain better results.</p><p>The other error type of the Sentence Extractor is the extracted sentences were not sufficient to answer the question, since we restricted the component to only extract five sentences. This error occurs when the number of relevant sentences is more than five sentences. An example of such cases is shown below. For this example, our system extracted some of the necessary sentences, but the questions asked about the information which corresponded to a larger range.</p><p>We failed to acquire all necessary information in similar cases, which might have reduced the accuracy for the Recognizing Textual Entailment component. These two reasons degraded the performance of our NII1 and NII2.</p><p>The error for the Recognizing Textual Entailment component is a common problem. Even in the recent workshop <ref type="bibr" coords="9,308.80,167.82,9.96,8.74" target="#b8">[9]</ref>, a state-of-the-art system could not solve difficult questions. As described in the introduction section, the Entrance Exams task requires a high level of common sense, which high school students are expected to have, but our component has not reached such a level. The following is an example.</p><p>-Text: Our plane would have to fly hundreds of miles out of our way to get around it. If we flew through the cloud, the engines might get full of ash and stop.</p><p>-Hypothesis: The pilot had to fly off the regular course for the sake of safety.</p><p>A human would easily come to the conclusion that the hypothesis is correct, but when it comes to the Recognizing Textual Entailment component, it seems that it is difficult to solve this kind of problems. This also reduced the accuracy of our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We described our system that was used in the Entrance Exams task of QA4MRE CLEF 2013. Our system consists of three components, Character Resolver, Sentence Extractor and Recognizing Textual Entailment. In our developed system, the documents are processed by the Character Resolver to tag each story character an ID. The Sentence Extractor then extracts related sentences for each question and creates a Hypothesis and Text. Finally it inputs this T/H pair into the Recognizing Textual Entailment system to select an answer.</p><p>The best run of our system was NII3, which obtained 0.35 in c@1 score. This run used the entire document text as a Text, which helped to collect useful information for our Recognizing Textual Entailment component. The errors resulting from the Sentence Extractor and Recognizing Textual Entailment components negatively affected the accuracy of our system. Mitigating the limitations in these two components will be our future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,251.06,613.46,113.24,7.89;2,137.61,343.57,340.16,255.12"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. System Architecture</figDesc><graphic coords="2,137.61,343.57,340.16,255.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,140.99,554.47,302.95,8.77;8,140.99,577.96,339.62,8.77;8,151.70,589.94,256.07,8.74;8,140.99,613.40,339.61,8.77;8,151.70,625.35,236.69,8.77"><head>-</head><label></label><figDesc>Question: What happened when the author used a cash machine? -Extracted sentences: When I first tried to use a cash machine in a bank, I had an unpleasant experience. (and four other sentences) -Correct sentences: When I first tried to use a cash machine in a bank, I had an unpleasant experience. (and 7 other sentences)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,187.43,369.93,240.49,96.81"><head>Table 1 .</head><label>1</label><figDesc>Question Types</figDesc><table coords="5,187.43,390.73,240.49,76.01"><row><cell>Types</cell><cell>Interrogatives</cell><cell>Relevant expressions</cell></row><row><cell>OBJECT</cell><cell>What, Which</cell><cell>N/A</cell></row><row><cell cols="3">LOCATION Where, Which location city, country, village ...</cell></row><row><cell>PERSON</cell><cell>Who, Which person</cell><cell>Tom, James ...</cell></row><row><cell>DATE</cell><cell>When, What time</cell><cell>day, year ...</cell></row><row><cell cols="2">REASON Why</cell><cell>because of, due to ...</cell></row><row><cell>Others</cell><cell>How ...</cell><cell>N/A</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,174.86,389.88,265.64,85.45"><head>Table 2 .</head><label>2</label><figDesc>System Results</figDesc><table coords="7,174.86,410.69,265.64,64.64"><row><cell cols="5">System Correctly Answered Incorrect Answered Unanswered c@1</cell></row><row><cell>NII1</cell><cell>11</cell><cell>35</cell><cell>0</cell><cell>0.24</cell></row><row><cell>NII2</cell><cell>7</cell><cell>17</cell><cell>22</cell><cell>0.22</cell></row><row><cell>NII3</cell><cell>16</cell><cell>30</cell><cell>0</cell><cell>0.35</cell></row><row><cell>NII4</cell><cell>8</cell><cell>19</cell><cell>19</cell><cell>0.25</cell></row><row><cell>NII5</cell><cell>15</cell><cell>31</cell><cell>0</cell><cell>0.33</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,138.34,600.97,342.24,10.12;9,146.91,614.19,333.68,7.86;9,146.91,625.15,204.50,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,191.09,614.19,289.50,7.86;9,146.91,625.15,72.59,7.86">Overview of QA4MRE at CLEF 2011: Question answering for machine reading evaluation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Forner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Á</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sutcliffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Forascu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sporleder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,226.55,625.15,95.34,7.86">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.34,635.88,342.23,7.86;9,146.91,646.84,333.66,7.86;9,146.91,657.80,168.00,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,238.47,646.84,198.86,7.86">Question answering system for qa4mre@ clef 2012</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pakray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bandyopadhyay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,455.88,646.84,24.70,7.86;9,146.91,657.80,163.35,7.86">CLEF (Online Working Notes/Labs/Workshop)</title>
		<imprint>
			<date type="published" when="2012-09">2012, September</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.34,120.67,342.24,7.86;10,146.91,131.63,333.68,7.86;10,146.91,142.59,306.86,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,151.63,131.63,328.95,7.86;10,146.91,142.59,78.10,7.86">Enhancing a Question Answering System with Textual Entailment for Machine Reading Evaluation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Iftene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A-L</forename><surname>Gînsca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Moruz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Trandabat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Husarciuc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Boros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,236.50,142.59,188.46,7.86">CLEF (Online Working Notes/Labs/Workshop</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.34,153.55,342.23,7.86;10,146.91,164.51,333.67,7.86;10,146.91,175.47,333.68,7.86;10,146.91,186.43,108.66,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,267.33,164.51,209.51,7.86">Overview of ntcir-9 rite: Recognizing inference in text</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Shima</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kanayama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mitamura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">.</forename><forename type="middle">.</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,158.72,175.47,321.87,7.86;10,146.91,186.43,49.27,7.86">Proceedings of the 9th NII Test Collection for Information Retrieval Workshop (NTCIR&apos;11)</title>
		<meeting>the 9th NII Test Collection for Information Retrieval Workshop (NTCIR&apos;11)</meeting>
		<imprint>
			<date type="published" when="2011-12">2011, December</date>
			<biblScope unit="page" from="291" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.34,197.39,342.24,7.86;10,146.91,208.35,333.67,7.86;10,146.91,219.30,240.20,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,327.67,197.39,123.51,7.86">Accurate unlexicalized parsing</title>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,463.03,197.39,17.55,7.86;10,146.91,208.35,333.67,7.86">Proceedings of the 41st Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.34,230.26,342.24,7.86;10,146.91,241.22,333.67,7.86;10,146.91,252.18,333.67,7.86;10,146.91,263.14,196.69,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,425.39,230.26,55.19,7.86;10,146.91,241.22,304.78,7.86">Incorporating non-local information into information extraction systems by gibbs sampling</title>
		<author>
			<persName coords=""><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trond</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,463.03,241.22,17.55,7.86;10,146.91,252.18,329.85,7.86">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.34,274.10,342.24,7.86;10,146.91,285.06,333.67,7.86;10,146.91,296.02,256.75,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,192.48,285.06,288.11,7.86;10,146.91,296.02,48.21,7.86">Deterministic coreference resolution based on entity-centric, precisionranked rules</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Peirsman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,201.79,296.02,105.46,7.86">Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="1" to="54" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Just Accepted</note>
</biblStruct>

<biblStruct coords="10,138.34,306.98,342.23,7.86;10,146.91,317.93,333.67,7.86;10,146.91,328.89,91.77,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,151.39,317.93,269.58,7.86">Generating typed dependency parses from phrase structure parses</title>
		<author>
			<persName coords=""><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bill</forename><surname>Marie-Catherine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,432.50,317.93,48.08,7.86;10,146.91,328.89,32.91,7.86">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.34,339.85,342.24,7.86;10,146.91,350.81,333.67,7.86;10,146.91,361.77,333.67,7.86;10,146.91,372.73,333.67,7.86;10,146.91,383.69,333.67,7.86;10,146.91,394.65,322.58,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,286.19,350.81,194.40,7.86;10,146.91,361.77,226.66,7.86">SemEval-2013 task 7: The joint student response analysis and 8th recognizing textual entailment challenge</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">O</forename><surname>Dzikovska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">D</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Brew</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Leacock</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">.</forename><forename type="middle">.</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,391.14,361.77,89.43,7.86;10,146.91,372.73,266.54,7.86;10,433.74,372.73,46.84,7.86;10,146.91,383.69,333.67,7.86;10,146.91,394.65,20.48,7.86">conjunction with the Second Joint Conference on Lexical and Computational Semantcis (* SEM 2013)</title>
		<meeting><address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-06">2013, June. June</date>
		</imprint>
	</monogr>
	<note>Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
