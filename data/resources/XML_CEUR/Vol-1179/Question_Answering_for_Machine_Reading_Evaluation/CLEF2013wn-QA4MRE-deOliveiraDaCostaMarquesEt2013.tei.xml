<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,135.52,115.96,344.33,12.62;1,291.54,133.89,32.28,12.62">Combining text mining techniques for QA4MRE 2013</title>
				<funder ref="#_wWPPSbt">
					<orgName type="full">Research Foundation Flanders</orgName>
				</funder>
				<funder>
					<orgName type="full">Brazil&apos;s National Counsel of Technological and Scientific Development (Conselho Nacional de Desen-volvimento Científico e Tecnológico -CNPq)</orgName>
				</funder>
				<funder ref="#_PYVPAya">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,165.77,171.96,179.04,8.74"><forename type="first">Guilherme</forename><surname>De Oliveira Da Costa Marques</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Federal University of São Carlos (UFSCar)</orgName>
								<address>
									<settlement>campus Sorocaba</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,371.97,171.96,73.14,8.74"><forename type="first">Mathias</forename><surname>Verbeke</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">KU Leuven</orgName>
								<address>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,135.52,115.96,344.33,12.62;1,291.54,133.89,32.28,12.62">Combining text mining techniques for QA4MRE 2013</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5E55C644CA3BB9D9D1737C60BF796325</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes a lexical system developed for the main task of Question Answering for Machine Reading Evaluation 2013 (QA4MRE). The presented system executes the preprocessing of test documents, and generates hypotheses consisting of the question text combined with text from possible answers for the question. The hypotheses are compared to sentences from the text by the means of a set similarity measure. The k best similarity scores obtained by each hypothesis are averaged as ranking score for the hypothesis. Two variations of the developed system were utilized, one of them employing coreference detection and resolution techniques in order to take advantage of the discourse structure on the question answering process. The results generated by the systems in QA4MRE 2013 edition are presented and analyzed. The presented system should serve as a solid base for the development of a semantic approach on the task.</p><p>Question: What caused an improvement in sound quality in 1950?</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The QA4MRE competition <ref type="bibr" coords="1,258.04,451.67,10.52,8.74" target="#b0">[1]</ref> focuses on the Question Answering field of Machine Reading. Adopting the form of several tests spread over a few themes, it aims at evaluating a system's Natural Language Understanding capabilities by the means of multiple choice questions.</p><p>The main task of the competition is currently composed by four topics: "AIDS", "Climate Change", "Music and Society" and "Alzheimer's". A background collection of texts is provided for all topics. This collection attempts to encompass all specific domain knowledge of the topic.</p><p>The 2013 edition consists of 16 reading tests, 4 on each topic. Each reading test presents a text document followed by 15 to 20 questions about it. Those are multiple choice questions with 5 alternatives, the last one being "None of the above".</p><p>Questions are distributed over different degrees of complexity as to the knowledge and inference required to devise the correct answer. The simplest ones have both the question fact and the answer appearing directly in the same sentence of the text. Others have the question fact and the answer appearing in distinct sentences. Some questions require background knowledge or inference, and some may require the use of both.</p><p>1. the introduction of soundtrack recording on 35 mm magnetic tape 2. the use of an optical soundtrack 3. the adoption of a quadraphonic sountrack 4. the specialisation in silent films 5. none of the above</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1: Example question</head><p>An example question is presented in table 1. For this question, alternative 1 should be identified as correct.</p><p>The system described in this work is based on a text mining baseline system <ref type="bibr" coords="2,155.75,286.02,9.96,8.74" target="#b1">[2]</ref>. It was developed and tested with data from QA4MRE 2012 edition. Several parameters and system variations were tested, which are described more thoroughly in <ref type="bibr" coords="2,198.50,309.93,9.96,8.74" target="#b2">[3]</ref>. We do not employ the background collection in the current system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Two different systems were employed for the competition: a main system was elaborated, and employed both as standalone and as a base for a variation including coreference resolution. The structure of both systems is presented in figure <ref type="figure" coords="2,162.46,410.83,3.87,8.74" target="#fig_1">1</ref>.</p><p>Section 2.1 presents the text preprocessing employed on the test documents. Section 2.2 explains the procedure responsible for ranking the alternatives, while section 2.3 discusses the different techniques employed in cases where a tie occurs in the ranking. While all the previously mentioned characteristics are common to the two systems, section 2.4 explains the additions only present in the coreference variation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preprocessing</head><p>Preprocessing of the test documents proceeds according to the following sequence:</p><p>1. Unicode decoding: treats special unicode characters, generating ASCIIsafe strings. 2. Text fixing: corrects some of the formatting issues present in the 2012 test set, through the use of regular expressions. Although similar issues were not detected with 2013's test set, the procedure was maintained for safety and compatibility reasons.  5. Bag of words model: sentences are represented as sets of word strings. 6. Stopword and punctuation removal: punctuation signs and words classified as stopwords according to the English stopword corpus from NLTK <ref type="bibr" coords="4,470.07,142.46,10.52,8.74" target="#b3">[4]</ref> are removed from the sentences. 7. Stemming or Lemmatization: words are converted into word stems (by NLTK's Porter stemmer) or into lemmas (by NLTK's WordNet-based lemmatizer). 8. Joining of adjacent sentences (optional): in an attempt to perform a primitive form of discourse analysis, a procedure where the sets of words from adjacent sentences are joined prior to ranking was tested. Results generated by this strategy on QA4MRE'12 test set presented an overall improvement in accuracy, so this procedure was maintained for the base system.</p><p>With respect to item 7, the choice between stemming and lemmatization took into consideration the accuracy observed on 2012 test set. For the base system, lemmatization yielded marginally better results, while the coreference system presented improved results when stemming was employed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Ranking procedure</head><p>Ranking is performed by computing the similarity between sentences and hypotheses, as presented in Algorithm 1. The hypotheses are generated by joining question text and the text from alternatives: The similarity metric employed during the ranking procedure is the MASI similarity <ref type="bibr" coords="4,179.12,656.12,9.96,8.74" target="#b4">[5]</ref>, calculated by the formula below:</p><formula xml:id="formula_0" coords="4,134.77,396.43,199.72,49.88">H i = Q ∪ A i Algorithm 1</formula><formula xml:id="formula_1" coords="5,216.25,137.06,181.66,23.22">masi sim(set 1 , set 2 ) = |set 1 ∩ set 2 | max(|set 1 |, |set 2 |)</formula><p>According to the similarity scores calculated between hypotheses and text sentences, each hypothesis receives an overall ranking score computed as the average of the k best sentence similarities. The candidate answer chosen by the system corresponds to the hypothesis which presents the highest ranking score. The value of k is a parameter provided to the system, and was set to k = 2, considering the test results obtained with the QA4MRE'12 test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Handling of ties</head><p>In some cases, the ranking procedure results in a tie between two or more hypotheses. To handle those cases, four different strategies were envised:</p><p>1. All questions where no candidate answer was found (there was a ranking tie) were answered as "None of the above". 2. All questions where no candidate answer was found were left unanswered. 3. If the maximum ranking score between the alternatives is inferior to a certain threshold, the question is answered as "None of the above". Otherwise, it is left unanswered. 4. If the maximum ranking score between the alternatives is inferior to a certain threshold, the question is answered as "None of the above". Otherwise, one of the tied alternatives is selected at random.</p><p>The reasoning behind the threshold value utilized in 3 and 4 is that in questions where the ranking values were lower, there would be a higher chance that none of the alternatives was correct and the question had no answer. In contrast, in questions where the ranking values were higher, it would be more likely that there was a correct answer, but the system was unable to find it. This threshold was empirically set to 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Coreference variation</head><p>The system illustrated in Figure <ref type="figure" coords="5,273.83,539.07,10.52,8.74" target="#fig_1">1b</ref> includes a coreference detection phase, as well as a coreference resolver. This addition intends to take advantage of discourse analysis, allowing for the resolution af anaphoric pronouns, as well as other types of coreferences.</p><p>Coreference detection is performed by Stanford's NLP suite <ref type="bibr" coords="5,416.91,586.89,9.96,8.74" target="#b5">[6]</ref>, which outputs an XML file containing information on the detected coreferences present in the processed text. Information extracted from this XML file is employed by the coreference resolver in the following way:</p><p>1. The representative noun phrase is located. 2. Words from the representative reference are included in a word set.</p><p>3. Sentences where other references to the same entity appear are located. 4. The word set from the representative reference is joined into the sentences that refer to the same entity.</p><p>This simple strategy presents good results with regards to the resolution of referential pronouns, in the context of a hypothesis ranking computed through set similarity: since the words from the representative reference are included in the word set of referencing sentences, this has a positive impact on the similarity between those sentences and hypotheses that mention the same entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>Eight distinct runs were submitted to the competition, where the two presented systems were paired with each one of the four tie strategies described in section 2.3. General results from each run are presented in table 2. In this table, performance is measured according to two metrics: accuracy and the c@1 measure <ref type="bibr" coords="6,134.77,323.59,9.96,8.74" target="#b6">[7]</ref>. C@1 is the main performance measure employed in the competition, and is calculated as follows:</p><formula xml:id="formula_2" coords="6,257.59,353.39,98.98,24.94">c@1 = (n R + n U * n R n ) n</formula><p>where n R = number of correctly answered questions n U = number of unanswered questions n = total number of questions</p><p>The competition consisted of a total of 240 main questions, of which 44 required inference in the answering process. Those inference-demanding questions had simpler duplicates where the question was phrased in a way the inference was no longer required. The "c@1 main" accuracy only takes the 240 main questions into consideration, and "c@1 all " is calculated over all 284 questions. Run number 6 presented the best general results, with a c@1 measure of 0.30 on the main questions. The considerable increase in the c@1 metric between the main set of questions and the complete set reinforces the weakness of the employed systems with inference demanding questions. Table <ref type="table" coords="7,284.69,297.79,3.87,8.74">3</ref>: C@1 per topic Table <ref type="table" coords="7,176.41,342.40,4.98,8.74">3</ref> lists the c@1 results per topic, for the main set of questions. From the analysis of this table, a considerable variation in performance between topics can be noticed: the system presented the best results in topic 1, followed by topic 3; topics 2 and 4 have inferior performance. Another remarkable fact is that while the best results from the coreference system are superior to the results from the base system in topic 1, the differences in the results on other topics are negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>The presented methodology is entirely based on lexical similarity. Possible directions for improvement are the inclusion of techniques for proper handling of questions involving negation (added to 2013 main task, but not present in 2012). The system could also benefit from a weighted similarity measure that would prioritize words according to importance.</p><p>Although there is still room for improvement while maintaining the lexical character of the system, we believe that the ideal focus of future work would be on establishing a system able to deal with semantic relations through the development of strategies aiming at textual and logic inference. We also consider of crucial importance the development of techniques for knowledge base construction, which can perform the extraction of domain-specific knowledge from the background collection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,138.97,632.52,333.50,8.77;2,138.97,644.13,341.62,8.77;2,151.70,656.12,52.44,8.74"><head>3 . 4 .</head><label>34</label><figDesc>Sentence tokenization: splits sentences from text into separate strings. Word tokenzation: segments individual words and punctuation signs from text strings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,255.78,142.38,103.80,8.74;3,188.92,164.28,65.77,7.86;3,346.40,164.28,94.31,7.86;3,140.53,179.67,162.54,458.27"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: System structure(a) Base System (b) Coreference System</figDesc><graphic coords="3,140.53,179.67,162.54,458.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,143.98,437.58,184.39,176.86"><head></head><label></label><figDesc>Ranking Procedure</figDesc><table coords="4,143.98,453.15,184.39,161.29"><row><cell>ranks ← list()</cell></row><row><cell>for all hypothesis hyp in hypotheses list do</cell></row><row><cell>similarities ← list()</cell></row><row><cell>for all sentence sent in document do</cell></row><row><cell>sim ← similarity(sent, hyp)</cell></row><row><cell>similarities.append(sim)</cell></row><row><cell>end for</cell></row><row><cell>average ← 0</cell></row><row><cell>for all top k values sim in similarities do</cell></row><row><cell>average ← average + sim</cell></row><row><cell>end for</cell></row><row><cell>average ← average/k</cell></row><row><cell>ranks.append(average)</cell></row><row><cell>end for</cell></row><row><cell>selected ← indexOf (maximum(ranks))</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,214.78,531.37,185.80,119.94"><head>Table 2 :</head><label>2</label><figDesc>General results</figDesc><table coords="6,214.78,531.37,185.80,107.69"><row><cell>Algorithm</cell><cell></cell><cell>Main Qs.</cell><cell>All Qs.</cell></row><row><cell cols="4">System Tie Run Accur. c@1 Accur. c@1</cell></row><row><cell></cell><cell>1 02</cell><cell cols="2">0.28 0.28 0.34 0.34</cell></row><row><cell>Base + Join</cell><cell>2 03 3 04</cell><cell cols="2">0.23 0.26 0.29 0.33 0.27 0.28 0.33 0.35</cell></row><row><cell></cell><cell>4 05</cell><cell cols="2">0.28 0.28 0.35 0.35</cell></row><row><cell></cell><cell>1 06</cell><cell cols="2">0.30 0.30 0.33 0.33</cell></row><row><cell>Coreference</cell><cell>2 07 3 08</cell><cell cols="2">0.22 0.26 0.26 0.30 0.26 0.29 0.29 0.32</cell></row><row><cell></cell><cell>4 09</cell><cell cols="2">0.28 0.28 0.31 0.31</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>5 Acknowledgements <rs type="person">Guilherme de Oliveira da Costa Marques</rs> is funded by <rs type="funder">Brazil's National Counsel of Technological and Scientific Development (Conselho Nacional de Desen-volvimento Científico e Tecnológico -CNPq)</rs>. <rs type="person">Mathias Verbeke</rs> is funded by the <rs type="funder">Research Foundation Flanders</rs> (<rs type="projectName">FWO</rs>-project <rs type="grantNumber">G.0478.10 -</rs><rs type="projectName">Statistical Relational Learning of Natural Language</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_wWPPSbt">
					<idno type="grant-number">G.0478.10 -</idno>
					<orgName type="project" subtype="full">FWO</orgName>
				</org>
				<org type="funded-project" xml:id="_PYVPAya">
					<orgName type="project" subtype="full">Statistical Relational Learning of Natural Language</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,138.35,198.38,342.24,8.12;8,146.91,209.98,49.63,7.47" xml:id="b0">
	<monogr>
		<ptr target="http://celct.fbk.eu/QA4MRE/" />
		<title level="m" coord="8,146.91,198.38,250.91,7.86">CELCT: Question Answering for Machine Reading Evaluation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,220.30,120.97,7.86;8,283.44,220.30,197.15,7.86;8,146.91,231.26,54.52,7.86;8,233.37,231.26,247.22,7.86;8,146.91,242.21,25.60,7.86;8,263.56,242.86,217.03,7.47;8,146.91,253.82,174.02,7.47" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,283.44,220.30,197.15,7.86;8,146.91,231.26,49.07,7.86">A text mining approach as baseline for QA4MRE&apos;12</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Verbeke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/documents/71612/234cb84c-03a3-45c3-8844-9b1ca448d976" />
	</analytic>
	<monogr>
		<title level="m" coord="8,256.80,231.26,219.15,7.86">CLEF (Online Working Notes/Labs/Workshop)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,264.13,342.24,7.86;8,146.91,275.09,186.52,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="8,290.42,264.13,190.17,7.86;8,146.91,275.09,38.08,7.86">Combining text mining techniques for question answering</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>De Oliveira Da Costa Marques</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
		<respStmt>
			<orgName>KU Leuven</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct coords="8,138.35,286.05,342.24,7.86;8,146.91,297.01,25.60,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,271.71,286.05,166.35,7.86">Natural language processing with Python</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Loper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,447.44,286.05,33.15,7.86">O&apos;Reilly</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,307.97,342.24,7.86;8,146.91,318.93,333.68,7.86;8,146.91,329.89,260.72,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,219.56,307.97,261.04,7.86;8,146.91,318.93,104.74,7.86">Measuring agreement on set-valued items (MASI) for semantic and pragmatic annotation</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Passonneau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,276.79,318.93,203.81,7.86;8,146.91,329.89,191.69,7.86">Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC)</title>
		<meeting>the Fifth International Conference on Language Resources and Evaluation (LREC)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="831" to="836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,340.84,342.24,7.86;8,146.91,351.80,333.68,7.86;8,146.91,362.76,156.89,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,466.38,340.84,14.21,7.86;8,146.91,351.80,330.08,7.86">Deterministic coreference resolution based on entity-centric, precision-ranked rules</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Peirsman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,146.91,362.76,106.72,7.86">Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="1" to="54" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,371.45,342.24,10.13;8,146.91,384.68,333.68,7.86;8,146.91,395.64,333.68,7.86;8,146.91,407.24,221.09,7.47" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,323.47,384.68,157.12,7.86;8,146.91,395.64,212.12,7.86">Overview of ResPubliQA 2009: Question answering evaluation over european legislation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Forner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">F E</forename><surname>Sutcliffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Á</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Forascu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Alegria</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Osenova</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-15754-7_21</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-642-15754-7_21" />
	</analytic>
	<monogr>
		<title level="j" coord="8,386.48,395.64,27.26,7.86">CLEF</title>
		<imprint>
			<biblScope unit="page" from="174" to="196" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
