<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,163.36,115.96,288.65,12.62;1,189.55,133.89,236.26,12.62;1,188.72,154.37,237.93,10.52">Selecting answers with structured lexical expansion and discourse relations LIMSI&apos;s participation at QA4MRE 2013</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,143.54,191.05,60.15,8.74"><forename type="first">Martin</forename><surname>Gleize</surname></persName>
						</author>
						<author>
							<persName coords="1,214.25,191.05,59.22,8.74"><forename type="first">Brigitte</forename><surname>Grau</surname></persName>
						</author>
						<author>
							<persName coords="1,290.35,191.05,86.93,8.74"><forename type="first">Anne-Laure</forename><surname>Ligozat</surname></persName>
						</author>
						<author>
							<persName coords="1,394.17,191.05,64.07,8.74"><forename type="first">Van-Minh</forename><surname>Pho</surname></persName>
						</author>
						<author>
							<persName coords="1,180.81,203.00,59.59,8.74"><forename type="first">Gabriel</forename><surname>Illouz</surname></persName>
						</author>
						<author>
							<persName coords="1,257.30,203.00,79.96,8.74"><forename type="first">Frédéric</forename><surname>Giannetti</surname></persName>
						</author>
						<author>
							<persName coords="1,367.20,203.00,62.87,8.74"><forename type="first">Loïc</forename><surname>Lahondes</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">LIMSI-CNRS</orgName>
								<address>
									<addrLine>rue John von Neumann</addrLine>
									<postCode>91403</postCode>
									<settlement>Orsay cedex</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Université Paris-Sud</orgName>
								<address>
									<postCode>91400</postCode>
									<settlement>Orsay</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">ENSIIE</orgName>
								<address>
									<addrLine>1 Square de la Résistance</addrLine>
									<postCode>91000</postCode>
									<settlement>Evry</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,163.36,115.96,288.65,12.62;1,189.55,133.89,236.26,12.62;1,188.72,154.37,237.93,10.52">Selecting answers with structured lexical expansion and discourse relations LIMSI&apos;s participation at QA4MRE 2013</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F56436B91E721ADB534F83C13F0EFA09</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>question answering</term>
					<term>index expansion</term>
					<term>discourse relation</term>
					<term>question classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present the LIMSI's participation to QA4MRE 2013.We decided to test two kinds of methods. The first one focuses on complex questions, such as causal questions, and exploits discourse relations. Relation recognition shows promising results, however it has to be improved to have an impact on answer selection. The second method is based on semantic variations. We explored the English Wiktionary to find reformulations of words in the definitions, and used these reformulations to index the documents and select passages in the Entrance exams task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper we present the LIMSI's participation to QA4MRE 2013. We decided to experiment two kinds of methods. The first one focuses on complex questions, such as causal questions, and exploits discourse relations. We created a question typology based on the one proposed by QA4MRE organizers, and linked it to the type of relation expected between the answer and the question information. In order to detect these relations in the texts, we wrote rules based on parse trees and connectors.</p><p>The second method is based on semantic variations. We explored the English Wiktionary to find reformulations of words in their definition, and used these reformulations to index the documents and select passages in the Entrance exams task.</p><p>The paper is organized as follows: in section 2, in order to give an overview of the methods we developed, we present the general architecture of our system. Section 3 details question analysis. In relation to question classification, section 4 presents discourse relation recognition. We then present the two methods for passage selection and answer ranking in section <ref type="bibr" coords="2,349.10,118.99,3.87,8.74" target="#b4">5</ref>. Selection of answer according to question category and discourse relation is described in section 6 before presenting our experimentations and results in section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System overview</head><p>Figure <ref type="figure" coords="2,167.25,198.68,4.98,8.74" target="#fig_0">1</ref> presents the different modules developed for QA4MRE tasks, which form the QALC4MRE system. Reading documents used in main task and Alzheimer task are generally scientific papers and variations between words in questions and answers and words in the relevant passages of text are often based on paraphrases. Thus, these kinds of variations are handled by rules that take into account morphological, syntactic and semantic variants <ref type="bibr" coords="2,277.39,572.43,9.96,8.74" target="#b0">[1]</ref>. In entrance exams, there are more distant semantic variations between each set of words, such as hypernymy or causal relation for example. Thus, we tackle these problems by creating paths based on following dictionary definitions of question words towards document words. We developed two modules for passage retrieval: terms and variant indexing and word tree indexing. Question analysis is the same for all tasks. From the question parse trees, we generate hypotheses by applying rules written manually. For determining question types, we reuse existing question classification modules.</p><p>Complex types of questions are associated to discourse relations in documents which have to hold with the answer. In order to recognize these relations in documents, we wrote rules based on parse trees of document sentences.</p><p>Answers are ranked according to different measures. For answers to complex questions, if a corresponding relation if found on a candidate answer in the top passages, this candidate is returned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Question analysis</head><p>The aim of the question analysis module is to determine the question category. As we decided to focus on discourse relations, we adapted our existing systems to detect the kind of discourse relation between the answer and the question words.</p><p>We kept the Factoid questions subclasses based on the expected answer type in terms of named entity type: person, organization, location, date... We added the following classes, according to the task guidelines and to [2] taxonomy:</p><p>causal/reason: there is a cause-consequence relation between the answer and question information.</p><p>Why cannot bexarotene be used as a cure for Alzheimer's disease? method/manner: the question asks for the way something happens.</p><p>How do vitamin D and bexarotene correlate? opinion: the question asks about the opinion about something.</p><p>What was Cramer's attitude towards the music of Bach? definition: the expected answer is the definition, an instance or an equivalent of the question focus.</p><p>What is a common characteristic for the neurodegenerative diseases?, Give two symptoms of dementia. thematic: the question asks for an event at a given time.</p><p>What happened during the meal after the family had all taken their new seats?</p><p>We used two existing question analysis modules: one based on syntactic rules <ref type="bibr" coords="3,158.64,535.85,10.52,8.74" target="#b0">[1]</ref> and one based on machine learning classification <ref type="bibr" coords="3,387.15,535.85,9.96,8.74" target="#b2">[3]</ref>.</p><p>The first module parses the question with the Stanford Parser <ref type="bibr" coords="3,421.14,548.16,10.52,8.74" target="#b3">[4]</ref> which provides a constituency parse tree. Then, syntactic rules determine the question class by recognizing a syntactic pattern with Tregex and Tsurgeon <ref type="bibr" coords="3,432.32,572.07,9.96,8.74" target="#b4">[5]</ref>. For example, for the question Which singer made a hit record whose accompaniment was entirely synthesised?, the rules detect the interrogative pronoun which and that it possesses a son son in the parse tree; this noun is compared to a list of triggers and is recognized as a trigger of the person question class.</p><p>After the evaluation, we evaluated the results of this module on the test sets of QA4MRE 2013. 73% of questions were correctly classified. Most errors were due to question formulations which had not been taken into account, such as boolean questions, and some of them to misclassifications (for example What is the cause... was incorrectly classified as a factoid question).</p><p>The second module is based on an SVM based classifier using the LibSVM <ref type="bibr" coords="4,470.07,142.90,10.52,8.74" target="#b5">[6]</ref> tool with default parameters. The classifier was trained on <ref type="bibr" coords="4,390.61,154.86,10.52,8.74" target="#b1">[2]</ref> fine-grained question taxonomy, with each question category considered as a class. The features used are n-grams (n ranging 1..2) of words, lemmas and parts-of-speech (determined by the TreeTagger <ref type="bibr" coords="4,247.07,190.72,10.30,8.74" target="#b6">[7]</ref>), as well as the trigger lists of the first module and a regular expression based recognition of abbreviations. This module obtained 0.84 precision on <ref type="bibr" coords="4,211.23,214.64,10.52,8.74" target="#b1">[2]</ref> test corpus.</p><p>We also evaluated this module on QA4MRE 2013 test sets, and it obtained 0.85 correct classification. The main kinds of error are the misclassification of factoid question into definition questions and the absence of the opinion class in the hierarchy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discourse relation recognition</head><p>Our present work was a first attempt to take into account discourse relations in order to study if it was possible to relate them to question categories and thus to provide a supplementary criterion for selecting an answer. Thus we decided to model the recognition of some of them by rules in a first time, as we did not have an annotated corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">List of relations</head><p>We took into account the following four binary relations:</p><p>-Causality, to be related to causal/reason questions -Opinion, to be related to opinion questions -Definition, to be related to definition questions -Example, to be related to questions asking for a concept in factual questions, such as which animal ... ?</p><p>Being binary relations, each of these relations presents two components which we detail below:</p><p>-Causality is composed of a cause and a consequence.</p><p>[He would not provide his last name] Csqce [because] M ark [he did not want people to know he had the E. coli strain.] Cause -Opinion is composed of a Source and a Target.</p><p>[Some users of the Apple computer] Src [say] M ark [it smells sickening.] T rgt -Definition is composed of a Concept and an Explanation.</p><p>[a Rube Goldberg machine] Cpt [is] M ark [a complicated contraption, an incredibly over-engineered piece of machinery that accomplishes a relatively simple task] Exp -Example is composed of a Concept and a List.</p><p>[other endangered North American animals] Cpt [such as] M ark [the red wolf and the American crocodile.] List</p><p>Causes and consequences of causality relations can be found between two clauses or between phrases in a sentence; they can also be found in consecutive sentences. Thus we defined rules that recognize each of the two members separately.</p><p>Opinion relations were restricted to reported discourse. Definition relations gather all types of clause that helps defining or specifying a precise concept. These can be embodied as appositive, as in the tiger, the largest of all the big cats, reformulation, as in polar regions known as the cryosphere or a canonical model of definition, as in Rickettsia mooseri is a parasite of rats.</p><p>Example relations encompass any instance of a larger concept. The expected result is a list of n instances, as to be found in luxuries such as home air conditioning and swimming pools or great Black players like Michael Jordan or Elgin Baylor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Relation extraction</head><p>Regular expressions were defined on the syntactic trees of sentences. They were obtained by parsing a significant portion of the background collection of QA4MRE 2012 using Stanford Parser. We first defined a set of discriminating clue words ( M ark ) for each of the aforementioned relations based on the selected corpus. We then developped a series of syntactic rules implemented according to the Tregex formalism <ref type="bibr" coords="5,215.93,379.01,10.52,8.74" target="#b4">[5]</ref> which allows to create tgrep-type patterns for matching tree node configurations. Constraints in rules are defined on left, right, child and parent nodes of the M ark . They are about expected types of syntagms and POS categories.</p><p>In total, we defined a set of 42 rules to extract the different types of relations.</p><p>To evaluate the extraction of rules, we manually annotated the four texts of each thematic of the evaluation for the Main Task 2013 and the nine texts for English Exams Task. Twenty-five annotated documents were thus annotated, containing 162 causality relations, 53 opinions, 114 definitions and 57 examples, for a total of 416 relations.</p><p>We then compared the manual annotation to the one made by our system on these documents. To achieve this, we categorized found relationships in two types. If the relationship annotated by hand is strictly the same as the relationship found automatically, i.e. same type and same related members, this relationship is classified as "exact". If the relationship is incomplete, i.e. if there are missing or extra words in the related members, the relationship is classified as "loose". We will consider these kinds of relations as correct in a lenient evaluation. If the type of the relationship automatically annotated is false, it is "incorrect". Finally, we compute a fourth counter: the number of "missed" relationships, calculated as the difference between the number of manual annotations and the sum of the number of "correct" and "loose" relationships.</p><p>Results are given in tables 1 and 2. We can see that we obtain a very good precision in the lenient evaluation, which shows that relation types are well identified. As expected, recall is lower, but remains reasonable. 5 Passage and answer weighting</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">QALC4MRE strategy</head><p>We apply the weighting scheme of <ref type="bibr" coords="6,292.10,373.88,10.52,8.74" target="#b0">[1]</ref> for sentences according to the question words and answers, named P REP, the overlapping of weighted common words between a sentence and an answer, TERp and treeEdit distances between a sentence and an hypothesis.</p><p>For selecting answers, we give priority to passage weight, and secondary to answer weight, and define several combinations of these weights:</p><p>the most frequent answer in the n top sentences. In case of equality of different answers, the answer in the best sentence is selected, and if several candidate answers remain in the same sentence, the answer with the best weight is selected. This selection scheme is named freqTop. the most frequent answer in the n top sentences which contain a candidate answer, with the same options in case of answer equality, named maxS.</p><p>the best answer in the n top sentences, named maxSTop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Dictionary-based passage retrieval</head><p>In a question answering system, passage retrieval aims at extracting the short text excerpt most likely to contain the answer from a relevant document. For the most realistic questions, direct matching of the surface form of the query and text sentences is not sufficient. As one of the most challenging and important processes in a QA system, passage retrieval would thus benefit from a more semantic approach. We propose a passage retrieval method focusing on finding deep semantic links between words. We view a dictionary entry as a kind of word tree structure: taken as a bag of words, the definition of a word makes up its children. Then the words in the definition of a child are this child's own children, and so on. From this point forward we will designate as words only lemmas from verbs, nouns, adjectives, adverbs and pronouns that are not stop-words. We assume a single purely textual document. Document words are words in the document.</p><p>Indexing the document This document pre-processing phase builds an index off of all the words in the document and their descendants in a given dictionary. This is similar to the index expansion of Attardi et al. <ref type="bibr" coords="7,382.17,220.77,9.96,8.74" target="#b7">[8]</ref>, except we use dictionaries and not background documents. An entry in this index is composed of:</p><p>a word w (the key in the index) a list Inv(w) of pairs (index of a sentence containing w in the document, index of w in this sentence): this is standard inverted indexing. the tree T (w) of w's word descendants (implemented as pointers to the entries of w's children) a list Anc(w) of document word ancestors, pairs (w 2 document word, d depth) such that: w 2 ∈ Anc(w) with depth d iff we can find w in w 2 's tree at depth d (For example: at depth 2, we look at children of children of w 2 and w is among them).</p><p>To index a given word w, we check if w isn't already in the index (otherwise we build and add the entry), and we update the entry recursively, using an auxiliary children update procedure UPDATE in the main procedure INDEX(w, d, doc ancestor):</p><p>1. w as key 2. if w is a document word: (a) add to Inv(w) the pair (index of S, index of w in S).</p><p>(b) add (w, 0) to Anc(w). Indeed, w is a document word, and he's the root of its tree (the only node of depth 0). 3. build T (w) with an update procedure UPDATE(w, d max , doc ancestor), which we define in the following.</p><p>In dictionaries, traversing all the words in a definition tree might not terminate.</p><p>There are cycles: it can happen than the word itself appears in the definition of words of its own definition. So we choose to explore at most d max levels of depth when building T (w) for any w.</p><p>Let's now define UPDATE(w, d, doc ancestor), which updates T (w):</p><p>1. look up the definition of w in the dictionary. If not found we don't touch T (w).  To build the complete index of the document, we simply run INDEX(w, d max , w) for each w of each sentence (we use StanfordCoreNLP for tokenization and tagging <ref type="bibr" coords="8,169.13,162.47,10.30,8.74" target="#b8">[9]</ref>). This is the basis of our indexing, bar minor details of implementation (re-indexing in case we need to explore an indexed word at a greater depth, handling of multiple senses and POS-tags, . . . )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Passage retrieval</head><p>We first consider words of the query, then use the index to score their relevance, and finally compute a density-based sliding window ranking function to retrieve passages. For each word w q in the query, we run a version of INDEX(w q , d max , NIL) which does not update document ancestors Anc(w) (as the word of the query isn't truly a document word). In T (w q ), we find descendants w of w q which have been previously built during the indexing phase and thus have an non-empty Anc(w), their document word ancestors, which are essentially the document words that initiated the access to w in the dictionary. We can compute a similarity between w q and those document words, therefore rating the relevance of document words relatively to the query word:</p><formula xml:id="formula_0" coords="8,189.77,354.53,290.82,12.07">Sim(w q , (w doc anc , d)) = idf (w doc anc ) × base -(dmin+d)<label>(1)</label></formula><p>d min = min wc∈T (wq) at depth dc|w doc anc ∈Anc(wc)</p><formula xml:id="formula_1" coords="8,390.07,378.42,90.51,9.65">(d c )<label>(2)</label></formula><p>We choose base depending on how strongly we want to penalize words as we go deeper in the tree. We found base = 2 to be a good start, but the final system uses the number of children at the depth of the closest child containing w doc anc in Anc. The intuition is that the more words used in the definition of w, the less confident we are that each definition word is semantically related to w. We compute the similarity for each w q in the query and each w doc in document word ancestors and sum over the w q to obtain a relevance score for the document word:</p><formula xml:id="formula_2" coords="8,192.61,497.45,287.98,19.61">Relevance(w doc ) = wq∈ query (max d Sim(w q , (w doc , d)))<label>(3)</label></formula><p>Finally, we select candidate passages with a sliding window of 3 consecutive sentences, and rank them using a similar method to SiteQ's density-based scoring function described in <ref type="bibr" coords="8,229.50,553.22,14.61,8.74" target="#b9">[10]</ref>, using Relevance as the weight of keywords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Answer selection related to discourse relation</head><p>To select an answer which takes into account question category and discourse relations, we combine weights and discourse relations of the passages. First, we filter relations according to the category of the question and presence of the answer associated with the passage in the relations. Only relations whose type is the same as the category of the question and containing an answer are kept.</p><p>Then, passages are sorted according to their weights. Among the top n passages, if any of them has a relation, the answer associated with the best weighted passage is selected. Otherwise, we consider only passages containing relations and select the answer associated with the best of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Main task and Alzheimer task</head><p>Table <ref type="table" coords="9,162.01,227.69,4.98,8.74" target="#tab_1">3</ref> presents results obtained on the QA4MRE2012 corpus, for the different selection scheme presented section 5.1, with the number n of top sentences set to 5 after experiments. The strategy described in 6 did not lead to different results, as either question category was not correctly identified or the associate relation was not recognized in the correct passage. We can see that, while textual entailment distances between an hypothesis and a sentence are useful to select an answer in Alzheimer task, they are overcome by lexical overlap weighting in the main task. This can be due to differences in answer length in the two tasks: shorter answers in Alzheimer task favour measures based on sentence structure.</p><p>We obtained analogous results on the 2013 evaluation for Alzheimer task, best c@1 is 0.42 for treeEdit combined with freqTop, while results on the main task are lower with a best c@1 at 0.28 with the combination P REP with maxS. It may be due to new kinds of questions introduced this year, and the new kind "do not know" of answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Entrance exam task</head><p>The form of the task is essentially the same as the main task. Multiple-choice questions are taken from reading tests of Japanese university entrance exams. A crucial difference from the other QA4MRE tasks is that background text collections are not provided. Given the difficulty of the questions and the lack of background knowledge, passage retrieval quickly appeared as a strong bottleneck for any question answering system attempting to solve the task. That is why we decided to design the dictionary-based lexical expansion described in 5.2 and use Simple English Wiktionary <ref type="bibr" coords="10,187.68,130.95,15.50,8.74" target="#b10">[11]</ref> as the dictionary. Simple English Wiktionary is a collaborative dictionary written in a simplified form of English, primarily for children and English learners. Its definitions are clear, concise and get to the essence of the word without superfluous details, and seem fitted to acquire the "common sense knowledge" we need to solve this task <ref type="bibr" coords="10,303.19,178.77,14.61,8.74" target="#b11">[12]</ref>. We submitted a run at QA4MRE 2013 which used only this passage retrieval system and very simple heuristics to choose an answer. The results were worse than the random baseline, due to bugs in the early implementation and the discriminating roles a passage retrieval system alone cannot fill, as we will see in the following. We instead present the evaluation of our system for the sole task of passage retrieval, on the 9 reading tests (46 questions) of the test set, following Tellex's quantitative evaluation methodology <ref type="bibr" coords="10,376.93,262.46,14.61,8.74" target="#b12">[13]</ref>. We first annotated passages of the test set (which 2-to-4-sentence passage must be read to answer the question) to create a gold standard. We found quite straight forward to limit those annotations to contiguous passages, with only 2 questions needing disjoint passages. We then implemented several runs:</p><p>-MITRE as a weak baseline: simple word overlap algorithm <ref type="bibr" coords="10,410.70,333.55,15.50,8.74" target="#b13">[14]</ref> -SiteQ as a strong baseline: sentences are weighted based on query term density <ref type="bibr" coords="10,187.07,358.30,14.61,8.74" target="#b9">[10]</ref>, and include keyword forms such as lemmas, stems, and synonyms/hyponyms from WordNet synsets. -SI(d max ), our Simple English Wiktionary-based indexing system, parameterized by d max We used the following measures:</p><p>-MRR: mean reciprocal rank p@n: number of correct passages found in the top n nf: number of correct passages which weren't found at all Results are shown in table 4. Our system outperforms both baselines significantly on all types of tasks and measures. The difference is most noticeable when the systems do not have access to choices of answers, which is really what we seek for the broader view of question answering. What is also interesting is the increase in performance for SI as we increase the maximum depth of search in the dictionary. This seems to confirm that Simple English Wiktionary fits this task well and that our score functions scale correctly with the amount of knowledge that it provides. Furthermore, although the question paired with the correct answer seems to yield a more reliable passage selection compared to with an incorrect answer, it is not by much, so it is unlikely that we could differentiate right and wrong answers by only looking at the passages they yield. It can be explained by the relatively high difficulty of the test: no answer choice seems completely absurd and is always related in some way to the relevant passage in the text. This confirms the wellknown necessity of deeper answer processing to make the final call, which our earliest run attempt lacked.</p><p>Algorithm MRR p@1 p@3 p@5 p@10 nf Question alone MITRE 0.215 0.13 0.20 0.26 0.37 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion and perspectives</head><p>This paper describes different experiments we conducted for QA4MRE 2013. We worked on two problems. The purpose of the first one was answering complex questions by recognizing discourse relations. The categorization of questions shows very good results while discourse relation recognition results allow us to see that this approach merits further consideration. Thus we will work on the improvement of this module and the integration of this criterion for selecting an answer. The second problem we studied was passage retrieval, especially for answering entrance exams, as semantic distance between questions, answers and text are important. We proposed indexing passages with expansion of question and answer words computed by accounting for recursive definition of words in a dictionary. This module shows good results. We now have to evaluate this approach on the other tasks and improve answer selection within best passages.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,234.01,490.83,147.34,7.89;2,151.77,242.05,311.81,234.01"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Architecture of QALC4MRE</figDesc><graphic coords="2,151.77,242.05,311.81,234.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,138.97,632.18,341.62,9.65;7,151.70,644.13,87.01,9.65;7,138.97,656.12,242.89,8.74"><head></head><label></label><figDesc>2. run INDEX(w c , d -1) if needed (d &gt; 1 and w c not indexed), for each child w c in the definition. 3. store the pointers to words of the definition in T (w).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,138.97,118.99,214.83,9.65"><head>4 .</head><label>4</label><figDesc>add (doc ancestor, d max -d) to each Anc(w c ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,162.16,117.78,291.03,178.61"><head>Table 2 .</head><label>2</label><figDesc>Recognition of relations in terms of precision and recall</figDesc><table coords="6,162.16,117.78,291.03,167.70"><row><cell></cell><cell>-</cell><cell cols="4">Correct Loose Incorrect Missed</cell></row><row><cell></cell><cell>Causality</cell><cell>39</cell><cell>48</cell><cell>4</cell><cell>86</cell></row><row><cell></cell><cell>Opinion</cell><cell>23</cell><cell>19</cell><cell>12</cell><cell>50</cell></row><row><cell></cell><cell cols="2">Definition 50</cell><cell>13</cell><cell>14</cell><cell>51</cell></row><row><cell></cell><cell>Examples</cell><cell>22</cell><cell>10</cell><cell>2</cell><cell>14</cell></row><row><cell></cell><cell>Total</cell><cell>134</cell><cell>90</cell><cell>32</cell><cell>201</cell></row><row><cell></cell><cell cols="5">Table 1. Recognition of relations</cell></row><row><cell>-</cell><cell cols="5">Strict Precision Strict Recall Lenient Precision Lenient Recall</cell></row><row><cell>Causality</cell><cell>0.429</cell><cell>0.225</cell><cell></cell><cell>0.956</cell><cell>0.503</cell></row><row><cell>Opinion</cell><cell>0.426</cell><cell>0.250</cell><cell></cell><cell>0.778</cell><cell>0.457</cell></row><row><cell>Definition</cell><cell>0.649</cell><cell>0.439</cell><cell></cell><cell>0.818</cell><cell>0.553</cell></row><row><cell>Examples</cell><cell>0.647</cell><cell>0.478</cell><cell></cell><cell>0.941</cell><cell>0.696</cell></row><row><cell>Total</cell><cell>0.523</cell><cell>0.315</cell><cell></cell><cell>0.875</cell><cell>0.527</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,140.62,308.40,334.11,85.12"><head>Table 3 .</head><label>3</label><figDesc>Results on the 2012 corpora in term of number of right selected answers.</figDesc><table coords="9,152.50,308.40,310.36,64.25"><row><cell></cell><cell></cell><cell cols="2">Alzheimer Task</cell><cell></cell><cell>Main Task</cell></row><row><cell></cell><cell>freq</cell><cell>freqTop</cell><cell>maxSTop</cell><cell>freq</cell><cell>freqTop</cell><cell>maxSTop</cell></row><row><cell>P REP</cell><cell cols="2">10 / 0.25 14 / 0.367</cell><cell>-</cell><cell cols="3">60 / 0.382 56 / 0.370 62 / 0.395</cell></row><row><cell>TERp</cell><cell cols="6">11 / 0.288 15 / 0.393 9 / 0.225 50 / 0.318 50 / 0.330 51 / 0.325</cell></row><row><cell cols="7">treeEdit12 / 0.315 15 / 0.393 14 / 0.35 52 / 0.331 52 / 0.343 53 / 0.337</cell></row><row><cell>baseline</cell><cell></cell><cell>8 / 0.2</cell><cell></cell><cell></cell><cell>32 / 0.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="11,145.38,140.47,324.59,151.90"><head>Table 4 .</head><label>4</label><figDesc>Evaluation of passage retrieval on QA4MRE 2013 Entrance exam task</figDesc><table coords="11,425.11,140.47,9.43,7.89"><row><cell>13</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,142.96,536.79,337.64,7.86;11,151.52,547.74,329.07,7.86;11,151.52,558.70,71.41,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,170.93,547.74,155.43,7.86">Adaptation of limsi&apos;s qalc for qa4mre</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Grau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">M</forename><surname>Pho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">L</forename><surname>Ligozat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zweigenbaum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,354.71,547.74,125.88,7.86;11,151.52,558.70,36.64,7.86">CLEF 2012 Working notes on QA4MRE</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,569.53,297.99,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,222.54,569.53,116.63,7.86">Learning Question Classifiers</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,360.89,569.53,46.25,7.86">COLING&apos;02</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,580.35,284.87,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,211.41,580.35,122.71,7.86">Question classification transfer</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">L</forename><surname>Ligozat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,356.03,580.35,38.93,7.86">ACL 2013</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,591.18,337.63,7.86;11,151.52,602.14,329.07,7.86;11,151.52,613.10,329.07,7.86;11,151.52,624.06,60.92,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,268.76,591.18,124.30,7.86">Accurate unlexicalized parsing</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,420.65,591.18,59.94,7.86;11,151.52,602.14,329.07,7.86;11,151.52,613.10,45.74,7.86">Proceedings of the 41st Annual Meeting on Association for Computational Linguistics -Volume 1. ACL &apos;03</title>
		<meeting>the 41st Annual Meeting on Association for Computational Linguistics -Volume 1. ACL &apos;03<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,634.88,337.63,7.86;11,151.52,645.84,329.07,7.86;11,151.52,656.80,192.50,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,244.26,634.88,236.33,7.86;11,151.52,645.84,78.90,7.86">Tregex and Tsurgeon: tools for querying and manipulating tree data structures</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,253.84,645.84,226.74,7.86;11,151.52,656.80,159.73,7.86">Proceedings Fifth international conference on Language Resources and Evaluation (LREC 2006)</title>
		<meeting>Fifth international conference on Language Resources and Evaluation (LREC 2006)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,119.67,337.64,7.86;12,151.52,130.61,329.07,7.89;12,151.52,141.59,218.98,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,252.91,119.67,195.81,7.86">LIBSVM: A library for support vector machines</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/cjlin/libsvm" />
	</analytic>
	<monogr>
		<title level="j" coord="12,458.84,119.67,21.75,7.86;12,151.52,130.63,209.39,7.86">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,152.55,337.63,7.86;12,151.52,163.51,247.61,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,202.03,152.55,278.56,7.86;12,151.52,163.51,14.98,7.86">Improvements in Part-of-Speech Tagging with an Application to German</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,189.65,163.51,175.73,7.86">Proceedings of the ACL SIGDAT-Workshop</title>
		<meeting>the ACL SIGDAT-Workshop</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,174.47,337.64,7.86;12,151.52,185.43,284.91,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,282.68,174.47,197.92,7.86;12,151.52,185.43,38.08,7.86">Index expansion for machine reading and question answering</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Attardi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Atzori</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Simi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,212.00,185.43,186.47,7.86">CLEF (Online Working Notes/Labs/Workshop</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,196.39,337.64,7.86;12,151.52,207.34,329.07,7.86;12,151.52,218.30,329.07,7.86;12,151.52,229.26,329.07,7.86;12,151.52,240.22,60.92,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,372.07,196.39,108.52,7.86;12,151.52,207.34,165.52,7.86">Feature-rich part-of-speech tagging with a cyclic dependency network</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,338.63,207.34,141.97,7.86;12,151.52,218.30,329.07,7.86;12,151.52,229.26,122.96,7.86">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,251.18,337.98,7.86;12,151.52,262.14,203.63,7.86;12,413.73,262.14,66.87,7.86;12,151.52,273.10,329.07,7.86;12,151.52,284.06,249.39,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,258.34,262.14,96.81,7.86;12,413.73,262.14,66.87,7.86;12,151.52,273.10,201.94,7.86">Siteq: Engineering high qa system using lexico-semantic pattern matching and shallow nlp</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">K</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,377.78,273.10,102.82,7.86;12,151.52,284.06,140.34,7.86">Proceedings of the Tenth Text REtrieval Conference (TREC</title>
		<meeting>the Tenth Text REtrieval Conference (TREC</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="volume">442</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,295.02,211.84,7.86" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="12,250.45,295.02,104.01,7.86">Simple english wiktionary</title>
		<imprint>
			<publisher>Wikimedia Foundation</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,305.98,337.98,7.86;12,151.52,316.93,329.07,7.86;12,151.52,327.89,329.07,7.86;12,151.52,338.85,329.07,7.86;12,151.52,349.81,192.86,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,235.96,305.98,244.63,7.86;12,151.52,316.93,84.54,7.86">Limsiiles: Basic english substitution for student answer assessment at semeval 2013</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gleize</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Grau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,257.63,316.93,222.97,7.86;12,151.52,327.89,72.31,7.86;12,274.42,327.89,206.17,7.86;12,151.52,338.85,162.20,7.86">Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013)</title>
		<meeting>the Seventh International Workshop on Semantic Evaluation (SemEval 2013)<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-06">June 2013</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="598" to="602" />
		</imprint>
	</monogr>
	<note>Second Joint Conference on Lexical and Computational Semantics (*SEM)</note>
</biblStruct>

<biblStruct coords="12,142.62,360.77,337.98,7.86;12,151.52,371.73,329.07,7.86;12,151.52,382.69,329.07,7.86;12,151.52,393.65,172.34,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,384.92,360.77,95.67,7.86;12,151.52,371.73,223.38,7.86">Quantitative evaluation of passage retrieval algorithms for question answering</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tellex</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Marton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,403.19,371.73,77.41,7.86;12,151.52,382.69,329.07,7.86;12,151.52,393.65,89.17,7.86">Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval</title>
		<meeting>the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="41" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,404.61,337.97,7.86;12,151.52,415.54,329.07,7.89" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,347.59,404.61,133.00,7.86;12,151.52,415.56,120.65,7.86">Analyses for elucidating current question answering technology</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Light</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Breck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,281.00,415.56,123.87,7.86">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page">325</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
