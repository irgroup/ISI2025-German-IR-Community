<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,158.64,142.31,294.11,11.85;1,193.08,158.75,225.36,11.85">Using Anaphora resolution in a Question Answering system for Machine Reading Evaluation</title>
				<funder ref="#_8sNnFT9">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,222.48,193.15,50.35,10.37"><forename type="first">Adrian</forename><surname>Iftene</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UAIC</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,280.68,193.15,45.55,10.37"><forename type="first">Alex</forename><surname>Moruz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UAIC</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computer Science</orgName>
								<orgName type="institution">Romanian Academy Iasi Branch</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,340.20,193.15,45.56,10.37"><forename type="first">Eugen</forename><surname>Ignat</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UAIC</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,158.64,142.31,294.11,11.85;1,193.08,158.75,225.36,11.85">Using Anaphora resolution in a Question Answering system for Machine Reading Evaluation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">89D989CCB4A9D700825BBE6738C76DE6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Question Answering for Machine Reading Evaluation</term>
					<term>Information Retrieval</term>
					<term>Textual Entailment</term>
					<term>Anaphora Resolution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes UAIC 1 's Question Answering for Machine Reading Evaluation systems participating in the QA4MRE 2013 evaluation task. We submitted two types of runs, both type of runs based on our system from 2012 edition of QA4MRE, and both used anaphora resolution system. Differences come from the fact the textual entailment component was used or not. The results offered by organizer showed that runs based on textual entailment component were better.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As in the 2012 campaign, the Question Answering for Machine Reading Evaluation (QA4MRE <ref type="foot" coords="1,184.92,427.46,2.81,6.25" target="#foot_1">2</ref> ) task in 2013 intends to cross-evaluate the ability of systems to read and understand texts <ref type="foot" coords="1,206.40,438.26,2.81,6.25" target="#foot_2">3</ref> . The systems involved in this task must have ability on reading single documents and to identify the correct answer from a set of five multiple choice answers, using different kinds of inferences and previously acquired background knowledge. The background knowledge is based on 2012 collection and it is related to four topics: AIDS, Climate Change, Music and Society, Alzheimer <ref type="bibr" coords="1,405.84,481.03,9.94,10.37" target="#b0">[1]</ref>. In 2013 was involved 5 different languages (Arabic, Bulgarian, English, Romanian and Spanish), and the test data was the same (parallel translations) and the background knowledge was available to all participants. In comparison with previous editions this year appear two main differences: <ref type="bibr" coords="1,233.16,524.23,11.03,10.37" target="#b0">(1)</ref> was inserted questions based on modality and negation aspects, and (2) a portion of questions have no correct answer and the correct option was "none of above".</p><p>The system used by our group in 2013 QA4MRE edition is an improved version of the system used in 2012 QA4MRE edition <ref type="bibr" coords="1,314.76,567.55,10.03,10.37" target="#b1">[2]</ref>. The system from 2012 was further improved by adapting an anaphora resolution component for the Question Answering module.</p><p>The rest of the paper is structured as follows: Section 2 details the general architecture of our Question Answering system for Machine Reading Evaluation and the new textual entailment module, Section 3 presents the results and an error analysis, while the last Section discusses the conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System components</head><p>In QA4MRE 2013, UAIC submitted runs only for Romanian. For that, we use the system from the previous edition of QA4MRE 2012 <ref type="bibr" coords="2,362.64,243.43,10.12,10.37" target="#b1">[2]</ref>, consisting in modules specialized for test data processing, background knowledge indexing, snippet extraction, identification of the correct answer and textual entailment. In addition in pre-processing part this year we used the anaphora resolution component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The base architecture</head><p>In 2013, the Romanian background knowledge was based on version from 2012 and it consisted of a collection of 184,263 documents in text format (32,631 correspond to the AIDS topic, 19,190 to Alzheimer topic, 63,207 to Climate Change topic and 92,268 to Music and Society topic). The test data consists in an XML file with 16 test documents (4 documents for each of the four topics), 15 to 20 questions for each document (284 questions in total) and 5 possible answers for each question (1,420 possible answers in total).</p><p>The base architecture is similar to the system used for the 2012 edition of the QA4MRE competition, presented in <ref type="bibr" coords="2,297.12,415.03,9.94,10.37" target="#b1">[2]</ref>. Thus, after indexing the background collection using Lucene<ref type="foot" coords="2,235.32,424.42,3.04,6.75" target="#foot_3">4</ref> libraries <ref type="bibr" coords="2,276.00,425.83,9.94,10.37" target="#b6">[7]</ref>, the system processes the test data applying 3 operations: (a) extracting documents from the background knowledge, (b) analyzing the test questions and (c) processing possible answers. If the first step is performed using Lucene indexing of the background collection, for analyzing the question we used our question processing module <ref type="bibr" coords="2,293.88,469.03,10.79,10.37" target="#b1">[2]</ref> and the web services available from the Sentimatrix<ref type="foot" coords="2,187.08,480.38,2.81,6.25" target="#foot_4">5</ref> project <ref type="bibr" coords="2,222.36,479.83,10.79,10.37" target="#b4">[5]</ref> to eliminate stop words, perform lemmatization and identify the Named Entity in the question. Then, a Lucene query is built. For instance, in the case of the question with q_id = "1": Ro: Cu ce scop unii cobai au fost injectați cu gene umane care provoacă Alzheimer?</p><p>En: For what purpose were some mice injected with human genes that cause Alzheimer's? the execution of the above steps has the following results:</p><p>-in the first step, the following stop words are eliminated: -in the next step, lemmas for the words injectați, gene, umane, provoacă (En: injected, genes, human, cause) are identified; -in the third step, Alzheimer is identified as a Named Entity; -in the last step, the Lucene query is build:</p><formula xml:id="formula_0" coords="3,170.88,177.81,297.39,28.85">" scop cobai (injectați^2 injecta) (gene^2 geană) (umane^2 uman) (provoacă^2 provoca) Alzheimer^3".</formula><p>From the above Lucene query, one can notice that we consider named entities to be of most relevance (hence receiving a boost of 3, expressed as using the ^ operator), while the inflected form of the words existing in the question receive a lower boost value (2 in the example above).</p><p>Another module analyzes the possible answers types and features, using the ontology presented in <ref type="bibr" coords="3,229.80,272.59,9.94,10.37" target="#b5">[6]</ref>, more specifically the relations between regions and cities and the relations between cities and countries, in order to eliminate the answers with low probability to be the required answer. For instance, for the question with q_id="14":</p><p>Ro: Ce ţară este liderul REDD în America? En: Which country is the leader of REDD in America?, we eliminate from the list of possible answers the answers with non-American states.</p><p>As presented in <ref type="bibr" coords="3,223.08,370.15,9.94,10.37" target="#b1">[2]</ref>, the index of background knowledge is queried, and all retrieved documents are placed in separate indexes. The results of this step are 284 separate indexes for every question from the initial test data. Then every index is searched for every answer, and a list of documents with Lucene relevance scores are returned, where Score(d, a) is the relevance score for document d when we search with the Lucene query associated to the answer a.</p><p>In 2013 we submitted two types of runs: (1) first without textual entailment module, and (2) second using textual entailment module. In the first case, after above steps a normalized value is computed for all answers associated to a question, and the answer with the highest value is selected as the most probable answer. In the second case, we perform the following three steps (i) we build a pattern with variables for every question according to the question type; (ii) using a pattern and all possible answers, we build a set with 5 hypotheses for each of the questions; (iii) we assign to the document tag from the initial XML file the role of text T and we run the TE system for all obtained pairs <ref type="bibr" coords="3,252.00,521.35,10.26,10.37" target="#b2">[3,</ref><ref type="bibr" coords="3,264.60,521.35,6.76,10.37" target="#b3">4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Anaphora resolution</head><p>For the QA4MRE 2013 besides the improved system from QA4MRE 2012, we added anaphora resolution based run. Anaphora resolution is defined as the process of resolving an anaphoric expression to the expression it refers to <ref type="bibr" coords="3,407.28,595.75,10.03,10.37" target="#b7">[8]</ref>. The UAIC developed tool that handles anaphora resolution is called RARE (Robust Anaphora Resolution Engine) and uses the work done in <ref type="bibr" coords="3,319.92,617.35,9.94,10.37" target="#b8">[9]</ref>, where the process uses three layers (Figure <ref type="figure" coords="3,173.28,628.15,3.47,10.37" target="#fig_0">1</ref>):</p><p>• The text layer, which contains referential expressions(RE's) as they appear in the discourse;</p><p>• An intermediate layer (projection layer) that contains any specific information that can be extracted from the corresponding referential expressions;</p><p>• A semantic layer that contains the descriptions of the discourse entities (DE). Here the information contributed by chains of referential expressions is accumulated. The core of the RARE system is language independent, yet to be able to localize it to one language it needs specific resources. These specific resources are:</p><p>• Constrains -a file that contains the rules which will match the conditions between anaphor and antecedent;</p><p>• Stopwords -a file that contains a common list of stop-words;</p><p>• Tagset -a file that contains the mapping from the tagset used in the input file to a simplified tagset used by the system;</p><p>• Window -a file that contains the length of the window where the antecedent should be looked by the system; the length is in tokens.</p><p>The process of anaphora resolution runs in the following chain: First the input is read from left to right; when a new noun phrase is found, a new referential expression (RE) is created which contains the morphologic, syntactic and semantic features; all these features are then tested by the rules defined in the constraints file and it is decided where this new RE defines a new discourse entity or it refers a before mentioned one, and finds which one. The system outputs chains of co-referential expressions. Each such chain is characterized by a feature structure that sums up all the features of the RE's present in the chain.</p><p>For this year's QA4MRE the use of an anaphora system was needed because the analysis from last year's results revealed there was a problem in missing too many possible good excerpts. This problem could be solved using an anaphora resolution engine, since it would be possible to find antecedent references and link them, giving the QA system a better chance of finding possible excerpts. Thus the process of using the anaphora resolution engine is as follows: extract the text from each input file; pass this text to a pre-processing chain that involves: sentence splitting, tokenization, pos-tagging and lemmatization, noun phrase chunking; after this pre-processing the resulted tagged data is passed to RARE, which outputs the co-reference chains; a post-processing tool comes and reads RARE's output, and changes the original input file, such as the references appear solver (change the referent with the antecedent); this new input file containing anaphora resolution is then passed to the QA system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results and Evaluation</head><p>For the QA4MRE 2013 task, our team submitted 5 runs, all of which were for the Romanian-Romanian language pair.</p><p>As was the case for the past QA4MRE editions, the evaluation of the results was carried out in two different manners: on a global level and on a document specific level. At the global level, the purpose of the evaluation is to provide a general measure for the quality of the system in a general setting, on any type of background knowledge. As an important note, the global evaluation does not include the evaluation of the auxiliary questions. At the document level, the purpose is to determine the value of the "c@1 measure", which is a description of how well a given text is "understood" by the QA system. The c@1 measure is also computed at the topic level. These results are then used to obtain statistical measures, such as the mean, average and standard deviation over values grouped by topic or as an overall view.</p><p>In the tables below we give the results obtained by four of our five submitted runs for Romanian, each representing a specific configuration of the system. In the case of the first two runs (C1 and C2), the system configuration included the Textual Entailment module. The difference between the runs is given by the difference in choosing the threshold for providing the "NOA" response. Our intent was to evaluate the impact of a more permissive configuration, which gives less "NOA" answers versus a more restrictive one. For the last two runs (C3 and C4), the Textual Entailment module was not used; the difference between them also comes from different NOA thresholds. The final run yielded identical results to the first one (using a similar architecture and different NOA thresholds) and is not included in the tables because of this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Evaluation at the question answering level</head><p>Table <ref type="table" coords="5,166.92,544.75,4.67,10.37" target="#tab_1">1</ref> below gives the results for the four runs described above. As can be seen in Table <ref type="table" coords="6,251.76,204.55,3.45,10.37" target="#tab_1">1</ref>, the best result of our system in terms of both overall C@1 measure and overall accuracy is obtained for the run in which the Textual Entailment module was used, together with a lower threshold for the unanswered questions (which leads to the system submitting more answers). The major drawback of the decreased threshold for submitting an answer is the fact the large majority of unanswered questions are, in fact, questions for which the system did not extract the correct answer. For example, decreasing the threshold for the C2 run meant that all the correct answers extracted by the system were actually submitted (22 answers) but that 85 more wrong answers were also submitted, greatly decreasing the general accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation at the reading test level</head><p>In Table <ref type="table" coords="6,178.56,354.43,3.45,10.37" target="#tab_2">2</ref>, we present the median and mean for the C@1 measure for each of the 4 topics, Topic1 (Alzheimer), Topic2 (Music and Society), Topic3 (Climate Change) and Topic4 (AIDS) and their overall values for the Ro-Ro runs. These results in term of average and median are consistent with the trend introduced in Table <ref type="table" coords="6,224.88,562.39,3.51,10.37" target="#tab_1">1</ref>. The best overall average was obtained on the second run, which was also the highest scoring in terms of overall accuracy and C@1 measure. As can be seen above, the lowest scoring topic (in the best scoring run) is the second one (Music and society), and we have chosen to carry out our error analysis on this topic alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Error analysis</head><p>In extension to the analysis carried out above, we have also performed an error analysis over the reported results. The analysis was carried out exclusively over the questions in topic 2 (the lowest scoring topic in terms of C@1 measure in the second run, which yielded the best results), and a report of the most relevant error sources is given below. In interpreting the analysis results, two important factors need to be taken into account:</p><p>Firstly, the analyzed run is obtained using the textual entailment enhanced system. In order to use the TE engine, we generated 5 hypotheses out of the initial question and its 5 potential answers (the potential answer was included in the hypothesis according to a set of patterns depending on the question and expected answer type). Secondly, the run for which we have chosen to carry out error analysis was obtained using a lower threshold for submitting an answer, which resulted in a large number of incorrect answers being reported.</p><p>A major source of errors for our system was the fact that we did not properly model the questions for which one of the answer variants was "none of the above". In those cases where the correct answer is note "none of the above", our system is slightly favored, because a query generated from this phrase scores very low because the keywords are rarely found in the target text. This is the case for question 3, reading 5 topic 2, Ro: În ce manieră se arată influenţa lui Beethoven în sonate? En: How is Beethoven's influence seen in the sonatas? However, in those cases where the correct answer is indeed "none of the above", our system has almost no chance to extract the correct answer. In the case of question 2, reading 5, topic 2, Ro: La ce dată a debutat Cramer ca dirijor? En: When did Cramer begin his conducting career? the query generated from the correct variant does not return any snippets. Because of this the score for this query is lowest, and the correct answer cannot be chosen. In order to prevent this type of error, we propose that if all the queries score less than a given threshold (which we need to find experimentally), the selected variant should be "none of the above".</p><p>One type of error which we have encountered frequently is due to the way in which we create two sets of queries in order to make use of the provided variants while searching for the correct answer. In the case of question 1, reading test 5, topic 2, the answer our system has selected was answer one, while the correct solution was number 4.</p><p>Ro: Cât sunt de renumite sonatele târzii ale lui Cramer în ziua de azi? En: How well known are Cramer's sonatas today?</p><p>Even though the initial query scores first, it is penalized by the secondary query, which scores third and thus misses the correct answer. The problem in such error cases usually stems from the fact that the correct answer as too few keywords, and therefore it is harder for such a query to score high enough to be picked. We propose boosting the scores of answer variants with fewer words in order to correct these types of errors.</p><p>Another type of common error is caused by the fact that, because of computational and time restrictions, we have chosen to not perform significant preprocessing on the indexed text. Because of this, especially in the case of the Romanian language, which is heavily inflected, some queries score extremely low because the words in the query are not in the same inflection as the words in the original text. This is the case for question 5, reading 5, topic 2:</p><p>Ro: Ce organizaţie l-a numit director pe Cramer în 1822? En: Which organization made Cramer its director in 1822?</p><p>where the original text contains the keywords in a different inflection: "Academia Regală de Muzică" vs. "Academiei Regale de Muzică". If this difference in inflection would have been detected, the system would have chosen the correct answer. To this end we are considering the preprocessing of all the background knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>This paper presents the updated Question Answering system developed by UAIC for the Machine Reading Evaluation task within CLEF 2012 labs. The presented systems were built starting from the main components of our QA systems (the question processing and information retrieval modules), but the multiple choice questions were addressed using a textual entailment component.</p><p>The evaluation shows a best overall median for all 4 topics of 0.22. We can observe the influence of the correctly unanswered questions in the C@1 measure when comparing the number of right answers for the best run (C2), with the run C1. Although in the C2 run, a higher number of questions were correctly answered (68 right answers) than in the C1 run (45 right answers), the C@1 measure obtained for the C1 run (0.23) is very close to C2 run (0.25). This is explained by the difference in the number of correctly unanswered questions: 24 for C1 and only 1 for C2.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,243.84,361.03,134.39,10.37"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Three layers structure<ref type="bibr" coords="4,367.32,361.03,10.91,10.37" target="#b8">[9]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,190.56,566.91,230.23,78.37"><head>Table 1 :</head><label>1</label><figDesc>Results of UAIC's Ro-Ro runs at question answering level</figDesc><table coords="5,204.24,585.51,200.78,59.78"><row><cell></cell><cell>C1</cell><cell>C2</cell><cell>C3</cell><cell>C4</cell></row><row><cell>Answered right</cell><cell>45</cell><cell>68</cell><cell>44</cell><cell>36</cell></row><row><cell>Answered wrong</cell><cell>117</cell><cell>202</cell><cell>211</cell><cell>149</cell></row><row><cell>Total answered</cell><cell>162</cell><cell>270</cell><cell>255</cell><cell>185</cell></row><row><cell>Unanswered right</cell><cell>24</cell><cell>1</cell><cell>7</cell><cell>15</cell></row><row><cell>Unanswered wrong</cell><cell>96</cell><cell>11</cell><cell>22</cell><cell>84</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,203.76,398.19,203.95,130.59"><head>Table 2 :</head><label>2</label><figDesc>Results of UAIC's Ro-Ro runs at reading test level</figDesc><table coords="6,208.56,416.67,197.42,112.12"><row><cell></cell><cell>C1</cell><cell>C2</cell><cell>C3</cell><cell>C4</cell></row><row><cell>Topic 1 median</cell><cell>0.21</cell><cell>0.22</cell><cell>0.14</cell><cell>0.14</cell></row><row><cell>Topic 2 median</cell><cell>0.14</cell><cell>0.17</cell><cell>0.17</cell><cell>0.17</cell></row><row><cell>Topic 3 median</cell><cell>0.19</cell><cell>0.22</cell><cell>0.25</cell><cell>0.26</cell></row><row><cell>Topic 4 median</cell><cell>0.25</cell><cell>0.28</cell><cell>0.07</cell><cell>0.08</cell></row><row><cell>Overall median</cell><cell>0.21</cell><cell>0.22</cell><cell>0.12</cell><cell>0.13</cell></row><row><cell>Topic 1 average</cell><cell>0.22</cell><cell>0.28</cell><cell>0.18</cell><cell>0.17</cell></row><row><cell>Topic 2 average</cell><cell>0.14</cell><cell>0.23</cell><cell>0.16</cell><cell>0.16</cell></row><row><cell>Topic 3 average</cell><cell>0.19</cell><cell>0.22</cell><cell>0.24</cell><cell>0.24</cell></row><row><cell>Topic 4 average</cell><cell>0.27</cell><cell>0.28</cell><cell>0.08</cell><cell>0.08</cell></row><row><cell>Overall average</cell><cell>0.22</cell><cell>0.25</cell><cell>0. 17</cell><cell>0.16</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,148.32,613.83,140.67,9.37"><p>University "Al. I. Cuza" of Iasi, Romania</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,148.32,623.79,171.98,9.37"><p>QA4MRE: http://celct.fbk.eu/QA4MRE/index.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="1,149.40,633.75,319.03,9.37;1,151.20,643.59,258.77,9.37"><p>QA4MRE Guidelines: http://celct.fbk.eu/QA4MRE/scripts/downloadFile.php?file=/websites/ ResPubliQA/resources/guideLinesDoc/main/QA4MRE2013_Guidelines.pdf</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,148.32,633.51,112.87,9.37"><p>Lucene: http://lucene.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="2,148.32,643.47,134.95,9.37"><p>Sentimatrix: http://www.sentimatrix.eu/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. The research presented in this paper was funded by the project <rs type="projectName">MUCKE (Multimedia and User Credibility Knowledge Extraction)</rs>, number <rs type="grantNumber">2 CHIST-ERA/01.10.2012</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_8sNnFT9">
					<idno type="grant-number">2 CHIST-ERA/01.10.2012</idno>
					<orgName type="project" subtype="full">MUCKE (Multimedia and User Credibility Knowledge Extraction)</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,146.37,254.43,321.94,9.37;9,156.00,264.15,312.29,9.37;9,156.00,273.87,312.28,9.37;9,156.00,283.59,174.77,9.37" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,249.36,264.15,218.93,9.37;9,156.00,273.87,98.54,9.37">Overview of QA4MRE at CLEF 2012: Question Answering for Machine Reading Evaluation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Forner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Á</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sutcliffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sporleder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Forascu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Benajiba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Osenova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,260.88,273.87,207.40,9.37;9,156.00,283.59,21.04,9.37">CLEF 2012 Evaluation Labs and Workshop Working Notes Papers</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-09-20">17-20 September, 2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,146.37,293.07,321.89,9.37;9,156.00,303.03,312.31,9.37;9,156.00,312.87,312.43,9.37;9,156.00,322.47,40.13,9.37" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,425.76,293.07,42.51,9.37;9,156.00,303.03,308.78,9.37">Enhancing a Question Answering system with Textual Entailment for Machine Reading Evaluation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Iftene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">L</forename><surname>Gînscă</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moruz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Trandabăț</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Husarciuc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Boroș</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,156.00,312.87,220.16,9.37">Notebook Paper for the CLEF 2012 LABs Workshop -QA4MRE</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-09-20">17-20 September. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,146.37,332.19,322.01,9.37;9,156.00,341.91,312.29,9.37;9,156.00,351.75,202.73,9.37" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="9,271.80,332.19,196.58,9.37;9,156.00,341.91,266.20,9.37">Textual Entailment on Romanian. The third Workshop on Romanian Linguistic Resources and Tools for Romanian Language Processing</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Iftene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Balahur-Dobrescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007-12-15">14-15 December. 2007</date>
			<biblScope unit="page" from="109" to="118" />
			<pubPlace>Iasi, Romania</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,146.37,361.47,322.01,9.37;9,156.00,371.19,312.41,9.37;9,156.00,380.79,312.31,9.37;9,156.00,390.63,312.31,9.37;9,156.00,400.35,54.89,9.37" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,246.48,361.47,206.20,9.37">Answer Validation on English and Romanian Languages</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Iftene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Balahur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,156.00,371.19,312.41,9.37;9,156.00,380.79,178.09,9.37">Evaluating Systems for Multilingual and Multimodal Information Access, 9th Workshop of the Cross-Language Evaluation Forum, CLEF 2008</title>
		<title level="s" coord="9,268.92,390.63,123.98,9.37">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Aarhus, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">September 17-19, 2008. 2009. 2009</date>
			<biblScope unit="volume">5706</biblScope>
			<biblScope unit="page" from="385" to="392" />
		</imprint>
	</monogr>
	<note>Revised Selected Papers</note>
</biblStruct>

<biblStruct coords="9,146.37,409.83,322.06,9.37;9,156.00,419.79,312.39,9.37;9,156.00,429.63,312.29,9.37;9,156.00,439.23,223.97,9.37" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,198.72,419.79,188.04,9.37">Sentimatrix -Multilingual Sentiment Analysis Service</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">L</forename><surname>Gînscă</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Boroș</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Iftene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Trandabăţ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Toader</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Corîci</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">A</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cristea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,403.44,419.79,64.95,9.37;9,156.00,429.63,312.29,9.37;9,156.00,439.23,69.37,9.37">Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis (ACL-WASSA2011)</title>
		<meeting>the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis (ACL-WASSA2011)<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">June 19-24. (2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,146.37,448.95,321.89,9.37;9,156.00,458.67,312.41,9.37;9,156.00,468.51,128.45,9.37" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,279.72,448.95,173.46,9.37">Named Entity Relation Mining Using Wikipedia</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Iftene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Balahur-Dobrescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,156.00,458.67,312.41,9.37;9,156.00,468.51,8.42,9.37">Proceedings of the Sixth International Language Resources and Evaluation (LREC&apos;08). 28-30</title>
		<meeting>the Sixth International Language Resources and Evaluation (LREC&apos;08). 28-30<address><addrLine>Marrakech, Morocco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-05">May. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,146.37,478.23,167.37,9.37" xml:id="b6">
	<monogr>
		<ptr target="http://lucene.apache.org/java/docs/" />
		<title level="m" coord="9,156.00,478.23,30.61,9.37">LUCENE</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,146.37,487.95,322.01,9.37;9,156.00,497.55,243.17,9.37" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,337.20,487.95,131.18,9.37;9,156.00,497.55,31.42,9.37">Anaphora Resolution Exercise -An Overview</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Orăsan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cristea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mitkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Branco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,202.68,497.55,91.72,9.37">Proceedings of LREC-2008</title>
		<meeting>LREC-2008<address><addrLine>Marrakech, Morocco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,146.37,507.39,322.01,9.37;9,156.00,517.11,312.31,9.37;9,156.00,526.83,67.61,9.37" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,243.12,507.39,169.73,9.37">An integrating framework for anaphora resolution</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cristea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">G</forename><surname>Dima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,428.04,507.39,40.34,9.37;9,156.00,517.11,81.64,9.37">Information Science and Technology</title>
		<meeting><address><addrLine>Bucharest</addrLine></address></meeting>
		<imprint>
			<publisher>Romanian Academy Publishing House</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="273" to="291" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
