<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,127.97,152.78,339.56,12.53;1,242.26,170.78,110.38,12.53">Question Answering System Using Query Expansion and Heuristic Features</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,173.59,209.89,69.25,9.07"><forename type="first">Yolanda</forename><surname>Sjafrizal</surname></persName>
							<email>yolanda.sjafrizal@ui.ac.id</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="laboratory">Information Retrieval Laboratory Lab</orgName>
								<orgName type="institution">Universitas Indonesia Depok</orgName>
								<address>
									<country key="ID">Indonesia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,248.98,209.89,41.12,9.07"><forename type="first">Indra</forename><surname>Budi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="laboratory">Information Retrieval Laboratory Lab</orgName>
								<orgName type="institution">Universitas Indonesia Depok</orgName>
								<address>
									<country key="ID">Indonesia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,297.18,209.89,124.48,9.07"><forename type="first">Mirnaadriani</forename><surname>Andphilip Arthur</surname></persName>
							<email>philip.arthur@ui.ac.id</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science</orgName>
								<orgName type="laboratory">Information Retrieval Laboratory Lab</orgName>
								<orgName type="institution">Universitas Indonesia Depok</orgName>
								<address>
									<country key="ID">Indonesia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,127.97,152.78,339.56,12.53;1,242.26,170.78,110.38,12.53">Question Answering System Using Query Expansion and Heuristic Features</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">213B09638BBC5E8E1BDB61E18D54EE04</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Question answering system</term>
					<term>query expansion</term>
					<term>pseudo relevance feedback</term>
					<term>WordNet</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the design of question answering system that participates in the maintask of QA4MRE at CLEF 2013. This system will initially perform preprocessing stage of the document and the documents related questions. Then, it identifies the type of questions in order to be able to search the answers with the most appropriate approach. In order to finding the answers, the system uses eight heuristics features and two query expansion techniques, namely Pseudo Relevance feedback and WordNet as the lexical database. There are nine types of runs were submitted at CLEF 2013, where the difference lies in the combination of query expansion techniques and features used. Evaluation will be based on the c@1 measure. Best results are obtained when it uses both query expansion techniques and all features with certain weights.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.2" lry="706.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.2" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.2" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.2" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.2" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.2" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.2" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.2" lry="841.92"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.2" lry="841.92"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.2" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>CLEF (Conference and Labs of the Evaluation Forum) is an organization that plays an important role in improving research and development in the field of information retrieval, by providing the infrastructure for a number of activities as well as a series of lab evaluations on information systems and a number of conferences. One lab evaluation that conducted by CLEF is QA4MRE (Question Answering for Machine Reading Evaluation), with the main objective is to develop a best methodology for machine reading system, which will be evaluated through a process of questioning and testing the level of understanding of a reading <ref type="bibr" coords="1,329.66,611.05,10.53,9.07" target="#b0">[1]</ref>. Based on these issues, we conducted a number of experiments to develop a question answering system based on a dataset of QA4MRE in 2011 and 2012. The main task of this system is to conduct a thorough understanding of the single document provided as reading material, and then answer a number of questions related to the document. Questions are provided in the form of multiple choices, which each have 5 answer choices.</p><p>This paper proposed two query expansion techniques, Pseudo Relevance Feedback <ref type="bibr" coords="2,124.85,162.13,10.70,9.07" target="#b1">[2]</ref>, and using WordNet as the lexical database. Beside of that, it is also uses eight heuristics features as that will give a score for each answer choice. To support the process of finding answers, the system also uses REVERB 1 to obtain the relation of the phrases of documents and snippets from Google snippets as additional knowledge.</p><p>In section 2 below, we describe the condition of dataset of QA4MRE at CLEF 2013, that used by this question answering system. Then, further explanation of the stages of the process will be described in section 3. The results of the system on a dataset of 2013 are described in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset QA4MRE at CLEF 2013</head><p>The dataset provided by QA4MRE at CLEF 2013, consisting of four main topics, namely AIDS, climate change, Music &amp; society and Alzheimer. A total of four pieces of reading test are provided at each such topic, which a reading test also consists of a single document with 10 questions and 5 answer choices for each question. So in general, for the dataset of 2013 there will be:</p><p> 16 test documents (4 documents for each topic)  240/320 questions (15/20 questions for each document)  1200/1600 choice answers (5 answer choices for each question)</p><p>Dataset of 2013 is provided in a number of languages, like Arabic, Bulgarian, English, Romanian and Spanish. For this experiment, its scope was limited to the dataset that use English only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3</head><p>Steps of The System</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pre-Processing</head><p>We conducted pre-processing on the dataset (document and the questions). The process are consists of 10 standard stages in the natural languages, such as convert the format of all numbers into a standard format, tokenization, named-entity recognition, filtering all punctuation characters that are considered useless, lowercasing, anaphora resolution, number recognition, lemmatization, removal of stopword, and processing negation. In negation processing, every word that has a meaning of negation, such as "not present" (containing the word "not"), changes to form of "not-present". We do this process so that the system can distinguish the word with its negation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Identification</head><p>At in this step, we identify the types of questions which are divided into factoid and non-factoid. The identification process based on the words that contains in the question, such as "what", "why", "where", "who", "when", "how", and "do", as well as the identifier at the right of the question words (if any), such as countries, time, year, many, and etc. In addition, there are also questions that do not contain the question word, so it is more shaped like a command line. Sample questions as follows:</p><p> "In what year she went to England? There is a question word "what" and identifier "year", so the question is identified as type of DATE.  "Who is the president?"There is a question word "who", so the question is identified as a type of PERSON.</p><p>Having this type of question has been identified; next, the question will be given tags to indicate what kind of answer is expected from the question. We uses eleven tags, which are: "DATE", "LOCATION,"PERSON","OPINION","NUMBER","YES_NO","MONEY","CAUS AL","METHOD","PURPOSE", and "WHICH_IS_TRUE ".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Passage Retrieval</head><p>Passaging process will form a small part or a unit of text that called passage, wherein each passage consisting of m pieces of sentences. As example, in the text there are n sentences,S1, S2, S3, ...,Sn. Of the entire sentence will be formed a passage p, each consisting of 6 sentences, where So from the passaging process, will obtain a collection of passage P, with the formula:</p><p>where k is number of passage that is formed from a text, m is number of sentences in a passage and n is number of sentences in the text. After the passaging process has been successfully carried out, we do the indexing process on the passages. We use Lucene<ref type="foot" coords="3,154.39,543.87,3.24,5.83" target="#foot_0">2</ref>  <ref type="bibr" coords="3,160.39,545.98,11.75,9.07" target="#b2">[3]</ref> to create the inverted-index.</p><p>After that, we do the retrieval process, getting the passages that contain the answers. In this system, we assumed that the top 13 passage that has the greatest similarity value (most relevant) to the query will be taken. This figure is obtained after seeing the accuracy of the system, when conducted experiments using the numbers 1 to 15. After that, the system looks for the appropriate answer based on the relevant passages retrieved. When it finds no answer to the question, then the system will perform query expansion technique in order to obtain a new set of passage that is more relevant to the query. Next subsection describes more on the query expansion technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.4</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Expansion</head><p>Query expansion is a process of adding a number of words that are relevant to a query, in order for the query to be more clear and unambiguous <ref type="bibr" coords="4,394.93,182.05,10.70,9.07" target="#b3">[4]</ref>. There are two types of query expansion methods that are implemented in this system, using PRF and WordNet <ref type="bibr" coords="4,164.71,206.05,10.53,9.07" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query expansion with Pseudo Relevance Feedback (PRF) .</head><p>This technique is based on the n-top passage that obtained using the original query prior to expansion. Each unique word from those passages will be sorted (ranked) based on its TF-IDF value <ref type="bibr" coords="4,237.40,272.07,10.53,9.07" target="#b4">[5]</ref>. Every word that has a TF-IDF value higher than the threshold will be acquired as an additional word to expand the initial query. Based on experiments that have been conducted on a number of possible threshold values and their influence on the final score, it is determined that the threshold used in this study was 9.5. TF-IDF method was chosen because it is believed to determine the words that have a strong connection with the initial passage obtained, so that with the addition of the words in the query is expected to obtain correctly relevant passages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query expansion with WordNet</head><p>WordNet is an online lexical database system, where every noun, verb, adjectives and adverbs will be grouped in a set of synonyms or often called synsets (Synonym sets) <ref type="bibr" coords="4,145.96,404.12,10.70,9.07" target="#b5">[6]</ref>. This system uses WordNet to find additional words in queries, which have a semantic relationship, such as hypernym, hyponym, similar adjective and troponym.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Answer Selection</head><p>To determine the answer to a question, it uses a number of feature to assess all the answer choices given (there are 5 pieces of possible answers to a question). When more than one feature is used, the score of an answer choice is accumulated from scores foreach feature. We select an option as the answer of the question which is has the highest final score. Contribution of each feature to score the answer choices vary depending on the weight of the feature. Here is the formula to assign a score to an option . is score of based on i-th feature, is weight of i-th feature,n is number of features to assess .</p><p>The system uses a threshold value in selecting the answer, if the difference between answer with the highest score and the score of the other four answer falls below the threshold, then it produces no-answer for the corresponding question.</p><p>Answer Selection for Factoid Question.</p><p>Finding answers of factoid questions only consisting of a single NE analysis feature which composing into four phases of assessment, which are:</p><p>1. Assessment using NE 2. Expansion of the candidate answers for questions of type DATE 3. Assessment using snippet 4. Assessment using average distance weight features and cosine similarity Initially, this conducts the first assessment phase. If the answer has been found at this stage, the process of finding the answer to a question will be stopped, and three other phases will not be executed. However, if there is finding no answer, then the system carries out the next phase until the fourth phase. When there is still finding no answer after four phases, then the system produce no-answer for the corresponding question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Assessment using NE.</head><p>The process of assessment procedures using NE as follows:</p><p>1. Take a set of passage that is relevant to the query 2. Check each passage, whether it has NE or not. When itis find NE, check the type of NE, whether it is similar to the NE of the answer choice. When the NE type of passage is similar tothe type of answer choice, then that NE will be selected to be recorded and given a score. 3. The score of each answer choice is accumulated. 4. The system chooses the answer choice that has the highest score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Expansion of The Candidate Answers for Questions of Type "DATE" .</head><p>For the questions of type "DATE", the procedure as follows:</p><p>1. If candidates containing the word "later" (eg "9 years later"), but all the answer options that exist in the form of year (eg "1985" or "1997"), then the candidates will be processed first in order to have the same form. 2. If the candidates is in the form of year, while the answer choices are in the range of years(such as "1950s" and the "beginning of 1900") then the candidates will be processed first in order to obtain the proper range.</p><p>Assessment Using Snippet.</p><p>The system carries out this phase using the search engine Google to acquire additional knowledge. Steps of this phase as follows:</p><p>1. Each answer choice is inserted as a query to Google, and the search results that associated with those answer choices will be retrieved. Five pieces of text snippets from the top search resultsare acquired to be an additional knowledge in the form of new documents related to the answer choices. These documents will be indexed with Lucene. 2. Perform step 1 again using keyword from question as the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>Score for an answer choice is obtained by accumulating the value of similarity between the questions and snippet belongs , and similarity values between and snippet belongs questions.</p><p>S 1 = Similarity values between the questions and snippet that belongs to . S 2 = Similarity values between and snippet that belongs to questions. 4. The system chooses the answer choice that has the highest Score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Assessment Using Average Distance Weight Features and Cosine Similarity.</head><p>This phase uses a combination of average distance weight (ADW) and cosine similarity features. Each of these features will file his own score for each answer choice.</p><p>So for each answer choice, scores of ADW and cosine similarity features will be added to the original score, thus forming a new score. The answer choice having the highest total score is selected as the answer for the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer Selection forNon-Factoid Question</head><p>For non-factoid type questions, selection answers process based on the following eight heuristic features. These features are given a weight based on its ability to determine the correct answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cosine Similarity.</head><p>This feature calculates the similarity between two documents based on the weight of each word contained in both documents. This feature implements the concept of vector space model (VSM), where the two documents will be represented as a 2dimensional vector n (n is the number of unique words), with its components is the weight of each word <ref type="bibr" coords="6,210.00,456.20,10.53,9.07" target="#b3">[4]</ref>. The similarity between the two documents will be calculated using the cosine similarity algorithm <ref type="bibr" coords="6,274.01,468.20,10.53,9.07" target="#b1">[2]</ref>.</p><p>The procedure of this feature as follows:</p><p>1. Taken 6 sets of passage (assumed as document), whereas one of them is relevant to the question, namely P, and the others are relevant to each choice answers, namely A,B,C,D and E. 2. Each document is represented as a vector, where the components are the weights of each word. For this study, the weight of each word is obtained by taking the value of term frequency using built-in functions provided by Lucene. 3. Finding the cosine value of the angle between two documents. Compute the similarity between document related to the question (P) and documents associated with each choice answers (A, B, C, D and E). The system chooses A, B, C, D or E which having the highest similarity score to P.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Average Distance Weight.</head><p>The ADW feature will calculate the smallest distance between each word in answer choice and each word in questions in a set of passage. Steps for the implementation of the ADW feature as follows:</p><p>1. Take a set of passage that is relevant to the first answer choice, then combine that into a paragraph. 2. Save all positions of each word in the question and positions of each word in the answer choice that is found in paragraph. 3. Attach each word in the answer choice with each word in the questions, then compute the smallest distance between the positions of both the paired words. 4. Having obtained the smallest distance for each pair of words, add all the distances and divide by the number of word pairs are formed. This outcome is a score for the first answer choice . 5. Do again steps 1-5 for the other four answer choices, in order to obtain a score for each answer choices.</p><p>Word Matching. This feature will calculate the score of each answer choice, based on the number of occurrences of its words on the set of the relevant passage (examined at each passage). The more the words found in the passages, or the more passage that containing the words is found, and then the score for the answer choice will be higher. Likewise, if the words of the answer choices found in the top ranked passage, then the score for this answer choice will also be higher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cluster Matching.</head><p>In this feature, each answer choice will be given a score based on the value of similarity/fitness between a set of passage that relevant to the answer choice, with a set of passage that relevant to the question. This feature is proposed under the assumption that the answer of the question, of course located adjacent to the questions in a same set passage, so that both set of passage are considered relatively similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Choice Tilling.</head><p>This feature will utilize the maximum number of words in the answer choice that can appear in a sequence in the sentence of passages, which is relevant to the question. The order of appearance of each word from an answer choice will be considered on a sentence in the top 3 passages. For example, for the question "How are people infected by HIV?", If the answer is option "through sexual intercourse", hence the word "through" will have the first position, "sexual" has a second position, and "intercourse" has the third position. When in a sentence in a passage only found the words "sexual" and "intercourse" appearing in sequence, in which the word "through" instead appear after those two words, then the score for this condition is 2. Do this for each sentence in the passage, accumulate the score and grab the highest value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Maximum occurence.</head><p>This feature has a similar function with the word matching feature; both features consider the number of occurrences of answer choice words in a set of relevant passage. The difference lies in the process of checking the appearance of the words. In the word matching feature, any word on the answer choice will be checked at the same time on one passage. So that for each passage, the score only be increased by the number of words in answer choices at the most, even though the words appear repeatedly in the passage. In this maximum feature, the occurrence of feature will be checked for each sentence in a passage. So, each time the words of that answer choice found in a sentence, then the score will increase by 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Maximum Similarity.</head><p>This feature using similarity functions in Lucene. At first, it takes sets of passage that relevant to each answer choice. Then, it computes the similarity score between an answer choice to every relevant passage (scores ranging from 0-1) using Lucene. The system chooses the answer choice which has the highest similarity score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Relation.</head><p>This feature works almost similar to the maximum similarity features; both use the Lucene functions to see the similarity between the answer choices with its relevant passages. This feature uses REVERB to obtain the index; meanwhile, the maximum similarity feature uses reading-test. By using REVERB, it obtains lists of phrases relation that related to the reading-test. Those relation phrases will be indexed by Lucene, resulting in a new index for this feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Evaluation</head><p>We use c@1 method to evaluate the performance of this system <ref type="bibr" coords="8,391.80,404.12,10.70,9.07" target="#b6">[7]</ref>. The concept of this evaluation method assumes that, instead of giving a wrong answer, the question is better left not have an answer, so the number of wrong answers can be reduced by maintaining the number of correct answers. The formula of evaluation method c@1 as follows:</p><p> is the number of questions that answered correctly by the system  is the number of questions that are not answered by the system  n is the total number of questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Result of experiment on 2013 datasets</head><p>The result of the experiment run describes in the Table  <ref type="table" coords="9,234.34,493.18,3.66,9.07">1</ref>, we can see that the system obtain optimal results when using the two query expansion techniques, along with 5 heuristics features, namely Word Matching, Question Choice Tilling, Semantic Relations, Cosine Similarity and ADW. Two proposed query expansion techniques, PRF and WordNet, both give a positive contribution to the system, especially when they are using together. Meanwhile, among the 8 existing features, features that provide a considerable influence on the performance of the system is 5 features that previously mentioned. The difference in the performance of the system when the weight of features is modified, also became evident that the determination of the appropriate weights for each existing feature, will greatly affect the system performance and the final score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Conclusions</head><p>Based on the problems posed by QA4MRE, this study has developed a question answering system, which will be used to understand the documents and answer multiple-choice questions that related to documents. To increase the number of relevant </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main Main+auxiliary</head><p>C@1 Features Run passages that is obtained, this study propose two query expansion techniques, namely PRF and WordNet. In the process of finding answers, the system proposed eight heuristic features that will give scores to each answer choice.</p><p>According to the analysis result that obtained in the dataset of 2013, the system will give the best performance when using both the query expansion techniques and all heuristic features with predetermined weights. The result of the final score of the overall system is still relatively low. One reason is indicated because the system does not use the background collection as additional knowledge, so knowledge is only obtained from the documents and snippets from Google. In addition, the system performance when dealing with non-factoid type questions are still not good, so the percentage of questions that answered incorrectly is still very high.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.8</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="9,259.26,176.86,5.02,9.88;9,282.33,176.86,5.02,9.88;9,305.39,176.86,5.02,9.88;9,328.45,176.86,51.79,9.88;9,398.29,176.86,5.02,9.88;9,421.35,176.86,5.02,9.88;9,444.40,176.86,34.69,9.88;9,150.54,190.01,329.83,9.88;9,150.54,203.19,81.32,9.88;9,259.26,203.19,221.12,9.88;9,150.54,216.35,85.93,9.88;9,254.64,216.35,225.73,9.88;9,150.54,229.53,80.64,9.88;9,254.64,229.53,225.73,9.88;9,150.54,242.68,25.95,9.88;9,254.64,242.68,225.74,9.88;9,150.54,255.86,34.48,9.88;9,254.64,255.86,225.73,9.88;9,150.54,269.02,56.98,9.88;9,254.64,269.02,225.74,9.88;9,150.54,282.19,66.19,9.88;9,254.64,282.19,225.74,9.88;9,145.94,295.35,58.96,9.88;9,254.64,295.35,225.74,9.88"><head></head><label></label><figDesc>W-P-M-C-Ma (0.75) 0.33 0.27 0.24 0.2 0.26 0.33 0.39 0.22 0.27 0.3 3 A-W-M-C-Ma (0.5) 0.3 0.25 0.22 0.2 0.24 0.3 0.38 0.21 0.25 0.29 4 A-W-M-C-Ma (0.75) 0.33 0.25 0.22 0.21 0.25 0.33 0.38 0.21 0.28 0.3 5 A-P-M-C-Ma(0.75) 0.36 0.25 0.23 0.15 0.25 0.36 0.38 0.22 0.23 0.3 6 A-W 0.34 0.22 0.25 0.19 0.25 0.34 0.35 0.23 0.25 0.29 7 A-P-W 0.33 0.23 0.26 0.19 0.26 0.33 0.36 0.25 0.25 0.3 8 A-W-M (0.5) 0.33 0.23 0.25 0.16 0.25 0.33 0.36 0.23 0.21 0.29 9 A-W-Ma (0.75) 0.33 0.25 0.24 0.2 0.25 0.33 0.34 0.23 0.26 0.29 10 A-W-Ma (1) 0.33 0.23 0.23 0.19 0.24 0.33 0.33 0.22 0.25 0.28</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,357.73,569.05,7.32,9.07"><head></head><label></label><figDesc>1.    </figDesc><table coords="9,136.13,317.22,334.79,185.03"><row><cell>Descrition of features:</cell></row><row><cell>A = Word Matching, Question Choice Tilling, Semantic Relation, Cosine</cell></row><row><cell>Similarity &amp; Average Distance Weight</cell></row><row><cell>W = Wordnet</cell></row><row><cell>P = Pseudo Relevance Feedback</cell></row><row><cell>M = Maximum Similarity</cell></row><row><cell>C = Cluster Matching</cell></row><row><cell>Ma = Maximum Occurrence</cell></row><row><cell>Description of topics:</cell></row><row><cell>1. Alzheimer</cell></row><row><cell>2. Music &amp; Society</cell></row><row><cell>3. Climate Change</cell></row><row><cell>4. AIDS</cell></row><row><cell>According to the Table</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="3,136.13,686.12,86.52,8.21"><p>http://lucene.apache.org</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,132.83,319.79,337.60,8.21;10,141.89,330.83,128.33,8.21" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="10,234.77,319.79,235.66,8.21;10,141.89,330.83,98.14,8.21">Overview of QA4MRE at CLEF 2011: Question answering for machine reading evaluation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
		<respStmt>
			<orgName>CLEF</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.83,341.87,338.09,8.21;10,141.89,352.91,164.85,8.21" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<title level="m" coord="10,369.26,341.87,101.66,8.21;10,141.89,352.91,31.04,8.21">Introduction to information retrieval</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2008-07">2008, July</date>
		</imprint>
	</monogr>
	<note>1 ed.</note>
</biblStruct>

<biblStruct coords="10,132.83,363.86,337.96,8.30;10,141.89,374.99,90.75,8.21" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="10,299.40,363.95,63.04,8.21">Lucene in action</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hatcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Gospodnetić</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Manning Publication Co</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.83,385.81,338.08,8.21;10,141.89,396.85,62.49,8.21" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="10,322.92,385.81,111.18,8.21">Modern information retrieval</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Addison-Wesley</publisher>
			<pubPlace>Inggris</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.83,407.89,337.65,8.21;10,141.89,418.93,71.58,8.21" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="10,281.74,407.89,164.72,8.21">Introduction to modern information retrieval</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Mcgill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>McGrawHill</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.83,429.97,337.80,8.21;10,141.89,441.01,163.86,8.21" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,292.86,429.97,174.25,8.21">WordNet: A lexical database for english</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,141.89,441.01,101.88,8.21">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="1995-11">1995, November</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.83,451.81,338.08,8.21;10,141.89,462.85,329.06,8.21;10,141.89,473.91,151.73,8.21" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,304.18,451.81,162.99,8.21">A simple measure to assess non response</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rodrigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,141.89,462.85,325.55,8.21">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1415">2011, June. 1415-1424</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
