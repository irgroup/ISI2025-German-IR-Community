<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,126.52,153.12,358.14,12.22;1,170.10,171.12,271.02,12.22">VENSES GetAsk: a System for Hybrid Question Answering And Answer Recovery using Text Entailment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,268.26,210.54,74.62,8.64"><forename type="first">Rodolfo</forename><surname>Delmonte</surname></persName>
							<email>delmont@unive.it</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Language Studies and Comparative Cultures</orgName>
								<orgName type="department" key="dep2">Department of Computer Science Ca&apos; Foscari</orgName>
								<orgName type="institution">University</orgName>
								<address>
									<postCode>1075, 30123</postCode>
									<settlement>Dorsoduro, VENEZIA</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,126.52,153.12,358.14,12.22;1,170.10,171.12,271.02,12.22">VENSES GetAsk: a System for Hybrid Question Answering And Answer Recovery using Text Entailment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3129A8F869B74AFC716B29DADA02A1D7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>syntactic and semantic processing</term>
					<term>predicate argument structures</term>
					<term>logical forms</term>
					<term>propositional level semantics</term>
					<term>semantic disambiguation 1</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a system that couples techniques belonging to Information Extraction and deep linguistic processing for Question Answering. The system presented in the paper has undergone extensive testing and the parser has been trained on available testsuites. The system uses text entailment processing to select best sentences to match with each question. Both sentences and questions need to parsed syntactically and semantically and a logical form has to be produced with predicate argument structures and propositional level analysis. In order to pick the right answer from a set of five, after extracting the best sentence/s from the text, we organized different strategies according to question type and semantic propositional type. The system has access to a wide range of computational lexica, ontologies and datasets to carry out the task: for common sense knowledge we used ConceptNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>We present a system for question answering that couples statistical processing and deep linguistic analysis, GetAsk, based on GETARUNS, the system for text understanding developed at the Ca' Foscari University of Venice. Like other similar systems (see in particular <ref type="bibr" coords="1,167.82,580.14,112.84,8.64">Bos et al. and Ahn et al.)</ref>, the architecture of the system is organized as a standard pipeline of interconnected modules: Text or Passage Analysis, Question Analysis, Answer Extraction and Reranking. There is no Document Retrieval phase in our case, since we are supposed to receive texts/passages already selected from a bigger set and supposedly containing the answer to the question.</p><p>Both text and question are analysed by our system for text understanding and the output of text analysis is recorded on file in a linear nonrecursive unscoped Logical Form format which is derived directly from the Situation Semantics representation that the system computes. In case of failure of the deep modules, the system still produces a Logical form directly from Dependency structure. Answer extraction and reranking is performed by means of three sequential and incremental filters or sieves:</p><p>-at first we use information retrieval methodologies -the second pass through the text is done by applying semantic similarity measures to the lemmata of sentences selected by the previous filter;</p><p>-eventually, we reinforce our previous choices by adding words selected on the basis of Logical Forms as they are computed from question analysis and text analysis.</p><p>More details of the whole system in the sections below. We present GETARUNS at first and then the backoff system that runs before and after the deep parser, in order to recover from possible failures; then in section 3 we present the deep method to compute answers directly from the Discourse Model; in section 4 the hybrid version of the system, where we comment in detail on QA4MRE dataset; in section 5 we report some evaluation and we end up with some conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>The System VENSES GETARUNS GETARUNS is organized into three subsystems. Venses is the shallow or partial version of GETARUNS: it is fully bottom-up and is responsible for tagging and chunking and produces a full-fledged syntactic and semantic analysis in case the second system fails. System two is the main deep system: it is organized in two versions. The first version runs fully top-down and the second one on the contrary runs bottom-up. They have access to the same rules which however are taken in a strict top-down order, by the first system; whereas the second system has a bottom-up access to the rules by means of a recursive procedure that is triggered by the current string and information coming from shallow analysis. The output structure is an annotated c-structure which is interpreted by the same Lexical Semantic interpretation module described below. GETARUNS switches to the second bottom-up deep system whenever there is a failure in the top-down parser, or when the sentence to be parsed is longer than 50 tokens. It switches to the "shallow" version in case of failure of the deep system as a whole. Also the "shallow" system produces a semantic representation which is partially coincident with the one produced by the deep system. "Partially" here means that only essential semantic modules are activated: semantic roles assignment; pronominal binding and anaphora resolution; logical form creation. No spatiotemporal resoning is present in the partial system, nor quantifier raising.</p><p>The deep system is equipped with three main modules: a lower module for parsing where sentence strategies are implemented; a middle module for semantic interpretation and discourse model construction which is cast into Situation Semantics; and a higher module where reasoning and generation may take place.</p><p>The system is based on LFG (Lexical-Functional Grammar) theoretical framework and has a highly interconnected modular structure. The Closed Domain version of the system is a top-down depth-first DCG-based parser written in Prolog Horn Clauses, which uses a strong deterministic policy by means of a lookahead mechanism. A second version of the same set of rules is activated in case of failure, but with a bottomup schema. The output of this second pass is then submitted to the same interpretation module that checks for grammaticality on the basis of lexical subcategorization information made available by any of the currently available computational lexica. Eventually, in case of failure of this second pass, the system derives an interpretation from the "shallow" or partial parse computed as a starting pass, where also tagging takes place, and Head-Dependent structures are built for further use. In fact, the output of this parser is used by the bottomup deep parser to detect the presence of a verbal constituent while recursively consuming the input string.</p><p>The system is divided up into a pipeline of sequential but independent modules which realize the subdivision of a parsing scheme as proposed in LFG theory. We build a cstructure before the f-structure can be projected by unification into a DAG (Direct Acyclic Graph) -however we map c-structures to DAG using Prolog unification. In this sense we try to apply in a given sequence phrase-structure rules as they are ordered in the grammar: whenever a syntactic constituent is successfully built, it is checked for semantic consistency. In case the governing predicate expects obligatory arguments to be lexically realized they will be searched and checked for uniqueness and coherence as LFG grammaticality principles require.</p><p>Syntactic and semantic information is accessed and used as soon as possible: in particular, both categorial and subcategorization information attached to predicates in the lexicon is extracted as soon as the main predicate is processed, be it adjective, noun or verb, and is used to subsequently restrict the number of possible structures to be built. Adjuncts are computed by semantic compatibility tests on the basis of selectional restrictions of main predicates and adjuncts heads. The subdivision of arguments and adjuncts is guided by available lexica, and ambiguity is solved by frequency counts associated to Verb or Noun argument/adjunct taken from Penn Treebank.</p><p>The grammar is equipped with a core lexicon containing most frequent 5000 fully specified inflected word forms where each entry is followed by its lemma and a list of morphological features, organised in the form of attribute-value pairs. However, morphological analysers for English are also available with big root dictionaries (25,000 for English) which only provide for syntactic subcategorization, though. In addition to that there are all lexical form provided by a fully revised version of COMLEX, and in order to take into account phrasal and adverbial verbal compound forms, we also use lexical entries made available by UPenn and TAG encoding. Their grammatical verbal syntactic codes have then been adapted to our formalism and are used to generate a subcategorization schemes with an aspectual and semantic class associated to it -however no selctional restrictions can reasonably be formulated on arguments of predicates. Semantic inherent features for Out of Vocabulary Words, be they nouns, verbs, adjectives or adverbs, are provided by a fully revised version of WordNet -plus EuroWordnet, with a number of additions coming from additional specialized semantic fields like computer, economics, and advertising -in which we used 75 semantic classes similar to those provided by CoreLex.</p><p>When each sentence is parsed, tense aspect and temporal adjuncts are accessed to build the basic temporal interpretation to be used by the temporal reasoner. Eventually two important modules are fired: Quantifier Raising and Pronominal Binding. QR is computed on f-structure which is represented internally as a DAG. It may introduce a pair of functional components: an operator where the quantifier can be raised, and a pool containing the associated variable where the quantifier is actually placed in the f-structure representation. This information may then be used by the following higher system to inspect quantifier scope. Pronominal binding is carried out at first at sentence internal level. DAGs will be searched for binding domains and antecedents matched to the pronouns if any to produce a list of possible bindings. Best candidates will then be chosen. After these modules have been successfully fired, the f-structure is completed and cannot undergo further changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Upper Module</head><p>GETARUNS, has a common (for both versions of the deep system) linguistically based semantic module which is used to build up the Discourse Model. Semantic processing is strongly modularized and distributed amongst a number of different sub-modules which take care of Spatio-Temporal Reasoning, Discourse Level Anaphora Resolution, and other subsidiary processes like Topic Hierarchy which cooperate to find the most probable antecedent of coreferring and cospecifying referential expressions when creating semantic individuals. These are then asserted in the Discourse Model (hence the DM), which is then the sole knowledge representation used to solve nominal coreference. The system uses two resolution submodules which work in sequence: they constitute independent modules and allow no backtracking. The first one is fired whenever a free sentence external pronoun is spotted; the second one takes the results of the first sub-module and checks for nominal anaphora. They have access to all data structures contemporarily and pass the resolved pair, anaphor-antecedent to the following modules. Semantic Mapping is performed in two steps: at first a Logical Form is produced which is a structural mapping from DAGs onto unscoped well-formed formulas. These are then turned into situational semantics informational units, infons which may become facts or sits. Each unit has a relation, a list of arguments which in our case receive their semantic roles from lower processing -a polarity, a temporal and a spatial location index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Incremental Shallow-to-Deep Parsing</head><p>The so-called shallow or partial module, is rather generic. As in most shallow parsers, we use a sequence or cascade of transducers: however, in our approach, since we intend to recover sentence level structure, the process goes from partial parses to full parses. Sentence and then clause level is crucially responsible for the right assignment of arguments and adjuncts to a governing predicate head. This is clearly paramount in our scheme which aims at recovering predicate-argument structures, besides performing a compositional semantic translation of each semantically headed constituent.</p><p>3 Hybrid Question-Answering</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">State of the art and our approach</head><p>When compared to our approach, totally shallow IR/IE approaches will always be lacking sufficient information for semantic processing at propositional level: in other words, as happens with our "Partial" modality, there will be no possibility of checking for precision in producing predicate-argument structures. Most systems would use some Word Matching algorithm that counts the number of words that appear in both the question and the sentence being considered after stripping stopwords: usually two words will match if they share the same morphological root after some stemming has taken place. Most QA systems presented in the literature rely on the classification of words into two classes: function and content words. They don't make use of a Discourse Model where input text has been transformed via a rigorous semantic mapping algorithm: they rather access tagged input text in order to sort best match words, phrases or sentences according to some matching scoring function (see the TREC QA series on NIST website). It is also common knowledge the fact that only by introducing or increasing the amount of linguistic knowledge over crude IR-based systems will contribute substantial improvements. In particular, systems based on simple Named-Entity identification tasks are too rigid to be able to match phrase relations constraints often involved in a natural language query. First objection is the impossibility to take into account pronominal expressions, their relations and properties as belonging to the antecedent, if no head transformation has taken place during the analysis process. Second objection is the use of grammatical function labels, like SUBJ/OBJects without an evaluation of their relevance in the utterance structure: higher level or main clause SUBJ/OBJects are more important than other SUBJects. In addition, there is no attempt at semantic role assignment which would come from a basic syntactic/semantic tagging of governing verbs: a distinction into movement verbs, communication verbs, copulative verbs, psychic verbs etc. would suffice to assign semantic roles to main arguments if present. It is usually the case that QA systems divide the question to be answered into two parts: the Question Target represented by the wh-word and the rest of the sentence; otherwise the words making up the yes/no question and then a match takes place in order to identify most likely answers in relation to the rest/whole of the sentence except for stopwords. However, it is just the semantic relations that need to be captured and not only the words making up the question that matter. Some system implemented more sophisticated methods (notably <ref type="bibr" coords="6,197.79,247.26,136.22,8.64">Hovy et al.; Litkowski; Bos et al.)</ref>: syntactic-semantic question analysis. This involves a robust syntactic-semantic parser to analyse the question and candidate answers, and a matcher that combines word-and parse-tree-level information to identify answer passages more precisely. More closely related to our approach are two systems that we shall comment here below. The first one is presented in Dan Moldovan et al. and is the LCC system called PowerAnswer. As the authors comment, it obtained a confidence weighted score of 0.85% on a dataset of 500 questions at TREC QA 2002. In their introduction the authors present the component of their system combines syntactic, semantic, lexical and world knowledge information sources <ref type="bibr" coords="6,209.78,355.26,71.29,8.64">(Moldovan et al.)</ref>. Questions and relevant document paragraphs are transformed into logical forms that together with world knowledge axioms extracted from WordNet glosses are fed to a logic prover <ref type="bibr" coords="6,314.37,379.26,73.96,8.64">(Moldovan et al.)</ref>. In order to allow the syntactic parser to work in a reasonable time they feed it with only relevant text excerpts that have been previously extracted by a summarization system. They also do coreference resolution by equating definite expressions with their antecedent in case it is a personal proper name; but also other more complex forms of coreference involving indefinite and definite noun phrase and pronoun coreference have been implemented <ref type="bibr" coords="6,414.42,439.26,70.29,8.64">(Moldovan et al.)</ref>. The output LF is then passed to a theorem logic prover that checks the result. The main difference with our approach lies in the fact that they produce a shallow syntactic analysis and only after that they start introducing logic constraints. On the contrary, we use all possible constraints at the moment of semantic mapping from syntactic structure which in our case is never shallow -not just considering surface structure but introducing all relevant missing and implicit arguments.</p><p>If we look at the other approach presented by <ref type="bibr" coords="6,319.65,523.26,73.18,8.64">Barker et al. 2007</ref>, we see that the same surface level syntactic -dependency-based -analysis is produced before mapping into logical forms. Their system introduces special axioms to take care of domain world knowledge, and some general semantic definition, as for instance, translating plural noun phrases into sets. The output LF is then passed on to a reasoner that checks the result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Our approach</head><p>We have a passage ranking component that takes a query and a set of documents, it extracts sentences, and assigns a score to them. This is done by two passages over each text, where on a first passage, after lowcasing and lemmatizing all words in text and query we retain information related to sentences where: -we count the number of non-stopword query word tokens (as opposed to types) present in the sentence that are positive to an identity match, and the result is not an empty set; On a second pass, on the contrary, we keep the original orthography and take care of words beginning with uppercase letters, and we count: -all words that match semantically, by accessing WordNet and other computational lexica -we use Sumo-Milo and FrameNet. The non empty matching results are then passed to another important filter that takes Logical Form of the query and looks for heads and predicates of predicate-argument structures contained there. The final score obtained is the sum of the previous computation and the last one, where we impose the presence of the most relevant lemmas in the choice of the best candidate sentence. Logical Forms are derived from DAGs of f-structure sentence level representation and are simplified in order to be useful for the question answering task. In particular, we come up with a non-recursive linear representation at propositional level where we introduce prefixes for each semantic head which are very close to DRS-conditions: -PRED, QUANT, CARD, ARG, MOD, ADJ, FOC where Foc contains the question type derived from a mapping of each wh-word, together with its possible nominal or adjectival head and a restricted set of semantic general classes, like MEASURE, MANNER, QUANTITY, REASON etc. The text representation is made in the form of Discourse Model which is simplified before matching takes place. In particular, we compose two types of semantic structures from the list of facts: -an event structure for each governing predicate which includes the arguments in their literal form and their semantic indices, together with the polarity and the two spatiotemporal indices; -an enriched version of the fact associated to each entity in the DM, which includes knowledge of the world (synset and definition) retrieved in one of the ontologies and computational lexica available; -a relational representation for each relation present in the DM that associates properties to entities and relations, including discourse markers at propositional level, attributes, modifiers, partonimy, generic unmarked relations (OF relation) etc. This level of representation is used to match possible answers with the chosen sentence, thus trying to select the most appropriate answer cadidates.</p><p>correct answer made available is not a direct answer but requires some reasoning to be in place in order for the system to select it. In some cases there are more answer right, while in other cases none of the answers is correct. As to resources used to answer questions, we found it very important to access the commonsense reasoning repositoire called CONCEPTNET, as made available by MIT AI laboratory. This is done whenever the similarity algorithm has attemped all possible semantic inferencing steps and has reached a failure. Eventually, access to commonsense reasoning is produced in order to fill in the gap of some intermediate reasoning link or step which needs to be restored in order for the appropriate answer to be selected. We will comment specific cases in the sections below. What the system does is to use Logical Forms in order to produce matches between Question appropriately turned into the corresponding prospective Answer, and sentences contained in the text. Whenever matches are found a score is generated which allows the system to grade best sentence candidates to be considered in the second part of the analysis, when the best answer is to be chosen from the set of answers made available in the dataset. At first we produce a surface level identity match of the actual words contained in question and candidate sentence using the typical Information Retrieval approach: we go through each word and skip stop words. If we don't find a match, we try with lemmaized version of question and text sentences. This first pass through the text produces a score which is then passed to the second level matching mechanism that relies on Semantics. It is worth noting, that in this second level, all unexpressed linguistic elements are placed in their required position by Logical Form constraints that need, for instance, SUBJects to be in place before a complete Predicate-Argument structure is built. We also recover antecedents of pronominal expressions as they have been computed by the Anaphora Resolution algorithm included in our system. Matches are produced by doing Identity match at first, between Heads that constitute the Predicate-Argument structure contained in the LF of the Question and the candidate sentence. We are using a mechanism which is derived directly from our previous work on RTE, which not only allows us to detect mismatches but also contradictions thus rejecting the candidate with a low score. All similarity matches are produced by inferencing with WordNet and other similar resources, also on the basis of semantic general tags, like the ones introduced by SUMO-MILO. We will only comment on Text 13 in details: this text is one of the most difficult to answer -if not the most difficult. Difficulties arise basically due to the need to produce both anaphora and coreference resultion links between entities and events mentioned in succession. Questions "easy" to answer are those that "literally" coincide with the semantic contents of one sentence in the text: that is, the predicate-argument structure coincides with the one of the question, and the entities mentioned are semantically identical or very similar to the one contained in the question. As will be clear from the comments below, there are only three questions over 18 which can be regarded "easy" to answer. This is also a text that contains 3 "why" questions and one "How many" question: these are usually regarded most difficult questions to answer. We report here below a long excerpt from the first part of the text, and then make short references to the remaining part. For each question of the 18 proposed, we list the answers and then make comments on the right choice and the difficulties inherent in finding it. Here is the excerpt:</p><p>The appointment of a former top executive of a major U.S. pharmaceutical company and major Republican contributor as President George W. Bush's global AIDS co-ordinator has stunned and outraged AIDS experts and activists. Bush's choice of former Eli Lilly &amp; Co. boss Randall Tobias was announced at the White House on July 1, just a few days before Bush's first trip as president to Africa. The U.S. Senate must confirm the nomination. Tobias, who retired from Lilly in 1998 and more recently has served as vice chairman of AT&amp;T, where he also worked before going to Lilly in the early 1990s, is supposed to receive the rank of ambassador and report to Secretary of State Colin Powell, a major force behind a five-year, 15-billion-dollar anti-AIDS initiative -called the "Emergency Program" -first proposed by Bush last January and approved by Congress in a somewhat amended form in May. Implementation of that initiative, which is targeted at 12 sub-Saharan African and two Caribbean countries, will be Tobias' first responsibility, according to Bush. "Randy Tobias has a mandate directly from me to get our AIDS initiative up and running as soon as possible," he said. Surreal Appointment Prof. Jeffrey Sachs, head of Columbia University's Earth Institute and a special adviser to UN Secretary General Kofi Annan on the AIDS crisis, called the appointment "surreal" and continued that "This is an emergency that requires someone who's worked in the field and knows it thoroughly. We don't need someone who raises all sorts of questions about commitment and agenda." Advocacy groups called for senators to closely scrutinize Tobias' credentials and philosophy and determine whether, given his past ties to the industry, he will be able to fight on behalf of the millions of poor HIV/AIDS victims in desperate need of cheap anti-retroviral drugs in the face of opposition from the major western pharmaceutical companies, often referred to as Big Pharma. "This decision is another deeply disturbing sign that the President may not be prepared to fulfill his pledge to take emergency action on AIDS," noted Paul Zeitz, executive director of the Global AIDS Alliance. "It raises serious questions of conflict of interest and the priorities of the White House." "Both the people of Africa and the people of the United States will lose if the president's AIDS initiative fails to use the lowest-cost, generic medications," Zeitz said, noting that the pharmaceutical companies have successfully pressed the Bush administration to go back on an earlier pledge to carve out an exception in international patent laws that would enable needy countries to import generic anti-AIDS drugs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quest.: 1, What is the main objective of the Emergency Program ?</head><p>ans(1, to make anti-retroviral drugs available to the poor), ans(2, to use the lowest-cost generic medications), ans(3, to change the international patent laws), ans(4, to import lifesaving drugs), ans(5, none of the above)</p><p>The best right answer is answer 1 and can be found in the text reported above, further down, four sentences below after the reference to Tobias. Also answer 2 is correct and can be found in a comment at the end of the excerpt. The problem is that this can only happen in case all anaphora and coreference resolution steps have been correctly performed. At the beginning we are told that Tobias is responsible for the implementation of the "Emergency Program" which is then mentioned as "that initiative". The same program is coreferred to by Annan as "this emergency". Eventually, the goals of the initiative are introduced in a following sentence, where "Tobias' credentials" will be scrutinized to determine whether "he will be able to fight on behalf of the millions of poor HIV/AIDS victims in desperate need of cheap anti-retroviral drugs".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quest.: 2, Why were AIDS activists not happy with Randall Tobias being appointed as global AIDS co-ordinator ?</head><p>ans(1, because he was the head of Columbia University), ans(2, because he was supposed to favour the pharmaceutical industries), ans(3, because he lived in Caribbean countries), ans(4, because he was a person with great acumen), ans(5, none of the above) Question two is best answered by answer 2. and is found in the same piece of text reported above. Here again we may note that the answer uses a different wording from what can be found in the text with the same meaning: "pharmaceutical industries" rather than "pharmaceutical companies". However understanding that the portion of selected text is actually talking about AIDS activists unhappy with Randall Tobias appointed as global AIDS coordinator is not an easy task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quest.: 3, Why is Randall Tobias supposed to receive the rank of ambassador ?</head><p>ans(1, because he was a major Republican contributor), ans(2, because he was a former top executive of a major U.S. pharmaceutical company), ans(3, because he retired from Lilly), ans(4, because he was vice chairman of A&amp;T), ans(5, none of the above) Question 3 doesn't have an answer, so answer 5 would be the best choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quest.: 4, Has Randall Tobias been confirmed as President George W. Bush's global AIDS co-ordinator ?</head><p>ans(1, Yes, a few days before Bush's first trip as president to Africa), ans(2, Not yet), ans(3, Yes, on July 1), ans(4, Yes, last January), ans(5, none of the above)</p><p>Here there is only one possible answer, and it is answer 2. This is derivable from this excerpt, where we see that there has been an "announcement" of nomination but it hasn't been confirmed yet:</p><p>The appointment of a former top executive of a major U.S. pharmaceutical company and major Republican contributor as President George W. Bush's global AIDS co-ordinator … Bush's choice of former Eli Lilly &amp; Co. boss Randall Tobias was announced at the White House on July 1, just a few days before Bush's first trip as president to Africa. The U.S. Senate must confirm the nomination.</p><p>In order to be able to associate "Not yet" to the second sentence, the system needs to corefer "Nomination" to "Bush's best choice", and link the latter to "Appointment" in the previous sentence. In other words, the text reports an "appointment" then a "choice" and eventually a "nomination". If appointment and nomination are perfect synonyms, "choice" isn't included in any synset related to them. The link between choice and nomination is then missing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quest.: 5, What does the author of the book "The End of Poverty" think about the appointment of Randall Tobias ?</head><p>ans(1, he defines it as important), ans(2, he defines it as successful), ans(3, he defines it as serious), ans(4, he defines it as surreal), ans(5, none of the above)</p><p>No author of a book is mentioned in the text so the answer has to be answer 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quest.: 6, Who will be in charge of carrying out effectively the "Emergency Plan" ?</head><p>ans(1, George W. Bush), ans(2, the former chief executive officer of Eli Lilly &amp; Co), ans(3, Secretary of State Colin Powell), ans(4, the head of Columbia University), ans(5, none of the above)</p><p>The right answer is answer 2, with a long description of properties which are again referring to Tobias. However "carrying out effectively" is to be understood as a paraphrase of "implementation", which is what we find in the text. The synonym link appears in WordNet, but coreference between a noun "implementation" and the verb "carry out" is not easy to perform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quest.: 7, What does Jeffrey Sachs think about the appointment of Randall Tobias ?</head><p>ans(1, he defines it as important), ans(2, he defines it as successful), ans(3, he defines it as serious), ans(4, he defines it as surreal), ans(5, none of the above)</p><p>Right answer is answer no. 4, where the appointment is defined as "surreal". This is the only easy question to answer. The problem in this case is constituted by the need to use a coreferring singular definite nominal "appointment" that needs to be linked to the previous mention, beginning of the text, where however its subject is only indirectly referred to Tobias: Prof. Jeffrey Sachs, head of Columbia University's Earth Institute and a special adviser to UN Secretary General Kofi Annan on the AIDS crisis, called the appointment "surreal"…</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quest.: 8, What types of drug are used by the U.S. Administration for the Emergency Program ?</head><p>ans(1, generics), ans(2, it is to be decided), ans(3, brand-name anti-viral medicines), ans(4, triple combinations of anti-retroviral drugs), ans(5, none of the above)</p><p>Here the right answer is answer 2. Again the answer is not directly available and needs some inference to be fired from the following excerpt:</p><p>While the administration has suggested it will use generics in the Emergency Program, it has not been made a formal decision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quest.: 9, How many countries are included in the Emergency Program ?</head><p>ans(1, 12), ans(2, 2), ans(3, 18), ans(4, 10), ans(5, none of the above) None of the above is the right answer, as can be gathered from the first excerpt reported above. Of course in order to properly understand the content of the question and pair it with the right piece of text, some inference is needed. The question says "included in the Emergency Program", and the text says "Implementation of that initiative, which is targeted to..." where "targeted to" is followed by the countries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quest.: 10, What is a strong characteristic of Randall Tobias ?</head><p>ans(1, his experience with AIDS), ans(2, his background in public health), ans(3, his experience with working in poor countries), ans(4, his contacts with the World Trade Organization (WTO)), ans(5, none of the above)</p><p>Here the right answer is answer 5, "none of the above". This is again difficult to get.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quest.: 11, Who was the adviser of the Ghanaian diplomat ?</head><p>ans(1, Randall Tobias), ans(2, Jeffrey Sachs), ans(3, Colin Powell), ans(4, George W. Bush), ans(5, none of the above)</p><p>As before, the right answer is no. 5, "none of the above". In the text there is no reference to Ghanian diplomats.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quest.: 12, Who was the adviser of Kofi Annan ?</head><p>ans(1, Randall Tobias), ans(2, Jeffrey Sachs), ans(3, Colin Powell), ans(4, George W. Bush), ans(5, None of the above)</p><p>Here the right answer is no. 2, Jeffrey Sachs. This is the second easy question to answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quest.: 13, What is Randall Tobias reputation ?</head><p>ans(1, he is a down-to-earth business person), ans(2, he is incomprehensible), ans(3, he is an impoverished man), ans(4, he is a man of philosophy), ans(5, None of the above) Right answer is no. 1. The adjective qualifying the property of being a "business person", is however different in the question, from what is found in the text. In the question we have "down-to-earth" and in the text we have "a no-nonsense": no synonyms are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quest.: 14, What is Randall Tobias' reputation ?</head><p>ans(1, he is a no-nonsense businessman), ans(2, he is incomprehensible), ans(3, he is an impoverished man), ans(4, he is a man of philosophy), ans(5, none of the above)</p><p>Here on the contrary, the adjective used in the aswers is the same that appears in the text, and is contained in answer no. 1. So this is the third easy answer to get.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quest.: 15, Where were the agreements on international patent law signed ?</head><p>ans(1, at the World Trade Organization meeting in Doha), ans(2, at Big Pharma), ans(3, at the office of Management and Budget), ans(4, at the Health Global Access project meeting), ans(5, none of the above)</p><p>The right answer is no. 5, "none of the above". In the text there is no spatial location associated to the event of "signing of agreements".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quest.: 16, Why is Big Pharma considered the major organization responsible for contributing to the Global Fund ?</head><p>ans(1, because Big Pharma will provide $200 million), ans(2, because Big Pharma is against the Emergency program), ans(3, because Big Pharma produces drugs in India , Thailand and Brazil), ans(4, because Big Pharma wants to import generic anti-AIDS and other life-saving drugs), ans(5, none of the above)</p><p>The right answer is no. 5, because Big Pharma is not contributing to the Global Fund. On the contrary, we know from text that it is the "major culprit behind the administration's niggardliness towards the Fund". But obviously, making negative decisions, or finding the contrary of what is being asserted is very difficult.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quest.: 17, What is the annual US contribution to the Global Fund to fight AIDS ?</head><p>ans(1, $200 million), ans(2, $1 billion), ans(3, $20 million), ans(4, $2 billion), ans(5, None of the above)</p><p>Here the information needs to be badly filtered and inferences fired. The sentence containing the answer is the following one:</p><p>Although Congress has authorized an annual contribution of up to $1 billion for the Fund -which is already fast running out of money -the administration has said it intends to provide only $200 million a year.</p><p>The answer in this case is again "none of the above" and it is hard to compute from the text. In the extracted sentence, we can see that neither the concessive headed by "although", nor the main clause constitute a factual assertion. Since that is what is required by the question, the answer is left unsatisfied and unanswered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quest.: 18, What are activists most concerned about ?</head><p>ans(1, about statistics of the AIDS toll in Africa), ans(2, about importing generic anti-AIDS drugs), ans(3, about the International AIDS Trust), ans(4, about the World Trade Organization), ans(5, None of the above)</p><p>The question uses a superlative "most concerned" which only pairs with the last of three questions posed by activists on Tobias nomination. The text contains the expression "particularly worried" which should be understood as synonymous to the previous adjectives. But then the object does not match any of the possible answer, and so again the right answer is no. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>As said above, out system doesn't have to go through a training phase. This is positive on the one side but it could become negative in case some unforseen and unpredictable linguistic problem arises in the analysis of either the text or the questions. Results obtained in the final run are not very satisfactory. This is due to difficulties in analyzing some of the texts; but also in some cases to types of questions which were not understood by the system. For this reason we decided to produce a "Late Run", one week after. At first we report here below results of the analysis of the regular first run. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis Result :</head><p>The dataset was composed of a total of 284 questions of which:</p><p>-240 are main questions -44 are auxiliary questions</p><p>The difference between main and auxiliary questions resides in the presence of a inference. In fact an auxiliary question is just a duplicate of a main question minus the inference. The idea is that the simpler versions (auxiliary) could be added to a main questions: if a system gets the difficult version wrong and the easy version right, it could be that it could not perform the required inference.</p><p>Statistics are given both considering main questions only and all questions (main + auxiliary).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation on the main questions A) Evaluation at question-answering level</head><p>The file vens1301enen_Main_Task_5_20_2013_12_7_20.xml contains a total of 240 questions.</p><p>-number of questions ANSWERED : 231 -number of questions UNANSWERED : 9 </p><formula xml:id="formula_0" coords="15,203.30,267.49,2.23,5.78">-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis Result :</head><p>The dataset was composed of a total of 284 questions of which:</p><p>-240 are main questions -44 are auxiliary questions</p><p>The difference between main and auxiliary questions resides in the presence of a inference. In fact an auxiliary question is just a duplicate of a main question minus the inference. The idea is that the simpler versions (auxiliary) could be added to a main questions: if a system gets the difficult version wrong and the easy version right, it could be that it could not perform the required inference.</p><p>Statistics are given both considering main questions only and all questions (main + auxiliary).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation on the main questions A) Evaluation at question-answering level</head><p>The file vens1301enen_Main_Task_5_20_2013_12_7_20.xml contains a total of 240 questions.</p><p>-number of questions ANSWERED : 231 -number of questions UNANSWERED : 9 Overall c@1 per topic: c@1 topic t_id '1' = (17+0(17/60))/60 = 0.28 c@1 topic t_id '2' = (16+1(16/60))/60 = 0.27 c@1 topic t_id '3' = (9+7(9/60))/60 = 0.17 c@1 topic t_id '4' = (9+1(9/60))/60 = 0.15  Overall c@1 per topic: c@1 topic t_id '1' = (17+0(17/60))/60 = 0.28 c@1 topic t_id '2' = (24+1(24/78))/78 = 0.31 c@1 topic t_id '3' = (12+8(12/74))/74 = 0.18 c@1 topic t_id '4' = (12+1(12/72))/72 = 0.17 Evaluation on all questions (main + auxiliary)</p><formula xml:id="formula_1" coords="15,194.18,501.42,1.97,5.09">-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A) Evaluation at question-answering level</head><p>The file vens1301enen_Main_Task_5_20_2013_12_7_20.xml contains a total of 284 questions.</p><p>-number of questions ANSWERED : 274 -number of questions UNANSWERED : 10 Overall c@1 per topic: c@1 topic t_id '1' = (17+0(17/60))/60 = 0.28 c@1 topic t_id '2' = (24+1(24/78))/78 = 0.31 c@1 topic t_id '3' = (12+8(12/74))/74 = 0.18 c@1 topic t_id '4' = (12+1(12/72))/72 = 0.17  And here below we report results of the "LATE run" which however are not remarkably better, except for the fact that the system managed to answer all questions. Analysis Result :</p><formula xml:id="formula_2" coords="16,193.78,258.20,1.96,5.08">-</formula><p>The dataset was composed of a total of 284 questions of which:</p><p>-240 are main questions -44 are auxiliary questions</p><p>The difference between main and auxiliary questions resides in the presence of a inference. In fact an auxiliary question is just a duplicate of a main question minus the inference. The idea is that the simpler versions (auxiliary) could be added to a main questions: if a system gets the difficult version wrong and the easy version right, it could be that it could not perform the required inference.</p><p>Statistics are given both considering main questions only and all questions (main + auxiliary).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation on the main questions A) Evaluation at question-answering level</head><p>The file vens1302enen_Main_Task_LATE_RUN.xml contains a total of 240 questions.</p><p>-number of questions ANSWERED : 240 -number of questions UNANSWERED : 0  </p><formula xml:id="formula_3" coords="16,201.16,222.14,2.23,5.76">-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis Result :</head><p>The dataset was composed of a total of 284 questions of which:</p><p>-240 are main questions -44 are auxiliary questions</p><p>The difference between main and auxiliary questions resides in the presence of a inference. In fact an auxiliary question is just a duplicate of a main question minus the inference. The idea is that the simpler versions (auxiliary) could be added to a main questions: if a system gets the difficult version wrong and the easy version right, it could be that it could not perform the required inference.</p><p>Statistics are given both considering main questions only and all questions (main + auxiliary).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation on the main questions A) Evaluation at question-answering level</head><p>The file vens1302enen_Main_Task_LATE_RUN.xml contains a total of 240 questions.</p><p>-   Topic t_id = '3' -Climate Change Median: 0.13 -Average: 0.11 -Standard Deviation: 0.08 -calculated over the c@1 of the four reading tests -c@1 measure for reading-test r_id '9' = (3+0(3/18))/18 = 0.17 -c@1 measure for reading-test r_id '10' = (0+0(0/18))/18 = 0.00 -c@1 measure for reading-test r_id '11' = (2+0(2/18))/18 = 0.11 -c@1 measure for reading-test r_id '12' = (3+0(3/20))/20 = 0.15</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic t_id = '4' -AIDS</head><p>Median: 0.17 -Average: 0.14 -Standard Deviation: 0.06 -calculated over the c@1 of the four reading tests -c@1 measure for reading-test r_id '13' = (3+0(3/18))/18 = 0.17 -c@1 measure for reading-test r_id '14' = (3+0(3/18))/18 = 0.17 -c@1 measure for reading-test r_id '15' = (1+0(1/18))/18 = 0.06 -c@1 measure for reading-test r_id '16' = (3+0(3/18))/18 = 0.17</p><p>Evaluation on all questions (main + auxiliary)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A) Evaluation at question-answering level</head><p>The file vens1302enen_Main_Task_LATE_RUN.xml contains a total of 284 questions.</p><p>-number of questions ANSWERED : 284 -number of questions UNANSWERED : 0 Overall c@1 per topic: c@1 topic t_id '1' = (17+0(17/60))/60 = 0.28 c@1 topic t_id '2' = (23+0(23/78))/78 = 0.29 c@1 topic t_id '3' = (12+0(12/74))/74 = 0.16 c@1 topic t_id '4' = (16+0(16/72))/72 = 0.22</p><formula xml:id="formula_4" coords="17,194.65,246.37,2.03,5.26">-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B) Evaluation at reading-test level</head><p>Median: 0.26 -Average: 0.24 -Standard Deviation: 0.12 -calculated over c@1 of all 16 reading tests Topic t_id = '1' -Alzheimer Median: 0.30 -Average: 0.28 -Standard Deviation: 0.11 -calculated over the c@1 of the four reading tests -c@1 measure for reading-test r_id '1' = (6+0(6/15))/15 = 0.40 -c@1 measure for reading-test r_id '2' = (4+0(4/15))/15 = 0.27 -c@1 measure for reading-test r_id '3' = (5+0(5/15))/15 = 0.33 2 di 3 28/05/13 17:01 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>Eventually, the evaluation of the system on the test set is not satisfactory. But this is certainly due to the intrinsic difficulty of the dataset and the way in which questions have been formulated. Our system does both anaphora and coreference resolution and subsequently should be able to allow for long distance coreference. But clearly, the level of accuracy of these two processes is fairly low -differently from pronominal binding which averages 75% accuracy. What we actually must admit is that in order to find the correct answer, the contribution of the Logical Form and semantic Discourse Model is limited to a 30% improvement over an approach in which structural information plays no role whatsoever. In general, BOWs approach is totally inefficient and produces confusing results when the selection of the right answer has to be performed solely on the basis of content word identity match. When lemmatization is added there are improvements but they are not very significant, and this is due to the fact that scoring the best candidate on the basis of word identity match is not enough to distinguish relevant from irrelevant linguistic material. This happens even when we compute on deep rather than surface level analysis. Semantic similarity matches are worked out on the basis of available resources, which however in many case is not sufficient. We also considered very important the need to distinguish different types of questions, not only on the basis of the question word or question NP, but also and foremost in case the overall question structure requires specific semantic processing to be in place. But the task has been made much harder by the presence of null or negative answers: they are represented by the option no.5 "none of the above". In order for the system to choose this option, quantitative evaluations should be available that would allow to use graded scales or thresholds to prevent it from accepting approximate solutions. This is not always feasible and our system has not been tuned yet to check for a fine-grained level of semantic consistency. This is going to be our improvements for the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="15,207.37,267.49,182.83,5.78;15,203.30,275.60,194.44,5.78;15,203.30,283.71,191.17,5.78;15,203.30,291.82,195.14,5.78;15,203.30,299.93,171.26,5.78;15,203.30,409.74,204.51,5.78;15,203.30,417.85,90.16,5.78;15,203.30,434.08,146.48,5.78;15,203.30,450.30,74.84,5.78;15,203.30,468.67,19.23,3.63;15,210.64,476.78,137.84,3.63;15,210.64,484.89,118.85,3.63;15,210.64,493.00,96.30,3.63;15,203.30,509.22,89.98,3.63;15,203.30,523.30,138.84,5.78;15,203.30,540.55,63.48,4.74;15,210.64,547.63,118.81,5.78;15,210.64,555.74,118.81,5.78;15,210.64,563.85,112.43,5.78;15,210.64,571.96,112.43,5.78;15,203.30,591.53,123.32,5.84;15,203.30,608.54,299.91,4.74;15,203.30,625.87,79.13,3.63;15,210.64,631.83,302.43,5.78;15,221.64,639.95,170.43,5.78;15,24.83,678.28,22.78,9.12;15,523.03,678.28,60.83,9.12;15,81.55,239.23,58.76,11.87;15,81.55,255.44,218.34,7.12;15,81.55,264.65,44.22,7.28;15,86.16,301.45,16.90,5.48;15,86.16,309.70,29.17,5.48;15,86.16,317.94,80.97,5.48;15,86.16,326.18,46.44,5.48;15,86.16,334.43,32.67,5.48;15,86.16,342.67,32.13,5.48;15,86.16,350.91,26.33,5.48;15,86.16,359.16,37.36,5.48;15,86.16,367.40,25.39,5.48;15,86.16,375.64,15.19,5.48;15,86.16,400.95,64.15,5.48;15,86.16,426.25,15.93,5.48;15,86.16,451.56,31.76,5.48;15,86.16,459.80,25.54,5.48;15,86.16,468.05,20.16,5.48"><head></head><label></label><figDesc>Number of questions ANSWERED with RIGHT candidate answer : 51 -Number of questions ANSWERED with WRONG candidate answer : 180 -Number of questions UNANSWERED with RIGHT candidate answer : 1 -Number of questions UNANSWERED with WRONG candidate answer : 2 -Number of questions UNANSWERED with EMPTY candidate : 6 Accuracy (answered with judgment=correct) calculated over all questions: Overall accuracy = 51/240 = 0.21 Proportion of answers correctly discarded: 2/9 = 0.22 C@1 = (nr + nu * (nr/n)) / n where: nr: is the number of correctly answered questions nu: is the number of unanswered questions n: is the total number of questions *for more information click here Overall c@1 measure = (51+9(51/240))/240 = 0.22 Overall c@1 per topic: c@1 topic t_id '1' = (17+0(17/60))/60 = 0.28 c@1 topic t_id '2' = (16+1(16/60))/60 = 0.27 c@1 topic t_id '3' = (9+7(9/60))/60 = 0.17 c@1 topic t_id '4' = (9+1(9/60))/60 = 0.15 B) Evaluation at reading-test level Median: 0.18 -Average: 0.19 -Standard Deviation: 0.10 -calculated over c@1 of all 16 reading tests Topic t_id = '1' -Alzheimer Median: 0.33 -Average: 0.28 -Standard Deviation: 0.10 -calculated over the c@1 of the four reading tests -c@1 measure for reading-test r_id '1' = (5+0(5/15))/15 = 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="15,197.76,501.42,161.02,5.09;15,194.18,508.57,171.24,5.09;15,194.18,515.71,168.36,5.09;15,194.18,522.85,171.86,5.09;15,194.18,530.00,150.82,5.09;15,194.18,626.72,180.10,5.09;15,194.18,633.86,79.41,5.09;15,194.18,648.15,129.00,5.09;15,194.18,662.44,65.91,5.09;15,194.18,678.62,16.94,3.20;15,200.64,685.76,121.39,3.20;15,200.64,692.91,104.66,3.20;15,200.64,700.05,84.81,3.20;15,194.18,714.34,79.24,3.20;15,194.18,726.73,122.27,5.09"><head></head><label></label><figDesc>Number of questions ANSWERED with RIGHT candidate answer : 51 -Number of questions ANSWERED with WRONG candidate answer : 180 -Number of questions UNANSWERED with RIGHT candidate answer : 1 -Number of questions UNANSWERED with WRONG candidate answer : 2 -Number of questions UNANSWERED with EMPTY candidate : 6 Accuracy (answered with judgment=correct) calculated over all questions: Overall accuracy = 51/240 = 0.21 Proportion of answers correctly discarded: 2/9 = 0.22 C@1 = (nr + nu * (nr/n)) / n where: nr: is the number of correctly answered questions nu: is the number of unanswered questions n: is the total number of questions *for more information click here Overall c@1 measure = (51+9(51/240))/240 = 0.22</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="16,203.53,7.01,186.91,5.79;16,203.53,15.14,194.45,5.79;16,203.53,23.26,191.18,5.79;16,203.53,31.39,195.15,5.79;16,203.53,39.52,171.27,5.79;16,203.53,149.55,204.52,5.79;16,203.53,157.68,90.17,5.79;16,203.53,173.94,149.68,5.79;16,203.53,190.19,74.85,5.79;16,203.53,208.60,19.23,3.64;16,210.87,216.72,137.85,3.64;16,210.87,224.85,118.85,3.64;16,210.87,232.98,96.31,3.64;16,203.53,249.24,89.98,3.64;16,203.53,263.34,142.04,5.79"><head>-</head><label></label><figDesc>Number of questions ANSWERED with RIGHT candidate answer : 65 -Number of questions ANSWERED with WRONG candidate answer : 209 -Number of questions UNANSWERED with RIGHT candidate answer : 1 -Number of questions UNANSWERED with WRONG candidate answer : 2 -Number of questions UNANSWERED with EMPTY candidate : 7 Accuracy (answered with judgment=correct) calculated over all questions: Overall accuracy = 65/284 = 0.23 Proportion of answers correctly discarded: 2/10 = 0.20 C@1 = (nr + nu * (nr/n)) / n where: nr: is the number of correctly answered questions nu: is the number of unanswered questions n: is the total number of questions *for more information click here Overall c@1 measure = (65+10(65/284))/284 = 0.24</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="16,203.53,331.71,123.33,5.85;16,203.53,348.75,299.92,4.75;16,203.53,366.12,79.13,3.64;16,210.87,372.10,302.45,5.79;16,221.87,380.23,170.44,5.79;16,221.87,388.35,170.44,5.79;16,221.87,396.48,170.44,5.79;16,25.05,434.89,22.78,9.13;16,523.28,434.89,60.83,9.13;16,209.89,2.79,149.72,5.08;16,193.78,18.93,88.96,3.19;16,200.22,24.17,265.68,5.08;16,209.89,31.30,149.72,5.08;16,209.89,38.43,149.72,5.08;16,209.89,45.56,149.72,5.08;16,209.89,52.69,149.72,5.08;16,193.78,68.83,83.69,3.19;16,200.22,74.08,265.68,5.08;16,209.89,81.20,149.72,5.08;16,209.89,88.33,152.52,5.08;16,209.89,95.46,152.52,5.08;16,209.89,102.59,152.52,5.08;16,193.78,118.73,55.03,3.19;16,200.22,123.98,265.68,5.08;16,209.89,131.10,152.52,5.08;16,209.89,138.23,152.52,5.08;16,209.89,145.36,152.52,5.08;16,209.89,152.49,152.52,5.08"><head>B)</head><label></label><figDesc>Evaluation at reading-test level Median: 0.25 -Average: 0.24 -Standard Deviation: 0.11 -calculated over c@1 of all 16 reading tests Topic t_id = '1' -Alzheimer Median: 0.33 -Average: 0.28 -Standard Deviation: 0.10 -calculated over the c@1 of the four reading tests -c@1 measure for reading-test r_id '1' = (5+0(5/15))/15 = 0.33 -c@1 measure for reading-test r_id '2' = (5+0(5/15))/15 = 0.33 -c@1 measure for reading-test r_id '3' = (5+0(5/15))/15 = 0.33 2 di 3 07/06/13 09:44 -c@1 measure for reading-test r_id '4' = (2+0(2/15))/15 = 0.13 Topic t_id = '2' -Music and society Median: 0.21 -Average: 0.21 -Standard Deviation: 0.05 -calculated over the c@1 of the four reading tests -c@1 measure for reading-test r_id '5' = (4+1(4/20))/20 = 0.21 -c@1 measure for reading-test r_id '6' = (4+0(4/19))/19 = 0.21 -c@1 measure for reading-test r_id '7' = (3+0(3/20))/20 = 0.15 -c@1 measure for reading-test r_id '8' = (5+0(5/19))/19 = 0.26Topic t_id = '3' -Climate ChangeMedian: 0.11 -Average: 0.14 -Standard Deviation: 0.09 -calculated over the c@1 of the four reading tests -c@1 measure for reading-test r_id '9' = (2+0(2/18))/18 = 0.11 -c@1 measure for reading-test r_id '10' = (1+0(1/18))/18 = 0.06 -c@1 measure for reading-test r_id '11' = (4+4(4/18))/18 = 0.27 -c@1 measure for reading-test r_id '12' = (2+3(2/20))/20 = 0.12 Topic t_id = '4' -AIDS Median: 0.11 -Average: 0.13 -Standard Deviation: 0.07 -calculated over the c@1 of the four reading tests -c@1 measure for reading-test r_id '13' = (1+1(1/18))/18 = 0.06 -c@1 measure for reading-test r_id '14' = (2+0(2/18))/18 = 0.11 -c@1 measure for reading-test r_id '15' = (2+0(2/18))/18 = 0.11 -c@1 measure for reading-test r_id '16' = (4+0(4/18))/18 = 0.22</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="16,197.35,258.20,160.62,5.08;16,193.78,265.33,170.81,5.08;16,193.78,272.46,167.94,5.08;16,193.78,279.58,171.43,5.08;16,193.78,286.71,150.45,5.08;16,193.78,383.22,179.66,5.08;16,193.78,390.35,79.21,5.08;16,193.78,404.61,131.49,5.08;16,193.78,418.87,65.75,5.08;16,193.78,435.01,16.90,3.19;16,200.22,442.14,121.09,3.19;16,200.22,449.27,104.41,3.19;16,200.22,456.40,84.60,3.19;16,193.78,470.65,79.04,3.19;16,193.78,483.02,124.77,5.08"><head></head><label></label><figDesc>Number of questions ANSWERED with RIGHT candidate answer : 65 -Number of questions ANSWERED with WRONG candidate answer : 209 -Number of questions UNANSWERED with RIGHT candidate answer : 1 -Number of questions UNANSWERED with WRONG candidate answer : 2 -Number of questions UNANSWERED with EMPTY candidate : 7 Accuracy (answered with judgment=correct) calculated over all questions: Overall accuracy = 65/284 = 0.23 Proportion of answers correctly discarded: 2/10 = 0.20 C@1 = (nr + nu * (nr/n)) / n where: nr: is the number of correctly answered questions nu: is the number of unanswered questions n: is the total number of questions *for more information click here Overall c@1 measure = (65+10(65/284))/284 = 0.24</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="16,193.78,542.99,108.33,5.13;16,193.78,557.94,263.46,4.17;16,193.78,573.18,69.51,3.19;16,200.22,578.42,265.68,5.08;16,209.89,585.55,149.72,5.08;16,209.89,592.67,149.72,5.08;16,209.89,599.80,149.72,5.08;16,36.99,633.49,20.01,8.01;16,474.65,633.49,53.44,8.01"><head></head><label></label><figDesc>Median: 0.33 -Average: 0.28 -Standard Deviation: 0.10 -calculated over the c@1 of the four reading tests -c@1 measure for reading-test r_id '1' = (5+0(5/15))/15 = 0.33 -c@1 measure for reading-test r_id '2' = (5+0(5/15))/15 = 0.33 -c@1 measure for reading-test r_id '3' = (5+0(5/15))/15 = 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="16,205.23,222.14,182.76,5.76;16,201.16,230.22,194.36,5.76;16,201.16,238.30,191.09,5.76;16,201.16,246.39,195.06,5.76;16,201.16,254.47,171.19,5.76;16,201.16,363.90,204.42,5.76;16,201.16,371.99,90.13,5.76;16,201.16,388.15,146.42,5.76;16,201.16,404.32,74.81,5.76;16,201.16,422.63,19.22,3.62;16,208.49,430.71,137.78,3.62;16,208.49,438.79,118.80,3.62;16,208.49,446.88,96.26,3.62;16,201.16,463.04,89.94,3.62;16,201.16,477.07,138.78,5.76;16,201.16,494.27,63.45,4.73;16,208.49,501.32,118.76,5.76;16,208.49,509.40,118.76,5.76;16,208.49,517.49,112.38,5.76;16,208.49,525.57,118.76,5.76;16,201.16,545.07,123.27,5.82;16,201.16,562.02,299.78,4.73;16,201.16,579.29,79.10,3.62;16,208.49,585.24,302.31,5.76;16,219.49,593.32,170.36,5.76;16,22.76,631.53,22.77,9.08;16,520.75,631.53,60.80,9.08;16,78.28,187.38,60.83,12.31;16,78.28,204.19,226.04,7.38;16,78.28,213.75,45.78,7.55"><head></head><label></label><figDesc>Number of questions ANSWERED with RIGHT candidate answer : 50 -Number of questions ANSWERED with WRONG candidate answer : 190 -Number of questions UNANSWERED with RIGHT candidate answer : 0 -Number of questions UNANSWERED with WRONG candidate answer : 0 -Number of questions UNANSWERED with EMPTY candidate : 0 Accuracy (answered with judgment=correct) calculated over all questions: Overall accuracy = 50/240 = 0.21 Proportion of answers correctly discarded: 0/0 = 0.00 C@1 = (nr + nu * (nr/n)) / n where: nr: is the number of correctly answered questions nu: is the number of unanswered questions n: is the total number of questions *for more information click here Overall c@1 measure = (50+0(50/240))/240 = 0.21 Overall c@1 per topic: c@1 topic t_id '1' = (17+0(17/60))/60 = 0.28 c@1 topic t_id '2' = (15+0(15/60))/60 = 0.25 c@1 topic t_id '3' = (8+0(8/60))/60 = 0.13 c@1 topic t_id '4' = (10+0(10/60))/60 = 0.17 B) Evaluation at reading-test level Median: 0.17 -Average: 0.18 -Standard Deviation: 0.11 -calculated over c@1 of all 16 reading tests Topic t_id = '1' -Alzheimer Median: 0.30 -Average: 0.28 -Standard Deviation: 0.11 -calculated over the c@1 of the four reading tests -c@1 measure for reading-test r_id '1' = (6+0(6/15))/15 = 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="16,198.60,437.02,94.34,5.28;16,194.89,444.43,98.69,5.28;16,194.89,459.24,170.41,5.28;16,194.89,466.65,177.28,5.28;16,194.89,474.06,174.30,5.28;16,194.89,481.47,177.92,5.28;16,194.89,488.87,156.14,5.28;16,194.89,589.16,186.46,5.28;16,194.89,596.57,82.21,5.28;16,194.89,611.38,133.55,5.28;16,194.89,626.20,68.24,5.28;16,194.89,642.97,17.54,3.32;16,201.58,650.38,125.67,3.32;16,201.58,657.79,108.36,3.32;16,201.58,665.20,87.80,3.32;16,194.89,680.01,82.03,3.32;16,194.89,692.86,126.59,5.28;16,194.89,708.62,57.87,4.33;16,201.58,715.09,108.32,5.28;16,201.58,722.49,108.32,5.28;16,201.58,729.90,102.50,5.28;16,201.58,737.31,108.32,5.28;16,194.89,755.18,112.43,5.33;16,194.89,770.71,273.43,4.33;16,194.89,786.54,72.14,3.32;16,32.17,129.68,91.36,8.33;16,310.96,129.68,230.90,8.33"><head></head><label></label><figDesc>number of questions ANSWERED : 240 -number of questions UNANSWERED : 0 -Number of questions ANSWERED with RIGHT candidate answer : 50 -Number of questions ANSWERED with WRONG candidate answer : 190 -Number of questions UNANSWERED with RIGHT candidate answer : 0 -Number of questions UNANSWERED with WRONG candidate answer : 0 -Number of questions UNANSWERED with EMPTY candidate : 0 Accuracy (answered with judgment=correct) calculated over all questions: Overall accuracy = 50/240 = 0.21 Proportion of answers correctly discarded: 0/0 = 0.00 C@1 = (nr + nu * (nr/n)) / n where: nr: is the number of correctly answered questions nu: is the number of unanswered questions n: is the total number of questions *for more information click here Overall c@1 measure = (50+0(50/240))/240 = 0.21Overall c@1 per topic: c@1 topic t_id '1' = (17+0(17/60))/60 = 0.28 c@1 topic t_id '2' = (15+0(15/60))/60 = 0.25 c@1 topic t_id '3' = (8+0(8/60))/60 = 0celct.fbk.eu/QA4MRE/index.php?page=Pages/campaig...</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="17,226.07,150.33,178.17,5.80;17,199.72,158.46,90.17,5.80;17,199.72,174.73,146.49,5.80;17,199.72,191.00,74.85,5.80;17,199.72,209.43,19.23,3.64;17,207.06,217.56,137.85,3.64;17,207.06,225.70,118.86,3.64;17,207.06,233.83,96.31,3.64;17,199.72,250.10,89.98,3.64;17,199.72,264.22,138.85,5.80;17,199.72,281.53,63.48,4.76;17,207.06,288.62,118.82,5.80;17,207.06,296.76,118.82,5.80;17,207.06,304.89,118.82,5.80;17,207.06,313.03,118.82,5.80;17,199.72,332.66,123.33,5.86;17,199.72,349.71,299.93,4.76;17,199.72,367.10,79.14,3.64;17,207.06,373.08,302.46,5.80;17,218.06,381.22,170.45,5.80;17,218.06,389.35,170.45,5.80;17,218.06,397.49,170.45,5.80;17,21.23,435.93,22.78,9.14;17,519.48,435.93,60.83,9.14;17,194.65,1.30,92.18,3.30;17,201.32,4.13,275.31,5.26;17,211.34,11.51,155.15,5.26;17,211.34,18.89,155.15,5.26;17,211.34,26.27,155.15,5.26;17,211.34,33.65,155.15,5.26"><head></head><label></label><figDesc>(answered with judgment=correct) calculated over all questions: Overall accuracy = 68/284 = 0.24 Proportion of answers correctly discarded: 0/0 = 0.00 C@1 = (nr + nu * (nr/n)) / n where: nr: is the number of correctly answered questions nu: is the number of unanswered questions n: is the total number of questions *for more information click here Overall c@1 measure = (68+0(68/284))/284 = 0.24 Overall c@1 per topic: c@1 topic t_id '1' = (17+0(17/60))/60 = 0.28 c@1 topic t_id '2' = (23+0(23/78))/78 = 0.29 c@1 topic t_id '3' = (12+0(12/74))/74 = 0.16 c@1 topic t_id '4' = (16+0(16/72))/72 = 0.22 B) Evaluation at reading-test level Median: 0.26 -Average: 0.24 -Standard Deviation: 0.12 -calculated over c@1 of all 16 reading tests Topic t_id = '1' -Alzheimer Median: 0.30 -Average: 0.28 -Standard Deviation: 0.11 -calculated over the c@1 of the four reading tests -c@1 measure for reading-test r_id '1' = (6+0(6/15))/15 = 0.40 -c@1 measure for reading-test r_id '2' = (4+0(4/15))/15 = 0.27 -c@1 measure for reading-test r_id '3' = (5+0(5/15))/15 = 0and society Median: 0.26 -Average: 0.19 -Standard Deviation: 0.13 -calculated over the c@1 of the four reading tests -c@1 measure for reading-test r_id '5' = (5+0(5/20))/20 = 0.25 -c@1 measure for reading-test r_id '6' = (5+0(5/19))/19 = 0.26 -c@1 measure for reading-test r_id '7' = (0+0(0/20))/20 = 0.00 -c@1 measure for reading-test r_id '8' = (5+0(5/19))/19 = 0.26</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="17,198.35,246.37,166.44,5.26;17,194.65,253.75,177.00,5.26;17,194.65,261.12,174.03,5.26;17,194.65,268.50,177.64,5.26;17,194.65,275.88,155.90,5.26;17,194.65,375.78,186.17,5.26;17,194.65,383.16,82.08,5.26;17,194.65,397.91,133.34,5.26;17,194.65,412.67,68.13,5.26;17,194.65,429.38,17.51,3.30;17,201.32,436.76,125.48,3.30;17,201.32,444.14,108.19,3.30;17,201.32,451.52,87.67,3.30;17,194.65,466.28,81.91,3.30;17,194.65,479.08,126.39,5.26"><head></head><label></label><figDesc>Number of questions ANSWERED with RIGHT candidate answer : 68 -Number of questions ANSWERED with WRONG candidate answer : 216 -Number of questions UNANSWERED with RIGHT candidate answer : 0 -Number of questions UNANSWERED with WRONG candidate answer : 0 -Number of questions UNANSWERED with EMPTY candidate : 0 Accuracy (answered with judgment=correct) calculated over all questions: Overall accuracy = 68/284 = 0.24 Proportion of answers correctly discarded: 0/0 = 0.00 C@1 = (nr + nu * (nr/n)) / n where: nr: is the number of correctly answered questions nu: is the number of unanswered questions n: is the total number of questions *for more information click here Overall c@1 measure = (68+0(68/284))/284 = 0.24</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="15,75.41,1.32,97.19,234.50"><head>at CLEF 2013 Home Main Task Biomedical about Alzheimer Entrance Exams Documents Downloads Schedule Organization Contacts Links QA @ CLEF Repository Login User Home Analyses Logout</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="15,37.00,183.58,492.32,608.40"><head>Table 1 .</head><label>1</label><figDesc>Evaluation on Main Questions</figDesc><table /><note coords="15,194.18,786.84,108.60,5.14;15,37.00,183.58,88.25,8.03;15,306.29,183.58,223.03,8.03"><p>B) Evaluation at reading-test level QA4MRE at CLEF 2013 http://celct.fbk.eu/QA4MRE/index.php?page=Pages/campaig...</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="16,238.93,307.89,133.92,7.80"><head>Table 2 .</head><label>2</label><figDesc>Evaluation on All Questions</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="16,211.06,510.21,189.59,7.80"><head>Table 3 .</head><label>3</label><figDesc>Evaluation on Main Questions for Late Run</figDesc><table /><note coords="17,199.72,7.65,186.92,5.80;17,199.72,15.79,194.46,5.80;17,199.72,23.92,191.19,5.80;17,199.72,32.06,195.16,5.80;17,199.72,40.19,171.27,5.80;17,199.72,150.33,24.52,5.80"><p>-Number of questions ANSWERED with RIGHT candidate answer : 68 -Number of questions ANSWERED with WRONG candidate answer : 216 -Number of questions UNANSWERED with RIGHT candidate answer : 0 -Number of questions UNANSWERED with WRONG candidate answer : 0 -Number of questions UNANSWERED with EMPTY candidate : 0 Accuracy</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="17,214.81,296.85,182.14,7.80"><head>Table 4 .</head><label>4</label><figDesc>Evaluation on All Questions for Late Run</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="7,134.37,589.27,168.69,10.54;7,125.37,615.18,362.39,8.64;7,125.37,627.18,362.36,8.64"><p>The QA4MRE Main Task datasetIn the QA4MRE dataset for English, we go from simple factoid questions to highly complex and sometimes hardly understandable questions. In between, in some cases, the</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="18,130.08,176.46,65.93,8.64" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Bos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,199.07,176.46,288.75,8.64;18,143.22,187.98,344.59,8.64;18,143.22,199.50,344.56,8.64;18,143.22,211.02,252.42,8.64" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="18,352.34,176.46,135.48,8.64;18,143.22,187.98,344.59,8.64;18,143.22,199.50,41.44,8.64">The Pronto QA system at TREC-2007: harvesting hyponyms, using nominalisation patterns, and computing answer cardinality</title>
		<author>
			<persName coords=""><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Guzzetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,365.43,199.50,122.35,8.64;18,143.22,211.02,97.08,8.64">The Sixteenth Text RETrieval Conference, TREC 2007</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<meeting><address><addrLine>Gaitersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="726" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,130.49,222.54,357.41,8.64;18,143.22,234.06,344.60,8.64;18,143.22,245.58,307.45,8.64" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="18,451.87,222.54,36.03,8.64;18,143.22,234.06,147.51,8.64">Question Answering with QED at TREC-2005</title>
		<author>
			<persName coords=""><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,472.34,234.06,15.48,8.64;18,143.22,245.58,204.63,8.64">The Fourteenth Text REtrieval Conference, TREC 2005</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<meeting><address><addrLine>Gaitersburg, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,129.65,257.10,60.11,8.64" xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Bresnan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,192.33,257.10,237.95,8.64" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="18,250.95,257.10,104.46,8.64">Lexical-Functional Syntax</title>
		<author>
			<persName coords=""><forename type="first">Joan</forename><surname>Bresnan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Blackwell</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,130.11,268.62,357.70,8.64;18,143.22,280.14,344.59,8.64;18,143.22,291.66,221.58,8.64" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="18,218.16,268.62,269.65,8.64;18,143.22,280.14,344.59,8.64;18,143.22,291.66,40.71,8.64">Rodolfo Delmonte, Computational Linguistic Text Processing -Logical Form, Semantic Interpretation, Discourse Relations and Question Answering</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Delmonte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
			<publisher>Nova Science Publishers</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,130.11,303.18,357.70,8.64;18,143.22,314.46,344.59,8.64;18,143.22,325.98,69.38,8.64" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="18,216.66,303.18,271.16,8.64;18,143.22,314.46,228.69,8.64">Rodolfo Delmonte, Computational Linguistic Text Processing -Lexicon, Grammar, Parsing and Anaphora Resolution</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Delmonte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<publisher>Nova Science Publishers</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,130.08,337.50,102.05,8.64" xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Delmonte</forename><surname>Bos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,237.13,337.50,250.73,8.64;18,143.22,349.02,344.58,8.64;18,143.22,360.54,113.01,8.64" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="18,410.77,337.50,77.09,8.64;18,143.22,349.02,80.47,8.64">Semantics in Text Processing (STEP)</title>
	</analytic>
	<monogr>
		<title level="s" coord="18,237.69,349.02,171.03,8.64">Research in Computational Semantics</title>
		<editor>
			<persName><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rodolfo</forename><surname>Delmonte</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2008">2008</date>
			<publisher>College Publications</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,129.73,372.06,66.23,8.64" xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,199.70,372.06,288.22,8.64;18,143.22,383.58,137.38,8.64;18,125.37,395.10,160.19,8.64;18,125.37,406.62,276.92,8.64;18,125.37,418.14,215.44,8.64;18,125.37,429.66,231.00,8.64" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="18,310.76,372.06,172.81,8.64">WordNet: An Electronic Lexical Database</title>
		<ptr target="http://wndomains.fbk.eu/wnaffect.html" />
		<editor>Christiane Fellbaum</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,129.53,440.94,92.71,8.64" xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Schwitter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,226.03,440.94,261.78,8.64;18,132.47,452.46,355.34,8.64;18,132.47,463.98,355.28,8.64;18,132.47,475.50,94.66,8.64" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="18,456.77,440.94,31.05,8.64;18,132.47,452.46,238.70,8.64">Answer Extraction: Towards better Evaluations of NLP Systems</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schwitter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mollà</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fournier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,394.30,452.46,93.52,8.64;18,132.47,463.98,355.28,8.64;18,132.47,475.50,29.86,8.64">Proc. Works. Reading Comprehension Tests as Evaluation for Computer-Based Language Understanding Systems</title>
		<meeting>Works. Reading Comprehension Tests as Evaluation for Computer-Based Language Understanding Systems<address><addrLine>Seattle</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="20" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,130.08,487.02,95.56,8.64" xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Hirschman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,229.97,487.02,257.86,8.64;18,132.47,498.54,334.53,8.64" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="18,466.80,487.02,21.02,8.64;18,132.47,498.54,157.49,8.64">Deep Read: A reading comprehension system</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">Marc</forename><surname>Hirschman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Light</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">D</forename><surname>Breck</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Buger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,307.70,498.54,59.93,8.64">Proc. A CL &apos;99</title>
		<meeting>A CL &apos;99</meeting>
		<imprint/>
		<respStmt>
			<orgName>University of Maryland</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="18,130.46,510.06,73.37,8.64" xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Hovy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,208.11,510.06,279.76,8.64;18,132.47,521.58,355.36,8.64;18,132.47,533.10,182.14,8.64" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="18,401.92,510.06,85.95,8.64;18,132.47,521.58,104.83,8.64">The Use of External Knowledge in Factoid QA</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,426.41,521.58,61.42,8.64;18,132.47,533.10,115.36,8.64">The Tenth Text Retrieval Conference (TREC</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2001">2002. 2001</date>
			<biblScope unit="page" from="644" to="652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,129.58,544.62,69.87,8.64" xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Litkowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,203.90,544.62,283.90,8.64;18,132.47,556.14,355.36,8.64;18,132.47,567.66,167.97,8.64" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="18,312.59,544.62,175.22,8.64;18,132.47,556.14,81.26,8.64">Syntactic Clues and Lexical Resources in Question-Answering</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">C</forename><surname>Litkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,422.75,556.14,65.08,8.64;18,132.47,567.66,124.98,8.64">The Ninth Text Retrieval Conference (TREC-9)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="157" to="166" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
