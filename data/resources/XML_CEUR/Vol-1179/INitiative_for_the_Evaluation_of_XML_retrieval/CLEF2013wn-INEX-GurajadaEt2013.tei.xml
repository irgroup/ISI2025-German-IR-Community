<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,140.39,115.96,334.57,12.62">Overview of the INEX 2013 Linked Data Track</title>
				<funder ref="#_YhxWV8a">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,161.05,153.63,75.12,8.74"><forename type="first">Sairam</forename><surname>Gurajada</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,246.73,153.63,54.44,8.74"><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,311.72,153.63,65.98,8.74"><forename type="first">Arunav</forename><surname>Mishra</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,388.26,153.63,58.81,8.74"><forename type="first">Ralf</forename><surname>Schenkel</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Passau</orgName>
								<address>
									<settlement>Passau</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,223.49,165.58,74.47,8.74"><forename type="first">Martin</forename><surname>Theobald</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Antwerp</orgName>
								<address>
									<settlement>Antwerp</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,327.90,165.58,59.49,8.74"><forename type="first">Qiuyue</forename><surname>Wang</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,140.39,115.96,334.57,12.62">Overview of the INEX 2013 Linked Data Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8F3EA9542909B975D3074252C9EBDE69</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>overview of the INEX Linked Data Track, which went into its second iteration in 2013.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As in the previous year <ref type="bibr" coords="1,243.24,327.90,9.96,8.74" target="#b6">[7]</ref>, the goal of the INEX Linked Data Track 1 was to investigate retrieval techniques over a combination of textual and highly structured data, where rich textual contents from Wikipedia articles serve as the basis for retrieval and ranking techniques, while additional RDF properties carry key information about semantic relationships among entities that cannot be captured by keywords alone. As opposed to the previous year, the Linked Data Track employed a new form of a reference collection, which was purely based on openly available dumps of English Wikipedia articles (using a snapshot from June 1st, 2012, in MediaWiki XML format) plus two canonical subsets of the DBpedia 3.8 <ref type="bibr" coords="1,191.92,435.50,10.52,8.74" target="#b2">[3]</ref> and YAGO2 <ref type="bibr" coords="1,262.24,435.50,10.52,8.74" target="#b3">[4]</ref> collections (in RDF NT format). In addition to this reference collection, we provided two supplementary collections, one in an article-centric XML format and one in a pure text format, respectively, in order to allow for a large variety of retrieval techniques, based on either RDF, XML, or text to be incorporated into this retrieval setting. Moreover, links among the Wikipedia, DBpedia 3.8, and YAGO2 URI's were provided (again in RDF NT format) in order to allow for an easy integration of all of the above data sources. Participants were thus free to choose their preferred format of the collections in order to submit their runs. The goal in organizing this new track thus follows one of the key themes of INEX, namely to explore and investigate if and how structural information could be exploited to improve the effectiveness of adhoc retrieval. In particular, we were interested in how this combination of data could be used together with structured queries Jeopardy-style natural-language clues and questions. The Linked Data Track thus aims to close the gap between IR-style keyword search and Semantic-Web-style reasoning techniques, with the goal to bring together different communities and to foster research at the intersection of Information Retrieval, Databases, and the Semantic Web. For INEX 2013, we specifically explored the following two retrieval tasks:</p><p>1 https://inex.mmci.uni-saarland.de/tracks/lod/ -The Ad-hoc Retrieval Task investigates informational queries to be answered mainly by the textual contents of the Wikipedia articles. -The Jeopardy Task employs natural-language Jeopardy clues which are manually translated into a semi-structured query format based on SPARQL with additional keyword-based filter conditions.</p><p>2 Data Collections</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Reference Collection</head><p>As for the reference collection, the Linked Data Track employed a combination of three data collections from Wikipedia, DBpedia 3.8 and YAGO2.</p><p>Since Wikipedia, DBpedia 3.8 and YAGO2 employ different URI's as identifiers for their target entities, valid results were restricted to a provided list of valid DBpedia URI's<ref type="foot" coords="3,225.45,350.96,3.97,6.12" target="#foot_0">2</ref> , which contains one RDF triple of the form &lt;DBpedia-URI&gt; lod:isValid "true" for each valid result entity; other forms of this list (such as valid Wikipedia article ids) were available on request. If a run included an entity not in this list, the entity was considered as non-relevant.</p><p>As in the previous year, the Linked Data Track was explicitly intended to be an "open track" and thus invited participants to include more Linked Data sources (see, e.g., http://linkeddata.org) or other sources that go beyond "just" DBpedia and YAGO2. Any inclusion of further data sources was welcome, however, workshop submissions and follow-up research papers should explicitly mention these sources when describing their approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Supplementary XML Collection</head><p>The new version of the XML-based Wikipedia-LOD (v2.0) collection (compare to <ref type="bibr" coords="3,148.58,553.12,10.52,8.74" target="#b5">[6,</ref><ref type="bibr" coords="3,160.76,553.12,7.75,8.74" target="#b1">2]</ref>) was again hosted at the Max Planck Institute for Informatics and has been made available for download in March 2013 from the Linked Data Track homepage. The collection consists of 4 compressed tar.gz files and contains an overall amount of 12.2 Million individual XML articles with more than 1 Billion XML elements. Each Wikipedia-LOD article consists of a mixture of XML tags, attributes, and CDATA sections, containing infobox attributes, free-text contents of the Wikipedia articles which describe the entity or category that the article captures, and a section with RDF properties exported from the DBpedia 3.8 and YAGO2 subsets of the reference collection that are related to the article's entity. All sections contain links to other Wikipedia articles (including links to the corresponding DBpedia and YAGO2 resources), Wikipedia categories, and external Web pages. Figure <ref type="figure" coords="4,260.43,154.86,4.98,8.74" target="#fig_0">1</ref> shows the structure of such a Wikipedia article in XML format about the entity Albert Einstein. It depicts the five main components of the XML markup of these articles: i) the metadata section, which contains information about the author, title, and id of the article, as well as possible links to other Linked Data URI's, ii) the infobox properties taken from the original attributes and values from the Wiki markup of this article, iii) the Wikipedia section with additional Linked Data links to related entities in Wikipedia, DBpedia 3.8, YAGO2, and links to external web pages, iv) the DBpedia properties section, with RDF properties from DBpedia 3.8 about the entity that is described by this article, and v) a similar section with YAGO2 properties about the entity that is described by this article.</p><p>Wikipedia to XML Conversion. For converting the raw Wikipedia articles into our XML format, we modified and substantially extended the wiki2xml parser<ref type="foot" coords="4,161.44,366.21,3.97,6.12" target="#foot_1">3</ref> as it is provided for the MediaWiki </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Supplementary Text Collection</head><p>As a second supplementary text collection, all XML articles of the Wikipedia-LOD v2.0 collection were once more transformed into a plain text format by extracting all the CDATA sections of the content-related XML elements (including the infobox and RDF properties sections). In order to keep the original text structure of the Wikipedia articles intact as much as possible, our transformation tools marks links, infobox tags, and RDF properties by additional brackets. All fulltext dumps of this second supplementary collection are available from the Linked Data Track homepage (https://inex.mmci.uni-saarland.de/tracks/lod/). A provided file<ref type="foot" coords="5,199.62,501.80,3.97,6.12" target="#foot_3">5</ref> again maps each DBpedia entity to its corresponding text file.</p><p>The resulting size of this text collection amounts to 11,945,084 files with nonempty text contents, with an overall size of 17 GB in uncompressed form and 5.5 GB in compressed form, respectively.</p><p>3 Retrieval Tasks and Topics</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Ad-hoc Task</head><p>The goal of the Ad-hoc Task is to return a ranked list of results in response to a search topic that is formulated as a keyword query. Results had to be represented by their Wikipedia page ID's, which in turn had to be linked to the set of valid Submission Format. Participants were allowed to submit up to 3 runs. Each run could contain a maximum of 1,000 results per topic, ordered in decreasing value of relevance. As in the previous year, a result is an article or an entity, identified by its Wikipedia page ID (so only entities from DBpedia or, equivalently, articles from Wikipedia were counted as valid results). The results of one run had to be contained in a single submission file, so up to three files could be submitted by each participant in total. Submissions were required to be in the familiar TREC format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt;qid&gt; Q0 &lt;file&gt; &lt;rank&gt; &lt;rsv&gt; &lt;run_id&gt;</head><p>Where:</p><p>-The first column is the topic number.</p><p>-The second column is the query number within that topic. As of the early TREC days, this field is unused and should always be Q0. -The third column is the ID of the result Wikipedia page.</p><p>-The fourth column is the rank of the result.</p><p>-The fifth column shows the score (integer or floating point) that generated the ranking. -The sixth column is called the "run tag" and should be a unique identifier for the participating group and for the method used. Run tags must contain 12 or fewer letters and numbers, with no punctuation, to facilitate labeling graphs with the tags.</p><p>An example submission thus might have looked as follows:</p><p>2013001 Q0 12 1 0.9999 2013UniXRun1 2013001 Q0 997 2 0.9998 2013UniXRun1 2013001 Q0 9989 3 0.9997 2013UniXRun1</p><p>This run contains three results for the topic 2013001. The first result is the Wikipedia target entity that is associated with the page ID "12". The second result is the page with ID "997", and so on. Mappings between Wikipedia page ID's and DBpedia URI's were available from the DBpedia-to-Wikipedia-Page-Links file which is part of the reference collection. Results were restricted to target entities in the list of valid DBpedia URI's (see above).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Jeopardy Task</head><p>As The &lt;jeopardy clue&gt; element contains the original Jeopardy clue as a naturallanguage sentence; the &lt;keyword title&gt; element contains a set of keywords that has been manually extracted from this title and has been reused as part of the Ad-hoc Task; and the &lt;sparql ft&gt; element contains the result of a manual conversion of the natural-language sentence into a corresponding SPARQL query. The category attribute of the &lt;topic&gt; element may be used as an additional hint for disambiguating the query. In the above query, ?s is a variable for an entity of type http://dbpedia.org/class/yago/GermanPoliticians (in the first triple pattern), and it should be in a http://dbpedia.org/property/successor relationship with another entity denoted by the variable ?s1. The FTContains filter condition restricts ?s to those entities that should be associated with the keywords "stepped down early" via its linked Wikipedia article.</p><p>Since this particular variant of SPARQL with full-text filter conditions cannot be run against a standard RDF collection (such as DBpedia 3.8 or YAGO2) alone, participants were again encouraged to develop individual solutions to index both the RDF and textual contents of the Wikipedia reference or supplementary collections in order to process these queries.</p><p>Submission Format. Similar to the Ad-hoc Task (see above), each participating group was allowed to submit up to 3 runs. Each run could contain a maximum of 1,000 results per topic, ordered by decreasing value of relevance (although we expect most topics to have just one or a combination of a few target entities). The results of one run must be contained in a single submission file, that is, up to 3 files could be submitted per group in total. For relevance assessments and evaluation of the results, the runs were again required to be in the familiar TREC format, however containing one row of target entities (denoted by their Wikipedia page ID's, which are available in the reference collection through the http://dbpedia.org/ontology/wikiPageID properties) for each query result. Each row of target entities must reflect the order of query variables as specified by the SELECT clause of the Jeopardy topic. In case the SELECT clause contained more than one query variable, the row should consist of a comma-or semicolonseparated list of such target entity ID's. Thus, an example submission may have looked as follows: 2012374 Q0 12;24 1 0.9999 2012UniXRun1 2012374 Q0 997;998 2 0.9998 2012UniXRun1 2012374 Q0 9989;12345 3 0.9997 2012UniXRun1</p><p>Here, there are 3 results for topic "2012374"; and we can see this topic requests two entities per result, since it has two variables in the SELECT clause. The first result is the entity pair (denoted by their Wikipedia page ID's) with the ID's "12" and "24", the second result is the entity pair with the ID's "997" and "998", and the third result is the entity pair with the ID's "9989" and "12345". For the evaluation, symmetric results, where the order of the returned entities did not matter, were considered as duplicates and were automatically removed at the lower rank of the run at which the duplicate occurred. Mappings between DBpedia URI's and Wikipedia page ID's were available from the DBpedia-to-Wikipedia-Page-Links file which was part of the reference collection. And, again, results were restricted to target entities contained in the list of valid DBpedia URI's (see above).</p><p>All run submissions were to be uploaded via the INEX website via the URL: https://inex.mmci.uni-saarland.de/. The due date for the submission of all Linked Data Track runs was May 15, 2012. In total, 5 Ad-hoc search runs were submitted by 2 participants, i.e., Oslo and Akershus University College of Applied Sciences (OAUC), Renmin University of China (RUC), and 3 Jeopardy runs were submitted by the Max-Planck Institute for Informatics (MPI).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Assessments</head><p>For the Ad-hoc Task, assessments for the 72 single-entity Jeopardy topics were done on Amazon Mechanical Turk by pooling the top-100 ranks from the 8 submitted runs in a round-robin fashion. Assessments for the remaining 72 Ad-hoc Task topics from INEX 2009 and 2010 were taken over from the previous years. (Notice that the latter provide only an approximation of the actual relevance judgments for these topics, since the collection has meanwhile changed.) Table <ref type="table" coords="9,475.61,311.22,4.98,8.74" target="#tab_3">2</ref> provides detailed statistics about the assessments of the 144 Ad-hoc Task topics. For the Jeopardy Task, assessments for 77 single-and multi-entity topics were additionally done on Crowdflower by pooling the top-10 results from the 3 Jeopardy submissions for the single-entity topics and by pooling the top-20 for the multi-entity topics, respectively, again in a round-robin fashion. These assessments were done based on an entity-centric rather than a document-centric evaluation mode, i.e., there was usually just a single target entity (or a short list of target entities) to marked as relevant for a given SPARQL-FT topic. Overall, 144 Ad-hoc topics and 77 Jeopardy topics were finally assessed this way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Metrics</head><p>The TREC-eval tool was adapted to calculate the following well-known metrics (see <ref type="bibr" coords="9,153.97,656.12,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="9,166.15,656.12,7.75,8.74" target="#b4">5]</ref>) used in ad-hoc and entity ranking settings: Precision, Recall, Average-Precision (AP), Mean-Average-Precision (MAP), Mean-Reciprocal-Rank (MRR), and Normalized-Discounted-Cumulated-Gain (NDCG).</p><p>For the Ad-hoc Task, we employed the usual binary relevance assessments obtained from a majority vote over the judgments obtained from AMT for each result. For the Jeopardy Task, which yielded different QRels than the Ad-hoc Task, we additionally had to distinguish between four types of search topics in order to obtain similar binary relevance assessments. These four types divide the set of 105 Jeopardy topics as follows:</p><p>-46 single-entity, single-target topics: these are typical Jeopardy clues which have just one relevant target entity as result. -27 single-entity, multiple-target topics: these are entity-centric topics which may have an entire list of relevant target entities as result. -17 multiple-entity, single-target topics: these are enhanced Jeopardy clues which have just one combination of relevant target entities as result. -15 multiple-entity, multiple-target topics: these are enhanced entitycentric topics which may have an entire list of combinations of relevant target entities as result.</p><p>For the multiple-entity topics, a combination of entities was considered to be relevant at a particular rank, only if all the entities of this combination formed a correct answer to the topic. That is, relevance judgments for Jeopardy topics were still based on binary assessments. Moreover, duplicate results (including duplicates due to symmetric answers for multi-entity topics) were removed from the lower ranks of the run files at which they occurred. For completeness, we next list the detailed definition of the above metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P recision(P ) =</head><p>N umber of relevant results returned T otal number of results returned</p><p>Precision-at-k (P @k) = N umber of relevant results at rank k k</p><p>Precision(P) is defined as the ability of a system to present all relevant items. It is a simple statistical set-based measure calculated as shown by Equation <ref type="formula" coords="10,472.84,508.31,3.87,8.74" target="#formula_0">1</ref>. Precision-at-K (P@K) is the portion of the relevant documents in the first K ranks and is calculated as shown by Equation <ref type="formula" coords="10,337.45,532.22,3.87,8.74" target="#formula_1">2</ref>.</p><formula xml:id="formula_2" coords="10,193.53,550.97,287.07,22.31">Recall(R) = N umber of relevant results returned T otal number of relevant results<label>(3)</label></formula><p>Recall(R) is also a set-based measure that can be perceived as the probability of a system to return correct entities. It can be computed as shown in Equation <ref type="formula" coords="10,472.84,595.81,3.87,8.74" target="#formula_2">3</ref>.</p><p>A standard technique to compute Interpolated-Precision (iP) at a given recall level is to use the maximum precision for any actual recall level greater than or equal to the recall level in question. This is modeled by Equation <ref type="formula" coords="10,422.91,631.67,3.87,8.74" target="#formula_3">4</ref>.</p><formula xml:id="formula_3" coords="10,205.87,652.53,274.72,9.65">Interpolated -Precision-at-k = max k &gt;k (P (k )),<label>(4)</label></formula><p>where k and k are recall levels.</p><p>To measure the average performance of a system over a set of queries, each with different number of relevant entities, we compute the Interpolated-Precision at a set of 11 standard recall levels ( specifically, 1%, 10%, 20%, 30%, 40%, 50%, 60%, 70%, 80%, 90% and 100%). Average-Interpolated-Precision (AiP) is a single-valued measure that reflects the performance of an engine over all the relevant results. We thus report the Mean-Average-Interpolated-Precision (MAiP) that reflects the performance of a system over all the results. This is simply the mean of the AiP for each topic as shown by Equation <ref type="formula" coords="11,421.11,234.55,3.87,8.74" target="#formula_4">5</ref>.</p><formula xml:id="formula_4" coords="11,144.13,256.06,332.21,31.63">Mean-Average-Interpolated -Precision(MAiP ) = 1 |Q| |Q| j=1 1 m j mj k=1 iP (RL j ) (<label>5</label></formula><formula xml:id="formula_5" coords="11,476.34,267.55,4.24,8.74">)</formula><p>where |Q| is the total number of topics, m j is the total number of relevant results for topic q j , RL j is the ranked list of results returned for topic q j .</p><p>The Reciprocal-Rank (1/r) of a query can be defined as the rank r at which a system returns the first relevant entity. In our case, we report the average of the reciprocal rank scores over all topics, known as Mean-Reciprocal-Rank (MRR).</p><p>Finally, we present the Normalized-Discounted-Cumulative-Gain (NDCG) at top 5, 10 and 15 results to evaluate systems in an ah-hoc and entity-oriented retrieval setting. Discounted-Cumulative-Gain (DCG) uses a graded relevance scale to measure the gain of a system based on the positions of the relevant entities in the result set. This measure gives a lower gain to relevant entities returned in the lower ranks to that of the higher ranks. This makes a sensible measure to use for our task as we reward engines that retrieve relevant results at the top ranks. NDCG reports a single-valued score by normalizing the DCG, thus accounting for differently sized output lists. N DCG(Q, K), i.e., NDCG at K for a set of queries Q, is computed as shown in Equation <ref type="formula" coords="11,398.30,503.18,3.87,8.74" target="#formula_6">6</ref>.</p><formula xml:id="formula_6" coords="11,210.29,524.91,266.06,31.18">N DCG(Q, k) = 1 |Q| |Q| j=1 Z kj k m=1 2 R(j,m) -1 log 2 (1 + m) (<label>6</label></formula><formula xml:id="formula_7" coords="11,476.35,536.18,4.24,8.74">)</formula><p>where |Q| is the total number of topics, R(j, e) is the binary relevance score obtained for an individual result of topic j, Z jk is the normalization factor, k is the rank at which NDCG is calculated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ad-hoc Task</head><p>As mentioned above, 144 Ad-hoc Task topics were collected from two different sources: 72 of them are old topics from INEX 2009 and 2010, and 72 of them are single-entity Jeopardy topics. In this section, we will first present the evaluation results over the whole set of Ad-hoc Task topics for all the submitted runs, and then analyze the effectiveness of the runs for each of the two sets of topics. Table <ref type="table" coords="12,161.59,215.91,4.98,8.74">3</ref> presents the evaluation results for all the 8 submitted runs. Even though 3 runs were submitted to the Jeopardy Task, we evaluated them altogether since there are 72 Ad-hoc Task topics are the same as for the Jeopardy Task. The results show that the 3 Jeopardy runs have higher the Mean-Reciprocal-Rank (MRR), which means most of time they returned the first relevant results earlier than other runs. But in terms of MAiP and other metrics, the run from RUC performed best. Table <ref type="table" coords="12,232.72,287.65,4.98,8.74" target="#tab_4">4</ref> shows the evaluation results for the 5 Ad-hoc Task runs over the 72 old topics from the INEX 2009 and 2010 Ad-Hoc Tasks. Table <ref type="table" coords="12,475.61,299.60,4.98,8.74" target="#tab_5">5</ref> shows the results over the 72 single-entity Jeopardy topics for all the submitted runs, now evaluated by MRR. We can observe that the 3 runs submitted to the Jeopardy Task have much higher MRR. That means that most of time they returned the first relevant results earlier than the other 5 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Jeopardy Task</head><p>Table <ref type="table" coords="12,163.30,390.54,4.98,8.74" target="#tab_6">6</ref> depicts the detailed retrieval results for the 3 runs submitted to the Jeopardy Task by the MPI group, which was the only group that participated in this task. These evaluations are based on the distinct set of QRels, which were specifically created for the Jeopardy Task (see above).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>The Linked Data Track, which was continued track in INEX 2012, was organized towards our goal to close the gap between IR-style keyword search and Semantic-Web-style reasoning techniques. The track thus continues one of the earliest guiding themes of INEX, namely to investigate whether structure may help to improve the results of ah-hoc keyword search. As a core of this effort, we introduced a new and much larger supplementary XML collection, coined Wikipedia-LOD v2.0, with XML-ified Wikipedia articles which were additionally annotated with RDF properties from both DBpedia 3.8 and YAGO2. However, due to the very low number of participating groups, in particular for the Jeopardy, detailed comparisons of the underlying ranking and evaluation techniques can only be drawn very cautiously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>MAiP MRR P@5 P@10 P@20 P@30 ruc-all-2200 0.454 0. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,147.13,458.07,321.10,7.89"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. XML-ified Wikipedia articles with DBpedia 3.8 and YAGO2 properties.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="13,143.41,254.70,328.53,253.36"><head></head><label></label><figDesc></figDesc><graphic coords="13,143.41,254.70,328.53,253.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="15,134.77,298.11,345.83,175.91"><head></head><label></label><figDesc></figDesc><graphic coords="15,134.77,298.11,345.83,175.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,134.77,117.75,345.83,180.63"><head>Table 1 .</head><label>1</label><figDesc>Wikipedia-LOD v2.0 collection statistics. DBpedia URI's (see above). A set of 144 Ad-hoc Task search topics for the INEX 2013 Linked Data track had been released in March 2013 and was made available for download from the Linked Data Track homepage. In addition, the set of QRels from the 2012 Ad-Hoc Task topics was provided for training.</figDesc><table coords="6,210.24,117.75,191.81,74.44"><row><cell>Property</cell><cell>Count</cell></row><row><cell>XML documents</cell><cell>12,216,083</cell></row><row><cell>XML elements</cell><cell>1,169,642,510</cell></row><row><cell cols="2">Internal Wikipedia links resolved 144,481,793</cell></row><row><cell>Wikipedia URI's resolved</cell><cell>215,621,680</cell></row><row><cell>DBpedia URI's resolved</cell><cell>144,497,401</cell></row><row><cell>YAGO2 URI's resolved</cell><cell>156,763,342</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,134.77,231.43,355.66,353.15"><head></head><label></label><figDesc>in 2012, the Jeopardy Task continued to investigate retrieval techniques over a set of natural-language Jeopardy clues, which were manually translated into SPARQL query patterns with additional keyword-based filter conditions. A set of 105 Jeopardy Task search topics, out of which 74 topics were taken over from 2012 and 31 topics were newly added to the 2013 setting. 72 single-entity topics (with one query variable) were also included into the set of 144 Ad-hoc topics. All topics were made available for download in March 2013 from the Linked Data Track homepage. In analogy to the Ad-hoc Task, the set of topics from 2012 was provided together with their QRels for training.We illustrate the topic format with the example of topic 2012374 from the set of the 2013 topics. It is represented in XML format as follows:</figDesc><table coords="7,134.77,373.04,355.66,211.54"><row><cell>&lt;topic id="2012374" category="Politics"&gt;</cell></row><row><cell>&lt;jeopardy_clue&gt;</cell></row><row><cell>Which German politician is a successor of another politician</cell></row><row><cell>who stepped down before his or her actual term was over,</cell></row><row><cell>and what is the name of their political ancestor?</cell></row><row><cell>&lt;/jeopardy_clue&gt;</cell></row><row><cell>&lt;keyword_title&gt;</cell></row><row><cell>German politicians successor other stepped down before</cell></row><row><cell>actual term name ancestor</cell></row><row><cell>&lt;/keyword_title&gt;</cell></row><row><cell>&lt;sparql_ft&gt;</cell></row><row><cell>SELECT ?s ?s1 WHERE {</cell></row><row><cell>?s rdf:type &lt;http://dbpedia.org/class/yago/GermanPoliticians&gt;.</cell></row><row><cell>?s1 &lt;http://dbpedia.org/property/successor&gt; ?s.</cell></row><row><cell>FILTER FTContains (?s, "stepped down early").</cell></row><row><cell>}</cell></row><row><cell>&lt;/sparql_ft&gt;</cell></row><row><cell>&lt;/topic&gt;</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,134.77,368.35,345.83,104.90"><head>Table 2 .</head><label>2</label><figDesc>Topic Set Number of topics Number of relevant results per topic Total Statistics of the assessment results for the 72 Ad-hoc Task topics from INEX 2009/2010 and the 72 Jeopardy Task topics of INEX 2013.</figDesc><table coords="9,143.55,383.32,323.54,53.89"><row><cell></cell><cell></cell><cell cols="4">Min Max Median Mean Std. Deviation</cell><cell></cell></row><row><cell>2009/2010</cell><cell>72</cell><cell>24 95</cell><cell>63</cell><cell>63</cell><cell>16</cell><cell>4542</cell></row><row><cell>Jeopardy</cell><cell>72</cell><cell>3 72</cell><cell>26</cell><cell>27</cell><cell>12</cell><cell>1929</cell></row><row><cell>all</cell><cell>144</cell><cell>3 95</cell><cell>42</cell><cell>45</cell><cell>23</cell><cell>6471</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="14,134.77,188.49,345.82,368.04"><head>Table 4 .</head><label>4</label><figDesc>Evaluation results for all Ad-hoc Task runs over the 72 INEX 2009 and 2010 topics.</figDesc><table coords="14,335.87,188.49,143.36,7.72"><row><cell>9491 0.8389 0.8153 0.7833 0.7648</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="14,134.77,580.37,345.83,18.85"><head>Table 5 .</head><label>5</label><figDesc>Evaluation results for all Ad-hoc Task runs over the 72 INEX 2013 singleentity Jeopardy topics.</figDesc><table coords="15,138.93,117.63,337.37,177.67"><row><cell></cell><cell cols="3">MPIUltimatum Phrase MPIUltimatum NoPhrase MPISupremacy</cell></row><row><cell>MAiP</cell><cell>0.7491</cell><cell>0.701</cell><cell>0.719</cell></row><row><cell>MRR</cell><cell>0.7671</cell><cell>0.7358</cell><cell>0.7539</cell></row><row><cell cols="2">NDCG@5 0.7723</cell><cell>0.7307</cell><cell>0.7393</cell></row><row><cell cols="2">NDCG@10 0.7864</cell><cell>0.7347</cell><cell>0.7598</cell></row><row><cell cols="2">NDCG@15 0.7968</cell><cell>0.7484</cell><cell>0.7728</cell></row><row><cell>AiP@1%</cell><cell>0.7804</cell><cell>0.7411</cell><cell>0.7669</cell></row><row><cell cols="2">AiP@10% 0.7804</cell><cell>0.7411</cell><cell>0.7669</cell></row><row><cell cols="2">AiP@20% 0.7804</cell><cell>0.731</cell><cell>0.7653</cell></row><row><cell cols="2">AiP@30% 0.7804</cell><cell>0.731</cell><cell>0.763</cell></row><row><cell cols="2">AiP@40% 0.7772</cell><cell>0.7255</cell><cell>0.7468</cell></row><row><cell cols="2">AiP@50% 0.7737</cell><cell>0.7232</cell><cell>0.7417</cell></row><row><cell cols="2">AiP@60% 0.7337</cell><cell>0.6803</cell><cell>0.6991</cell></row><row><cell cols="2">AiP@70% 0.7245</cell><cell>0.6771</cell><cell>0.6952</cell></row><row><cell cols="2">AiP@80% 0.7223</cell><cell>0.6747</cell><cell>0.685</cell></row><row><cell cols="2">AiP@90% 0.7208</cell><cell>0.673</cell><cell>0.6817</cell></row><row><cell cols="2">AiP@100% 0.7208</cell><cell>0.6662</cell><cell>0.6694</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="15,134.77,478.83,345.83,18.85"><head>Table 6 .</head><label>6</label><figDesc>Evaluation results for the three Jeopardy Task runs over the set of 105 INEX 2013 topics.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="3,144.73,657.44,305.41,7.47"><p>http://inex-lod.mpi-inf.mpg.de/2013/List of Valid DBpedia URIs.ttl</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="4,144.73,646.48,225.95,7.47"><p>http://www.mediawiki.org/wiki/Extension:Wiki2xml</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="4,144.73,657.44,117.68,7.47"><p>http://www.mediawiki.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="5,144.73,657.44,287.15,7.47"><p>http://inex-lod.mpi-inf.mpg.de/2013/dbpedia-textfiles-map.ttl</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgements</head><p>This work is partially supported by <rs type="programName">National 863 High-tech</rs> project, No: <rs type="grantNumber">2012AA011001</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_YhxWV8a">
					<idno type="grant-number">2012AA011001</idno>
					<orgName type="program" subtype="full">National 863 High-tech</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>MAiP MRR P@5 P@10 P@20 P@30 ruc-all-2200 0.3733 0.8772 0.7028 0.6424 0.  <ref type="table" coords="13,163.10,527.04,4.13,7.89">3</ref>. Evaluation results for all submitted runs over all the 144 Ad-hoc Task topics.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="13,138.35,591.37,342.24,7.86;13,146.91,602.33,128.39,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="13,317.96,591.37,162.63,7.86;13,146.91,602.33,11.14,7.86">XML search: languages, INEX and scoring</title>
		<author>
			<persName coords=""><surname>Amer-Yahia</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Sihem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mounia</forename><surname>Lalmas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,165.85,602.33,66.38,7.86">SIGMOD Record</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,138.35,612.96,342.25,7.86;13,146.91,623.92,333.68,7.86;13,146.91,634.88,333.68,7.86;13,146.91,645.84,333.68,7.86;13,146.91,656.80,134.60,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="13,385.88,645.84,90.51,7.86">Report on INEX 2012</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bellot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chappell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Geva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gurajada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kazai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koolen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Landoni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Marx</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Moriceau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mothe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Preminger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ramírez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sanjuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Scholer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Schuh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Tannier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Trappett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Trotman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,146.91,656.80,53.49,7.86">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="50" to="59" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,138.35,535.00,342.24,7.86;15,146.91,545.96,333.68,7.86;15,146.91,556.91,77.82,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="15,180.35,545.96,230.56,7.86">DBpedia -A crystallization point for the Web of Data</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hellmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,423.74,545.96,52.39,7.86">J. Web Sem</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="154" to="165" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,138.35,568.29,342.24,7.86;15,146.91,579.25,333.68,7.86;15,146.91,590.21,20.99,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="15,397.58,568.29,83.01,7.86;15,146.91,579.25,229.38,7.86">YAGO2: A spatially and temporally enhanced knowledge base from Wikipedia</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Berberich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,384.25,579.25,48.27,7.86">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="28" to="61" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,138.35,601.59,342.24,7.86;15,146.91,612.54,333.68,7.86;15,146.91,623.50,167.92,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="15,418.47,601.59,62.12,7.86;15,146.91,612.54,80.35,7.86">Focused Access to XML Documents</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pehcevski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kazai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lalmas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,239.26,612.54,180.97,7.86">chapter on INEX 2007 Evaluation Measures</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="24" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,138.35,634.88,342.25,7.86;15,146.91,645.84,333.68,7.86;15,146.91,656.80,20.99,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="15,341.91,634.88,138.68,7.86;15,146.91,645.84,229.64,7.86">Design and evaluation of an IRbenchmark for SPARQL queries with fulltext conditions</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gurajada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Theobald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,398.91,645.84,25.90,7.86">ESAIR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="9" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,138.35,119.67,342.24,7.86;16,146.91,130.63,333.68,7.86;16,146.91,141.59,333.68,7.86;16,146.91,152.55,120.30,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="16,146.91,141.59,204.95,7.86">Overview of the INEX 2012 Linked Data Track</title>
		<author>
			<persName coords=""><forename type="first">Qiuyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Georgina</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Camps</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>Marx</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anne</forename><surname>Schuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sairam</forename><surname>Gurajada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arunav</forename><surname>Mishra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,380.51,141.59,100.09,7.86;16,146.91,152.55,91.65,7.86">CLEF (Online Working Notes/Labs/Workshop)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
