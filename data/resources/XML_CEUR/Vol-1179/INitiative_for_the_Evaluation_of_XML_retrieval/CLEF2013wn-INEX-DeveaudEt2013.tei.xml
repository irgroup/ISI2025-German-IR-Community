<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,134.77,116.95,345.82,12.62;1,146.79,134.89,321.78,12.62;1,254.40,152.82,106.57,12.62">Effective Tweet Contextualization with Hashtags Performance Prediction and Multi-Document Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,221.42,190.49,74.72,8.74"><forename type="first">Romain</forename><surname>Deveaud</surname></persName>
							<email>romain.deveaud@univ-avignon.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">LIA -University of Avignon</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,323.30,190.49,66.17,8.74"><forename type="first">Florian</forename><surname>Boudin</surname></persName>
							<email>florian.boudin@univ-nantes.fr</email>
							<affiliation key="aff1">
								<orgName type="institution">LINA -University of Nantes</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,134.77,116.95,345.82,12.62;1,146.79,134.89,321.78,12.62;1,254.40,152.82,106.57,12.62">Effective Tweet Contextualization with Hashtags Performance Prediction and Multi-Document Summarization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">031DD0201DB90A4CCD6B90D1606EC9C3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we describe our participation in the INEX 2013 Tweet Contextualization track and present our contributions. Our approach is the same as last year, and is composed of three main components: preprocessing, Wikipedia articles retrieval and multi-document summarization. We however took advantage of a larger use of hashtags in the topics and used them to enhance the retrieval of relevant Wikipedia articles. We also took advantage of the training examples from last year which allowed us to learn the weights of each sentence selection feature. Two of our submitted runs achieved the two best informativeness results, while our generated contexts where almost as readable as those of the most readable system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Tweets are short and ambiguous by nature and it can be hard for a user without any background knowledge to understand what the Tweet is about. The INEX Tweet Contextualization track makes the assumption that it is possible to overcome this lack of knowledge by providing the user with a bunch of sentences that give some context or additional information about the Tweet. While topical information may certainly be the most important for this task, one may also want some political context to understand a sarcastic Tweet for example. Our approach specifically focuses on the topical context, and aims at producing informative contexts.</p><p>Within the framework of this track, sentences must be extracted from the version of Wikipedia provided by the organizers. Our approach sequentially involves Information Retrieval (IR) and Text Summarization (TS) techniques. First, we extend the Tweet's topical context by retrieving related Wikipedia articles that are likely to contain contextually relevant sentences or passages. Then, we tackle the context generation step as a summarization task where we summarize the retrieved Wikipedia articles. The sentences achieving the best linear combination of weighted features are added to the context (in the 500 words limit established by the organizers). So far, this approach is the same as the one we experimented last year <ref type="bibr" coords="2,175.62,119.99,10.52,8.74" target="#b1">[2,</ref><ref type="bibr" coords="2,186.14,119.99,7.01,8.74" target="#b5">6]</ref>, we however added a hashtag performance prediction component to the Wikipedia retrieval step.</p><p>The rest of the paper is organized as follows. Section 2 describes the process we followed to extract candidate sentences, which includes Tweet formatting and document retrieval on Wikipedia. Then, we describe in Section 3 the various sentence-level features that we used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Candidate Sentence Extraction</head><p>Considering that the task is to provide context from Wikipedia text, one crucial step is to retrieve Wikipedia articles that are relevant to the Tweet. Hopefully, these articles contain sentences that provide enough contextual information to (fully) understand the meaning of the Tweet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">#HashtagSplitting and Tweet formatting</head><p>Hashtags in Tweets are very important pieces of information, since they are tags that were generated by the user. Making a parallel with TREC-like topics, we can view the hashtags as the title while the Tweet itself is the description.</p><p>However the main problem with hashtags is that they often are composed of several words concatenated together (e.g. #WhitneyHouston). We used an algorithm based on Peter Novig's chapter on "Natural Language Corpus Data" in <ref type="bibr" coords="2,146.64,388.81,10.52,8.74" target="#b6">[7]</ref> to split the hashtags. For each Tweet, all the hashtags we converted into a short keyword query.</p><p>We also removed all the retweet mentions (RT), user mentions (@somebody) and stopwords (based on the standard INQUERY stoplist) from the Tweets. The final output of this Tweet formatting process is a clean Tweet without stopwords or useless mentions, as well as a very short and user-generated representation of this Tweet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Retrieving Wikipedia articles</head><p>Retrieving relevant Wikipedia articles is the first crucial part for finding contextually relevant sentences. For this purpose we use the well-known Markov Random Field model <ref type="bibr" coords="2,229.42,532.19,10.52,8.74" target="#b3">[4]</ref> to represent dependencies between query words. It has indeed performed consistently well on several variety of ad-hoc search tasks across the years.</p><p>Given an initial Tweet T , the output of the method described in the previous section is a set of hashtags H T and a set of terms Q T . We then score a Wikipedia article D according to the following function:</p><formula xml:id="formula_0" coords="2,154.22,624.54,306.91,9.65">s(H T , Q T , D) = α × score M RF (H T , D) + (1 -α) × score M RF (Q T , D)</formula><p>where α is the parameter which controls the influence of the hashtags with respect to the entire Tweet text. We describe in the following section how we set this parameter. We used the Sequential Dependence Model instantiation of MRF, which is defined as follows:</p><formula xml:id="formula_1" coords="3,214.86,153.91,185.64,93.97">score M RF (Q, D) = λ T q∈Q f T (q, D) + λ O |Q|-1 i=1 f O (q i , q i+1 , D) + λ U |Q|-1 i=1 f U (q i , q i+1 , D)</formula><p>where the features weights are set according to the author's recommendation (λ T = 0.85, λ O = 0.1, λ U = 0.05). f T , f O and f U are the log maximum likelihood estimates of query terms in document D, computed over the target collection with a Dirichlet smoothing (µ = 2500).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Hashtags performance prediction</head><p>The importance of hashtags is also contextual. Since they can sometimes be noise rather than useful pieces of information, we need an automatic way of setting a varying α for each Tweet. We thus rely on a well-known pre-retrieval query performance predictor: the clarity score <ref type="bibr" coords="3,337.87,376.51,9.96,8.74" target="#b0">[1]</ref>. This score being actually the Kullback-Leibler divergence between the hashtags language model and the background Wikipedia collection language model, it is formally defined as:</p><formula xml:id="formula_2" coords="3,246.11,419.39,121.95,26.80">α = w∈V P (w|H T ) P (w|H T ) P (w|C)</formula><p>where V is the vocabulary. The hashtags language model is estimated through pseudo-relevant feedback:</p><formula xml:id="formula_3" coords="3,232.64,489.96,150.08,20.06">P (w|H T ) = D∈R P (w|D)P (D|H T )</formula><p>The set R of pseudo-relevant documents is composed of the top 5 ranked Wikipedia articles for a the H T query. Then α achieves higher values when documents of R are homogeneous and different from the background documents of the collection. This parameter thus allows us to predict if hashtags are discriminative, and to weigh their importance in the query accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Sentence scoring</head><p>From the ranked list of Wikipedia articles, we only consider the top 5 articles as relevant. The underlying assumption is that a Tweet may discuss only a very limited amount of topics, due to the 140 characters limit. Since encyclopedic topics are very well delimited between Wikipedia articles, we think that 5 articles is a reasonable number allowing us to avoid topic drift while hopefully providing a comprehensive coverage of the Tweet's topics. After selecting the 5 best ranked Wikipedia articles with respect to a Tweet T , the next step is sentence segmentation. Each article is divided into sentences using the nltk<ref type="foot" coords="4,424.53,154.28,3.97,6.12" target="#foot_0">1</ref> toolkit. We describe in this section the various scoring methods we use to estimate their importance with respect to the Tweet's context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sentence features</head><p>We computed several features for each candidate sentence in order to further rank them and produce the Tweet's context. There are four categories of features:</p><p>centrality of the sentence within the Wikipedia article from which the sentence is extracted, relevance of the sentence with respect to the Tweet (also including hashtags), relevance of the sentence with respect to an URL embedded in the Tweet, relevance of the Wikipedia article from which the sentence is extracted.</p><p>All the computed features use cleansed versions of sentences and Tweets. We remove stopwords and stem remaining words using the standard Porter stemming algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence centrality</head><p>The importance of a sentence within the document where it appears is estimated using the TextRank <ref type="bibr" coords="4,326.66,387.68,10.52,8.74" target="#b4">[5]</ref> algorithm.</p><p>Sentence relevance regarding the Tweet We compute the word overlap and the cosine similarity between the candidate sentence and the entire Tweet, and also between the candidate sentence and the hashtags alone.</p><p>Sentence relevance regarding the URL Tweets sometimes provide link to external web pages which generally contain a lot of contextual information. Organizers consider these web pages as the "answer" of the question asked by the Tweet. This is why using these web pages (even automatically) is considered as a manual run in the Tweet Contextualization track. Considering it worked very well for us last year, we still computed some features using the text of these web pages. More specifically, we compute the word overlap and the cosine similarity between the candidate sentence and the entire text of the linked page, as well as with the title of the web page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wikipedia article relevance</head><p>The articles from which candidate sentences are extracted contain different contextual information and thus have different importance. Then, a sentence belonging to a high ranked document has a higher chance of being relevant. We use as feature the probability of the document from which the candidate sentence has been extracted.</p><p>Final score of a candidate sentence We compute the final importance score of each sentence as a weighted linear combination of the above features. The weights were learned using the 2012 data and are presented in Table <ref type="table" coords="5,436.96,143.90,3.87,8.74" target="#tab_0">1</ref>. After every sentence has been attributed a score, they are ordered and the top-ranked sentences are selected to form context (within the limit of 500 words). If two sentences are extracted from the same document, we keep their original order to improve readability and coherence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Runs</head><p>We submitted three different runs this year, which we describe in this section.</p><p>LIA-title-only-notrain This first run only uses features c2, c3 and c10 without using the trained weights. Sentences of the context are thus ordered using their linear combination of three features.</p><p>LIA-all-notrain For this run we use all features described in the previous section without using the trained weights.</p><p>LIA-all-train Finally, this runs uses all features combined with the weights from Table <ref type="table" coords="5,162.16,576.08,3.87,8.74" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Official Results</head><p>We report in Table <ref type="table" coords="5,219.83,633.20,4.98,8.74" target="#tab_1">2</ref> the official results released by the organizers of the 10 best performing systems. The evaluation measure computes divergences <ref type="bibr" coords="5,438.39,645.16,9.96,8.74" target="#b5">[6]</ref>, hence lower scores are better. We see that our approach performed very well and achieved the best results of the track. Although our best performing run was tagged as manual, we did not manually intervene at any time in our contextualization process.</p><p>In Table <ref type="table" coords="6,188.57,346.71,4.98,8.74" target="#tab_2">3</ref> are reported the readability results of the top 10 best systems. Although our best informative run does not achieve the best readability results, we see that it is very close to the run 275. It also produces the less redundant contexts overall. We however do not clearly understand why our three runs achieve such different readability results since the context generation process is the same. We can for example hypothesize from Table <ref type="table" coords="6,326.41,406.48,4.98,8.74" target="#tab_1">2</ref> that the contexts which LIA-titleonly-notrain outputs are very similar to those of LIA-all-notrain. Then what could explain such a huge readability difference between the two (very similar) approaches? We think that these problems are worth further investigation. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,135.95,180.72,343.47,144.69"><head>Table 1 .</head><label>1</label><figDesc>Feature weights used for our 2013 runs, learned on the 2012 available data.</figDesc><table coords="5,213.79,180.72,188.88,122.46"><row><cell></cell><cell>Name</cell><cell>Value Significance</cell></row><row><cell>c1</cell><cell>TextRank</cell><cell>8.996 p &lt; 2 -16</cell></row><row><cell>c2</cell><cell>Overlap Tweet</cell><cell>2.496 p = 2.38 -6</cell></row><row><cell>c3</cell><cell>Cosine Tweet</cell><cell>5.849 p = 4 -15</cell></row><row><cell>c4</cell><cell cols="2">Overlap hashtags -2.051 p = 0.1368</cell></row><row><cell>c5</cell><cell>Cosine hashtags</cell><cell>0.671 p = 0.3074</cell></row><row><cell>c6</cell><cell>Overlap title URL</cell><cell>1.373 p = 0.2719</cell></row><row><cell>c7</cell><cell>Cosine title URL</cell><cell>0.788 p = 0.6287</cell></row><row><cell>c8</cell><cell>Overlap text URL</cell><cell>0.543 p = 0.4337</cell></row><row><cell>c9</cell><cell cols="2">Cosine text URL 10.374 p = 0.0195</cell></row><row><cell cols="2">c10 Document score</cell><cell>0.782 p &lt; 2 -16</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,134.77,121.96,345.83,155.65"><head>Table 2 .</head><label>2</label><figDesc>Official informativeness results of the 2013 Tweet Contextualization track (top 10 best performing systems). Starred runs are manual runs.</figDesc><table coords="6,216.42,121.96,179.44,122.46"><row><cell>Run</cell><cell>All.skip All.bi All.uni</cell></row><row><cell>LIA-all-notrain  *</cell><cell>0.8861 0.881 0.782</cell></row><row><cell cols="2">LIA-title-only-notrain 0.8943 0.8908 0.7939</cell></row><row><cell>275</cell><cell>0.8969 0.8924 0.8061</cell></row><row><cell>273</cell><cell>0.8973 0.8921 0.8004</cell></row><row><cell>274</cell><cell>0.8974 0.8922 0.8009</cell></row><row><cell>LIA-all-train  *</cell><cell>0.8998 0.8969 0.7987</cell></row><row><cell>254</cell><cell>0.9242 0.9229 0.8331</cell></row><row><cell>276</cell><cell>0.9301 0.927 0.8169</cell></row><row><cell>270</cell><cell>0.9397 0.9365 0.8481</cell></row><row><cell>267</cell><cell>0.9468 0.9444 0.8838</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,134.77,489.12,345.82,154.97"><head>Table 3 .</head><label>3</label><figDesc>Official readability results of the 2013 Tweet Contextualization track (top 10 best performing systems).</figDesc><table coords="6,137.83,489.12,339.73,117.05"><row><cell>Run</cell><cell cols="4">Mean Average Relevancy Non redundancy Soundness Syntax</cell></row><row><cell>275</cell><cell cols="2">72.44% 76.64%</cell><cell cols="2">67.30% 74.52% 75.50%</cell></row><row><cell>LIA-all-notrain</cell><cell>72.13%</cell><cell>74.24%</cell><cell>71.98%</cell><cell>70.78% 73.62%</cell></row><row><cell>274</cell><cell>71.71%</cell><cell>74.66%</cell><cell>68.84%</cell><cell>71.78% 74.50%</cell></row><row><cell>273</cell><cell>71.35%</cell><cell>75.52%</cell><cell>67.88%</cell><cell>71.20% 74.96%</cell></row><row><cell>LIA-all-train</cell><cell>69.54%</cell><cell>72.18%</cell><cell>65.48%</cell><cell>70.96% 72.18%</cell></row><row><cell>254</cell><cell>67.46%</cell><cell>73.30%</cell><cell>61.52%</cell><cell>68.94% 71.92%</cell></row><row><cell>LIA-title-only-notrain</cell><cell>65.97%</cell><cell>68.36%</cell><cell>64.52%</cell><cell>66.04% 67.34%</cell></row><row><cell>276</cell><cell>49.72%</cell><cell>52.08%</cell><cell>45.84%</cell><cell>51.24% 52.08%</cell></row><row><cell>267</cell><cell>46.72%</cell><cell>50.54%</cell><cell>40.90%</cell><cell>49.56% 49.70%</cell></row><row><cell>270</cell><cell>44.17%</cell><cell>46.84%</cell><cell>41.20%</cell><cell>45.30% 46.00%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,144.73,658.44,75.32,7.47"><p>http://nltk.org/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>We presented in this paper our contributions to the INEX 2013 Tweet Contextualization Track as well as the official results released by the organizers. We saw that a simple contextualization system composed of an effective Wikipedia retrieval system and a multi-document summarizer achieved the best informativeness results of the track. While it did not achieve the best readability results, it was very close to the best system.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="7,138.35,259.15,342.24,7.86;7,146.91,270.11,333.68,7.86;7,146.91,281.07,298.26,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,340.83,259.15,120.36,7.86">Quantifying Query Ambiguity</title>
		<author>
			<persName coords=""><forename type="first">Steve</forename><surname>Cronen-Townsend</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,146.91,270.11,333.68,7.86;7,146.91,281.07,93.25,7.86">Proceedings of the Second International Conference on Human Language Technology Research, HLT &apos;02</title>
		<meeting>the Second International Conference on Human Language Technology Research, HLT &apos;02<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="104" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,302.99,342.24,7.86;7,146.91,313.95,162.48,7.86" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="7,308.44,302.99,172.15,7.86;7,146.91,313.95,77.84,7.86">LIA/LINA at the INEX 2012 Tweet Contextualization track</title>
		<author>
			<persName coords=""><forename type="first">Romain</forename><surname>Deveaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Florian</forename><surname>Boudin</surname></persName>
		</author>
		<editor>Forner et al.</editor>
		<imprint>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,324.90,342.24,7.86;7,146.91,335.86,333.68,7.86;7,146.91,346.82,60.97,7.86" xml:id="b2">
	<monogr>
		<title level="m" coord="7,433.42,324.90,47.17,7.86;7,146.91,335.86,217.86,7.86">CLEF 2012 Evaluation Labs and Workshop, Online Working Notes</title>
		<editor>
			<persName><forename type="first">Pamela</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jussi</forename><surname>Karlgren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Christa</forename><surname>Womser-Hacker</surname></persName>
		</editor>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">September 17-20, 2012, 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,357.78,342.24,7.86;7,146.91,368.74,333.68,7.86;7,146.91,379.70,333.67,7.86;7,146.91,390.66,117.25,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,296.92,357.78,183.67,7.86;7,146.91,368.74,27.38,7.86">A markov random field model for term dependencies</title>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,193.01,368.74,287.58,7.86;7,146.91,379.70,247.12,7.86">Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;05</title>
		<meeting>the 28th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;05<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="472" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,401.62,342.24,7.86;7,146.91,412.58,333.68,7.86;7,146.91,423.53,101.43,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,274.37,401.62,133.75,7.86">Textrank: Bringing order into text</title>
		<author>
			<persName coords=""><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,424.82,401.62,55.78,7.86;7,146.91,412.58,333.68,7.86;7,146.91,423.53,10.75,7.86">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;04</title>
		<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;04</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="404" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,434.49,342.24,7.86;7,146.91,445.45,333.68,7.86;7,146.91,456.41,12.29,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="7,180.84,445.45,227.63,7.86">Overview of the inex 2012 tweet contextualization track</title>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Sanjuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Véronique</forename><surname>Moriceau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xavier</forename><surname>Tannier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrice</forename><surname>Bellot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Josiane</forename><surname>Mothe</surname></persName>
		</author>
		<editor>Forner et al.</editor>
		<imprint>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,467.37,342.25,7.86;7,146.91,478.33,153.73,7.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="7,306.39,467.37,174.21,7.86;7,146.91,478.33,58.04,7.86">Beautiful Data: The Stories Behind Elegant Data Solutions</title>
		<author>
			<persName coords=""><forename type="first">Toby</forename><surname>Segaran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeff</forename><surname>Hammerbacher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>O&apos;Reilly Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
