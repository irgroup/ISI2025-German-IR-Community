<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,218.39,115.96,178.58,12.62;1,162.47,133.89,290.40,12.62;1,200.15,151.82,215.06,12.62;1,214.34,169.76,186.67,12.62">Tweet Contextualization: a Strategy Based on Document Retrieval Using Query Enrichement and Automatic Summarization</title>
				<funder ref="#_N5wnqB4">
					<orgName type="full">Spanish projects RICOTERM</orgName>
				</funder>
				<funder ref="#_RVnZVfv">
					<orgName type="full">Juan de la Cierva</orgName>
				</funder>
				<funder ref="#_Exaen6x #_M9thJGF #_jChva9U">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,236.87,207.45,57.73,8.74"><forename type="first">Jorge</forename><surname>Vivaldi</surname></persName>
							<email>jorge.vivaldi@upf.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Universitat Pompeu Fabra Institut Universitari de Lingüística Aplicada Barcelona</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,317.29,207.45,61.19,8.74"><forename type="first">Iria</forename><surname>Da Cunha</surname></persName>
							<email>iria.dacunha@upf.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Universitat Pompeu Fabra Institut Universitari de Lingüística Aplicada Barcelona</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,218.39,115.96,178.58,12.62;1,162.47,133.89,290.40,12.62;1,200.15,151.82,215.06,12.62;1,214.34,169.76,186.67,12.62">Tweet Contextualization: a Strategy Based on Document Retrieval Using Query Enrichement and Automatic Summarization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">723D4B09CCAE84C3AC686D9928CC37B9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Question-Answering</term>
					<term>Tweets</term>
					<term>Terms</term>
					<term>Named Entities</term>
					<term>Hashtags</term>
					<term>Twitter Users Accounts</term>
					<term>Wikipedia</term>
					<term>Automatic Summarization</term>
					<term>REG</term>
					<term>Cortex</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The aim of the tweet contextualization INEX (Initiative for the Evaluation of XML retrieval) task at CLEF 2013 (Conference and Labs of the Evaluation Forum) is to build a system that provides automatically information related with different tweets, that is, a summary that explains a specific tweet. In this article, our strategy and results are presented. The methodology for the task in English includes three stages. First, automatic reformulations of the initial queries provided for the task, that is, the tweets, are performed. In this research, we use words sequences that agree with the typical terminological patterns, name entities, hashtags and Twitter users accounts, since we consider that they are representative of tweets' topics. Second, related documents are retrieved from Wikipedia with the search engine Indri, using the reformulated queries. Third, the obtained documents are summarized by using two different automatic summarization systems, in order to provide the final summary associated to each query. Regarding the pilot task for Spanish, our strategy includes a first stage where automatic reformulations of the initial queries provided for the task (similar to English) are carried out. However, it does not include neither the search engine Indri nor the summarization systems REG and Cortex. In this case, we directly extract relevant text passages from Wikipedia pages using the generated queries and we build the summary with the first sentences of these pages.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Twitter is a real-time information network that connects people to the latest stories, ideas, opinions and news about what users find interesting. Nowadays, about 340 millions of tweets are written every day. Each tweet is 140 characters long. However, 140 characters long messages are rarely self-content. The aim of the tweet contextualization INEX (Initiative for the Evaluation of XML retrieval) task at CLEF 2013 (Conference and Labs of the Evaluation Forum) is to build a system that provides automatically information related with different tweets, that is, a summary that explains a specific tweet. In order to get this aim, it is necessary to combine different types of processing, from information retrieval to multi-document summarization. This INEX task started in 2010, and the results show that best systems combine passage retrieval, sentence segmentation and scoring, named entity recognition and Part of Speech (POS) analysis. The evaluation of the participant systems involves two aspects: informativity and readability. In 2013, the goal of the task and the evaluation metrics has remained unchanged but tweet diversity has been improved. More specially, a significant part of tweets with hashtags have been included in the tweet set. Hashtags are authors' annotation on key terms of their tweets. In the two past years, hashtags have been underused, although they are core components of tweets.</p><p>Like in 2012, the use case of this task is the following: given a new tweet, the system must provide some context about the subject of the tweet, in order to help the reader to understand it, i.e. answering questions of the form "what is this tweet about?" using a recent cleaned dump of the Wikipedia. The general process involves: tweet analysis, passage and/or XML elements retrieval and construction of the answer. This context should take the form of a readable summary, not exceeding 500 words, composed of passages from a provided Wikipedia (WP) corpus. The summaries should contain relevant information but also contain as little non-relevant information as possible.</p><p>Specifically, this year, 598 tweets in English have been collected by the organizers from Twitter. They were selected among informative accounts (for example, @CNN, @TennisTweets, @PeopleMag, @science...), in order to avoid purely personal tweets that could not be contextualized. Information such as the user name, tags or URLs are provided in JSON format. These tweets are available in a single xml file with three fields: topic, title and txt.</p><p>In this edition, an extra set of topics (only tweet texts) has been released in Spanish to try a different language and a slightly different task. Topics in Spanish are opinionated personal tweets about music bands, cars and politics. They were manually selected from CLEF RepLab 2013 test set among those without external url and with at least 15 words. Contextualization should help the reader to also understand the opinion polarity, allusions and humor.</p><p>We use two different strategies for the task in English and Spanish. Our strategy for English includes three main stages. First, we perform automatic reformulations of the initial queries provided for the task, that is the title of the tweeets, using terminological patterns, name entities, hashtags and Twitter users accounts. Second, we obtain related documents from WP with the search engine Indri <ref type="bibr" coords="2,161.01,620.25,9.96,8.74" target="#b2">[1]</ref>, using the reformulated queries. Third, we summarize the obtained documents by using two different automatic summarization systems: REG <ref type="bibr" coords="2,470.08,632.21,10.52,8.74">[2]</ref> and Cortex <ref type="bibr" coords="2,187.16,644.16,9.96,8.74" target="#b4">[3]</ref>, in order to provide the final summary associated to each query. We think that automatic summarization can be useful in this task, taking into account that a summary can be defined as"a condensed version of a source document having a recognizable genre and a very specific purpose: to give the reader an exact and concise idea of the contents of the source" <ref type="bibr" coords="3,411.47,142.90,9.96,8.74" target="#b5">[4]</ref>.</p><p>This strategy is similar to the one used at QA@INEX track 2010 <ref type="bibr" coords="3,443.49,154.86,9.96,8.74" target="#b6">[5]</ref>, 2011 <ref type="bibr" coords="3,134.77,166.81,10.52,8.74" target="#b7">[6]</ref> and 2012 <ref type="bibr" coords="3,193.71,166.81,9.96,8.74" target="#b8">[7]</ref>. In (those three) such INEX editions, we used only the REG summarization system. This year, we use also Cortex, in order to evaluate the impact of the summarization system on the final results. In 2010 and 2011 editions, the system was semi-automatic, while in 2012 and this year the system is totally automatic. The main difference among our participations in all the INEX editions is related with the reformulation and expansion of the queries. This year, the main innovation is the use of the hashtags and the users accounts included in the tweets. As the INEX 2013 organizers suggest, the information given by hashtags can be used to determinate the main topic of the tweet. We combine this information with the information offered by terminological patterns and name entities (that we have previously used in past editions).</p><p>Regarding the pilot task for Spanish, our strategy includes a first stage where automatic reformulations of the initial queries provided for the task (similar to English) are carried out. However, it does not include neither the search engine Indri nor the summarization systems REG and CORTEX. In this case, we directly extract relevant text passages from WP pages using the generated queries and we build the summary with the first sentences of these pages.</p><p>In Section 2, the state of the art and the resources used are shown. In Section 3, the methodology is explained. In Section 4, the experimental settings and results are presented. Finally, in Section 5, the conclusions and the future work are exposed.</p><p>2 State-of-the-art and Resources</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Term and Name Entity Extraction</head><p>A term can be defined as a lexical unit that has a specific sense in a thematically restricted domain <ref type="bibr" coords="3,238.19,488.75,9.96,8.74" target="#b9">[8]</ref>. Terminology extractors are useful for any NLP task containing a domain specific component such as: ontology and terminological dictionary building, text indexing, automatic translation and summarization systems, among others. There are several methods to obtain the terms from a corpus: methods based on linguistic knowledge or statistical measures, also some authors suggest a combination of both linguistic knowledge and statistically measures. For a review on terminology extraction, see <ref type="bibr" coords="3,410.95,560.48,9.96,8.74" target="#b10">[9]</ref>. Most of the tools, in particular those including an important linguistic component, take into consideration the fact that terms usually follow a small number of POS patterns (mostly: noun, noun-adjective and noun-preposition-noun). In this work, we use the terminology extractor YATE <ref type="bibr" coords="3,280.56,608.30,14.61,8.74" target="#b11">[10]</ref>, to obtain such patterns.</p><p>Named Entity extraction may be basically defined as the task to identify names referring to persons, organizations and locations in free text; later this task has been expanded to obtain other entities like dates and numeric expressions. The recent interest in emerging areas like bioinformatics allows to expand this recognition task to proteins, drugs and chemical names. Name Entity extractors are useful for a number of NLP tasks as question answering, textual entailment and coreference resolution, among others. While early studies were mostly based on handcrafted rules, most recent ones use supervised machine learning as a way to automatically induce rule-based systems or sequence labeling algorithms starting from a collection of training examples. Often, corpus processing tools include some text handling facilities to perform simple Name Entity detection. Some of them are based in language specific peculiarities such as initial upper case letters together with some heuristics for name entities placed at the beginning of the sentence. This is the case of the tool used for this experiment <ref type="bibr" coords="4,162.72,238.55,14.61,8.74" target="#b12">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Hashtags and Users Accounts</head><p>On the one hand, hashtags are sequences of characters and numbers prefixed by a '#'. Social networks largely use this facility to indicate relevant words, groups or topics. From December 2008 to October 2009 hashtags became widely adopted on the site of Twitter. As <ref type="bibr" coords="4,287.66,321.80,15.50,8.74" target="#b13">[12]</ref> state: "A hashtag is the specific name for a tag in Twitter. Hashtags derive their name from the fact that they are preceded by the symbol # also known as a hash mark, e.g., #nowplaying."</p><p>Hashtags can be considered as one of the main topics of the tweet. There are several studies on this new type of tagging, which highlight the purpose of these elements. For example, <ref type="bibr" coords="4,236.97,393.53,15.50,8.74" target="#b13">[12]</ref> point out that "While tweets without hashtags were also displayed in trending topic lists, the act of tagging a tweet increased the likelihood of a tweet being displayed in a group of tweets on a trending topic."</p><p>On the other hand, Twitter is a relevant social network used, as mentioned in the introduction, for INEX competition. In this case, users' accounts are designated by a user name prefixed by '@'. Therefore, this string may be considered as a name entity that could be considered the author of the tweet, or another user related with the tweet.</p><p>Both types of information, hashtags and users accounts, will be used in our experiments to improve the queries used to retrieve documents from Wikipedia using Indri and, therefore, to improve the final summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Automatic Summarization</head><p>REG <ref type="bibr" coords="4,159.85,560.48,10.52,8.74">[2]</ref> is a system for extractive summarization, using a graph approach. The strategy of this system has two main stages: a) to carry out an adequate representation of the document and b) to give a weight to each sentence of the document. In the first stage, the system makes a vectorial representation of the document. In the second stage, the system uses a greedy optimization algorithm. The summary generation is done with the concatenation of the most relevant sentences (previously scored in the optimization stage).</p><p>Cortex <ref type="bibr" coords="4,182.37,644.16,10.52,8.74" target="#b4">[3]</ref> is a single-document extract summarization system using an optimal decision algorithm that combines several metrics. These metrics result from processing statistical and informational algorithms on the VSM representation. In order to reduce the complexity, a preprocessing is performed on the topic and the document: words are filtered, lemmatized or/and stemmed. A representation in bag-of-words produces a S[P x N] matrix of frequencies/absences of u = 1, ..., P sentences (rows) and a vocabulary of i = 1, ...,N terms (columns). Cortex system can use several metrics to evaluate the sentences relevance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, the two different methodologies used for the task in English and the task in Spanish are explained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Methodology for the task in English</head><p>The methodology for the task in English includes three stages:</p><p>1) Automatic reformulations of the initial queries provided for the task, that is the title of the tweeets, are performed. In this research, we consider that word sequences that agree with the typical terminological patterns, name entities, hashtags and Twitter users accounts are representative elements of tweet topics. Therefore, in order to reformulate and build the query we extract automatically two types of information from the tweets: a) Terms and name entities. First, we POS tag the tweets file, obtaining proper nouns info, which we consider as name entities. Second, we carry out terminological pattern extraction, by using an already existent module of the YATE term extraction tool <ref type="bibr" coords="5,257.13,409.74,14.61,8.74" target="#b11">[10]</ref>. Third, we check if these elements are included in the Wikipedia. If they are included, we maintain them for the query string for Indri. Otherwise, they are erased.</p><p>b) Hashtags and users accounts. Usually, hashtags and Twitter accounts are not fully arbitrary strings but very often the actual entity may be discovered just analyzing the string.</p><p>First, we extract elements tagged with hashtags (#) and users accounts (@) in each tweet. Second, we check if these elements are included in WP. If they are included, we maintain them for the query string for Indri. If they are not included, the system segments the letters of hashtags and the user accounts, in order to find all the possible combinations of real words (with a maximum of two words). We apply this strategy taking into account that many times hashtags and user accounts include no spaces, so two or more different words can be joined. For our purpose, we need to have the words separated, in order to check if they are included in Wikipedia. For example:</p><p>-The user account @BarbraStreisand is not present as such in Wikipedia.</p><p>Nevertheless, Wikipedia includes "Barbra Streisand", that will be included in our query. -The hashtag #EUbudget is not present as a single unit in Wikipedia. However, Wikipedia includes both "EU" and "budget", therefore these elements separately will be included in our query.</p><p>-The hashtag #NASA Astronauts is not included as a single unit in Wikipedia. Although, Wikipedia contains "NASA" and "Astronauts" separately. We include both elements in our query.</p><p>If the system does not find any real word after applying all possible segmentations in hashtags and users accounts, we do not include these elements in the query string for Indri.</p><p>In order to enrich the queries, we use a local installation of a Wikipedia dump<ref type="foot" coords="6,159.67,210.37,3.97,6.12" target="#foot_0">1</ref> to expand the terms with redirection information from such Wikipedia info. Some care has been taken to keep track of multiword sequences as indicated by the Indri query language specification (see examples below). For example, for the tweet:</p><p>Behind the scenes on Charlie Brooker's Black Mirror finale we obtain the following query: scenes,finale,#1(Black Mirror),#1(Charlie Brooker)</p><p>2) Related documents (specifically 50 texts) are retrieved from WP with the search engine Indri, using the reformulated queries. The resulting set of WP pages has been splitted in several documents. Each document contains the pages relevant to the query.</p><p>3) The obtained documents are summarized by using REG and CORTEX summarization systems, in order to provide the final summary associated to each query.</p><p>Figure <ref type="figure" coords="6,181.14,421.75,4.98,8.74" target="#fig_0">1</ref> illustrates the methodology for the task in English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Methodology for the task in Spanish</head><p>Our strategy for Spanish includes neither the search engine Indri nor the summarization systems REG and Cortex. This strategy includes two main stages: 1) To carry out automatic reformulations of the original queries provided for the task. First, name entities, hashtags and Twitter users accounts are extracted from the tweet. Second, the system checks if they have a main page in WP: if they have a page in WP, they are included in the final query; if they do not have a page in WP, they are eliminated. Third, if after this process, the query includes 1 or 0 elements, terminological patterns are used to extract more units from the tweet, in order to include in the query those which have a main entry in WP. For example, for the tweet:</p><p>Vaya asco de árbitros hay en la liga BBVA, ya solo por los árbitros que hay, nadie tendría que decir que es la mejor liga del mundo... we obtain the following query: mundo,liga,#syn("BBVA" #1(Banco Bilbao Vizcaya Argentaria))</p><p>2) To extract relevant text passages from WP pages using the generated queries, which are considered as a summary. Our system carries out a summary including about 500 words, giving the same importance to the name entities, the terminological units, the hashtags and the Twitter users accounts included in the query. For example, if a query contains three of these elements, the system extracts the first sentences of the WP page of each of the three, to obtain a summary including a third of sentences for each element. Some filters are applied before obtaining the final version of the summary: a) ambiguous WP pages are eliminated, b) passages in brackets are erased, c) only passages included at the beginning of the WP page are selected (text appearing after the first subtitle of the WP page is not considered), and d) lists of elements are eliminated.</p><p>Figure <ref type="figure" coords="7,181.14,586.25,4.98,8.74" target="#fig_0">1</ref> illustrates the methodology for the task in Spanish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments Settings and Results</head><p>In this section, the experiments and results for the task in English and the task in Spanish are explained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiments for the task in English</head><p>For the task in English, we have applied our strategy using the 598 tweets that have been collected by the organizers from Twitter. From these tweets, 529 included different hashtags and/or users accounts. From this set of elements, 391 were found in Wikipedia.</p><p>The evaluation of all the participant systems in the INEX 2011 QA Track involves two aspects: informativity and readability. On the one hand, to evaluate informativity, the automatic FRESA [FRamework for Evaluating Summaries Automatically] [14] package is used. This evaluation framework includes document-based summary evaluation measures based on probabilities distribution, specifically, the Kullback-Leibler (KL) divergence and the Jensen-Shannon (JS) divergence. As in the ROUGE package <ref type="bibr" coords="8,331.50,262.98,14.61,8.74" target="#b16">[15]</ref>, FRESA supports different ngrams and skip n-grams probability distributions. FRESA environment has been used in the evaluation of summaries produced in several European languages (English, French, Spanish and Catalan), and it integrates filtering and lemmatization in the treatment of summaries and documents. FRESA is available in the following link: http://lia.univ-avignon.fr/fileadmin/axes/TALNE/ Ressources.html.</p><p>Informativity has been evaluated based on three overlapping references:</p><p>prior set of relevant pages selected by organizers while building the 2013 topics (40 tweets, 380 passages, 11 523 tokens), pool selection of most relevant passages from participant submissions for tweets selected by organizers (45 tweets, 1 760 passages, 58 035 tokens), all relevant text merged together with an extra selection of relevant passages from a random pool of ten tweets (70 tweets, 2 378 passages, 77 043 tokens)</p><p>Table <ref type="table" coords="8,177.56,459.31,4.98,8.74" target="#tab_0">1</ref> include the results about informativity. In this table, we have only included the best run of each different team. The complete table can be retrieved in the official website of INEX 2013 (https://inex.mmci.uni-saarland.de/).</p><p>Informativity results show that the impact of the summarization system in our strategy is not high. The results using Cortex (run 262) are slightly better than the results using REG (run 255).</p><p>On the other hand, readability has been evaluated by organizers over the ten tweets having the largest text references (t-rels). For these tweets, summaries are expected to have almost 500 words since the reference is much larger. For each participant summary, organizers have checked the number of words over 500 in passages that are:</p><p>-Relevant (T) i.e. clearly related to the tweet. -Sound (A) i.e. no issues about resolving references to earlier or later items in the discourse. -Non redundant (R) with previous passages.</p><p>-Syntactically (S) correct. Non relevant passages have also been considered non sound, redundant and syntactically incorrect. Runs are ranked according to mean average scores per summary over Soundness, Non redundancy and Syntactically correctness among Relevant passages. Tabla 2 shows the results about readability.</p><p>Readability results show that the impact of the summarization system in our strategy is higher than in the case of informativity results: the results using REG (run 255) obtain a higher score than the results using Cortex (run 262). Anyway, this difference is not significant, since none of the systems appears in the best positions.</p><p>Since the process is fully automatic, we find some limitations. The main limitation that we have found is that Indri does not retrieve relevant documents. For example, one of the queries includes the name entity "Jennifer Hudson" and only one of the 50 documents retrieved by Indri includes this name entity. This fact limits our results: if the input given to the summarization systems is not relevant, the summary will be not relevant either.</p><p>Another limitation of our strategy is the kind of summarization systems that we have used. Both REG and Cortex are mono-document summarization systems. We offer to these systems a single text including the 50 documents retrieved by Indri, and they produce an extractive summary. This fact could produce a lack of correference and coherence, which can affect the final results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiments for the task in Spanish</head><p>For the task in Spanish, we have applied our strategy using the 354 tweets that have been collected by the organizers from Twitter. Nowadays, we do not have the final official results evaluation in the INEX 2013 Tweet Contextualization Track. We will include the results in a more complete version of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>In this paper, our strategy for the INEX 2013 Tweet Contextualization Track are presented, both for the task in English and in Spanish. In our work, as explained, a reformulation of the initial queries (tweets) is carried out by using terminological patterns, named entities, hashtags and Twitter users accounts. For the task in English, the automatic summarization systems REG and Cortex are used to summarize the 50 documents obtained by Indri using these queries. We have proved that the use of different summarizers has not a significant impact over the results. Also, we have detected that the main problem is related with the documents retrieved by Indri. Therefore, for the pilot task for Spanish, we have not used Indri. We hope to improve the results in this way.</p><p>As usual, ambiguous terms represent a serious problem in a procedure like the proposed in this experiment. In this work, ambiguous terms (that is, terms linked to a WP disambiguation page) have not been considered, in order to prioritize precision. In the future, we would like to experiment with systems such as those proposed by <ref type="bibr" coords="10,190.99,620.25,15.50,8.74" target="#b17">[16]</ref> or <ref type="bibr" coords="10,221.98,620.25,15.50,8.74" target="#b18">[17]</ref> in order to perform some disambiguation and therefore improve the selection of the relevant elements for the queries. Other improvement could be achieved by clustering the WP pages provided by Indri with the aim to obtain a better WP pages selection and then summarize them.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,217.78,384.95,179.79,7.89;7,165.95,115.84,283.48,254.34"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Full methodology (English/Spanish).</figDesc><graphic coords="7,165.95,115.84,283.48,254.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="9,136.84,115.91,341.67,162.56"><head>Table 1 .</head><label>1</label><figDesc>Results of informativity in the INEX 2013 Tweet Contextualization Track.</figDesc><table coords="9,140.82,138.70,330.64,139.77"><row><cell cols="5">Run All.skip All.bi All.uni Pool.skip Pool.bi Pool.uni Prior.skip Prior.bi Prior.uni</cell></row><row><cell cols="2">256 0.8861 0.881 0.782</cell><cell>0.8752</cell><cell>0.87 0.7813</cell><cell>0.921 0.9134 0.7814</cell></row><row><cell cols="2">275 0.8969 0.8924 0.8061</cell><cell cols="2">0.8789 0.8745 0.7941</cell><cell>0.9172 0.9106 0.7899</cell></row><row><cell cols="2">254 0.9242 0.9229 0.8331</cell><cell cols="2">0.9162 0.9159 0.8363</cell><cell>0.9473 0.943 0.8223</cell></row><row><cell cols="2">276 0.9301 0.927 0.8169</cell><cell cols="2">0.9333 0.9302 0.8285</cell><cell>0.9718 0.9678 0.8286</cell></row><row><cell cols="2">270 0.9397 0.9365 0.8481</cell><cell cols="2">0.9274 0.9246 0.8418</cell><cell>0.9686 0.9642 0.8529</cell></row><row><cell cols="2">277 0.9662 0.9649 0.8995</cell><cell cols="2">0.9642 0.9626 0.9005</cell><cell>0.9792 0.9773 0.9102</cell></row><row><cell>261</cell><cell>0.967 0.9668 0.8639</cell><cell cols="2">0.9656 0.9659 0.8666</cell><cell>0.9888 0.9862 0.8687</cell></row><row><cell cols="2">262 0.9747 0.9734 0.8738</cell><cell cols="2">0.9736 0.9727 0.8775</cell><cell>0.9821 0.9788 0.8635</cell></row><row><cell cols="2">255 0.9783 0.9771 0.8817</cell><cell cols="2">0.9759 0.9748 0.8801</cell><cell>0.9938 0.9914 0.8941</cell></row><row><cell cols="2">265 0.9789 0.9781 0.8793</cell><cell cols="2">0.9751 0.9749 0.8821</cell><cell>0.9927 0.9904 0.8845</cell></row><row><cell cols="2">266 0.9835 0.9824 0.9059</cell><cell cols="2">0.9865 0.9859 0.9132</cell><cell>0.9903 0.9877 0.8952</cell></row><row><cell cols="2">269 0.9999 0.9999 0.9965</cell><cell cols="2">0.9999 0.9999 0.9972</cell><cell>1 0.9999 0.9962</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="6,144.73,656.80,168.96,7.86"><p>This resource has been otained using<ref type="bibr" coords="6,296.79,656.80,13.52,7.86" target="#b14">[13]</ref>.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work has been partially financed by the <rs type="funder">Spanish projects RICOTERM</rs> <rs type="grantNumber">4</rs> (<rs type="grantNumber">FFI2010-21365-C03-01</rs>) and <rs type="grantNumber">APLE 2</rs> (<rs type="grantNumber">FFI2012-37260</rs>), and a <rs type="funder">Juan de la Cierva</rs> grant (<rs type="grantNumber">JCI-2011-09665</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_N5wnqB4">
					<idno type="grant-number">4</idno>
				</org>
				<org type="funding" xml:id="_Exaen6x">
					<idno type="grant-number">FFI2010-21365-C03-01</idno>
				</org>
				<org type="funding" xml:id="_M9thJGF">
					<idno type="grant-number">APLE 2</idno>
				</org>
				<org type="funding" xml:id="_RVnZVfv">
					<idno type="grant-number">FFI2012-37260</idno>
				</org>
				<org type="funding" xml:id="_jChva9U">
					<idno type="grant-number">JCI-2011-09665</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,174.73,115.91,298.92,7.89" xml:id="b0">
	<monogr>
		<title level="m" coord="10,181.93,115.94,287.50,7.86">Results of readability in the INEX 2013 Tweet Contextualization Track</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,140.19,138.70,331.91,7.86;10,140.19,150.06,13.82,7.86;10,189.21,150.06,28.67,7.86;10,249.47,150.06,28.67,7.86;10,335.86,150.06,28.67,7.86;10,397.37,150.06,28.67,7.86;10,443.43,150.06,28.67,7.86;10,140.19,161.02,13.82,7.86;10,189.21,161.02,28.67,7.86;10,249.47,161.02,28.67,7.86;10,335.86,161.02,28.67,7.86;10,397.37,161.02,28.67,7.86;10,443.43,161.02,28.67,7.86;10,140.19,171.98,13.82,7.86;10,189.21,171.98,28.67,7.86;10,249.47,171.98,28.67,7.86;10,335.86,171.98,28.67,7.86;10,397.37,171.98,28.67,7.86;10,443.43,171.98,28.67,7.86;10,140.19,182.94,13.82,7.86;10,189.21,182.94,28.67,7.86;10,249.47,182.94,28.67,7.86;10,335.86,182.94,28.67,7.86;10,397.37,182.94,28.67,7.86;10,443.43,182.94,28.67,7.86;10,140.19,193.90,13.82,7.86;10,189.21,193.90,28.67,7.86;10,249.47,193.90,28.67,7.86;10,335.86,193.90,28.67,7.86;10,397.37,193.90,28.67,7.86;10,443.43,193.90,28.67,7.86;10,140.19,204.85,13.82,7.86;10,189.21,204.85,28.67,7.86;10,249.47,204.85,28.67,7.86;10,335.86,204.85,28.67,7.86;10,397.37,204.85,28.67,7.86;10,443.43,204.85,28.67,7.86;10,140.19,215.81,13.82,7.86;10,189.21,215.81,28.67,7.86;10,249.47,215.81,28.67,7.86;10,335.86,215.81,28.67,7.86;10,397.37,215.81,28.67,7.86;10,443.43,215.81,28.67,7.86;10,140.19,226.77,13.82,7.86;10,189.21,226.77,28.67,7.86;10,249.47,226.77,28.67,7.86;10,335.86,226.77,28.67,7.86;10,397.37,226.77,28.67,7.86;10,443.43,226.77,28.67,7.86;10,140.19,237.73,13.82,7.86;10,189.21,237.73,28.67,7.86;10,249.47,237.73,28.67,7.86;10,335.86,237.73,28.67,7.86;10,397.37,237.73,28.67,7.86;10,443.43,237.73,28.67,7.86;10,140.19,248.69,13.82,7.86;10,189.21,248.69,28.67,7.86;10,249.47,248.69,28.67,7.86;10,335.86,248.69,28.67,7.86;10,397.37,248.69,28.67,7.86;10,443.43,248.69,28.67,7.86;10,140.19,259.65,13.82,7.86;10,189.21,259.65,28.67,7.86;10,249.47,259.65,28.67,7.86;10,335.86,259.65,28.67,7.86;10,397.37,259.65,28.67,7.86;10,443.43,259.65,28.67,7.86;10,140.19,270.61,13.82,7.86;10,189.21,270.61,28.67,7.86;10,249.47,270.61,28.67,7.86;10,335.86,270.61,28.67,7.86;10,397.37,270.61,28.67,7.86;10,443.43,270.61,28.67,7.86;10,140.19,281.57,13.82,7.86;10,189.21,281.57,28.67,7.86;10,249.47,281.57,28.67,7.86;10,335.86,281.57,28.67,7.86;10,397.37,281.57,28.67,7.86;10,443.43,281.57,28.67,7.86;10,140.19,292.53,13.82,7.86;10,189.21,292.53,28.67,7.86;10,249.47,292.53,28.67,7.86;10,335.86,292.53,28.67,7.86;10,397.37,292.53,28.67,7.86;10,443.43,292.53,28.67,7.86;10,140.19,303.48,13.82,7.86;10,189.21,303.48,28.67,7.86;10,249.47,303.48,28.67,7.86;10,335.86,303.48,28.67,7.86;10,397.37,303.48,28.67,7.86;10,443.43,303.48,28.67,7.86;10,140.19,314.44,13.82,7.86;10,189.21,314.44,28.67,7.86;10,249.47,314.44,28.67,7.86;10,335.86,314.44,28.67,7.86;10,397.37,314.44,28.67,7.86;10,443.43,314.44,28.67,7.86;10,140.19,325.40,13.82,7.86;10,189.21,325.40,28.67,7.86;10,249.47,325.40,28.67,7.86;10,335.86,325.40,28.67,7.86;10,397.37,325.40,28.67,7.86;10,443.43,325.40,28.67,7.86;10,140.19,336.36,13.82,7.86;10,189.21,336.36,28.67,7.86;10,249.47,336.36,28.67,7.86;10,335.86,336.36,28.67,7.86;10,397.37,336.36,28.67,7.86;10,443.43,336.36,28.67,7.86;10,140.19,347.32,13.82,7.86;10,189.21,347.32,28.67,7.86;10,249.47,347.32,28.67,7.86;10,335.86,347.32,28.67,7.86;10,397.37,347.32,28.67,7.86;10,443.43,347.32,28.67,7.86;10,140.19,358.28,13.82,7.86;10,189.21,358.28,28.67,7.86;10,249.47,358.28,28.67,7.86;10,335.86,358.28,28.67,7.86;10,397.37,358.28,28.67,7.86;10,443.43,358.28,28.67,7.86;10,140.19,369.24,13.82,7.86;10,189.21,369.24,28.67,7.86;10,249.47,369.24,28.67,7.86;10,335.86,369.24,28.67,7.86;10,397.37,369.24,28.67,7.86;10,443.43,369.24,28.67,7.86;10,140.19,380.20,13.82,7.86;10,189.21,380.20,28.67,7.86;10,249.47,380.20,28.67,7.86;10,335.86,380.20,28.67,7.86;10,397.37,380.20,28.67,7.86;10,443.43,380.20,28.67,7.86;11,134.77,199.84,62.94,10.52" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,140.19,138.70,316.55,7.86">Run Mean Average Relevancy (T) Non redundancy (R) Soundness (A) Syntax</title>
		<idno>% 256 72.13% 74.24% 71.98% 70.78% 73.62% 274 71.71% 74.66% 68.84% 71.78% 74.50% 273 71.35% 75.52% 67.88% 71.20% 74.96% 257 69.54% 72.18% 65.48% 70.96% 72.18% 254 67.46% 73.30% 61.52% 68.94% 71.92% 258 65.97% 68.36% 64.52% 66.04% 67.34% 276 49.72% 52.08% 45.84% 51.24% 52.08% 267 46.72% 50.54% 40.90% 49.56% 49.70% 270 44.17% 46.84% 41.20% 45.30% 46.00% 271 38.76% 41.16% 35.38% 39.74% 41.16% 264 38.56% 41.26% 33.16% 41.26% 41.26% 260 38.21% 38.64% 37.36% 38.64% 38.64% 265 37.92% 39.46% 36.46% 37.84% 39.46% 259 37.70% 38.78% 35.54% 38.78% 38.78% 255 36.59% 38.98% 31.82% 38.98% 38.98% 261 35.99% 36.42% 35.14% 36.42% 36.42% 263 32.75% 34.48% 31.86% 31.92% 34.48% 262 32.35% 33.34% 30.38% 33.34% 33.34% 266 25.64% 25.92% 25.08% 25.92% 25.92% 277 20.00% 20.00% 20.00% 20.00% 20.00% 269 00.04% 00.04% 00.04% 00.04% 00.04%</idno>
	</analytic>
	<monogr>
		<title level="j" coord="10,463.90,138.70,8.19,7.86">S)</title>
		<imprint>
			<biblScope unit="volume">275</biblScope>
			<biblScope unit="issue">52% 75</biblScope>
			<biblScope unit="page" from="30" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,226.10,337.64,7.86;11,151.52,237.06,329.07,7.86;11,151.52,248.02,125.07,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="11,382.50,226.10,98.09,7.86;11,151.52,237.06,164.01,7.86">Indri: A language-model based search engine for complex queries</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Croft</surname></persName>
		</author>
		<idno>IR-407</idno>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts Amherst</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">CIIR Technical Report</note>
</biblStruct>

<biblStruct coords="11,142.96,259.23,337.63,7.86;11,151.52,270.19,329.07,7.86;11,151.52,281.15,180.49,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,324.26,259.23,156.33,7.86;11,151.52,270.19,129.12,7.86">REG : un algorithme glouton appliqué au résumé automatique de texte</title>
		<author>
			<persName coords=""><forename type="first">J-M</forename><surname>Torres-Moreno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ramírez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,287.83,270.19,192.76,7.86;11,151.52,281.15,120.15,7.86">Proceedings of the 10th Int. Conference on the Statistical Analysis of Textual</title>
		<meeting>the 10th Int. Conference on the Statistical Analysis of Textual<address><addrLine>Roma, Italia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,292.36,337.63,7.86;11,151.52,303.32,329.07,7.86;11,151.52,314.28,259.34,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,426.38,292.36,54.21,7.86;11,151.52,303.32,140.30,7.86">Condensés de textes par des méthodes numériques</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Torres-Moreno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Velázquez-Morales</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">G</forename><surname>Meunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,298.91,303.32,181.68,7.86;11,151.52,314.28,177.79,7.86">Proceedings of the 6th Int. Conference on the Statistical Analysis of Textual Data (JADT)</title>
		<meeting>the 6th Int. Conference on the Statistical Analysis of Textual Data (JADT)<address><addrLine>St. Malo</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="723" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,325.49,337.64,7.86;11,151.52,336.45,228.05,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,293.89,325.49,186.71,7.86;11,151.52,336.45,49.41,7.86">Generating Indicative-Informative Summaries with SumUM</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Saggion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Lapalme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,209.89,336.45,106.72,7.86">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="497" to="526" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,347.66,337.64,7.86;11,151.52,358.62,329.07,7.86;11,151.52,369.58,193.85,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,345.59,347.66,135.01,7.86;11,151.52,358.62,211.79,7.86">The REG summarization system with question reformulation at QA@INEX track 2010</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vivaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Da Cunha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ramírez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="11,370.51,358.62,110.08,7.86;11,151.52,369.58,58.60,7.86">Lecture Notes in Computer Science (LNCS</title>
		<imprint>
			<biblScope unit="volume">6932</biblScope>
			<biblScope unit="page" from="295" to="302" />
			<date type="published" when="2011">2011</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,380.79,337.64,7.86;11,151.52,391.75,329.07,7.86;11,151.52,402.71,193.85,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,284.10,380.79,196.50,7.86;11,151.52,391.75,213.23,7.86">QA@INEX Track 2011: Question Expansion and Reformulation Using the REG Summarization System</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vivaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Da Cunha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="11,371.67,391.75,108.93,7.86;11,151.52,402.71,58.60,7.86">Lecture Notes in Computer Science (LNCS</title>
		<imprint>
			<biblScope unit="volume">7424</biblScope>
			<biblScope unit="page" from="257" to="268" />
			<date type="published" when="2012">2012</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,413.92,337.63,7.86;11,151.52,424.88,329.07,7.86;11,151.52,435.84,329.07,7.86;11,151.52,446.80,224.70,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,288.49,413.92,192.10,7.86;11,151.52,424.88,329.07,7.86;11,151.52,435.84,41.57,7.86">INEX Tweet Contextualization Track at CLEF 2012: Query Reformulation using Terminological Patterns and Automatic Summarization</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vivaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Da Cunha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,433.23,435.84,47.36,7.86;11,151.52,446.80,124.63,7.86">CLEF 2012 Evaluation Labs and Workshop</title>
		<title level="s" coord="11,283.98,446.80,88.00,7.86">Online Working Notes</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Karlgren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Womser-Hacker</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,458.01,337.64,7.86;11,151.52,468.97,294.55,7.86" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Cabré</surname></persName>
		</author>
		<title level="m" coord="11,238.01,458.01,192.93,7.86">La terminología: Representación y comunicación</title>
		<meeting><address><addrLine>Barcelona</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
		<respStmt>
			<orgName>Institut Universitari de Lingüistica Aplicada, Universitat Pompeu Fabra</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,480.19,337.64,7.86;11,151.52,491.14,312.67,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,335.93,480.19,144.66,7.86;11,151.52,491.14,72.04,7.86">Automatic term detection. A review of current systems</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Cabré</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Estopà</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vivaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,230.78,491.14,196.03,7.86">Recent Advances in Computational Terminology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="53" to="87" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,502.36,337.98,7.86;11,151.52,513.32,329.07,7.86;11,151.52,524.28,75.15,7.86" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vivaldi</surname></persName>
		</author>
		<title level="m" coord="11,233.77,502.36,246.82,7.86;11,151.52,513.32,107.86,7.86">Extracción de candidatos a término mediante combinación de estrategias heterogéneas</title>
		<meeting><address><addrLine>Barcelona (Spain)</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>Universitat Politècnica de Catalunya</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct coords="11,142.61,535.49,337.98,7.86;11,151.52,546.45,329.07,7.86;11,151.52,557.41,222.83,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,338.30,535.49,142.29,7.86;11,151.52,546.45,114.88,7.86">Text handling as a Web Service for the IULA processing pipeline</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vivaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,273.31,546.45,207.28,7.86;11,151.52,557.41,191.01,7.86">Proceedings of the 7th Conference on International Language Resources and Evaluation (LREC&apos;10)</title>
		<meeting>the 7th Conference on International Language Resources and Evaluation (LREC&apos;10)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="22" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,568.62,337.98,7.86;11,151.52,579.58,329.07,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,375.11,568.62,105.49,7.86;11,151.52,579.58,28.28,7.86">Conversational Tagging in Twitter</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">M</forename><surname>Thornton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">N</forename><surname>Efthimiadis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,186.73,579.58,289.19,7.86">Proceedings of the 21st ACM Conference on Hypertext and Hypermedia</title>
		<meeting>the 21st ACM Conference on Hypertext and Hypermedia</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,590.79,337.98,7.86;11,151.52,601.75,329.07,7.86;11,151.52,612.71,267.38,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,323.30,590.79,157.30,7.86;11,151.52,601.75,128.29,7.86">Extracting Lexical Semantic Knowledge from Wikipedia and Wiktionary</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zesch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,288.21,601.75,192.38,7.86;11,151.52,612.71,217.11,7.86">Proceedings of the 6th Conference on International Language Resources and Evaluation (LREC&apos;08)</title>
		<meeting>the 6th Conference on International Language Resources and Evaluation (LREC&apos;08)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1646" to="1652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,623.92,337.97,7.86;11,151.52,634.88,329.07,7.86;11,151.52,645.84,329.07,7.86;11,151.52,656.80,96.56,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="11,252.92,634.88,227.67,7.86;11,151.52,645.84,25.80,7.86">Multilingual Summarization Evaluation without Human Models</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Saggion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J-M</forename><surname>Torres-Moreno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Da Cunha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sanjuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Velázquez-Morales</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sanjuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,186.14,645.84,294.45,7.86;11,151.52,656.80,64.18,7.86">Proceedings of the 23rd Int. Conference on Computational Linguistics (COLING 2010)</title>
		<meeting>the 23rd Int. Conference on Computational Linguistics (COLING 2010)<address><addrLine>Pekin</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,119.67,337.97,7.86;12,151.52,130.63,312.67,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="12,228.00,119.67,247.88,7.86">ROUGE: A Package for Automatic Evaluation of Summaries</title>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,151.52,130.63,280.78,7.86">Proceedings of Text Summarization Branches Out: ACL-04 Workshop</title>
		<meeting>Text Summarization Branches Out: ACL-04 Workshop</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,141.59,337.97,7.86;12,151.52,152.55,329.07,7.86;12,151.52,163.51,225.99,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="12,236.45,141.59,219.12,7.86">Wikify! Linking Documents to Encyclopedic Knowledge</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,463.04,141.59,17.56,7.86;12,151.52,152.55,329.07,7.86;12,151.52,163.51,144.06,7.86">Proceedings of the sixteenth ACM conference on Conference on information and knowledge management: CIKM &apos;07 Pages</title>
		<meeting>the sixteenth ACM conference on Conference on information and knowledge management: CIKM &apos;07 Pages<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="233" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,174.47,337.98,7.86;12,151.52,185.43,329.07,7.86;12,151.52,196.39,329.07,7.86;12,151.52,207.34,42.52,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="12,290.59,174.47,190.00,7.86;12,151.52,185.43,132.26,7.86">TAGME: On-the-fly Annotation of Short Text Fragments (byWikipedia Entities)</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ferragina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Scaiella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,289.92,185.43,190.68,7.86;12,151.52,196.39,283.12,7.86">Proceedings of the 19th ACM international conference on Information and knowledge management: CIKM &apos;10 Pages</title>
		<meeting>the 19th ACM international conference on Information and knowledge management: CIKM &apos;10 Pages<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1625" to="1628" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
