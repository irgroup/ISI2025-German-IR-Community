<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,142.34,152.67,310.35,12.64">IRIT at INEX 2013: Tweet Contextualization Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,232.85,192.18,63.81,8.96"><forename type="first">Liana</forename><surname>Ermakova</surname></persName>
							<email>liana.ermakova.87@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Institut de Recherche en Informatique</orgName>
								<orgName type="institution">Toulouse</orgName>
								<address>
									<addrLine>118 Route de Narbonne</addrLine>
									<postCode>31062, Cedex 9</postCode>
									<settlement>Toulouse</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,304.04,192.18,58.03,8.96"><forename type="first">Josiane</forename><surname>Mothe</surname></persName>
							<email>josiane.mothe@irit.fr</email>
							<affiliation key="aff0">
								<orgName type="department">Institut de Recherche en Informatique</orgName>
								<orgName type="institution">Toulouse</orgName>
								<address>
									<addrLine>118 Route de Narbonne</addrLine>
									<postCode>31062, Cedex 9</postCode>
									<settlement>Toulouse</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,142.34,152.67,310.35,12.64">IRIT at INEX 2013: Tweet Contextualization Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">39EEF8E5657574A993862F2D7BF9068C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Information retrieval</term>
					<term>tweet contextualization</term>
					<term>summarization</term>
					<term>sentence extraction</term>
					<term>readability</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The paper presents IRIT's approach used at INEX Tweet Contextualization Track 2013. Systems had to provide a context to a tweet. This year we further modified our approach presented at INEX 2011 and 2012 underlain by the product of scores based on hashtag processing, TF-IDF cosine similarity measure enriched by smoothing from local context and document beginning, named entity recognition and part-of-speech weighting. We assumed that relevant sentences come from relevant documents therefore we multiply sentence score by document relevance. We also used generalized POS (e.g. we merge regular adverbs, superlative and comparative into a single adverb group). We introduced sentence quality measure based on Flesch reading ease test, lexical diversity, meaningful word ratio and punctuation ratio. Our approach was ranked first, second and third over 24 runs submitted by all participants on different reference pools according to informativeness evaluation. At the same time it obtained the best readability score.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="706.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Twitter is an online social network and microblogging that enables to send and read text messages up to 140 characters <ref type="bibr" coords="1,267.41,521.27,10.69,8.96" target="#b0">[1]</ref>. In March 2013, the Twitter got more than 200 million active users how write more that 400 million tweet every day <ref type="bibr" coords="1,413.95,533.27,10.69,8.96" target="#b1">[2]</ref>. However, tweets are quite short and they may contain information that is not understandable to a user without some context. Therefore, providing concise coherent context seems to be helpful. INEX Tweet Contextualization Track aims to evaluate systems providing context to a tweet <ref type="bibr" coords="1,202.61,581.30,10.69,8.96" target="#b2">[3]</ref>. The context should be a readable summary up to 500 words extracted from a dump of the Wikipedia from November 2012. This year two languages were used: English and Spanish. English query set included 598 tweets in English, while Spanish subtrack was based on 354 personal tweets in Spanish. The paper presents IRIT's approach used at INEX Tweet Contextualization Track 2013. We consider tweet contextualization task as multi-document extractive summarization. This year we further modified our approach presented at INEX 2011 <ref type="bibr" coords="1,441.79,653.30,11.69,8.96" target="#b3">[4]</ref> and 2012 <ref type="bibr" coords="1,149.06,665.30,11.58,8.96" target="#b4">[5]</ref> underlain by the product of scores based on hashtag processing, TF-IDF cosine similarity measure enriched by smoothing from local context and document beginning, named entity (NE) recognition and part-of-speech (POS) weighting. We assumed that relevant sentences come from relevant documents therefore we multiply sentence score by document relevance. We also used generalized POS (e.g. we merge regular adverbs, superlative and comparative into a single adverb group). We introduced sentence quality measure based on Flesch reading ease test, lexical diversity, meaningful word ratio and punctuation ratio.</p><p>The paper is organized as follows. Firstly, we recall the principles of the 2011-2012 system we developed and describe the modifications we made. Then, we present the results and discuss them. Future development description concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>Method Description</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preprocessing</head><p>Preprocessing includes several steps. Firstly, we treat tweets themselves, i.e. special symbols like hashtags and replies. The hashtag symbol # "is used to mark keywords or topics in a Tweet. It was created organically by Twitter users as a way to categorize messages" and facilitate a search <ref type="bibr" coords="2,154.22,371.25,10.69,8.96" target="#b5">[6]</ref>. Hashtags are inserted before relevant keywords or phrases anywhere in tweets. Popular hashtags often represents trending topics. Bearing it in mind, we put higher weight to words occurring in hashtags. Usually key phrases are marked as a single hashtag. Thus, we split hashtags by capitalized letters.</p><p>Moreover, important information may be found in @replies, e.g. when a user reply to the post of a politician or other famous person. "An @reply is any update posted by clicking the "Reply" button on a Tweet" <ref type="bibr" coords="2,297.65,443.27,10.69,8.96" target="#b6">[7]</ref>. Since people may use their names as Twitter accounts we treat them analogically to hashtags, i.e. they are split by capitalized letters.</p><p>We assume that relevant sentence come from relevant documents, so we applied a search engine to find them. We use the tweet as a query. We choose the Terrier platform <ref type="bibr" coords="2,147.50,503.27,10.69,8.96" target="#b7">[8]</ref>, an open-source search engine developed by the School of Computing Science, University of Glasgow. It implements various weighting and retrieval models and allows stemming and blind relevance feedback. Terrier is suitable for different languages including English and Spanish. We choose Porter stemmer <ref type="bibr" coords="2,407.11,539.27,11.69,8.96" target="#b8">[9]</ref> for the English subtrack and Snowball stemmer <ref type="bibr" coords="2,272.21,551.27,16.72,8.96" target="#b9">[10]</ref> for the Spanish one.</p><p>The next step is to parse tweets and retrieved texts. For the English subtrack we applied Stanford CoreNLP which integrates such tools as POS tagger <ref type="bibr" coords="2,420.79,575.30,15.43,8.96" target="#b10">[11]</ref>, named entity recognizer <ref type="bibr" coords="2,195.41,587.30,15.43,8.96" target="#b11">[12]</ref>, parser and the co-reference resolution system. It uses the Penn Treebank tag set <ref type="bibr" coords="2,196.85,599.30,15.43,8.96" target="#b12">[13]</ref>. For the Spanish subtask we integrated Tree Tagger <ref type="bibr" coords="2,435.79,599.30,16.72,8.96" target="#b13">[14]</ref> and Apache OpenNLP <ref type="bibr" coords="2,203.21,611.30,15.43,8.96" target="#b14">[15]</ref>. Tree Tagger was used for lemmatization and POS tagging, while sentence detector, named entity recognition were performed by OpenNLP.</p><p>Then, we merged annotation obtained by parsers and Wikipedia tagging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Searching for Relevant Sentences</head><p>We modified the extraction component developed for INEX 2011-2012. The general idea of the approach 2011 was to compute similarity between the query and sentences and to retrieve the most similar passages. We model a sentence as a set of vectors. The first vector represents the tokens occurred within the sentence (unigram representation). Tokens are associated with lemmas. A lemma has the following features: POS, frequency and IDF. The second vector corresponds to bigrams. In both vector representation stop-words are retrieved. However, functional words, such as conjunctions, prepositions and determiners, are not taken into account in the unigram representation. NE comparison is hypothesized to be very efficient for contextualizing tweets about news. Therefore, the third vector refers to found named entities. Thereby, the same token may appear in several vectors.</p><p>For unigram and bigram vectors, we computed cosine, Jaccard and dice similarity measures, between a sentence and a target tweet. NE vectors are treated in the following way:</p><p>(</p><p>where is floating point parameter given by a user (by default it is equal to 1.0), is the number of NE appearing in both query and sentence, is the number of NE appearing in the query.</p><p>Each sentence has a set of attributes, e.g. which section it belongs to, whether it is a title or header, whether it has personal verbs etc.</p><p>We introduced an algorithm for smoothing from the local context. We assumed that the importance of the context reduces as the distance increases. Thus, the nearest sentences should produce more effect on the target sentence sense than others. For sentences with the distance greater than k this coefficient was zero. The total of all weights should be equal to one. The system allows taking into account k neighboring sentences with the weights depending on their remoteness from the target sentence.</p><p>Moreover, this year we added smoothing from document beginning. Wikipedia abstracts contain the summary of the entire paper; therefore they can be also used for smoothing.</p><p>In 2013, we did not applied anaphora resolution since it did not improve much our system according to evaluation in 2012 <ref type="bibr" coords="3,286.37,568.58,10.69,8.96" target="#b4">[5]</ref>. Neither we used sentence reordering as it was not evaluated.</p><p>We assumed that relevant sentences come from relevant documents therefore we multiply sentence score by document relevance or/and by inverted document rank. We tried to use generalized POS (e.g. we merge regular adverbs, superlative and comparative into a single adverb group).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Improving Readability</head><p>We introduced sentence quality measure based on the product of the Flesch reading ease test <ref type="bibr" coords="4,160.70,182.22,15.43,8.96" target="#b15">[16]</ref>, lexical diversity, meaningful word ratio and punctuation score.</p><p>Flesch Reading Ease test is a readability test designed to indicate comprehension difficulty when reading a passage (higher scores corresponds to texts that are easier to read):</p><p>(2)</p><p>We defined lexical diversity as the number of different lemmas used within a sentence divided by the total number of tokens in this sentence.</p><p>Analogically, meaningful word ration is the number of non-stop words within a sentence divided by the total number of tokens in this sentence.</p><p>Punctuation score is estimated by the formula:</p><p>(3)</p><p>In order to treat redundancy each sentence was mapped into a noun set. These sets were compared pairwise and if the normalized intersection was greater than a predefined threshold the sentences were rejected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>Summaries in English were evaluated according to their informativeness and readability <ref type="bibr" coords="4,138.74,461.87,10.69,8.96" target="#b2">[3]</ref>. Informativeness was estimated as the overlap of a summary with 3 pools of relevant passages: As in previous years, the lexical overlap between a summary and a pool was estimated in three terms: Unigrams, Bigrams and Skip bigrams representing the proportion of shared unigrams, bigrams and bigrams with gaps of two tokens respectively. Official ranking was based on decreasing score of divergence with ALL estimated by skip bigrams.</p><p>At the English subtrack we submitted 3 runs differing by sentence quality score and smoothing.</p><p>Our best run 275 was ranked first, second and third over 24 runs submitted by all participants on the PRIOR, POOL and ALL respectively (see Table <ref type="table" coords="4,399.07,681.98,3.90,8.96" target="#tab_1">1</ref>; IRIT's runs are set off in bold). It means that our best run is composed from the sentence of the most relevant documents. Among automatic runs our method was classified first (PRIOR and POOL) and second (ALL): the run 256 is marked as manual. It is also obvious that ranking is sensitive to not only pool selection, but also choice of divergence. According to bigrams and skip bigrams our best run is 275, while according to unigrams the best run is 273. We can also see than the runs 273 and 274 are quite close. In the run 273 each sentence is smoothed by its local context and first sentences from Wikipedia article which it is taken from. The run 274 has the same parameters except it does not have any smoothing. So, we can conclude that smoothing improves Informativeness. In our best run 275 punctuation score is not taken into account, it has slightly different formula for NE comparison and no penalization for numbers.</p><p>Readability was estimated as mean average scores per summary over soundness (no unresolved anaphora), non-redundancy and syntactical correctness among relevant passages of the ten tweets having the largest text references. According to all metrics except redundancy our approach was the best among all participants (see Table <ref type="table" coords="5,151.34,330.21,3.90,8.96" target="#tab_2">2</ref>; IRIT's runs are set off in bold). Runs were officially ranked according to mean average scores. Readability evaluation also showed that the run 275 is the best by relevance, soundness and syntax. However, the run 274 is much better in terms of avoiding redundant information. The runs 273 and 274 are close according readability assessment as well. 1 256 y 0,886 0,881 0,782 0,875 0,870 0,781 0,921 0,913 0,781 2 258 n 0,894 0,891 0,794 0,880 0,877 0,792 0,929 0,923 0,799 3 275 n 0,897 0,892 0,806 0,879 0,875 0,794 0,917 0,911 0,790 4 273 n 0,897 0,892 0,800 0,880 0,875 0,792 0,924 0,916 0,786 5 274 n 0,897 0,892 0,801 0,881 0,875 0,793 0,923 0,915 0,787 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This year we further developed our approach firstly introduced at INEX 2011 which is based on hashtag processing, TF-IDF cosine similarity measure enriched by smoothing from local context and document beginning, named entity recognition and part-of-speech weighting. We enriched our method by sentence quality measure based on Flesch reading ease test, lexical diversity, meaningful word ratio and punctuation ratio. We also used generalized POS (e.g. we merge regular adverbs, superlative and comparative into a single adverb group). Sentence score depends on document relevance and sentence type. We submitted 3 runs in English differing by sentence quality score and smoothing and 1 run in Spanish.</p><p>Our approach was ranked first, second and third over 24 runs submitted by all participants on the PRIOR, POOL and ALL respectively. Among automatic runs our method was classified first (PRIOR and POOL) and second (ALL).</p><p>Readability was estimated as mean average scores per summary over resolved anaphora, non-redundancy and syntactical correctness among relevant passages of the ten tweets having the largest text references. According to all metrics except redundancy our approach was the best.</p><p>In future we plan to automatize parameter selection by machine learning methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,132.06,401.68,327.04,73.11"><head>Table 1 . Informativeness evaluation</head><label>1</label><figDesc></figDesc><table coords="5,132.06,431.81,327.04,42.98"><row><cell>Rank</cell><cell>Run</cell><cell>Manual</cell><cell>All.skip</cell><cell>All.bi</cell><cell>All.uni</cell><cell>Pool.skip</cell><cell>Pool.bi</cell><cell>Pool.uni</cell><cell>Prior.skip</cell><cell>Prior.bi</cell><cell>Prior.uni</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,164.22,149.63,269.96,174.42"><head>Table 2 . Readability evaluation</head><label>2</label><figDesc></figDesc><table coords="6,164.22,172.45,269.96,151.60"><row><cell>Rank</cell><cell>Run</cell><cell>Mean Average</cell><cell>Relevancy (T)</cell><cell>Non</cell><cell>redundancy (R)</cell><cell>Soundness (A)</cell><cell>Syntax (S)</cell></row><row><cell>1</cell><cell>275</cell><cell cols="6">72.44% 76.64% 67.30% 74.52% 75.50%</cell></row><row><cell>2</cell><cell>256</cell><cell cols="6">72.13% 74.24% 71.98% 70.78% 73.62%</cell></row><row><cell>3</cell><cell>274</cell><cell cols="6">71.71% 74.66% 68.84% 71.78% 74.50%</cell></row><row><cell>4</cell><cell>273</cell><cell cols="6">71.35% 75.52% 67.88% 71.20% 74.96%</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,128.47,177.18,341.80,8.96;7,143.90,189.18,326.16,8.96;7,143.90,201.18,301.20,8.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,277.39,177.18,192.88,8.96;7,143.90,189.18,104.59,8.96">Tweet, Tweet, Retweet: Conversational Aspects of Retweeting on Twitter</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Golder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Lotan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,256.82,189.18,213.24,8.96;7,143.90,201.18,126.16,8.96">Proceedings of the 2010 43rd Hawaii International Conference on System Sciences</title>
		<meeting>the 2010 43rd Hawaii International Conference on System Sciences</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,128.47,213.18,342.11,8.96;7,143.90,225.18,33.53,8.96" xml:id="b1">
	<monogr>
		<ptr target="https://blog.twitter.com/2013/celebrating-twitter7" />
		<title level="m" coord="7,143.90,213.18,151.00,8.96">Celebrating #Twitter7 | Twitter Blog</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,128.47,237.18,342.23,8.96;7,143.90,249.21,89.17,8.96" xml:id="b2">
	<monogr>
		<ptr target="https://inex.mmci.uni-saarland.de/tracks/qa/" />
		<title level="m" coord="7,143.90,237.18,218.79,8.96">INEX 2013 Tweet Contextualization Track</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,128.47,261.21,341.66,8.96;7,143.90,273.21,224.46,8.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,255.96,261.21,172.19,8.96">IRIT at INEX: Question Answering Task</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ermakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mothe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,436.86,261.21,33.27,8.96;7,143.90,273.21,136.10,8.96">Focused Retrieval of Content and Structure</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="219" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,128.47,285.21,341.72,8.96;7,143.90,297.21,294.97,8.96;7,143.90,309.21,90.80,8.96" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ermakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mothe</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/documents/71612/3e9ecc64-fae6-4af3-93fd-1a" />
		<title level="m" coord="7,265.32,285.21,200.79,8.96">IRIT at INEX 2012: Tweet Contextualization</title>
		<imprint>
			<date type="published" when="2012">6a6fabb5d6, (2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,128.47,321.21,342.01,8.96;7,143.90,333.21,280.68,8.96" xml:id="b5">
	<monogr>
		<ptr target="https://support.twitter.com/articles/49309-what-are-hashtags-symbols" />
		<title level="m" coord="7,143.90,321.21,322.03,8.96">Twitter Help Center | What Are Hashtags (&quot;#&quot; Symbols)?</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,128.47,345.21,341.97,8.96;7,143.90,357.21,282.85,8.96;7,143.90,369.21,225.71,8.96" xml:id="b6">
	<monogr>
		<ptr target="https://support.twitter.com/groups/31-twitter-basics/topics/109-tweets-messages/articles/14023-what-are-replies-and-mentions" />
		<title level="m" coord="7,143.90,345.21,322.07,8.96">Twitter Help Center | What are @Replies and Mentions?</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,128.47,381.21,341.77,8.96;7,143.90,393.21,326.22,8.96;7,143.90,405.13,326.73,9.05;7,143.90,417.23,170.50,8.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,439.68,381.21,30.56,8.96;7,143.90,393.21,269.53,8.96">Terrier: A High Performance and Scalable Information Retrieval Platform</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,421.30,393.21,48.82,8.96;7,143.90,405.13,326.73,9.05;7,143.90,417.23,21.54,8.96">Proceedings of ACM SIGIR&apos;06 Workshop on Open Source Information Retrieval (OSIR 2006)</title>
		<meeting>ACM SIGIR&apos;06 Workshop on Open Source Information Retrieval (OSIR 2006)<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,128.47,429.23,342.11,8.96;7,143.90,441.23,243.39,8.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,201.42,429.23,131.62,8.96">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,339.83,429.23,130.74,8.96;7,143.90,441.23,6.45,8.96">Readings in information retrieval</title>
		<meeting><address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,133.08,453.23,166.87,8.96" xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Snowball</surname></persName>
		</author>
		<ptr target="http://snowball.tartarus.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,133.08,465.23,337.62,8.96;7,143.90,477.23,326.92,8.96;7,143.90,489.23,326.31,8.96;7,143.90,501.23,326.80,8.96;7,143.90,513.23,266.67,8.96" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,385.62,465.23,85.07,8.96;7,143.90,477.23,198.54,8.96">Feature-rich part-ofspeech tagging with a cyclic dependency network</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,349.73,477.23,121.08,8.96;7,143.90,489.23,326.31,8.96;7,143.90,501.23,181.90,8.96">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,133.08,525.23,337.48,8.96;7,143.90,537.23,326.17,8.96;7,143.90,549.23,326.80,8.96;7,143.90,561.23,287.30,8.96" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,321.52,525.23,149.05,8.96;7,143.90,537.23,221.68,8.96">Incorporating non-local information into information extraction systems by Gibbs sampling</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,373.06,537.23,97.01,8.96;7,143.90,549.23,250.01,8.96">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<title level="s" coord="7,456.28,549.23,14.42,8.96;7,143.90,561.23,157.46,8.96">Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,133.08,573.26,336.96,8.96;7,143.90,585.26,185.06,8.96" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="7,360.64,573.26,109.40,8.96;7,143.90,585.26,148.74,8.96">Building a large annotated corpus of English: the Penn Treebank</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,133.08,597.26,337.74,8.96;7,143.90,609.26,326.92,8.96;7,143.90,621.26,137.08,8.96" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="7,200.24,597.26,244.66,8.96">Probabilistic Part-of-Speech Tagging Using Decision Trees</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,453.23,597.26,17.59,8.96;7,143.90,609.26,326.92,8.96;7,143.90,621.26,27.79,8.96">Proceedings of the International Conference on New Methods in Language Processing</title>
		<meeting>the International Conference on New Methods in Language Processing<address><addrLine>Manchester, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,133.08,633.26,41.27,8.96;7,196.44,633.26,40.44,8.96;7,259.25,633.26,3.32,8.96;7,284.69,633.26,38.13,8.96;7,345.02,633.26,7.75,8.96;7,374.88,633.26,30.56,8.96;7,427.53,633.26,43.02,8.96;7,143.90,645.26,150.22,8.96" xml:id="b14">
	<monogr>
		<ptr target="http://opennlp.apache.org/index.html" />
		<title level="m" coord="7,143.90,633.26,30.45,8.96;7,196.44,633.26,40.44,8.96;7,259.25,633.26,3.32,8.96;7,284.69,633.26,38.13,8.96;7,345.02,633.26,7.75,8.96;7,374.88,633.26,30.56,8.96;7,427.53,633.26,37.64,8.96">Apache OpenNLP -Welcome to Apache OpenNLP</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,133.08,657.26,337.10,8.96;7,143.90,669.16,76.69,9.06" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="7,196.07,657.26,115.61,8.96">A new readability yardstick</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Flesch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,320.26,657.26,128.23,8.96">Journal of Applied Psychology</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="221" to="233" />
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
