<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,160.38,116.50,294.59,12.42">RSLIS at INEX 2013: Social Book Search Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,242.63,154.26,53.52,8.66"><forename type="first">Toine</forename><surname>Bogers</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Royal School of Library and Information Science</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<addrLine>Birketinget 6</addrLine>
									<postCode>2300</postCode>
									<settlement>Copenhagen</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,317.17,154.26,55.56,8.66"><forename type="first">Birger</forename><surname>Larsen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Royal School of Library and Information Science</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<addrLine>Birketinget 6</addrLine>
									<postCode>2300</postCode>
									<settlement>Copenhagen</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,160.38,116.50,294.59,12.42">RSLIS at INEX 2013: Social Book Search Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">060122C40728C95A788190D98574313F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>XML retrieval</term>
					<term>social tagging</term>
					<term>controlled metadata</term>
					<term>book recommendation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we describe our participation in the INEX 2013 Social Book Search track. We investigate the contribution of different types of document metadata, both social and controlled, as well as the contribution of the new 'query' topic representations. We find that the best results are obtained using all available document fields and topic representations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper, we describe our participation in the INEX 2013 Social Book Search track <ref type="foot" coords="1,155.67,398.70,3.72,6.06" target="#foot_0">1</ref> . Our goal for the Social Book Search task was to investigate the contribution of the new 'query' topic field representation provided for this year's task.</p><p>The structure of this paper is as follows. We start in Section 2 by describing our methodology: pre-processing the data, which document and topic fields we used for retrieval, and our evaluation. In Section 3, we describe the results of our contentbased retrieval runs, including the effect of the additional topic field representation. Section 4 describes which runs we submitted to INEX, with the results of those runs presented in Section 5. We discuss our results and conclude in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data and Preprocessing</head><p>In our experiments we used the Amazon/LibraryThing collection provided by the organizers of the INEX 2012 Social Book Search track. This collection contains XML representations of 2.8 million books, with the book representation data crawled from both Amazon.com and LibraryThing (LT). The 2012 collection is identical to the collection provided for the 2011 track <ref type="bibr" coords="1,308.91,611.85,13.77,9.56" target="#b0">[1]</ref> in all but two ways: the collection has been expanded with additional library records from the British Library (BL) and the Library of Congress (LoC). Of the 2.8 million books in the collection, 1.15 million have a BL record and 1.25 have a LoC record. Together these two sources cover 1.82 million of the 2.8 million books in the collection.</p><p>We converted the collection's original XML schema into a simplified version to retain only those metadata fields that were most likely to contribute to the successful retrieval of relevant books<ref type="foot" coords="2,292.39,165.52,3.72,6.06" target="#foot_1">2</ref> . After these pre-processing steps, we were left with the following 19 content-bearing XML fields in our collection: &lt;isbn&gt;, &lt;title&gt;, &lt;publisher&gt;, &lt;editorial&gt;, &lt;creator&gt;, &lt;series&gt;, &lt;award&gt;, &lt;character&gt;, &lt;place&gt;, &lt;blurber&gt;, &lt;epigraph&gt;, &lt;firstwords&gt;, &lt;lastwords&gt;, &lt;quotation&gt;, &lt;dewey&gt;, &lt;subject&gt;, &lt;browseNode&gt;, &lt;review&gt;, and &lt;tag&gt;.</p><p>We replaced the numeric Dewey codes in the original &lt;dewey&gt; fields by their proper textual descriptions using the 2003 list of Dewey category descriptions <ref type="foot" coords="2,464.81,237.51,3.72,6.06" target="#foot_2">3</ref> to enrich the controlled metadata assigned to each book. For example, the XML element &lt;dewey&gt;519&lt;/dewey&gt; was replaced by the element &lt;dewey&gt;Probabilities &amp; applied mathematics&lt;/dewey&gt;. The BL and LoC records were provided in MODS format <ref type="foot" coords="2,194.18,285.33,3.72,6.06" target="#foot_3">4</ref> , we mapped this format to the appropriate new XML fields and added them to the book representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Field categories and Indexing</head><p>The 19 selected XML fields in our collection's book representations fall into different categories. Some fields, such as &lt;dewey&gt; and &lt;subject&gt;, are examples of controlled metadata produced by LIS professionals, whereas other fields contains usergenerated metadata, such as &lt;review&gt; and &lt;tag&gt;. Yet other fields contain 'regular' book metadata, such as &lt;title&gt; and &lt;publisher&gt;. Fields such as &lt;quotation&gt; and &lt;firstwords&gt; represent a book's content more directly.</p><p>To examine the influence of these different types of fields, we divided the document fields into five different categories, each corresponding to an index. To examine the contribution of the additional BL/LoC controlled metadata we created two versions of the index containing controlled metadata: one with and one without this additional controlled metadata. In addition, we combined all five groups of relevant fields for an index containing all fields. This all-fields index also comes in two variants: one with and one without the BL/LoC metadata. This resulted in a total of eight indexes:</p><p>All fields For our first index all-doc-fields we simply indexed all of the available XML fields (see the previous section for a complete list). The all-doc-fields-plus index contains all of the original 2011 fields as well as the BL/LoC metadata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metadata</head><p>In our metadata index, we include all metadata fields that are immutably tied to the book itself and supplied by the publisher: &lt;title&gt;, &lt;publisher&gt;, &lt;editorial&gt;, &lt;creator&gt;, &lt;series&gt;, &lt;award&gt;, &lt;character&gt;, and &lt;place&gt;. Content For lack of access to the actual full-text books, we grouped together all XML fields in the content index that contain some part of the book text: blurbs, epigraphs, the first and last words, and quotations. This corresponded to indexing the fields &lt;blurber&gt;, &lt;epigraph&gt;, &lt;firstwords&gt;, &lt;lastwords&gt;, and &lt;quotation&gt;.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Controlled metadata</head><p>In our controlled-metadata index, we include the three controlled metadata fields curated by library professionals harvested from Amazon: &lt;browseNode&gt;, &lt;dewey&gt;, and &lt;subject&gt;. The controlled-metadata-plus index contains the original metadata as well as the BL/LoC metadata. Tags We split the social metadata contained in the document collection into two different types: tags and reviews. For the tags index, we used the tag field, expanding the tag count listed in the original XML. For example, the original XML element &lt;tag count="3"&gt;fantasy&lt;/tag&gt; would be expanded as &lt;tag&gt;fantasy fantasy fantasy&lt;/tag&gt;. This ensures that the most popular tags have a bigger influence on the final query-document matching. Reviews All user reviews belonging to a single book were combined in a single document representation for that book and added to our review index reviews.</p><p>We used the Indri 5.1 retrieval toolkit <ref type="foot" coords="3,304.65,311.28,3.72,6.06" target="#foot_4">5</ref> for indexing and retrieval. We performed stopword filtering on all of our indexes using the SMART stopword list, and preliminary experiments showed that using the Krovetz stemmer resulted in the best performance. Topic representations were processed in the same manner. Group The LibraryThing forum is divided into different groups covering different topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Topics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As</head><p>Narrative The first message of each forum topic, typically posted by the topic creator, describes the information need in more detail. This often contains a description of the information need, some background information, and possibly a list of books the topic creator has already read or is not looking for. The narrative typically contains the richest description of the topic. Query In 2013, three annotators hired to classify aspects of the 386 topics were also asked to provide query representations for each topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>All topic fields</head><p>We also performed runs with the title, group and narrative fields combined, referred to as all-topic-fields.</p><p>All topic fields + query We also performed runs with all four individual fields combined, referred to as all-plus-query to test the contribution of the query field.</p><p>In our experiments with the training and the test set, we restricted ourselves to automatic runs using the query, title, all-topic-fields, and all-plus-query representations (based on our experiments for INEX 2011-2012 <ref type="bibr" coords="4,366.85,297.98,12.25,9.56" target="#b1">[2]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Experimental setup</head><p>In all our retrieval experiments, we used the language modeling approach with Jelinek-Mercer (JM) smoothing as implemented in the Indri 5.1 toolkit. We preferred JM smoothing over Dirichlet smoothing, because previous work has shown that for longer, more verbose queries JM smoothing outperforms Dirichlet smoothing <ref type="bibr" coords="4,150.55,398.50,12.32,9.56" target="#b3">[3]</ref>, which matches the richer topic descriptions provided in the topic sets.</p><p>For the best possible performance, we optimized the λ parameter, which controls the influence of the collection language model, with higher values giving more influence to the collection language model. We varied λ in steps of 0.1, from 0.0 to 1.0 using the training set of topics. We also examined the value of stop word filtering and stemming and use the SMART stop word list and Krovetz stemming in these cases. This resulted in 44 different possible combinations of these three parameters. For each topic we retrieved up to 1000 documents and we used NDCG@10 as our evaluation metric <ref type="bibr" coords="4,210.37,494.43,12.32,9.56" target="#b4">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Content-based Retrieval</head><p>In order to produce a competitive baseline for our experiments, we conducted a first round of experiments focused on optimizing a standard content-based retrieval approach for all index and topic representations<ref type="foot" coords="4,340.71,575.83,3.72,6.06" target="#foot_5">6</ref> . We found that the best results were always produced with stop word filtering and Krovetz stemming, so all results reported in this paper share these settings. We compared the different index and the different topic representations for a total of 16 different content-based retrieval runs. Table <ref type="table" coords="4,183.29,625.52,5.32,8.66" target="#tab_0">1</ref> shows the best NDCG@10 results for each run on the training set. We can see several interesting results in Table <ref type="table" coords="5,341.03,304.23,3.99,8.66" target="#tab_0">1</ref>. First, we see that the best overall content-based run used all topic fields for the training topics, retrieved against the index containing all document fields (all-doc-fields) with an NDCG@10 score of 0.3058. Retrieving on the all-doc-fields index performs best on both topic sets (all-topic-fields and title), so for the runs submitted to the 2013 Social Book Search track we restricted ourselves to the all-doc-fields index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Submitted runs</head><p>We selected three automatic runs for submission to INEX<ref type="foot" coords="5,376.53,417.52,3.72,6.06" target="#foot_6">7</ref> based on the results of our content-based runs.</p><p>Run 1 (query.all-doc-fields) This run used the query field of the test topics and ran this against the index containing all available document fields (with the exception of the additional BL/LoC metadata). Run 2 (all-topic-fields.all-doc-fields) This run used the title, narrative, and group fields of the test topics and ran this against the index containing all available document fields (with the exception of the additional BL/LoC metadata). Run 3 (all-plus-query.all-doc-fields) This run used all four topic fields and ran this against the index containing all available document fields (with the exception of the additional BL/LoC metadata).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>The runs submitted to the INEX 2013 Social Book Search track were evaluated using graded relevance judgments 8 . All runs were evaluated using NDCG@10, P@10, MRR, with NDCG@10 as the main metric. Table <ref type="table" coords="6,348.61,119.30,5.32,8.66">2</ref> shows the official evaluation results.</p><p>Table <ref type="table" coords="6,159.01,161.19,3.74,7.76">2</ref>. Results of the three submitted runs on the test set, evaluated using all 386 topics with relevance judgments extracted from the LibraryThing forum topics. The best run scores are printed in bold.</p><p>Run # Run description NDCG@10 P@10 MRR 1 query.all-doc-fields We see that, unsurprisingly, the best-performing run on all 386 topics was run 3 with an NCDG@10 of 0.1361. Run 3 used all available topic fields including the new query field and all document fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion &amp; Conclusions</head><p>On both the training and the test sets the best results were achieved by combining all topic and document fields. This shows continued support for the principle of polyrepresentation <ref type="bibr" coords="6,227.44,388.21,13.77,9.56" target="#b5">[5]</ref> which states that combining cognitively and structurally different representations of the information needs and documents will increase the likelihood of finding relevant documents.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,147.04,405.32,333.55,8.66;3,134.76,417.28,345.83,8.66;3,134.76,429.23,345.83,8.66;3,134.76,441.19,345.83,8.66;3,134.76,453.14,345.83,8.66;3,134.76,465.10,345.83,8.66;3,134.76,477.05,345.83,8.66;3,134.76,489.01,345.83,8.66;3,134.76,500.96,345.83,8.66;3,134.76,512.92,230.65,8.66;3,149.71,525.52,330.88,8.66;3,134.76,537.48,345.83,8.66;3,134.76,549.44,345.83,8.66;3,134.76,561.39,95.54,8.66;3,134.76,585.88,345.83,9.56;3,151.70,598.51,328.89,8.66;3,151.70,609.77,94.13,9.36"><head></head><label></label><figDesc>part of the INEX 2013 Social Book Search track three sets of topics were released with requests for book recommendations based on textual description of the user's information need: three training sets and a test set. All topic sets were extracted from the LibraryThing forum. The original training set of 43 topics created for the 2011 Social Book Search track came with unverified relevance judgments. The 2011 Social Book Search track also supplied participants with 211 topics with relevance judgments derived from the books recommended on the LibraryThing discussion threads of these 211 topics. We used these 2011 test set as our training set to optimize our retrieval algorithms in the different runs. The results we report in Sections 3 and ?? were obtained using this training set. The test set for 2013 contains 386 new topics which were used to rank and compare the different participants' systems at INEX 2013. The results listed in Section 5 were obtained on this combined set of 386 topics. Each topic is represented by several different fields: Title The &lt;title&gt; field contains the title of the forum topic and typically provide a concise description of the information need. Runs that only use the topic title are referred to as title.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,336.05,215.71,95.18,7.79;6,193.49,226.67,4.79,7.79;6,213.95,226.03,217.28,8.42;6,193.49,237.62,4.79,7.79;6,213.95,236.99,106.04,8.29;6,335.46,237.64,96.35,7.76"><head></head><label></label><figDesc>-fields.all-doc-fields 0.1295 0.0647 0.2190 3 all-plus-query.all-doc-fields 0.1361 0.0653 0.2286</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,134.77,116.46,345.83,149.99"><head>Table 1 .</head><label>1</label><figDesc>Results of the 16 different content-based retrieval runs on the training set using NDCG@10 as evaluation metric. Best-performing runs for each topic representation are printed in bold.</figDesc><table coords="5,198.55,159.51,213.45,106.94"><row><cell>Document fields</cell><cell>title</cell><cell cols="2">Topic fields all-topic-fields</cell></row><row><cell>metadata</cell><cell cols="2">0.0915</cell><cell>0.2015</cell></row><row><cell>content</cell><cell cols="2">0.0108</cell><cell>0.0115</cell></row><row><cell>controlled-metadata</cell><cell cols="2">0.0406</cell><cell>0.0496</cell></row><row><cell>controlled-metadata-plus</cell><cell cols="2">0.0514</cell><cell>0.0691</cell></row><row><cell>tags</cell><cell cols="2">0.0792</cell><cell>0.2056</cell></row><row><cell>reviews</cell><cell cols="2">0.1041</cell><cell>0.2832</cell></row><row><cell>all-doc-fields</cell><cell cols="2">0.1129</cell><cell>0.3058</cell></row><row><cell>all-doc-fields-plus</cell><cell cols="2">0.1120</cell><cell>0.3029</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,657.03,238.27,8.03"><p>https://inex.mmci.uni-saarland.de/tracks/books/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,144.73,634.55,281.35,8.61"><p>Please consult<ref type="bibr" coords="2,200.03,634.55,12.39,8.61" target="#b1">[2]</ref> for more details on this filtering and conversion process.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,144.73,646.07,309.46,8.03"><p>Available at http://www.library.illinois.edu/ugl/about/dewey.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,144.73,657.03,271.77,8.03"><p>See http://www.loc.gov/standards/mods/ for more information.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="3,144.73,657.03,188.03,8.03"><p>Available at http://www.lemurproject.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="4,144.73,645.49,335.86,8.42;4,144.73,656.44,123.28,8.42"><p>This did not include the query and all-query-plus representations as the 2011 test set did not include the query topic field.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="5,144.73,635.16,100.05,7.79"><p>Our participant ID was 54.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="5,144.73,646.07,335.87,8.03;5,144.73,657.03,230.55,8.03"><p>The rules for assigning the different relevance grades can be found at https://inex. mmci.uni-saarland.de/tracks/books/results.jsp.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,138.35,468.24,342.24,7.79;6,146.93,479.20,333.66,7.79;6,146.93,490.16,100.68,7.79" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,375.07,468.24,105.52,7.79;6,146.93,479.20,110.54,7.79">Overview of the INEX 2011 Book and Social Search Track</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kazai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koolen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Landoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,278.04,479.20,202.55,7.79;6,146.93,490.16,46.10,7.79">INEX 2011 Workshop pre-proceedings. INEX Working Notes Series</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="11" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,501.12,342.24,7.79" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="6,302.21,501.12,174.47,7.79">RSLIS at INEX 2011: Social Book Search Track</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Bogers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">W</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Larsen</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,146.93,512.08,333.66,7.79;6,146.93,523.03,333.66,7.79;6,146.93,533.99,324.34,7.79" xml:id="b2">
	<analytic>
	</analytic>
	<monogr>
		<title level="m" coord="6,309.70,512.08,170.89,7.79;6,146.93,523.03,265.35,7.79">INEX 2011: Proceedings of the 10th International Workshop of the Initiative for the Evaluation of XML Retrieval</title>
		<title level="s" coord="6,146.93,533.99,129.09,7.79">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Geva</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Kamps</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Schenkel</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">7424</biblScope>
			<biblScope unit="page" from="45" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,544.95,342.24,7.79;6,146.93,555.91,333.66,7.79" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,231.90,544.95,248.70,7.79;6,146.93,555.91,79.46,7.79">A Study of Smoothing Methods for Language Models Applied to Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,233.01,555.91,159.55,7.79">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="214" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,566.87,342.24,7.79;6,146.93,577.83,227.92,7.79" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,256.86,566.87,195.64,7.79">Cumulated Gain-based Evaluation of IR Techniques</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kekäläinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,462.20,566.87,18.39,7.79;6,146.93,577.83,139.41,7.79">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="422" to="446" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,588.79,342.24,7.79;6,146.93,599.75,252.25,7.79" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,202.52,588.79,278.07,7.79;6,146.93,599.75,72.50,7.79">Cognitive Perspectives of Information Retrieval Interaction: Elements of a Cognitive IR Theory</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ingwersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,226.72,599.75,98.30,7.79">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="50" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
