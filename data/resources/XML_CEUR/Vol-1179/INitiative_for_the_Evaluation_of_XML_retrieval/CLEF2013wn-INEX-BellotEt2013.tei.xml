<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,152.27,116.95,310.81,12.62;1,270.87,134.89,73.62,12.62">Overview of INEX Tweet Contextualization 2013 track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,143.89,172.56,60.08,8.74"><forename type="first">Patrice</forename><surname>Bellot</surname></persName>
							<email>patrice.bellot@univ-amu.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">LSIS -Aix-Marseille University (</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,214.52,172.56,87.79,8.74"><forename type="first">Véronique</forename><surname>Moriceau</surname></persName>
							<email>moriceau@limsi.fr</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">LIMSI-CNRS</orgName>
								<orgName type="institution" key="instit2">University Paris-Sud (</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,312.86,172.56,63.02,8.74"><forename type="first">Josiane</forename><surname>Mothe</surname></persName>
							<email>josiane.mothe@irit.fr</email>
							<affiliation key="aff2">
								<orgName type="laboratory" key="lab1">IRIT</orgName>
								<orgName type="laboratory" key="lab2">UMR 5505</orgName>
								<orgName type="institution" key="instit1">Université de Toulouse</orgName>
								<orgName type="institution" key="instit2">Institut Universitaire de Formation des Maitres Midi-Pyrénées (France)</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,386.44,172.56,58.42,8.74"><forename type="first">Eric</forename><surname>Sanjuan</surname></persName>
							<email>eric.sanjuan@univ-avignon.fr</email>
							<affiliation key="aff3">
								<orgName type="laboratory">LIA</orgName>
								<orgName type="institution">Université d&apos;Avignon et des Pays de Vaucluse (France)</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,272.76,184.51,65.36,8.74"><forename type="first">Xavier</forename><surname>Tannier</surname></persName>
							<email>xtannier@limsi.fr</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">LIMSI-CNRS</orgName>
								<orgName type="institution" key="instit2">University Paris-Sud (</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,152.27,116.95,310.81,12.62;1,270.87,134.89,73.62,12.62">Overview of INEX Tweet Contextualization 2013 track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AC723E8FECB519905908D8987B5138FA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Short text contextualization</term>
					<term>Tweet understanding</term>
					<term>Automatic summarization</term>
					<term>Question answering</term>
					<term>Focus information retrieval</term>
					<term>XML</term>
					<term>Natural language processing</term>
					<term>Wikipedia</term>
					<term>Text readability</term>
					<term>Text informativeness</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Twitter is increasingly used for on-line client and audience fishing, this motivated the tweet contextualization task at INEX. The objective is to help a user to understand a tweet by providing him with a short summary (500 words). This summary should be built automatically using local resources like the Wikipedia and generated by extracting relevant passages and aggregating them into a coherent summary. The task is evaluated considering informativeness which is computed using a variant of Kullback-Leibler divergence and passage pooling. Meanwhile effective readability in context of summaries is checked using binary questionnaires on small samples of results. Running since 2010, results show that only systems that efficiently combine passage retrieval, sentence segmentation and scoring, named entity recognition, text POS analysis, anaphora detection, diversity content measure as well as sentence reordering are effective.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Motivation</head><p>Text contextualization <ref type="bibr" coords="1,234.52,585.38,10.52,8.74" target="#b9">[8,</ref><ref type="bibr" coords="1,246.70,585.38,7.75,8.74" target="#b8">7]</ref> differs from text expansion in that it aims at helping a human to understand a text rather than a system to better perform its task. For example, in the case of query expansion in IR, the idea is to add terms to the initial query that will help the system to better select the documents to be retrieved. Text contextualization on the contrary can be viewed as a way to provide more information on the corresponding text in the objective to make it understandable and to relate this text to information that explains it.</p><p>In the context of micro-blogging, which is increasingly used for many purposes such as for on-line client and audience fishing, contextualization is specifically important since 140 characters long messages are rarely self-content. This motivated the proposal in 2011 of a new track at Clef INEX lab of Tweet Contextualization.</p><p>The use case is as follows: given a tweet, the user wants to be able to understand the tweet by reading a short textual summary; this summary should be readable on a mobile device without having to scroll too much. In addition, the user should not have to query any system and the system should use a resource freely available. More specifically, the guideline specified the summary should be 500 words long and built from sentences extracted from a dump of Wikipedia. Wikipedia has been chosen both for evaluation purpose and because this is an increasing popular ressource while being generally trustable. In this paper, details the 2013 track set up and results. The use case and the topic selection remained stable since 2011 <ref type="bibr" coords="2,248.72,287.62,12.45,8.74" target="#b9">[8]</ref>, so that 2011 and 2012 topics could be used as a training set. However, In 2013 we considered more diverse types of tweets for this year edition, so that participants could better measure the impact of hashtag processing on their approaches.</p><p>The remaining of the paper is organised as follows: In section 2 we describe in detail the 2013 data collection. Section 3 presents the results and Section 4 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data collection</head><p>This section describes the document collection that is used as the resource for contextualization, as well as the topics selected for the test set which correspond to the tweets to contextualize.</p><p>The document collection has been built based on a recent dump of the English Wikipedia from November 2012. Since we target a plain XML corpus for an easy extraction of plain text answers, like in past years, we used the same perl programs released for all participants to remove all notes and bibliographic references that are difficult to handle and keep only non empty Wikipedia pages (pages having at least one section).</p><p>Resulting automatically generated documents from Wikipedia dump, consist of a title (title), an abstract (a) and sections (s). Each section has a sub-title (h). Abstract and sections are made of paragraphs (p) and each paragraph can contain entities (t) that refer to other Wikipedia pages.</p><p>Over 2012 and 2013 editions, evaluated topics were made of 120 (60 topics each year) tweets manually collected by organizers. These tweets were selected and checked, in order to make sure that:</p><p>-They contained "informative content" (in particular, no purely personal messages); Only non-personal accounts were considered (i.e. @CNN, @TennisTweets, @PeopleMag, @science. . . ).</p><p>-The document collection from Wikipedia contained related content, so that a contextualization was possible.</p><p>From the same set of accounts, more than 1,800 tweets were then collected automatically. These tweets were added to the evaluation set, in order to avoid that fully manual, or not robust enough systems could achieve the task. All tweets were then to be treated by participants, but only the 120 short list was used for evaluation. Participants did not know which topics were selected for evaluation.</p><p>These tweets were provided in a text-only format without metadata and in a JSON format with all associated metadata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>This year the entire evaluation process was carried out by organizers.</p><p>Tweet contextualization <ref type="bibr" coords="3,258.95,309.02,10.52,8.74" target="#b7">[6]</ref> is evaluated on both informativeness and readability. Informativeness aims at measuring how well the summary explains the tweet or how well the summary helps a user to understand the tweet content. On the other hand, readability aims at measuring how clear and easy to understand the summary is.</p><p>Informativeness measure is based on lexical overlap between a pool of relevant passages (RPs) and participant summaries. Once the pool of RPs is constituted, the process is automatic and can be applied to unofficial runs. The release of these pools is one of the main contributions of Tweet Contextualization tracks at INEX <ref type="bibr" coords="3,167.42,416.85,15.36,8.74" target="#b9">[8,</ref><ref type="bibr" coords="3,184.44,416.85,7.01,8.74" target="#b8">7]</ref>.</p><p>By contrast, readability is evaluated manually and cannot be reproduced on unofficial runs. In this evaluation the assessor indicates where he misses the point of the answers because of highly incoherent grammatical structures, unsolved anaphora, or redundant passages. Like in 2012, three metrics were used: Relevancy (or Relaxed) metric, counting passages where the T box has not been checked; Syntax, counting passages where the S box was not checked either, and the Structure (or Strict) metric counting passages where no box was checked at all.</p><p>Participant runs were ranked according to the average, normalized number of words in valid passages.</p><p>In 2013, a total number of 13 teams from 9 countries (Brasil, Canada, France, India, Ireland, Mexico, Russia, Spain, USA) submitted 24 runs to the Tweet Contextualization track in the framework of CLEF INEX lab 2013.</p><p>Infomativity results are presented in Table <ref type="table" coords="3,343.64,585.15,4.98,8.74">1</ref> and statistical significance of differencies between scores are indicated in Table <ref type="table" coords="3,356.35,597.11,3.87,8.74">2</ref>. Table <ref type="table" coords="3,395.98,597.11,4.98,8.74">1</ref> shows readability scores.</p><p>This year, the best participating system (199) used hashtag preprocessing introduced in <ref type="bibr" coords="3,187.11,633.20,9.96,8.74" target="#b2">[1]</ref>. The best run by this participant used all available tweet features including web links which was not allowed by organisers. However his second best run without using linked web pages is ranked first among official runs. This participant also tried to weight hashtags based on 2012 results but this did not improve results. Perhaps because topics evaluated in 2012 were too specific.</p><p>Second best participant (182) in informativity and best in readability used state of the art NLP tools. This participant was first in informativity in 2011 <ref type="bibr" coords="4,467.31,158.06,9.96,8.74" target="#b3">[2]</ref>. Differences between these two best systems are not statistically significant. Third best participant system (65) was first in 2012 <ref type="bibr" coords="4,379.70,184.17,9.96,8.74" target="#b5">[4]</ref>, so the same system performs well even on a more diversify set of tweets.</p><p>Reference system by organisers (62-276) available online through an API is not more among three best systems. This systems is a robust focused information retrieval system <ref type="bibr" coords="4,209.42,234.19,10.52,8.74" target="#b7">[6]</ref> that was not smoothed for tweets. This year we also set up a baseline (62 -278) using a state of the art IR system on sentences. Its informativity scores are high but its readability is very low.</p><p>Overall, informativity and readability scores are this year strongly correlated (Kendall test: τ &gt; 90%, p &lt; 10 -3 ) which shows that all systems have integrated this constrain. Remenber that since 2012, readability is evaluated in the context of the tweet. Passages not related to the tweet are considered as unreadable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>Like in 2012, almots all participants used language models.</p><p>Terminology extraction and reformulation applied to tweets was also used in 2013 like in previous editions <ref type="bibr" coords="5,278.45,482.55,9.96,8.74">[9]</ref>. Appropriate stemming and robust parsing of both tweets and wikipedia pages also seems to be an important issue. Most systems having a run among the top ten in informativeness used the Standford Core NLP tool or the TreeTagger.</p><p>It also seems that automatic readability evaluation and anaphora detection helps improving readability scores, but also informativeness density in summaries. It is now clear that state of the art summarization methods based on sentence scoring <ref type="bibr" coords="5,208.50,568.64,10.52,8.74" target="#b6">[5]</ref> proved to be helpful on this task even though they need to be combined with an IR engine.</p><p>Best run in 2013 also experimented a tweet hashtag scoring technique introduced in 2012 <ref type="bibr" coords="5,198.42,606.90,10.52,8.74" target="#b2">[1]</ref> while generating the summary.</p><p>Finally, this time the state of the art system proposed by organizers since 2010 combining LM indexation, terminology graph extraction and summarization based on shallow parsing was not ranked among the six best runs which shows that participant systems improved on this task over the three editions.</p></div>		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,204.96,345.89,205.42,7.86;4,213.35,357.25,4.61,7.86;4,245.22,357.25,13.82,7.86;4,277.92,357.25,123.77,7.86;4,213.35,368.21,4.61,7.86;4,245.22,368.21,13.82,7.86;4,280.22,368.21,121.47,7.86;4,213.35,379.17,4.61,7.86;4,245.22,379.17,13.82,7.86;4,280.22,379.17,121.47,7.86;4,213.35,390.13,4.61,7.86;4,245.22,390.13,13.82,7.86;4,280.22,390.13,121.47,7.86;4,213.35,401.08,4.61,7.86;4,245.22,401.08,13.82,7.86;4,280.22,401.08,121.47,7.86;4,213.35,412.04,4.61,7.86;4,245.22,412.04,13.82,7.86;4,277.92,412.04,123.77,7.86;4,213.35,423.00,4.61,7.86;4,247.52,423.00,9.22,7.86;4,280.22,423.00,121.47,7.86;4,213.35,433.96,4.61,7.86;4,247.52,433.96,9.22,7.86;4,280.22,433.96,121.47,7.86;4,213.35,444.92,4.61,7.86;4,247.52,444.92,9.22,7.86;4,280.22,444.92,121.47,7.86;4,211.04,455.88,9.22,7.86;4,247.52,455.88,9.22,7.86;4,280.22,455.88,121.47,7.86;4,211.04,466.84,9.22,7.86;4,247.52,466.84,9.22,7.86;4,280.22,466.84,121.47,7.86;4,211.04,477.80,9.22,7.86;4,247.52,477.80,9.22,7.86;4,280.22,477.80,121.47,7.86;4,211.04,488.76,9.22,7.86;4,245.22,488.76,13.82,7.86;4,280.22,488.76,121.47,7.86;4,211.04,499.71,9.22,7.86;4,245.22,499.71,13.82,7.86;4,280.22,499.71,121.47,7.86;4,211.04,510.67,9.22,7.86;4,245.22,510.67,13.82,7.86;4,280.22,510.67,121.47,7.86;4,211.04,521.63,9.22,7.86;4,245.22,521.63,13.82,7.86;4,280.22,521.63,121.47,7.86;4,211.04,532.59,9.22,7.86;4,245.22,532.59,13.82,7.86;4,280.22,532.59,121.47,7.86;4,211.04,543.55,9.22,7.86;4,245.22,543.55,13.82,7.86;4,280.22,543.55,121.47,7.86;4,211.04,554.51,9.22,7.86;4,245.22,554.51,13.82,7.86;4,280.22,554.51,121.47,7.86;4,211.04,565.47,9.22,7.86;4,245.22,565.47,13.82,7.86;4,280.22,565.47,121.47,7.86;4,211.04,576.43,9.22,7.86;4,245.22,576.43,13.82,7.86;4,280.22,576.43,121.47,7.86;4,211.04,587.39,9.22,7.86;4,245.22,587.39,13.82,7.86;4,280.22,587.39,121.47,7.86;4,211.04,598.35,9.22,7.86;4,245.22,598.35,13.82,7.86;4,280.22,598.35,121.47,7.86;4,211.04,609.30,9.22,7.86;4,245.22,609.30,13.82,7.86;4,277.92,609.30,123.77,7.86;4,173.45,620.69,268.46,7.89;5,218.50,117.23,196.36,13.82;5,202.24,136.29,210.88,7.86;5,202.24,147.25,210.88,7.86;5,202.24,158.20,210.89,7.86;5,202.24,169.16,210.89,7.86;5,202.24,180.12,210.89,7.86;5,202.24,191.08,210.88,7.86;5,202.24,202.04,210.89,7.86;5,202.24,213.00,210.88,7.86;5,202.24,223.96,210.88,7.86;5,202.24,234.92,210.88,7.86;5,202.24,245.88,210.88,7.86;5,202.24,256.83,210.88,7.86;5,202.24,267.79,210.89,7.86;5,202.24,278.75,210.88,7.86;5,202.24,289.71,210.88,7.86;5,202.24,300.67,210.88,7.86;5,202.24,311.63,210.88,7.86;5,202.24,322.59,210.89,7.86;5,202.24,333.55,210.89,7.86;5,202.24,344.51,210.89,7.86;5,202.24,355.46,210.89,7.86;5,202.24,366.42,210.88,7.86;5,202.24,377.38,133.77,7.86;5,410.05,377.38,3.07,7.86;5,134.77,388.76,345.83,7.89;5,134.77,399.75,115.47,7.86" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="4,204.96,345.89,205.42,7.86;4,213.35,357.25,4.61,7.86">Rank Participant Run unigram bigram with 2-gap 1</title>
		<idno>3 3 258 1 -----3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 275 ------2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 273 ------2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 274 ------2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 257 2 -----3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 254 3 3 2 2 2 3 --2 2 3 3 3 3 3</idno>
		<imprint>
			<biblScope unit="page" from="264" to="266" />
		</imprint>
	</monogr>
	<note>Informativeness results(official results are &quot;with 2-gap&quot;)</note>
</biblStruct>

<biblStruct coords="6,136.56,118.78,345.70,7.86;6,144.94,130.13,73.28,7.86;6,244.04,130.13,28.67,7.86;6,317.76,130.13,28.67,7.86;6,392.11,130.13,28.67,7.86;6,446.29,130.13,28.67,7.86;6,144.94,141.09,73.28,7.86;6,244.04,141.09,28.67,7.86;6,317.76,141.09,28.67,7.86;6,392.11,141.09,28.67,7.86;6,446.29,141.09,28.67,7.86;6,144.94,152.05,73.28,7.86;6,244.04,152.05,28.67,7.86;6,317.76,152.05,28.67,7.86;6,392.11,152.05,28.67,7.86;6,446.29,152.05,28.67,7.86;6,144.94,163.01,73.28,7.86;6,244.04,163.01,28.67,7.86;6,317.76,163.01,28.67,7.86;6,392.11,163.01,28.67,7.86;6,446.29,163.01,28.67,7.86;6,144.94,173.97,73.28,7.86;6,244.04,173.97,28.67,7.86;6,317.76,173.97,28.67,7.86;6,392.11,173.97,28.67,7.86;6,446.29,173.97,28.67,7.86;6,144.94,184.93,73.28,7.86;6,244.04,184.93,28.67,7.86;6,317.76,184.93,28.67,7.86;6,392.11,184.93,28.67,7.86;6,446.29,184.93,28.67,7.86;6,144.94,195.89,73.28,7.86;6,244.04,195.89,28.67,7.86;6,317.76,195.89,28.67,7.86;6,392.11,195.89,28.67,7.86;6,446.29,195.89,28.67,7.86;6,144.94,206.85,73.28,7.86;6,244.04,206.85,28.67,7.86;6,317.76,206.85,28.67,7.86;6,392.11,206.85,28.67,7.86;6,446.29,206.85,28.67,7.86;6,144.94,217.81,73.28,7.86;6,244.04,217.81,28.67,7.86;6,317.76,217.81,28.67,7.86;6,392.11,217.81,28.67,7.86;6,446.29,217.81,28.67,7.86;6,142.64,228.76,75.59,7.86;6,244.04,228.76,28.67,7.86;6,317.76,228.76,28.67,7.86;6,392.11,228.76,28.67,7.86;6,446.29,228.76,28.67,7.86;6,142.64,239.72,75.59,7.86;6,244.04,239.72,28.67,7.86;6,317.76,239.72,28.67,7.86;6,392.11,239.72,28.67,7.86;6,446.29,239.72,28.67,7.86;6,142.64,250.68,75.59,7.86;6,244.04,250.68,28.67,7.86;6,317.76,250.68,28.67,7.86;6,392.11,250.68,28.67,7.86;6,446.29,250.68,28.67,7.86;6,142.64,261.64,75.59,7.86;6,244.04,261.64,28.67,7.86;6,317.76,261.64,28.67,7.86;6,392.11,261.64,28.67,7.86;6,446.29,261.64,28.67,7.86;6,142.64,272.60,75.59,7.86;6,244.04,272.60,28.67,7.86;6,317.76,272.60,28.67,7.86;6,392.11,272.60,28.67,7.86;6,446.29,272.60,28.67,7.86;6,142.64,283.56,75.59,7.86;6,244.04,283.56,28.67,7.86;6,317.76,283.56,28.67,7.86;6,392.11,283.56,28.67,7.86;6,446.29,283.56,28.67,7.86;6,142.64,294.52,75.59,7.86;6,244.04,294.52,28.67,7.86;6,317.76,294.52,28.67,7.86;6,392.11,294.52,28.67,7.86;6,446.29,294.52,28.67,7.86;6,142.64,305.48,75.59,7.86;6,244.04,305.48,28.67,7.86;6,317.76,305.48,28.67,7.86;6,392.11,305.48,28.67,7.86;6,446.29,305.48,28.67,7.86;6,142.64,316.44,75.59,7.86;6,244.04,316.44,28.67,7.86;6,317.76,316.44,28.67,7.86;6,392.11,316.44,28.67,7.86;6,446.29,316.44,28.67,7.86;6,142.64,327.39,75.59,7.86;6,244.04,327.39,28.67,7.86;6,317.76,327.39,28.67,7.86;6,392.11,327.39,28.67,7.86;6,446.29,327.39,28.67,7.86;6,142.64,338.35,75.59,7.86;6,244.04,338.35,28.67,7.86;6,317.76,338.35,28.67,7.86;6,392.11,338.35,28.67,7.86;6,446.29,338.35,28.67,7.86;6,142.64,349.31,75.59,7.86;6,244.04,349.31,28.67,7.86;6,317.76,349.31,28.67,7.86;6,392.11,349.31,28.67,7.86;6,446.29,349.31,28.67,7.86;6,142.64,360.27,75.59,7.86;6,244.04,360.27,28.67,7.86;6,317.76,360.27,28.67,7.86;6,392.11,360.27,28.67,7.86;6,446.29,360.27,28.67,7.86;6,249.81,371.15,115.74,7.89;6,134.77,384.09,62.94,10.52" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,136.56,118.78,330.34,7.86">Rank Run Mean AVG Relevancy (T) Non redundancy (R) Soundness (A) Syntax</title>
		<idno>13% 74.24% 71.98% 70.78% 73.62% 3 274 71.71% 74.66% 68.84% 71.78% 74.50% 4 273 71.35% 75.52% 67.88% 71.20% 74.96% 5 257 69.54% 72.18% 65.48% 70.96% 72.18% 6 254 67.46% 73.30% 61.52% 68.94% 71.92% 7 258 65.97% 68.36% 64.52% 66.04% 67.34% 8 276 49.72% 52.08% 45.84% 51.24% 52.08% 9 267 46.72% 50.54% 40.90% 49.56% 49.70% 10 270 44.17% 46.84% 41.20% 45.30% 46.00% 11 271 38.76% 41.16% 35.38% 39.74% 41.16% 12 264 38.56% 41.26% 33.16% 41.26% 41.26% 13 260 38.21% 38.64% 37.36% 38.64% 38.64% 14 265 37.92% 39.46% 36.46% 37.84% 39.46% 15 259 37.70% 38.78% 35.54% 38.78% 38.78% 16 255 36.59% 38.98% 31.82% 38.98% 38.98% 17 261 35.99% 36.42% 35.14% 36.42% 36.42% 18 263 32.75% 34.48% 31.86% 31.92% 34.48% 19 262 32.35% 33.34% 30.38% 33.34% 33.34% 20 266 25.64% 25.92% 25.08% 25.92% 25.92% 21 277 20.00% 20.00% 20.00% 20.00% 20.00% 22 269 00.04% 00.04% 00.04% 00.04</idno>
	</analytic>
	<monogr>
		<title level="j" coord="6,474.07,118.78,8.19,7.86">S)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">30% 74</biblScope>
			<biblScope unit="page" from="256" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,407.80,342.24,7.86;6,146.91,418.76,76.72,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="6,253.23,407.80,223.52,7.86">Lia/lina at the inex 2012 tweet contextualization track</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Deveaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Boudin</surname></persName>
		</author>
		<editor>Forner et al.</editor>
		<imprint>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,429.47,342.24,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,248.48,429.47,42.80,7.86">Irit at inex</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ermakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mothe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,317.16,429.47,94.98,7.86">Tweet contextualization</title>
		<editor>
			<persName><surname>Forner</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,451.13,342.24,7.86;6,146.91,462.09,331.55,7.86" xml:id="b4">
	<monogr>
		<title level="m" coord="6,362.71,451.13,117.87,7.86;6,146.91,462.09,151.81,7.86">CLEF 2012 Evaluation Labs and Workshop, Online Working Notes</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Karlgren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Womser-Hacker</surname></persName>
		</editor>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">September 17-20, 2012 (2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,472.79,342.24,7.86;6,146.91,483.75,218.58,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="6,319.78,472.79,160.81,7.86;6,146.91,483.75,134.82,7.86">Dcu@inex-2012: Exploring sentence retrieval for tweet contextualization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Leveling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
		<editor>Forner et al.</editor>
		<imprint>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,494.46,342.24,7.86;6,146.91,505.42,199.62,7.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="6,312.11,494.46,168.48,7.86;6,146.91,505.42,115.99,7.86">Two statistical summarizers at inex 2012 tweet contextualization track</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M T</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Velázquez-Morales</surname></persName>
		</author>
		<editor>Forner et al.</editor>
		<imprint>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,516.12,342.24,7.86;6,146.91,527.08,333.68,7.86;6,146.91,538.04,333.68,7.86;6,146.91,549.00,25.60,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,352.57,516.12,128.02,7.86;6,146.91,527.08,123.25,7.86">Overview of the inex 2010 question answering track (qa@inex)</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sanjuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bellot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Moriceau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Tannier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="6,172.70,538.04,171.13,7.86">INEX. Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Geva</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Kamps</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Schenkel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Trotman</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">6932</biblScope>
			<biblScope unit="page" from="269" to="281" />
			<date type="published" when="2010">2010</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,559.70,342.24,7.86;6,146.91,570.66,221.12,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="6,397.28,559.70,83.31,7.86;6,146.91,570.66,137.49,7.86">Overview of the inex 2012 tweet contextualization track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sanjuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Moriceau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Tannier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bellot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mothe</surname></persName>
		</author>
		<editor>Forner et al.</editor>
		<imprint>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,581.36,342.24,7.86;6,146.91,592.32,333.68,7.86;6,146.91,603.28,333.68,7.86;6,146.91,614.24,192.56,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,413.59,581.36,67.00,7.86;6,146.91,592.32,185.76,7.86">Overview of the inex 2011 question answering track (qa@inex)</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sanjuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Moriceau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Tannier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bellot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mothe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,185.55,603.28,176.21,7.86">Focused Retrieval of Content and Structure</title>
		<title level="s" coord="6,369.16,603.28,111.43,7.86;6,146.91,614.24,27.78,7.86">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Geva</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Kamps</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Schenkel</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">7424</biblScope>
			<biblScope unit="page" from="188" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,624.95,342.24,7.86;6,146.91,635.90,333.68,7.86;6,146.91,646.86,26.79,7.86" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="6,250.09,624.95,230.50,7.86;6,146.91,635.90,286.66,7.86">Inex tweet contextualization track at clef 2012: Query reformulation using terminological patterns and automatic summarization</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vivaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Da Cunha</surname></persName>
		</author>
		<editor>Forner et al.</editor>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
