<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,242.00,116.95,131.36,12.62;1,226.15,134.89,163.06,12.62">IPL at CLEF 2013 Medical Retrieval Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,214.13,173.08,97.07,8.74"><forename type="first">Spyridon</forename><surname>Stathopoulos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="laboratory">Information Processing Laboratory</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<addrLine>76 Patission Str, 104.34</addrLine>
									<settlement>Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,319.09,173.08,77.42,8.74"><forename type="first">Ismini</forename><surname>Lourentzou</surname></persName>
							<email>lourentzouismini@hotmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="laboratory">Information Processing Laboratory</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<addrLine>76 Patission Str, 104.34</addrLine>
									<settlement>Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,193.05,185.03,100.40,8.74"><forename type="first">Antonia</forename><surname>Kyriakopoulou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="laboratory">Information Processing Laboratory</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<addrLine>76 Patission Str, 104.34</addrLine>
									<settlement>Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,320.94,185.03,101.37,8.74"><forename type="first">Theodore</forename><surname>Kalamboukis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="laboratory">Information Processing Laboratory</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<addrLine>76 Patission Str, 104.34</addrLine>
									<settlement>Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,242.00,116.95,131.36,12.62;1,226.15,134.89,163.06,12.62">IPL at CLEF 2013 Medical Retrieval Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">21BDD65E916841C6BDAA0F4926208EC3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>LSA</term>
					<term>LSI</term>
					<term>CBIR</term>
					<term>Data Fusion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article presents an experimental evaluation on using a refined approach to the Latent Semantic Analysis (LSA) for efficiently searching very large image databases. It also describes IPL's participation to the image CLEF ad-hoc textual and visual retrieval as well as modality classification for the Medical Task in 2013. We report on our approaches and methods and present the results of our extensive experiments applying early data fusion with LSA on several low-level visual and textual features.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the years latent semantic analysis has been applied with success in text retrieval providing successful results in several applications <ref type="bibr" coords="1,403.19,501.17,9.96,8.74" target="#b0">[1]</ref>. However, due to the challenges of LSA in terms of computational and memory requirements in cases of image retrieval, only small datasets have been tested. In our approach, our aim is the visual representation of an image with a feature vector of a moderate size, (m), and the use of a by-pass solution to the singular value decomposition which overcomes all its deficiencies and makes the method attractive for content-based image retrieval <ref type="bibr" coords="1,318.01,572.91,9.96,8.74" target="#b1">[2]</ref>. In this way instead of performing SVD to the feature-by-document matrix C, (m × n) we solve the eigenproblem of the feature-correlation matrix CC T , (m × m).</p><p>Concerning the stability of the eigensolution for the matrix CC T , the method may be unstable for two reasons: first, the conditioning number of the matrix is much higher, and second, perturbations introduced while forming the normal matrix (CC T ) may change its rank. In such cases, the normal matrix will be more sensitive to perturbations in the data than the data matrix (C).</p><p>However the numerical stability of an eigenproblem is ensured when the eigenvalues are well separated <ref type="bibr" coords="2,266.89,131.95,9.96,8.74" target="#b2">[3]</ref>. During preliminary experiments and previous work <ref type="bibr" coords="2,159.76,143.90,10.20,8.74" target="#b1">[2]</ref>[4], we have observed that the eigenvalues of CC T follow a power law distribution. This ensures that the largest eigenvalues are well separated. It was also indicated that a value of k (k largest eigenvalues) between 50 and 100 gives optimal results. Furthermore, matrix C is stored in integer form for both visual and textual data. Thus, no rounding is introduced in the computation of CC T matrix. To further reduce the size of the CC T matrix, we have applied a variance based feature selection. Thus, the largest eigenproblem that was required to be solved for this years' challenge was that of a CC T [1400 x 1400] matrix.</p><p>In order to overcome the increased memory demands for the computation of the correlation matrix CC T , matrix C is split into a number of blocks, such that each block can be accommodated into the memory. Subsequently, the eigenproblem is solved and the k largest eigenvalues, S k , with their corresponding eigenvectors, U k , are selected. The original feature vectors are then projected into the k-th dimensional space, using the transformation, y k = U T k y, on the original vector representation of an image y. The same projection is also applied for a query image q k with q k = U T k q and the similarity with an image score(q k , y k ), is calculated by the cosine function.</p><p>The proposed method seems to greatly improve the final database size, query response time and memory requirements. It is also shown that the efficiency of this method still holds in cases of large databases in cases such as the PubMED Database with 306.000 figures, used in this year's medical task <ref type="bibr" coords="2,422.67,383.00,9.96,8.74" target="#b4">[5]</ref>. It should be noted that by using the traditional solution of SVD for this database, the ad-hoc visual retrieval task would be impossible with our computer resources. This approach can also exploit the dimensionality reduction and enable the early data fusion of different low-level visual features, without increasing the cost in memory, disk space and response time of a retrieval system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Visual Retrieval</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Image Representation</head><p>For the low-level representation of each image, a set of localized image descriptors was extracted. In order to address the variations in resolution between images, first, a size normalization was performed by re-scaling each image to a fixed size of 256 x 256 pixels using bi-linear interpolation. Next, each image was split into 3 x 3 fixed sized blocks and a local visual descriptor was extracted from each block. The image's final feature vector was constructed by concatenating each local vector. i.e if for an image, we extract a gray color histogram in 128 colors per block, for a total of 9 blocks, the resulting feature vector will be of size 9 x 128 = 1152. This process is depicted in Figure <ref type="figure" coords="2,339.42,613.53,3.87,8.74" target="#fig_0">1</ref>.</p><p>In our experiments, the vector representation was based on seven types of low-level visual features:</p><p>1. Gray color histogram (CH) extracted in 128 gray scales. All the features were extracted using the Java library Caliph&amp;Emir of the Lire CBIR system <ref type="bibr" coords="3,220.24,453.15,9.96,8.74" target="#b5">[6]</ref>. Finally, by using feature selection, the vector for each descriptor was reduced into a fraction of its original size. Table <ref type="table" coords="3,410.14,465.10,4.98,8.74" target="#tab_0">1</ref> lists the visual descriptors along with their corresponding vector size per block and per image before and after feature selection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Early Fusion</head><p>In order to improve retrieval efficiency, different low level visual descriptors were combined with early fusion. Image descriptor vectors were concatenated to create a single fused vector thus, creating the feature-by-document matrix C previously mentioned. The query feature vectors were fused with the same order and the resulting vector was projected as described in 1. For example, the early fusion of CH, CEDD and FCTH features will form the matrix C = [CH; CEDD; F CT H; ] in matlab notation of size 306.000 x 800.</p><p>The solution of the CC T U = U S 2 problem was done for a CC T of size 800 x 800. The eigs function of Matlab was used for this solution. For the visual retrieval task, several data fusion combinations of different descriptors were tested. These combinations are presented in Table <ref type="table" coords="4,320.49,259.20,4.98,8.74" target="#tab_1">2</ref> along with the task's corresponding run ID. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Modality Class Vectors</head><p>To further improve the retrieval efficiency, modality class vectors were constructed by using a subset of the classifiers used for the modality classification task. For this method, a combination of low level feature vectors was extracted from each image as described above. In addition, each image was classified into one of the 31 modalities and a very sparse vector CV of size [31x1] was created by setting the value '1' at the index that corresponds to the predicted modality. The rest of the vector's elements are set to '0'. Finally, this vector was early fused with the rest of the visual vectors, i.e for the previous example, the matrix C = [CH; CEDD; F CT H; CV ; ] was formed. Table <ref type="table" coords="4,368.46,536.10,4.98,8.74" target="#tab_2">3</ref> presents the runs using this method with their corresponding run ID, visual descriptors and classifier used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Results</head><p>For the AMIA's medical task <ref type="bibr" coords="4,265.77,609.29,10.52,8.74" target="#b4">[5]</ref> we have submitted a total of eight visual runs using different combinations of low level feature descriptors with early fusion and class filtering. In Table <ref type="table" coords="4,264.45,633.20,3.87,8.74" target="#tab_3">4</ref>, we list the runs ids with their corresponding results. Finally, we attempted to test how our retrieval method responds in image datasets of different sizes and context. Thus, in this year's ImageCLEF <ref type="bibr" coords="4,452.28,657.11,9.96,8.74" target="#b6">[7]</ref>, we have also participated to the ImageCLEF's Personal photo retrieval sub-task <ref type="bibr" coords="5,470.08,218.88,10.52,8.74" target="#b7">[8]</ref> using the feature combinations listed in Table <ref type="table" coords="5,339.01,230.84,3.87,8.74" target="#tab_1">2</ref>. Since no form of classification data were provided for this task, class filtering methods were not tested. In Table <ref type="table" coords="5,134.77,254.75,3.87,8.74" target="#tab_4">5</ref>, we list the corresponding results obtained from the Average user ground truth set. The results for other types of users (non-expert, expert etc), were similar. In our approach, images were represented as structured units, consisting of several fields. We used Lucene's API in order to index and store each image as a set of fields alongside with boosting the fields of each image when submitting a query. This technique helped in experimenting with different weights and combinations of these fields.</p><p>For every image in the given database we stored five features: Title, Caption, Abstract, MeSH and Textual References. MeSH terms related with each article provide extra information for the contained figures. MeSH terms were downloaded from the Pubmed ID of the article. Finally, we extracted every sentence inside the article that refers to an image, and used this set of sentences as a consistent field named Textual References.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experiments and Results</head><p>Details on the ad-hoc textual image retrieval task are given in <ref type="bibr" coords="6,415.25,236.11,9.96,8.74" target="#b4">[5]</ref>. Our experiments were based on previous participations <ref type="bibr" coords="6,336.11,248.06,10.52,8.74" target="#b3">[4]</ref> of the Information Processing Laboratory.</p><p>To achieve even higher MAP values than our 2012 runs, we carried out several experiments with different boosting factors, using as a train set the qrels from ImageClef 2012.</p><p>Motivated to achieve better results, we experimented in field selection, which revealed that the use of the Title along with Caption provides a strong combination. Moreover, a heuristic was applied to find the best boosting factors per field. Experiments with the best MAP values for the CLEF-2012 database are presented in Table <ref type="table" coords="6,221.19,356.97,3.87,8.74" target="#tab_5">6</ref>, where T, C, A, M and TR are the boosting factors for Title, Caption, Abstract, MeshTerms, Textual References respectively. In addition, TC is a joint combination of Title and Caption in one field. The use of this field without any boosting factor was placed second in this year's ad-hoc textual retrieval task. These runs were our submissions to the textual ad-hoc image-based retrieval task. In r4 and r5 (Table <ref type="table" coords="7,247.32,131.95,4.43,8.74" target="#tab_5">6</ref>) we have kept the boosting factors from our former participation at ImageClef 2012. In Table <ref type="table" coords="7,318.79,143.90,4.98,8.74" target="#tab_6">7</ref> we present the final results of these eight submissions of the IPL Group. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiments Settings</head><p>All our experiments were run using various combinations of the seven types of low-level visual features presented in Section 2.1 and of the textual data described in Section 3, with and without early data fusion and/or LSA applied. We employed the SVM light <ref type="bibr" coords="7,258.50,446.18,11.15,8.74" target="#b8">[9]</ref>[10] implementation of Support Vector Machines (SVMs) and Transductive Support Vector Machines (TSVMs) to perform multiclass classification, using a one-against-all voting scheme. It should be noted here that with the term multi-class we refer to problems in which any instance is assigned exactly one class label. In our experiments, following the one-against-all method, k binary SVM/TSVM classifiers (where k is the number of classes) were trained to separate one class from the rest. The classifiers were then combined by comparing their decision values on a test data instance and labeling it according to the classifier with the highest decision value. No parameter tuning was performed. A binary classifier was constructed for each dataset, a linear kernel was used and the weight C of the slack variables was set to default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Details on the modality classification task are given in <ref type="bibr" coords="7,369.91,621.25,9.96,8.74" target="#b4">[5]</ref>. In Table <ref type="table" coords="7,424.65,621.25,3.87,8.74" target="#tab_7">8</ref>, we present the results of the above experiments for the various types of classification, i.e. for textual, visual, and mixed classification. As a measure of classification performance we used accuracy.</p><p>As expected, mixed classification, on both visual and textual features, yielded the best performance in all cases compared to visual or textual only classification, scoring a 71.42% accuracy when applying SVMs on a combination of textual features (Title and Caption in one field with no boosting factor (TC)) with CORR,CL,CEDD, and CH visual descriptors. LSA was tested for different values of the k largest eigenvalues <ref type="bibr" coords="8,292.40,179.77,16.60,8.74">(50,</ref><ref type="bibr" coords="8,313.49,179.77,17.71,8.74">100,</ref><ref type="bibr" coords="8,335.67,179.77,17.71,8.74">150,</ref><ref type="bibr" coords="8,357.86,179.77,17.27,8.74">200)</ref>. The best results were accomplished for k = 200, for some descriptors it was almost equal to SVMs applied on the whole dataset.</p><p>Textual classification with SVMs succeeds a 65.29% accuracy score. When LSA is applied on the dataset for k = 150, it gives competitive results compared to the original vectors. It should be reminded that the original vectors have ≈ 147.000 features.</p><p>For classification on visual features only, the CEDD descriptor with SVMs has the best performance against the other descriptors with 61.19% accuracy score. When more than one low level feature descriptors are combined with early fusion into one fused vector, SVMs perform better in all cases. LSA was tested for different values of the k largest eigenvalues (50, 100, 150). The best results were accomplished for k = 150 and it should be noted that they highly approximate those of SVM when applied on the original feature vectors. The runs that were submitted to the modality classification task were based on the experiments presented above, but other methods were also tested. A de-scription of the runs is given in Table <ref type="table" coords="9,297.66,119.99,3.87,8.74" target="#tab_8">9</ref>. It should be noted that the textual data used in the runs contained only terms with document frequency larger than 1000. In this case, the dimensionality of the textual dataset is dramatically reduced to ≈ 10.000 features, drastically less than the ≈ 147.000 features of the textual dataset used in the former experiments. Also, apart from using TSVMs for classification, we also experimented on using class-centroid-based classification <ref type="bibr" coords="9,462.33,179.77,14.61,8.74" target="#b10">[11]</ref>. This method had the advantage of short training and testing time due to its computational efficiency. However, the accuracy of the centroid-based classifiers was inferior. We speculate that the centroids found during construction were far from perfect locations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Conclusions</head><p>We have presented an approach to LSA for CBIR replacing the SVD analysis of the feature matrix C (m × n) by the solution of the eigenproblem for the matrix CC T (m × m). The method overcomes the high cost of SVD in terms of memory and computing time. More work on stability issues is currently underway. Moreover, some cases of the usage of modality class vectors in early fusion techniques, have shown that can further improve retrieval results. Additional work in this direction is also in progress, by systematically testing more advanced classifiers and different low-level features.</p><p>Also, the inclusion of textual information, extracted from the meta-data provided, is also investigated. Specifically, the number of the extracted textual terms (≈ 147, 000) is far greater in comparison to the size of visual features. Hence, increased memory requirements and complexity is introduced. This problem is open for future research on several solutions like term selection or the use of an ontology in order to extract semantic keywords that strongly define a document.</p><p>For the modality classification task, mixed classification, both visual and textual features, yielded the best performance in all cases compared to visual or textual only classification. The application of SVMs for image classification had a positive impact, verifying previous findings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,239.06,307.19,137.24,7.89;3,145.68,117.55,323.98,174.87"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Feature extraction process</figDesc><graphic coords="3,145.68,117.55,323.98,174.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,134.77,521.73,350.54,114.99"><head>Table 1 .</head><label>1</label><figDesc>Visual descriptors and their corresponding vector size before and after feature selection.</figDesc><table coords="3,136.16,551.72,349.14,85.00"><row><cell>Visual descriptor</cell><cell cols="3">Size per block Size per image Final size per image</cell></row><row><cell>Gray Color Histogram (CH)</cell><cell>128</cell><cell>1152</cell><cell>100</cell></row><row><cell>Color Layout (CL)</cell><cell>120</cell><cell>1080</cell><cell>300</cell></row><row><cell>Scalable Color (SC)</cell><cell>64</cell><cell>576</cell><cell>200</cell></row><row><cell>Edge Histogram (EH)</cell><cell>80</cell><cell>720</cell><cell>300</cell></row><row><cell>CEDD</cell><cell>144</cell><cell>1296</cell><cell>400</cell></row><row><cell>FCTH</cell><cell>192</cell><cell>1728</cell><cell>300</cell></row><row><cell>Color Correlogram (CORR)</cell><cell>1024</cell><cell>9216</cell><cell>600</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,195.53,300.59,224.30,72.90"><head>Table 2 .</head><label>2</label><figDesc>Visual runs with combined image descriptors.</figDesc><table coords="4,208.29,321.36,195.72,52.12"><row><cell>Run ID</cell><cell>Combined descriptors</cell></row><row><cell>IPL13 visual r1</cell><cell>CORR,CL,CEDD</cell></row><row><cell>IPL13 visual r2</cell><cell>CORR, CL, CH</cell></row><row><cell>IPL13 visual r3</cell><cell>CORR, CL, CEDD, CH</cell></row><row><cell>IPL13 visual r4</cell><cell>CORR, CEDD, FCTH, CH</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,145.82,116.91,340.35,72.90"><head>Table 3 .</head><label>3</label><figDesc>Visual runs with class filtering and the classifier used.</figDesc><table coords="5,145.82,137.68,340.35,52.12"><row><cell>Run ID</cell><cell>Combined descriptors</cell><cell>Classifier</cell></row><row><cell>IPL13 visual r5</cell><cell>CORR,CL,CEDD,CH</cell><cell>Centroids CEDD</cell></row><row><cell>IPL13 visual r6</cell><cell>CORR,CL,CEDD,CH</cell><cell>Centroids SVD CEDD</cell></row><row><cell>IPL13 visual r7</cell><cell>CORR,CL,CEDD,CH</cell><cell>Improved Centroids SVD CEDD</cell></row><row><cell>IPL13 visual r8</cell><cell>CORR,CL,CEDD,CH</cell><cell>Centroids CEDD &amp; CH</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,146.19,299.06,322.98,116.73"><head>Table 4 .</head><label>4</label><figDesc>IPL's performance results from medical task's visual retrieval.</figDesc><table coords="5,146.19,319.83,322.98,95.96"><row><cell>Run ID</cell><cell>MAP</cell><cell>GM-MAP</cell><cell>bpref</cell><cell>p10</cell><cell>p30</cell></row><row><cell>IPL13 visual r1</cell><cell>0.0083</cell><cell>0.0002</cell><cell>0.0176</cell><cell>0.0314</cell><cell>0.0257</cell></row><row><cell>IPL13 visual r2</cell><cell>0.0071</cell><cell>0.0001</cell><cell>0.0162</cell><cell>0.0257</cell><cell>0.0257</cell></row><row><cell>IPL13 visual r3</cell><cell>0.0087</cell><cell>0.0003</cell><cell>0.0173</cell><cell>0.0286</cell><cell>0.0257</cell></row><row><cell>IPL13 visual r4</cell><cell>0.0081</cell><cell>0.0002</cell><cell>0.0182</cell><cell>0.0400</cell><cell>0.0305</cell></row><row><cell>IPL13 visual r5</cell><cell>0.0085</cell><cell>0.0003</cell><cell>0.0178</cell><cell>0.0314</cell><cell>0.0257</cell></row><row><cell>IPL13 visual r6</cell><cell>0.0119</cell><cell>0.0003</cell><cell>0.0229</cell><cell>0.0371</cell><cell>0.0286</cell></row><row><cell>IPL13 visual r7</cell><cell>0.0079</cell><cell>0.0003</cell><cell>0.0175</cell><cell>0.0257</cell><cell>0.0267</cell></row><row><cell>IPL13 visual r8</cell><cell>0.0086</cell><cell>0.0003</cell><cell>0.0173</cell><cell>0.0286</cell><cell>0.0257</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,134.77,463.62,336.39,130.13"><head>Table 5 .</head><label>5</label><figDesc>IPL's performance results from photo retrieval task for Average user.</figDesc><table coords="5,134.77,484.39,336.39,109.36"><row><cell>Run ID</cell><cell>MAP</cell><cell>p5</cell><cell>p10</cell><cell>p20</cell><cell>p30</cell><cell>p100</cell></row><row><cell>IPL13 visual r1</cell><cell>0.1118</cell><cell>0.6594</cell><cell>0.5152</cell><cell>0.4125</cell><cell>0.3725</cell><cell>0.3077</cell></row><row><cell>IPL13 visual r2</cell><cell>0.1082</cell><cell>0.6303</cell><cell>0.4955</cell><cell>0.3899</cell><cell>0.3499</cell><cell>0.2910</cell></row><row><cell>IPL13 visual r3</cell><cell>0.0771</cell><cell>0.5769</cell><cell>0.4141</cell><cell>0.3138</cell><cell>0.2741</cell><cell>0.2226</cell></row><row><cell>IPL13 visual r4</cell><cell>0.1162</cell><cell>0.6627</cell><cell>0.5152</cell><cell>0.4173</cell><cell>0.3713</cell><cell>0.3126</cell></row><row><cell cols="5">3 Textual-based Ad-hoc Image Retrieval</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,147.67,435.65,322.71,226.32"><head>Table 6 .</head><label>6</label><figDesc>Experimental results in ImageClef 2012 queries.</figDesc><table coords="6,147.67,456.42,322.71,205.55"><row><cell>Run</cell><cell cols="2">Fields weight</cell><cell>MAP</cell><cell>GM-</cell><cell>bpref</cell><cell>p10</cell><cell>p30</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>MAP</cell><cell></cell><cell></cell><cell></cell></row><row><cell>r1</cell><cell cols="2">T=0.65 A=0.57</cell><cell>0.2051</cell><cell>0.0763</cell><cell>0.2071</cell><cell>0.3227</cell><cell>0.2061</cell></row><row><cell></cell><cell cols="2">C=3.50 M=0.57</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>r2</cell><cell cols="2">T=0.625 A=0.57</cell><cell>0.2051</cell><cell>0.0762</cell><cell>0.2071</cell><cell>0.3227</cell><cell>0.2076</cell></row><row><cell></cell><cell cols="2">C=3.50 M=0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>r3</cell><cell>T=0.625</cell><cell></cell><cell>0.2050</cell><cell>0.0757</cell><cell>0.2061</cell><cell>0.3227</cell><cell>0.2045</cell></row><row><cell></cell><cell cols="2">A=0.555 C=3.50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>M=0.555</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>r4</cell><cell cols="2">T=0.1 A=0.113</cell><cell>0.2016</cell><cell>0.0765</cell><cell>0.1991</cell><cell>0.2955</cell><cell>0.2091</cell></row><row><cell></cell><cell cols="2">C=0.335 M=0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>r5</cell><cell cols="2">T=1 A=1 C=6</cell><cell>0.2021</cell><cell>0.0729</cell><cell>0.2003</cell><cell>0.3182</cell><cell>0.2076</cell></row><row><cell></cell><cell>M=0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>r6</cell><cell cols="2">TC (no boosting</cell><cell>0.2177</cell><cell>0.0848</cell><cell>0.2322</cell><cell>0.3500</cell><cell>0.2045</cell></row><row><cell></cell><cell>factor)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>r7</cell><cell>T=0.3</cell><cell>A=0.79</cell><cell>0.2106</cell><cell>0.0797</cell><cell>0.2047</cell><cell>0.3227</cell><cell>0.2182</cell></row><row><cell></cell><cell cols="2">C=3.50 M=0.73</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>TR=0.11</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>r8</cell><cell cols="2">TC=0.26 A=0.02</cell><cell>0.2215</cell><cell>0.0824</cell><cell>0.2397</cell><cell>0.3273</cell><cell>0.2136</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="7,134.77,188.49,339.11,178.61"><head>Table 7 .</head><label>7</label><figDesc>IPL's performance results from textual retrieval.</figDesc><table coords="7,134.77,209.26,339.11,157.84"><row><cell>Run ID</cell><cell>MAP</cell><cell>GM-MAP</cell><cell>bpref</cell><cell>p10</cell><cell>p30</cell></row><row><cell>IPL13 textual r1</cell><cell>0.2355</cell><cell>0.0583</cell><cell>0.2307</cell><cell>0.2771</cell><cell>0.2095</cell></row><row><cell>IPL13 textual r2</cell><cell>0.2350</cell><cell>0.0583</cell><cell>0.229</cell><cell>0.2771</cell><cell>0.2105</cell></row><row><cell>IPL13 textual r3</cell><cell>0.2354</cell><cell>0.0604</cell><cell>0.2294</cell><cell>0.2771</cell><cell>0.2124</cell></row><row><cell>IPL13 textual r4</cell><cell>0.2400</cell><cell>0.0607</cell><cell>0.2373</cell><cell>0.2857</cell><cell>0.2143</cell></row><row><cell>IPL13 textual r5</cell><cell>0.2266</cell><cell>0.0431</cell><cell>0.2285</cell><cell>0.2743</cell><cell>0.2086</cell></row><row><cell>IPL13 textual r6</cell><cell>0.2542</cell><cell>0.0422</cell><cell>0.2479</cell><cell>0.3314</cell><cell>0.2333</cell></row><row><cell>IPL13 textual r7</cell><cell>0.2355</cell><cell>0.0579</cell><cell>0.2358</cell><cell>0.2800</cell><cell>0.2171</cell></row><row><cell>IPL13 textual r8</cell><cell>0.2355</cell><cell>0.0579</cell><cell>0.2358</cell><cell>0.2800</cell><cell>0.2171</cell></row><row><cell cols="3">4 Modality Classification</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="8,163.44,367.51,288.48,249.19"><head>Table 8 .</head><label>8</label><figDesc>Classification performance on visual, textual and mixed data.</figDesc><table coords="8,171.24,388.28,260.66,228.41"><row><cell>Classification Type</cell><cell>Features</cell><cell>SVM</cell><cell>LSA, SVM</cell></row><row><cell>Textual</cell><cell>TC</cell><cell>65.29%</cell><cell>64.60%</cell></row><row><cell>Visual</cell><cell>CORR</cell><cell>48.53%</cell><cell>46.44%</cell></row><row><cell></cell><cell>CL</cell><cell>47.95%</cell><cell>45.39%</cell></row><row><cell></cell><cell>CEDD</cell><cell>61.19%</cell><cell>60.81%</cell></row><row><cell></cell><cell>FCTH</cell><cell>59.60%</cell><cell>55.58%</cell></row><row><cell></cell><cell>CH</cell><cell>41.67%</cell><cell>41.32%</cell></row><row><cell></cell><cell>CORR,CEDD,FCTH</cell><cell>62.94%</cell><cell>61.08%</cell></row><row><cell></cell><cell>CORR,CEDD,CH</cell><cell>62.94%</cell><cell>60.42%</cell></row><row><cell></cell><cell>CORR,CL,CEDD</cell><cell>61.74%</cell><cell>61.31%</cell></row><row><cell></cell><cell>CORR,CL,CEDD,CH</cell><cell>63.67%</cell><cell>61.85%</cell></row><row><cell>Mixed</cell><cell>TC,CORR</cell><cell>68.36%</cell><cell>65.33%</cell></row><row><cell></cell><cell>TC,CL</cell><cell>67.43%</cell><cell>62.47%</cell></row><row><cell></cell><cell>TC,CEDD</cell><cell>69.25%</cell><cell>65.37%</cell></row><row><cell></cell><cell>TC,FCTH</cell><cell>69.13%</cell><cell>66.22%</cell></row><row><cell></cell><cell>TC,CH</cell><cell>66.62%</cell><cell>66.11%</cell></row><row><cell></cell><cell>TC,CORR,CEDD,FCTH</cell><cell>70.95%</cell><cell>66.46%</cell></row><row><cell></cell><cell>TC,CORR,CEDD,CH</cell><cell>70.29%</cell><cell>67.19%</cell></row><row><cell></cell><cell>TC,CORR,CL,CEDD</cell><cell>71.11%</cell><cell>64.52%</cell></row><row><cell></cell><cell cols="2">TC,CORR,CL,CEDD,CH 71.42%</cell><cell>65.10%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="9,136.16,260.85,356.94,259.20"><head>Table 9 .</head><label>9</label><figDesc>IPL's performance results from modality classification.</figDesc><table coords="9,136.16,281.62,356.94,238.43"><row><cell>Run id</cell><cell>Classification Type</cell><cell>Description</cell><cell>Accuracy score</cell></row><row><cell></cell><cell></cell><cell>1. Early fusion: on CEDD, CH, and</cell><cell></cell></row><row><cell></cell><cell></cell><cell>FCTH descriptors and textual data.</cell><cell></cell></row><row><cell>IPL13 mod cl mixed r1</cell><cell>Mixed</cell><cell>2. LSA applied on the fused vectors</cell><cell>9.56%</cell></row><row><cell></cell><cell></cell><cell>with k=50 3. Classify with class</cell><cell></cell></row><row><cell></cell><cell></cell><cell>centroids.</cell><cell></cell></row><row><cell></cell><cell></cell><cell>1. Early fusion: on CEDD, CH, and</cell><cell></cell></row><row><cell>IPL13 mod cl mixed r2</cell><cell>Mixed</cell><cell>FCTH descriptors and textual data.</cell><cell>61.03%</cell></row><row><cell></cell><cell></cell><cell>2. Classify using TSVMs.</cell><cell></cell></row><row><cell></cell><cell></cell><cell>1. Early fusion: on CEDD descrip-</cell><cell></cell></row><row><cell>IPL13 mod cl mixed r3</cell><cell>Mixed</cell><cell>tor and textual data. 2. Classify us-</cell><cell>58.98%</cell></row><row><cell></cell><cell></cell><cell>ing TSVMs.</cell><cell></cell></row><row><cell></cell><cell></cell><cell>1. LSA applied on a combination of</cell><cell></cell></row><row><cell>IPL13 mod cl visual r1</cell><cell>Visual</cell><cell>CEDD, CH, and FCTH descriptors with k=50 2. Classify with class</cell><cell>6.19%</cell></row><row><cell></cell><cell></cell><cell>centroids.</cell><cell></cell></row><row><cell>IPL13 mod cl visual r2</cell><cell>Visual</cell><cell>1. Classify using TSVMs on CEDD descriptor</cell><cell>52.05%</cell></row><row><cell></cell><cell></cell><cell>1. LSA applied on textual data with</cell><cell></cell></row><row><cell>IPL13 mod cl textual r1</cell><cell>Textual</cell><cell>k=50 2. Classify with class cen-</cell><cell>9.02%</cell></row><row><cell></cell><cell></cell><cell>troids.</cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,142.96,306.97,337.64,7.86;10,151.52,317.90,268.15,7.89" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,151.52,317.93,145.60,7.86">Indexing by latent semantic analysis</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">C</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,305.01,317.93,25.21,7.86">JASIS</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,328.89,337.63,7.86;10,151.52,339.85,329.07,7.86;10,151.52,350.81,329.07,7.86;10,151.52,361.77,287.51,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,305.08,328.89,175.51,7.86;10,151.52,339.85,58.73,7.86">An svdbypass latent semantic analysis for image retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Stathopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kalamboukis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,449.10,339.85,31.49,7.86;10,151.52,350.81,222.02,7.86">Medical Content-Based Retrieval for Clinical Decision Support</title>
		<title level="s" coord="10,450.23,350.81,30.36,7.86;10,151.52,361.77,107.98,7.86">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Muller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Syeda-Mahmood</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">7723</biblScope>
			<biblScope unit="page" from="122" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,372.73,337.63,7.86;10,151.52,383.68,79.68,7.86" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">F</forename><surname>Van Loan</surname></persName>
		</author>
		<title level="m" coord="10,275.30,372.73,85.24,7.86">Matrix computations</title>
		<imprint>
			<publisher>Johns Hopkins University Press</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
	<note>3. ed.</note>
</biblStruct>

<biblStruct coords="10,142.96,394.64,337.63,7.86;10,151.52,405.60,329.07,7.86;10,151.52,416.56,126.25,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,349.87,394.64,130.72,7.86;10,151.52,405.60,15.40,7.86">Ipl at clef 2012 medical retrieval task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Stathopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Sakiotis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kalamboukis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,386.33,405.60,94.26,7.86;10,151.52,416.56,88.28,7.86">CLEF (Online Working Notes/Labs/Workshop</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Karlgren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Womser-Hacker</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,427.52,337.63,7.86;10,151.52,438.48,329.07,7.86;10,151.52,449.44,93.18,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,151.52,438.48,183.77,7.86">Overview of the imageclef 2013 medical tasks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,358.68,438.48,117.72,7.86">Working notes of CLEF 2013</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,460.40,337.64,7.86;10,151.52,471.36,193.81,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,278.88,460.40,201.71,7.86;10,151.52,471.36,25.12,7.86">Lire: lucene image retrieval: an extensible java cbir library</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,198.40,471.36,69.27,7.86">ACM Multimedia</title>
		<imprint>
			<biblScope unit="page" from="1085" to="1088" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,482.31,337.64,7.86;10,151.52,493.27,329.07,7.86;10,151.52,504.23,329.07,7.86;10,151.52,515.19,181.67,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,453.20,493.27,27.39,7.86;10,151.52,504.23,219.37,7.86">Imageclef 2013: the vision, the data and the open challenges</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zellhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Martinez Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Garcia Varea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cazorla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,393.81,504.23,86.78,7.86;10,151.52,515.19,16.79,7.86">Proceedings of CLEF 2013</title>
		<meeting>CLEF 2013<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Springer LNCS</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,526.15,337.63,7.86;10,151.52,537.11,216.76,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,207.67,526.15,254.20,7.86">Overview of the imageclef 2013 personal photo retrieval subtask</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zellhöfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,151.52,537.11,116.32,7.86">Working notes of CLEF 2013</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,548.07,337.64,7.86;10,151.52,559.03,88.07,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="10,211.02,548.07,234.11,7.86">Learning to Classify Text Using Support Vector Machines</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Kluwer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,569.99,337.98,7.86;10,151.52,580.94,329.07,7.86;10,151.52,591.90,140.29,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,212.34,569.99,268.26,7.86;10,151.52,580.94,34.86,7.86">Transductive inference for text classification using support vector machines</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,208.68,580.94,206.74,7.86">16th International Conference on Machine Learning</title>
		<meeting><address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="200" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,602.86,337.97,7.86;10,151.52,613.82,329.07,7.86;10,151.52,624.78,244.78,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,267.83,602.86,212.75,7.86;10,151.52,613.82,80.65,7.86">Centroid-based document classification: Analysis &amp; experimental results</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">H S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,255.44,613.82,225.15,7.86;10,151.52,624.78,175.46,7.86">4th European Conference on Principles and Practice of Knowledge Discovery in Databases (PKDD)</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="424" to="431" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
