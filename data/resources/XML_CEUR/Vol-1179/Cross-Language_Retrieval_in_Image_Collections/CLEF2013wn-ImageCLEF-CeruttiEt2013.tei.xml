<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,153.38,116.95,308.59,12.62;1,207.85,134.89,193.44,12.62">Late Information Fusion for Multi-modality Plant Species Identification</title>
				<funder ref="#_Jrc2CVy">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,142.21,172.56,79.80,8.74"><forename type="first">Guillaume</forename><surname>Cerutti</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université de Lyon</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Université Lyon 2</orgName>
								<orgName type="institution" key="instit2">LIRIS</orgName>
								<address>
									<postCode>UMR5205, F-69676</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,238.90,172.56,60.22,8.74"><forename type="first">Laure</forename><surname>Tougne</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université de Lyon</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Université Lyon 2</orgName>
								<orgName type="institution" key="instit2">LIRIS</orgName>
								<address>
									<postCode>UMR5205, F-69676</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,316.01,172.56,54.80,8.74"><forename type="first">Céline</forename><surname>Sacca</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université de Lyon</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Université de Saint Etienne</orgName>
								<orgName type="institution" key="instit2">EVS</orgName>
								<address>
									<addrLine>UMR5600</addrLine>
									<postCode>F-42000</postCode>
									<settlement>Saint Etienne</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,387.70,172.56,71.87,8.74"><forename type="first">Thierry</forename><surname>Joliveau</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université de Lyon</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Université de Saint Etienne</orgName>
								<orgName type="institution" key="instit2">EVS</orgName>
								<address>
									<addrLine>UMR5600</addrLine>
									<postCode>F-42000</postCode>
									<settlement>Saint Etienne</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,172.80,184.51,98.74,8.74"><forename type="first">Pierre-Olivier</forename><surname>Mazagol</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université de Lyon</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Université de Saint Etienne</orgName>
								<orgName type="institution" key="instit2">EVS</orgName>
								<address>
									<addrLine>UMR5600</addrLine>
									<postCode>F-42000</postCode>
									<settlement>Saint Etienne</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,288.43,184.51,61.88,8.74"><forename type="first">Didier</forename><surname>Coquin</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">LISTIC</orgName>
								<address>
									<addrLine>Domaine Universitaire</addrLine>
									<postCode>F-74944</postCode>
									<settlement>Annecy le Vieux</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,360.87,184.51,77.21,8.74"><forename type="first">Antoine</forename><surname>Vacavant</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">Clermont Université</orgName>
								<orgName type="institution" key="instit2">Université d&apos;Auvergne</orgName>
								<orgName type="institution" key="instit3">ISIT</orgName>
								<address>
									<postCode>F-63001</postCode>
									<settlement>Clermont-Ferrand</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,153.38,116.95,308.59,12.62;1,207.85,134.89,193.44,12.62">Late Information Fusion for Multi-modality Plant Species Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">39FE3B4044208A71993055B800BD59A9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>plant identification</term>
					<term>confidence fusion</term>
					<term>biogeographical information</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article presents the participation of the ReVeS project to the ImageCLEF 2013 Plant Identification challenge. Our primary target being tree leaves, some extra effort had to be done this year to process images containing other plant organs. The proposed method tries to benefit from the presence of multiple sources of information for a same individual through the introduction of a late fusion system based on the decisions of classifiers for the different modalities. It also presents a way to incorporate the geographical information in the determination of the species by estimating their plausibility at the considered location. While maintaining its performance on leaf images (ranking 3rd on natural images and 4th on plain backgrounds) our team performed honorably on the brand new modalities with a 6th position.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Considering the identification of a plant species, the study of its leaves is a common path, and undoubtedly the most fitting for an automatic system, given the relative visual stability of the planar, slowly growing objects that are leaves. However, they are not the organs on which was built the classification of the Plantae kingdom, which considers the temporal proximity of the species as a baseline grouping criterion. Flowers and fruits, representative of the plant's intrinsic reproduction properties are the most evident witnesses of this proximity, and have been used since the earliest classification works <ref type="bibr" coords="1,343.01,555.84,15.50,8.74" target="#b11">[12]</ref> as key features defining the species. Nevertheless, they constitute much touchier objects for automatic recognition, given their 3-dimensional complex structure, their high variability and short life cycles.</p><p>The main objective of the ReVeS project is to build a tree species identification system based on an explicit botanical description of leaf images. Unlike for the previous editions, the challenge laid by this year's identification task <ref type="bibr" coords="1,451.17,627.57,10.52,8.74" target="#b6">[7]</ref> was quite distant from this initial perspective, and required us to address different objects that are not suitable for the same analytic shape processing. The literature still provides numerous examples of methods dealing with flower <ref type="bibr" coords="1,449.60,663.44,15.50,8.74" target="#b12">[13,</ref><ref type="bibr" coords="1,465.09,663.44,11.62,8.74" target="#b15">16]</ref>, bark <ref type="bibr" coords="1,158.18,675.39,15.50,8.74" target="#b18">[19,</ref><ref type="bibr" coords="1,173.68,675.39,7.75,8.74" target="#b8">9,</ref><ref type="bibr" coords="1,181.42,675.39,7.75,8.74" target="#b5">6]</ref> and even fruit <ref type="bibr" coords="1,258.66,675.39,15.50,8.74" target="#b14">[15]</ref> images, using generally statistical descriptions and rarely explicit modelling. Considering that most of the 5092 test images could be grouped by individual, producing sets of images of different organs for the same plant, our work focused especially on a strategy to combine those various sources of information, and on a way to make use of the GPS location provided to enhance the performance of the classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Image Processing and Feature Extraction</head><p>This year's database approximately doubled in size compared to last year, and it was separated into 2 big categories :</p><p>-Sheet As Background images, containing only leaves, and thus combining the categories Scan and Pseudoscan of previous editions <ref type="bibr" coords="2,427.31,259.11,9.96,8.74" target="#b7">[8]</ref>, with the difference of containing some images that have previously fallen in the Photograph category, photographs of leaves on a plain but sometimes dark background. -Natural Background images, being photographs of plant organs (or entire plants) in their natural environment. Only one type of content (Leaf, Flower, Fruit, Stem or Entire) is given for each image, and no distinction is made between images containing only one whole organ, part of an organ, several organs more or less in focus, or even different types of organs.</p><p>Among the 20985 images forming the Train database, we could find 250 plant species, but a distinction can be made between tree species (127 generally ligneous, taller than 1.2 m plant species) that were the ones considered in the previous editions and the ones represented in the SheetAsBackground category, and the 123 rather herbaceous remaining species. This distinction is important for the identification, the tree species presenting for example bark in the Stem images, that may be analyzed in a specific way, more accurately than a generic stem image.</p><p>The Test database contained 5092 images with the same information on the content, but with no indication whatsoever whether the species was a tree or not, which forced us to make assumptions and selections for the good running of our methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Manual Image Formatting</head><p>Before any actual processing, we had to perform some transformations on the Natural Backgroung images to convert them into suitable entry points for our algorithm. As mentioned previously, our objective is to develop a mobile application for identification, which allows a certain part of interaction with the user, by giving guidelines for the picture taking or directly asking for help in the image processing phase though the tactile interface. The procedure we applied on the images simply simulates what we would have asked of the supposed user of the application.</p><p>Most of the Leaf, Fruit and Flower images contained more than one example of the desired object. A first selection was done by placing all the images for which no object big and in focus enough could be found in the Entire category, and keeping the others. We subsequently cut those images to a region of interest containing only one object, and rotated them (90 or 180 rotations only) so that the resulting images correspond to what input a user asked to take a photograph In addition to this first manipulation, we drew on top of each of the obtained images a rough mark inside the object that is used as an initialization of our segmentation algorithms. Note that in the case of leaves where the methods are the most developed, a distinction is made between simple and compound leaves : in the first case, one single mark is required whereas we require three distinct marks in three of the top leaflets for second type of leaves. An example of such quick colouring, as it would be done with the finger on a mobile device, is shown in Figure <ref type="figure" coords="3,177.83,377.71,19.51,8.74" target="#fig_0">1 (c)</ref>.</p><p>For Stem images, where no segmentation process was considered, we simply rotated and cut the images to obtain a vertical view containing only bark pixels. The images clearly corresponding to non-tree species were discarded to the Entire category, as well as the non-tree leaves and fruits. Unfortunately, we did not have the time to do anything with the images of this extended category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Leaf Understanding</head><p>Leaf images were processed with almost the same method as last year <ref type="bibr" coords="3,447.05,499.09,10.52,8.74" target="#b1">[2]</ref> with the idea of introducing some modelling of the leaf. This is a purpose that has proven to work quite efficiently <ref type="bibr" coords="3,277.75,523.00,11.15,8.74" target="#b0">[1,</ref><ref type="bibr" coords="3,288.90,523.00,11.15,8.74" target="#b19">20]</ref>. The goal here is to represent faithfully the botanical properties of the leaf, in an attempt to model the analytic determination process used by botanists. For the recall, a leaf shape model is used as both a pre-segmentation and a description of the global shape <ref type="bibr" coords="3,423.90,558.87,9.96,8.74" target="#b4">[5]</ref>, and after the segmentation by a constrained active contour, descriptors accounting for the margin <ref type="bibr" coords="3,168.55,582.78,9.96,8.74" target="#b2">[3]</ref>, base and apex shapes are computed. The main differences concern the processing of compound leaves in which we try to describe all the leaflets at the same time. This is achieved by estimating the number and location of the leaflets, and jointly modelling their global shape, using highly variable deformable templates <ref type="bibr" coords="4,323.37,155.86,9.96,8.74" target="#b3">[4]</ref>, and then segmenting each leaflet independently with multiple region-based active contours. Figure <ref type="figure" coords="4,428.57,167.81,4.98,8.74" target="#fig_1">2</ref> illustrates the running of this new process.</p><p>Another improvement lies in the representation of the margin that now uses a string-like structure to keep track of the spatial repartition of teeth along the leaf contour. When our former descriptors used aggregative methods (computation of histograms, or averaging of curvature-scale measures) this new description is more discriminant as it can represent different levels of dentition or different orderings of teeth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Fruit, Bark and Flower Image Analysis</head><p>The methods we used on the other organs were not nearly as dedicated and thought through as the ones elaborated for leaves, as they are not our main concern. We made the decision to rely on statistical shape descriptors and on vocabulary learning to produce color, texture and interest point based features. The only particularity came from the fact that in the case of Flower and Fruit images, a segmentation had to be performed first for these descriptors to be extracted correctly.</p><p>Fruits and flowers may be easier to isolate in an image given how their colors stand out, but they still require some advanced processing <ref type="bibr" coords="4,414.10,384.72,16.13,8.74" target="#b15">[16,</ref><ref type="bibr" coords="4,430.23,384.72,12.10,8.74" target="#b17">18]</ref> or some modelling <ref type="bibr" coords="4,180.32,396.68,15.50,8.74" target="#b12">[13]</ref> to be segmented. The segmentation algorithm we used is derived from the one implemented for leaves. However, given the impossibility to model efficiently the shapes of fruits and flowers in the images, it relies only on the color dissimilarity part of the method. Starting from the user-made colouring, the average expected color is propagated throughout the image, leading to a map of the dissimilarity of image pixels to a local color model for the organ <ref type="bibr" coords="4,467.30,456.45,9.96,8.74" target="#b1">[2]</ref>. This dissimilarity map is then simply thresholded to produce a segmentation of the desired object. Some results of this algorithm are shown in Figure <ref type="figure" coords="4,443.06,480.36,3.87,8.74" target="#fig_2">3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bagging features</head><p>To represent the color information, we used a bag-of-colors-like feature based on an organ-specific color vocabulary, much like what can be found in other works considering flowers <ref type="bibr" coords="5,249.07,155.86,14.61,8.74" target="#b12">[13]</ref>. This vocabulary was learned in a first phase by clustering the L*a*b* colors encountered in the considered (segmented) regions of all images into a predefined number of reference colors. Then the pixels for a given image are associated with one word of the color vocabulary, and the occurrences of each word are counted to form a histogram, later normalized by the size of the region. This generates fixed-sized, comparable, and even averageable features, that are displayed in Figure <ref type="figure" coords="5,298.91,227.59,3.87,8.74" target="#fig_3">4</ref>. Such color features were extracted only for Fruit and Flower images, with respective vocabulary sizes of 100 and 200 color words. The same idea was adopted for texture description keeping for each pixel in the region, instead of a 3-channel L*a*b* vector, a vector of 30 values corresponding to the responses obtained by filtering the image with the commonly employed Gabor wavelets <ref type="bibr" coords="5,253.02,491.79,10.52,8.74" target="#b8">[9]</ref> of 6 different orientations and 5 different sizes. Then again a vocabulary is learned based on these features and bags of textures used to describe the image. This was performed on Stem and Fruit images, with a vocabulary size of 500 for each.</p><p>Finally, we computed the classical bag of visual words by repeating the same procedure again, this time on SURF descriptors obtained on detected keypoints (limited to 500 per image, for computational reasons). The only difference is that the histograms are not normalized by the number of keypoints extracted on the image, otherwise it is in every respect similar. These bag of words were used on Stem images only, with a vocabulary of 1000 visual words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shape descriptors</head><p>To capture the shape properties of the segmented organs, we decided to rely on well established, robust shape descriptors, commonly used in the literature for object recognition purposes, namely :</p><p>-Centered moments (8 values) -Eccentricity (1 value) -Hu moments (7 values) -Zernike moments (9 values) These values were computed on the binary segmentation images resulting from the algorithm described above. Therefore, they were only extracted on Fruit and Flower images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Species Classification</head><p>We chose to view the problem of species identification as a classification problem rather than an image retrieval one, and subsequently trained classifiers to perform the recognition. Unfortunately, the time dedicated to this task was too short to reproduce the random forest classification that gave us the best results last year <ref type="bibr" coords="6,175.21,232.37,10.52,8.74" target="#b1">[2]</ref> and we had to rely on a more naive process. Furthermore, the content of the database somehow pushed us to reconsider the decision making, with the obvious interest of combining the different views and organs of the same plant individual to improve the accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Naive Distance-based Classification</head><p>For each set of the aforementioned descriptors, and for each considered species, we were able to model the distribution of the parameters by gaussians, evaluating the means and standard deviations from all the examples. This was also what we did for the histogram-like features that are maybe not the most suitable for this type of approach. The only difference concerns the strings describing leaf margins, non-vectorial structures that had to be addressed differently, simply by estimating an average string per species. The set of considered species, on which the training was performed and that are likely to appear in the classification results, was also different depending on the organ : To classify a new image, we simply evaluate the distances d o (x, S) of the descriptors extracted on it to the gaussian models of the retained species (S o ) for the corresponding organ o. The metric we used for histogram-like features is a simple L2 distance, for the string structures, a normalized Levenshtein, and for the vector features a so-called ellipsoid distance, designed to take variability into account, without distorting the parameter space as would the Mahalanobis distance. That distance represents the distance of a point x to the surface of the ellipsoid defined by the center µ and the axes of the covariance matrix Σ. Using the Mahalanobis distance x -µ M = (x -µ)Σ -1 (x -µ) T , the metric we compute can be written as :</p><formula xml:id="formula_0" coords="6,209.73,622.71,270.86,23.22">x -µ E = x -µ 2 • max 1 - 1 x -µ M , 0<label>(1)</label></formula><p>Each one of the N D sets of descriptors extracted on the image thus produces a distance, that is then normalized by the average distance to the correct class (learned for each feature during the training phase). Once those terms are on the same scale, they are just summed, and the set of all considered species are ordered according to this last measure. This way, for each image, we produce a ranked list of species along with a distance value measuring their similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Individual-based Information Fusion</head><p>This ranking could be enough to give a suitable answer per image, but it is much more interesting to try and make use of the different modalities available for a same individual plant. The images corresponding to a same value of the IndividualPlantID are actually different views of the same plant, taken at the exact same location. Consequently, the answer to each of these images needs to be the same, and every element that has to be taken into account to produce it with as much accuracy as possible.</p><p>To perform an efficient fusion of these various data sources, we used a weighted sum of confidence measures obtained from the distances to species issuing from the different classifiers. The fusion process is illustrated in Figure <ref type="figure" coords="7,431.56,246.54,3.87,8.74" target="#fig_5">5</ref>. An individual may be associated to a certain number of images (up to 24) of different organs. Each one having been processed independently by the according classifier, we retrieve for each of the n o images of the organ o the distances d o (1..n o ) to the different species considered in the context of this organ. The distances are first converted into confidence measures. Each distance value being a sum of N D,o terms expected to be equal to 1 for the correct species, we define the confidence score associated to the distance as following :</p><formula xml:id="formula_1" coords="7,207.18,581.35,273.41,41.63">C o (x, S) =      exp - d o (x, S) N D,o 2 if S ∈ (S o ) 0 if S / ∈ (S o )<label>(2)</label></formula><p>In the end, even if distances were computed for only a subset of all the considered species, we now have for each image a set of confidences for each and every species. The value is 0 for those that were not learned but they are still present in the possible answer list. These confidence values all lie between 0 and 1 and the normalization of the distance by n D,o makes sure that they constitute comparable measures even for different organs. However, the results we obtained on the different modalities tended to show that some are more reliable than others. With this in mind, the confidence measures are weighted by an organ-wise coefficient w o derived from the classification rates we could observe on the training base, in a common attempt of giving more importance to reliable sources of information <ref type="bibr" coords="8,281.64,155.86,14.60,8.74" target="#b16">[17]</ref>.</p><p>Finally, we used an additive process on these weighted confidences, in order not to penalize too much the species that were absent for a given organ. The sum of weighted confidences is then normalized again by the sum of weights to remain between 0 and 1. The final confidence score for a species S is then computed as :</p><formula xml:id="formula_2" coords="8,213.67,233.28,266.92,29.65">C(S) = o∈{L,f,F,B} ( no i=1 w o • C o (x i , S)) o∈{L,f,F,B} ( no i=1 w o )<label>(3)</label></formula><p>With these similarity measures, we can produce a new ranking of the species, this time all the 250 of them, that will be the same for all the images of the considered individual. The information available on each of the image is then taken into account, with a degree of trust depending on the modality, and the ordering that would have been obtained on one image only is then refined and hopefully improved by the content of the other associated images. Of course in the case when there is only one image per individual, the order remains unchanged, but when different organs are available, or different leaves of the same plant, the species appearing regularly as close will be put up front.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Introducing Biogeographical Knowledge</head><p>Geographical and environmental factors play a big part in the development (and survival) of plants, and not all species are likely to be found under every conditions. Knowing where a plant was observed, and what species may grow in that particular location might constitute a great help for identification. With this perspective, it was a core objective of our project to come up with methods to benefit from the location (easily accessible through GPS on mobile devices, for example) to enhance the identification by knowing in advance the plausibility of species.</p><p>In this respect, a large-scale work of digital cartography was undertaken (focusing on the French metropolitan territory) to map some environmental parameters that are related to the presence of species, with a resolution of 90 meters. Indeed, variables such as temperature, altitude, precipitation, humidity, exposition have a key influence on the repartition of species. Along with these raw explanatory variable, we also considered more global, integrative values elaborated by botanists and phytosociologists :</p><p>-Altitudinal zonation, or vegetation levels as defined in phytogeography <ref type="bibr" coords="8,462.32,596.12,14.61,8.74" target="#b10">[11]</ref>,</p><p>9 zones for different altitudes, temperatures and orientations (modelled using CNRS vegetation maps at scale 1:200,000)</p><p>-GRECO (great ecological regions) from the French forest institute <ref type="bibr" coords="8,448.59,637.79,14.61,8.74" target="#b9">[10]</ref>, 10 regions of homogeneous geological and environmental characteristics identified by letters</p><p>The images in the PlantView database were all referenced in terms of longitude and latitude, and most of them issued from the french territory, making them suitable to experiment this geographical approach. Unfortunately, there was a big incertitude about the precision of this location, whether it was the place where the image was actually acquired or simply uploaded. In the case of scanned leaves, is it the location of the tree or the scanner that is supplied? Therefore, trying to include local parameters like the raw measurements hardly made any sense, and we chose to take only levels and regions into account. The maps for those two variables can be seen in Figure <ref type="figure" coords="9,358.81,395.86,3.87,8.74" target="#fig_6">6</ref>.</p><p>To derive the plausibility of finding a species at a given location, we did not use the training base but included external botanical knowledge about the species. Based on the repartition knowledge and maps one can find in floras <ref type="bibr" coords="9,134.77,443.68,14.61,8.74" target="#b13">[14]</ref>, we associated for each region and each zone a presence value of the species. However, this task was only performed for native tree species, leaving almost half of the challenge scope empty of information. The presence levels were simply 0 or 1 for the vegetation zones but 0, 1, 2, 3, 4 and 5 (in increasing order of plausibility) for the ecological regions.</p><p>Then, for given values of the region index g and of the vegetation level l, it is possible to compute a numerical value of plausibility for each species given its theoretical repartition. Knowing the presence values of the species in the region p(S, g) and the level p(S, l) we simply derive an additional plausibility score :</p><formula xml:id="formula_3" coords="9,164.12,553.75,316.48,83.93">P (S, g, l) = 1 2                    1 if p(S, g) = 5 0.95 if p(S, g) = 4 0.9 if p(S, g) = 3 0.8 if p(S, g) = 2 0.7 if p(S, g) = 1 0.5 if p(S, g) = 0 + 1 2 1 if p(S, l) = 1 0.5 if p(S, l) = 0<label>(4)</label></formula><p>For visualisation purposes, we computed this plausibility for different species all over the French territory, for which the values of vegetation levels and ecological regions were mapped. The result can be seen as a repartition map of the species, in which the places where the species is most plausible appear in darker green, on those where it will certainly not be found in lighter green. Examples of such maps are shown in Figure <ref type="figure" coords="9,284.57,705.30,3.87,8.74" target="#fig_7">7</ref>. For the species on which we had no prior repartition knowledge, we ensured the geography would not have a big influence on classification by setting the P (S, l, g) to 1 for all values of l and g. This of course is not an optimal solution, and actually favours those species, since they will at least be as plausible as the other species, whatever the location.</p><p>The way we insert the geographical knowledge into the classification process is fairly simple. Given the coordinates (lon; lat) of the observation, we can extract from the mapped data the values g(lon, lat) and l(lon, lat) of the corresponding GRECO and level. Then for each species S the confidence score used to rank it simply becomes :</p><formula xml:id="formula_4" coords="10,198.81,394.05,281.78,9.73">C G (S, lon, lat) = P (S, g(lon, lat), l(lon, lat)) • C(S)<label>(5)</label></formula><p>The introduction of this geographical plausibility leaves unchanged the confidence score for the species that are perfectly adequate with the observation's location. However, it will penalize the species which are not, making them drop in the rankings and conversely letting the more plausible species make their way up to the first answers. It is a way to make a rearrangement of the top answers without actually overshadowing the information extracted from the image. It is in any case coherent with the idea that geography should not contradict the evidence found in the content of the image, but simply provide a lighting on the results of its analysis to make a better final decision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Comments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training Results</head><p>The classifications of the different plant organs do not perform all the same, the most effective one naturally being the classification of leaves on plain backgrounds. As a matter of fact, leaves are the organ for which most work has been dedicated, and the methods used to process them were the most extensively tested. It is then only natural that the experimental results makes them appear clearly as the best performing modality.</p><p>The Figure <ref type="figure" coords="10,200.55,645.53,4.98,8.74" target="#fig_8">8</ref> sums up the experimental classification rates obtained for the 4 modalities we addressed. Those scores were measured on the Train database, in a cross validation process (validation rates are displayed, with the rates obtained on the training data left in lighter color). If the Leaf category (126 classes, 4 descriptor sets) clearly benefits from the efforts put in the segmentation and the botany-inspired description (nearly 85% of presence of the correct species in the The Fruit category (44 classes, 3 descriptor sets) is the second best performing (more than 50% at 5 answers) but this performance has to be moderated by the fact the the number of classes is relatively small. On the other hand the Stem category (78 classes, 2 descriptor sets) performs honorably (more than 40% at 5 answers) given the number of species and the rawness of the description Same goes for the Flower modality (139 classes, 2 descriptors) for which it might be interesting to add that most of the accuracy (37% at 5 answers) actually comes from the simple color feature we used. All in all, these results underline the limits of the descriptions (hastily) implemented, and in the case of fruits and flowers the difficulties of the segmentation, given the low contribution of the shape description. It is also crucial to point out the greater abstraction power of the dedicated description created for leaves compared to the generic features used on the other organs ; this is visible in the blatant gap between the curves obtained on the training and validation sets. A very interesting point is to measure the contribution of geography in the performance of the classification. To do so, we considered only leaf images from tree species that are native in France, and on which we had the necessary theoretical geogaphical information. The Figure <ref type="figure" coords="12,335.98,155.86,4.98,8.74" target="#fig_9">9</ref> shows the comparison between a classification where the geographical plausibility of the species is taken into account and one where it is not. More than the actual rates we obtain, it is clearly appearing that geographical knowledge leads to a better classification, and simply improves the ranking of the correct species. The Figure <ref type="figure" coords="12,441.09,203.68,9.96,8.74" target="#fig_10">10</ref> shows this improvement by measuring the difference between the rank of the correct species in the classification without geographical information and with it, a difference that goes in the right direction for a major share of examples for which the ranking could still be improved. Even with unprecise location (obviously for scanned images) the region and level prove to give enough information to refine the decision in a more accurate way. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">ImageCLEF 2013 Results</head><p>We submitted two distinct runs this year, both relying on the same visual analysis process but differing in the fusion phase. They both make use of the In-didualPlantID field, but the second run (LirisReVeS run2) additionally used the longitude and latitude coordinates to compute the geographical plausibility of the species and use it to rearrange the results of the image processing phase.</p><p>Both runs were then mixed (visual + textual) runs, with an automatic method on the SheetAsBackground category and manual initalizing on the Natu-ralBackground category. To be exhaustive, the images remaining untreated, and not belonging to a processed individual were submitted as specimens of Ginkgo biloba, L. with an absolute confidence of 1.</p><p>As expected, the SheetAsBackground category, containing almost only leaves, is the one on which our leaf-focused algorithms gave the most satisfying results. As shown in Figure <ref type="figure" coords="12,222.48,669.07,8.49,8.74" target="#fig_11">11</ref>, we reach a score of 41% bringing us in 4th place among the 12 teams that have submitted runs.</p><p>Concerning the NaturalBackground images, the results are with no surprise lower, but our ranking on the Stem images (3rd place, yet with a score of 16%) was unexpectedly quite interesting. Our low performance on fruits and flowers (respectively 8% and 10%) is a sign, if needed, that the representation of such organs requires more dedicated descriptors, and would probably benefit from the introduction of some kind of model to focus on the discriminant criteria from a botanical point of view. And finally, our 3rd place on the Leaf images corresponds to our previous rankings, with again the same limitations (only broad-leaved tree species considered, some images out of our processing scope, learning performed on plain background images) that might explain the rather low raw score value of nearly 17 %. Concerning geography, the run LirisReVeS run2 gave slightly better results than the run LirisReVeS run1 from which the location information was completely left out. But the difference (almost 1% in every category) does not appear as clearly as it did on native species, and it is hard to draw any conclusion. There were however too many manifest flaws in the context of this task to make this a valid experiment for us. First, we had theoretical phytogeographical data at hand for hardly half of the 250 species, which made the plausibility values we computed only half-relevant. More importantly, because of the doubt about the precise location of the observation, we were unable to use the local geographical parameters that might be more accurate. The fact that many images come from cities, where it is more difficult to assess whether the plant grew in its natural environment, also made the problem a bit less clear. It is nevertheless an encouraging step towards the use of geographical information to help the identification, a direction that will in any case be further investigated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This year's task constituted a real new challenge for our leaf-oriented project, and it came as no surprise that the generic, mechanical, methods we had the time to implement on fruits, barks and flowers were largely outperformed by the now mature leaf analysis process put into practice in the Folia application § . It was however a good chance to experiment with a late fusion mechanism which seemed to bear fruit. It was also the opportunity for a first testing of the novel approach proposed to combine the image information with geographical knowledge. The results of this last contribution are engaging, and we believe that with an adequate scope of species and a greater certainty on location it may constitute a real asset for field plant identification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,134.77,224.98,345.83,7.89;3,134.77,235.96,122.99,7.86;3,172.19,111.85,124.50,93.37"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Manual transformation of a natural background leaf image (a) : rotated and cropped (b) and user mark (c)</figDesc><graphic coords="3,172.19,111.85,124.50,93.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,140.49,714.18,334.38,7.89;3,177.99,608.74,259.37,90.66"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The use of deformable templates to segment and describe compound leaves</figDesc><graphic coords="3,177.99,608.74,259.37,90.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,134.77,687.58,345.83,7.89;4,138.23,590.02,103.75,77.81"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Segmenting flowers : user mark (a), dissimilarity map (b) and segmentation (c)</figDesc><graphic coords="4,138.23,590.02,103.75,77.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,134.77,429.45,345.83,7.89;5,134.77,440.43,37.44,7.86;5,144.38,357.92,69.15,51.78"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Representing fruit colors : color vocabulary (a), original image (b) and bag of colors (c)</figDesc><graphic coords="5,144.38,357.92,69.15,51.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,140.99,427.17,339.60,8.77;6,140.99,440.47,226.87,8.77;6,140.99,453.77,220.34,8.77;6,140.99,467.07,169.60,8.77;6,140.99,480.37,218.06,8.77"><head>-</head><label></label><figDesc>Leaf (L) : 126 tree species, separated in simple-leaved and compound-leaved -Fruit (F ) : 44 tree species with enough examples -Flower (f ) : 139 species, essentially herbaceous -Bark (B) : 78 species, only ligneous -Entire plant (p) : No training, by lack of time</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="7,197.43,515.89,220.50,7.89;7,176.46,309.34,259.38,201.74"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Example of the individual-based fusion process</figDesc><graphic coords="7,176.46,309.34,259.38,201.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="9,134.77,262.11,345.82,7.89;9,134.77,273.41,345.83,8.74;9,145.98,285.36,334.61,8.74;9,134.77,297.32,145.69,8.74;9,166.67,121.81,121.04,121.04"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Maps of two integrative geographical values on metropolitan France : Vegetation Levels (a) ( Snow, Alpine, Subalpine, Montane, Foothill, Planar, Supramediterranean, Mesomediterranean, Thermomediterranean, ) and Ecological Regions (GRECO) (b)</figDesc><graphic coords="9,166.67,121.81,121.04,121.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="10,134.77,237.30,345.83,7.89;10,134.77,248.29,117.66,7.86;10,178.96,101.89,121.04,115.66"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Plausibility maps obtained for two tree species : Fagus sylvatica, L. (a) and Acer monspessulanum, L. (b)</figDesc><graphic coords="10,178.96,101.89,121.04,115.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="11,134.77,274.41,345.83,7.89;11,134.77,285.40,268.63,7.86;11,134.77,306.59,345.82,8.74;11,134.77,318.54,147.94,8.74;11,149.71,332.96,330.88,8.74;11,134.77,344.91,345.83,8.74;11,134.77,356.87,345.83,8.74;11,134.77,368.82,345.83,8.74;11,134.77,380.78,320.48,8.74;11,134.77,392.73,345.83,8.74;11,134.77,404.69,345.83,8.74;11,134.77,416.64,345.82,8.74;11,134.77,428.60,345.82,8.74;11,134.77,440.55,345.83,8.74;11,134.77,452.51,345.83,8.74;11,134.77,464.46,345.82,8.74;11,134.77,476.42,345.83,8.74;11,134.77,488.38,194.13,8.74;11,194.71,106.87,207.50,162.74"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Classification performance for the different modalities : Leaf (SheetAsBackground), Leaf (NaturalBackground), Fruit, Stem, Flower top 5 answers, up to 70% in the case of natural images), the other modalities are not as far behind as expected.The Fruit category (44 classes, 3 descriptor sets) is the second best performing (more than 50% at 5 answers) but this performance has to be moderated by the fact the the number of classes is relatively small. On the other hand the Stem category (78 classes, 2 descriptor sets) performs honorably (more than 40% at 5 answers) given the number of species and the rawness of the description Same goes for the Flower modality (139 classes, 2 descriptors) for which it might be interesting to add that most of the accuracy (37% at 5 answers) actually comes from the simple color feature we used. All in all, these results underline the limits of the descriptions (hastily) implemented, and in the case of fruits and flowers the difficulties of the segmentation, given the low contribution of the shape description. It is also crucial to point out the greater abstraction power of the dedicated description created for leaves compared to the generic features used on the other organs ; this is visible in the blatant gap between the curves obtained on the training and validation sets.</figDesc><graphic coords="11,194.71,106.87,207.50,162.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="11,134.77,692.26,345.83,7.89;11,134.77,703.24,345.82,7.86;11,134.77,714.20,20.99,7.86;11,203.93,514.75,207.50,162.74"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Influence of geography on tree leaf classification : classification scores on plain background images of native species leaves without and with geographical plausibility</figDesc><graphic coords="11,203.93,514.75,207.50,162.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="12,136.78,477.13,341.80,7.89;12,166.28,287.63,276.66,170.25"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Rank difference of the correct species when adding geographical plausibility</figDesc><graphic coords="12,166.28,287.63,276.66,170.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="13,154.71,270.21,305.94,7.89;13,176.46,106.87,259.37,158.53"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Results on the Leaf (SheetAsBackground) images : 4th team on 12</figDesc><graphic coords="13,176.46,106.87,259.37,158.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12" coords="13,134.77,702.72,345.83,7.89;13,134.77,713.70,288.47,7.86;13,138.61,581.51,165.98,101.45"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Results on the NaturalBackground images : 3rd on Leaf images (a), 6th on Fruit images (b), 3rd on Stem images (c) and 7th on Flower images (d)</figDesc><graphic coords="13,138.61,581.51,165.98,101.45" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We would like to thank <rs type="person">Tristan Coulange</rs> for his contribution in the hard task of developing of competitive flower segmentation methods, as well as <rs type="person">Marine Aghadjanian</rs>, <rs type="person">Nicolas Charrel</rs>, <rs type="person">Nadya Dahir</rs> and <rs type="person">Maxime Mélinon</rs> for their precious help in processing and analyzing thousands of fruit, bark and flower images.</p></div>
			</div>
			<div type="funding">
<div><p>This work has been supported by the <rs type="funder">French National Agency for Research</rs> with the reference <rs type="grantNumber">ANR-10-CORD-005</rs> (<rs type="projectName">REVES</rs> project).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_Jrc2CVy">
					<idno type="grant-number">ANR-10-CORD-005</idno>
					<orgName type="project" subtype="full">REVES</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="14,142.96,579.08,337.63,7.86;14,151.52,590.04,329.07,7.86;14,151.52,601.00,195.49,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="14,421.18,579.08,59.40,7.86;14,151.52,590.04,312.36,7.86">A plant identification system using shape and morphological features on segmented leaflets</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Bagmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bhattacharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,151.52,601.00,166.85,7.86">CLEF (Notebook Papers/Labs/Workshop)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,612.40,337.64,7.86;14,151.52,623.35,329.07,7.86;14,151.52,634.31,327.45,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="14,207.12,623.35,273.47,7.86;14,151.52,634.31,88.45,7.86">Reves participation -tree species classification using random forests and botanical features</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cerutti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Antoine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Violaine Tougne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julien</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Valet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Coquin</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Vacavant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,259.36,634.31,190.96,7.86">CLEF (Online Working Notes/Labs/Workshop)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,645.71,337.63,7.86;14,151.52,656.67,329.07,7.86;14,151.52,667.63,329.07,7.86;14,151.52,678.59,125.33,7.86;14,137.73,701.23,3.43,5.24;14,169.79,702.99,200.28,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="14,359.35,645.71,121.24,7.86;14,151.52,656.67,308.40,7.86">Curvature-scale-based contour understanding for leaf margin shape recognition and species identification</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cerutti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Tougne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Coquin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vacavant</surname></persName>
		</author>
		<ptr target="https://itunes.apple.com/app/folia/id547650203" />
	</analytic>
	<monogr>
		<title level="m" coord="14,151.52,667.63,329.07,7.86;14,151.52,678.59,97.26,7.86">VISAPP 2013 -Proceedings of the International Conference on Computer Vision Theory and Applications</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,120.67,337.64,7.86;15,151.52,131.63,329.07,7.86;15,151.52,142.59,182.72,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="15,419.38,120.67,61.21,7.86;15,151.52,131.63,252.51,7.86">A model-based approach for compound leaves understanding and identification</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cerutti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Tougne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vacavant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Coquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,422.30,131.63,58.29,7.86;15,151.52,142.59,154.65,7.86">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,153.55,337.63,7.86;15,151.52,164.51,329.07,7.86;15,151.52,175.46,99.10,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="15,365.91,153.55,114.68,7.86;15,151.52,164.51,175.02,7.86">A parametric active polygon for leaf segmentation and shape estimation</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cerutti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Tougne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vacavant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Coquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,348.80,164.51,131.79,7.86;15,151.52,175.46,70.40,7.86">7th International Symposium on Visual Computing</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,186.42,337.63,7.86;15,151.52,197.38,329.07,7.86;15,151.52,208.34,66.52,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="15,259.11,186.42,221.48,7.86;15,151.52,197.38,111.92,7.86">Automated identication of tree species from images of the bark, leaves and needles</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Fiel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sablatnig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,283.44,197.38,197.15,7.86;15,151.52,208.34,37.73,7.86">Proceedings of the 16th Computer Vision Winter Workshop</title>
		<meeting>the 16th Computer Vision Winter Workshop</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,219.30,337.64,7.86;15,151.52,230.26,312.68,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="15,464.72,219.30,15.88,7.86;15,151.52,230.26,156.46,7.86">The imageclef 2013 plant identification task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Barthelemy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-F</forename><surname>Molino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,327.43,230.26,108.52,7.86">CLEF 2013 Working Notes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,241.22,337.63,7.86;15,151.52,252.18,329.07,7.86;15,151.52,263.14,148.30,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="15,298.12,252.18,178.62,7.86">The clef 2012 plant images classification task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Barthelemy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-F</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Birnbaum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mouysset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,163.04,263.14,108.52,7.86">CLEF 2012 Working Notes</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.96,274.09,337.63,7.86;15,151.52,285.05,329.07,7.86;15,151.52,296.01,264.08,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="15,419.88,274.09,60.71,7.86;15,151.52,285.05,254.43,7.86">Bark classification based on gabor filter features using rbpnn neural network</title>
		<author>
			<persName coords=""><forename type="first">Z.-K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z.-H</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S.-B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,428.00,285.05,52.60,7.86;15,151.52,296.01,182.19,7.86">International Conference on Neural Information Processing</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="80" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.61,306.97,337.98,7.86;15,151.52,317.93,44.09,7.86" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Inventaire</forename><surname>Forestier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">National</forename></persName>
		</author>
		<title level="m" coord="15,271.23,306.97,205.38,7.86">Les Sylvoécorégions (SER) de France métropolitaine</title>
		<imprint>
			<publisher>IGN</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.61,328.89,337.98,7.86;15,151.52,339.85,35.83,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="15,189.66,328.89,151.92,7.86">Carte phytogéographique de la France</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Julve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,349.50,328.89,126.88,7.86">Cahiers de Géographie Physique</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.61,350.81,337.98,7.86;15,151.52,361.77,329.07,7.86;15,151.52,372.73,315.81,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="15,201.63,350.81,278.97,7.86;15,151.52,361.77,329.07,7.86;15,151.52,372.73,139.01,7.86">Species plantarum : exhibentes plantas rite cognitas, ad genera relatas, cum differentiis specificis, nominibus trivialibus, synonymis selectis, locis natalibus, secundum systema sexuale digestas</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">V</forename><surname>Linne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,298.38,372.73,141.34,7.86">Holmiae : Impensis Laurentii Salvii</title>
		<imprint>
			<date type="published" when="1753">1753</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.61,383.68,337.97,7.86;15,151.52,394.64,285.16,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="15,293.09,383.68,183.22,7.86">Delving into the whorl of flower segmentation</title>
		<author>
			<persName coords=""><forename type="first">M.-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,163.04,394.64,140.31,7.86">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="570" to="579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.61,405.60,337.98,7.86;15,151.52,416.56,329.07,7.86;15,151.52,427.52,126.57,7.86" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rameau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mansion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Dumé</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Timbal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lecointe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Keller</surname></persName>
		</author>
		<title level="m" coord="15,198.56,416.56,207.68,7.86">Flore forestière française: Guide écologique illustré</title>
		<imprint>
			<publisher>Institut pour le Développement Forestier</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.61,438.48,337.97,7.86;15,151.52,449.44,329.07,7.86;15,151.52,460.40,83.97,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="15,397.54,438.48,83.04,7.86;15,151.52,449.44,144.44,7.86">Automatic fruit and vegetable classification from images</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">C</forename><surname>Hauagge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wainer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Goldenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,307.14,449.44,169.53,7.86">Computers and Electronics in Agriculture</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="96" to="104" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.61,471.36,337.98,7.86;15,151.52,482.31,288.98,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="15,258.83,471.36,169.69,7.86">Automatic recognition of blooming flowers</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Saitoh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kaneko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,447.52,471.36,33.07,7.86;15,151.52,482.31,165.08,7.86">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="27" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.61,493.27,337.97,7.86;15,151.52,504.23,150.15,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="15,238.63,493.27,69.04,7.86">Confidence fusion</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,325.17,493.27,105.28,7.86;15,458.32,493.27,22.27,7.86;15,151.52,504.23,121.99,7.86">IEEE Workshop on Robotic Sensing)</title>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
	<note>Proceedings of ROSE&apos;2004</note>
</biblStruct>

<biblStruct coords="15,142.61,515.19,337.98,7.86;15,151.52,526.15,329.07,7.86;15,151.52,537.11,190.30,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="15,384.55,515.19,96.05,7.86;15,151.52,526.15,131.59,7.86">Pathos: Part-based tree hierarchy for object segmentation</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Suta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Scuturuici</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V.-M</forename><surname>Scuturici</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Miguet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,301.26,526.15,179.34,7.86;15,151.52,537.11,144.08,7.86">International Conference on Computer Analysis of Images and Patterns (CAIP)</title>
		<imprint>
			<date type="published" when="2013-08">aug 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.61,548.07,337.98,7.86;15,151.52,559.03,329.07,7.86;15,151.52,569.99,255.54,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="15,355.40,548.07,125.19,7.86;15,151.52,559.03,146.09,7.86">Bark texture feature extraction based on statistical texture analysis</title>
		<author>
			<persName coords=""><forename type="first">Y.-Y</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z.-R</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,320.17,559.03,160.42,7.86;15,151.52,569.99,164.43,7.86">International Symposium on Intelligent Multimedia, Video and Speech Processing</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="482" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.61,580.94,337.97,7.86;15,151.52,591.90,329.07,7.86;15,151.52,602.86,157.83,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="15,343.03,580.94,137.56,7.86;15,151.52,591.90,252.00,7.86">Sabanci-okan system at imageclef 2012: Combining features and classifiers for plant identification</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">A</forename><surname>Yanikoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Aptoula</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Tirkaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,422.29,591.90,58.30,7.86;15,151.52,602.86,129.19,7.86">CLEF (Online Working Notes/Labs/Workshop)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
