<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,152.16,152.64,291.17,12.45;1,152.86,170.37,288.97,12.45">DEMIR at ImageCLEFMed 2013: The Effects of Modality Classification to Information Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,165.99,210.28,84.39,8.94"><forename type="first">Okan</forename><surname>Ozturkmenoglu</surname></persName>
							<email>okan.ozturkmenoglu@deu.edu.tr</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Engineering</orgName>
								<orgName type="department" key="dep2">DEMIR Dokuz Eylül Multi-media Information Retrieval Research Group</orgName>
								<orgName type="institution" key="instit1">Dokuz Eylül University</orgName>
								<orgName type="institution" key="instit2">Tinaztepe Izmir</orgName>
								<address>
									<postCode>35160</postCode>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,257.85,210.28,88.52,8.94"><forename type="first">Nefise</forename><forename type="middle">Meltem</forename><surname>Ceylan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Engineering</orgName>
								<orgName type="department" key="dep2">DEMIR Dokuz Eylül Multi-media Information Retrieval Research Group</orgName>
								<orgName type="institution" key="instit1">Dokuz Eylül University</orgName>
								<orgName type="institution" key="instit2">Tinaztepe Izmir</orgName>
								<address>
									<postCode>35160</postCode>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,370.28,210.28,58.89,8.94"><forename type="first">Adil</forename><surname>Alpkocak</surname></persName>
							<email>alpkocak@cs.deu.edu.tr</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Engineering</orgName>
								<orgName type="department" key="dep2">DEMIR Dokuz Eylül Multi-media Information Retrieval Research Group</orgName>
								<orgName type="institution" key="instit1">Dokuz Eylül University</orgName>
								<orgName type="institution" key="instit2">Tinaztepe Izmir</orgName>
								<address>
									<postCode>35160</postCode>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,152.16,152.64,291.17,12.45;1,152.86,170.37,288.97,12.45">DEMIR at ImageCLEFMed 2013: The Effects of Modality Classification to Information Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2D7360AB9283B3BC4C62B55F5ACECEB6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Modality classification</term>
					<term>multimodal information retrieval</term>
					<term>contentbased image retrieval</term>
					<term>medical image retrieval</term>
					<term>information retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper present the details of participation of DEMIR (Dokuz Eylül University Multimedia Information Retrieval) research team to the Im-ageCLEF 2013 Medical Retrieval task. This year, we participated to two subtasks: modality classification and ad-hoc image-based retrieval. For them, our central method is integrated combination multimodal retrieval applied to retrieved documents sets of each visual and text features, after than our information retrieval based classification algorithm is performed. In modality classification subtask, we proposed an approach for modality classification based on information retrieval techniques. The main elements of this method are information retrieval techniques. Additionally, in ad-hoc image-based retrieval subtask, we assumed as a baseline that our methods which were obtained our best performances in ImageCLEF 2012. We added on our proposed classification method to these baseline runs and we evaluated impact of classification on the modalities of documents.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.153" lry="707.01"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.153" lry="842.01"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.153" lry="842.01"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.153" lry="842.01"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.153" lry="842.01"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.153" lry="842.01"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.153" lry="842.01"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.153" lry="842.01"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.153" lry="842.01"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.153" lry="842.01"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper we present the experiments performed by Dokuz Eylül University Multimedia Information Retrieval (DEMIR) Group, Turkey, in the context of our participation to the ImageCLEF 2013 AMIA: Medical task. In this year, we participated to two subtasks: modality classification and ad-hoc image-based retrieval. The main focus of this work is evaluation of result improvement using the classification methods on the modalities of documents in data collection. We performed these methods as a baseline to our best results in ImageCLEF 2012 Medical Image <ref type="bibr" coords="1,436.60,610.00,30.37,8.94;1,124.84,621.70,166.82,8.94">Classification and Retrieval (Vahid et al., 2012)</ref> and so we are expected to increase performance than last year. In this year, as a baseline, we assumed that we applied inter modality integrated combination of text and low-level image features; and we utilized this method to combine result of different low level features of images as well as last year. On the other hand, we tried to filter out of irrelevant documents using classification algorithm.</p><p>After explanation of modality classification and ad-hoc image-based retrieval tasks definition (Section 2), we describe the textual and visual features (Section 3) we used. Section 4 contains that our classification technique for filtering the data collection out and our studies on modality classification subtask. After we describe methods, submitted runs and results on ad-hoc image-based retrieval subtask (Section 5), and then Section 6 concludes the paper by pointing out the open issues and possible avenues of further research for content-based image retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>Task Definition</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Modality Classification</head><p>For this task, past studies showed that methods based on visual features gives better results than text-based techniques. These studies provide better results as filtering out search results using modality information, which can be extracted from the image itself using visual features, such as Goldminer or Yottalook. So, the search results can be improved significantly using the modality classification. We expected to get better results using this modality information. With this in mind, our current and previous studies <ref type="bibr" coords="2,155.70,370.94,94.11,8.94" target="#b0">(Alpkocak et al., 2011)</ref> and the studies of IBM group have shown that mixed methods perform slightly better results in an appreciable ratio. For example, in modality classification task, IBM group classified in %80.79 for textual modality but for mixed modality, the count of correctly classified documents increased to %81.68 at ImageCLEF 2013 AMIA: Medical. Within that perspective; firstly we apply our information retrieval based classifier to textual data. Then, we performed integrated combination multimodal retrieval technique and information retrieval based classifier technique as mixed method approach.</p><p>In this task, the data collection contains a modality hierarchy, which has three main modality categories of image and totally 31 categories, training set and test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Ad-hoc Image-based Retrieval</head><p>The data collection of ad-hoc image-based retrieval task in ImageCLEF 2013 AMIA: Medical Retrieval has textual and visual information. It has about 75K articles in collection with approximately 306K figures. Participants were given a set of 35 queries with 2-3 sample images for each query. The queries are classified into mixed, visual and semantic, based on the methods that are expected to yield the best results.</p><p>We performed our experiments using ImageCLEF 2012 Medical Image Classification and Retrieval track's data. We check the variation of retrieval methods on textual and visual information to gain the best result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Definition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Textual Features</head><p>We used Terrier IR Platform API <ref type="bibr" coords="3,262.09,196.80,76.21,8.94" target="#b5">(Ounis et al., 2006)</ref>, which is an open source search engine written in Java and is developed at the School of Computing Science, University of Glasgow, to generate VSM. Terrier provides efficient and effective search methods supported by many different parameters.</p><p>In modality classification task, we used caption tags of each test and train image as textual features and modeled in vector space model (VSM). We can summarize applied parameters of Terrier under three topics. Firstly; we apply token normalization; remove all punctuation characters and convert all characters to lower-case. After then, stop words are removed from the collection, and we applied porter stemmer algorithm on normalized tokens. But we did not apply stemming for all runs. The parameters of different runs are given in detail in section 4.2. Beside these parameters, we apply different weighting schemes. According to defined parameters, two different text collections are obtained. In the first set, we used porter stemmer and InL2 weighting approach. For the second set, we did not use stemmer and did apply TF×IDF weighting scheme.</p><p>In ad-hoc image-based retrieval task, we first split the XML file for textual metadata and represented each image in the collection as a structured document of xml file. We also expanded the XML file using related article full text, abstract and title as new tags. We used Terrier for our text-based information retrieval subsystem and we performed our experiments on textual features using TF×IDF. The order in which transformations were applied is as follows:</p><p>1. Noise character removal: characters with no meaning, like punctuation marks or blanks, are all eliminated; 2. Stop-word removal: discarding of semantically empty words, very high-frequency words; 3. Token normalization: converting all words to lower case; 4. Stemming: we used the Porter stemmer <ref type="bibr" coords="3,302.50,518.81,57.19,8.94" target="#b6">(Porter, 1997)</ref> as a process for removing the common morphological endings from words in English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Visual Features</head><p>We extracted features for all images in test collection and query examples using Rummager tool <ref type="bibr" coords="3,191.83,592.59,118.39,8.94" target="#b3">(Chatzichristofis et al., 2009)</ref>, which is developed in the Automatic Control Systems &amp; Robotics Laboratory at the Democritus University of Thrace-Greece. We used FCTH, CLD and CEDD features. Our previous studies <ref type="bibr" coords="3,418.12,616.70,52.51,8.94;3,124.79,628.76,23.17,8.94" target="#b7">(Vahid et al., 2012)</ref> have shown that these three low-level features allowed us access to our best scores so we extracted features for all images. Here we explained these features we used below:</p><p>• Color and edge directivity descriptor (CEDD): The CEDD includes texture information produced by the six-bin histogram of the fuzzy system that uses the five digital filters proposed by the MPEG-7 EHD. Additionally, for color information the CEDD uses the 24-bin color histogram produced by the 24-bin fuzzy-linking system. Overall, the final histogram has 144 regions. This feature combines EHD with color histogram information and named "Color and Edge Directivity Descriptor". Important attribute of the CEDD is the low computational power needed for its extraction, in comparison to the needs of the most MPEG-7 descriptors <ref type="bibr" coords="4,136.19,246.10,145.67,8.94">(Chatzichristofis and Boutalis, 2008a</ref>). • Fuzzy color and texture histogram (FCTH): The FCTH descriptor includes the texture information produced in the eight-bin histogram of the fuzzy system that uses the high frequency bands of the Haar wavelet transform. For color information, the descriptor uses the 24-bin color histogram produced by the 24-bin fuzzy-linking system. Overall, the final histogram includes192 regions. This feature fuzzy version of CEDD feature which contains fuzzy set of color and texture histogram and named "Fuzzy Color and Texture Histogram". This feature contains results from the combination of 3 fuzzy systems including histogram, color and texture information <ref type="bibr" coords="4,215.99,354.26,146.27,8.94">(Chatzichristofis and Boutalis, 2008b</ref>). • Color layout descriptor (CLD): This descriptor effectively represents the spatial distribution of color of visual signals in a very compact form. This compactness allows visual signal matching functionality with high retrieval efficiency at very small computational costs. It provides image-to-image matching as well as ultrahigh-speed sequence-to-sequence matching, which requires so many repetitions of similarity calculations <ref type="bibr" coords="4,226.97,426.61,128.69,8.94" target="#b4">(Lux and Chatzichristofis, 2008)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Modality Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Methods</head><p>Text. We applied a new approach for modality classification based on information retrieval System (IRS) where IRS system is used as text classifier.</p><p>Mixed. Mixed method for modality classification based on combining CEDD, FCTH and CLD features or visual terms with textual information. Algorithm of this method is similar to the algorithm given for text based classification. Additionally, it combines the retrieved documents of different visual features and text features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Runs</head><p>In order to assess the above mentioned methods, we set up a set of experiments on the data collection of modality classification task in ImageCLEF 2013 AMIA: Medical Retrieval. We submitted 6 runs to ImageCLEF 2013 AMIA: Medical Retrieval for modality classification task two categories as following:</p><p>Textual.</p><p>• DEMIR_MC_1: In this run we applied porter stemmer algorithm during text processing procedure and used InL2 (Divergence from Randomness Framework) matching model to calculate weighting factor of each term. • DEMIR_MC_2: The main difference of this run and the previous run are no-stemming and usage of TF×IDF weighting scheme.</p><p>Mixed.</p><p>• DEMIR_MC_3: This run uses Visual feature Set 2 and the same textual features with run DEMIR_MC_2. This weighting factor try to balance text and visual features, where the weighting factor for both visual and texture feature is equal to 1 • DEMIR_MC_4: The only difference of this run and the run DEMIR_MC_3 is the weight of text similarity score is 1.7. • DEMIR_MC_5: Text feature set and ICWF factor of this run is the same with run DEMIR_MC_3. Includes CLD, FCTH and CEDD as visual features and weight of visual and textual fearures are equal. • DEMIR_MC_6: This run is similar to run DEMIR_MC_5, weight for textual similarity score is to 1.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>For modality classification subtask, we submit runs for text and mixed methods. Among our runs we get the best performance in mixed category, inputs are visual CEDD, CLD, FCTH and textual features. Integrated combination multimodal retrieval applied to retrieved image sets of each visual and text features, after than our information retrieval based classification algorithm is performed. If we evaluate run results for each method separately, for text based-method, applying information retrieval based classification algorithm to text feature set correctly classify %62.70 of test images by run DEMIR_MC_1 and DEMIR_MC_2. This performance is the third best performance among other groups' runs. Text based run results show that applying a stemming algorithm during text processing or different term weighting algorithms do not affect the performance directly. Following, best run performance for the mixed method is obtained by run DEMIR_MC_5. Textual features of this run is the same with the run DEMIR_MC_2 and CLD, FCTH and CEDD visual features' values used as input to our algorithm. The most important point of this run is the value of ICWF value is 1.0. Mixed method runs indicate the effect of two major parameters. Firstly; weighting visual and text features equally by ICWF gives better results. Following, our synthetic visual terms generated by clustering do not have a positive impact on test results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5</head><p>Ad-hoc Image-based Retrieval</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Methods</head><p>In this year, we tested effectiveness of our information retrieval System (IRS) based test classification approach on information retrieval performance. So, in addition to the results of last year's baseline runs, we preferred to filter out such documents using classification methods, which is, explained details in section 4.1 and narrowing data collection down.</p><p>For the results of textual modality, we applied the basic processes in information retrieval system that consists of preprocessing, indexing and retrieval stages using Terrier. Additionally, in some runs, we assumed the results of modality classification as textual modality's result.</p><p>For the results of visual modality, we extracted CEDD, FCTH and CLD features using Rummager tool <ref type="bibr" coords="6,214.95,661.78,116.33,8.94" target="#b3">(Chatzichristofis et al., 2009)</ref>. We created a VSM for each feature, for each document in collection. After we calculated the similarities using using Euclidean distance function. Then, we normalized among them and combined by averaging as a visual modality result.</p><p>For mixed modality, we kept the results for textual and visual modalities and performed integrated weighted CombSUM combination such that coefficient of text modality was 1.7 folds of visual modality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Runs</head><p>We submitted 10 runs to ImageCLEF 2013 AMIA: Medical Retrieval for ad-hoc image-based retrieval task three categories. Below, we provide a short description of each run, shortly.</p><p>Textual.</p><p>• DEMIR1: This run is our baseline retrieval result for textual modality. In this run, caption tag is used in indexing documents. Retrieval weighting model is TF×IDF. UTF tokeniser and stop-word list were used and porter stemmer was applied. We used en-description field in topics file for each topic when retrieved. We obtained the best result in ImageCLEF2012 Medical Image Classification and Retrieval, using this method so we have got it as a baseline for this year. • DEMIR6: We assumed that the result of modality classification is a result of textual modality, instead of Terrier result. We evaluated according to result of order 1 (k=1; first k modalities of sorted candidate modalities given in text based modality classification method) using modality classifications. • DEMIR8: After classification of documents and topics using our modality classification based methods, we filtered out the documents and retrieved from the classified documents which were result of order 0 using textual features distance. • DEMIR9: This run is similar to run DEMIR8, but in this run, we retrieved from the classified documents that were ordered in the first three documents (k={1,2,3}; first k modalities of sorted candidate modalities given in text based modality classification method) using textual features distance.</p><p>Visual.</p><p>• DEMIR2: This run is our baseline retrieval for visual retrieval type. We used CEDD, FCTH and MPEG7-CLD features, and Euclidean distance to calculate the similarities between topics and documents. We normalized distance scores as topic level. We calculated median of sum used features scores and combined them. We used the maximum values for getting topic results from their figures results that has a combination of used visual features. • DEMIR4: After classification of documents and topics, we filtered out the documents and retrieved from the classified documents which were result of order 0 using visual features distance.</p><p>• DEMIR5: This run is similar to run DEMIR4, but in this run, we retrieved from the classified documents that were ordered in the first three documents using visual features distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mixed.</head><p>• DEMIR3: We performed integrated weighted combination such that coefficient of text modality was 1.7 folds of visual modality. This run is baseline for mixed retrieval type. • DEMIR7: We performed integrated weighted combination such that coefficient of text modality in DEMIR6 was 1.7 folds of visual modality in DEMIR2. We obtained text modality results from modality classification results be applied to documents. • DEMIR10: We performed integrated weighted combination such that coefficient of text modality in DEMIR1 was 1.7 folds of visual modality in DEMIR2 and we filtered out results using modality classification results according to order 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>For ad-hoc image-based retrieval subtask, we submitted 10 runs to ImageCLEF Medical Retrieval task, in three different categories: textual-only, visual-only and mixed retrieval types. Among our runs, we get the best performance in mixed category, in run DEMIR3; inputs are visual CEDD, CLD, FCTH and textual features as in the modality classification subtask. If we evaluate run results (DEMIR9, DEMIR1, DEMIR6, DEMIR8) for textual-only retrieval type, when we filtered out documents using information retrieval based classification algorithm and retrieved from the classified documents, we achieved the best result as in run DEMIR9 than other runs. As in the textual-only, the information retrieval based classification algorithm improves the performance for visual-only retrieval type. We applied classification algorithm in runs DEMIR4 and DEMIR5, so these runs performances are better than DEMIR2 for visual-only. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this year, we examined effects of modality classification to retrieval performance. Among our runs, we get the best performance rather than our other submitted runs in mixed category for both subtasks. Integrated combination multimodal retrieval applied to retrieved image sets of each visual and text features, after than our information retrieval based classification algorithm is performed. We also used our integrated combination method, which was used by our team DEMIR at last year, on different level of multimodality retrieval system and again, we agree that proper combination model can improve the performance of multimodal retrieval systems. We used CEDD, FCTH and CLD features as low-level features in visual modality.</p><p>For modality classification subtask, we apply integrated combination multimodal retrieval and our information retrieval based classification algorithm. Text based run results show that applying a stemming algorithm during text processing or different term weighting algorithms do not affect the performance directly. Mixed method runs indicate the effect of two major parameters. Firstly; weighting visual and text features equally by ICWF gives better results. Another one, our synthetic visual terms generated by clustering do not have a positive impact on test results.</p><p>For ad-hoc image-based retrieval subtask, our proposed approach is based on the information retrieval based classification algorithm. We aim to evaluate the effects of classification method on information retrieval system. So, in addition to the results of last year's baseline runs, we preferred to filter out such documents using modality classification method and retrieved from the classified documents. As in the textual-only, the information retrieval based classification algorithm improves the performance for visual-only retrieval type.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,127.33,150.00,341.52,317.98"><head>Table 1 .</head><label>1</label><figDesc>Runs of DEMIR group for modality classification task in ImageCLEFMed 2013 AMIA: Medical Retrieval</figDesc><table coords="6,127.33,185.12,341.52,282.87"><row><cell></cell><cell></cell><cell></cell><cell cols="2">Text Parameters</cell><cell cols="2">Mixed Parameters</cell></row><row><cell></cell><cell></cell><cell>Correctly</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RunID</cell><cell>Type</cell><cell>classified in %</cell><cell>Stemming Method</cell><cell>Matching Model</cell><cell>Visual Fea-ture Set</cell><cell>ICWF</cell></row><row><cell>DEMIR_MC_5</cell><cell>Mixed</cell><cell>64.60</cell><cell>No Stem-ming</cell><cell>TF_IDF</cell><cell>CEDD, CLD, FCTH</cell><cell>1.0</cell></row><row><cell>DEMIR_MC_3</cell><cell>Mixed</cell><cell>64.48</cell><cell>No Stem-ming</cell><cell>TF_IDF</cell><cell>VT CEDD, VT VT FCTH, CLD,</cell><cell>1.7</cell></row><row><cell>DEMIR_MC_6</cell><cell>Mixed</cell><cell>64.09</cell><cell>No Stem-ming</cell><cell>TF_IDF</cell><cell>CEDD, CLD, FCTH</cell><cell>1.0</cell></row><row><cell>DEMIR_MC_4</cell><cell>Mixed</cell><cell>63.67</cell><cell>No Stem-ming</cell><cell>TF_IDF</cell><cell>VT CEDD, VT VT FCTH, CLD,</cell><cell>1.7</cell></row><row><cell>DEMIR_MC_1</cell><cell>Textual</cell><cell>62.70</cell><cell>Porter Stemmer</cell><cell>InL2</cell><cell>-</cell><cell></cell></row><row><cell>DEMIR_MC_2</cell><cell>Textual</cell><cell>62.70</cell><cell>No Stem-ming</cell><cell>TF_IDF</cell><cell>-</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,125.20,538.73,344.93,135.19"><head>Table 2 .</head><label>2</label><figDesc>Runs of DEMIR group for ad-hoc image-based retrieval task in ImageCLEFMed 2013 AMIA: Medical Retrieval</figDesc><table coords="8,144.35,573.12,308.93,100.81"><row><cell>RunID</cell><cell>Type</cell><cell>MAP</cell><cell>GM-MAP</cell><cell>bpref</cell><cell>P10</cell><cell>P30</cell></row><row><cell>DEMIR3</cell><cell>Mixed</cell><cell>0.2168</cell><cell>0.0345</cell><cell>0.2255</cell><cell>0.3143</cell><cell>0.1914</cell></row><row><cell>DEMIR9</cell><cell cols="2">Textual 0.2003</cell><cell>0.0352</cell><cell>0.2158</cell><cell>0.2943</cell><cell>0.1952</cell></row><row><cell>DEMIR1</cell><cell cols="2">Textual 0.1951</cell><cell>0.0289</cell><cell>0.2036</cell><cell>0.2714</cell><cell>0.1895</cell></row><row><cell>DEMIR6</cell><cell cols="2">Textual 0.1951</cell><cell>0.0289</cell><cell>0.2036</cell><cell>0.2714</cell><cell>0.1895</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,124.85,655.79,345.80,7.98;9,124.85,667.85,345.64,7.98;10,124.85,151.07,307.24,7.98" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,430.94,655.79,39.71,7.98;9,124.85,667.85,345.64,7.98;10,124.85,151.07,88.43,7.98">DEMIR at ImageCLEFMed 2011: Evaluation of Fusion Techniques for Multimodal Content-based Medical Image Retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Alpkocak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Ozturkmenoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Berber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">H</forename><surname>Vahid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">G</forename><surname>Hamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,229.48,151.07,198.29,7.98">Conference and Labs of the Evaluation Forum (CLEF)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,124.85,174.83,345.85,7.98;10,124.85,186.89,346.15,7.98;10,124.85,198.95,345.90,7.98;10,124.85,211.01,27.40,7.98" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,295.45,174.83,175.25,7.98;10,124.85,186.89,192.99,7.98">CEDD: color and edge directivity descriptor: a compact descriptor for image indexing and retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Boutalis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,335.53,186.89,135.47,7.98;10,124.85,198.95,147.09,7.98">Proceedings of the 6th international conference on Computer vision systems</title>
		<meeting>the 6th international conference on Computer vision systems<address><addrLine>Santorini, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="312" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,124.85,223.07,345.83,7.98;10,124.85,234.77,345.99,7.98;10,124.85,246.83,345.90,7.98;10,124.85,258.89,114.65,7.98" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,293.68,223.07,177.00,7.98;10,124.85,234.77,197.32,7.98">FCTH: Fuzzy Color and Texture Histogram -A Low Level Feature for Accurate Image Retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Boutalis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,345.46,234.77,125.38,7.98;10,124.85,246.83,286.27,7.98">Proceedings of the 2008 Ninth International Workshop on Image Analysis for Multimedia Interactive Services</title>
		<meeting>the 2008 Ninth International Workshop on Image Analysis for Multimedia Interactive Services</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="191" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,124.85,270.95,345.80,7.98;10,124.85,283.01,345.99,7.98;10,124.85,295.07,345.90,7.98;10,124.85,306.77,59.32,7.98" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,321.34,270.95,149.30,7.98;10,124.85,283.01,109.82,7.98">Img(Rummager): An Interactive Content Based Image Retrieval System</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Boutalis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,252.18,283.01,218.66,7.98;10,124.85,295.07,129.08,7.98">Proceedings of the 2009 Second International Workshop on Similarity Search and Applications</title>
		<meeting>the 2009 Second International Workshop on Similarity Search and Applications<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="151" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,124.85,318.83,345.70,7.98;10,124.85,330.89,345.90,7.98;10,124.85,342.95,208.29,7.98" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,271.68,318.83,198.86,7.98;10,124.85,330.89,22.73,7.98">Lire: lucene image retrieval: an extensible java CBIR library</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,164.93,330.89,258.16,7.98">Proceedings of the 16th ACM international conference on Multimedia</title>
		<meeting>the 16th ACM international conference on Multimedia<address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1085" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,124.85,355.01,345.64,7.98;10,124.85,367.07,345.99,7.98;10,124.85,378.77,311.50,7.98" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,412.14,355.01,58.34,7.98;10,124.85,367.07,215.88,7.98">Terrier: A High Performance and Scalable Information Retrieval Platform</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,360.35,367.07,110.48,7.98;10,124.85,378.77,153.25,7.98">ACM SIGIR&apos;06 Workshop on Open Source Information Retrieval (OSIR)</title>
		<meeting><address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,124.85,390.83,321.78,7.98" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,190.46,390.83,117.27,7.98">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,323.47,390.83,119.85,7.98">Readings in information retrieval</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,289.42,402.89,126.71,7.98;10,124.85,414.95,345.80,7.98;10,124.85,427.01,345.90,7.98;10,124.85,439.07,201.55,7.98" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,431.65,414.95,39.00,7.98;10,124.85,427.01,60.47,7.98">DEMIR at ImageCLEFMed</title>
		<author>
			<persName coords=""><forename type="first">Morgan</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Publishers</forename><surname>Inc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">H</forename><surname>Vahid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Alpkocak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">G</forename><surname>Hamed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">M</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Ozturkmenoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,222.38,427.01,160.39,7.98">CLEF 2012 Evaluation Labs and Workshop</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Karlgren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Andwomser-Hacker</surname></persName>
		</editor>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
