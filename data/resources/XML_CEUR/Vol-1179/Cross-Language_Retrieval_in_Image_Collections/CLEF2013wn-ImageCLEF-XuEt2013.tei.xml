<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,145.20,115.69,325.05,12.84">MIAR ICT participation at Robot Vision 2013</title>
				<funder ref="#_qf59RTM">
					<orgName type="full">Key Technologies R&amp;D Program of China</orgName>
				</funder>
				<funder ref="#_DuJyfYr">
					<orgName type="full">National Basic Research Program of China</orgName>
				</funder>
				<funder ref="#_SEJ96Qz #_5GS2Jew">
					<orgName type="full">National Natural Science Founda-tion of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,134.76,152.96,45.40,9.96"><forename type="first">Ruihan</forename><surname>Xu</surname></persName>
							<email>ruihan.xu@vipl.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences No</orgName>
								<address>
									<addrLine>6 Kexueyuan South Road Zhongguancun</addrLine>
									<settlement>Haidian District Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,188.75,152.96,64.98,9.96"><forename type="first">Shuqiang</forename><surname>Jiang</surname></persName>
							<email>shuqiang.jiang@vipl.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences No</orgName>
								<address>
									<addrLine>6 Kexueyuan South Road Zhongguancun</addrLine>
									<settlement>Haidian District Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,261.23,152.96,59.18,9.96"><forename type="first">Xinhang</forename><surname>Song</surname></persName>
							<email>xinhang.song@vipl.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences No</orgName>
								<address>
									<addrLine>6 Kexueyuan South Road Zhongguancun</addrLine>
									<settlement>Haidian District Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,328.30,152.96,57.21,9.96"><forename type="first">Shuang</forename><surname>Wang</surname></persName>
							<email>shuang.wang@vipl.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences No</orgName>
								<address>
									<addrLine>6 Kexueyuan South Road Zhongguancun</addrLine>
									<settlement>Haidian District Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,394.30,152.96,26.52,9.96"><forename type="first">Yi</forename><surname>Xie</surname></persName>
							<email>yi.xie@vipl.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences No</orgName>
								<address>
									<addrLine>6 Kexueyuan South Road Zhongguancun</addrLine>
									<settlement>Haidian District Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,428.52,152.96,46.55,9.96"><forename type="first">Fang</forename><surname>Wang</surname></persName>
							<email>fang.wang@vipl.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences No</orgName>
								<address>
									<addrLine>6 Kexueyuan South Road Zhongguancun</addrLine>
									<settlement>Haidian District Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,297.11,164.96,40.54,9.96"><forename type="first">Xiong</forename><surname>Lv</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences No</orgName>
								<address>
									<addrLine>6 Kexueyuan South Road Zhongguancun</addrLine>
									<settlement>Haidian District Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,145.20,115.69,325.05,12.84">MIAR ICT participation at Robot Vision 2013</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4FBBF36CD126EE3BDC7CADC7F6A048A1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>kernel descriptor</term>
					<term>scene classification</term>
					<term>object recognition</term>
					<term>temporal continuity</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of our team -MIAR ICT in the ImageCLEF 2013 Robot Vision Challenge. The task of the Challenge asked participants to classify imaged indoor scenes and recognize the predefined objects appeared in the imaged scene. Our approach is based on the recently proposed Kernel Descriptors framework, which is an effective representation for images. For the provided visual and depth sequences, we make a simple fusion at feature level. Then we use Linear Support Vector Machine (L-SVM) classifiers for both scene classification and object recognition. At last, the temporal continuity of the given sequences is considered. Our team ranked the first among all the participants, showing the effectiveness of our proposed scheme.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the 5th Robot Vision Challenge of the ImageCLEF 2013, image sequences were captured by a perspective camera and a Kinect <ref type="bibr" coords="1,342.46,464.12,13.08,9.96" target="#b0">[1]</ref> mounted on a mobile robot within an office environment. Visual (RGB) images and depth images generated from 3D point clouds were available. Training sequences were labelled not only with semantic labels (corridor, kitchen, office, etc.) but also with the objects that were represented in them (fridge, chair, computer, etc.). The test sequence were acquired within the same building and floor, but there could be variations in the lighting conditions (very bright places or very dark ones) or the acquisition procedure (clockwise and counter clockwise). Given test sequences, participants were asked to classify different indoor scenes, and judge the existence of the given objects within each image.</p><p>This paper describes the participation of our team in the Robot Vision Challenge. For the image features extraction part, we used the state-of-the-art Kernel Descriptors <ref type="bibr" coords="1,181.73,607.64,12.81,9.96" target="#b1">[2]</ref> framework, which has proven to be useful for many problems with RGB-D (visual and depth) information <ref type="bibr" coords="1,301.55,619.52,12.80,9.96" target="#b8">[9]</ref>. We applied L-SVM <ref type="bibr" coords="1,400.12,619.52,20.97,9.96" target="#b10">[11]</ref> for classification, and the temporal continuity is utilized during the classification stage.</p><p>The rest part of this paper is organized as follows. In Section 2, we briefly give a overview of our classification system. In Section 3, we describe in detail the image feature we used in our scheme. In Section 4 we describe how we apply classifier for both two tasks, and how we make use of the temporal continuity.</p><p>In Section 5, we give some of our experiments and show our final result on test sequence. In Section 6, we draw some conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview</head><p>In this section, we describe the procedure of our scheme. Both scene classification and object recognition tasks can be solved using classification framework based on supervised learning. The training stage for scene classification is shown in Fig. <ref type="figure" coords="2,152.39,512.00,4.41,9.96" target="#fig_0">1</ref> and the test stage is shown in Fig. <ref type="figure" coords="2,306.00,512.00,4.14,9.96" target="#fig_2">2</ref>. Framework for the recognition of each object is similar, except that training labels and predicted labels are replaced by the existence of each predefined object.</p><p>During the preprocessing stage of our scheme, the given 3D Point Cloud data are transformed to depth images,which afterwards will be treated as grayscale images. Then for both visual images and depth images, we extract Kernel Descriptors <ref type="bibr" coords="2,174.72,583.76,10.57,9.96" target="#b1">[2]</ref> as local descriptors, and use efficient match kernels (EMK) to transform and aggregate the descriptors to the features of images. We represent each frame by concatenating the two kinds of features extracted from each visual image and depth image. Then we choose L-SVM as our classifier for both scene classification and object recognition. In consideration of temporal continuity, we assign the averaged L-SVM scores of one frame's temporal neighbors to its final score. More details of our scheme will be described in the following sections.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Image Features</head><p>In this section, we describe the features of image, which have been used in our work. The feature extraction procedure consists of two steps. The first step is to design match kernels using pixel attributes, and the second is to learn compact features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Kernel descriptors</head><p>Kernel descriptors are able to generate rich patch-level features from different types of pixel attributes. For visual images, we use gradient, local binary pattern (LBP) <ref type="bibr" coords="3,158.69,501.80,14.36,9.96" target="#b7">[8]</ref> and color kernels. For depth images, we use depth gradient, depth LBP, spin/surface normal kernels. The gradient match kernel is:</p><formula xml:id="formula_0" coords="3,179.28,550.05,301.36,23.41">K grad (P, Q) = p∈P q∈Q m (p) m (q) k o θ (p) , θ (q) k p (p, q) ,<label>(1)</label></formula><p>where P and Q are the set of nearby points around the reference point p and p, respectively . k p (p, q) = exp -γ p p-q 2 is a Gaussian position kernel with z denoting the 2D position of a pixel in an image patch (normalized to [0, 1]), and k o (θ (p) , θ (q)) = exp -γ o θ (p) -θ (q) 2 is a Gaussian kernel over orientations.</p><p>Kernel view of orientation histograms provides a way to turn pixel attributes into patch-level features, which can also be extended to LBP match kernel:</p><formula xml:id="formula_1" coords="4,185.64,131.12,295.00,20.78">K LBP (P, Q) = p∈P q∈Q s (p) s (q) k b (b (p) , b (q)) k p (p, q) ,<label>(2)</label></formula><p>where s = s (p) / p∈P s (p) 2 + ǫ s ,s (p) is the standard deviation of values in the 3 × 3 neighborhood around p ,ǫ s is a small constant, and b (p) is a binary column vector which binarizes the pixel value differences in a local window around p. Similar to gradient and LBP kernels, the color match kernel can be formulated as:</p><formula xml:id="formula_2" coords="4,210.12,253.28,270.52,20.90">K col (P, Q) = p∈P q∈Q k c (c (p) , c (q)) k p (p, q) ,<label>(3)</label></formula><p>where c (p) is the pixel color at position z (intensity for gray images and RGB values for color images). k p (c (p) , c (q)) = exp(-γ c c (p)-c (q) 2 ) measures how similar two pixel values are.</p><p>Since depth images are treated as grayscale images, depth gradient and depth LBP kernels are constructed in a similar way like the gradient and LBP kernels for visual images. Here we just describe another one of the depth kernelsspin/surface normal kernel <ref type="bibr" coords="4,248.42,355.16,11.78,9.96" target="#b2">[3]</ref>.</p><p>In spin images <ref type="bibr" coords="4,209.40,367.40,12.78,9.96" target="#b5">[6]</ref>, a reference point in a local 3D point cloud is represented as the pair(p, n) formed by its 3D coordinate p and surface normal n. The spin image attribute of a point p ∈ P represented by the pair (p, n) is given by the triple [η p , ς p , β p ], where the elevation coordinate η p is the signed perpendicular distance from the point p to the tangent plane defined by the pair (p, n), the radial coordinate ς p is the perpendicular distance from the point p to the line through the normal n, and β p is the angle between the normals n and n. The point attributes [η p , ς p , β p ] can be aggregated into local shape features by the following kernel:</p><formula xml:id="formula_3" coords="4,185.64,485.37,295.00,23.41">K spin (P, Q) = p∈P q∈Q k a βp , βq k spin ([η p , ς p ] , [η q , ς q ]) ,<label>(4)</label></formula><p>where βp = [sin (β p ) , cos (β p )], P is the set of nearby points around the reference point p. Gaussian kernels k a and k spin are used to measure the similarities of attributes β, η and ς, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning Compact Features</head><p>Evaluating kernels is computationally expensive when image patches are large. For both computational efficiency and representational convenience, the feature can be extracted as following:</p><p>1. uniformly and densely sample sufficient basis vectors from support region to guarantee accurate approximation to match kernels.</p><p>2. learn compact basis vectors using kernel principal component analysis.</p><p>EMK combines the advantage of both bag-of-words and set kernels. Here we briefly describe how the EMK transforms kernel descriptors to low dimensional space to achieve compact features (see <ref type="bibr" coords="5,305.36,142.28,10.45,9.96" target="#b3">[4]</ref> for details).</p><p>Take feature based on gradient match kernel for example, other kinds of feature can be extracted in the same way. Rewriting the Eq.1:</p><formula xml:id="formula_4" coords="5,207.00,188.84,273.64,45.31">   k o θ (p) , θ (q) = φ o θ (p) ⊤ φ o θ (q) k p (p, q) = φ p (p) ⊤ φ p (q) ,<label>(5)</label></formula><p>the feature over image patches will be:</p><formula xml:id="formula_5" coords="5,217.20,256.89,259.19,23.42">F grad (P ) = p∈P m (p) φ o θ (p) ⊗ φ p (p) . (<label>6</label></formula><formula xml:id="formula_6" coords="5,476.38,259.52,4.25,9.96">)</formula><p>where ⊗ is the Kronecker product. A straightforward way to dimension reduction is to sample sufficient image patches from training images and perform KPCA for match kernels.</p><p>Sufficient Finite-dimensional Approximation Finite-dimensional features can be learned by projecting F grad (P ) into a set of basis vectors. A key issue in this projection process is how to choose a set of basis vectors which makes the finite-dimensional kernel approximate well the original kernel. Given a set of basis vectors {ϕ o (x i )} do i=1 where x i are sampled normalized gradient vectors, a infinite-dimensional vector can be approximated by a infinite-dimensional vector ϕ o (θ (p)) by its projection into the space spanned by the set of these d o basis vectors. Such a procedure is equivalent to using a finite-dimensional kernel:</p><formula xml:id="formula_7" coords="5,186.84,446.36,293.80,16.46">ko θ (p) , θ (q) = k o θ (p) , X K -1 o ⊤ ij k o θ (p ′ ) , X ,<label>(7)</label></formula><p>which can be rewritten as:</p><formula xml:id="formula_8" coords="5,186.12,494.84,294.52,16.70">ko θ (p) , θ (q) = Gk o θ (p) , X ⊤ Gk o θ (q) , X .<label>(8)</label></formula><p>Here</p><formula xml:id="formula_9" coords="5,134.76,524.24,345.97,44.66">k o θ (p) , X = k o θ (p) , x 1 , • • • , k o θ (p) , x do ⊤ is a d o ×1 vector, K o is a d o × d o matrix with K oij = k o (x i , x j ), and K = G ⊤ G. The resulting feature map ϕ o (θ (p)) = Gk o (θ (p) , X) is now only d o -dimensional.</formula><p>Compact Features The size of basis vectors can be further reduced by performing kernel principal component analysis over joint basis vectors:</p><formula xml:id="formula_10" coords="5,215.16,620.00,265.48,10.34">ϕ o (x 1 ) ⊗ ϕ p (y 1 ) , • • • , ϕ o (x do ) ⊗ ϕ p y dp ,<label>(9)</label></formula><p>where ϕ p (y s ) are basis vectors for the position kernel and d p is the number of basis vectors. The t-th kernel principal component can be written as:</p><formula xml:id="formula_11" coords="6,231.48,127.70,249.16,31.65">P C t = do i=1 dp j=1 α t ij φ o (x i ) ⊗ φ p (y j ) ,<label>(10)</label></formula><p>where α t ij is learned through kernel principal component analysis <ref type="bibr" coords="6,431.79,166.76,16.11,9.96" target="#b9">[10]</ref>. Under the framework of kernel principal component analysis, the gradient kernel descriptor for the patch P has the form:</p><formula xml:id="formula_12" coords="6,176.52,210.68,304.12,45.31">F grad (P ) = do i=1 dp j=1 α t ij    p∈P m (p) k o θ (p) , x i k p (p, y j )    .<label>(11)</label></formula><p>With patch descriptors available, we apply bag-of-words model and spatial pyramid <ref type="bibr" coords="6,174.00,264.32,10.57,9.96" target="#b6">[7]</ref> to get the final reprensentation of images. The details of parameter setting will be discussed in Sec.5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Classifier</head><p>In our work, we applied the LibLinear <ref type="bibr" coords="6,300.81,348.32,13.23,9.96" target="#b4">[5]</ref> as our classifier, since SVM is widely used for classification task and performs effective especially when the scale of data is small. For scene classification, we train a multiclass one-vs-all L-SVM classifier. As there are 10 different concepts of indoor scene, 10 binary L-SVMs are trained for each concept. For object recognition task, we treat the existence of each object as a binary classification problem. Frames that contain the predefined object are taken as positive samples, and the rest are taken as negative samples.</p><p>For better comprehension, let us introduce some notation here. Let I n be one image of the test sequence, n ∈ {1, 2, 3, • • • , N }, where N is the number of all images in the test sequence.</p><p>For scene classification, let S c n be the L-SVM output score for test image I n on concept c, c ∈ {1, 2, 3, • • • , C}, where C is the number of concepts, and C = 10 in this task. Then the predicted label c pred n of a test image I n is decided following the rule below:</p><formula xml:id="formula_13" coords="6,265.20,526.10,215.44,18.45">c pred n = argmax c S c n .<label>(12)</label></formula><p>For recognition of object obj k , k ∈ {1, 2, 3, • • • , K}, where K is the number of objects to be recognized, and K = 8 in this task. Let S n,k be the L-SVM output score of test image I n for obj k , and c pred n,k indicates the predicted concept of I n for obj k , where c pred n,k ∈ {-1, 1}, -1 for concept absence and 1 for concept occurrence. Whether a certain kind of object exists in the test image can be judged as below:</p><formula xml:id="formula_14" coords="6,251.04,625.04,225.15,51.31">c pred n,k =      1 0 -1 S n,k &gt; 0 S n,k = 0 S n,k &lt; 0 , (<label>13</label></formula><formula xml:id="formula_15" coords="6,476.19,641.36,4.45,9.96">)</formula><p>where the prediction 0 in Eq.( <ref type="formula" coords="7,284.65,118.28,9.30,9.96" target="#formula_14">13</ref>) means that whether the object exists or not is ambiguous, and we deal with this situation with not classifying it. This happens only when S n,k = 0, which means that for the test image I n , object obj k has the same confidence on both concept occurrence and absence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Consideration of Temporal Continuity</head><p>Since all the images in training and test sequences are captured continuously, it is reasonable and feasible to make full use of the temporal continuity. In our work, we apply a smoothing method for the L-SVM score to improve the classification performance.</p><p>We empirically think that the concept of an image is quite likely the same with that of its temporal neighbors, and the L-SVM score should be less changed compared to its neighbors. Based on this assumption, we smooth the L-SVM score for both scene classification and object recognition task as bellow:</p><formula xml:id="formula_16" coords="7,259.32,309.26,216.87,30.69">S c n = 1 2r + 1 n+r k=n-r S c k , (<label>14</label></formula><formula xml:id="formula_17" coords="7,476.19,319.16,4.45,9.96">)</formula><p>where r is the radius of smooth window. Eq.( <ref type="formula" coords="7,350.03,351.56,9.30,9.96" target="#formula_16">14</ref>) indicates that the final L-SVM score for image I n on a certain concept c is determined by all the scores of neighbors within the smoothing window. With all the L-SVM scores updated, we do classification and recognition on the basis of these new scores.</p><p>Choosing appropriate r is very important, for it has a relevance with the specific data and differs from scene classification and object recognition. The details of choosing r will be discussed in Sec. 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets and experimental setup</head><p>For Robot Vision Challenge this year, two training sequences are provided with 1947 and 3316 images respectively. An additional (labelled) validation sequence with 1869 images is also provided. The final test sequence involves 3315 unlabelled images. For all the sequences, RGB images and Point Cloud Data (PCD) are available. As mentioned in Sec.1, there can be variations in the lighting conditions or the acquisition procedure between test sequence and training sequences, and the validation sequence is similar to the test to some degree.</p><p>For depth features extraction, we transformed all the given PCDs into depth images, and crop the useless blank border. See the example in Fig. <ref type="figure" coords="7,425.29,595.64,4.14,9.96">3</ref>.</p><p>For the scene classification task, we pick the same size of images for each class in the training sequences to avoid the imbalance between semantic classes. In our work, we pick out the concept with minimum training data, count the number of training images in the concept and set this number as the size of training data for all the other concepts. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Point Cloud Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB image Preprocessing for PCD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Kernel descriptors selection</head><p>To choose appropriate kernel descriptors, we evaluate 6 kinds of kernel descriptors (gradient, LBP, color, depth gradient, depth LBP, spin/surface normal) through scene classification task on validation sequence. Fig. <ref type="figure" coords="8,404.74,486.08,4.41,9.96">4</ref> shows the evaluation of all the 6 kernel descriptors. Depth kernel descriptors perform a little better than visual kernel descriptors, due to the variations in the lighting conditions between the training sequences and validation sequence. For the final test, we select the three descriptors (gradient, depth gradient, depth LBP) which get the highest classification accuracy on validation sequence, since the test sequence is similar with validation sequence.</p><p>After 3 optimal kernel descriptors are chosen, we apply spatial pyramid (1, 2× 2, 3×3) and perform the EMK transform with 1000 words. Then we get the image feature with a total length of 1000 × 1 + 2 2 + 3 2 × 3 = 42000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Smoothing window radius</head><p>As explained in Sec.4.2, the radius of smoothing window is important for the final performance. We perform experiments on different radius r for validation se-  quences, and choose the r which corresponds to the highest accuracy. Fig. <ref type="figure" coords="9,451.63,298.40,28.86,9.96" target="#fig_5">5 (left)</ref> shows how the scene classification accuracy varies with the radius of smoothing window r. We set the step width as 5, and find out that the accuracy reaches the climax when r is around 25.</p><p>Since the radius is related to the length of continuous scene images with the same concept, and there is a proportion between the quantity of validation images and test images, the estimated radius r should be multiplied the proportion to fit the test sequence. According to Eq.15, we get the estimated radius r scene test = 44 for test sequence.</p><formula xml:id="formula_18" coords="9,236.04,425.97,244.60,23.89">r scene test = r scene validation × N test N validation .<label>(15)</label></formula><p>For object recognition, Fig. <ref type="figure" coords="9,269.75,471.08,4.41,9.96" target="#fig_5">5</ref> (right) shows how the object recognition accuracy varies with the radius of smoothing window r, and it reaches the climax when r is around 10. Then we use the same method to get the estimated radius r object test = 18 for test sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results on validation sequence</head><p>We applied our method on the validation sequence, and computed the classification accuracy for each scene concept and object category. See Table <ref type="table" coords="9,455.63,607.64,4.98,9.96" target="#tab_0">1</ref> and Table <ref type="table" coords="9,162.60,619.52,4.98,9.96" target="#tab_1">2</ref> for detailed results. We notice that our method has good performance on each concept except 'StudentOffice' and 'TechnicalRoom'. This is due to the large variation of luminance between training and validation sequences on these two concepts images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Results on Robot Vision task</head><p>In the 5th edition of the Robot Vision Challenge,Our team ranked the first out of six participants, results are listed in Table <ref type="table" coords="10,333.23,332.24,3.91,9.96" target="#tab_2">3</ref>. </p><formula xml:id="formula_19" coords="10,187.68,365.27,7.68,8.98">#</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper we present our scheme on the 5th Robot Vision Challenge. Our approach leverages the state-of-the-art methods in the fields of RGB-D image classification. Among all the participants for the Challenge, our team ranked the first, showing the effectiveness of our approach. Since the predefined objects have high dependence on indoor scenes, we achieve high accuracy on object recognition by using the representation of whole image. This method is useful for the specific task of this Challenge, and we plan to investigate more effective methods to better tackle general object recognition problem in future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,217.92,350.51,179.32,8.98"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Training stage of scene classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,226.32,324.47,162.52,8.98"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Test stage of scene classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,134.76,204.71,345.84,8.98;8,134.76,215.63,175.24,8.98"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig. 3. (left) PCDs are transformed into depth images without blank border, (right) the RGB image is shown here for reference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="9,134.76,245.51,345.88,8.98;9,134.76,256.43,219.04,8.98"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. (left)Variation of scene classification accuracy with r on validation sequence. (right)Variation of object recognition accuracy with r.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="10,134.76,118.72,345.88,137.70"><head>Table 1 .</head><label>1</label><figDesc>Scene classification results on validation sequence. Here * means that there is no images with concept Hall, VisioConference or Warehouse in the validation sequence.</figDesc><table coords="10,140.04,118.72,335.38,137.70"><row><cell cols="2">Scene Concept Corridor</cell><cell>Hall</cell><cell cols="4">ProfessorOffice StudentOffice TechnicalRoom Toilet</cell></row><row><cell>Accuracy</cell><cell>0.986</cell><cell>*</cell><cell>0.850</cell><cell>0.472</cell><cell>0.453</cell><cell>0.979</cell></row><row><cell cols="4">Scene Concept Secretary VisioConference Warehouse</cell><cell>ElevatorArea</cell><cell>Total</cell><cell></cell></row><row><cell>Accuracy</cell><cell>0.934</cell><cell>*</cell><cell>*</cell><cell>1.000</cell><cell>0.827</cell><cell></cell></row><row><cell cols="7">Object Category Extinguisher Computer Chair Printer Urinal Fridge Screen Trash Total</cell></row><row><cell>Accuracy</cell><cell>0.949</cell><cell>0.884</cell><cell cols="4">0.907 0.899 0.935 0.998 0.919 0.960 0.935</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,187.32,259.07,240.77,8.98"><head>Table 2 .</head><label>2</label><figDesc>Object recognition results on validation sequence.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="10,189.12,365.27,238.66,87.94"><head>Table 3 .</head><label>3</label><figDesc>Robot Vision final results.</figDesc><table coords="10,189.12,365.27,238.66,77.14"><row><cell></cell><cell>Group</cell><cell cols="3">Score Class Score Objects SCORE TOTAL</cell></row><row><cell cols="3">1 MIAR ICT 3168.5</cell><cell>2865.000</cell><cell>6033.500</cell></row><row><cell>2</cell><cell>NUDT</cell><cell>3002.0</cell><cell>2720.500</cell><cell>5722.500</cell></row><row><cell>3</cell><cell>SIMD*</cell><cell>1988.0</cell><cell>3016.750</cell><cell>5004.750</cell></row><row><cell>4</cell><cell>REGIM</cell><cell>2223.5</cell><cell>2414.750</cell><cell>4638.250</cell></row><row><cell>5</cell><cell>MICA</cell><cell>2063.0</cell><cell>2416.875</cell><cell>4479.875</cell></row><row><cell>6</cell><cell>GRAM</cell><cell>-487.0</cell><cell>0.000</cell><cell>-487.000</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was supported in part by <rs type="funder">National Basic Research Program of China</rs> (<rs type="programName">973 Program</rs>): <rs type="grantNumber">2012CB316400</rs>, in part by <rs type="funder">National Natural Science Founda-tion of China</rs>: <rs type="grantNumber">61070108</rs>, and <rs type="grantNumber">61035001</rs>, in part by the <rs type="funder">Key Technologies R&amp;D Program of China</rs> under Grant no. <rs type="grantNumber">2012BAH18B02</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_DuJyfYr">
					<idno type="grant-number">2012CB316400</idno>
					<orgName type="program" subtype="full">973 Program</orgName>
				</org>
				<org type="funding" xml:id="_SEJ96Qz">
					<idno type="grant-number">61070108</idno>
				</org>
				<org type="funding" xml:id="_5GS2Jew">
					<idno type="grant-number">61035001</idno>
				</org>
				<org type="funding" xml:id="_qf59RTM">
					<idno type="grant-number">2012BAH18B02</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="11,142.88,185.87,223.01,8.98" xml:id="b0">
	<monogr>
		<ptr target="http://www.xbox.com/en-us/kinect" />
		<title level="m" coord="11,151.56,186.17,65.80,8.67">Microsoft Kinect</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.88,196.79,337.52,8.98;11,151.56,207.71,246.28,8.98" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,320.27,196.79,156.21,8.98">Kernel descriptors for visual recognition</title>
		<author>
			<persName coords=""><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaofeng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,151.56,208.01,207.67,8.67">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.88,218.75,337.60,8.98;11,151.56,229.67,328.85,8.98;11,151.56,240.59,200.80,8.98" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,335.99,218.75,144.49,8.98;11,151.56,229.67,43.05,8.98">Depth kernel descriptors for object recognition</title>
		<author>
			<persName coords=""><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaofeng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,214.92,229.97,265.49,8.67;11,151.56,240.89,81.95,8.67">Intelligent Robots and Systems (IROS), 2011 IEEE/RSJ International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="821" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.88,251.63,337.66,8.98;11,151.56,262.55,328.84,8.98;11,151.56,273.47,42.77,8.98" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,320.75,251.63,159.78,8.98;11,151.56,262.55,116.85,8.98">Efficient match kernel between sets of features for visual recognition</title>
		<author>
			<persName coords=""><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,275.76,262.85,200.53,8.67">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.88,284.39,337.76,8.98;11,151.56,295.43,328.99,8.98;11,151.56,306.35,115.73,8.98" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,151.56,295.43,188.96,8.98">Liblinear: A library for large linear classification</title>
		<author>
			<persName coords=""><forename type="first">Rong-En</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiang-Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,347.28,295.73,133.27,8.67;11,151.56,306.65,33.20,8.67">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.88,317.27,337.60,8.98;11,151.56,328.31,329.01,8.98;11,151.56,339.23,177.90,8.98" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,327.12,317.27,153.35,8.98;11,151.56,328.31,138.37,8.98">Using spin images for efficient object recognition in cluttered 3d scenes</title>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,301.68,328.61,175.16,8.67;11,151.56,339.53,88.43,8.67">Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="433" to="449" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note>IEEE Transactions on</note>
</biblStruct>

<biblStruct coords="11,142.88,350.15,337.65,8.98;11,151.56,361.19,328.97,8.98;11,151.56,372.11,328.99,8.98;11,151.56,383.03,151.01,8.98" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,379.56,350.15,100.97,8.98;11,151.56,361.19,267.80,8.98">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName coords=""><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,441.00,361.49,39.53,8.67;11,151.56,372.41,124.33,8.67;11,304.73,372.41,154.25,8.67">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
	<note>IEEE Computer Society Conference on</note>
</biblStruct>

<biblStruct coords="11,142.88,394.07,337.63,8.98;11,151.56,404.99,328.82,8.98;11,151.56,415.91,321.90,8.98" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,374.87,394.07,105.63,8.98;11,151.56,404.99,288.13,8.98">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName coords=""><forename type="first">Timo</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matti</forename><surname>Pietikainen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Topi</forename><surname>Maenpaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,450.24,405.29,30.14,8.67;11,151.56,416.21,232.44,8.67">Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>IEEE Transactions on</note>
</biblStruct>

<biblStruct coords="11,142.88,426.95,337.68,8.98;11,151.56,437.87,329.11,8.98;11,151.56,448.79,183.89,8.98" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,330.95,426.95,149.61,8.98;11,151.56,437.87,40.83,8.98">Rgb-(d) scene labeling: Features and algorithms</title>
		<author>
			<persName coords=""><forename type="first">Xiaofeng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,216.36,438.17,264.31,8.67;11,151.56,449.09,55.90,8.67">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2759" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.54,459.83,338.00,8.98;11,151.56,470.75,328.74,8.98;11,151.56,481.67,44.81,8.98" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,418.91,459.83,61.63,8.98;11,151.56,470.75,187.97,8.98">Nonlinear component analysis as a kernel eigenvalue problem</title>
		<author>
			<persName coords=""><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,348.96,471.05,77.22,8.67">Neural computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1299" to="1319" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.54,492.71,307.61,8.98" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="11,225.24,493.01,158.08,8.67">The nature of statistical learning theory</title>
		<author>
			<persName coords=""><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
