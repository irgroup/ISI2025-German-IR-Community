<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,139.82,115.96,335.73,12.62">The ImageCLEF 2013 Plant Identification Task</title>
				<funder>
					<orgName type="full">Agropolis</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,135.45,153.91,56.55,8.74"><forename type="first">Hervé</forename><surname>Goëau</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">INRIA</orgName>
								<orgName type="institution" key="instit2">IMEDIA &amp; ZENITH teams</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,202.56,153.91,60.95,8.74"><forename type="first">Pierre</forename><surname>Bonnet</surname></persName>
							<email>pierre.bonnet@cirad.fr</email>
							<affiliation key="aff1">
								<orgName type="laboratory">CIRAD</orgName>
								<orgName type="institution">UMR AMAP</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,274.05,153.91,48.08,8.74"><forename type="first">Alexis</forename><surname>Joly</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">INRIA</orgName>
								<orgName type="institution" key="instit2">IMEDIA &amp; ZENITH teams</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,332.68,153.91,47.76,8.74"><forename type="first">Vera</forename><surname>Bakic</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">INRIA</orgName>
								<orgName type="institution" key="instit2">IMEDIA &amp; ZENITH teams</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,391.00,153.91,81.67,8.74"><forename type="first">Daniel</forename><surname>Barthelemy</surname></persName>
							<email>daniel.barthelemy@cirad.fr</email>
							<affiliation key="aff2">
								<orgName type="department">BIOS Direction and INRA</orgName>
								<orgName type="laboratory">CIRAD</orgName>
								<orgName type="institution">UMR AMAP</orgName>
								<address>
									<postCode>F-34398</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,206.93,165.87,74.02,8.74"><forename type="first">Nozha</forename><surname>Boujemaa</surname></persName>
							<email>boujemaa@inria.fr</email>
							<affiliation key="aff4">
								<orgName type="department">Direction of Saclay Center</orgName>
								<orgName type="institution" key="instit1">INRIA</orgName>
								<orgName type="institution" key="instit2">nozha</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,310.89,165.87,93.06,8.74"><forename type="first">Jean-François</forename><surname>Molino</surname></persName>
							<email>jean-francois.molino@ird.fr</email>
							<affiliation key="aff3">
								<orgName type="laboratory">IRD</orgName>
								<orgName type="institution">UMR AMAP</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,139.82,115.96,335.73,12.62">The ImageCLEF 2013 Plant Identification Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">78258E224B41F2D3BBA6328945D812B8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ImageCLEF</term>
					<term>plant</term>
					<term>leaves</term>
					<term>leaf</term>
					<term>flowers</term>
					<term>fruits</term>
					<term>bark</term>
					<term>stem</term>
					<term>species</term>
					<term>retrieval</term>
					<term>images</term>
					<term>collection</term>
					<term>identification</term>
					<term>fine-grained classification</term>
					<term>evaluation</term>
					<term>benchmark</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The ImageCLEFs plant identification task provides a testbed for a system-oriented evaluation of plant identification about 250 species trees and herbaceous plants based on detailed views of leaves, flowers, fruits, stems and bark or some entire views of the plants. Two types of image content are considered: SheetAsBackgroud which contains only leaves in a front of a generally white uniform background, and NaturalBackground which contains the 5 kinds of detailed views with unconstrained conditions, directly photographed on the plant. The main originality of this data is that it was specifically built through a citizen sciences initiative conducted by Tela Botanica, a French social network of amateur and expert botanists. This makes the task closer to the conditions of a realworld application. This overview presents more precisely the resources and assessments of task, summarizes the retrieval approaches employed by the participating groups, and provides an analysis of the main evaluation results. With a total of twelve groups from nine countries and with a total of thirty three runs submitted, involving distinct and original methods, this third year task confirms Image Retrieval community interest for biodiversity and botany, and highlights further challenging studies in plant identification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Convergence of multidisciplinary research is a key to answer profound challenges of humanity related to health, biodiversity or sustainable energy. The integration of life sciences and computer sciences has a major role to play towards managing and analyzing cross-disciplinary scientific data at a global scale. More specifically, building accurate knowledge of the identity, geographic distribution and uses of plants is essential if agricultural development is to be successful and biodiversity is to be conserved. Unfortunately, such basic information is often only partially available for professional stakeholders, scientists and citizens, and often incomplete for ecosystems that possess the highest plant diversity. A noticeable consequence, expressed as the taxonomic gap, is that identifying plant species is usually impossible for the general public, and often a difficult task for professionals, such as farmers or wood exploiters and even for the botanists themselves. The only way to overcome this problem is to speed up the collection and integration of raw observation data, while simultaneously providing to potential users an easy and efficient access to this botanical knowledge. In this context, content-based visual identification of plant's images is considered as one of the most promising solution to help bridging the taxonomic gap. Evaluating recent advances of the Image Retrieval community on this challenging task is therefore an important issue. This paper presents the plant identification task that was organized for the third year running within ImageCLEF <ref type="foot" coords="2,279.17,296.75,3.97,6.12" target="#foot_0">6</ref> [10] dedicated to the system-oriented evaluation of visual based plant identification. Like previous year, the task is more related to a retrieval task instead of a pure classification task in order to consider a ranked list of retrieved species rather than a single brute determination. Visual content was being the main available information but with additional information including contextual meta-data (author, date, locality name and geotag, names at different taxonomic ranks) and some EXIF data. Each year try to take to the next level the challenge to a more realistic scenario by covering progressively one entire flora at the scale of one wide region like France. After two years focused exclusively on leaves mainly from Mediterranean tree species, the task focused this year on 250 species of herbs and trees species living in France with different views or organs of plants: photographs of flowers, fruits, barks, leaves and the entire view of the plants. Finally, it was two types of content which were considered: a SheetAsBackground category containing scans and scan-like photographs of leaves in a front of a generally white uniform white background, and a NaturalBackground with most of the time a cluttered natural background of the 5 types of organs. The main originality of this data is that it was specifically built through a citizen sciences initiative conducted by Tela Botanica<ref type="foot" coords="2,433.37,499.98,3.97,6.12" target="#foot_1">7</ref> , a French social network of amateur and expert botanists. This makes the task closer to the conditions of a real-world application: (i) organs of the same species are coming from distinct plants living in distinct areas and with at distinct growing stages, (ii) pictures and scans are taken by different users that might not used the same protocol to collect the leaves and/or acquire the images, (iii) pictures and scans are taken at different periods in the year.</p><p>2 Task resources 2. <ref type="bibr" coords="3,144.52,142.27,4.88,8.77" target="#b0">1</ref> The Pl@ntView dataset Building effective computer vision and machine learning techniques is not the only side of the taxonomic gap problem. Speeding-up the collection of raw observation data is clearly another crucial one. The most promising approach in that way is to build real-world collaborative systems allowing any user to enrich the global visual botanical knowledge <ref type="bibr" coords="3,286.42,209.45,14.61,8.74" target="#b12">[15]</ref>. To build the evaluation data of Image-CLEF plant identification task, we therefore set up a citizen science project around the identification of common woody species covering the Metropolitan French territory. This was done in collaboration with Tela Botanica social network and with researchers specialized in computational botany. Technically, images and associated tags were collected through a crowd-sourcing web applications <ref type="bibr" coords="3,212.02,281.18,14.61,8.74" target="#b12">[15]</ref>, <ref type="bibr" coords="3,234.43,281.18,15.50,8.74">[13]</ref> and were all validated by expert botanists. Several cycles of such collaborative data collection and taxonomical validation occurred. Scans of leaves were the first type of pictures collected thanks to the work of active contributors from Tela Botanica since the summer 2009. The idea of collecting only scans of leaves first was to initialize training data with limited noisy background and to focus on plant variability rather than mixed plant and view conditions variability. This allowed to collect a first dataset of 2228 scans over 55 species. A first public crowd-sourcing web application<ref type="foot" coords="3,386.18,363.29,3.97,6.12" target="#foot_2">8</ref> was then opened in October 2010 and additional data were collected up to March 2011. The new collected images were either scans, or photographs with uniform background (referred as scan-like photos), or unconstrained photographs with natural background. It involved besides 15 new species from the previous set of 55 species. In April 2011 a new version of the web application has opened <ref type="foot" coords="3,391.36,423.07,3.97,6.12" target="#foot_3">9</ref> and the acquisition protocol was extended to 4 more types of views with a natural background mentioned below, and focusing to the same limited set of species. During the last two years, members from Tela Botanica contribute regularly every month on more and more species, the final ambition being to cover the entire vascular French flora (around 6000 species) with numerous pictures of different plant organs, with numerous plant observations spread all over France at different growing stages photographed by a crowd of photographers, introducing slowly over the months great visual and morphological variabilities. However, for each year task, we decided to limit the number of species in the task by keeping only the most populated ones in terms of images and plant observations. This is why like the first year we decided to focus again only on leaves during the ImageCLEF 2012 Plant Identification task with a number of 125 species, because at the time of the task we didn't collected sufficiently pictures of complementary organs. This year, we decided to propose to add these complementary views while we added to the previous dataset Pl@ntLeaves 125 new more species more focusing on herbaceous plants than threes in order to cover more diversity of the French flora. Complementary views concerning flowers, fruits, stems and entire views associated to previous species and previous plant observations yet contained in the 2012 dataset were also added. Finally, the Pl@ntView dataset used within ImageCLEF2013 plant task contained 26077 images collected by 327 distinct contributors: 11031 for the SheetAsBackground category and 15046 for the Na-turalBackground (in more details 16% of leaves, 18% of flowers, 8% of fruits, 8% of stems and 8% of entire plant). The figure 2.1 gives some examples illustrating the type of views, but illustrating also the fact that a species does not contain systematically at least one image for each organ. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pl@ntView metadata</head><p>Each image of Pl@ntView dataset is associated with the following meta-data:</p><p>-IndividualPlantID: plant observation identifier -Date: date and time of plant observation -Type: SheetAsBackground or NaturalBackground -Content: Flower, Fruit), Leaf, Stem or Entire -Taxon: full taxon name according the botanical database[4](Regnum, Class, Subclass, Superorder, Order, Family, Genus, Species) -ClassId : species identifier -VernacularNames: English common name -Author name of the author of the picture -Organization name of the organization of the author -Locality locality name (a district or a country division or a region) -GPSLocality GPS coordinates of the locality.</p><p>Concerning the locality information, note that sometimes the GPS can be very imprecise when the locality was not mentioned: in this case we used the GPS coordinates of the district or the country division or a region, according to the level of information available. Metadata is stored in independent xml files, one for each image. Additional but partial meta-data information can be found in the image's EXIF, and might include the camera or the scanner model, the image resolution and dimension, the optical parameters, the white balance, the light measures, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">About other plant datasets</head><p>A crucial added-value of this collection over older ones used in the literature (such as Swedish <ref type="bibr" coords="5,214.42,276.86,14.61,8.74" target="#b18">[22]</ref>, ICL <ref type="bibr" coords="5,258.41,276.86,9.96,8.74" target="#b0">[1]</ref>, Flavia <ref type="bibr" coords="5,307.39,276.86,15.50,8.74" target="#b19">[23]</ref> or Smithsonian <ref type="bibr" coords="5,398.58,276.86,10.30,8.74" target="#b4">[7]</ref>), is that it was built in a collaborative manner, through a citizen sciences initiative, and in collaboration with a well established social network specialized in botany. This makes it closer to the conditions of a real-world application: (i) pictures of organs of the same species are coming from distinct plants living in distinct areas (ii) pictures and scans are taken by different users that might not used the same protocol to collect the leaves and/or acquire the images (iii) pictures and scans are taken at different periods in the year. Intra-species visual variability and view conditions variability are therefore more stressed-out. In the end, this makes our identification challenge much more realistic but also more complex. We can mention here two other challenging datasets, the OxfordFlower <ref type="bibr" coords="5,405.15,396.41,18.80,8.74" target="#b16">[19]</ref> dataset, and the MobileFlora <ref type="bibr" coords="5,210.29,408.36,10.52,8.74" target="#b2">[5]</ref> one, which are indirectly built in a collaborative manner through web crawling but without (or very partially), contextual information like the author, the location, the date, etc. They unfortunately also come with a set of drawbacks or unrealistic properties: (i) they include only flower images (ii) they focus on the most represented species on the web rather than the most represented in a given area (iii) the definition of the taxonomic classes is not rigorous (sometimes genus, sometimes species, sometimes nothing well defined). Finally, the plant branch of the huge crowdsourced dataset ImageNet <ref type="bibr" coords="5,434.71,492.05,19.19,8.74" target="#b9">[12]</ref> could be interesting for our problem but it unfortunately contains too much errors, noisy classes and too sparse tags (typically about the type of view or the depicted organ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Pl@ntViews variability</head><p>The ImageCLEF 2012 overview <ref type="bibr" coords="5,274.37,578.18,15.50,8.74" target="#b11">[14]</ref> provided numerous illustrations of the wide visual variability of the leaves. We present here more visual variability concerning the new introduced organs.</p><p>Flower There is a great diversity within flowers and it is an intensive subject of studies by botanists since the flower is often the key for identify a species. Flowers of the dataset can be categorized according to the color (see figure <ref type="figure" coords="5,468.97,656.12,3.87,8.74">2</ref>), the symmetry (see figure <ref type="figure" coords="6,250.07,118.99,3.87,8.74">3</ref>), the number of petals (see figure <ref type="figure" coords="6,413.41,118.99,4.43,8.74">4</ref>) and the size (see figure <ref type="figure" coords="6,183.95,130.95,3.87,8.74" target="#fig_2">5</ref>). Most of the time one species is associated to one category, but there are exceptions like in figure <ref type="figure" coords="6,284.74,142.90,4.98,8.74">6</ref> where for one same species the flowers can have different colors.</p><p>Besides this first categorisation, botanists studied the Brown White Green Rose Blue Yellow inflorescence, i.e. the internal structure of the flower and the organisation of the flowers on a plant. Species from a same taxonomical group generally share a same organization, and thus a same visual appearance. Figure <ref type="figure" coords="6,417.45,565.98,4.98,8.74">7</ref> gives all the type of inflorescence contained in the dataset. Some type of inflorescence can be very noticeable and very typical of a group of species. However, at the opposite some very distinct groups of species in the taxonomical hierarchy can have a very distinct visual appearance, but sharing a same type of inflorescence.</p><p>Fruit The fruit is the transformation of the flower and it can be also categorized into distinct types. The figure <ref type="figure" coords="6,269.18,656.12,4.98,8.74">8</ref> shows the great diversity of type of fruits that  are represented through the 250 species of the Pl@ntView dataset. The different modes of dissemination gives a second complementary and interesting way to show the visual diversity of the fruits in the dataset. Indeed, a same mode of dissemination of (even very) distinct species involves generally some same morphological features. For instance, for the endozoochory dissemination (seed dispersal by animals) the fruits are generally colored for attracting birds for instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stem</head><p>The stem is generally a difficult plant sub-part for identifying a species, maybe because the visual information is mainly expressed with the texture, less by the color and or the shape. Age of the plant is second difficulty for analysing the stem, more precisely for the trees and theirs barks. Through the collaborative process, the dataset contains for numerous species, different plants at different ages and fill partially the wide diversity of the barks. The figure <ref type="figure" coords="8,431.41,510.54,9.96,8.74" target="#fig_5">10</ref> shows a representative example for the species Robinia pseudo-acacia with young and old trees: more the tree is young more it has some thorns as a strategy of defence. Entire The entire view is maybe the most difficult view for identifying with precision a species, because this kind of view generally does not contain sufficiently information, and because a same species can have a very different general appearance depending to the geographical and climatic conditions. However, more the plant is small (young or intrinsically small), more the "useful" organs for identification are visible. The figure <ref type="figure" coords="9,295.32,178.77,9.96,8.74" target="#fig_6">11</ref> shows one big tree of Magnolia grandiflora L. where it is very difficult or even impossible for identifying the pant if we look the entire view. The second plant is a small herbaceous species of Gentiana pneumonanthe L. where we can see that the flower is very visible on the entire view for identification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Species Flower Entire</head><p>Magnolia grandiflora L.</p><p>Gentiana pneumonanthe L. 3 Task description</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training and Test data</head><p>The precise goal of the task was to retrieve the correct species among the top k species of a ranked list of returned species, one list for each image of a test dataset. Participants received a first training set of annotated images in order to explore different techniques and train their system. Six weeks later participants received the test set containing images without species labels, but with the view type, organ type, author, organization and plant identifier tags. Then, 2 months later, participants were allowed to submit up to 4 run files, most of the time related to variations of the same method. A particular attention was paid when splitting the data into training and test subsets to avoid any bias. Several pictures in the dataset might actually depict the same individual plant (or neighboring plants) observed in the same conditions (same person, day, device, lightening conditions, etc.). Randomly splitting images in a nave way would therefore favor having such near-duplicate images in both the training and the test subsets, making the recognition much more easy. To avoid this bias, we therefore performed our random split at the observation level rather than at the image level thanks to associated metadata (observation id when available, author, date, etc. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Task objective and evaluation metric</head><p>According to similar concerns, the primary metric used to evaluate the submitted runs uses a two-stage average of raw image scores, one at the observation level (i.e. we compute the average score of all images belonging to the same observed plant), and one at the user level (i.e. we average the scores of the observations of a given user). A flat mean would actually have introduce some new bias with regard to a real world identification system. Indeed, as the dataset was built in a collaborative manner, it appears that few contributors often provide much more pictures than many other contributors who provided few (long tail distribution). Since we want to evaluate the ability of a system to provide correct answers to any user, we rather measure the mean of the average classification score per author. Furthermore, some authors sometimes provided many pictures of the same individual plant (to enrich training data with less efforts). Since we want to evaluate the ability of a system to provide the correct answer based on a single plant observation, we also decided to average the classification rate on each individual plant. The raw image score itself is computed for each test image as the inverse of the rank of the correct species in the list of retrieved species.</p><p>More formally, our primary metric was defined as the following average score S:</p><formula xml:id="formula_0" coords="11,234.63,139.22,245.97,31.28">S = 1 U U u=1 1 P u Pu p=1 1 N u,p Nu,p n=1 s u,p,n<label>(1)</label></formula><p>U : number of users (who have at least one image in the test data) P u : number of individual plants observed by the u-th user N u,p : number of pictures taken from the p-th plant observed by the u-th user s u,p,n : score between 1 and 0 equals to the inverse of the rank of the correct species for the n-th picture taken from the p-th plant observed by the u-th user It is important to notice that while making the task more realistic, the normalized classification score also makes it more difficult. Indeed, it works as if a bias was introduced between the statistics of the training data and the one of the test data. It highlights the fact that bias-robust machine learning and computer vision methods should be preferred to train such real-world collaborative data. Finally, to isolate and evaluate the impact of the image acquisition type (Sheet-AsBackground, NaturalBackground, a normalized classification score S was computed for each type separately. Participants were therefore allowed to train distinct classifiers, use different training subsets or use distinct methods for each data type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Participants and techniques</head><p>With 12 finalist groups coming from all around the world over 9 countries and 33 submitted runs, the 2013 edition of the task confirmed its increasing attractiveness (respectively 10 and 11 groups crossed the finish line in 2011 and 2012) although its complexity was higher (with heterogeneous view types). Participants were mainly academics, specialized in computer vision and multimedia information retrieval. We list below the participants and give a brief overview of the techniques they used in their runs. We remind here that ImageCLEF benchmark is a system-oriented evaluation and not a deep or fine evaluation of the underlying algorithms. Readers interested by the scientific and technical details of any of these methods should refer to the ImageCLEF 2013 working notes of each participant (referenced below): AGSPPR (3 runs) <ref type="bibr" coords="11,233.08,536.54,16.80,8.77" target="#b21">[25]</ref>, China. AGSPPR team focused their work on the SheetAsBackground category and submitted 3 runs using distinct visual features and approaches: a global shape feature (i.e. the leafs area length of major axis and length of minor axis), SIFT features (run 2), and an extension of the CENTRIST approach (CENsus Transform hISTogram []) called SPACT designed for reducing the number of descriptors with a PCA algorithm (run 3). For run 1 and 3, they used a multiclass Support Vector Machine (SVM) classifier with a radial basis kernel function, while they used a pure matching approach for the 2nd run. A two-stage late fusion scheme is then apply to combine the image response lists of the different modalities and of the different types of view. Metadata was also successfully used (in run 2), in particular the date for the flower category and the plant observation identifiers (to share the query images of the same plant). LAPI (1 run) <ref type="bibr" coords="12,206.97,448.41,11.46,8.77" target="#b6">[9]</ref>, Romania. LAPI team proposed to exploit a complex approach for image description based on contour extraction, curve partitioning and abstraction. They suggest that their approach is a "structural alternative" to the prevailing gradient-based features (e.g. SIFT). Contrary to other teams, they considered a more difficult task by automatically recognizing the view type before recognizing the plant species. They used a classical Linear Discriminant Analysis (LDA) as classifier for both the view type recognition and the species prediction.</p><p>LIRIS REVES (2 runs) <ref type="bibr" coords="12,253.82,546.27,16.80,8.77" target="#b8">[11]</ref>, France. ReVes team used the same supervised model-based segmentation strategy than the one they used during the 2012 leaforiented campaign and tried to extend it to the other types of view (although it was more difficult to build a priori shape models of that organs). They used a late fusion approach to combine the decisions of the classifiers of each modality as well as to combine the multiple images of a given individual plant when this occurred in the query set. They finally attempted to use the geo-tags available in the metadata by interpolating them thanks to external climatic data. MICA (3 runs) <ref type="bibr" coords="12,220.10,644.13,16.80,8.77" target="#b14">[17]</ref>, Vietnam. MICA team experimented 3 distinct approaches. Run 1 used a GIST descriptor with a k-nearest neighbors rule on all types of view. Run2 was based on the same approach but with additional color and texture features for the Flower and Entire types of view. Run3 used a Bag of visual Words (BoW) approach based on SURF local features and an "un-sharp masking" pre-processing step to filter some background information.</p><p>Classification was achieved through a multi-class SVM. NLAB UTOKYO (3 runs) <ref type="bibr" coords="13,268.24,181.51,16.80,8.77" target="#b15">[18]</ref>, Japan. NLAB participant focused his work on visual features learning for building accurate image descriptions. A set of local features, mostly SIFT variations and a Self Similarity descriptor, were densely extracted in each picture according to a regular grid and then "augmented" with a supervised polynomial embedding technique taking into account neighboring local features. Further, these locally embedded and augmented features were encoded into a global Fisher Vector representation which allows an accurate classification with any linear classifier. In this work, linear logistic regression models were used. An independent classifier was trained for each raw descriptor and a late-fusion based an average log-likelihood of posterior probabilities was used to merge independent classifier results. SABANCI-OKAN (1 run) [24], Turkey. This team submitted only one run using distinct features for the two categories. For the SheetAsBackground category, an automatic segmentation was performed using edge preserving morphological simplification by means of area attribute filters, followed by an adaptive threshold. Then, a variety of shape and texture features were extracted (the same than the ones used during the 2012 campaign). For the NaturalBackground category, a set of global features was extracted: HSV color auto-correlograms, weigthed-saturation hue histogram and other texture descriptors, depending on the considered organ. For the Flower, Fruit and Entire view types only color features were used, while for the Stem view type, texture features were used after a segmentation preprocessing step. The dates provided in the metadata were also exploited for the three first view types (that are likely to be more time dependent). Classification was performed through independent SVM classifiers, one for each view type. SCG USP (4 runs), Brazil. This team submitted one run with a fully automatic approach (run 1) and three other ones runs involving human assistance for a background/foreground segmentation. More precisely, training pictures were segmented with the semi-supervised Grabcut algorithm, while test images were manually segmented. Then, numerous features were extracted: Gabor, LBP, fractal, geometrical features. The final classification step was performed with a LDA classifier, except for the 3rd run where a SVM classifier was used. Only the 4th run tried to train independent classifiers (i.e. one for each view type). UIAC (3 runs) <ref type="bibr" coords="13,212.50,584.36,16.80,8.77" target="#b17">[21]</ref>, Romania. Unlike the other groups, UAIC explored the strategy of integrating additional external training data to boost their performances. They actually crawled 507 additional pictures from Wikimedia Commons with relevant annotations. And this confirms the difficulty of collecting dense and accurate data specific to a given flora. From the technological point of view, they used the LIRe (Lucene Image Retrieval) engine and, after preliminary tests, they selected the Joint Composite Descriptor (JCD). The LIRe engines gives for each test image a list of training images where a candidate species potentially appears several times. Thus, they used 3 distinct approaches of combination in order to obtain a single score for each species: a max operator, a normalized sum, and a naive Bayes classifier. The results were further refined and ranked based on GPS metadata, author names and organization tags, assuming that certain authors and organizations would have a greater interest in certain plant species. Table <ref type="table" coords="14,178.04,203.08,4.98,8.74" target="#tab_2">2</ref> attempts to summarize the methods used at different stages (feature, classification, subset selection,...) in order to highlight the main choices of participants. This table should be used in next section on result analysis, in order to see if there are some common techniques which tend to lead to good performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Global analysis</head><p>We present here an overview of the official results of the task and discuss the main findings. SheetAsBackground: Table <ref type="table" coords="14,335.90,344.49,4.98,8.74" target="#tab_3">3</ref> and figure <ref type="figure" coords="14,392.58,344.49,9.96,8.74" target="#fig_7">12</ref> present the identification scores of the 33 submitted runs for the SheetAsBackground category. As expected, results on scans and scan-like images of leaves are generally higher than the photographs of the NaturalBackground category. The Sabanci Okan teams reached the highest scores of 0.607 with an approach mainly centered on leaf shape boundary features. Using contour-based approaches is confirmed to be an effective strategy by the good performances of the Inria PlantNet group and the Liris team. Interestingly, one team which used a more generic approach in computer vision (the NLabUTokyo team working with Fisher Vector representations), also obtained very good identification scores whereas they used exactly the same technique for the NaturalBackground category. Other teams who attempted to use non-contour based approaches obtained significantly lower scores.</p><p>Compared to the raw identification scores obtained during the 2012 campaign, we only noticed a slight increase (0.607 vs 0.58 for scans and 0.55 for scan-like). But it is important to remark that the task itself was more complex in several aspects: (i) scans and scan-like pictures have been merged in a single category (ii) the number of species was increased from 115 (scans) or 83 (scan-like) to 126 this year (iii) test images themselves were more complex (weaker lighting conditions, more shadows, more old dried leaves and less uniform background. NaturalBackground: Table <ref type="table" coords="14,266.46,584.39,4.98,8.74">4</ref> and figure <ref type="figure" coords="14,320.57,584.39,9.96,8.74" target="#fig_8">13</ref> present the identification scores of the 33 submitted runs for the NaturalBackground category. As expected, results are significantly lower than the SheetAsBackground category due to the noisy backgrounds and clutter effects. The highest scores, obtained by the NLabU-Tokyo team, reached equivalent values than the 2012 task, but without any human assistance in the workflow, contrary to last year best runs that involved semi-automatic segmentation mechanisms. This is even more remarkable given that their approach was purely based on the visual content contrary to the second best run of the task (by Inria Plantnet team) which did make use of the date and the plant identifier tags. The contribution of using the metadata can be observed by comparing this run with the second best one of that team (Inria Plantnet run 1) that was purely based on visual data. Overall, the runs of these two teams represent the head of the pack with six (or even seven) runs clearly outperforming the other runs.    Entire Flower Fruit Leaf Stem Nat. NlabUTokyo Run 3 run3 0,297 0,472 0,311 0,275 0,253 0,393 Inria PlantNet Run 2 plantnet inria run2 0,274 0,494 0,26 0,272 0,24 0,385 NlabUTokyo Run 2 run2 0,273 0,484 0,259 0,273 0,285 0,371 Inria PlantNet Run 1 plantnet inria run1 0,254 0,437 0,249 0,24 0,211 0,353 NlabUTokyo Run 1 all siftcopphsv cca 0,236 0,423 0,209 0,269 0,276 0,341 Inria PlantNet Run 3 plantnet inria run3 0,216 0,421 0,238 0,195 0,176 0,325 Inria PlantNet Run 4 plantnet inria run4 0,15 0,327 0,137 0,165 0,171 0,245 Sabanci Okan Run 1 Sabanci-Okan-Run1 0,174 0,223 0,194 0,049 0,106 0,181 DBIS Run 2 DBISForMaT run2 train2012 svm Scan12 Photo4 -1 4 0,102 0,264 0,082 0,034 0,095 0,159</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DBIS Run 3</head><p>DBISForMaT run3 cross-val2013 svm feature4 con-fig60 1 2 3 0,109 0,256 0,079 0,035 0,095 0,158 DBIS Run 4 DBISForMaT run4 cross-val2013 svm feature5 con-fig80 Photo14 1 3 3 0,152 0,206 0,104 0,027 0,042 0,141 UAIC Run 4 run wiki max 1 0,09 0,136 0,12 0,08 0,128 0,127 DBIS Run 1 DBISForMaT run1 train2012 svm Scan4 Photo2 1 2 3 0,067 0,168 0,1 0,052 0,103 0,12 UAIC Run 1 run wiki sum 3 0,089 0,109 0,132 0,093 0,104 0,119 UAIC Run 2 run author10 GSP10 lire80 0,092 0,105 0,127 0,096 0,11 0,117 Liris ReVeS Run 2 LirisReVeS run2 0,026 0,102 0,082 0,161 0,166 0,092 Liris ReVeS Run 1 LirisReVeS run1 0,021 0,098 0,081 0,151 0,153 0,089 UAIC Run 3 run lire naivebayes 0,068 0,055 0,111 0,049 0,102 0,081 Vicomtech Run 1 outputCLEFTestMean 0,095 0,117 0 0 0,1 0,081 Vicomtech Run 2 outputCLEFTestMax 0,091 0,116 0 0 0,094 0,08 LAPI Run 1 LAPI run1 0,026 0,073 0,025 0,084 0,043 0,058 Mica Run 2 MICA-run2 0,016 0,086 0,048 0,014 0,014 0,053 Mica Run 3 Run3 0,016 0,013 0,048 0,11 0,014 0,042 SCG USP Run 3 SCG USP run3 0,017 0,025 0,042 0,047 0,054 0,03 I3S Run 1 new 100 0,017 0,023 0,041 0,038 0,025 0,026 I3S Run 2 new2 100 0,017 0,023 0,041 0,038 0,025 0,026 SCG USP Run 1 SCG USP run1 0,02 0,026 0,027 0,02 0,037 0,025 SCG USP Run 2 SCG USP run2 0,027 0,029 0,02 0,018 0,019 0,025 Mica Run 1 MICA-run1 0,016 0,013 0,048 0,014 0,014 0,023 SCG USP Run 4 SCG USP run4 0,019 0,014 0,022 0,031 0,021 0,017 AgSPPR Run 1 AgSPPR run1 0 0 0 0 0 0 AgSPPR Run 2 AgSPPR run2 0 0 0 0 0 0 AgSPPR Run 3 AgSPPR run3 0 0 0 0 0 0 Table <ref type="table" coords="18,165.70,592.13,4.13,7.89">4</ref>. Normalized and detailed cores for each run for the NaturalBackground. HA=humanly assisted, Auto=full automatic. by most participants were significantly more accurate on that image type. This confirms the botanical expertise on the important role of flowers in the identification mechanisms as this organ was historically used as the primary one to distinguish species between each others (for flowering plants of course). This is good news that computer vision methods go in the same direction. Besides the Flower category, there was no clear second best organ or view type. Stem images provided surprisingly good results relatively to the botanist knowhow. Bark morphology is actually not considered as a the most accessible identification criterion for non-specialists. The texture itself is for instance highly correlated with the age of the plant. Identification results on the Entire plant views are also rather surprising regarding their higher complexity and variability. Overall, an important remark is that the ranking of the runs did not change much from an organ to another one, fostering the idea that generic methods might solve heterogeneous fine-grained classification problems. Metadata: Regarding the use of metadata, two runs (Sabanci Okan run 1 and Inria Plantnet run 2) exploited successfully the date for improving the results. Using the observation date complementary to the visual content was a simple and efficient way to obtain a gain of up to 4 points on the Flower view type (thanks to the relatively short flourishing period of many species). Inria Plantnet run number 2 exploited also the observation identifier tag in order combine the results of the query images coming from the same plant. But since the whole NaturalBackground test dataset did contain only a few plant observations with multiple images, the impact of using this tag is much lower than the impact of  using the date field. On the other side, this multiple-image strategy was much more beneficial for the SheetAsBackground category as a significant number of plants were represented by several images (leaves used for scans are actually more likely to be collected in mass from the same plant). The runs of Inria    In particular, Liris teams proposed to use the raw GPS data of the training set complementary to external environmental data in order to interpolate them and build coarse species distribution maps. These maps where used afterwards to prune the species returned by the visual search and keep only the most probable ones. Unfortunately, the results do not show a great improvement over the purely visual runs of these teams. This can be explained by the fact that the database doesn't yet contain enough numerous and dense observations to build an accurate geographic repartition of the species. Also, the geo-localization data is partially noisy due to heterogeneous precisions in the localization (points, cities, departments). Finally, the UAIC team tried to explore author and organization tags assuming that an authors or a group of author from a same organizations have more interest on specific groups of species. However the results did not show clearly some gain by using these user informations. None of teams neither explored the hierarchical taxonomy structure, nor the common names, which could be source of improvements. External data: UAIC explored the strategy of integrating additional external training data to boost their performances. They focused their search on Wikimedia Commons which contain more and more reliable contents related to species of life in general. They managed to crawl 507 additional pictures which is fine but clearly not sufficient to make a strong difference compared to tens of thousands of images in the training set. This confirms the difficulty of collecting dense and accurate data, specific to a given flora, and with relevant annotations (like organ and view type).</p><p>Impact of the global training strategy: Whereas some of the teams used a classical leave-one-image-out strategy cross-validate their training, some other ones used a more sophisticated leave-one-plant-out strategy that is closer to the real-world problem evaluated by the task. This second option seems to have take benefits to the teams using it, namely Sabanci Okan, NLabUTokyo, Liris ReVes and Inria Plantnet. Indeed, they all mentioned that they did not split images from the same individual plant in the training set, in order to avoid overfitting problems (images of the same plant can actually be very similar).</p><p>Back to purely visual approaches: I3S and MICA teams experimented, at least through one run, a standard approach in image categorization with SIFT or SURF features, visual bag of words (BoW) representations and SVM multiclass. MICA team obtained intermediate scores contrary to I3S team. Explanations for this difference, can be that MICA use of preprocessing step for unsharp mask of Leaf images from SheetAsBackground and NaturalBackground (see figure12 MICA run 3 and I3S runs where scores are very different on Leaf ). The more recent approach in image categorization based on Fisher Vector (FV) representations, which can be see as an extension of BoW, showed a clear gain regarding to the BoW runs as we can see with the 3 NLabUTokyo runs and the Plantnet Inria run 4 on the NaturalBackground category. NLabUTokyo obtained the best scores, maybe because they capture local spatial information by enriching dense local descriptors with polynomials, contrary to the Inria Plantnet run where patch are extracted around Harris corners and descriptors are directly embedded in Fisher Vector representations. Moreover NLabUTokyo used also a late fusion where classifiers are trained independently for each descriptor, while Inria Plantnet run 4 used an intermediate fusion by concatenating FV representations from the different type of descriptors. Besides, late fusion is also are shared approach for the best runs of Inria Plantnet team.</p><p>The fact that NLabUTokyo runs obtained almost the best results for all subcategories, confirms the idea that FV representation is a successful generic approach in spite of different type of visual contents. It is important to notice that the run 2 obtained close scores to the best one (run 3) without considering subcategory tags, which show that views tags are may be not essential for succeeding the task. This is an important conclusion since image tagging is an heavy process with users. However, this generic approach is not the most efficient on SheetAsBackground compared to contour based approaches dedicated to leaf shape analysis. This may show that generic approaches like the one used by the NLabUTokyo team is dependent to the background, and that through a dense grid patch extraction, their system learn a contextual information off the background. In particular this can be observed with the Fruit subcategory where NLabUTokyo run 3 outperforms other methods: fruits are generally small elements in the pictures difficult to capture, and also these organs appear often after the leafage, thus we can suppose these cluttered backgrounds have a non negligible contribution in the species contribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performances per morphological features</head><p>Like in the previous working note with the leaf <ref type="bibr" coords="24,355.79,137.71,14.61,8.74" target="#b11">[14]</ref>, we try here to present some complementary results by analysing some morphological features, more precisely on the sexual organs which are the flower and the fruit. Beyond the methods used, we try to analyse which feature, which kind of flower or fruit is intrinsically more difficult than the others. The figure <ref type="figure" coords="24,372.47,185.53,9.96,8.74" target="#fig_18">20</ref> shows detailed results by category of color. The graph to the left shows the proportion of image test by color used for computing and displaying the graph to the right. Results in this second graph are sorted in a decreasing order of mean performance over all the submitted run (except the AgSPPR's runs which not really participate to the NaturalBackground ). One can notice that the two most represented colors, the yellow and the white (more than 50% of the database) are not the ones which enables the best results, maybe rigthly because there is more species and thus more confusions and ambiguity. Green flowers, which is not so rare, seem to be the most difficult color maybe because it expresses no color in a sense and thus it is a difficult information to capture, notably if the flowers hidden with a background of leafage or grass. Similarly, the brown flowers may also be very confused with barks for trees where flowers appear before leaves, which can explain the performances on this color. The figure <ref type="figure" coords="24,383.50,340.94,9.96,8.74" target="#fig_0">21</ref> attempts to give a complementary perspective of results about flower according to the inflorescence structure as mentioned in section 2.4. First of all we can see that the inflorescence categories are strongly unbalanced in terms of image number. Thus the best scores obtained by Cyathium, Panicle, Umbel and to a lesser extent Umbel, Captitulum and Solitary are no very representative for making some relevant conclusions on these types of inflorescences. Concerning the most representative ones the Cyme seems to be the type where the runs performed the best on average. The figure <ref type="figure" coords="25,222.94,118.99,9.96,8.74" target="#fig_19">22</ref> gives the results according to the fruit type. As for the Fig. <ref type="figure" coords="25,234.26,295.95,9.04,7.89" target="#fig_0">21</ref>. Detailed results by inflorescence type inflorescence, unfortunately, some types of fruits are not well represented in the dataset like Silique, Cone, Folicle and to a lesser extent Legume. Even if this last type of fruit is not so well represented, it is interesting to note that all teams seem to have the best scores on Legume because the associated species are from a very large family of plants called Fabaceae which is spread all other the world. Then, it is difficult to highlight one type of fruit over the others, because there is always one best method at the same score around 0.4. We have just to note that the Samara (like "helicopters" from maple for instance) seem to be clearly the most difficult type of fruit, even when we look at the best run (not over 0.2). This paper presented the overview and the results of ImageCLEF 2013 plant identification testbed following the two previous one in 2011 and 2012. The number of participants increased from 8 to 12 groups showing an increasing interest in applying multimedia search technologies to environmental challenges. This year the challenge climb one step by considering multiple type of view and organs of plants while the number of species increased from 125 to 250 species and plant observations densely covered the French territory. Results are encouraging by scaling state-of-the-art plant recognition technologies to a real-world application with thousands of species might still be a difficult task. Despite increasing difficulties on SheetAsBackground images and the number of species, scores are high and show that leaf analyses is still the best way for identifying a plant, even if collecting new scans is more difficult than shooting photographs. Performances obtained on NaturalBackground category of unconstrained pictures of plant organs are very encouraging especially for the Flower when we look detailed results and where best methods can compete with scores SheetAs-Background. It corroborates a well-know usage of botanists for identifying plants and this is good news in a sense that computer vision methods go in the same direction. An interesting conclusion is that these good results on NaturalBackground images are obtained with generic visual classification technique without any specificity related to plants. With the emergence of more and more plant identification apps [2] <ref type="bibr" coords="26,232.38,384.36,9.96,8.74" target="#b3">[6]</ref>, <ref type="bibr" coords="26,249.13,384.36,9.96,8.74" target="#b2">[5]</ref>, <ref type="bibr" coords="26,265.88,384.36,10.52,8.74" target="#b1">[3]</ref> and the ecological urgency to build real-world and effective identification tools, we believe that the detailed results and conclusions of the task will be of high interest for the computer vision and machine learning community.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,134.77,445.87,345.83,7.89;4,134.77,456.86,228.80,7.86;4,186.64,246.08,242.06,185.03"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Examples from the two categories and the five subcategories. Species does not contain systematically at least one image for each organ.</figDesc><graphic coords="4,186.64,246.08,242.06,185.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,237.62,279.98,140.13,7.89;6,223.37,305.25,92.64,69.17"><head>Fig. 2 .Fig. 3 .Fig. 4 .</head><label>234</label><figDesc>Fig. 2. Color variability of flowers.</figDesc><graphic coords="6,223.37,305.25,92.64,69.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,260.78,204.61,93.81,7.89;7,261.20,224.73,92.64,69.17"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Sizes of flowers.</figDesc><graphic coords="7,261.20,224.73,92.64,69.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,148.20,303.06,318.95,7.89;7,204.00,224.74,54.41,69.17"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig. 6. Color variability of flowers from one same species (Iris lutescens Lam.).</figDesc><graphic coords="7,204.00,224.74,54.41,69.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,268.67,292.76,78.01,7.89;8,286.49,312.85,93.34,69.69"><head>Fig. 8 .Fig. 9 .</head><label>89</label><figDesc>Fig. 8. Fruit types.</figDesc><graphic coords="8,286.49,312.85,93.34,69.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="8,167.10,655.03,281.16,7.89;8,258.64,566.25,51.65,69.17"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. The visual diversity of the bark of the Robinia pseudoacacia.</figDesc><graphic coords="8,258.64,566.25,51.65,69.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="9,134.77,418.86,345.83,7.89;9,134.77,429.84,283.81,7.86;9,276.93,340.64,69.17,69.17"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. The entire views for herbaceous species and three are very different. The smaller one has the possibility to show useful organs for identification.</figDesc><graphic coords="9,276.93,340.64,69.17,69.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="17,209.27,337.78,196.81,7.89;17,134.77,115.84,345.83,211.38"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Scores for SheetAsBackground category.</figDesc><graphic coords="17,134.77,115.84,345.83,211.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="17,210.04,582.88,195.28,7.89;17,134.77,361.26,345.83,211.06"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Scores for NaturalBackground category.</figDesc><graphic coords="17,134.77,361.26,345.83,211.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="17,134.77,620.22,144.25,8.77"><head></head><label></label><figDesc>Detailed results: Figures 17, 14</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="17,279.02,620.25,201.58,8.74;17,134.77,632.21,345.83,8.74;17,134.77,644.16,345.82,8.74;17,134.77,656.12,345.83,8.74;18,158.63,118.78,41.60,7.86;18,225.49,118.78,47.13,7.86"><head></head><label></label><figDesc>, 18,19,16, and figure 15 display the identification scores for each view type separately (still for the NaturalBackground category). It shows that the average identification scores are significantly boosted by the good performances obtained on the flower images. Most techniques used Run name runfilename</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="19,210.09,419.55,195.18,7.89;19,134.77,197.78,345.84,211.20"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Detailed scores for Flower subcategory.</figDesc><graphic coords="19,134.77,197.78,345.84,211.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12" coords="20,182.24,337.60,250.87,7.89;20,134.77,115.83,345.84,211.20"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Detailed scores for NaturalBackground subcategories.</figDesc><graphic coords="20,134.77,115.83,345.84,211.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13" coords="20,213.87,582.87,187.62,7.89;20,134.77,361.10,345.84,211.20"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. Detailed scores for Stem subcategory.</figDesc><graphic coords="20,134.77,361.10,345.84,211.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14" coords="21,211.41,337.60,192.54,7.89;21,134.77,115.83,345.84,211.20"><head>Fig. 17 .</head><label>17</label><figDesc>Fig. 17. Detailed scores for Entire subcategory.</figDesc><graphic coords="21,134.77,115.83,345.84,211.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15" coords="21,213.79,582.14,187.77,7.89;21,134.77,360.37,345.84,211.20"><head>Fig. 18 .</head><label>18</label><figDesc>Fig. 18. Detailed scores for Fruit subcategory.</figDesc><graphic coords="21,134.77,360.37,345.84,211.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16" coords="21,134.77,618.79,345.82,8.74;21,134.77,630.74,127.94,8.74;21,134.77,644.16,345.82,8.74;21,134.77,656.12,345.83,8.74"><head></head><label></label><figDesc>Plantnet and Liris ReVes teams exploited successfully this information for the SheetAsBackground category. As the previous years, several teams, like Liris Reves or UIAC, attempted to exploit the geo-localization information in order to refine candidate species list.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17" coords="22,214.97,337.60,185.42,7.89;22,134.77,115.83,345.84,211.20"><head>Fig. 19 .</head><label>19</label><figDesc>Fig. 19. Detailed scores for Leaf subcategory.</figDesc><graphic coords="22,134.77,115.83,345.84,211.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18" coords="24,134.77,520.08,345.82,7.89;24,134.77,531.06,345.83,7.86;24,134.77,542.02,345.83,7.86;24,134.77,552.98,196.05,7.86;24,134.77,371.76,345.80,137.76"><head>Fig. 20 .</head><label>20</label><figDesc>Fig. 20. Detailed results by flower color. The graph to the left represent the proportions of the tested images used for computing the detailed results in the graph to the right.This second graph gives the minimum, the median, the mean and the maximum scores over all the submitted runs for each flower color.</figDesc><graphic coords="24,134.77,371.76,345.80,137.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19" coords="25,229.63,628.75,156.10,7.89;25,134.77,474.02,345.80,144.17"><head>Fig. 22 .</head><label>22</label><figDesc>Fig. 22. Detailed results by fruit type.</figDesc><graphic coords="25,134.77,474.02,345.80,144.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="10,134.77,142.90,345.83,280.52"><head>Table 1 .</head><label>1</label><figDesc>). Numerous images of the different views were automatically integrated in the training dataset since the associated plant observations were yet integrated last year task with the leaves. The training data finally resulted in 20985 images while the test data resulted in 5092 images. Detailed statistics of the composition of the training and test data are provided in Table 1. Composition of the training and test data</figDesc><table coords="10,156.56,226.10,296.22,186.42"><row><cell></cell><cell></cell><cell cols="4">Images Plants Authors Species</cell></row><row><cell>SheetAsBackground</cell><cell>Train Test</cell><cell>9781 1250</cell><cell>732 150</cell><cell>36 14</cell><cell>126 70</cell></row><row><cell>NaturalBackground</cell><cell>Train Test</cell><cell>11204 3842</cell><cell>2553 2454</cell><cell>176 229</cell><cell>244 238</cell></row><row><cell>Entire</cell><cell>Train Test</cell><cell>1455 694</cell><cell>955 567</cell><cell>104 107</cell><cell>234 177</cell></row><row><cell>Flower</cell><cell>Train Test</cell><cell>3521 1233</cell><cell>1328 970</cell><cell>127 142</cell><cell>233 203</cell></row><row><cell>Fruit</cell><cell>Train Test</cell><cell>1387 520</cell><cell>512 302</cell><cell>64 77</cell><cell>156 103</cell></row><row><cell>Leaf</cell><cell>Train Test</cell><cell>13285 2040</cell><cell>1046 420</cell><cell>73 68</cell><cell>210 143</cell></row><row><cell>Stem</cell><cell>Train Test</cell><cell>1337 605</cell><cell>629 408</cell><cell>38 35</cell><cell>131 77</cell></row><row><cell>All</cell><cell>Train Test</cell><cell>20985 5092</cell><cell>11204 3842</cell><cell>176 229</cell><cell>250 241</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="11,134.77,632.18,345.83,32.68"><head></head><label></label><figDesc>DBIS (4 runs)[20], Germany. DBIS team runs are based on global visual features and a multiclass SVM classifier. These participants experimented numerous (early) combinations of about thirty global features, in order to select the best combination for each type of view. The selected features are predominantly based on color (Auto Color Correlogram, Border Interior Color, Color Histogram, Color Layout, Color Structure, EdgeHistogram, tamura, CEDD, FCTH). They also experimented several SVM parameters in order to boost theirs results. I3S (2 runs)<ref type="bibr" coords="12,203.46,169.00,16.81,8.77" target="#b13">[16]</ref>, France. I3S team used a popular approach in the field of image classification: they extracted SIFT features in order to produce Bag of visual Words (BoW), one BoW vector by picture, from a 1000 visual words dictionary (built with kMeans clustering algorithm). BoW's are then exploited to train species model with SVM classifiers, one for each species and type of view. Species prediction of test images are produced with a one-against-all procedure. INRIA PLANTNET (4 runs)<ref type="bibr" coords="12,283.79,242.95,11.46,8.77" target="#b5">[8]</ref>, France. For the SheetAsBackground category, after a basic Otsu segmentation, INRIA team used multiscale triangle representations, alone and combined with other shape-based descriptors (Directional Fragment Histogram and shape parameters). In addition, multi-image queries were considered, by using images belonging to the same plant observation in order to boost the results. For the NaturalBackground category, all the 4 submitted runs are based on local features (SURF, Fourier, rotation invariant Local Binary Patterns, Edge Orientation Histogram, weighted RGB and HSV</figDesc><table /><note coords="12,134.77,338.63,345.83,8.74;12,134.77,350.58,345.83,8.74;12,134.77,362.54,345.82,8.74;12,134.77,374.49,345.83,8.74;12,134.77,386.45,334.53,8.74"><p>histograms). The last one uses a multi-cue Fisher Vector embedding [] with a one-against-all multiclass SVM classifier. The three first runs use Hamming embedding and hash-based approximate knn matching: all local features are hashed, indexed and searched in separate indices (one for each each type of view and type of feature) and retrieved images are scored by the number of matches.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="15,134.77,121.82,346.71,549.74"><head>Table 2 .</head><label>2</label><figDesc>Approaches used by participants. In training subset column, All means that the 2 categories where not distinguished, while Cat. means it is the case, and Subcat. means that the sub-categories (Flower, Fruit, Leaf, Stem, Entire) were considered for the NaturalBackground category. Column IP indicates if participants avoid to split images from a same Individual Plant during evaluation on training dataset.</figDesc><table coords="15,135.96,121.82,345.52,493.34"><row><cell>Classification Train Metadata IP</cell><cell>subsets</cell><cell>Multiclass SVM with a radial basis func-</cell><cell>tion kernel Cat.</cell><cell>(run 1&amp;3). Matching approach (run2).</cell><cell>Distinct kernels by run for Multiclass Subcat. × SVM (RBF, polynomial, sigmoid) ×</cell><cell></cell><cell>One against all Mutliclass SVM, linear Subcat. × ×</cell><cell>kernel</cell><cell>Large Scale Matching approaches with Flowering</cell><cell cols="2">late fusion schema Subcat. or One against all multiclass SVM clas-date, Indi-vidual plant ×</cell><cell>sifier applied to Fisher Vectors ids</cell><cell cols="2">Linear Discriminant Analysis Subcat. × √</cell><cell>GPS with</cell><cell cols="2">Naive distance based classification Subcat. climate area extension, ×</cell><cell>Individual</cell><cell>plant ids</cell><cell></cell><cell>SVM Cat. (?) × ×</cell><cell></cell><cell cols="2">Linear Logistic Regression, late fusion Cat., × √ All.,</cell><cell>Subcat.</cell><cell>SVM classifiers Subcat. date (flower, √</cell><cell>fruit)</cell><cell>LDA or SVM (run3) Subcat. GPS ×</cell><cell>(only</cell><cell>run4)</cell><cell>image result list fusion with max or sum All operator, or naive bayesian approach GPS, au-thor, organi-zation Linear SVM 1-vs-all multi-class strategy All × × ×</cell></row><row><cell>Segmentation Features</cell><cell></cell><cell cols="2">√ (run 1) SPACT, SIFT, global shape</cell><cell>Auto Color Correlogram, Border Inte-</cell><cell>× rior Color, Color Histogram, Color Lay-out, Color Structure, Edge Histogram,</cell><cell>Tamura, CEDD, FCTH.</cell><cell>× SIFT + BOW</cell><cell></cell><cell cols="5">Otsu (Sheet, and Sheet: Multiple triangular representa-tions eventually combined with DFH de-run 3 for Natural scriptors and shape parameters with an abort cri-Natural: Harris-like key points + SURF, terion LBP, weighted RGB, HSV, Fourier, EOH</cell><cell>× contour extraction with Canny + curve partitioning and abstraction + PCA</cell><cell cols="2">Bagging features: Lab colors, Gabor</cell><cell cols="3">Semi-supervised wavelets, SURF Shape: centered moment, eccentricity, segmentation Hu/Zernike moments</cell><cell>Gist (run1, 2), Color and texture his-</cell><cell>Auto togram (run2 flower, entire), SURF +</cell><cell>Bow (run3)</cell><cell>dense grid SIFT, C-SIFT, Opponent-</cell><cell cols="2">× SIFT, HSV-SIF, self-similarity SSIM + polynomial embedding + Fisher Vec-</cell><cell>tor Sheet: variety of contour based fea-</cell><cell>tures, texture (Fourrier), color descrip-</cell><cell>Auto tors, edge background/foreground his-togram.</cell><cell>Natural: HSV color auto-correlograms,</cell><cell>weigthed-saturation hue histogram,</cell><cell>run1: auto, run</cell><cell>2-3-4: manual for Gabor, LBP, fractal, geometrical</cell><cell>test, semi for train</cell><cell>× Joint Composite Descriptor</cell><cell>color segmentation trace transform, shape relationship</cell></row><row><cell>Team</cell><cell></cell><cell></cell><cell>AGSPPR</cell><cell></cell><cell>DBIS</cell><cell></cell><cell>I3S</cell><cell></cell><cell></cell><cell>IINRIA</cell><cell cols="2">PLANTNET</cell><cell></cell><cell>LAPI</cell><cell></cell><cell></cell><cell>LIRIS</cell><cell>REVES</cell><cell></cell><cell></cell><cell>MICA</cell><cell></cell><cell></cell><cell>NLAB</cell><cell>UTOKYO</cell><cell>SABANCI-</cell><cell>OKAN</cell><cell>SCG USP</cell><cell>UIAC</cell><cell>VICOM-</cell><cell>TECH</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="16,134.77,503.46,345.83,18.85"><head>Table 3 .</head><label>3</label><figDesc>Normalized classification scores for each run for the SheetAsBackground. HA=humanly assisted, Auto=full automatic.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_0" coords="2,144.73,645.84,126.99,7.86"><p>http://www.imageclef.org/2013</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_1" coords="2,144.73,656.80,123.92,7.86"><p>http://www.tela-botanica.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_2" coords="3,144.73,634.88,335.86,7.86;3,144.73,645.84,122.51,7.86"><p>it is closed now, but a newer a application can be found at http://identify.plantnetproject.org/en/base/plantscan</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_3" coords="3,137.50,655.03,3.65,5.24;3,144.73,656.80,157.25,7.86"><p><ref type="bibr" coords="3,137.50,655.03,3.65,5.24" target="#b6">9</ref> http://identify.plantnet-project.org/fr/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was funded by the <rs type="funder">Agropolis</rs> fundation through the project Pl@ntNet (http://www.plantnet-project.org/) and the EU through the CHORUS+ Coordination action (http://avmediasearch.eu/). Thanks to all participants. Thanks to <rs type="person">Jennifer Carré</rs>, <rs type="person">Violette Roche</rs> and all contributors from <rs type="person">Tela Botanica</rs>. Thanks to <rs type="person">Souheil Selmi</rs> from <rs type="affiliation">Inria</rs> and <rs type="person">Julien Barbe</rs> from <rs type="affiliation">Amap</rs> for their help.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="26,142.96,588.70,317.07,7.86;26,139.37,600.13,3.58,7.86" xml:id="b0">
	<monogr>
		<ptr target="http://www.intelengine.cn/English/dataset2" />
		<title level="m" coord="26,151.53,588.70,126.64,7.86">The icl plant leaf image dataset</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="26,142.96,611.55,295.01,7.86" xml:id="b1">
	<analytic>
		<title/>
		<ptr target="https://itunes.apple.com/fr/app/folia/id547650203" />
	</analytic>
	<monogr>
		<title level="j" coord="26,151.53,611.55,19.58,7.86">Folia</title>
		<imprint>
			<date type="published" when="2012-11">Nov 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,142.96,634.41,35.95,7.86;26,213.47,634.41,17.94,7.86;26,265.97,634.41,21.24,7.86;26,321.77,634.41,24.58,7.86;26,380.90,634.41,99.69,7.86;26,151.52,645.37,135.75,7.86" xml:id="b2">
	<analytic>
		<title/>
		<ptr target="https://itunes.apple.com/us/app/mobileflora/id592906385" />
	</analytic>
	<monogr>
		<title level="j" coord="26,151.53,634.41,27.38,7.86;26,213.47,634.41,17.94,7.86">Mobile flora</title>
		<imprint>
			<date type="published" when="2013-05">May 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,142.96,656.80,302.70,7.86" xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Plantnet</surname></persName>
		</author>
		<ptr target="https://itunes.apple.com/fr/app/plantnet/id600547573" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,142.96,119.67,337.63,7.86;27,151.52,130.63,329.07,7.86;27,151.52,141.59,329.07,7.86;27,151.52,152.55,60.92,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="27,213.12,141.59,218.97,7.86">First steps toward an electronic field guide for plants</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Feiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Kress</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">B</forename><surname>Dixit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Shirdhonkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="27,439.83,141.59,25.08,7.86">Taxon</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="597" to="610" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,142.96,163.46,337.63,7.86;27,151.52,174.42,329.07,7.86;27,151.52,185.38,227.85,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="27,240.02,174.42,240.58,7.86;27,151.52,185.38,15.40,7.86">Inria&apos;s participation at imageclef 2013 plant identification task</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Bakić</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mouine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ouertani-Litayem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Verroust-Blondet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="27,187.92,185.38,162.78,7.86">Working notes of CLEF 2013 conference</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,142.96,196.29,337.64,7.86;27,151.52,207.25,298.80,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="27,242.34,196.29,238.25,7.86;27,151.52,207.25,79.72,7.86">Has an image classification approach any chance at all (in plant classification)?</title>
		<author>
			<persName coords=""><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="27,258.88,207.25,162.78,7.86">Working notes of CLEF 2013 conference</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,142.62,218.16,337.98,7.86;27,151.52,229.12,329.07,7.86;27,151.52,240.08,324.59,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="27,407.88,229.12,72.71,7.86;27,151.52,240.08,176.70,7.86">ImageCLEF 2013: the vision, the data and the open challenges</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zellhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">G</forename><surname>Varea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cazorla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="27,349.26,240.08,98.18,7.86">Proc CLEF 2013. LNCS</title>
		<meeting>CLEF 2013. LNCS</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,142.62,250.99,337.97,7.86;27,151.52,261.95,329.07,7.86;27,151.52,272.91,205.53,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="27,197.66,261.95,279.26,7.86">Late information fusion for multi-modality plant species identification</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cerutti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Tougne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sacca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joliveau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">O</forename><surname>Mazagol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Coquin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vacavant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="27,165.60,272.91,162.78,7.86">Working notes of CLEF 2013 conference</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,142.62,283.82,337.97,7.86;27,151.52,294.78,195.03,7.86;27,134.77,305.69,7.85,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="27,399.39,283.82,81.20,7.86;27,151.52,294.78,137.62,7.86">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="27,310.72,294.78,35.84,7.86;27,134.77,305.69,7.85,7.86">CVPR09 13</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="27,142.62,305.69,337.98,7.86;27,151.52,316.65,329.07,7.86;27,151.52,327.60,329.07,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="27,211.11,316.65,124.92,7.86">Multi-organ plant identification</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Barbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Bakic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Barthelemy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="27,355.63,316.65,124.96,7.86;27,151.52,327.60,251.90,7.86">Proceedings of the 1st ACM international workshop on Multimedia analysis for ecological data</title>
		<meeting>the 1st ACM international workshop on Multimedia analysis for ecological data</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,142.62,338.51,337.97,7.86;27,151.52,349.47,329.07,7.86;27,151.52,360.43,25.60,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="27,210.27,349.47,196.59,7.86">The ImageCLEF 2012 plant identification task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Itheri</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Barthélémy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Molino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="27,431.57,349.47,49.02,7.86">ImageCLEF</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,142.62,371.34,337.98,7.86;27,151.52,382.30,329.07,7.86;27,151.52,393.26,80.38,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="27,429.55,371.34,51.05,7.86;27,151.52,382.30,203.72,7.86">Visual-based plant species identification from crowdsourced data</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Selmi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mouysset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Joyeux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="27,375.86,382.30,104.74,7.86;27,151.52,393.26,51.71,7.86">Proceedings of ACM Multimedia 2011</title>
		<meeting>ACM Multimedia 2011</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,142.62,404.17,337.98,7.86;27,151.52,415.13,309.61,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="27,313.24,404.17,167.35,7.86;27,151.52,415.13,96.65,7.86">Sift, bow architecture and one-against-all support vector machines</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Issolah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lingrand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Precioso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="27,269.68,415.13,162.78,7.86">Working notes of CLEF 2013 conference</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,142.62,426.04,337.97,7.86;27,151.52,437.00,130.20,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="27,242.32,426.04,159.93,7.86">Imageclef2013 plant identification mica</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">L</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">H</forename><surname>Pham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="27,422.84,426.04,57.74,7.86;27,151.52,437.00,101.53,7.86">Working notes of CLEF 2013 conference</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,142.62,447.91,337.97,7.86;27,151.52,458.87,154.32,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="27,212.20,447.91,215.10,7.86">Nlab-utokyo at imageclef 2013 plant identification task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Nakayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="27,446.53,447.91,34.06,7.86;27,151.52,458.87,125.64,7.86">Working notes of CLEF 2013 conference</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,142.62,469.78,337.97,7.86;27,151.52,480.74,35.84,7.86;27,134.77,491.65,345.83,7.86;27,151.52,502.61,191.46,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="27,282.37,469.78,179.50,7.86">A visual vocabulary for flower classification</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="27,151.52,480.74,35.84,7.86;27,151.52,502.61,162.78,7.86">Working notes of CLEF 2013 conference</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>CVPR06</note>
</biblStruct>

<biblStruct coords="27,142.62,513.52,337.98,7.86;27,151.52,524.48,329.07,7.86;27,151.52,535.44,285.14,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="27,458.58,513.52,22.02,7.86;27,151.52,524.48,329.07,7.86;27,151.52,535.44,52.48,7.86">Combining image retrieval, metadata processing and naive bayes classification at plant identification</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Serba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Siriteanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gheorghiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Iftene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Alboaie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Breaban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="27,245.21,535.44,162.78,7.86">Working notes of CLEF 2013 conference</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,142.62,546.35,337.98,7.86;27,151.52,557.31,329.07,7.86;27,151.52,568.27,102.51,7.86" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="27,231.51,546.35,245.10,7.86">Computer Vision Classification of Leaves from Swedish Trees</title>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">J O</forename><surname>Söderkvist</surname></persName>
		</author>
		<idno>liTH-ISY-EX-3132</idno>
		<imprint>
			<date type="published" when="2001-09">September 2001</date>
			<pubPlace>Linköping, Sweden</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Linköping University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct coords="27,142.62,579.18,337.98,7.86;27,151.52,590.14,329.07,7.86;27,151.52,601.10,25.60,7.86;27,134.77,612.01,7.85,7.86" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="27,473.68,579.18,6.91,7.86;27,151.52,590.14,329.07,7.86">A leaf recognition algorithm for plant classification using probabilistic neural network</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">S</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Liang Xiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,142.62,612.01,337.97,7.86;27,151.52,622.96,329.07,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="27,324.69,612.01,155.90,7.86;27,151.52,622.96,123.72,7.86">Sabanci-okan system at imageclef 2013 plant identification competition</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Yanikoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Aptoula</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">T</forename><surname>Yildiran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="27,294.74,622.96,158.12,7.86">Working notes of CLEF 2013 conference</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,142.62,633.88,337.98,7.86;27,151.52,644.83,154.32,7.86" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="27,228.91,633.88,197.41,7.86">Agsppr at imageclef 2013 plant identification task</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="27,446.53,633.88,34.06,7.86;27,151.52,644.83,125.64,7.86">Working notes of CLEF 2013 conference</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
