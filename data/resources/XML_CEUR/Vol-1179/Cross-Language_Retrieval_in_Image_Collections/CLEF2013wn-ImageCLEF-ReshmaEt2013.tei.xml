<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,134.77,116.95,345.83,12.62;1,279.73,134.89,55.91,12.62">KDEVIR at ImageCLEF 2013 Image Annotation Subtask</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,186.26,172.58,82.19,8.74"><forename type="first">Ismat</forename><forename type="middle">Ara</forename><surname>Reshma</surname></persName>
							<email>reshma1@kde.cs.tut.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Toyohashi University of Technology</orgName>
								<address>
									<addrLine>1-1 Hibarigaoka, Tempaku-Cho</addrLine>
									<postCode>441-8580</postCode>
									<settlement>Toyohashi, Aichi</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,279.00,172.58,58.67,8.74"><roleName>Md</roleName><forename type="first">Zia</forename><surname>Ullah</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Toyohashi University of Technology</orgName>
								<address>
									<addrLine>1-1 Hibarigaoka, Tempaku-Cho</addrLine>
									<postCode>441-8580</postCode>
									<settlement>Toyohashi, Aichi</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,367.61,172.58,57.34,8.74;1,424.95,171.01,1.83,6.12"><forename type="first">Masaki</forename><surname>Aono</surname></persName>
							<email>aono@tut.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Toyohashi University of Technology</orgName>
								<address>
									<addrLine>1-1 Hibarigaoka, Tempaku-Cho</addrLine>
									<postCode>441-8580</postCode>
									<settlement>Toyohashi, Aichi</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,134.77,116.95,345.83,12.62;1,279.73,134.89,55.91,12.62">KDEVIR at ImageCLEF 2013 Image Annotation Subtask</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C4494A36F9F80991382BB4A8487B27E5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Visual feature</term>
					<term>Bag-of-Visual-Words</term>
					<term>textual feature</term>
					<term>image annotation</term>
					<term>classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The explosive growth of image data on the web leads to the research and development of visual information retrieval systems. However, these visual contents do not allow user to query images using semantic meanings. To resolve this problem, automatically annotating images with a list of semantic concepts is an essential and beneficial task. In this paper, we describe our approach for annotating images with controlled semantic concepts, which is a scalable concept image annotation subtask in the Photo Annotation and Retrieval task of the ImageCLEF 2013. We label training images with semantic concepts. After that, given a test image, the most k similar images are retrieved from the training image set. And finally, we extract and aggregate the concepts of the k matched training images, and choose the top n concepts as annotation. In our proposed method, the textual concepts of the training images are weighted by introducing BM25. Then, we utilizes some combination of visual features vectors, which are constructed from global descriptor such as color histogram, gist as well as local descriptor including SIFT and some variations of SIFT. The visual feature vectors are used to measure the similarity between two images by employing cosine similarity or inverse distance similarity (IDsim) that we introduce here. For a given test image, we find the k-nearest neighbors (kNN ) from the training image set based on the image similarity values. Furthermore, we aggregate the concepts of the kNN images, and choose top n concepts as annotation. We evaluate our methods by estimating F -measure and mean average precision (MAP ). The result turns out that our system achieves the average performance in this subtask.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>To enable the user for searching images using semantic meaning, automatically annotating images with some concepts or keywords using machine learning technique in scalable and efficient method is to be performed. In this paper, we describe our method in scalable concept image annotation subtask <ref type="bibr" coords="1,439.12,657.11,10.52,8.74" target="#b0">[1]</ref> of the Photo Annotation and Retrieval task in ImageCLEF 2013 <ref type="bibr" coords="2,396.23,119.99,9.96,8.74" target="#b1">[2]</ref>. Detail information on this subtask, the training, development and test set, the concepts and the evaluation measures can be found in the overview paper <ref type="bibr" coords="2,402.02,143.90,10.52,8.74" target="#b0">[1]</ref> of ImageCLEF 2013. In this subtask, the objective is to develop systems that can easily change or scale the list of concepts used for image annotation. In other words, the list of concepts can also be considered to be an input to the system. Thus the system when given an input image and a list of concepts, its job is to give a score to each of the concepts in the list and decide how many of them assign as annotations.</p><p>In our participation to the ImageCLEF 2013, we develop a system named KDEVIR to automatically annotate images with semantic concepts. We divide our approach into two steps: preprocessing and main processing. In the preprocessing step, we conduct filtering the textual features of training images, and match them with a list of controlled semantic concepts. And then, the concepts of the training images are weighted and used to label training images. After that, we measure all-pair similarity of visual feature vectors between test set and training set, and choose the k most similar images from the training set as matched images satisfying a threshold value. In main processing step, given a test image, our system retrieves the k-nearest neighbor(kNN ) <ref type="bibr" coords="2,356.08,323.28,10.52,8.74" target="#b2">[3]</ref> from the matched images that was produced in the preprocessing step. And then, we aggregate all the labelled concepts of the k matched images, and measure their candidate weights. After that, we ranked the concepts based on their candidate weight, and choose the top n concepts as annotation. Our system produces good result 25 percent correct result over all test images.</p><p>The rest of the paper is organized as follows. Section 2 describes the general terminology to comprehend the essence of the paper while our system architecture is articulated in Section 3. We describes performance evaluation in Section 4, and Section 5 includes conclusion and some future direction of our works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">General Terminology</head><p>This section introduces some basic definitions of terminology to familiarize the readers with the notions used throughout the paper. It includes the definitions of BM25, Cosine similarity, and our proposed IDsim methods to comprehend the essence of our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Okapi BM25</head><p>The Okapi best matching 25 (BM25 ) <ref type="bibr" coords="2,301.13,573.43,10.52,8.74" target="#b3">[4]</ref> approach is based on the probabilistic retrieval framework developed in the 1970s and 1980s by <ref type="bibr" coords="2,386.15,585.38,10.52,8.74" target="#b4">[5]</ref> (1981). The BM25 formula is used for measuring the similarity between a user query Q and a document d. It is used to rank a set of documents based on the query terms appearing in each document, regardless of the inter-relationship between the query terms within a document (e.g., their relative proximity). It is not a single function, but actually a whole family of scoring functions, with slightly different components and parameters. One of the most prominent instantiations of the function is as follows. Given a query Q, containing keywords {q 1 , q 2 ..., q n }, the BM25 score of a document d for the query Q is defined as follows:</p><formula xml:id="formula_0" coords="3,144.03,147.95,326.10,34.81">weight(Q, d) = n i=1 T F qi,d • (k 1 + 1) k 1 • ((1 -b) + (b • |d| avgl )) + T F qi,d × log N -n(q i ) + 0.5 n(q i ) + 0.5</formula><p>where T F qi,d is the q i 's term frequency in the document d, N is the total number of documents in the collection, |d| avgl is the ratio of the length of document d to the average document length, and n(q i ) is number of documents where the term q i appears. k 1 and b are free parameters, usually chosen, in absence of an advanced optimization, as k 1 ∈ [1.2, 2.0] and b = 0.75 <ref type="bibr" coords="3,364.33,241.62,11.62,8.74" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Cosine Similarity</head><p>Cosine similarity metric is frequently used when trying to determine similarity between two documents. In this metric, the features (or words, in the case of the documents) is used as a vector to find the normalized dot product of the two documents. By determining the cosine similarity, the user is effectively trying to find cosine of the angle between the two objects. The cosine similarity is described as follows:</p><formula xml:id="formula_1" coords="3,248.95,363.38,231.65,22.31">CosSim(x, y) = x • y ||x|| * ||y||<label>(1)</label></formula><p>The similarity values depends on the features vectors. In the case of information retrieval, the cosine similarity of two documents will range from 0 to 1, since the term frequencies (tf-idf weights) cannot be negative. The angle between two term frequency vectors cannot be greater than 90.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">IDsim</head><p>Cosine similarity metric is only sensitive to vector direction, but does not consider vector length. However, to find out vector similarity, we have to consider not only vector directions but also their vector lengths. To solve these problem, we proposed a new similarity method named inverse distance similarity (IDsim). If U and V are two vectors, then IDSim is defined as follows:</p><formula xml:id="formula_2" coords="3,172.98,540.37,307.61,25.98">IDsim = U i * V i U 2 i * V 2 i * (log 10 ( (U i -V i ) 2 + 1) + 1)<label>(2)</label></formula><p>The similarity values depends on the features vectors. The IDsim similarity of two documents will range from 0 to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">System Architecture</head><p>In this section, we describes our method for annotating images with a list of semantic concepts. We divide our method into two steps: preprocessing and main processing. Our whole system is depicted in figure <ref type="figure" coords="3,381.43,657.11,3.87,8.74" target="#fig_0">1</ref>.  attributes, and 3) the word distance to the image 1 . In order to ease the main processing, and reduce memory and time complexity, our system applies multiple filtering on the textual features. Because, the textual features contains some stop words, misspelled words, sometimes words from different languages than English, and some word with no semantic relation with the controlled semantic concepts. We filter out the textual features by stop words, and then, we filter out the non-English words. After that, we semantically match the feature with the list of controlled concepts. In this regard, we extend the concepts list by finding their synonyms from Wordnet 3.0 <ref type="bibr" coords="5,263.49,361.54,9.96,8.74" target="#b5">[6]</ref>. Now, we examine If the current word feature is exactly match with concept or the synonyms of the concept, and then, we consider this feature as a semantic concept. If the current word feature does not exactly match with concept list, then we apply stemming the word feature using Lucene 2 stemmer <ref type="bibr" coords="5,239.89,409.36,9.96,8.74" target="#b6">[7]</ref>, and again reexamine the word feature whether it is sense of the concept or not. If it does not exactly match with the concept, we just discard it from the feature list. After matching the all the textual features of the training images, we apply bm25 <ref type="bibr" coords="5,306.33,445.23,10.52,8.74" target="#b4">[5]</ref> to estimate the weights of annotated concepts of the train textual features. Thus, our system figures out the weighted concepts list for each training image.</p><p>Visual Features Organizer provided to each participant a list of visual features vectors of training images. Visual features vectors of the training images have been computed: GIST, Color Histograms, SIFT, C-SIFT, RGB-SIFT and OPPONENT-SIFT. For the *-SIFT descriptors a bag-of-words representation is provided. Furthermore, Organizers also provided the corresponding visual feature vectors of the development or test image sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Main Processing</head><p>In this section, we describes the steps for annotated images with concepts. Given a set of development/test images, we select a test/development image's feature vector, and measure similarity of the image with all the training images using cosine similarity or IDsim. And then, we choose the k-nearest neighbors of images from the training set as matched images satisfying a threshold value. After that, we aggregate all the concepts of the k matched train images, and measure the candidate weights of the concepts. And finally, we ranked the concepts based on their candidate weight, and choose the top n concepts as annotation.</p><p>Finding image similarity We apply content based image retrieval (CBIR) approach to find out similar images using equation 1 or 2. In order to find similar images of each development/test image, our system compare each image with all training images using similarity metric 1 or 2. If computed similarity exceeds a predefined threshold value which is determined empirically, then the system keeps track of those similar images with their similarity values. Finally, among all similar images of each development/test images, the system keeps track of the k nearest neighbors. We examine all combination of visual features and, empirically find out the best matching images, which we used in final runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concepts retrieval</head><p>In this steps, we aggregate all the concepts of train images from the weighted training image concept features as the concepts of corresponding development/test images. During aggregation, we measure the candidate weights of the concepts. We measure the candidate weight of concept by multiplying its own bm25 weight by the amount of similarity of its training image with current development/test image pair. Thus, we find out some candidate weighted concepts for each development/test image. And then, we rank the weighted concepts and choose the top n concepts. However, we empirically select top most n concepts from the ranked list of concepts list as the annotation of the corresponding development/test image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Runs and results</head><p>The official results of our runs are illustrated in Table <ref type="table" coords="6,382.73,482.02,3.87,8.74">1</ref>. During experiment, we noticed that with single visual features, for example, C-SIFT produces best result. When we combine two or more features, result increases gradually. For example, the MF-sample of run 5 is 24.6 percent, which increases at run 3, 4 by adding one more feature SIFT and RGB-SIFT respectively. And the increment continued at run 1 by adding one more features Color histogram. During experiment, we also tried with TF-IDF instead of BM25, however BM25 produces better result than TF-IDF; that is why finally we did not use TF-IDF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this task, we filtered the textual features of the training images, and matched them with the concepts list to extract concepts for the train images, finally, estimated the weights of the concepts for every training images. After that, we conducted all-pair similarity measure between the test image visual feature vectors with the training images visual feature vectors by introducing a similarity metric named IDsim satisfying a threshold. And then, we selected the kNN images from matched trained images. After that, we aggregated all concepts from the kNN images and measured the candidate weights. And finally, the aggregated concepts are ranked, and the top n concepts are selected as annotation for a test image. Our Result at ImageCLEF 2013 was at middle position. We will improve our system by implementing efficient semantic matching of the features including hyponym. We will also try to introduce efficient machine learning technique to develop scalable image annotation system.</p><p>6 Web sites </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,251.06,591.82,113.23,7.89"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. System Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,134.77,269.85,358.17,22.27"><head>1</head><label></label><figDesc>Scalable Concept Image Annotation, http://imageclef.org/2013/photo/annotation 2 Lucene Search Engine, http://lucene.apache.org</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,138.35,338.85,342.24,7.86;8,146.91,349.81,294.68,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,320.40,338.85,160.19,7.86;8,146.91,349.81,135.99,7.86">Overview of the imageclef 2013 scalable concept image annotation subtask</title>
		<author>
			<persName coords=""><forename type="first">Mauricio</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roberto</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,305.21,349.81,103.78,7.86">CLEF 2013 working notes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,360.77,342.25,7.86;8,146.91,371.73,333.68,7.86;8,146.91,382.69,304.11,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,418.99,371.73,61.60,7.86;8,146.91,382.69,176.70,7.86">Imageclef 2013: the vision, the data and the open challenges</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zellhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Varea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">G</forename><surname>Cazorla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,345.67,382.69,76.68,7.86">Proc CLEF, LNCS</title>
		<meeting>CLEF, LNCS</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,393.65,342.24,7.86;8,146.91,404.61,273.49,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,375.94,393.65,104.65,7.86;8,146.91,404.61,48.64,7.86">When is nearest neighbor meaningful?</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Shaft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,213.72,404.61,99.82,7.86">Database TheoryICDT</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="217" to="235" />
			<date type="published" when="1999">1999</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,415.56,342.25,7.86;8,146.91,426.52,333.68,7.86;8,146.91,437.46,151.41,7.89" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,323.01,415.56,157.58,7.86;8,146.91,426.52,233.72,7.86">A probabilistic model of information retrieval: development and comparative experiments:: Part 2</title>
		<author>
			<persName coords=""><forename type="first">Sparck</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,387.74,426.52,92.85,7.86;8,146.91,437.48,61.95,7.86">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="809" to="840" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,448.44,342.25,7.86;8,146.91,459.40,280.91,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,307.93,448.44,172.66,7.86;8,146.91,459.40,97.06,7.86">Okapi at trec-7: automatic ad hoc, filtering, vlc and interactive track</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beaulieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,251.91,459.40,111.92,7.86">Nist Special Publication SP</title>
		<imprint>
			<biblScope unit="page" from="253" to="264" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,470.36,172.82,7.86" xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>WordNet. Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,481.32,318.29,7.86" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hatcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Gospodnetic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mccandless</surname></persName>
		</author>
		<title level="m" coord="8,341.31,481.32,65.37,7.86">Lucene in action</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
