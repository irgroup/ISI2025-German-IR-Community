<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,146.28,163.29,314.02,14.67;1,280.44,182.13,34.28,14.67">MICA at ImageClef 2013 Plant Identification Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,231.60,214.94,53.25,10.99"><forename type="first">Thi-Lan</forename><surname>Le</surname></persName>
							<email>thi-lan.le@mica.edu.vn</email>
							<affiliation key="aff0">
								<orgName type="department">UMI2954 -HUST</orgName>
								<orgName type="institution">International Research Institute MICA</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,292.80,214.94,82.03,10.99"><forename type="first">Ngoc-Hai</forename><surname>Pham</surname></persName>
							<email>ngoc-hai.pham@mica.edu.vn</email>
							<affiliation key="aff0">
								<orgName type="department">UMI2954 -HUST</orgName>
								<orgName type="institution">International Research Institute MICA</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,146.28,163.29,314.02,14.67;1,280.44,182.13,34.28,14.67">MICA at ImageClef 2013 Plant Identification Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E64E6492246681888DCB223C8787330F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Introduction</head><p>In the framework of ImageClef 2013 <ref type="bibr" coords="1,291.12,301.60,10.60,8.96" target="#b0">[1]</ref>, plant identification task <ref type="bibr" coords="1,415.44,301.60,10.69,8.96" target="#b1">[2]</ref>, we have submitted three runs. For the first run named Mica Run 1, we employ GIST descriptor with k-nearest neighbor (kNN) for all sub-categories. Concerning Mica Run 2, we observe that global descriptors such as color histogram and texture are able to distinguish classes of two sub-categories that are flower and entire. For the others sub-categories, we still employ GIST descriptor and kNN. Based on our work for leaf identification, for the third run (named MICA run 3), we have proposed to apply our method for leaf images for both SheetasBackground and Natural background. For the remaining subcategories, we used the same method as the two first runs. Concerning our method for leaf images, we firstly apply Un-sharp Masking (USM) on SheetAsBackground images. Then, we extract Speeded-Up Robust Features (SURF). Finally, we used Bag-of-Words (BoW) for calculating feature vector of each image and Support Vector Machine (SVM) for training the model of each class in training dataset and for predicting class id of new images. In this paper, we describe in detail the algorithms used in our runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. Our plant identification methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Plant identification method of MICA run 1</head><p>Results of variety of state of the art scene recognition algorithms <ref type="bibr" coords="1,410.16,552.23,12.03,9.50" target="#b2">[3]</ref> shown that GIST features 1 [11] obtains an acceptable result of outdoor scene classification (appr. 73 -80 %). Therefore, in this study, we would like to investigate if GIST features are still good for plant identification. In this section, we briefly describe procedures of GIST feature extractions proposed in <ref type="bibr" coords="1,337.92,600.59,10.89,9.50" target="#b3">[4]</ref>.</p><p>To capture remarkable/considering of a scene, Oliva et al in <ref type="bibr" coords="1,394.56,612.59,12.27,9.50" target="#b3">[4]</ref> evaluated seven characteristics of a outdoor scenes such as naturalness, openness, roughness, expansion, ruggedness, so on. The authors in <ref type="bibr" coords="2,322.08,149.63,17.55,9.50">[11]</ref> suggested that these characteristics may be reliably estimated using spectral and coarsely localized information. Steps to extract GIST features are explained in <ref type="bibr" coords="2,346.56,173.87,11.16,9.50" target="#b3">[4]</ref>. Firstly, an original image is converted and normalized to gray scale image I(x,y). We then apply a prefiltering to reduce illumination effects and to prevent some local image regions to dominate the energy spectrum. The image I(x,y) is decomposed by a set of Gabor filters. The 2-D Gabor filter is defined as follows: Gabor filter h(x,y), we obtain all those components in the image that have their energies concentrated near the spatial frequency point ( 0 u , 0 v ). Therefore, the gist vector is calculated using energy spectrum of 32 responses. We calculated averaging over each grid of 16 x 16 pixels on each response. Totally, a GIST feature vector is reduced to 512 dimensions. After feature extraction procedure, K-Nearest neighbor (K-NN) classifier is selected for classification. Given a testing image, we found K cases in the training set that have minimum distance between the gist vectors of the input images and those of the training set. A decision of the label of testing image was based on majority vote of the K label found. The fact that no general rule for selected appropriate dissimilarity measures (Minkowsky, Kullback-Leibler, Intersection..). In this work, we select Euclidian distance that is usually realized in the context of image retrieval. In our run, the value of K is 32.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Plant identification method of MICA run 2</head><p>Concerning the second run, for two sub-categories (flower and stem), we apply global descriptors that are color histogram combining with color moment and texture. These features are described in <ref type="bibr" coords="2,261.12,561.64,10.69,8.96" target="#b4">[5]</ref>. For the remaining subcategories, we still use GIST and kNN as described in the first run.</p><p>3. Plant identification method of MICA run 3 For this run, based on the obtained result of our work on leaf identification, we apply our method for leaf images for both SheetasBackground and Natural background. This method is shown in Fig. <ref type="figure" coords="2,296.04,621.64,3.76,8.96" target="#fig_1">1</ref>. a. Preprocessing First of all, segmentation methods are usually used for leaf shape recognition, while our search focuses on local features such as leaf veins and textures. Secondly, while segmentation methods work well with most uniformed background image data, they may not work well with complex background image. Thus, using segmentation methods in our work may constraint the possibility of further system development.</p><p>Instead of using segmentation methods for image preprocessing, we decided to apply image normalization and Unsharp Mask (USM) algorithms for grayscaleconverted image. These preprocessing algorithms help enhance the detail of our input image as well as improve system performance.</p><p>The preprocessing procedure will be taken in three main steps: • Grayscale conversion: Convert the original image to grayscale image, which is a matrix of pixel intensities.</p><p>• Image normalization: Change the range of pixel intensity values in order to enhance the grayscale image.</p><p>• Unsharp masking: Sharpen the image local details to help feature extraction more accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>b. Grayscale Conversion</head><p>Grayscale images (also known as intensity level image) are digital image in which the value of each pixel is a single sample, that is carries only intensity information. Grayscale images are commonly called black-and-white image and exclusively composed of shades of gray, varying from weakest intensity (black) to strongest intensity (white) <ref type="bibr" coords="3,192.96,529.12,10.69,8.96" target="#b5">[6]</ref>.</p><p>A common strategy to convert image to grayscale is to match the luminance of the grayscale image to the luminance of the color image. Usually, the luminance component of the grayscale image in the YUV and YIQ models used in PAL and NTSC is calculated by: R, G and B in the above equation represent Red, Green and Blue channels respectively. The coefficients represent human perception of colors, in particular that humans are more sensitive to green and least sensitive to blue. In our research, we desire to have all input images intensity levels to be normalized from 0 to 255. The normalization is applied directly to the grayscale image produced from grayscale conversion process above. First of all, we make the positive image by applying Gaussian blur filter to a clone version of the normalized grayscale image produced from the last step. As our original leaf image is a normalized grayscale image, we apply the equation of Gaussian filter in one dimension matrix using the following Gaussian function:</p><p>The blurred version of the original is then subtracted away from the original to detect the presence of edges, creating the unsharp mask (effectively a high-pass filter). Contrast is then selectively increased along these edges using this mask leaving behind a sharper final image. An unsharp mask improves sharpness by increasing acutance, although resolution remains the same. In short, there are two steps in unsharp masking:</p><p>•</p><p>Step 1: Detect Edges and Create Mask •</p><p>Step 2: Increase Contrast at Edges The process of unsharp masking is described below: Figure <ref type="figure" coords="5,166.92,547.24,4.18,8.96">4</ref>: Two steps of unsharp masking procedure Note that the "mask overlay" is when image information from the layer above the unsharp mask passes through and replaces the layer below in a way which is proportional to the brightness in that region of the mask. The upper image does not contribute to the final for regions where the mask is black, while it completely replaces the layer below in regions where the unsharp mask is white.</p><p>Final image shows better local characteristics of the leaf as we expected. e. Feature Extraction After processing all input images, the image matrices will then be passed through SURF feature detector and descriptor extractor, which is the local feature extraction approach. We prefer local feature extraction approach over shape feature extraction because it gives us the advantage of recognizing and classifying complex background leaf images. In our research, we take full advantage of the implemented OpenCV methods, which are configurable and bug-free.</p><p>In the first step of feature extraction, SURF feature detector received preprocessed image as input. It attempted to detect points of interest (keypoints) over the whole image using Fast Hessian detection algorithm. In the first run of our system, the detector operated as it with no parameterization, meaning that the minimum Hessian threshold, number of octaves and number of octave layers are default. However, in later experiment, we tried to modify the parameters in order to challenge the system at different running conditions.</p><p>If no keypoints were found in an input image, the system will give a warning message, ignore the error image and continue the extraction process.</p><p>Below figure illustrates the keypoints founded by SURF feature detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 6: Detected keypoints</head><p>The second step of feature extraction involves computing SURF descriptors based on detected keypoints (known as descriptor extraction process). From detected keypoint, descriptor based on sum of Haar Wavelet responses is computed. The computed descriptors for each image are then stored into the memory as a matrix vector of floating point numbers, which represents the detected keypoints, their size and orientation. The number of rows in descriptor matrix represents the number of detected keypoints, while the number of columns is the size of each keypoint, typically 64 or 128.</p><p>In our research, we ran feature extraction algorithm on a computer with high memory capacity, thus there were no limitation to the number of detected keypoints.</p><p>In practice, we expect to have the maximum number of keypoints to be 2000, in order to make our final software runs smoothly on any computer with average memory capacity.</p><p>f. Feature Dimensionality Redution In combination with SURF feature extraction method, we use BOW model to reduce the dimensionality of our computed SURF descriptors. In our research, we use a slightly different version BOW model from OpenCV library which is composed of two main components: BOW k-means trainer and BOW image descriptor extractor.</p><p>As discussed above, computed SURF descriptors of each image will be stored in the machine memory as a descriptor matrix. This matrix will then become input for BOW k-means trainer in order to cluster the histograms of descriptors which have similar characteristics into separate visual words. BOW k-means trainer takes the predefined dictionary size to be parameter K for k-means++ algorithm introduced in <ref type="bibr" coords="7,456.24,312.16,10.69,8.96" target="#b7">[8]</ref>. This parameter has impact on not only the speed of the system but also its performance. Determining the suitable dictionary size parameter is a real challenge and there were no existing research about this issue. Hence, we chose to give BOW trainer a fix dictionary size value each time we ran the system. Specifically, we selected four dictionary size values which are 256, 512, 1024 and 2048. With different dictionary size values, the system produced different classification outcomes, which will be discussed in the next chapter of this work.</p><p>After clustering the histograms of descriptors into different visual words using BOW trainer, we saved the clustered dictionary into local machine storage as XML file. The dictionary produced by BOW trainer is actually a matrix of floating point number in which the number of columns is the size of SURF descriptor, and the number of rows is the size of BOW dictionary. Below figure shows a sample dictionary data.</p><p>Figure <ref type="figure" coords="7,166.92,652.12,4.18,8.96">7</ref>: Sample dictionary data BOW image descriptor extractor component then takes the dictionary data as its input and extracts (computes) the BOW descriptor matrix for each input image keypoint. This process involves using Fast Library for Approximate Nearest Neighbors (FLANN) matcher <ref type="bibr" coords="8,256.56,162.16,11.71,8.96" target="#b8">[9]</ref> to match between features. The result BOW descriptors are stored as actual feature set for our classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>g. Classification</head><p>The classification layer of our system takes the feature set from the last layer as its input. In our research, we prefer using Support Vector Machine (SVM) as our supervised learning method. Similar to other supervised learning approaches, depend on the type of input data, SVM has two main functions which are training and testing. OpenCV library has already provided all needed functions for SVM training and testing, thus we made full use of the library for our system benefits.</p><p>SVM classifier takes two main parameters for classification: . This is the most popular kernel type for SVM.</p><p>Other support parameters are selected automatically by the system library function which has less impact on the classification performance.</p><p>SVM training time depends mostly on the size of the input data. When experimenting with Flavia dataset, we noticed that our training time range from 1 to 2 hours depends mostly on the size of BOW dictionary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. Results and Discussion</head><p>Concerning SheetAsBackground, our third run has obtained the highest score among three runs (0.314) while the first and the second have the low value of score (0.09). This means that the method based on SURF, BOW and SVM is robust for SheetAsBackground category.</p><p>With the Natural Background, MICA run 2 has a greater value of score than MICA run 1 and MICA run 3. The main reason is that the global descriptor used in MICA run 2 is effective for the Flower category. The obtained score for Leaf category for MICA run 3 is relatively high. This prove that the method based on SURF, BOW and SVM is robust not only for leaf with SheetAsBackground but also for leaf with natural background.</p><p>The results of our runs show that GIST is robust for scene classification but it is not relevant descriptor for plant identification. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,353.76,245.98,2.29,11.79;2,383.16,245.98,2.29,11.79;2,379.08,250.40,3.07,6.23;2,372.00,250.40,3.07,6.23;2,363.60,250.40,3.07,6.23;2,356.16,250.40,3.46,6.23;2,342.12,250.40,1.92,6.23;2,318.60,238.04,3.07,6.23;2,299.76,238.04,3.07,6.23;2,330.60,252.54,5.26,10.67;2,274.44,252.54,5.26,10.67;2,252.48,252.54,5.26,10.67;2,241.68,252.54,5.26,10.67;2,230.64,252.54,5.93,10.67;2,319.80,251.87,2.19,4.45;2,300.96,251.87,2.19,4.45;2,375.36,253.43,2.47,4.45;2,360.12,253.43,2.47,4.45;2,320.16,247.07,2.47,4.45;2,322.56,236.27,2.47,4.45;2,301.56,247.07,2.47,4.45;2,303.72,236.39,2.47,4.45;2,345.00,250.40,3.46,6.23;2,285.36,247.88,3.46,6.23;2,285.12,238.04,3.46,6.23;2,258.36,252.54,3.95,10.67;2,247.08,252.54,2.96,10.67;2,236.88,252.54,3.95,10.67;2,367.44,248.13,3.80,9.01;2,336.36,248.13,3.80,9.01;2,327.60,244.75,2.65,6.97;2,327.60,238.39,2.65,6.97;2,327.60,250.03,2.65,6.97;2,327.60,233.95,2.65,6.97;2,289.68,244.75,2.65,6.97;2,289.68,238.39,2.65,6.97;2,289.68,250.03,2.65,6.97;2,289.68,233.95,2.65,6.97;2,307.92,240.21,3.80,9.01;2,280.20,240.21,3.80,9.01;2,265.20,248.66,6.51,15.45;2,348.48,247.74,3.80,9.53;2,315.00,246.18,3.42,9.53;2,296.40,246.18,3.42,9.53;2,136.08,270.95,72.71,9.50;2,217.20,276.10,3.21,6.51;2,209.88,264.69,6.12,17.07;2,223.20,272.01,2.25,8.10;2,234.24,276.21,3.14,6.37;2,226.56,265.10,5.99,16.70;2,239.88,270.95,230.77,9.50;2,124.68,290.51,175.92,9.50;2,308.28,295.66,3.61,6.51;2,302.04,289.01,14.01,11.16;2,322.80,295.66,3.61,6.51;2,317.64,289.01,153.15,11.16;2,124.68,305.99,345.99,9.50;2,124.68,321.11,143.04,9.50;2,276.00,326.26,3.21,6.51;2,268.68,314.85,6.12,17.07;2,282.00,322.17,2.25,8.10;2,293.04,326.37,3.14,6.37;2,285.36,315.26,5.99,16.70;2,298.68,321.11,171.97,9.50"><head></head><label></label><figDesc>The parameters ( x δ , y δ ) are the standard deviation of the Gaussian envelope along vertical and horizontal directions; ( 0 u , 0 v ) refers to spatial central frequen- cy of Gabor filters. The configuration of Gabor filters contains 4 spatial scales and 8 directions. At each scale ( x δ , y δ ), by passing the image I(x,y) through a</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,136.08,217.36,301.38,8.96;3,138.48,147.36,329.52,66.96"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Plant identification method for Leaf category in MICA run 3</figDesc><graphic coords="3,138.48,147.36,329.52,66.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,136.08,224.56,137.38,8.96"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A grayscale leaf image</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,136.08,607.00,265.75,8.96;4,208.44,511.56,189.72,92.40"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Image normalization produces better image contrast</figDesc><graphic coords="4,208.44,511.56,189.72,92.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,136.08,236.08,333.01,8.96"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Normalized and sharpened grayscale leaf image with new histogram</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,181.92,297.02,288.68,95.02"><head></head><label></label><figDesc>• SVM type: Type of SVM formulation. In our research, we define this parameter as C_SVC or C-Support Vector Classification. It allows imperfect separation of classes for n-class classification with penalty multiplier C for outliers. • Kernel type: Type of SVM kernel. Here we use Radial Basis Function (RBF) kernel for our classification which has the equation</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,136.08,673.72,268.57,8.96"><head>Table 1 : Obtained results of our runs with natural background</head><label>1</label><figDesc></figDesc><table coords="9,124.68,150.88,334.50,83.96"><row><cell></cell><cell>MICA run 1</cell><cell>MICA run 2</cell><cell>MICA run 3</cell></row><row><cell>Entire</cell><cell>0.016</cell><cell>0.016</cell><cell>0.016</cell></row><row><cell>Flower</cell><cell>0.013</cell><cell>0.086</cell><cell>0.013</cell></row><row><cell>Fruit</cell><cell>0.048</cell><cell>0.048</cell><cell>0.048</cell></row><row><cell>Leaf</cell><cell>0.014</cell><cell>0.014</cell><cell>0.11</cell></row><row><cell>Stem</cell><cell>0.014</cell><cell>0.014</cell><cell>0.014</cell></row><row><cell>Naturalbackground</cell><cell>0.023</cell><cell>0.053</cell><cell>0.042</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,143.76,661.36,326.82,8.96;1,124.68,673.36,308.62,8.96"><p>Gist feature present a brief observation or a report at the first glance of a outdoor scene that summarizes the quintessential characteristics of an image</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,128.44,264.04,3.76,8.96;9,160.68,264.04,309.90,8.96;9,160.68,276.04,130.05,8.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,241.20,264.04,229.38,8.96;9,160.68,276.04,41.21,8.96">ImageCLEF 2013: the vision, the data and the open challenges</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,218.76,276.04,41.79,8.96">CLEF2013</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,128.44,288.04,3.76,8.96;9,160.68,288.04,309.68,8.96;9,160.68,300.04,140.97,8.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,232.32,288.04,194.66,8.96">The ImageCLEF 2013 Plant Identification Task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,446.04,288.04,24.32,8.96;9,160.68,300.04,43.10,8.96">CLEF 2013. 2013</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,128.45,312.04,3.76,8.96;9,164.52,312.04,306.07,8.96;9,160.68,324.04,309.81,8.96;9,160.68,336.04,51.21,8.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,285.60,312.04,109.31,8.96">Recognizing Indoor Scenes</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,414.00,312.04,56.59,8.96;9,160.68,324.04,305.53,8.96">Proceeding of the International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>eeding of the International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,128.44,347.44,3.76,8.96;9,159.36,347.44,311.14,8.96;9,160.68,358.84,309.97,8.96;9,160.68,370.36,45.93,8.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,277.56,347.44,192.94,8.96;9,160.68,358.84,156.19,8.96">Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,324.36,358.84,88.93,8.96">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,128.44,382.48,3.76,8.96;9,161.04,382.48,309.54,8.96;9,160.68,394.48,309.81,8.96;9,160.68,406.48,266.49,8.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,284.52,382.48,186.06,8.96;9,160.68,394.48,84.56,8.96">An interactive image retrieval system: from symbolic to semantic</title>
		<author>
			<persName coords=""><forename type="first">Thi-Lan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alain</forename><surname>Boucher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,252.72,394.48,217.77,8.96;9,160.68,406.48,118.67,8.96">International Conference on Electronics, Information, and Communications (ICEIC)</title>
		<meeting><address><addrLine>Hanoi (Vietnam)</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">août 2004</date>
			<biblScope unit="page" from="16" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,128.44,418.48,3.76,8.96;9,160.68,418.48,309.90,8.96;9,160.68,430.48,28.05,8.96" xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Digital</forename><surname>Stephen Johnson On</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Photography</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>O&apos;Reilly Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,128.44,442.48,3.76,8.96;9,160.68,442.48,305.73,8.96" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Rafael</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">E W</forename><surname>González</surname></persName>
		</author>
		<title level="m" coord="9,278.40,442.48,101.07,8.96">Digital Image Processing</title>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,128.44,454.48,3.76,8.96;9,160.68,454.48,310.02,8.96;9,160.68,466.48,309.82,8.96;9,160.68,478.48,70.89,8.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,247.44,454.48,206.16,8.96">k-means++: The Advantages of Careful Seeding</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,160.68,466.48,309.82,8.96;9,160.68,478.48,41.64,8.96">Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms</title>
		<meeting>the eighteenth annual ACM-SIAM symposium on Discrete algorithms</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,128.44,490.48,3.76,8.96;9,160.68,490.48,309.94,8.96;9,160.68,502.48,309.99,8.96;9,160.68,514.48,154.53,8.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,251.64,490.48,218.98,8.96;9,160.68,502.48,98.58,8.96">Fast Approximate Nearest Neighbors with Automatic Algorithm Configuration</title>
		<author>
			<persName coords=""><forename type="first">Marius</forename><surname>Muja</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,277.20,502.48,193.47,8.96;9,160.68,514.48,125.38,8.96">VISAPP International Conference on Computer Vision Theory and Applications</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
