<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,338.91,116.75,141.68,12.93;1,180.39,134.69,254.58,12.93">at ImageCLEF 2013 Scalable Concept Image Annotation</title>
				<funder ref="#_JY6aecR">
					<orgName type="full">Basic Research funds in Renmin University of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,194.51,172.06,41.96,9.96"><forename type="first">Xirong</forename><surname>Li</surname></persName>
							<email>xirong@ruc.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="laboratory" key="lab1">Key Lab of Data Engineering and Knowledge Engineering</orgName>
								<orgName type="laboratory" key="lab2">MOE</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Information</orgName>
								<orgName type="laboratory">Multimedia Computing Lab</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<addrLine>Zhongguancun Street 59</addrLine>
									<postCode>100872</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,253.36,172.06,46.35,9.96"><forename type="first">Shuai</forename><surname>Liao</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory" key="lab1">Key Lab of Data Engineering and Knowledge Engineering</orgName>
								<orgName type="laboratory" key="lab2">MOE</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,310.28,172.06,46.76,9.96"><forename type="first">Binbin</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Information</orgName>
								<orgName type="laboratory">Multimedia Computing Lab</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<addrLine>Zhongguancun Street 59</addrLine>
									<postCode>100872</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,367.60,172.06,48.77,9.96"><forename type="first">Gang</forename><surname>Yang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Information</orgName>
								<orgName type="laboratory">Multimedia Computing Lab</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<addrLine>Zhongguancun Street 59</addrLine>
									<postCode>100872</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,226.32,184.02,32.78,9.96"><forename type="first">Qin</forename><surname>Jin</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Information</orgName>
								<orgName type="laboratory">Multimedia Computing Lab</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<addrLine>Zhongguancun Street 59</addrLine>
									<postCode>100872</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,269.68,184.02,47.45,9.96"><forename type="first">Jieping</forename><surname>Xu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Information</orgName>
								<orgName type="laboratory">Multimedia Computing Lab</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<addrLine>Zhongguancun Street 59</addrLine>
									<postCode>100872</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,327.69,184.02,56.87,9.96"><forename type="first">Xiaoyong</forename><surname>Du</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory" key="lab1">Key Lab of Data Engineering and Knowledge Engineering</orgName>
								<orgName type="laboratory" key="lab2">MOE</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,338.91,116.75,141.68,12.93;1,180.39,134.69,254.58,12.93">at ImageCLEF 2013 Scalable Concept Image Annotation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">97A67DB6FA3226E3F30640157326CCFD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image annotation</term>
					<term>learning from web</term>
					<term>stacked model</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we describe our image annotation system participated in the ImageCLEF 2013 scalable concept image annotation task. The system leverages multiple base classifiers, including singlefeature and multi-feature k NN classifiers and histogram intersection kernel SVMs, all of which are learned from the provided 250K web images and provided features with no extra manual verification. These base classifiers are combined into a stacked model, with the combination weights optimized to maximize the geometric mean of F-samples, F-concepts, and AP-samples metrics on the provided development set. By varying the configuration of the system, we submitted five runs. Evaluation results show that for all of our runs, model stacking with optimized weights performs best. Our system can annotate diverse Internet images purely based on the visual content, at the following accuracy level: F-samples of 0.290, F-concepts of 0.304, and AP-samples of 0.380. What is more, a system-to-system comparison reveals that our system and the best submission this year are complementary with respect to the best annotated concepts, suggesting the potential for future improvement.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Annotating unlabeled images by computers is crucial for organizing and retrieving the ever-growing amounts of images on personal devices and the Internet. Due to the semantic gap, i.e., the lack of coincidence between visual features extracted from the visual data and a user's interpretation on the same data, image auto-annotation is challenging.</p><p>In the context of annotating Internet images, the semantic gap becomes even bigger, as a specific concept exhibits significant diversity in its visual appearance. The imagery of a concept does not limit to realistic photographs, but can also be artificial correspondences such as posters, drawings, and cartoons, as demonstrated in Fig. <ref type="figure" coords="1,199.58,644.49,3.87,9.96" target="#fig_0">1</ref>. To annotate the uncontrolled visual content with a large set of concepts, a promising line of research is to learn from web data which contains many images but with unreliable annotations <ref type="bibr" coords="2,339.72,295.10,7.74,9.96" target="#b0">[1]</ref><ref type="bibr" coords="2,347.46,295.10,3.87,9.96" target="#b1">[2]</ref><ref type="bibr" coords="2,347.46,295.10,3.87,9.96" target="#b2">[3]</ref><ref type="bibr" coords="2,347.46,295.10,3.87,9.96" target="#b3">[4]</ref><ref type="bibr" coords="2,347.46,295.10,3.87,9.96" target="#b4">[5]</ref><ref type="bibr" coords="2,351.33,295.10,7.74,9.96" target="#b5">[6]</ref>. In these works, k Nearest Neighbors (k NN) and Support Vector Machines (SVMs) are two popular classifiers, as have been separately used in <ref type="bibr" coords="2,310.94,319.01,8.18,9.96" target="#b0">[1]</ref><ref type="bibr" coords="2,319.12,319.01,4.09,9.96" target="#b1">[2]</ref><ref type="bibr" coords="2,323.21,319.01,8.18,9.96" target="#b2">[3]</ref> and <ref type="bibr" coords="2,355.57,319.01,7.74,9.96" target="#b3">[4]</ref><ref type="bibr" coords="2,363.31,319.01,3.87,9.96" target="#b4">[5]</ref><ref type="bibr" coords="2,367.18,319.01,7.74,9.96" target="#b5">[6]</ref>. Given the difficulty of the Scalable Concept Image Annotation 2013 task <ref type="bibr" coords="2,359.99,330.96,9.95,9.96" target="#b6">[7]</ref>, we believe that a single classifier is inadequate. In that regard, we develop an image annotation system that combines a number of base classifiers into a stacked model. In our 2013 experiments we submitted five runs, with the purpose of verifying the effectiveness of model stacking.</p><p>The remainder of the paper is organized as follows. We first describe our image annotation system in Section 2. Then we detail our experiments in Section 3, with conclusions given in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The RUC Image Annotation System</head><p>Our image annotation system consists of multiple base classifiers, which are then combined into a stacked model to make final predictions. The base classifiers are learned from the 250K Internet images provided by the 2013 task, while the stacked model is optimized using the development set of 1,000 ground-truthed images. A conceptual diagram of the system is given in Fig. <ref type="figure" coords="2,399.16,515.27,3.87,9.96" target="#fig_2">2</ref>.</p><p>Next, we describe the stacked model in Section 2.1, followed by the base classifiers in Section 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Stacked Image Annotation Model</head><p>For the ease of consistent description, let x be a specific image. For a given concept ω, let g(x, ω) be an image annotation model which produces a relevance score of ω with respect to x. A set of t models are denoted by {g 1 (x, ω), . . . , g t (x, ω)}.</p><p>We define our stacked model as a convex combination of the t models:  where λ i is the nonnegative weight of the i-th classifier, with</p><formula xml:id="formula_0" coords="2,249.60,638.73,230.98,30.60">g Λ (x, ω) = t i=1 λ i • g i (x, ω),<label>(1)</label></formula><formula xml:id="formula_1" coords="3,134.77,280.01,345.81,31.50">t i=1 λ i = 1. Notice that g Λ (x, ω) is to indicate that the stacked model is parameterized by Λ = {λ i }.</formula><p>We look for the setting of {λ i } that maximizes the image annotation performance. The 2013 task specifies three performance metrics <ref type="bibr" coords="3,403.06,318.39,9.95,9.96" target="#b6">[7]</ref>: F-measure for images, F-measure for concepts, and Average Precison for images, as given in the Appendix. To jointly maximize the three metrics, we take their geometric mean as a combined metric. Weights of Eq. ( <ref type="formula" coords="3,337.06,354.26,4.24,9.96" target="#formula_0">1</ref>) optimized with respect to the combined metric are found by a coordinate ascent algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Base Classifiers</head><p>We choose k Nearest Neighbors (k NN) and Support Vector Machines (SVMs) as two types of base classifiers, for their good performance.</p><p>Base classifier I: k NN. Given a test image x, we define the k NN classifier as</p><formula xml:id="formula_2" coords="3,240.68,465.67,239.90,30.60">g knn (x, ω) = k i=1 rel(x i , ω) • w i ,<label>(2)</label></formula><p>where rel(x i , ω) denotes the relevance score between the i-th neighbor and the concept, and w i is the neighbor weight. In this work, we instantiate rel(x i , ω) using the provided textual feature, which is derived from the web page of x i <ref type="bibr" coords="3,467.31,530.53,9.95,9.96" target="#b6">[7]</ref>.</p><p>For w i , we choose a Bayesian implementation <ref type="bibr" coords="3,332.51,543.87,9.95,9.96" target="#b2">[3]</ref>, computing w i as k -1 k j=i j -1 to give more importance to closer neighbors.</p><p>Base Classifier II: SVMs. We choose the histogram intersection kernel SVMs (hikSVMs) which is known to effective for bag of visual words features of medium size <ref type="bibr" coords="3,153.59,596.67,9.95,9.96" target="#b7">[8]</ref>, such as the 5,000-dim features provided by the task. More importantly, the decision function of a hikSVMs can be efficiently computed by a few linear interpolations on a set of precomputed points.</p><p>To obtain relevant positive training examples for a given concept from the 250K Internet images, we utilize both the textual feature and the query log generated when collecting the images from web image search engines. We describe the connection of an Internet image x to a web image search engine s by a triplet &lt; q, r, s &gt;, where q represents a query keyword, r is the rank of x in the search results of q returned by s. An image can be associated with multiple triplets. To determine the positiveness of the image x with respect to the given concept ω, we propose to compute a search engine based score as</p><formula xml:id="formula_3" coords="4,219.29,188.74,257.04,30.60">positiveness(x, ω) = l i=1 sim(q i , ω) w(s i ) √ r i , (<label>3</label></formula><formula xml:id="formula_4" coords="4,476.34,198.69,4.24,9.96">)</formula><p>where l is the number of triplets associated with the image, and sim(q i , ω) is a tag-wise similarity measure, computed on the base of tag co-occurrence in 1.2 million Flickr images <ref type="bibr" coords="4,231.62,254.15,9.95,9.96" target="#b3">[4]</ref>. The variable w(s i ) indicates the weight of a specific search engine, which we empirically set to be 1, 0.5, and 0.5 for Google, Yahoo, and Bing, respectively. The positiveness score in Eq. 3 is further combined with the given textual feature. For each concept, we sort images labeled with the concept in descending order by their positiveness scores, and select the top 500 ranked images as positive training examples.</p><p>For the acquisition of negative training examples, we consider the following two approaches. The first approach is to sample negative examples at random. Given a specific concept and the 500 selected positive examples, we randomly sample a set of 500 negative examples, to make the training data perfectly balanced. We repeat the random sampling 10 times, yielding a set of 10 hikSVMs. The second approach is Negative Bootstrap <ref type="bibr" coords="4,323.87,385.84,9.95,9.96" target="#b8">[9]</ref>. Different from random sampling, this approach iteratively selects negative examples which are most misclassified by present classifiers, and thus most relevant to improve classification. Per iteration, the approach randomly samples 5,000 examples to form a candidate set. An ensemble of classifiers obtained in the previous iterations are used to classify each candidate example. The top 500 most misclassified examples are selected and used together with the 500 positives to train a new hikSVMs. We conduct Negative Bootstrap with 10 iterations, producing 10 hikSVMs for each concept. For efficient classification, we leverage a model compression technique <ref type="bibr" coords="4,134.77,493.43,15.48,9.96" target="#b9">[10]</ref> to compress the ensemble into a single classifier such that the prediction time complexity is independent of the ensemble size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Submitted Runs</head><p>This year we submitted five runs, which are listed as follows. For all runs, we empirically preserve for each image the top six ranked concepts as its final annotation.</p><p>Run: RUC Crane. This run uses a k NN classifier with early fused multiple features (Multi-Feat-k NN). We use all the 7 provided features, i.e., getlf (256-D), colorhist (576-D), gist (480-D), sift (5000-D), csift (5000-D), rgbsift (5000-D), and opponentsift (5000-D). For each feature, we compute the l 1 distance. Distance values of individual features are zero-score normalized, and then averaged to obtain k = 256 nearest neighbors.</p><p>Run: RUC Snake. This run combines multiple single feature k NN (Single-Featk NN) and SVMs classifiers with uniform weights. In particular, five features (colorhist, gist, csift, rgbsift, and opponentsift) are used separately for Single-Feat-k NN, again with the l 1 distance and k = 256. For SVMs, we use the three variants of sift, i.e., csift, rgbsift, and opponentsift. In total, this run employs 5+3+3=11 base classifiers.</p><p>Run: RUC Monkey. This run uses the same 11 base classifiers as have been used in the Snake run, but with optimized weights. Consequently, the effectiveness of the stacked model can be verified by comparing the Monkey and the Snake.</p><p>Run: RUC Mantis. This run combines the Multi-Feat-k NN, multiple Single-Feat-k NN, and 6 variants of SVMs with uniform weights. So the run employs 12 base classifiers in total.</p><p>Run: RUC Tiger. This run uses the same 12 base classifiers as have been used in the Mantis run, but with optimized weights. The Tiger is our primary run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>The performance scores of the five runs are summarized in Table <ref type="table" coords="5,438.07,407.00,3.87,9.96" target="#tab_0">1</ref>. For all performance metrics, the Tiger run, which combines all the base classifiers using the coordinate ascent optimized weights, is the best. On the test set, this run reaches an F-samples score of 0.290, an F-concepts score of 0.304, and an APsamples score of 0.380. The Monkey run using optimized weights is better than the Snake run using uniform weights. Moreover, by comparing the performance on the development set and on the test set, we find that the system generalizes well to unseen images and concepts. These results justify the importance of model stacking and weights optimization for image annotation. Fig. <ref type="figure" coords="6,154.40,486.60,4.12,8.08">3</ref>. A system-to-system comparison. The concepts have been sorted in terms of their F-concepts scores. The median score of each concept is the median of the F-concepts scores of all the 58 submissions. The RUC Tiger run is clearly above the average level. What we find more interesting is that our system and the best submission seem complementary, showing the potential for future improvement.</p><p>Fig. <ref type="figure" coords="6,170.40,620.58,4.98,9.96">3</ref> shows a system-to-system comparison, measured by F-concepts. The RUC image annotation system is clearly above the average level. Moreover, when compared with the best submission of this year, we find that the two systems are complementary, as their best annotated concepts differ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussions and Conclusions</head><p>This paper documents our experiments in the ImageCLEF 2013 Scalable Concept Image Annotation, a testbed for developing image annotation systems using generic web data. We have built such a system.</p><p>Our system annotates images purely based on the visual content. It combines multiple base classifiers, i.e., variants of k NN and SVMs, into a stacked model. In all of our five submitted runs, the stacked model with optimized weights performs best.</p><p>Through a system-to-system comparison, we find that our system and the best submission this year is complementary in the sense of the best annotated concepts. Given that we use relatively simple base classifiers, we consider this finding interesting, as it suggests the potential of future improvement.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,134.77,252.01,345.83,8.96;2,134.77,262.96,189.22,8.96;2,141.69,123.79,90.72,91.02"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Internet images and ground truth tags from the development set of the Scalable Concept Image Annotation 2013 task.</figDesc><graphic coords="2,141.69,123.79,90.72,91.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,134.77,238.52,345.80,8.08;3,134.77,249.49,277.68,8.08"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. A conceptual diagram of the RUC image annotation system participated in the Scalable Concept Image Annotation 2013 task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,134.77,534.02,361.36,126.69"><head>Table 1 .</head><label>1</label><figDesc>Settings and performance of our submitted runs. The symbol + indicates that a specific base classifier is used in a specific run. Our primary run Tiger, which combines all base classifiers with optimized weights, performs best.</figDesc><table coords="5,138.78,579.85,357.34,80.86"><row><cell></cell><cell></cell><cell>Based classifiers</cell><cell></cell><cell cols="2">F-samples F-concepts AP-samples</cell></row><row><cell>Run</cell><cell cols="4">Multi-Feat-kNN Single-Feat-kNN SVMs Weights dev test dev test</cell><cell>dev test</cell></row><row><cell>Crane</cell><cell>+</cell><cell>-</cell><cell>-N.A.</cell><cell cols="2">28.8 25.4 26.6 23.9 36.1 32.4</cell></row><row><cell>Snake</cell><cell>-</cell><cell>+</cell><cell cols="3">+ uniform 28.8 26.5 30.8 28.5 38.2 35.5</cell></row><row><cell>Monkey</cell><cell>-</cell><cell>+</cell><cell cols="3">+ optimized 31.0 28.3 32.7 29.6 40.5 37.6</cell></row><row><cell>Mantis</cell><cell>+</cell><cell>+</cell><cell cols="3">+ uniform 29.8 27.8 31.4 29.2 39.4 36.9</cell></row><row><cell>Tiger</cell><cell>+</cell><cell>+</cell><cell cols="3">+ optimized 31.6 29.0 33.4 30.4 41.2 38.0</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This research was supported by the <rs type="funder">Basic Research funds in Renmin University of China</rs> from the central government (<rs type="grantNumber">13XNLF05</rs>). The authors are grateful to the <rs type="institution">ImageCLEF</rs> coordinators for the benchmark organization efforts.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_JY6aecR">
					<idno type="grant-number">13XNLF05</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>A1. Performance Measures F-samples. Given a test image x with its relevant tag set R x and a predicted tag set P x , its F-samples score is computed as</p><p>where precision(x) is</p><p>F-concepts. Given a test concept ω with its relevant image set R ω and a set of images P ω labeled with ω by the annotation system, its F-concepts score is computed as</p><p>where precision(ω) is</p><p>AP-samples. Given a test image x with m tags sorted in descending order by relevance scores, its AP-samples score is computed as</p><p>where r i is the number of relevant tags among the top i tags, and δ(i) is 1 if the i-th tag is in R x , 0 otherwise.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="8,142.95,142.98,337.63,8.96;8,151.52,153.94,329.05,8.96;8,151.52,164.91,121.95,8.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,331.16,142.98,149.43,8.96;8,151.52,153.94,53.63,8.96">Annotating images by mining image search results</title>
		<author>
			<persName coords=""><forename type="first">X</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,214.22,153.94,266.36,8.96">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1919" to="1932" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.95,175.86,337.63,8.96;8,151.52,186.82,140.94,8.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,280.59,175.86,200.00,8.96;8,151.52,186.82,54.75,8.96">Annotating images by harnessing worldwide usertagged photos</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,228.62,186.82,35.21,8.96">ICASSP</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.95,197.78,337.63,8.96;8,151.52,208.74,159.41,8.96" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="8,262.20,197.78,218.39,8.96;8,151.52,208.74,66.83,8.96">A k-nn approach for scalable image annotation using general web data</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<editor>BigVision.</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.95,219.70,337.62,8.96;8,151.52,230.66,324.76,8.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,350.61,219.70,129.97,8.96;8,151.52,230.66,57.33,8.96">Harvesting social images for biconcept search</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,216.86,230.66,138.46,8.96">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1091" to="1104" />
			<date type="published" when="2012-08">Aug. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.95,241.61,337.62,8.96;8,151.52,252.57,329.06,8.96;8,151.52,263.54,41.46,8.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,289.08,241.61,191.50,8.96;8,151.52,252.57,105.51,8.96">Sampling and ontologically pooling web images for visual concept learning</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">W</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,265.23,252.57,138.70,8.96">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1068" to="1078" />
			<date type="published" when="2012-08">Aug. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.95,274.49,337.63,8.96;8,151.52,285.45,209.89,8.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,294.88,274.49,185.71,8.96;8,151.52,285.45,131.10,8.96">Evaluating sources and strategies for learning video concepts from social media</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kordumova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,305.24,285.45,27.51,8.96">CBMI</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.95,296.40,337.63,8.96;8,151.52,307.37,294.67,8.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,316.75,296.40,163.84,8.96;8,151.52,307.37,135.98,8.96">Overview of the imageclef 2013 scalable concept image annotation subtask</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,309.82,307.37,103.77,8.96">CLEF 2013 working notes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.95,318.33,337.63,8.96;8,151.52,329.28,158.54,8.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,267.66,318.33,212.92,8.96;8,151.52,329.28,79.17,8.96">Classification using intersection kernel support vector machines is efficient</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,252.23,329.28,29.19,8.96">CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.95,340.24,337.64,8.96;8,151.52,351.21,159.91,8.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,345.97,340.24,134.63,8.96;8,151.52,351.21,81.38,8.96">Social negative bootstrapping for visual categorization</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,255.01,351.21,22.22,8.96">ICMR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.61,362.16,337.97,8.96;8,151.52,373.12,329.05,8.96;8,151.52,384.08,81.52,8.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,396.50,362.16,84.09,8.96;8,151.52,373.12,152.66,8.96">Bootstrapping visual categorization with relevant negatives</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Koelma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,314.12,373.12,140.36,8.96">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="933" to="945" />
			<date type="published" when="2013-06">Jun. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
