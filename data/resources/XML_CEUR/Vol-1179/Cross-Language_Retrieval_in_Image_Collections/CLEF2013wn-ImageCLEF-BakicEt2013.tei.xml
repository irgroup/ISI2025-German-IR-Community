<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,141.95,115.96,331.46,12.62;1,241.98,133.89,131.39,12.62">Inria&apos;s participation at ImageCLEF 2013 Plant Identification Task</title>
				<funder>
					<orgName type="full">Agropolis</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,134.98,171.56,45.99,8.74"><forename type="first">Vera</forename><surname>Bakić</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inria</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,188.83,171.56,63.90,8.74"><forename type="first">Sofiene</forename><surname>Mouine</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inria</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,261.06,171.56,107.54,8.74"><forename type="first">Saloua</forename><surname>Ouertani-Litayem</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inria</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,376.63,171.56,99.19,8.74"><forename type="first">Anne</forename><surname>Verroust-Blondet</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inria</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,206.79,183.51,63.83,8.74"><forename type="first">Itheri</forename><surname>Yahiaoui</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inria</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,278.49,183.51,54.24,8.74"><forename type="first">Hervé</forename><surname>Goëau</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inria</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,360.50,183.51,48.08,8.74"><forename type="first">Alexis</forename><surname>Joly</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inria</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,141.95,115.96,331.46,12.62;1,241.98,133.89,131.39,12.62">Inria&apos;s participation at ImageCLEF 2013 Plant Identification Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C60724250FF1FADB04A8DE08AA751750</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Pl@ntNet</term>
					<term>Inria</term>
					<term>ImageCLEF</term>
					<term>plant</term>
					<term>leaves</term>
					<term>flowers</term>
					<term>fruits</term>
					<term>stem</term>
					<term>multi-organ</term>
					<term>image</term>
					<term>collection</term>
					<term>identification</term>
					<term>classification</term>
					<term>evaluation</term>
					<term>benchmark</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of Inria within the Pl@ntNet project 1 at ImageCLEF2013 plant identification task. For the SheetAsBackground category (scans or photographs of leaves with a uniform background), the submitted runs used a multiscale triangle-based approaches, either alone or combined with other shape-based descriptors. For the NaturalBackground category (unconstrained photographs of leaves, flowers fruits, stems,...), the four submitted runs used local features extracted using different geometric constraints. Three of them were based on large scale matching of individual local feature, while the last one used a fisher vector representation. Metadata like the flowering date or/and plant identifier were successfully combined to the visual content. Overall the proposed methods performed very well for all categories and sub-categories.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The plant identification task of ImageCLEF2013 <ref type="bibr" coords="1,347.97,459.96,10.52,8.74" target="#b2">[3]</ref>  <ref type="bibr" coords="1,361.48,459.96,10.52,8.74" target="#b6">[7]</ref> was organized as a plant species retrieval task over 250 species with visual content being the main available information. Two categories were considered: (i) a SheetAsBackground category containing exclusively leaves on a white background and (ii) a Natural-Background category containing unconstrained photographs of leaves, flowers, fruits, stems and entire plant views. The identification score was related to the rank of the correct species in the list of retrieved species averaged over the authors of the pictures and the observed plants (the task organizer argues that this weighted metric reduce some bias induced by this particular context of botanical dataset as explained in last year plant task overview <ref type="bibr" coords="1,359.42,567.56,12.65,8.74" target="#b7">[8]</ref>).</p><p>Inria, within the Pl@ntNet project, submitted four runs, in both image categories. But the methods used different algorithms: for the SheetAsBackground queries, the submitted runs were based on shape boundary features (see Section 2), while large scale matching approaches or fisher vectors with SVM classifiers were used for the NaturalBackground category (see <ref type="bibr" coords="1,382.16,627.33,42.67,8.74">Section 3)</ref>. Results are discussed in Section 4, followed by conclusions and perspectives in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods used for the SheetAsBackground category</head><p>We used the triangular representations presented in <ref type="bibr" coords="2,365.98,154.75,14.61,8.74" target="#b15">[16]</ref>: they are fast to compute and yielded promising results on scan-like images. Moreover, these efficient local approaches are robust to partial leaf occlusions. Two declinations of multiscale triangle-based descriptors were used: -TOA: a multiscale triangular shape descriptor, where the triangles are described by two successive oriented angles <ref type="bibr" coords="2,310.38,214.53,14.61,8.74" target="#b15">[16]</ref>. We used 400 sample contour points and 20 triangles associated to each contour point, with a distance d = 2 between the triangle points at two successive scales.</p><p>-TSLA: a multiscale triangular shape descriptor where the triangles are described by their lengths and an angle <ref type="bibr" coords="2,305.41,262.35,14.61,8.74" target="#b15">[16]</ref>. Here, the leaf contour is described by 400 sample points, each point is represented by 10 triangles, with a distance d = 5 between the triangle points at two successive scales. We experimented also combinations of these representations with two complementary descriptions: -DFH -Shapes: a 2D Directional Fragment Histogram (DFH) that computes directions and relative lengths on a succession of elementary fragments on the contour, associated with a set of geometric metrics of the shape (Shapes) <ref type="bibr" coords="2,457.31,346.03,14.61,8.74" target="#b22">[23]</ref>.</p><p>-The SC2 descriptor proposed within the advanced shape context <ref type="bibr" coords="2,433.44,357.99,14.61,8.74" target="#b14">[15]</ref>, which computes the spatial correlation between the leaf salient points, computed by an Harris detector, and its margin. All these descriptors require a preliminary leaf boundary extraction, performed here by using the Otsu thresholding method <ref type="bibr" coords="2,333.62,405.81,14.61,8.74" target="#b17">[18]</ref>. The species retrieval process involves a local matching process for TOA, TSLA and SC2, while global comparisons are performed for DFH -Shapes (see <ref type="bibr" coords="2,338.25,429.72,15.50,8.74" target="#b22">[23,</ref><ref type="bibr" coords="2,355.41,429.72,12.73,8.74" target="#b14">15,</ref><ref type="bibr" coords="2,369.80,429.72,12.73,8.74" target="#b15">16]</ref> for more details). In summary, we used the following descriptors: TOA in Inria PlantNet Run 1, TSLA in Inria PlantNet Run 2, TSLA + DFH -Shapes in Inria PlantNet Run 3 and TOA + SC2 in Inria PlantNet Run 4. Multiple image queries. For each run, we used the fact that images in the test dataset are associated with plant observations in order to perform multiple image queries for leaf images having the same P lantID value (cf. Figure <ref type="figure" coords="3,468.97,142.90,3.87,8.74" target="#fig_0">1</ref>). More precisely, for each descriptor:</p><p>-We first grouped all the images I 1 , ...I k coming from the same plant observation using the P lantID in metadata.</p><p>-Then, we computed the retrieved images similarity ranking lists L 1 , ...L k corresponding to the query images I 1 , ...I k .</p><p>-Finally, the 100 first image results were kept for each list and were merged into a final list L using a late fusion with the Leave Out algorithm (LO) <ref type="bibr" coords="3,440.17,226.59,15.50,8.74" target="#b13">[14]</ref> (lists are merged by setting the rank of an image to the minimum of the ranks in each list). Thus, the best position of an image among the returned lists is kept. Descriptors combination. For the two last runs, the descriptors combination has been performed by a late fusion on the feature similarity ranking lists resulting from the multiple image queries using the same LO fusion process.</p><p>3 Methods used for the NaturalBackground category</p><p>For the NaturalBackground category, we explored the following directions: the fact that a plant view organ is generally centred (Section 3.1 and 3.4), the fact that there are sometimes several images from the same plant observation, the fact that species have not the same flowering periods (Section 3.2), and finally whether the automatic segmentation improves performances (Section 3.3). The 4 runs are based on the same local features: Interest points detection. Harris corners were used at four distinct resolutions with multiple orientations ( <ref type="bibr" coords="3,251.74,432.93,10.52,8.74" target="#b5">[6]</ref>). In addition, as in <ref type="bibr" coords="3,346.24,432.93,9.96,8.74" target="#b0">[1]</ref>, to minimize the effect of the cluttered background, a rhomboid-shaped mask was applied to the input image and more weight was given to the points closer to the center of the image. Fig. <ref type="figure" coords="3,134.77,468.80,4.43,8.74" target="#fig_1">2</ref>(a) illustrates the detected points. About 400 points per image were output. Local features are extracted around each interest point from an oriented and scaled patch: rotation invariant Local Binary Pattern (ri-LBP) <ref type="bibr" coords="3,417.00,493.25,14.61,8.74" target="#b16">[17]</ref>; SURF <ref type="bibr" coords="3,470.08,493.25,10.52,8.74" target="#b1">[2]</ref> (sums of 2D Haar wavelet responses), we used OpenSURF <ref type="bibr" coords="3,389.62,505.21,9.96,8.74" target="#b3">[4]</ref>; a 20-dim. Fourier histogram <ref type="bibr" coords="3,181.15,517.16,9.96,8.74" target="#b4">[5]</ref>; an 8-dim. Edge Orientation Histogram (EOH); a 27-bin weighted RGB histogram (wght-RGB) <ref type="bibr" coords="3,264.23,529.12,10.52,8.74" target="#b4">[5]</ref> and a 30-bin HSV histogram;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Local features and weighted descriptor fusion → RUN1</head><p>After a series of tests with the training data, we concluded that not all type of local features should be used for all views. As might be expected, color is dominant for the flower but may lead to confusion for leaves, while texture plays an important role for stem and fruits. The most discriminant features retained for the methods are ri-LBP, SURF, Fourier, wght-RGB, histo-HSV for Flowers, Fruit, Stem and Entire, while only ri-LBP, SURF, Fourier, EOH are computed for Leaves. For each combination of one view and one type of feature, local features are computed, compressed and indexed using RMMH method <ref type="bibr" coords="4,455.68,333.52,15.50,8.74" target="#b11">[12,</ref><ref type="bibr" coords="4,472.84,333.52,7.75,8.74" target="#b8">9]</ref> using a 256-bit hash code, which led to a total of 24 unique hash tables.</p><p>For a query image Q, according to its view, the basic algorithm applied is: (i) to retrieve lists of similar images for each type of feature, (ii) transform each list of images into a probability distribution by species, (iii) merge probabilities for all types of features.</p><p>(i) Image retrieval: A response list R is composed of similar images I i ranked by a score S i . The score S i is the number of matches between an image I i and the query image Q, more precisely the number of instances of image I i retrieved through the nearest neighbors lists of each local feature of the query image Q: the description of a local feature is compressed with RMMH and its approximate 30-nearest neighbors are searched by probing multiple neighboring buckets in the consulted hash table (according to the a posteriori multi-probe algorithm described in <ref type="bibr" coords="4,190.12,494.82,14.76,8.74" target="#b12">[13]</ref>). We define i as the rank of the image in R (we limit i to 300), M i the plant observation id (referenced as IndividualP lantId in the metadata), and C i the label identifying a species.</p><p>(ii) Probability distribution: For converting an image response list to a species probability distribution, we use an adaptive rule focusing on plant observations (rather than images). Indeed, the more a species will be represented in a response through various plants observed by distinct users at different dates and locations, the more the associated images will be informative for predicting a species. In contrast, numerous redundant near duplicate images from the same plant observation will not be really informative for predicting a species.</p><p>Instead of using the top-K images for decision (as in the last year's runs <ref type="bibr" coords="4,134.77,632.21,10.30,8.74" target="#b7">[8]</ref>), we search for top-K classes (species) represented with at least K different plant observations. The values of K and K are determined empirically based on the given training database and are constant for a database: K is a per-Query image from class Cercis siliquastrum:  centage of the average size of the training class, while K is a percentage of the number of training classes. The response R is scanned from the most to the least similar image, the counter of the number of classes with at least K images is incremented accordingly, and when we find K such classes, we stop the scanning of the response. The adaptive criterion is imporant in order to avoid the noise in the final response: (a) had we searched for a fixed (and not dependent on the training data) number of classes with at least K plant observations, we would often output classes that are not relevant to fill-in the pre-defined requirement, or (b) had we output a fixed number of most similar classes, we would give more weight to the classes with small number of plant observations and would not reward the fact that some classes are well represented in terms of plant observations. Finally, our K per class resulted in the K per image of 85, ranging from 15 to 205, for the most to the least different-plant-observation-containing response; organ-wise, the values of K ranged from 66 for Stem to 101 for Fruit. Moreover, to eliminate the redundant images, we consider only two most similar images per one plant observation: the score per plant observation S m is a simple average of the image scores S i . The score for a class is a simple sum of the scores of its plant observations S m : this step actually favorizes the well-represented classes, and penalizes the classes with small number of plant observations. Finally, the classes with only one image comming from one plant observation are removed from the list as outliers. Figure <ref type="figure" coords="5,180.67,632.21,4.98,8.74" target="#fig_3">3</ref> shows an example of the probability computation from a ranked list of 8 images: the first returned image has high score S i , however, it is the only image of the only plant observation for it's class and this image is ignored in the final classes list; next three images belong to the same plant observation, so we will keep only the first two scores S i ; finally, the last four images belong to the same class as the query image, however, unlike for the previous group, they are all from different plant observations. Thus, we will use all scores in the calculation of the class probability.</p><p>(iii) Probability fusion: At this step we have several species probability distributions, one for each kind of feature and we use a weighted fusion in order to obtain a final probability distribution. Let us define F as a set of local features and P (C k f ) as probability of class C k for feature f ∈ F . In order to reflect the discriminating power of each local feature, we define the final probability: <ref type="table" coords="6,176.63,400.91,4.98,8.74" target="#tab_0">1</ref> shows the average weights attributed to each local feature for all test images, displayed per organ. The last two columns show the difference between the minimal and maximal average weight per feature and the average difference between the minimal and maximal weight per image. Color-based features have higher weights for Flower, Entire and Fruit, than gray-based ones. For graybased features, ri-LBP contributes more than SURF and Fourier for Flower, while it is Fourier which contributes the most for Stem. For Leaf, SURF and ri-LBP contributed more than Fourier and EOH. We can note that all the average weights are actually rather close to the W unif , and that all the chosen local features play an important role in the overall decision. The values ∆ avg (W ) ≈ W unif show that on the image level, there was always a local feature that had the weight significantly lower than the others, thus the influence of that, presumably, confusing response was minimized in the overall response.</p><formula xml:id="formula_0" coords="6,149.71,379.88,307.15,29.77">P (C k ) = f ∈F w(f ) * P (C k f ), where w(f ) = max ∀k P (C k f )/ f ∈F max ∀k P (C k f ) Table</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-image queries and filtering by flowering period → RUN2</head><p>This run is an extension of the RU N 1, with two modifications making use of metadata: (i) the responses from all images belonging to the same plant observation are merged and (ii) only for the Flower images, flowering dates for filtering irrelevant species in responses. Multi-image queries: As in the training dataset, images from the test dataset are sometimes related to a same plant observation which was explicitly men-  We can note here that for Entire and Leaf, the proposed species have lower probability with respect to the species proposed for other views, which results to higher weights assigned for Stem and Fruit (line 4(d)). Finally, the ranking list of proposed species 4(e) is lead by the first species from the, presumably, least confusing view response (Stem) while the correct species is at the 3 rd position. Finally, it is not only 2 query images of Fruit, but all the 10 query images which are associated with a relevant species response.</p><p>Filtering by flowering period: Unlike other views, Flower has an important feature: in many species these plant organs are present for just a quite short period of time, and each species has its own flowering periods. The metadata contains the date when the photograph was taken, for the training, as well as for the test datasets. A post-processing treatment is applied to the list of species obtained through pure visual search, and only the species for which the date of query image was observed in the training data were retained.</p><p>The flowering period histogram (Figure <ref type="figure" coords="8,329.79,568.96,4.98,8.74" target="#fig_4">5</ref> (a)) for a species is constructed by week, with ±3 additional weeks to account for geographical and year-to-year differences. Given a training image of class C k , taken in week w, histogram bins H C k (h), h = w -3, ..., w + 3 are incremented. For a query image Q taken in week w Q , an histogram H Q is constructed in the same manner, and finally a species</p><formula xml:id="formula_1" coords="8,134.77,627.63,206.35,11.96">C k is retained if ∃w|H Q (w) &gt; 0 ∧ H C k (w) &gt; 0.</formula><p>Figure <ref type="figure" coords="8,181.91,644.16,4.98,8.74" target="#fig_4">5</ref> (a) shows the flowering periods for four species that have similar color and texture. Cichorium intybus flowers appear cleary later than the other species over a year. Thus, any query images in this period will exclude the three other species, even if the visual content are very similar to these species.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Automatic segmentation → RUN3</head><p>In this run we tested the automatic fore-/back-ground segmentation in an attempt to reduce the number of the Harris points in the (cluttered) background. The segmentation algorithm used was Otsu <ref type="bibr" coords="9,327.89,203.28,14.61,8.74" target="#b17">[18]</ref>, with an addition of automatic selection of one of LUV colorspace channels that gives the best separation. Then, we automatically checked if the region was well-formed or if the fore-and background classes were too mixed as in <ref type="bibr" coords="9,299.77,239.14,9.96,8.74" target="#b0">[1]</ref>. For the correct segmentation, all the regions that do not touch the boundary significantly are considered as foreground object; for the rejected segmentation, we used rhomboid mask. Only the points that fall in the foreground regions were kept. Overall, the number of points was reduced by 30%, varying from 16% for Flower to 40% for Stem.</p><p>Figure <ref type="figure" coords="9,180.85,298.92,4.98,8.74" target="#fig_1">2</ref> shows (a) initial detected points and (b) the filtered set. The distribution in (a) is clearly in a rhomboid shape, while in the case (b) the points are all in the foreground object or on its edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Multi-modal image representation through embedded local features → RUN4</head><p>This run is performed in order to explore combining local features through an embedding schema for images representation and multi-class learning approach.</p><p>We have used fisher vectors representation <ref type="bibr" coords="9,326.79,407.12,15.50,8.74" target="#b19">[20]</ref> for the embedding of the local features as it has been proved as a successful extension of the popular bag-ofvisual word (BOV) representation <ref type="bibr" coords="9,286.20,431.03,14.61,8.74" target="#b21">[22]</ref>.</p><p>Image representation: Fisher vectors representation (FV) extends the BOV representation where the local patches are described by their deviation from a generative Gaussian mixture model (GMM). Let X = {x t , x t ∈ D , t = 1...T } be the set of T D-dimensional local descriptors of an image I. Assuming that the generation process of X can be modelled by the GMM model with parameters λ, we can characterize an image I by the following gradient vector <ref type="bibr" coords="9,426.28,502.76,14.61,8.74" target="#b9">[10]</ref>:</p><formula xml:id="formula_2" coords="9,251.56,523.51,112.23,30.20">G X λ = 1 T ∇ λ T t=1 log µ λ (x t )</formula><p>where λ = {ω i , µ i , σ i , i = 1...K} and ω i , µ i and σ i are respectively the mixture weight, mean vector and variance matrix (assumed diagonal) of Gaussian µ i . We used the gradient vector wrt. the mean only on GMM with K Gaussians as it was presented in <ref type="bibr" coords="9,222.13,596.34,14.61,8.74" target="#b10">[11]</ref>: Jegou et al. stated that in a set of experiments, for a fixed FV size, the option led to a very similar result to that of gradient with respect to the variance and mean. In that case, the FV is the concatenation of the gradient vectors G X i with respect to the mean µ i of Gaussian i. It is therefore K d-dimensional, K being the number of Gaussians and d the local descriptor dimensionality. Vocabulary learning: For each image I we used the same local features extracted for the three previous runs (Section 3.1). Let X desci = {x t , x t ∈ di , t = 1...T } be the set of T d i -dimensional local descriptors of an image I according to a descriptor desc i . Thus each image is represented by a set of descriptors {X desc1 , ...X descn }, and for each descriptor desc i , we estimated a d i -dimensional GMM with K = 256. As the GMM estimation is an unsupervised step, we used both the train and test datasets in order to enhance the representation. One GMM was estimated for each sub-category (Flower, Fruit, Stem, Entire, Leaf ) and description space. We computed separately a FV for each local description using implementation <ref type="bibr" coords="10,234.78,416.13,14.61,8.74" target="#b10">[11]</ref>. The generated FVs are power and L2-normalized <ref type="bibr" coords="10,134.77,428.09,14.61,8.74" target="#b20">[21]</ref>. Finally, each image I is characterized by a concatenation of the FVs. Assuming that Y is the fusion of the FV representations of the image I, Y is a D-dimensional vector:</p><formula xml:id="formula_3" coords="10,215.18,469.17,184.29,14.11">Y = {y j , j = 1...D} with D = n i=1 d i * K</formula><p>Classifier training: For each sub-category we learned a standard approach with a linear one-versus-all multi-class SVM classifier. For efficiency, we trained our classifiers using Stochastic Gradient Descent (SGD) algorithm as in <ref type="bibr" coords="10,450.61,516.09,14.61,8.74" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>SheetAsBackground. The four methods obtained very good results, all of them being in the top 5 with the run Inria PlantNet Run 2 reaching the second position. We discuss the fact that the ranking of the 4 runs differs from what we observed during preliminary tests with a Leave One Out procedure on the training dataset (see Table <ref type="table" coords="10,252.67,620.25,3.87,8.74" target="#tab_1">2</ref>). For instance TSLA method, which had the lowest preliminary results with a 4 th position on the training dataset, is finally the 1 st on the testing dataset. As noticed in <ref type="bibr" coords="10,294.32,644.16,14.61,8.74" target="#b15">[16]</ref>, the respective orders of the TSLA and TOA scores may depend on the dataset. Here, the train and the test datasets may lead to different results as they do not involve the same number of authors (36 for the train and 14 for the test) and the same number of individual plants (732 for the train and 150 for the test) and these two parameters are taken into account in the ImageCLEF scores. Nevertheless, the two scores are close, thus we can consider that this two descriptors have similar performances. Moreover, the combinations TSLA+DFH-Shapes and TOA+SC2 increase the scores of TSLA or TOA on the training dataset while it is not the case on the test images. This may be due to some unsuccessful automatic extractions of the leaf contour on a part of the test dataset as illustrated in Figure <ref type="figure" coords="11,392.94,367.64,3.87,8.74" target="#fig_7">7</ref>. The quality of the automatic segmentation is closely dependent on the noise that may be present in the image. Multiscale triangular methods are robust to partial occlusion and to local contour deformations, since the local descriptors associated to the contour points describe only a portion of the leaf boundary (cf. <ref type="bibr" coords="11,380.16,415.46,15.50,8.74" target="#b15">[16]</ref> for a discussion on this subject). But the local descriptors associated to the salient points in SC2 represent the relative positions of the points with respect to the margin and are sensitive to contour deformations. It is also the case for DFH-Shapes, which provides a description of the whole contour. Thus combining any multiscale triangular descriptor with either SC2 or DFH-Shapes decreases the score when the contour is incorrectly extracted. NaturalBackground. 4 th , 5 th and 7 th . Basically, our scores are closely interlaced with the ones from NlabUTokyo, whereas the runs of all other groups are significantly lower.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Inria PlantNet Run 2 had the best scores within our runs as well in details for the 5 sub-categories. We can note also that overall gain for Inria PlantNet Run 2 with respect to Inria PlantNet Run 1 was 0.032, ranging from 0.011 for Fruit to 0.057 for Flower, which shows the benefits that can be done when using multi-image queries and flowering period filtering.</p><p>Inria PlantNet Run 3 achieved only 0.028 (8%) smaller score than Inria PlantNet Run 1, while using 30% less interest points, which is an interesting result if we consider memory resources for implementation in a final system. But we must note that for Flower and Fruit, the losses were much smaller (3-4%) comparing to Entire, Leaf and Stem (15-19%). We suspect that this is due to the segmentation problems with the later three views and that in those cases, alternative segmentation methods could be applied.</p><p>Inria PlantNet Run 4 reached a quite good results if we compare with all submitted runs. However, the method experimented obtained lower score than the method used in Inria PlantNet Run 1 which was using the same local features. One explanation can be that the choice of a fusion by concatenation of Fisher Vectors computed in separated features space was not the most relevant fusion scheme because the discriminating powers of each local feature was not visible, or the concatenation led to normalisation problems. Also, the fisher embedding itself might degrade performances compared to the independent knn search of each local feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>Inria submitted four runs that used different processing methods for SheetAs-Background and NaturalBackgroud categories. The task was more challenging this year, with twice of species and images, and more visual diversity with the 5 different plant views. Our methods performed very well and our best runs were 2 nd in both categories.</p><p>For SheetAsBackground, the results obtained by our four runs (in the top 5 positions) confirmed the effectiveness of our multiscale triangle based approaches <ref type="bibr" coords="13,134.77,142.90,15.50,8.74" target="#b15">[16]</ref> for the identification of plant species from scan and scan-like leaf images.</p><p>For NaturalBackground, we tested local features matching with several declinations as the use of metadata or automatic segmentation. For the first three runs, we proposed to use a weighted fusion scheme for combining the responses from multiple indexes build for each combination of type of views and type of local features. It underlines the discriminant power of a non-confusing response. With the use of metadata, such as the plant ids and the date of photograph, we showed that multi-image queries and filtering by flowering period can clearly increase the baseline performance. On the other hand, with the use of automatic segmentation, we could reduce the size of the training features, while having a rather small performance decrease. We also note that, as for other participants, the score for Flower category is double the score for other categories and that the best Flower scores are approaching the scores obtained in SheetAsBackground category.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,134.77,644.07,345.57,8.37;2,134.77,655.05,345.82,7.86"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Multiple image queries. I1, ...I k are leaf images associated to the same P lantID tag and RP is the retrieval process involving either TOA, TSLA, SC2 or DFH -Shapes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,135.45,296.74,344.45,7.89;4,137.01,201.85,79.53,59.65"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Examples of filtering of the interest points with the background segmentation</figDesc><graphic coords="4,137.01,201.85,79.53,59.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,136.48,161.89,53.63,7.95;5,249.99,162.33,27.04,6.12;5,393.48,162.33,21.30,6.12;5,156.80,173.29,39.01,6.12;5,240.95,173.29,45.13,6.12;5,382.72,173.29,42.81,6.12;5,136.48,181.73,13.51,11.11;5,174.32,184.25,3.97,6.12;5,227.13,184.25,72.75,6.12;5,344.34,182.89,4.61,7.86;5,381.49,182.89,4.61,7.86;5,418.67,182.89,4.61,7.86;5,458.99,182.89,4.61,7.86;5,136.48,245.30,4.38,7.37;5,145.09,240.27,4.38,17.73;5,136.48,261.07,12.68,7.80;5,146.39,261.95,4.37,2.66;5,171.69,259.94,9.22,7.86;5,216.37,259.94,9.22,7.86;5,256.55,259.94,9.22,7.86;5,299.07,259.94,9.22,7.86;5,342.04,259.94,9.22,7.86;5,379.19,259.94,9.22,7.86;5,418.67,259.94,4.61,7.86;5,458.99,259.94,4.61,7.86;5,136.48,272.57,4.38,7.37;5,145.96,276.64,4.37,4.27;5,143.95,270.27,4.37,6.12;5,171.69,271.29,9.22,7.86;5,201.96,271.01,123.10,7.76"><head>21 (</head><label>21</label><figDesc>12 + 11)/2 = 11.5, ignore 3 rd img.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,148.92,345.30,317.51,7.89"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Example of the probabilities computation from a ranked list of images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,150.52,278.25,314.32,7.89;8,161.17,190.47,69.15,52.04"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Flowering periods for sample images with similar color and/or texture</figDesc><graphic coords="8,161.17,190.47,69.15,52.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="8,149.71,315.19,330.88,8.74;8,134.77,327.14,345.83,8.74;8,134.77,339.10,345.83,8.74;8,134.77,351.06,345.83,8.74;8,134.77,363.01,345.82,8.74;8,134.77,374.97,345.83,8.74;8,134.77,386.92,345.83,8.74;8,134.77,398.88,345.83,8.74;8,134.77,410.83,345.82,8.74;8,134.77,422.79,345.82,8.74;8,134.77,434.74,345.82,8.74;8,134.77,446.70,44.28,8.74;8,179.05,445.12,8.06,6.12;8,190.92,446.70,289.67,8.74;8,134.77,458.65,307.68,8.74"><head>Figure 4</head><label>4</label><figDesc>Figure 4 shows an example of the fusion on one set of query images from one plant observation (IndividualP lantID = 4165) from the training dataset in a leave one out procedure during preliminary tests. The correct species is Viburnum tinus and 4 views are represented by 10 images (4(a) shows sample images for each view). 4(b) shows the first proposed species per image and we can see that Viburnum tinus does not appear in all responses. 4(c) shows the responses combined per view.We can note here that for Entire and Leaf, the proposed species have lower probability with respect to the species proposed for other views, which results to higher weights assigned for Stem and Fruit (line 4(d)). Finally, the ranking list of proposed species 4(e) is lead by the first species from the, presumably, least confusing view response (Stem) while the correct species is at the 3 rd position. Finally, it is not only 2 query images of Fruit, but all the 10 query images which are associated with a relevant species response.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="10,157.83,274.70,299.70,7.89;10,134.77,115.84,345.81,144.09"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Multimodal image representation through embedded local features</figDesc><graphic coords="10,134.77,115.84,345.81,144.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="11,165.46,598.32,284.43,7.89;11,184.95,518.71,86.45,64.84"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Examples of unsuccessful segmentation on test dataset images.</figDesc><graphic coords="11,184.95,518.71,86.45,64.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,134.77,116.61,346.81,96.26"><head>Table 1 .</head><label>1</label><figDesc>Organ W unif SURF Fourier EOH ri-LBP wght-RGB histo-HSV ∆(W avg ) ∆ avg (W ) Distribution of average weights attributed to each local feature for test images (per organ) and average maximum weight delta over all test images.</figDesc><table coords="6,136.16,129.74,337.33,51.70"><row><cell cols="2">Flower 0.2</cell><cell>0.169 0.171</cell><cell>-</cell><cell>0.209</cell><cell>0.230</cell><cell>0.221</cell><cell>0.061</cell><cell>0.178</cell></row><row><cell cols="2">Entire 0.2</cell><cell>0.192 0.175</cell><cell>-</cell><cell>0.196</cell><cell>0.221</cell><cell>0.216</cell><cell>0.046</cell><cell>0.158</cell></row><row><cell>Fruit</cell><cell>0.2</cell><cell>0.188 0.182</cell><cell>-</cell><cell>0.186</cell><cell>0.223</cell><cell>0.221</cell><cell>0.041</cell><cell>0.158</cell></row><row><cell>Leaf</cell><cell cols="4">0.25 0.271 0.233 0.237 0.259</cell><cell>-</cell><cell>-</cell><cell>0.038</cell><cell>0.176</cell></row><row><cell>Stem</cell><cell>0.2</cell><cell>0.197 0.216</cell><cell>-</cell><cell>0.210</cell><cell>0.192</cell><cell>0.184</cell><cell>0.032</cell><cell>0.173</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="11,134.77,200.28,345.83,97.28"><head>Table 2 .</head><label>2</label><figDesc>SheetAsBackground scores of the 4 runs on the training dataset during preliminary experiments compared with the official ImageCLEF scores on test datatest. Best scores are in bold for each column.</figDesc><table coords="11,137.51,200.28,340.33,54.49"><row><cell></cell><cell>ImageCLEF run</cell><cell cols="2">Training dataset score ImageCLEF score</cell></row><row><cell>TOA</cell><cell>Inria PlantNet Run 1</cell><cell>0.763</cell><cell>0.557</cell></row><row><cell>TSLA</cell><cell>Inria PlantNet Run 2</cell><cell>0.726</cell><cell>0.577</cell></row><row><cell cols="2">TSLA+DFH-Shapes Inria PlantNet Run 3</cell><cell>0.766</cell><cell>0.572</cell></row><row><cell>TOA+SC2</cell><cell>Inria PlantNet Run 4</cell><cell>0.753</cell><cell>0.517</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="11,134.77,632.21,345.83,32.65"><head>Table 3 .</head><label>3</label><figDesc>Table3summarizes the official scores for the first 7 submitted runs together with our own runs. Inria PlantNet Run 2 is 2 nd overall and the 1 st for the sub-category Flower. Inria PlantNet Run 1, 3, 4 are respectively Official scores for the first two for NaturalBackground. The best result per image type is highlighted in bold, Inria PlantNet runs are marked with →.</figDesc><table coords="12,168.17,117.38,281.81,95.93"><row><cell></cell><cell>Overall</cell><cell>Score par organ</cell></row><row><cell>Run</cell><cell>score</cell><cell>Entire Flower Fruit Leaf Stem</cell></row><row><cell>NlabUTokyo Run 3</cell><cell>0.393</cell><cell>0.297 0.472 0.311 0.275 0.253</cell></row><row><cell cols="3">Inria PlantNet Run 2 → 0.385 → 0.274 0.494 0.260 0.272 0.240</cell></row><row><cell>NlabUTokyo Run 2</cell><cell>0.371</cell><cell>0.273 0.484 0.259 0.273 0.285</cell></row><row><cell cols="3">Inria PlantNet Run 1 → 0.353 → 0.254 0.437 0.249 0.240 0.211</cell></row><row><cell>NlabUTokyo Run 1</cell><cell>0.341</cell><cell>0.236 0.423 0.209 0.269 0.276</cell></row><row><cell cols="3">Inria PlantNet Run 3 → 0.325 → 0.216 0.421 0.238 0.195 0.176</cell></row><row><cell cols="3">Inria PlantNet Run 4 → 0.245 → 0.150 0.327 0.137 0.165 0.171</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,656.80,136.49,7.86"><p>http://www.plantnet-project.org/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. Part of this work was funded by the <rs type="funder">Agropolis</rs> foundation through the project Pl@ntNet (http://www.plantnet-project.org/)</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>tioned in metadata with the tag IndividualP lantId. Thus, we have some query images showing the same plant on distinct organs with different angles (Figure <ref type="figure" coords="7,134.77,527.23,3.87,8.74">4</ref>). In such cases, we try to take advantage from the complementarity of the views in order to compute a unique ranking list of species for one plant id. Then, the ranking list was repeated in the run file for each image of the associated plant.</p><p>For each plant associated to at least two image queries, the fusion is applied in two-stages: </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="13,142.96,377.92,337.63,7.86;13,151.52,388.88,329.07,7.86;13,151.52,399.84,317.29,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="13,283.85,388.88,196.75,7.86;13,151.52,399.84,94.76,7.86">Inria imedia2&apos;s participation at ImageCLEF 2012 plant identification task</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Bakić</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mouine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ouertani-Litayem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Ouertani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Verroust-Blondet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,267.28,399.84,168.12,7.86">CLEF (Notebook Papers/Labs/Workshop</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,410.19,337.64,7.86;13,151.52,421.15,253.31,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="13,344.89,410.19,131.91,7.86">Surf: Speeded up robust features</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,165.60,421.15,210.56,7.86">Computer Vision and Image Understanding (CVIU)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,431.51,337.64,7.86;13,151.52,442.47,329.07,7.86;13,151.52,453.42,324.59,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,407.88,442.47,72.71,7.86;13,151.52,453.42,176.70,7.86">ImageCLEF 2013: the vision, the data and the open challenges</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zellhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">G</forename><surname>Varea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cazorla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,349.26,453.42,98.18,7.86">Proc CLEF 2013. LNCS</title>
		<meeting>CLEF 2013. LNCS</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,463.78,333.04,7.86" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Evans</surname></persName>
		</author>
		<title level="m" coord="13,195.99,463.78,117.15,7.86">Notes on the opensurf library</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>University of Bristol</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. rep.</note>
</biblStruct>

<biblStruct coords="13,142.96,474.13,337.63,7.86;13,151.52,485.09,304.82,7.86" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ferecatu</surname></persName>
		</author>
		<title level="m" coord="13,207.65,474.13,272.94,7.86;13,151.52,485.09,104.40,7.86">Image retrieval with active relevance feedback using both visual and keyword-based descriptors</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>Univ. Versailles St-Quentin</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct coords="13,142.96,495.44,337.63,7.86;13,151.52,506.40,329.07,7.86;13,151.52,517.36,243.50,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,228.75,506.40,126.95,7.86">Multi-organ plant identification</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Barbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Bakić</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Barthélémy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,377.32,506.40,103.27,7.86;13,151.52,517.36,214.84,7.86">Workshop on Multimedia Analysis for Ecological Data Proceedings. MAED &apos;12</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,527.71,337.64,7.86;13,151.52,538.67,329.07,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,172.11,538.67,182.77,7.86">The ImageCLEF 2013 plant identification task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Bakić</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Barthélémy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Molino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,374.07,538.67,106.52,7.86">CLEF 2013 Working Notes</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,549.03,337.64,7.86;13,151.52,559.98,329.07,7.86;13,151.52,570.94,25.60,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,174.17,559.98,193.06,7.86">The ImageCLEF 2012 plant identification task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Bakić</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Barthélémy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Molino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,390.54,559.98,90.05,7.86">CLEF Working Notes</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,581.30,337.64,7.86;13,151.52,592.26,329.07,7.86;13,151.52,603.21,173.75,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,399.84,581.30,80.75,7.86;13,151.52,592.26,282.21,7.86">Participation of IN-RIA&amp; Pl@ntNet to ImageCLEF 2011 plant images classification task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mouysset</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,455.89,592.26,24.70,7.86;13,151.52,603.21,140.35,7.86">CLEF (Notebook Papers/Labs/Workshop</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,613.57,337.98,7.86;13,151.52,624.53,299.26,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,264.00,613.57,216.60,7.86;13,151.52,624.53,15.85,7.86">Exploiting generative models in discriminative classifiers</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,199.21,624.53,210.62,7.86">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,634.88,337.98,7.86;13,151.52,645.84,329.07,7.86;13,151.52,656.80,305.81,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,453.69,634.88,26.90,7.86;13,151.52,645.84,199.16,7.86">Aggregating local image descriptors into compact codes</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<ptr target="http://hal.inria.fr/inria-00633013" />
	</analytic>
	<monogr>
		<title level="j" coord="13,357.82,645.84,122.77,7.86;13,151.52,656.80,137.32,7.86">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,119.67,336.35,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="14,240.45,119.67,140.41,7.86">Random maximum margin hashing</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Buisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,402.17,119.67,48.12,7.86">CVPR 2011</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,130.63,337.98,7.86;14,151.52,141.59,329.07,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="14,240.86,130.63,200.66,7.86">A posteriori multi-probe locality sensitive hashing</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Buisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,463.04,130.63,17.56,7.86;14,151.52,141.59,300.77,7.86">Proceedings of the 16th ACM international conference on Multimedia. MM &apos;08</title>
		<meeting>the 16th ACM international conference on Multimedia. MM &apos;08</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,152.55,337.97,7.86;14,151.52,163.51,329.07,7.86;14,151.52,174.47,207.28,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="14,349.93,152.55,130.66,7.86;14,151.52,163.51,210.24,7.86">Image retrieval based on similarity score fusion from feature similarity ranking lists</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jović</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Hatakeyama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Hirota</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,383.01,163.51,97.58,7.86;14,151.52,174.47,178.60,7.86">International conference on Fuzzy Systems and Knowledge Discovery</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,185.43,337.98,7.86;14,151.52,196.39,329.07,7.86;14,151.52,207.34,232.33,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="14,343.84,185.43,136.76,7.86;14,151.52,196.39,184.87,7.86">Advanced shape context for plant species identification using leaf image retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mouine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Verroust-Blondet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,356.62,196.39,123.98,7.86;14,151.52,207.34,138.69,7.86">ACM International Conference on Multimedia Retrieval, ICMR&apos;12</title>
		<imprint>
			<date type="published" when="2012-06">Jun 2012</date>
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,218.30,337.98,7.86;14,151.52,229.26,329.07,7.86;14,151.52,240.22,289.59,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="14,347.78,218.30,132.81,7.86;14,151.52,229.26,229.71,7.86">A shape-based approach for leaf classification using a multiscale triangular representation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mouine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Verroust-Blondet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,403.14,229.26,77.45,7.86;14,151.52,240.22,186.08,7.86">ACM International Conference on Multimedia Retrieval, ICMR&apos;13</title>
		<imprint>
			<date type="published" when="2013-04">Apr 2013</date>
			<biblScope unit="page" from="309" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,251.18,337.97,7.86;14,151.52,262.14,329.07,7.86;14,151.52,273.10,326.08,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="14,323.62,251.18,156.96,7.86;14,151.52,262.14,177.99,7.86">Gray scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mäenpää</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,351.06,262.14,125.33,7.86">Computer Vision -ECCV 2000</title>
		<title level="s" coord="14,151.52,273.10,141.41,7.86">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">1842</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,284.06,337.98,7.86;14,151.52,295.02,110.15,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="14,190.25,284.06,233.72,7.86">A Threshold Selection Method From Gray-Level Histogram</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,430.73,284.06,49.86,7.86;14,151.52,295.02,81.48,7.86">IEEE Trans. Syst., Man, Cybern</title>
		<imprint>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,305.98,337.97,7.86;14,151.52,316.93,295.77,7.86" xml:id="b18">
	<monogr>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<title level="m" coord="14,385.12,305.98,95.47,7.86;14,151.52,316.93,133.58,7.86">IEEE Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Providence (RI), United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,327.89,337.98,7.86;14,151.52,338.85,114.95,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="14,267.35,327.89,213.24,7.86;14,151.52,338.85,38.65,7.86">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,211.18,338.85,26.62,7.86">CVPR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,349.81,337.97,7.86;14,151.52,360.77,231.05,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="14,314.38,349.81,166.21,7.86;14,151.52,360.77,76.41,7.86">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,248.64,360.77,23.24,7.86">ECCV</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.62,371.73,337.98,7.86;14,151.52,382.69,325.06,7.86" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="14,365.92,371.73,114.67,7.86;14,151.52,382.69,138.79,7.86">Image classification with the Fisher vector: Theory and practice</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">Inria, project team LEAR</note>
</biblStruct>

<biblStruct coords="14,142.62,393.65,337.98,7.86;14,151.52,404.61,125.56,7.86" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="14,326.25,393.65,154.35,7.86;14,151.52,404.61,51.37,7.86">Leaf shape descriptor for tree species identification</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Mzoughi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,223.71,404.61,24.70,7.86">ICME</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
