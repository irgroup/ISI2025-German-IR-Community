<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,140.92,116.81,333.51,15.48;1,229.23,138.73,156.90,15.48">REGIMRobvid: Objects and scenes detection for Robot vision 2013</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,134.77,179.09,44.51,8.64"><forename type="first">Amel</forename><surname>Ksibi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Engineering School of Sfax (ENIS)</orgName>
								<orgName type="laboratory">REGIM (REsearch Group on Intelligent Machines)</orgName>
								<orgName type="institution">University of Sfax</orgName>
								<address>
									<postBox>BP 1173</postBox>
									<postCode>3038</postCode>
									<settlement>Sfax</settlement>
									<country key="TN">Tunisia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,185.60,179.09,90.84,8.64"><roleName>Anis</roleName><forename type="first">Boudour</forename><surname>Ammar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Engineering School of Sfax (ENIS)</orgName>
								<orgName type="laboratory">REGIM (REsearch Group on Intelligent Machines)</orgName>
								<orgName type="institution">University of Sfax</orgName>
								<address>
									<postBox>BP 1173</postBox>
									<postCode>3038</postCode>
									<settlement>Sfax</settlement>
									<country key="TN">Tunisia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,278.75,179.09,80.88,8.64"><roleName>Chokri</roleName><forename type="first">Ben</forename><surname>Ammar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Engineering School of Sfax (ENIS)</orgName>
								<orgName type="laboratory">REGIM (REsearch Group on Intelligent Machines)</orgName>
								<orgName type="institution">University of Sfax</orgName>
								<address>
									<postBox>BP 1173</postBox>
									<postCode>3038</postCode>
									<settlement>Sfax</settlement>
									<country key="TN">Tunisia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,361.94,179.09,41.05,8.64"><forename type="first">Ben</forename><surname>Amar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Engineering School of Sfax (ENIS)</orgName>
								<orgName type="laboratory">REGIM (REsearch Group on Intelligent Machines)</orgName>
								<orgName type="institution">University of Sfax</orgName>
								<address>
									<postBox>BP 1173</postBox>
									<postCode>3038</postCode>
									<settlement>Sfax</settlement>
									<country key="TN">Tunisia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,422.00,179.09,58.59,8.64"><forename type="first">Adel</forename><forename type="middle">M</forename><surname>Alimi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Engineering School of Sfax (ENIS)</orgName>
								<orgName type="laboratory">REGIM (REsearch Group on Intelligent Machines)</orgName>
								<orgName type="institution">University of Sfax</orgName>
								<address>
									<postBox>BP 1173</postBox>
									<postCode>3038</postCode>
									<settlement>Sfax</settlement>
									<country key="TN">Tunisia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,140.92,116.81,333.51,15.48;1,229.23,138.73,156.90,15.48">REGIMRobvid: Objects and scenes detection for Robot vision 2013</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A6F612C6E33E0D7B3C0D4BCB92614301</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Scene selection</term>
					<term>object detection</term>
					<term>PEGASOS SVM</term>
					<term>PHOW</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of the REGIM team in the Im-ageCLEF 2013 Robot Vision Challenge. The competition was focused on the problem of objects and scenes classification in indoor environments. Objects and scenes are considered as concepts. During the competition, we aim to classify images according to the room in which they were acquired, using the information provided by the visual images only. Our system is based on PHOW features extraction and PEGASOS SVM algorithm to learn a multi-class classifier that is enable to detect the objects and the adequate scene. For this end, we focus on how to interpret the scores provided by the SVM classifier. Our system was ranked 4th among 6 teams.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, robotics has witnessed a large growth and profound change in scope.</p><p>Visual detection has becoming one of the most popular research topics and it is playing an important role in robotics ( <ref type="bibr" coords="1,258.05,459.99,10.45,8.64" target="#b0">[1]</ref>, <ref type="bibr" coords="1,275.25,459.99,10.58,8.64" target="#b1">[2]</ref>, <ref type="bibr" coords="1,292.62,459.99,11.62,8.64" target="#b3">[4]</ref> and <ref type="bibr" coords="1,325.15,459.99,10.45,8.64" target="#b7">[8]</ref>). The ImageCLEF 2013 Robot Vision challenge has been the fifth edition of a competition that started in 2009 within the ImageCLEF as part of the <ref type="bibr" coords="1,242.22,483.90,10.58,8.64" target="#b6">[7]</ref>. The challenge addresses the problem of semantic place classification using visual and depth information. This time, the task also addresses the challenge of object and scene recognition. The rooms/categories that appear in the  <ref type="figure" coords="2,398.10,132.26,3.60,8.64" target="#fig_0">1</ref>). The eight objects that can appear in any image of the database are: Extinguisher, Computer, Chair, Printer, Urinal, Screen, Trash, and Fridge (see fig. <ref type="figure" coords="2,304.02,156.17,4.15,8.64" target="#fig_1">2</ref>) <ref type="bibr" coords="2,314.81,156.17,10.58,8.64" target="#b4">[5]</ref>. In order to determine the presence or absence of an object, two thresholds are used (score average and score average plus standard deviation). The object does not exist if its score is below the average. Else, if its score exceeds the mean standard deviation, then the object exists. Else if the score is between the two values, this object is unknown. The scene is selected if its score is the maximum. If the score is over than the average, then the scene is relevant. Else, the scene is unknown. The remainder of this paper is organized as follows: the next section explains the proposed scene and object detection process. Section 3 summarizes the experiments of the work and discusses the obtained results. Finally, section 4 draws conclusions and provides suggestions for future work. In this section, we describe the developed system for RobotVision2013 task participation. Two sets of concepts are used: Object concepts and Scene concepts. In our system, we aim to learn two appropriate classifiers multi-classes using visual features and machines learning for objects detection and scene detection. Given an image, we hope to describe it using N objects and M locations or scenes. More specifically, we must detect one appropriate location and some objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Concept learning a)PHOW features extraction</head><p>Visual images are represented by a Pyramid Histogram of Visual Words (PHOW) <ref type="bibr" coords="2,466.48,657.43,10.58,8.64" target="#b2">[3]</ref>, Fig. <ref type="figure" coords="3,251.14,391.97,3.36,8.06">3</ref>. Overview of the proposed system which are a variant of dense SIFT descriptors, extracted at multiple scales. In fact, the PHOW descriptor involves computing visual words on a dense grid <ref type="bibr" coords="3,405.30,440.89,15.27,8.64" target="#b9">[10]</ref>. To compute these features, a dictionary of visual words was first generated by quantizing the SIFT descriptors that capture the local spatial distribution of gradients. ELKAN kmeans clustering is selected to perform quantization. We fixed, here, the dictionary size to 300 visual words. Then, in order to characterize the joint distribution of appearance and location of the visual words in an image, each image is divided into regions at multiple scales (44 subdivisions). So, a spatial histogram is computed for each image sub-region at each scale. b) Object and scene learning Object and location detection, in Robot Vision task, requires extremely fast classification. Or, automatic concept learning relying on computationally heavy kernel-based classifiers, such as non-linear SVMs, are disabled to accomplish this need resulting in a computational bottleneck. In fact, we argue that the critical efficiency criterion is the classifier evaluation cost. There have been numerous approaches to reduce the computational complexity from the level of standard non-linear SVMs such as the homogeneous kernel map which is used in our process over the Chi2 kernel SVM.</p><p>Firstly, given the obtained spatial histograms, we train the PEGASOS stochastic gradient descent as a linear SVM classifier in order to build efficiently, concepts models. While the PEGASOS SVM is very fast to train <ref type="bibr" coords="3,328.96,657.43,10.58,8.64" target="#b8">[9]</ref>, it cannot typically match the per-formance of non-linear, as it is limited to use an inner product to compare descriptors. Therefore, we perform a step of data pre-transforming through computing the homogeneous kernel map that provides a linear representation of a Chi2 kernel. This linear approximation is used, then, to train a Chi2-kernel SVM, by applying the linear SVM solver PEGASOS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Concept scores estimation</head><p>Given a test image, we classify it using two obtained models, respectively, for objects and location detection. The outputs of each classifier are the concept having the best score and two detection scores vectors. Since an image can contain more than one object, the decision of the object classifier is enough sufficient. So, we need to perform a process of object selection to deduce other relevant concepts underlying this image. In addition, the selected concept by the scene classifier can have a low score despite having the highest one among other concepts. Therefore, the process of scene selection needs to be improved in a way that for the corresponding case, the system will give the result "unknown".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Concept Selection</head><p>Concept selection can be either objects selection or scene selection. Objects selection aims to choose an optimal subset of a predefined concepts list that is able to capture the semantic content of the corresponding image. In contrast, scene selection aims to find the most adequate scene. As the obtained concepts scores are sparse, we need, firstly, a step of normalization and thresholding to discriminate the most representative objects and the most probably detected scene <ref type="bibr" coords="4,195.07,430.36,10.58,8.64" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>a. Scores normalization</head><p>The conventional normalization formula is as follows:</p><formula xml:id="formula_0" coords="4,252.67,482.98,227.92,22.32">Vector[i] = vector[i] -min max -min<label>(1)</label></formula><p>where i=1..N.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>i)Scores normalization by each image</head><p>For each image, we perform the normalization process using the above formula. We obtain in each one, obligatory one object having score equal to "1" and one object having a score equal to "0".</p><p>In case where all objects are present, this normalization will discard the object having the low scores. In contrast, in case where any object is present, this formula will detect, always, an object with score equal to "1".</p><p>To overcome this problem, we propose to use normalization by each concept.</p><p>ii) Scores normalization by each concept Given a matrix of all obtained scores for all images in the validation collection, we perform for each concept the above normalization formula.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>b. Threshold for concepts selection</head><p>After the normalization of the two probability scores, we perform a process of concept selection which aims to define an adequate threshold that separate relevant concepts from others. Defining an optimal threshold for all concepts is suboptimal. So, we need to estimate for each one the corresponding threshold. i) Object selection Given an object, the threshold is calculated with respect to the distribution of scores of this object in all images in the validation dataset. Two formulas are tested in experiments:</p><formula xml:id="formula_1" coords="5,262.01,236.28,90.13,55.41">τ =          1 N N ∑ i=1 c q i 1 N N ∑ i=1 c q i + σ<label>(2)</label></formula><p>Where: N is the number of images, c q i is the score of concept q in image i. σ is the standard deviation of all scores for N images. ii) Scene selection For a given image, the system selects the scene having the highest score. Meanwhile, we need to verify this decision. In fact, we define a threshold to decide if the system is sure or has an ambiguity. This threshold is equal to the average of all scores of images according to this scene concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and results</head><p>This section presents the results of the Robot Vision task of ImageCLEF 2013 for the subtask: task1. Six groups registered to the Robot Vision 2013. A total of 16 runs were submitted. The limit of the number of runs that could be submitted was 3. For the competition we submitted two systems: the first is a system based on PHOW features extraction and PEGASOS SVM classifier with normalization and one threshold, the second uses the same techniques PHOW + PEGASOS SVM but with normalization and two thresholds. Our system ranked forth, achieving 4638.250 points on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSIONS and future work</head><p>We have described in this article the fifth edition of the Robot Vision task at Image-CLEF 2013, which attracted an attention of 6 groups submitting runs. We propose an approach, which detects the location and some objects using PHOW features extraction and PEGASOS SVM classifier. First, an off line module was performed before starting the test. It consists of PHOW extraction descriptors for object and scene concepts and the training step using PEGA-SOS SVM learning method. The online process is the concepts scores estimation and the selection of concepts using the PEGASOS SVM model. Future work aims at exploiting the described methods to help elderly and disabled persons seated in wheeled chairs. Furthermore, it is also planned to use different techniques of tracking like Extended Kalman filter or incremental PCA (Principal Component Analysis) to track the detected objects.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,153.53,655.79,308.31,8.12;1,144.93,539.81,325.50,101.25"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Some existing rooms in the database (a) Hall (b) Proffessor office (c) Secretary</figDesc><graphic coords="1,144.93,539.81,325.50,101.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,155.41,432.18,304.55,8.12;2,145.30,318.44,324.75,99.00"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Some existing objects in the database (a) Extinguisher (b) Computer (c) Chair</figDesc><graphic coords="2,145.30,318.44,324.75,99.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="3,168.55,116.83,278.25,260.40"><head></head><label></label><figDesc></figDesc><graphic coords="3,168.55,116.83,278.25,260.40" type="bitmap" /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to acknowledge the financial support of this work by grants from General Direction of Scientific Research (DGRST), Tunisia, under the ARUB program.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="6,142.61,267.41,337.98,7.77;6,150.95,278.37,329.64,7.77;6,150.95,289.33,191.13,7.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,297.15,267.41,175.97,7.77">Learning System for Standing Human Detection</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Rokbani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Alimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,150.95,278.37,300.97,7.77">IEEE International Conference on Computer Science and Automation Engineering</title>
		<meeting><address><addrLine>CSAE; Shanghai</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06">2011. 10-12 June 2011</date>
			<biblScope unit="page" from="300" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.61,300.28,337.98,7.77;6,150.95,311.24,329.64,7.77;6,150.95,322.20,223.71,7.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,283.35,300.28,197.24,7.77;6,150.95,311.24,45.91,7.77">Incremental Learning Approach for Human Detection and Tracking</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Wali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Alimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,206.86,311.24,273.74,7.77;6,150.95,322.20,48.00,7.77">7th International Conference on Innovations in Information Technology (Innovations&apos;11)</title>
		<meeting><address><addrLine>Abu Dhabi</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-04-27">25-27 April 2011</date>
			<biblScope unit="page" from="128" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.61,333.16,337.98,7.77;6,150.95,344.12,73.31,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,296.80,333.16,180.43,7.77">Image classifcation using random forests and ferns</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Munoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,160.66,344.12,38.92,7.77">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.61,355.08,337.98,7.77;6,150.95,366.04,329.64,7.77;6,150.95,377.00,240.78,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,344.24,355.08,136.35,7.77;6,150.95,366.04,71.65,7.77">Learning system for mobile robot detection and tracking</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bousnina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Baklouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Alimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,233.20,366.04,247.39,7.77;6,150.95,377.00,54.82,7.77">International Conference on Communications and Information Technology (ICCIT)</title>
		<meeting><address><addrLine>Hammamet Tunisia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06">June 2012</date>
			<biblScope unit="page" from="384" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.61,387.96,337.98,7.77;6,150.95,398.91,329.64,7.77;6,150.95,409.87,290.63,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,359.82,398.91,120.78,7.77;6,150.95,409.87,101.68,7.77">ImageCLEF 2013: the vision, the data and the open challenges</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zellhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">Martinez</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Varea</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Cazorla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,258.52,409.87,96.08,7.77">Proceedings of CLEF 2013</title>
		<meeting>CLEF 2013</meeting>
		<imprint>
			<publisher>Springer LNCS</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.61,420.83,337.98,7.77;6,150.95,431.79,329.64,7.77;6,150.95,442.75,103.11,7.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,347.71,420.83,132.88,7.77;6,150.95,431.79,241.66,7.77">REGIMvid at ImageCLEF2012: Improving Diversity in Personal Photo Ranking Using Fuzzy Logic</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Feki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ksibi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Amar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,400.10,431.79,80.49,7.77;6,150.95,442.75,16.14,7.77">Proceedings of CLEF 2011</title>
		<meeting>CLEF 2011</meeting>
		<imprint>
			<publisher>Springer LNCS</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.61,453.71,337.98,7.77;6,150.95,464.67,329.64,7.77;6,150.95,475.63,20.17,7.77" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,435.91,453.71,44.68,7.77;6,150.95,464.67,148.99,7.77">Overview of the ImageCLEF 2013 Robot Vision Task</title>
		<author>
			<persName coords=""><forename type="first">Jesus</forename><surname>Martinez-Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ismael</forename><surname>Garcia-Varea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Miguel</forename><surname>Cazorla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,306.79,464.67,107.87,7.77">Working notes of CLEF 2013</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.61,486.59,337.98,7.77;6,150.95,497.55,228.50,7.77" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,291.39,486.59,189.20,7.77;6,150.95,497.55,72.65,7.77">Motion Detection and Object Tracking for an AIBO Robot Soccer Player</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ruiz-Del-Solar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">A</forename><surname>Vallejos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,233.13,497.55,53.16,7.77">Robotic Soccer</title>
		<imprint>
			<publisher>Pedro Lima</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.61,508.50,337.98,7.77;6,150.95,519.46,257.86,7.77" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,384.76,508.50,95.83,7.77;6,150.95,519.46,100.04,7.77">Pegasos: primal estimated sub-gradient solver for SVM</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cotter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,258.65,519.46,96.53,7.77">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="30" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.24,530.42,338.35,7.77;6,150.95,541.38,221.21,7.77" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,197.34,530.42,203.00,7.77">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,406.20,530.42,74.39,7.77;6,150.95,541.38,69.77,7.77">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
