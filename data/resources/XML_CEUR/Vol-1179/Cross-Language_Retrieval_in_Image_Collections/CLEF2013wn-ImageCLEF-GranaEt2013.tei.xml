<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,159.95,115.96,295.46,12.62;1,211.64,133.89,192.08,12.62">UNIMORE at ImageCLEF 2013: Scalable Concept Image Annotation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,148.19,171.56,77.92,8.74"><forename type="first">Costantino</forename><surname>Grana</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DIEF Department</orgName>
								<orgName type="institution">University of Modena and Reggio Emilia</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,236.66,171.56,66.32,8.74"><forename type="first">Giuseppe</forename><surname>Serra</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DIEF Department</orgName>
								<orgName type="institution">University of Modena and Reggio Emilia</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,313.54,171.56,70.07,8.74"><forename type="first">Marco</forename><surname>Manfredi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DIEF Department</orgName>
								<orgName type="institution">University of Modena and Reggio Emilia</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,394.17,171.56,65.76,8.74"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DIEF Department</orgName>
								<orgName type="institution">University of Modena and Reggio Emilia</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,206.71,183.51,84.04,8.74"><forename type="first">Riccardo</forename><surname>Martoglia</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">FIM Department</orgName>
								<orgName type="institution">University of Modena and Reggio Emilia</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,320.69,183.51,83.48,8.74"><forename type="first">Federica</forename><surname>Mandreoli</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">FIM Department</orgName>
								<orgName type="institution">University of Modena and Reggio Emilia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,159.95,115.96,295.46,12.62;1,211.64,133.89,192.08,12.62">UNIMORE at ImageCLEF 2013: Scalable Concept Image Annotation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B2C564D82167120915DDB1D42AE7162C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>image retrieval</term>
					<term>image classification</term>
					<term>multi-class</term>
					<term>multi-label</term>
					<term>stochastic gradient descent</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we propose a large-scale Image annotation system for the Scalable Concept Image Annotation task. For each concept to be detected a separated classifier is built using the provided textual annotation. Images are represented as a Multivariate Gaussian distribution of a set of local features extracted over a dense regular grid. Textual analysis, on the web pages containing training images, is performed to retrieve a relevant set of samples for learning each concept classifier. An online SVMs solver based on Stochastic Gradient Descent is used to manage the large amount of training data. Experimental results show that the combination of different kind of local features encoded with our strategy achieves very competitive performance both in terms of mAP and mean F-measure.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>University of Modena and Reggio Emilia group (UNIMORE) participated at ImageCLEF 2013 <ref type="bibr" coords="1,215.43,488.75,10.52,8.74" target="#b0">[1]</ref> to the Scalable Concept Image Annotation Task <ref type="bibr" coords="1,447.38,488.75,9.96,8.74" target="#b1">[2]</ref>, and identified two possible strategies to attack the problem: finding images similar to the query, and from those extract the image concepts, leveraging the provided textual annotation, or directly using the textual annotation to roughly annotate the training set and then for every concept building a classifier applicable to the query.</p><p>The first approach is the organizers baseline, while for the second we annotated every training image with a concept if the concept word was found in the scofeats file, then we used the provided BoW CSIFT features to build a classifier for each concept. This second strategy largely outperformed the baseline, so we further expanded this second approach. In our experiments we aimed at improving the features and the initial textual annotation. Instead of relying on the BoW model we propose to describe the local features as a Multivariate Gaussian Distribution, which employs a full rank covariance matrix, thus leading to a large feature vector (for a 128 dimensional SIFT descriptor a 8,384 dimensional vector for each spatial pyramid region). We partitioned the image into 1x1, 2x2, 1x3 regions, thus a total of 8 spatial regions and a 67,072 dimensional vector which becomes 201,216 dimensional for color based SIFTs.</p><p>For the textual part, stopword removal and stemming is performed on the scofeats file, then the titles of the original web pages are extracted and parsed. Moreover we built a context from the WordNet definition, in order to detect if different words fall in the same concept context. Finally a negative context is similarly built by other senses of the same word.</p><p>We used a linear SVM classifier for each concept, built using a Stochastic Gradient Descent online technique, which allowed us to provide one example at a time to the algorithm, thus allowing training within our memory limit (6 biprocessor Xeon machines with 32 GB of RAM each). Parallelization was achieved by separately training the classifiers on every machine on chunks of data read from disk. A late fusion averaging approach is used in our best run (UNIMORE5 test) with the HSVSIFT, OPPONENTSIFT, RGBSIFT, and SIFT features. We further improved the training set by querying about 100k images from Google Images with the concept name. We managed to be the best group in terms of mAP-samples, the second in terms of MF-concepts and the third in terms of MF-samples.</p><p>In Section 2 we describe the feature sumarization approach, while in Section 3 the textual annotation processing method is presented. In Section 4 the Stochastic Gradient Descent is briefly sumarized and our modifications are highlighted. Finally Section 5 describes the submitted runs in detail and reports the performance obtained on both the development and the test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Visual Information Processing</head><p>For an image W , we first extract features through densely sampling in a regular grid. Let F = {f 1 . . . f N } be a set of local features (e.g. SIFT descriptors, where d = 128) in W (or a sub-region of W , when Spatial Pyramid Matching is used), we describe them with a multivariate Gaussian distribution supposing that they are normally distributed. The multivariate Gaussian distribution of a set of ddimensional vectors F is given by</p><formula xml:id="formula_0" coords="2,214.97,527.39,265.63,23.74">N (f ; m, C) = 1 |2πC| 1 2 e -1 2 (f -m) T C -1 (f -m) ,<label>(1)</label></formula><p>where | • | is the determinant, m is the mean vector and C is the covariance matrix (f , m ∈ R d and C ∈ S d×d ++ , with S d×d ++ the space of real symmetric positive semi-definite matrices) defined as follows:</p><formula xml:id="formula_1" coords="2,233.08,602.99,247.51,30.32">m = 1 N N i=1 f i ,<label>(2)</label></formula><formula xml:id="formula_2" coords="2,234.36,637.94,147.92,30.32">C = 1 N -1 N i=1 (f i -m)(f i -m) T .</formula><p>(3)  The covariance matrix encodes information about the variance of the features and their correlation. Although it is very informative, it does not lie in a vector space since the covariance space is not closed under multiplication with a negative scalar. In fact, it lies in a Riemannian manifold. Most of the common machine learning algorithms assume that the data points form a vector space, therefore a suitable transformation is required prior to their use. Since the covariance matrix is symmetric positive definite we adopt the Log-Euclidean metric. The basic idea of the Log-Euclidean metric is to construct an equivalent relationship between the Riemannian manifold and the vector space of the symmetric matrix.</p><p>An approach to map from Riemannian manifolds to Euclidean spaces is introduced in <ref type="bibr" coords="3,187.96,382.63,10.52,8.74" target="#b2">[3]</ref> and used in <ref type="bibr" coords="3,255.06,382.63,9.96,8.74" target="#b3">[4]</ref>. The first step is the projection of the covariance matrices on an Euclidean space tangent to the Riemannian manifold, on a specific tangency matrix P. The second step is the extraction of the orthonormal coordinates of the projected vector. In the following, matrices (points in the Riemannian manifold) will be denoted by bold uppercase letters, while vectors (points in the Euclidean space) by bold lowercase ones.</p><p>More formally, the projected vector of a covariance matrix C is given by:</p><formula xml:id="formula_3" coords="3,217.45,474.64,263.14,14.23">t C = log P (C) = P 1 2 log P -1 2 CP -1 2 P 1 2<label>(4)</label></formula><p>where log is the matrix logarithm operator and log P is the manifold specific logarithm operator, dependent on the point P to which the projection hyperplane is tangent. The matrix logarithm operators of a matrix C can be computed by eigenvalue decomposition (C = UDU T ); it is given by:</p><formula xml:id="formula_4" coords="3,206.02,552.55,274.57,30.55">log(C) = ∞ k=1 (-1) k-1 k (C -I) k = Ulog(D)U T .<label>(5)</label></formula><p>The orthonormal coordinates of the projected vector t C in the tangent space at point P are then given by the vector operator:</p><formula xml:id="formula_5" coords="3,239.53,621.11,236.82,13.28">vec P (t C ) = vec I P -1 2 t C P -1 2 (<label>6</label></formula><formula xml:id="formula_6" coords="3,476.35,624.74,4.24,8.74">)</formula><p>where I is the identity matrix, while the vector operator on the tanget space at identity of a symmetric matrix Y is defined as:</p><formula xml:id="formula_7" coords="4,189.97,122.30,290.62,18.38">vec I (Y) = y 1,1 √ 2y 1,2 √ 2y 1,3 . . . y 2,2 √ 2y 2,3 . . . y d,d .<label>(7)</label></formula><p>Substituting t C from Eq. 4 in Eq. 6, the projection of C on the hyperplane tangent to P becomes</p><formula xml:id="formula_8" coords="4,242.97,183.83,237.62,13.28">c = vec I log P -1 2 CP -1 2 .<label>(8)</label></formula><p>Thus, after selecting an appropriate projection origin, every covariance matrix is projected to an Euclidean space. Since c is a symmetric matrix of size d × d a (d 2 + d)/2-dimensional feature vector is obtained. As observed in <ref type="bibr" coords="4,218.30,243.80,9.96,8.74" target="#b4">[5]</ref>, the projection point P is arbitrary and, even if it could influence the performance (distortion) of the projection, from a computational point of view, the best choice is the identity matrix, which simply translates the mapping into a standard matrix logarithm.</p><p>In short, our method (see Fig. <ref type="figure" coords="4,280.69,291.64,4.43,8.74" target="#fig_0">1</ref>) is to extract local descriptors from an image and then collect them in a spatial pyramid; each sub-region is described by a multivariate Gaussian distribution (MGD). The covariance matrix is projected on a Euclidean space and concatenated to the mean vector to obtain the final descriptor (in the case of SIFT descriptors, the dimensionality becomes 8384 per sub-region). We empirically observe that most of the values in the concatenated descriptor are low, while few are high. In order to distribute the values more evenly, we adopt the power normalization method proposed by Perronnin et al. <ref type="bibr" coords="4,148.60,387.29,9.96,8.74" target="#b5">[6]</ref>, i.e. to apply to each dimension the function:</p><formula xml:id="formula_9" coords="4,234.31,409.23,246.29,10.81">f (x) = sign(x)|x| α with α = 0.5<label>(9)</label></formula><p>Eventually, the concatenated descriptors are fed to a linear classifier. For a more detailed analysis of the proposed multivariate Gaussian descriptor see <ref type="bibr" coords="4,442.23,441.26,9.96,8.74" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Textual Information Processing</head><p>The goal of textual information processing is, given a list L conc = {c 1 , . . . , c n } of concepts of interest, to retrieve a relevant set of images from the ImageCLEF training set exploiting only the textual content of the web pages that referenced the images. The concepts c are expressed as WordNet 3 synsets, for instance airplane.n.1 is the first sense of the term airplane as a noun; the list can include more than one synset, as they could be equally relevant, as in L conc = {book.n.1, book.n.2}. The retrieved image set I will then be used to train ad-hoc image classifiers in identifying the specific concepts in the test image set. In particular, in order for the training to be effective, the text processing techniques should be designed to retrieve a set of images:</p><p>(a) sufficiently large so to perform training (a minimum number of images threshold th min should be exceeded);</p><p>(b) as relevant as possible to the concepts.</p><p>One of the most naive approaches for text processing, which also constitutes a typical baseline, could be accomplished in very few simple steps. For instance, for a given c ∈ L conc (e.g. airplane.n.1):</p><p>1. extract the main term t associated to c (e.g. airplane); 2. look for t in the "scofeats" data file, containing, for each of the training images, the processed text of the referencing web pages, and retrieve in I the images referenced from the web pages where t appears.</p><p>Following the above baseline, however, brings to very unsatisfying results due to a large number of both (a) false negatives and (b) false positives in I, thus failing to meet the above mentioned desiderata. The following are just some real examples for airplane.n.1:</p><p>many relevant and useful images are missing since the original pages described the concepts using different terms (e.g. aeroplane, jumbojet, etc); many unrelated images are retrieved, for instance the closeup of a hat (from a web page about "airplane pilot hats"), the album covers and group shots of the "Jefferson Airplane" music band, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Enhanced Text Processing Approach</head><p>In order to overcome the above mentioned issues, we exploit an enhanced text processing approach, whose steps are outlined in Figure <ref type="figure" coords="5,379.42,403.38,3.87,8.74" target="#fig_1">2</ref>. First of all, a textual information pre-processing is executed on the "scofeats" data and on the original web pages data (top part of the figure):</p><p>stopword removal and stemming is performed on the "scofeats" file, thus producing a "stemmed scofeats" file; the titles of the original web pages are extracted and parsed (title extraction and analysis step), thus producing a "parsed page titles" file.</p><p>Note that, in our approach, we choose to exploit first of all the textual features already extracted in the "scofeats" file (including the term scores), complementing them with specific information extracted from the original web pages which would be otherwise unavailable. The output of pre-processing, then, enables the actual textual information search process (bottom part of the figure) that, given an input list of concepts L conc , produces the associated result image set I. With respect to the baseline described in the previous section, and for each concept c ∈ L conc the processing is enhanced in two main directions, corresponding to the original desiderata:</p><p>the number of retrieved images associated to c is significantly higher, thanks to the candidate image search step, which produces an expanded candidate image set CI. Each image in CI is associated to its scofeat information for further refinements; irrelevant images in CI are discarded and a refined set I is produced in a result filtering and refinement step.</p><p>The techniques behind these two crucial steps, along with the ones propaedeutic to them, will shortly be discussed in the following.</p><p>Candidate image search. The first key idea here is to search for term t (e.g. airplane) associated to c in the web page processed text of the "stemmed scofeats" file, instead of the plain "scofeats" file. In this way, stemming <ref type="bibr" coords="6,451.50,439.40,10.52,8.74" target="#b7">[8]</ref> significantly improves the recall in the image retrieval process, also retrieving pages containing different inflexions of the term (e.g., the plural airplanes).</p><p>Moreover, the search is performed not only for term t but also for the terms t available in its synonyms and hyponyms (i.e. more specific terms) lists, syn c and hyp c , respectively. Such lists are extracted from WordNet in the synonyms and hyponyms analysis step. For instance, for airplane syn c will include aeroplane and hyp c biplane, jumbojet, etc., whereas for rodent hyp c will contain the more common mouse and rat terms. In this way, a set of images significantly larger than the baseline is retrieved, for instance 40% more for airplane and even 300% more for rodent.</p><p>Please note that including in the search all the synonyms and hyponyms of a term, without any discrimination, while enlarging the retrieved image set, could also bring negative effects on its precision, due to ambiguity of language. For instance, an hyponym of newspaper is daily, an hyponym of horse is bay; these are however terms which are more typically used in completely different contexts and which would, thus, produce noise in the results. For this reason, we chose the "safest" approach of selecting only those synonyms/hyponyms having a single sense in WordNet; alternatively word sense disambiguation approaches (such as <ref type="bibr" coords="7,134.77,118.99,10.52,8.74" target="#b8">[9,</ref><ref type="bibr" coords="7,146.95,118.99,12.45,8.74" target="#b9">10]</ref>) could be applied to the web pages before the search (their application will be investigated in the future).</p><p>Result filtering and refinement. In this step, various refinement techiques are exploited so to reduce the number of false positives in the CI image set:</p><p>-score threshold: the "stemmed scofeats" file contains, for each term of the referencing web page, a score which captures the concepts of term frequency <ref type="bibr" coords="7,151.70,210.87,10.52,8.74" target="#b7">[8]</ref> and term distance to the image: the more frequent and the more close to the image is a term, the more it should be representative of the image content. Therefore, we define a th score threshold which the score of term t should exceed in order for the image to be put in the final results. For instance, the scofeat for an helicopter image from a web page about various means of transport could contain the term airplane but with a very low score, and would thus not be considered; -context threshold: a term in a web page can have very different senses. For instance, when looking for elder.n.1, i.e. "a person who is older than you are", one should be very careful to exclude web pages (and its images) which are about elders as "shrubs or small trees". While word sense disambiguation <ref type="bibr" coords="7,151.70,343.07,10.52,8.74" target="#b8">[9,</ref><ref type="bibr" coords="7,163.88,343.07,12.73,8.74" target="#b9">10]</ref> could be applied to the web pages, in order to be able to directly exploit the "stemmed scofeats" information we chose to implicitly derive the sense of a term from its co-occurent terms. In particular, for a given concept c, the context generation step first derives a preliminary context pcont c from the nouns of its WordNet definition, then expands it to a context cont c containing the terms which most frequently co-occur with the ones of pcont c . For instance, for c =elder.n.1, pcont c ={elder, person}, while cont c will include additional terms as old, family, people, house, woman and man.</p><p>Then, we define a th cont threshold which is the minimum number of terms of cont c that an image scofeat should have in order for the image to be put in the final results. In this way, when looking for images about elder.n.1, images about bushes are typically excluded; -negative context check: in order to enforce context filtering, we also derive a negative context ncont c from the definitions of the other senses of c that the image scofeat should not include: for instance when looking for c =castle.n.2 ("a large building"), ncont c will include misleading terms such as chess; -page title check: in this further check we exploit the "parsed page titles" information so to exclude from the results the images whose referencing pages have a title not meeting the desired criteria. In particular, term t should not be used as a deteriminer in a noun phrase, or be present in capitalized form: for instance, images from pages about "airplane pilot hats" or "Jefferson Airplane songs" are excluded as deemed not representative for airplane.</p><p>Please note that the above refinement techniques are applied only: (a) if they are relevant to the case (for instance, context and negative context are not needed in case of single-sense terms) and (b) if the number of images in the candidate set CI does not fall under the minimum threshold th min (in particular, thresholds th cont and th score are adjusted so to satisfy the th min =500 limit).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Online learning for SVM training</head><p>Although there exist many off-the-shelf SVM solvers, such as SVMlight, SVMperf or LibSVM/LIBLINEAR, they are not feasible for training large volumes of data. This is because most of them are batch methods, which require to go through all data to compute gradient in each iteration and often need many iterations to reach a reasonable solution. Even worse, most off-the-shelf batch type SVM solvers require to pre-load training data into memory, which is impossible when the size of the training data explodes. Indeed, LIBLINEAR released an extended version that explicitly considered the memory issue, but in a recent test <ref type="bibr" coords="8,454.87,269.77,15.50,8.74" target="#b10">[11]</ref> it was shown that the performance dropped considerably and even on 80GB of training data it could not provide useful results. Therefore, a better solution may be provided by the stochastic gradient descent (SGD) algorithm recently introduced for SVM classifiers training.</p><p>We have training data that consists of T feature-label pairs, denoted as {x t , y t } T t=1 , where x t is a s × 1 feature vector representing an image and y t ∈ {-1, +1} is the label of the image. Then, the cost function for binary SVM classification can be written as</p><formula xml:id="formula_10" coords="8,208.99,383.76,271.60,30.20">L = T t=1 λ 2 w 2 + max 0, 1 -y t (w T x t + b) ,<label>(10)</label></formula><p>where w is s × 1 SVM weight vector, λ (nonnegative scalar) is a regularization parameter, and b (scalar) is a bias term. In the SGD algorithm, training data are fed to the system one by one, and the update rule for w and b respectively are</p><formula xml:id="formula_11" coords="8,248.41,470.02,232.18,21.64">w t = (1 -λη)w t-1 + ηy t x t b t = b t-1 + ηy t<label>(11)</label></formula><p>if margin ∆ t = y t (w T x t + b) is less than 1; otherwise, w t = (1 -λη)w t-1 and b t = b t-1 . The parameter η is the step size. We set η = (1 + λt) -1 , following the vl pegasos implementation <ref type="bibr" coords="8,262.01,524.61,14.61,8.74" target="#b11">[12]</ref>. To parallelize SVMs training, we randomize the data on disk. We load the data in chunks which fit in memory, then train the different classifiers in parallel threads on further randomizations of the chunks, so that different epochs will get the chunks data with different orderings. In our experimental setting each classifier is trained considering all the 250,000 images given as training; as a result the data are highly unbalanced, namely the negative samples are predominant, and in addition the number of samples per concept is unevenly distributed. We observed that this leads the classifier to incorrectly estimate the hyperplane, that is shifted towards the positive samples while maintaining a proper orientation. To reduce this effect, a simultaneous optimization of the SVMs bias for all the classifiers is conducted by maximizing the F-measure on all the training set. UNIMORE participated to the Scalable Concept Image Annotation task submitting six runs. All the different kind of SIFT descriptors are extracted at four scales, defined by setting the width of the local feature spatial bins to 4, 6, 8, and 10 pixels respectively, over a dense regular grid with a spacing of 3 pixels. We use the function vl phow provided by the vl feat library <ref type="bibr" coords="9,414.74,191.13,15.50,8.74" target="#b11">[12]</ref> and, apart from the spacing step, the defaults options are used. Images are hierarchically partitioned into 1×1, 2×2 and 1×3 blocks on 3 levels respectively. In the case of SIFT descriptor we obtain a 67,072 dimensional vector concatenating the MGD features of the 8 spatial windows, while for color SIFT descriptors (RGB, OP-PONENT and HSV) we describe a region by concatenating the MGD computed for each color channel separately (instead of using the full covariance matrix of 384 dimensions) obtaining a 201,216 dimensional feature vector. Stochastic gradient descent is used to learn a classifier for each concept, for each feature descriptor and each training set; detection scores are thresholded at zero to obtain the decisions. Finally, a late fusion averaging approach is used.</p><p>In some runs we added another training set of about 100k images queried from Google Image Search directly using the concepts list. Each image is automatically labeled with the concept word used in the query; synonyms and hyponyms analysis is also performed in order to identify labels relationship (e.g. images labeled as "car" are also tagged with the "vehicle" label). These images are described with RGBSIFT and summarized with a Multivariate Gaussian Descriptor. All experiments are performed on six biprocessor Xeon machines with 32 GB of RAM each. Runs are described and discussed in the following:</p><p>- Tab. 1 and 2 present the results obtained for each run in term of mAP (mean average precision) and MF (mean F-measure). The MF is computed analyzing both the samples (MF-samples) and the concepts (MF-concepts), whereas the mAP is computed analyzing the samples. It can be noted that the performance reported in all the three metrics in the development and test sets are strictly related, and shows slightly lower results in the latter. This is probably due to the higher number and variability of the concepts given in the test setting. The late fusion averaging approach proves to be a good solution for combining different features and training sets and for easily learning classifiers in parallel. In particular it greatly improves the mAP value, that increases for each new feature or training set added in the system (for example see runs UNIMORE 1 and UNIMORE 2). Adding our Google Images dataset, automatically downloaded using concepts list, increases the performance in term of mAP although the high level of label noise (see runs UNIMORE 3 and UNIMORE 4). Textual information processing is also essential for the proposed method, mainly because it increases the performance in term of MF and defines new training sets to be used in the late fusion approach. It can be noted that both textual processing steps contribute to the improvement of the performance: see for example the gap in terms of MF between runs UNIMORE 4 and UNIMORE 1 mainly caused by candidate image search strategy and the difference of mAP values between </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper we presented the approach developed to participate at ImageCLEF 2013 for the Scalable Concept Image Annotation task. Our proposal focus on the definition of a new image descriptor that encodes local features, densely extracted from a region, as a Multivariate Gaussian Distribution. A new textual information processing strategy is also presented to cope with the high level of noise of the training data. To deal with the large-scale nature of this task, we use an online linear SVM classifier based on the Stochastic Gradient Descent algorithm. Experimental results show that both visual and textual information processing are necessary to build a competitive system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,206.46,218.66,202.44,7.89"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of visual information processing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,134.77,301.21,345.83,7.89;6,134.77,312.20,22.02,7.86"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of textual information pre-processing (top) and search process (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,140.99,428.70,339.60,236.16"><head></head><label></label><figDesc>Three sets of training images, based of the scofeats file, are associated to a concept : 1) an image is linked to a concept when the concept word is present in the stemmed scofeats file; 2) images obtained by the candidate image search technique; 3) images obtained applying the complete textual information processing pipeline. For every image multiscale HSVSIFT, OPPONENTSIFT, RGBSIFT and SIFT features are extracted and summarized with a MGD. In addition, Google Images dataset is used for training. Two different combination strategies are used to compute decisions and score values: decisions are computed through a late fusion averaging approach of classifiers trained with images derived by the candidate image search technique and described with HSVSIFT, OPPONENTSIFT and RG-BSIFT descriptors, while the score values are obtained combining all the classifiers learned in this run. -UNIMORE 6 Two sets of training images, based on the scofeats file, are associated to a concept: 1) an image is linked to a concept when the concept word is present in the stemmed scofeat file; 2) images obtained by the candidate image search technique. For every image multiscale HSVSIFT, OPPO-NENTSIFT, RGBSIFT and SIFT features are extracted and summarized with a MGD. Google Images dataset is also used for training. Two different combination strategies are used to compute decisions and score values: decisions are computed through a late fusion averaging approach of classifier trained with images derived by the candidate image search technique and described with HSVSIFT and RGBSIFT features, while the score values are obtained combining all the classifiers learned in this run.</figDesc><table coords="9,140.99,428.70,339.60,236.16"><row><cell>-UNIMORE 5</cell></row><row><cell>UNIMORE 1: Training images are associated to a concept using the first</cell></row><row><cell>step of the textual information processing, called "candidate image search",</cell></row><row><cell>on the scofeats file. For every image multiscale HSVSIFT and RGBSIFT</cell></row><row><cell>features are extracted and summarized with a Multivariate Gaussian De-</cell></row><row><cell>scriptor.</cell></row><row><cell>-UNIMORE 2: Based on the scofeats file, two different sets of training im-</cell></row><row><cell>ages are associated to a concept: 1) images linked to a concept when the</cell></row><row><cell>concept word is present in the stemmed scofeats; 2) images obtained by the</cell></row><row><cell>candidate image search technique. For every image, multiscale HSVSIFT,</cell></row><row><cell>OPPONENTSIFT, RGBSIFT and SIFT features are extracted and summa-</cell></row><row><cell>rized with a MGD. Google Images dataset is also used for training.</cell></row><row><cell>-UNIMORE 3 Training images are associated to a concept only if the con-</cell></row><row><cell>cept word is present in the stemmed scofeats file. For every image multiscale</cell></row><row><cell>HSVSIFT, OPPONENTSIFT, RGBSIFT and SIFT features are extracted</cell></row><row><cell>and summarized with a MGD. Google Images dataset is also used for train-</cell></row><row><cell>ing.</cell></row><row><cell>-UNIMORE 4 Training images are associated to a concept if the concept</cell></row><row><cell>word is present in the stemmed scofeats file. For every image multiscale</cell></row><row><cell>HSVSIFT, OPPONENTSIFT, RGBSIFT features are extracted and sum-</cell></row><row><cell>marized with a MGD.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="11,185.76,115.91,243.83,123.51"><head>Table 1 .</head><label>1</label><figDesc>Development set results</figDesc><table coords="11,185.76,136.68,243.83,102.73"><row><cell></cell><cell cols="3">MF-samples MF-concepts mAP-Samples</cell></row><row><cell>baseline rand</cell><cell>6.2</cell><cell>4.8</cell><cell>10.9</cell></row><row><cell>baseline sift</cell><cell>17.8</cell><cell>11.0</cell><cell>24.0</cell></row><row><cell>UNIMORE 1</cell><cell>33.0</cell><cell>34.1</cell><cell>39.2</cell></row><row><cell>UNIMORE 2</cell><cell>27.3</cell><cell>34.2</cell><cell>46.0</cell></row><row><cell>UNIMORE 3</cell><cell>23.1</cell><cell>32.4</cell><cell>43.7</cell></row><row><cell>UNIMORE 4</cell><cell>26.8</cell><cell>31.7</cell><cell>39.7</cell></row><row><cell>UNIMORE 5</cell><cell>33.3</cell><cell>33.7</cell><cell>47.9</cell></row><row><cell>UNIMORE 6</cell><cell>33.0</cell><cell>34.1</cell><cell>46.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="11,134.77,254.49,345.82,171.33"><head>Table 2 .</head><label>2</label><figDesc>Test set results</figDesc><table coords="11,134.77,273.51,345.82,152.30"><row><cell></cell><cell cols="3">MF-samples MF-concepts mAP-Samples</cell></row><row><cell>baseline rand</cell><cell>4.6</cell><cell>3.6</cell><cell>8.7</cell></row><row><cell>baseline sift</cell><cell>15.9</cell><cell>11.0</cell><cell>21.0</cell></row><row><cell>UNIMORE 1</cell><cell>31.1</cell><cell>32.0</cell><cell>36.7</cell></row><row><cell>UNIMORE 2</cell><cell>27.5</cell><cell>33.1</cell><cell>44.1</cell></row><row><cell>UNIMORE 3</cell><cell>23.1</cell><cell>31.5</cell><cell>41.9</cell></row><row><cell>UNIMORE 4</cell><cell>24.1</cell><cell>29.5</cell><cell>36.2</cell></row><row><cell>UNIMORE 5</cell><cell>31.5</cell><cell>31.9</cell><cell>45.6</cell></row><row><cell>UNIMORE 6</cell><cell>31.1</cell><cell>32.0</cell><cell>44.1</cell></row><row><cell cols="4">runs UNIMORE 6 and UNIMORE 5 obtained applying the complete textual</cell></row><row><cell cols="2">information processing pipeline.</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,142.96,634.88,337.64,7.86;11,151.52,645.84,329.07,7.86;11,151.52,656.80,329.07,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,466.52,645.84,14.08,7.86;11,151.52,656.80,229.70,7.86">Imageclef 2013: the vision, the data and the open challenges</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zellhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Martinez Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Garcia Varea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cazorla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,402.98,656.80,43.61,7.86">Proc CLEF</title>
		<meeting>CLEF</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,119.67,337.64,7.86;12,151.52,130.63,329.07,7.86;12,151.52,141.59,25.60,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,316.75,119.67,163.85,7.86;12,151.52,130.63,134.71,7.86">Overview of the imageclef 2013 scalable concept image annotation subtask</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,307.43,130.63,102.50,7.86">CLEF 2013 working notes</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,152.55,337.63,7.86;12,151.52,163.48,262.81,7.89" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,277.77,152.55,202.82,7.86;12,151.52,163.51,58.30,7.86">Pedestrian Detection via Classification on Riemannian Manifolds</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,218.12,163.51,92.94,7.86">IEEE T. Pattern Anal</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1713" to="1727" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,174.47,337.63,7.86;12,151.52,185.43,329.07,7.86;12,151.52,196.39,25.60,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,332.83,174.47,147.76,7.86;12,151.52,185.43,234.23,7.86">Miniature illustrations retrieval and innovative interaction for digital illuminated manuscripts</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Borghesani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Grana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,397.45,185.43,83.14,7.86">Multimedia Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,207.34,337.63,7.86;12,151.52,218.30,329.07,7.86;12,151.52,229.26,25.60,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,434.68,207.34,45.91,7.86;12,151.52,218.30,233.11,7.86">An FPGAbased Classification Architecture on Riemannian Manifolds</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Martelli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tosato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Farenzena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,404.78,218.30,71.17,7.86">DEXA Workshops</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,240.22,337.63,7.86;12,151.52,251.18,155.86,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,314.74,240.22,165.85,7.86;12,151.52,251.18,76.41,7.86">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,249.66,251.18,29.04,7.86">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,262.14,337.64,7.86;12,151.52,273.10,194.93,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,360.07,262.14,120.53,7.86;12,151.52,273.10,115.97,7.86">Image classification with multivariate gaussian descriptors</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Grana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Manfredi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,289.50,273.10,28.28,7.86">ICIAP</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,284.06,337.63,7.86;12,151.52,295.02,259.84,7.86" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="12,310.79,284.06,122.26,7.86">Modern Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Addison-Wesley Longman Publishing Co., Inc</publisher>
			<pubPlace>Boston, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,305.98,337.64,7.86;12,151.52,316.91,310.49,7.89" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,277.07,305.98,203.53,7.86;12,151.52,316.93,71.74,7.86">Knowledge-Based Sense Disambiguation (Almost) For All Structures</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Mandreoli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Martoglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,231.36,316.93,141.19,7.86">Information Systems (Information)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="406" to="430" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,327.89,337.98,7.86;12,151.52,338.85,329.07,7.86;12,151.52,349.81,166.66,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,334.78,327.89,145.81,7.86;12,151.52,338.85,129.44,7.86">Versatile Structural Disambiguation for Semantic-aware Applications</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Mandreoli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Martoglia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Ronchetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,304.31,338.85,176.28,7.86;12,151.52,349.81,133.05,7.86">ACM International Conference on Information Knowledge and Management</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,360.77,337.97,7.86;12,151.52,371.73,242.45,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,354.77,360.77,125.82,7.86;12,151.52,371.73,162.62,7.86">Large-scale image classification: Fast feature extraction and svm training</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Cour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,336.12,371.73,29.18,7.86">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,382.69,337.98,7.86;12,151.52,393.65,198.27,7.86" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="12,267.96,382.69,212.63,7.86;12,151.52,393.65,67.44,7.86">VLFeat: An open and portable library of computer vision algorithms</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Fulkerson</surname></persName>
		</author>
		<ptr target="http://www.vlfeat.org/" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
