<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,156.22,115.96,302.91,12.62;1,181.00,133.89,253.36,12.62">Overview of the ImageCLEF 2013 Scalable Concept Image Annotation Subtask</title>
				<funder ref="#_rwbRBer">
					<orgName type="full">Spanish MEC</orgName>
				</funder>
				<funder ref="#_fANzUUZ #_XP3NT2h">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_a74dE2m">
					<orgName type="full">CLEF campaign</orgName>
				</funder>
				<funder ref="#_awK5Dja">
					<orgName type="full">European Union</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,183.93,171.56,74.58,8.74"><forename type="first">Mauricio</forename><surname>Villegas</surname></persName>
							<email>mvillegas@iti.upv.es</email>
							<affiliation key="aff0">
								<orgName type="department">ITI/DSIC</orgName>
								<orgName type="institution">Universitat Politècnica de València</orgName>
								<address>
									<addrLine>Camí de Vera s/n</addrLine>
									<postCode>46022</postCode>
									<settlement>València</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,270.08,171.56,72.34,8.74;1,342.42,169.98,1.83,6.12"><forename type="first">Roberto</forename><surname>Paredes</surname></persName>
							<email>rparedes@iti.upv.es</email>
							<affiliation key="aff0">
								<orgName type="department">ITI/DSIC</orgName>
								<orgName type="institution">Universitat Politècnica de València</orgName>
								<address>
									<addrLine>Camí de Vera s/n</addrLine>
									<postCode>46022</postCode>
									<settlement>València</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,369.26,171.56,58.01,8.74;1,427.27,169.98,1.83,6.12"><forename type="first">Bart</forename><surname>Thomee</surname></persName>
							<email>bthomee@yahoo-inc.com</email>
							<affiliation key="aff1">
								<address>
									<addrLine>Yahoo! Research Avinguda Diagonal 177</addrLine>
									<postCode>08018</postCode>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,156.22,115.96,302.91,12.62;1,181.00,133.89,253.36,12.62">Overview of the ImageCLEF 2013 Scalable Concept Image Annotation Subtask</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">68F75C4571429F2AE32C4FA2D156E02D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The ImageCLEF 2013 Scalable Concept Image Annotation Subtask was the second edition of a challenge aimed at developing more scalable image annotation systems. Unlike traditional image annotation challenges, which rely on a set of manually annotated images as training data for each concept, the participants were only allowed to use automatically gathered web data instead. The main objective of the challenge was to focus not only on the image annotation algorithms developed by the participants, where given an input image and a set of concepts they were asked to decide which of them were present in the image and which ones were not, but also on the scalability of their systems, such that the concepts to detect were not exactly the same between the development and test sets. The participants were provided with web data consisting of 250,000 images, which included textual features obtained from the web pages on which the images appeared, as well as various visual features extracted from the images themselves. To evaluate the performance of the submitted systems a development set was provided containing 1,000 images that were manually annotated for 95 concepts and a test set containing 2,000 images that were annotated for 116 concepts. In total 13 teams participated, submitting a total of 58 runs, most of which significantly outperformed the baseline system for both the development and test sets, including for the test concepts not present in the development set and thus clearly demonstrating potential for scalability.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="792.0" lry="612.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="792.0" lry="612.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic concept detection within images is a challenging and as of yet unsolved research problem. Over the past decades impressive improvements have been achieved, albeit admittedly not yet successfully solving the problem. Yet, these improvements have been typically obtained on datasets for which all images have been manually, and thus reliably, labeled. For instance, it has become common in past image annotation benchmark campaigns <ref type="bibr" coords="1,389.50,626.87,16.13,8.74" target="#b9">[10,</ref><ref type="bibr" coords="1,405.63,626.87,12.10,8.74" target="#b15">16]</ref> to use crowdsourcing approaches, such as the Amazon Mechanical Turk<ref type="foot" coords="1,388.64,637.25,3.97,6.12" target="#foot_0">1</ref> , in order to let mul-  tiple annotators label a large collection of images. Nonetheless, crowdsourcing is expensive and difficult to scale to a very large amount of concepts. The image annotation datasets furthermore usually include exactly the same concepts in the training and test sets, which may mean that the evaluated visual concept detection algorithms are not necessarily able to cope with detecting additional concepts beyond what they were trained on. To address these shortcomings a novel image annotation task <ref type="bibr" coords="2,259.35,415.84,15.50,8.74" target="#b19">[20]</ref> was proposed last year for which automatically gathered web data was to be used for concept detection, where the concepts varied between the evaluation sets. The aim of that task was to reduce the reliance of cleanly annotated data for concept detection and rather focus on uncovering structure from noisy data, emphasizing the importance of the need for scalable annotation algorithms able to determine for any given concept whether or not it is present in an image. The rationale behind the scalable image annotation task was that there are billions of images available online appearing on webpages, where the text surrounding the image may be directly or indirectly related to its content, thus providing clues as to what is actually depicted in the image. Moreover, images and the webpages on which they appear can be easily obtained for virtually any topic using a web crawler. In existing work such noisy data has indeed proven useful, e.g. <ref type="bibr" coords="2,248.32,559.30,15.90,8.74" target="#b16">[17,</ref><ref type="bibr" coords="2,264.21,559.30,11.92,8.74" target="#b21">22,</ref><ref type="bibr" coords="2,276.13,559.30,11.92,8.74" target="#b20">21]</ref>.</p><p>The second edition of the scalable image annotation task is what is presented in this overview paper, which is one of several ImageCLEF benchmark campaigns <ref type="bibr" coords="2,184.04,596.34,9.96,8.74" target="#b2">[3]</ref>. The paper is organized as follows. In Section 2 we describe the task in more detail, which includes introducing the dataset that was created specifically for this challenge, the baseline system and the evaluation measures. In Section 3 we then present and discuss the results submitted by the participants. Finally, we conclude the paper with final remarks and future outlooks in Section 4.</p><p>Overview of the ImageCLEF 2013 Annotation Subtask 3 2 Overview of the Subtask</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Motivation and Objectives</head><p>Image concept detection generally has relied on training data that has been manually, and thus reliably, annotated, which is an expensive and laborious endeavor that cannot easily scale. To address this issue, the ImageCLEF 2013 scalable annotation subtask concentrated exclusively on developing annotation systems that rely only on automatically obtained data. A very large amount of images can be easily gathered from the web, and furthermore, from the webpages that contain the images, text associated with them can be obtained. However, the degree of relationship between the surrounding text and the image varies greatly. Moreover, the webpages can be of any language or even a mixture of languages, and they tend to have many writing mistakes. Overall the data can be considered to be very noisy.</p><p>To illustrate the objective of the evaluation, consider for example that someone searches for the word "rainbow" in a popular image search engine. It would be expected that many results be of landscapes in which in the sky a rainbow is visible. However, other types of images will also appear, see Figure <ref type="figure" coords="3,446.80,327.76,8.49,8.74" target="#fig_1">1a</ref>. The images will be related to the query in different senses, and there might even be images that do not have any apparent relationship. In the example of Figure <ref type="figure" coords="3,467.86,351.67,8.49,8.74" target="#fig_1">1a</ref>, one image is a text page of a poem about a rainbow, and another is a photograph of an old cave painting of a rainbow serpent. See Figure <ref type="figure" coords="3,413.05,375.58,10.52,8.74" target="#fig_1">1b</ref> for a similar example on the query "sun". As can be observed, the data is noisy, although it does have the advantage that this data can also handle the possible different senses that a word can have, or the different types of images that exist, such as natural photographs, paintings and computer-generated imagery.</p><p>In order to handle the web data, there are several resources that could be employed in the development of scalable annotation systems. Many resources can be used to help match general text to given concepts, amongst which some examples are stemmers, word disambiguators, definition dictionaries, ontologies and encyclopedia articles. There are also tools that can help to deal with noisy text commonly found on webpages, such as language models, stop word lists and spell checkers. And last but not least, language detectors and statistical machine translation systems are able to process webpage data written in various languages.</p><p>In summary, the goal of the scalable image annotation subtask was to evaluate different strategies to deal with noisy data, so that the unsupervised web data can be reliably used for annotating images for practically any topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Challenge Description</head><p>The subtask 2 consisted of the development of an image annotation system given training data that only included images crawled from the Internet, the corresponding webpages on which they appeared, as well as precomputed visual and textual features. As mentioned in the previous section, the aim of the subtask was for the annotation systems to be able to easily change or scale the list of concepts used for image annotation. Apart from the image and webpage data, the participants were also permitted and encouraged to use any other automatically obtainable resources to help in the processing and usage of the training data. However, the most important rule was that the systems were not permitted to use any kind of data that had been explicitly and manually labeled for the concepts to detect.</p><p>For the development of the annotation systems, the participants were provided with the following:</p><p>-A training dataset of images and corresponding webpages compiled specifically for the subtask, including precomputed visual and textual features (see Section 2.3). -Source code of a simple baseline annotation system (see Section 2.4).</p><p>-Tools for computing the appropriate performance measures (see Section 2.5).</p><p>-A development set of images with ground truth annotations (including precomputed visual features) for estimating the system performance.</p><p>After a period of two months, a test set of images was released that did not include any ground truth labels. The participants had to use their developed systems to predict the concepts for each of the input images and submit these results to the subtask organizers. A maximum of 6 submissions (also referred to as runs) were allowed per participating group. Since one of the objectives was that the annotation systems be able to scale or change the list of concepts for annotation, the list of concepts for the test set was not exactly the same as those for the development set. The development set consisted of 1,000 images labeled for 95 concepts, and the test set consisted of 2,000 images labeled for 116 concepts (the same 95 concepts for development and 21 more).</p><p>To observe the possible overfitting of the development set and the difference of performance with respect to the test set, the participants were also required to submit the concept predictions of the development set, using exactly the same system and parameters as for the test set.</p><p>The concepts to be used for annotation were defined as one or more WordNet synsets <ref type="bibr" coords="4,170.28,509.86,9.96,8.74" target="#b3">[4]</ref>. So, for each concept there was a concept name, the type (either noun or adjective), the synset offset(s), and the sense number(s). Defining the concepts this way, made it straightforward to obtain the concept definition, synonyms, hyponyms, etc. Additionally, for most of the concepts, a link to a Wikipedia article about the respective concept was provided. The complete list of concepts, as well as the number of images in both the development and test sets, is included in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Dataset</head><p>The dataset<ref type="foot" coords="4,188.29,624.73,3.97,6.12" target="#foot_2">3</ref> used was mostly the same as the one in ImageCLEF 2012 for the first edition of this task <ref type="bibr" coords="4,261.09,638.26,14.60,8.74" target="#b19">[20]</ref>. To create the dataset, initially a database of over 31 million images was created by querying Google, Bing and Yahoo! using words from the Aspell English dictionary <ref type="bibr" coords="5,322.45,130.95,14.61,8.74" target="#b18">[19]</ref>. The images and corresponding webpages were downloaded, taking care to avoid data duplication. Then, a subset of 250,000 images (to be used as the training set) was selected from this database by choosing the top images from a ranked list. The motivation for selecting a subset was to provide smaller data files that would not be so prohibitive for the participants to download/handle, and because a limited amount of concepts had to be chosen for evaluation. The ranked list was generated by retrieving images from our database using a manually defined list of concepts, in essence more or less as if the search engines had only been queried for these concepts. From this ranked list, some types of problematic images were removed, and it was guaranteed that each image had at least one webpage in which they appeared. Unlike the training set, the development (1,000 images) and test (2,000 images) sets were manually selected and labeled for the concepts being evaluated. For further details on how the dataset was created, please refer to <ref type="bibr" coords="5,407.99,286.37,14.61,8.74" target="#b19">[20]</ref>.</p><p>The 250,000 training set images were exactly the same as the ones for Im-ageCLEF 2012. However, some images from the development and test sets had been changed. To guaranty that the visual features were the same for the new images, due to changes in software versions, the features were recalculated and therefore are different from those supplied in the previous edition of this subtask. Also this year the original images and webpages were provided. The most significant change of the dataset with respect to 2012 was the labeling of the development and test sets, where the images have now been labeled and linked to concepts in WordNet <ref type="bibr" coords="5,241.29,393.96,9.96,8.74" target="#b3">[4]</ref>, thus making it much easier to automatically obtain more information for each concept. Moreover, for most of the concepts a corresponding Wikipedia article was additionally supplied, which may prove to be a useful resource.</p><p>Textual Data: Since the textual data was to be used only during training, it was only provided for the training set. Four sets of data were made available to the participants. The first one<ref type="foot" coords="5,281.74,481.05,3.97,6.12" target="#foot_3">4</ref> was the list of words used to find the image when querying the search engines, along with the rank position of the image in the respective query and search engine it was found on. The second set of textual data 4 contained the image URLs as referenced in the webpages they appeared in. In many cases the image URLs tend to be formed with words that relate to the content of the image, which is why they can also be useful as textual features. The third set of data were the webpages in which the images appeared, for which the only preprocessing was a conversion to valid XML just to make any subsequent processing simpler. The final set of data 4 were features obtained from the text extracted near the position(s) of the image in each webpage it appeared in.</p><p>To extract the text near the image, after conversion to valid XML, the script and style elements were removed. The extracted text were the webpage title and all the terms closer than 600 in word distance to the image, not including the HTML tags and attributes. Then a weight s(t n ) was assigned to each of the words near the image, defined as</p><formula xml:id="formula_0" coords="6,210.23,150.69,270.36,26.88">s(t n ) = 1 ∀t∈T s(t) ∀tn,m∈T F n,m sigm(d n,m ) ,<label>(1)</label></formula><p>where t n,m are each of the appearances of the term t n in the document T , F n,m is a factor depending on the DOM (e.g. title, alt, etc.) similar to what is done in the work of La Cascia et al. <ref type="bibr" coords="6,273.32,214.58,9.96,8.74" target="#b6">[7]</ref>, and d n,m is the word distance from t n,m to the image. The sigmoid function was centered at 35, had a slope of 0.15 and minimum and maximum values of 1 and 10 respectively. The resulting features include for each image at most the 100 word-score pairs with the highest scores.</p><p>Visual Features: Seven types of visual features were made available to the participants. Before feature extraction, images were filtered and resized so that the width and height had at most 240 pixels while preserving the original aspect ratio. The first feature set Colorhist consisted of 576-dimensional color histograms extracted using our own implementation. These features correspond to dividing the image in 3 × 3 regions and for each region obtaining a color histogram quantified to 6 bits. The second feature set GETLF contained 256-dimensional histogram based features. First, local color-histograms were extracted in a dense grid every 21 pixels for windows of size 41 × 41. Second, these local colorhistograms were randomly projected to a binary space using 8 random vectors and considering the sign of the resulting projection to produce the bit. Thus, obtaining a 8-bit representation of each local color-histogram that can be considered as a word. Finally, the image is represented as a bag-of-words, leading to a 256-dimensional histogram representation. The third set of features consisted of GIST <ref type="bibr" coords="6,175.44,448.77,15.50,8.74" target="#b10">[11]</ref> descriptors. The other four feature types were obtained using the colorDescriptors software <ref type="bibr" coords="6,245.96,460.72,14.61,8.74" target="#b14">[15]</ref>. Features were computed for SIFT, C-SIFT, RGB-SIFT and OPPONENT-SIFT. The configuration was dense sampling with default parameters and a hard assignment 1,000 codebook using a spatial pyramid of 1 × 1 and 2 × 2 <ref type="bibr" coords="6,209.44,496.59,9.96,8.74" target="#b7">[8]</ref>. Since the vectors of the spatial pyramid were concatenated, this resulted in 5,000-dimensional feature vectors. Keeping only the first fifth of the dimensions would be like not using the spatial pyramid. The codebooks were generated using 1.25 million randomly selected features and the k-means algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Baseline Systems</head><p>A toolkit was supplied to the participants as a performance reference for the evaluation, as well as to serve as a starting point. This toolkit included software that computed the evaluation measures (see Section 2.5) and the implementations of two baselines. The first baseline was a simple random, which is important since any system that gets worse performance than random is useless. The other baseline, referred to as Co-occurrence Baseline, was a basic technique that gives better performance than random, although it was simple enough to give the participants a wide margin for improvement. In the latter technique, when given an input image, obtains its nearest K = 32 images from the training set using only the 1,000 bag-of-words C-SIFT visual features and the L1 norm. Then, the textual features corresponding to these K nearest images are used to derive a score for each of the concepts. This is done by using a concept-word co-occurrence matrix estimated from all of the training set textual features. In order to make the vocabulary size more manageable, the textual features are first processed keeping only English words. Finally, the annotations assigned to the image are always the top 6 ranked concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Performance Measures</head><p>Ultimately the goal of an image annotation system is to make decisions about which concepts to assign to given image from a predefined list of concepts. Thus to measure annotation performance what should be considered is how good are those decisions. On the other hand, in practice many annotations systems are based on estimating a score for each of the concepts and then a second technique uses these scores to finally decide which concepts are chosen. For systems of this type a measure of performance can be based only on the concept scores, which considers all aspects of the system except for the technique used for concept decisions, making it an interesting characteristic to measure. For this task, two basic performance measures have been used for comparing the results of the different submissions. The first one is the F-measure (F 1 ), which takes into account the final annotation decisions, and the other is the Average Precision (AP), which considers the concept scores.</p><p>The F-measure is defined as</p><formula xml:id="formula_1" coords="7,277.51,451.83,198.84,22.31">F 1 = 2P R P + R , (<label>2</label></formula><formula xml:id="formula_2" coords="7,476.35,458.57,4.24,8.74">)</formula><p>where P is the precision and R is the recall. In the context of image annotation, the F 1 can be estimated from two different perspectives, one being concept-based and the other sample-based. In the former, one F 1 is computed for each concept, and in the latter one F 1 is computed for each image to annotate. In both cases, the arithmetic mean is used as a global measure of performance, and will be referenced as MF 1 -concepts and MF 1 -samples, respectively. The AP is algebraically defined as</p><formula xml:id="formula_3" coords="7,256.47,577.81,219.88,31.41">AP = 1 |K| |K| k=1 k rank(k) , (<label>3</label></formula><formula xml:id="formula_4" coords="7,476.35,589.08,4.24,8.74">)</formula><p>where K is the ordered set of the ground truth annotations, being the order induced by the annotation scores, and rank(k) is the order position of the k-th ground truth annotation. The fraction k/ rank(k) is actually the precision at the k-th ground truth annotation, and has been written like this to be explicit on the way it is computed. In the cases that there are ties in the scores, a random permutation is applied within the ties. The AP can also be estimated for both the concept-based and sample-based perspectives, however, the concept-based AP is not a suitable measure of annotation performance (it is more adequate for a retrieval scenario), so only the sample-based AP has been considered in this evaluation. As a global measure of performance, also the arithmetic mean is used, which will be referred to as MAP-samples.</p><p>A bit of care must be taken when comparing systems using the MAP-samples measure. What the MAP-samples turns out saying is that if for a given image the scores are used to sort the concepts, how good would it rank the true concepts for the image. Depending on the system, its scores could or could not be optimal for ranking the concepts. Thus a system with a relatively low MAP-samples, could still have a good annotation performance if the method used to select the concepts is adequate for its concept scores. Because of this, as well as the fact that there can be systems that do not rely on scores, it was optional for the participants of the task to provide scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Participation</head><p>The participation was excellent, especially considering that this was the second edition of the task and last year there was only one participant. In total 13 groups took part, submitting 58 runs overall. The following teams participated:</p><p>-  a Unlike the other systems that take as input image visual features, the UNED&amp;UV system receives as input the image webpage. In Table <ref type="table" coords="10,175.03,310.33,4.98,8.74" target="#tab_0">1</ref> we provide a comparison of a the key details of the best submission of each group. For a more in depth look of the annotation systems of each team, please refer to their corresponding paper listed in the table. Note that there were four groups that did not submit a working notes paper describing their system, those submissions less information could be listed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>Table <ref type="table" coords="10,162.52,425.47,4.98,8.74" target="#tab_1">2</ref> presents the performance measures (mentioned in 2.5) for the baseline techniques and all of the submitted runs by the participants.The last column of the table corresponds to the MF 1 -concepts measure which was only computed for the 21 concepts that did not appear in the development set. The systems are ordered by performance, beginning at the top with the best performing one. This order of the systems has been derived by considering for the test set the average rank when comparing all of the systems, using the MF 1 -samples, the MF 1 -concepts and the MF 1 -concepts unseen in dev. measures, while breaking ties by the average of the same three performance measures.</p><p>For an easier comparison and a more intuitive visualization, the same results of Table <ref type="table" coords="10,175.70,546.77,4.98,8.74" target="#tab_1">2</ref> are presented as graphs in Figure <ref type="figure" coords="10,340.42,546.77,4.98,8.74">2</ref> (only for the test set). These graphs include for each result the 95% confidence intervals. These intervals have been estimated by Wilson's method, employing the standard deviation for the individual measures (for the samples or concepts, and for the average precisions (AP) or F-measures (F 1 ), depending on the case).</p><p>Finally, in Figure <ref type="figure" coords="10,229.47,608.30,4.98,8.74">3</ref> there is for each of the 116 test set concepts, a boxplot (or also known as box-and-whisker plot) for the F 1 -measures when combining all runs. In order to fit all of the concepts in the same graph, for multiple outliers with the same value, only one is shown. The concepts have been sorted by the median performance of all submissions, which in a way orders them by difficulty.  </p><formula xml:id="formula_5" coords="12,174.09,562.08,304.06,53.52">TPT #6 TPT #4 TPT #2 TPT #5 TPT #3 TPT #1 MIL #4 MIL #1 MIL #2 MIL #5 MIL #3 UNIMORE #2 UNIMORE #5 UNIMORE #1 UNIMORE #6 UNIMORE #3 UNIMORE #4 RUC #4 RUC #5 RUC #3 RUC #2 RUC #1 UNED&amp;UV #3 UNED&amp;UV #5 UNED&amp;UV #4 UNED&amp;UV #1 UNED&amp;UV #2 CEA LIST #4 CEA LIST #5 CEA LIST #3 CEA LIST #2 CEA LIST #1 KDEVIR #1 KDEVIR #3 KDEVIR #6 KDEVIR #4 KDEVIR #5 KDEVIR #2 URJC&amp;UNED #3 URJC&amp;UNED #2 URJC&amp;UNED #1 MICC #5 MICC #4 MICC #3 MICC #2 MICC #1 SZTAKI #1 SZTAKI #2 INAOE #3 INAOE #1 INAOE #2 INAOE #4 THSSMPAM #3 THSSMPAM #2 THSSMPAM #1 THSSMPAM #4 THSSMPAM #5 LMCHFUT #1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MF1-concepts unseen</head><p>Fig. <ref type="figure" coords="12,154.40,641.11,4.13,7.89">2</ref>: Graphs showing the test set performance measures (in %) for all the submissions. The error bars correspond to the 95% confidence intervals computed using Wilson's method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discussion</head><p>Due to the considerable participation in this evaluation very interesting results have been obtained. As can be observed in Table <ref type="table" coords="13,357.12,153.11,4.98,8.74" target="#tab_1">2</ref> and Figure <ref type="figure" coords="13,418.09,153.11,3.87,8.74">2</ref>, most of the submitted runs significantly outperformed the baseline system for both the development and test sets. When analyzing the sample based performances, very large differences can be observed amongst the systems. For both MAP-samples and MF 1 -samples the improvement has been from below 10% to over 40%. Moreover, the confidence intervals are relatively narrow, making the improvements quite significant. An interesting detail to note is that for MAP-samples there are several top performing systems, however, when comparing to the respective MF 1 -samples measures, three of the TPT submissions clearly outperform the rest. The key difference between these is the method for deciding which concepts are selected for a given image. This leads to believe that that many of the systems could improve greatly by changing that last step of their systems. As a side note, many of the participants chose to use the same scheme as the baseline system for selecting the concepts, the top N and fixed for all images.</p><p>The number of concepts per image is expected to be variable, thus making this strategy less than optimal. Future work should be addressed in this direction.</p><p>The MF 1 -concepts results in Figure <ref type="figure" coords="13,309.96,344.84,3.87,8.74">2</ref>, in contrast to the sample based performances, present much wider confidence intervals. This is due to two reasons, there are fewer concepts than sample images and the performance for different concepts varies greatly (see Figure <ref type="figure" coords="13,312.43,380.70,3.87,8.74">3</ref>). This effect is even greater for the MF 1 -concepts unseen, since these were only 21. Nevertheless, for MF 1 -concepts unseen, the top performing systems are statistically significantly better than the baselines and some of the lower performance systems. Moreover, in Figure <ref type="figure" coords="13,465.32,416.57,4.98,8.74">3</ref> it can be observed that the unseen concepts do not tend to perform worse. The difficulty of each particular concept affects more the performance than the fact that these have not been seen during development, or from another perspective the systems have been able to generalize rather well to the new concepts. Thus, this demonstrates potential for scalability of the systems. It would be desired for future benchmarking campaigns of this type to have more labeled data available for the evaluation, or find an alternative more automatic analysis, to be able to compare better the systems in this scalability performance aspect.</p><p>In contrast to usual image annotation evaluations with labeled training data, this challenge required work in more fronts, such as handling the noisy data, textual processing and multilabel annotations. This has given considerable freedom to the participants to concentrate their efforts in different aspects. Several teams extracted their own visual features, for which they did observe improvements with respect to the features provided by the organizers. On the other hand, for the textual processing, several different approaches were tried by the participants. Some of these teams (namely MIL, UNIMORE, CEA LIST, and URJC&amp;UNED) reported in their working notes papers and/or as observed in the results in this paper that as more information and additional resources are used (e.g. synonyms, plus hyponyms, etc.) the performance of the systems improved. Curiously, the best performing system, TPT, only used the provided visual features and did a very simple expansion of the concepts. Overall it seems that several of the proposed ideas by the participants are complementary, and thus considerable improvements could be expected in future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>This paper presented an overview of the ImageCLEF 2013 Scalable Concept Image Annotation Subtask, the second edition of a challenge aimed at developing more scalable image annotation systems. The goal was to develop annotation systems that for training only rely on unsupervised web data and other automatically obtainable resources, thus making it easy to add or change the concepts for annotation.</p><p>Considering that it is a relatively new challenge, the participation was excellent, 13 teams submitted in total 58 system runs. The performance of the submitted systems was considerably superior to the provided baselines, improving from below 10% to over 40% for both MAP-samples and MF 1 -samples measures. With respect to the performance of the systems when analyzed per concept, it was observed that the concepts vary greatly in difficulty. An important result was that for the concepts that were not seen during the development, the improvement was also significant, thus showing that the systems are capable of successfully using the noisy web data and generalizing well to new concepts. This clearly demonstrates potential for scalability of the systems. Finally, the participating teams presented several interesting approaches to address the proposed challenge, concentrating their efforts in different aspects of the problem. Many of these approaches are complementary, thus considerable improvements could be expected in future works.</p><p>Due to the success of this year's campaign and the very interesting results obtained, it would be important to continue organizing future editions. To be able to derive better conclusions about the performance generalization to unseen concepts, it would be desirable to have more labeled data available and/or find an alternative more automatic analysis which can help in giving more insight in this respect. Also, related challenges could be organized, for instance it could be assumed that for some concepts there is labeled data available, and find out how to take advantage of both the supervised and unsupervised data.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,215.02,190.51,185.32,7.89;2,223.72,285.57,167.92,7.89"><head>( a )</head><label>a</label><figDesc>Images from a search query of "rainbow".(b) Images from a search query of "sun".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,155.74,306.49,300.81,7.89;2,146.50,210.90,69.17,69.17"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Example of images retrieved by a commercial image search engine.</figDesc><graphic coords="2,146.50,210.90,69.17,69.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,144.73,409.18,335.87,8.77;8,144.73,421.16,335.87,8.74;8,144.73,433.12,120.02,8.74;8,134.02,441.96,346.58,11.26;8,144.73,456.43,306.90,8.74;8,134.02,467.77,346.57,8.77;8,144.73,479.75,335.86,8.74;8,144.73,491.71,274.36,8.74;8,134.02,503.04,346.57,8.77;8,144.73,515.02,137.99,8.74;8,134.02,526.36,346.57,8.77;8,144.73,538.34,335.87,8.74;8,144.73,550.30,326.86,8.74;8,134.02,561.63,346.57,8.77;8,144.73,573.62,335.87,8.74;8,144.73,585.57,72.59,8.74;8,134.02,596.90,346.58,8.77;8,144.73,608.89,335.86,8.74;8,144.73,620.84,223.53,8.74;8,134.02,632.18,346.57,8.77;8,144.73,644.16,335.86,8.74;8,144.73,656.12,67.55,8.74"><head></head><label></label><figDesc>CEA LIST: The team from the Vision &amp; Content Engineering group of CEA LIST (Gif-sur-Yvettes, France) was represented by Hervé Le Borgne, Adrian Popescu and Amel Znaidia. -INAOE: The team from the Instituto Nacional de Astrofísica, Óptica y Electrónica (Puebla, Mexico) was represented by Hugo Jair Escalante. -KDEVIR: The team from the Computer Science and Engineering department of the Toyohashi University of Technology (Aichi, Japan), was represented by Ismat Ara Reshma, Md Zia Ullah and Masaki Aono. -LMCHFUT: The team from Hefei University of Technology (Hefei, China) was represented by Yan Zigeng. -MICC: The team from the Media Integration and Communication Center of the Università degli Studi di Firenze (Florence, Italy) was represented by Tiberio Uricchio, Marco Bertini, Lamberto Ballan and Alberto Del Bimbo. -MIL: The team from the Machine Intelligence Lab of the University of Tokyo (Tokyo, Japan) was represented by Masatoshi Hidaka, Naoyuki Gunji and Tatsuya Harada. -RUC: The team from the School of Information of the Renmin University of China (Beijing, China) was represented by Xirong Li, Shuai Liao, Binbin Liu, Gang Yang, Qin Jin, Jieping Xu and Xiaoyong Du. -SZTAKI: The team from the Datamining and Search Research Group of the Hungarian Academy of Sciences (Budapest, Hungary) was represented by Bálint Daróczy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="10,134.02,118.96,346.57,8.77;10,134.02,132.67,346.57,8.77;10,144.73,144.65,107.43,8.74;10,134.02,158.33,346.57,8.77;10,144.73,170.32,335.87,8.74;10,144.73,182.27,335.87,8.74;10,144.73,194.23,224.35,8.74;10,134.02,207.90,346.58,8.77;10,144.73,219.89,335.86,8.74;10,144.73,231.84,310.26,8.74;10,134.02,245.52,346.58,8.77;10,144.73,257.50,335.87,8.74;10,144.73,269.46,335.86,8.74;10,144.73,281.41,336.90,8.74"><head>-</head><label></label><figDesc>THSSMPAM: The team from Beijing, China was represented by Jile Zhou. -TPT: The team of CNRS TELECOM ParisTech (Paris, France) was represented by Hichem Sahbi. -UNED&amp;UV: The team from the Universidad Nacional de Educación a Distancia (Madrid, Spain) and the Universitat de València was represented by Xaro Benavent, Angél Castellanos Gonzáles, Esther de Ves, D. Hernández-Aranda, Ruben Granados and Ana Garcia-Serrano. -UNIMORE: The team from the University of Modena and Reggio Emilia (Modena, Italy) was represented by Costantino Grana, Giuseppe Serra, Marco Manfredi, Rita Cucchiara, Riccardo Martoglia and Federica Mandreoli. -URJC&amp;UNED: The team of the Universidad Rey Juan Carlos (Móstoles, Spain) and the Universidad Nacional de Educación a Distancia (Madrid, Spain) was represented by Jesús Sánchez-Oro, Soto Montalvo, Antonio Montemayor, Juan Pantrigo, Abraham Duarte, Víctor Fresno and Raquel Martínez.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="9,107.36,52.34,569.65,486.25"><head>Table 1 :</head><label>1</label><figDesc>Comparison of the systems for the best submission of each group.</figDesc><table coords="9,107.36,75.43,569.65,463.16"><row><cell>System</cell><cell>Visual Features [Total Dim.]</cell><cell>Other Used Resources</cell><cell>Training Data Processing Highlights</cell><cell>Annotation Technique Highlights</cell></row><row><cell>TPT [13] #6</cell><cell>Provided by organizers (All 7) [Tot. Dim. = 21312]</cell><cell>* Morphological expansions</cell><cell>Manual morphological expansions of the concepts (plural forms). Training images selected by appearance of concept in supplied textual features.</cell><cell>Multiple SVMs per concept, with context dependent kernels. Annotation based on threshold (the same for all concepts).</cell></row><row><cell>MIL [6] #4</cell><cell>Fisher Vectors (SIFT, C-SIFT, LBP, GIST) [Tot. Dim. = 262144]</cell><cell>* WordNet * ActiveSupport library for word singularization</cell><cell>Extract webpage title, image attributes, surrounding text, and singularize nouns. Label training images by appearance of concept, defined by WordNet synonyms and hyponyms with a single meaning.</cell><cell>Linear multilabel classifier learned by PAAPL. Annotation of the top 5 concepts.</cell></row><row><cell>UNIMORE [5] #2</cell><cell>Multiv. Gauss. Distrib. of local desc. (HSV-SIFT, OPP-SIFT, RGB-SIFT) [Tot. Dim. = 201216]</cell><cell>* WordNet * NLTK (stopwords and stemmer) * +100k training images</cell><cell>Stopword removal and stemming of supplied other senses of concept word. preprocessed features and webpage title. Label training images by appearance of concept, defined by WordNet synonyms and hyponyms with a single meaning. Disambiguation by negative context from</cell><cell>Linear SVMs learned by stochastic gradient descent. Annotation based on threshold (the same for all concepts).</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Positive training images selected by a combination of</cell><cell></cell></row><row><cell>RUC [9] #4</cell><cell>Provided by organizers (All 7) [Tot. Dim. = 21312]</cell><cell>* Search engine keywords * Flickr tags dataset</cell><cell>supplied textual features and search engine keywords weighted by a tag co-occurrence measure derived from a Flickr dataset. Negative examples selected by</cell><cell>Multiple staked hikSVMs and kNNs (L1 distance). Annotation of the top 6 concepts.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Negative Bootstrap.</cell><cell></cell></row><row><cell>UNED&amp;UV [1] #3</cell><cell>-a</cell><cell>* Webpage of test images * WordNet * Lucene</cell><cell>Concept indexing (using Lucene) of WordNet definition, forms, hypernyms, hyponyms and related components.</cell><cell>Text retrieval of concepts using webpage img field. Annotation by a cut-off percentage of the maximum scored concept.</cell></row><row><cell>CEA LIST [2] #4</cell><cell>Bag of Visterms (SIFT) [Tot. Dim. = 8192]</cell><cell>* Wikipedia * Flickr tags dataset</cell><cell>Training images selected by ranking the images using tag models learned from Flickr and Wikipedia data. The first 100 ranked images as positive and the last 500 images as negative.</cell><cell></cell></row><row><cell>KDEVIR [12] #1</cell><cell>Provided by organizers (colorhist, C-SIFT, OPP-SIFT, RGB-SIFT) [Tot. Dim. = 15576]</cell><cell>* WordNet * Lucene stemmer</cell><cell>Stopwords and non-English words removal and scores per image. stemming of supplied textual features. Matching of features with concepts defined by WordNet synonyms and application of bm25 to obtain concept</cell><cell>kNN (IDsim) and aggregating concept scores (bm25). Annotation of top 10 concepts.</cell></row><row><cell>URJC&amp; UNED [14] #3</cell><cell>HSV histograms, LBP and provided by organizers (C-SIFT) [Tot. Dim. = 5384]</cell><cell>* Search engine keywords * WordNet * English Stopwords list * Porter stemmer</cell><cell>Stopword removal and stemming of supplied textual features, and enriched by WordNet synonyms and hyperonyms. Generation of keywords-concepts co-occurrence matrix.</cell><cell>kNN (Bhattacharyya, χ 2 , and L2 distances) and aggregating concept scores (co-occurrence). Annotation based on threshold (the same for all concepts).</cell></row><row><cell>MICC [18] #5</cell><cell>Provided by organizers (All 7) [Tot. Dim. = 21312]</cell><cell>* WordNet * Wikipedia * Search engine keywords * Training image URLs</cell><cell>Stopword removal of supplied textual features, search engine keywords and URL extracted words. Enriched textual features with WordNet synonyms and Wikipedia link structure.</cell><cell>kNN (Gaussian kernel distance) and rank concepts by tagRelevance. Annotation of the top 7 concepts.</cell></row><row><cell>SZTAKI #1</cell><cell>Fisher Vectors [Tot. Dim. = Unknown]</cell><cell>* Wikipedia</cell><cell>Fisher vector-based learning of visual model given training images per category.</cell><cell>Textual ranking of images based on Wikipedia concept descriptions. Prediction via visual models.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Documents are represented by a distribution of</cell><cell></cell></row><row><cell>INAOE #3</cell><cell>SIFT [Tot. Dim. = Unknown]</cell><cell>Unknown</cell><cell>occurrences over other documents in the corpus, so that documents are represented by their context,</cell><cell>Ensemble of linear classifiers per concept.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>yielding a prototype per concept.</cell><cell></cell></row><row><cell></cell><cell>Global: CEDD, Color,</cell><cell></cell><cell></cell><cell>Tags of NN image ranked by TF-IDF.</cell></row><row><cell>THSSMPAM #2</cell><cell>Bag of visterms (SIFT) Local: SIFT, SURF</cell><cell>* WordNet</cell><cell>Unknown</cell><cell>Similarity between tags and concepts using WordNet. Annotation by</cell></row><row><cell></cell><cell>[Tot. Dim. = Unknown]</cell><cell></cell><cell></cell><cell>bipartite graph algorithm.</cell></row><row><cell>LMCHFUT #1</cell><cell>Provided by organizers (SIFT) [Tot. Dim. = 5000]</cell><cell>Unknown</cell><cell>Training images selected by appearance of concept in supplied textual features.</cell><cell>Single SVM learned per concept given visual features of positive and negative training examples.</cell></row></table><note coords="9,541.19,281.97,128.23,6.12;9,541.19,289.94,135.05,6.12;9,541.19,297.91,130.25,6.12;9,541.19,305.88,25.85,6.12"><p>Linear SVM. Annotation of concepts with score above µ + σ (µ, σ are mean and standard deviation of all concept scores).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="11,134.77,127.32,345.82,567.20"><head>Table 2 :</head><label>2</label><figDesc>Performance measures (in %) for the baseline techniques and all submissions. The best submission for each team is highlighted in bold font.</figDesc><table coords="11,139.24,148.63,330.08,526.58"><row><cell>System</cell><cell cols="6">MAP-samples MF1-samples MF1-concepts</cell><cell>MF1-concepts unseen in dev.</cell></row><row><cell></cell><cell>dev.</cell><cell>test</cell><cell>dev.</cell><cell>test</cell><cell>dev.</cell><cell>test</cell><cell>test</cell></row><row><cell>Baseline OPP-SIFT</cell><cell>24.6</cell><cell>21.4</cell><cell>19.2</cell><cell>16.4</cell><cell>13.8</cell><cell>11.8</cell><cell>10.3</cell></row><row><cell>Baseline C-SIFT</cell><cell>24.2</cell><cell>21.2</cell><cell>18.6</cell><cell>16.2</cell><cell>10.7</cell><cell>10.5</cell><cell>10.8</cell></row><row><cell>Baseline RGB-SIFT</cell><cell>24.3</cell><cell>21.2</cell><cell>18.5</cell><cell>15.8</cell><cell>13.0</cell><cell>11.7</cell><cell>10.5</cell></row><row><cell>Baseline SIFT</cell><cell>24.0</cell><cell>21.0</cell><cell>17.8</cell><cell>15.9</cell><cell>11.0</cell><cell>11.0</cell><cell>10.1</cell></row><row><cell>Baseline Colorhist</cell><cell>22.1</cell><cell>19.0</cell><cell>16.1</cell><cell>13.9</cell><cell>8.0</cell><cell>8.0</cell><cell>9.6</cell></row><row><cell>Baseline GIST</cell><cell>20.9</cell><cell>17.8</cell><cell>14.5</cell><cell>12.5</cell><cell>6.1</cell><cell>6.9</cell><cell>7.3</cell></row><row><cell>Baseline GETLF</cell><cell>21.0</cell><cell>17.7</cell><cell>14.9</cell><cell>12.5</cell><cell>6.6</cell><cell>5.4</cell><cell>5.9</cell></row><row><cell>Baseline Random</cell><cell>10.9</cell><cell>8.7</cell><cell>6.2</cell><cell>4.6</cell><cell>4.8</cell><cell>3.6</cell><cell>2.3</cell></row><row><cell>TPT #6</cell><cell>50.4</cell><cell>44.4</cell><cell>51.3</cell><cell>42.6</cell><cell>45.0</cell><cell>34.1</cell><cell>45.1</cell></row><row><cell>TPT #4</cell><cell>48.9</cell><cell>43.2</cell><cell>50.7</cell><cell>41.8</cell><cell>42.5</cell><cell>33.7</cell><cell>45.3</cell></row><row><cell>MIL #4</cell><cell>43.8</cell><cell>41.4</cell><cell>34.0</cell><cell>32.4</cell><cell>34.7</cell><cell>32.3</cell><cell>35.8</cell></row><row><cell>MIL #1</cell><cell>44.5</cell><cell>42.1</cell><cell>34.6</cell><cell>33.2</cell><cell>35.2</cell><cell>32.6</cell><cell>33.8</cell></row><row><cell>MIL #2</cell><cell>43.1</cell><cell>40.7</cell><cell>34.3</cell><cell>32.7</cell><cell>33.9</cell><cell>31.8</cell><cell>31.4</cell></row><row><cell>UNIMORE #2</cell><cell>46.0</cell><cell>44.1</cell><cell>27.3</cell><cell>27.5</cell><cell>34.2</cell><cell>33.1</cell><cell>34.8</cell></row><row><cell>UNIMORE #5</cell><cell>47.9</cell><cell>45.6</cell><cell>33.3</cell><cell>31.5</cell><cell>33.7</cell><cell>31.9</cell><cell>31.9</cell></row><row><cell>UNIMORE #1</cell><cell>39.2</cell><cell>36.7</cell><cell>33.0</cell><cell>31.1</cell><cell>34.1</cell><cell>32.0</cell><cell>31.3</cell></row><row><cell>TPT #2</cell><cell>38.5</cell><cell>37.0</cell><cell>41.4</cell><cell>38.1</cell><cell>30.9</cell><cell>30.0</cell><cell>30.9</cell></row><row><cell>UNIMORE #6</cell><cell>46.0</cell><cell>44.1</cell><cell>33.0</cell><cell>31.1</cell><cell>34.1</cell><cell>32.0</cell><cell>31.3</cell></row><row><cell>RUC #4</cell><cell>41.2</cell><cell>38.0</cell><cell>31.6</cell><cell>29.0</cell><cell>33.4</cell><cell>30.4</cell><cell>32.8</cell></row><row><cell>MIL #5</cell><cell>42.2</cell><cell>39.7</cell><cell>34.0</cell><cell>31.7</cell><cell>33.4</cell><cell>30.9</cell><cell>30.2</cell></row><row><cell>MIL #3</cell><cell>42.5</cell><cell>39.6</cell><cell>34.2</cell><cell>31.8</cell><cell>33.4</cell><cell>30.2</cell><cell>29.5</cell></row><row><cell>RUC #5</cell><cell>40.5</cell><cell>37.6</cell><cell>31.0</cell><cell>28.3</cell><cell>32.7</cell><cell>29.6</cell><cell>31.5</cell></row><row><cell>UNED&amp;UV #3</cell><cell>27.1</cell><cell>26.6</cell><cell>22.5</cell><cell>23.1</cell><cell>31.5</cell><cell>31.3</cell><cell>43.2</cell></row><row><cell>UNIMORE #3</cell><cell>43.7</cell><cell>41.9</cell><cell>23.1</cell><cell>23.1</cell><cell>32.4</cell><cell>31.5</cell><cell>35.5</cell></row><row><cell>UNED&amp;UV #5</cell><cell>35.5</cell><cell>33.2</cell><cell>27.6</cell><cell>24.4</cell><cell>31.7</cell><cell>29.2</cell><cell>35.4</cell></row><row><cell>TPT #5</cell><cell>49.8</cell><cell>44.3</cell><cell>38.7</cell><cell>32.5</cell><cell>33.0</cell><cell>26.7</cell><cell>27.3</cell></row><row><cell>RUC #3</cell><cell>39.4</cell><cell>36.9</cell><cell>29.8</cell><cell>27.8</cell><cell>31.4</cell><cell>29.2</cell><cell>30.2</cell></row><row><cell>TPT #3</cell><cell>49.0</cell><cell>43.6</cell><cell>38.8</cell><cell>31.9</cell><cell>30.2</cell><cell>24.8</cell><cell>24.7</cell></row><row><cell>RUC #2</cell><cell>38.2</cell><cell>35.5</cell><cell>28.8</cell><cell>26.5</cell><cell>30.8</cell><cell>28.5</cell><cell>29.9</cell></row><row><cell>UNIMORE #4</cell><cell>39.7</cell><cell>36.2</cell><cell>26.8</cell><cell>24.1</cell><cell>31.7</cell><cell>29.5</cell><cell>28.0</cell></row><row><cell>UNED&amp;UV #4</cell><cell>31.0</cell><cell>29.8</cell><cell>29.9</cell><cell>30.0</cell><cell>26.3</cell><cell>22.8</cell><cell>24.6</cell></row><row><cell>UNED&amp;UV #1</cell><cell>32.8</cell><cell>30.3</cell><cell>25.0</cell><cell>23.0</cell><cell>27.5</cell><cell>25.0</cell><cell>31.7</cell></row><row><cell>RUC #1</cell><cell>36.1</cell><cell>32.4</cell><cell>28.8</cell><cell>25.4</cell><cell>26.6</cell><cell>23.9</cell><cell>22.7</cell></row><row><cell>UNED&amp;UV #2</cell><cell>32.4</cell><cell>30.6</cell><cell>24.4</cell><cell>22.9</cell><cell>26.1</cell><cell>24.0</cell><cell>30.6</cell></row><row><cell>CEA LIST #4</cell><cell>40.3</cell><cell>34.2</cell><cell>32.2</cell><cell>26.0</cell><cell>26.1</cell><cell>21.2</cell><cell>20.1</cell></row><row><cell>CEA LIST #5</cell><cell>39.2</cell><cell>33.6</cell><cell>31.6</cell><cell>25.7</cell><cell>25.4</cell><cell>21.0</cell><cell>20.0</cell></row><row><cell>CEA LIST #3</cell><cell>40.4</cell><cell>34.1</cell><cell>31.8</cell><cell>25.2</cell><cell>25.3</cell><cell>20.2</cell><cell>20.5</cell></row><row><cell>CEA LIST #2</cell><cell>39.6</cell><cell>33.6</cell><cell>30.2</cell><cell>24.2</cell><cell>24.6</cell><cell>20.1</cell><cell>20.1</cell></row><row><cell>CEA LIST #1</cell><cell>34.6</cell><cell>29.4</cell><cell>28.7</cell><cell>23.0</cell><cell>23.6</cell><cell>19.0</cell><cell>19.8</cell></row><row><cell>KDEVIR #1</cell><cell>28.7</cell><cell>26.1</cell><cell>25.3</cell><cell>22.2</cell><cell>21.1</cell><cell>18.0</cell><cell>17.3</cell></row><row><cell>URJC&amp;UNED #3</cell><cell>32.6</cell><cell>28.1</cell><cell>27.9</cell><cell>24.1</cell><cell>19.8</cell><cell>17.3</cell><cell>14.8</cell></row><row><cell>MICC #5</cell><cell>29.1</cell><cell>26.2</cell><cell>22.7</cell><cell>20.0</cell><cell>21.4</cell><cell>18.0</cell><cell>18.6</cell></row><row><cell>MICC #4</cell><cell>29.2</cell><cell>26.1</cell><cell>22.4</cell><cell>20.0</cell><cell>21.0</cell><cell>18.0</cell><cell>18.6</cell></row><row><cell>MICC #3</cell><cell>29.0</cell><cell>26.1</cell><cell>22.3</cell><cell>20.0</cell><cell>21.0</cell><cell>18.1</cell><cell>18.5</cell></row><row><cell>URJC&amp;UNED #2</cell><cell>32.2</cell><cell>27.6</cell><cell>27.7</cell><cell>23.8</cell><cell>19.7</cell><cell>17.2</cell><cell>14.6</cell></row><row><cell>URJC&amp;UNED #1</cell><cell>32.0</cell><cell>27.6</cell><cell>27.4</cell><cell>23.7</cell><cell>19.2</cell><cell>17.1</cell><cell>14.6</cell></row><row><cell>MICC #2</cell><cell>29.0</cell><cell>26.1</cell><cell>23.3</cell><cell>20.4</cell><cell>20.7</cell><cell>17.5</cell><cell>17.0</cell></row><row><cell>MICC #1</cell><cell>28.7</cell><cell>25.9</cell><cell>20.4</cell><cell>18.7</cell><cell>20.3</cell><cell>17.3</cell><cell>17.6</cell></row><row><cell>KDEVIR #3</cell><cell>28.6</cell><cell>24.8</cell><cell>24.8</cell><cell>21.1</cell><cell>18.7</cell><cell>15.9</cell><cell>15.6</cell></row><row><cell>TPT #1</cell><cell>38.6</cell><cell>36.8</cell><cell>30.2</cell><cell>23.0</cell><cell>24.2</cell><cell>19.2</cell><cell>8.2</cell></row><row><cell>KDEVIR #6</cell><cell>28.3</cell><cell>24.3</cell><cell>24.5</cell><cell>20.8</cell><cell>18.4</cell><cell>15.7</cell><cell>15.0</cell></row><row><cell>KDEVIR #4</cell><cell>29.2</cell><cell>26.4</cell><cell>24.7</cell><cell>20.5</cell><cell>18.5</cell><cell>15.4</cell><cell>15.3</cell></row><row><cell>KDEVIR #5</cell><cell>29.0</cell><cell>25.6</cell><cell>24.6</cell><cell>20.2</cell><cell>18.5</cell><cell>15.1</cell><cell>14.5</cell></row><row><cell>KDEVIR #2</cell><cell>26.4</cell><cell>23.5</cell><cell>25.0</cell><cell>20.7</cell><cell>19.2</cell><cell>14.8</cell><cell>12.6</cell></row><row><cell>SZTAKI #1</cell><cell>32.9</cell><cell>28.2</cell><cell>10.4</cell><cell>9.5</cell><cell>17.7</cell><cell>16.4</cell><cell>16.7</cell></row><row><cell>INAOE #3</cell><cell>24.0</cell><cell>19.1</cell><cell>19.7</cell><cell>15.4</cell><cell>17.7</cell><cell>15.2</cell><cell>11.1</cell></row><row><cell>SZTAKI #2</cell><cell>32.7</cell><cell>28.0</cell><cell>9.8</cell><cell>8.8</cell><cell>17.1</cell><cell>15.1</cell><cell>16.0</cell></row><row><cell>THSSMPAM #3</cell><cell>20.9</cell><cell>15.9</cell><cell>17.0</cell><cell>14.8</cell><cell>13.0</cell><cell>12.7</cell><cell>11.1</cell></row><row><cell>THSSMPAM #2 LMCHFUT #1</cell><cell>21.7 N/A a</cell><cell>16.1 N/A a</cell><cell>17.0 12.2</cell><cell>14.8 11.0</cell><cell>13.0 13.6</cell><cell>12.7 12.1</cell><cell>11.1 11.3</cell></row><row><cell>INAOE #1</cell><cell>21.5</cell><cell>17.5</cell><cell>21.3</cell><cell>16.9</cell><cell>9.0</cell><cell>6.9</cell><cell>5.1</cell></row><row><cell>THSSMPAM #1</cell><cell>16.3</cell><cell>12.0</cell><cell>18.2</cell><cell>11.8</cell><cell>13.7</cell><cell>10.0</cell><cell>6.6</cell></row><row><cell>INAOE #2</cell><cell>23.6</cell><cell>19.0</cell><cell>24.8</cell><cell>16.7</cell><cell>6.3</cell><cell>4.8</cell><cell>4.7</cell></row><row><cell>THSSMPAM #4</cell><cell>15.9</cell><cell>11.9</cell><cell>15.5</cell><cell>11.8</cell><cell>12.2</cell><cell>10.0</cell><cell>6.6</cell></row><row><cell>THSSMPAM #5</cell><cell>15.8</cell><cell>11.9</cell><cell>15.5</cell><cell>11.8</cell><cell>12.2</cell><cell>10.0</cell><cell>6.6</cell></row><row><cell>INAOE #4</cell><cell>17.9</cell><cell>8.3</cell><cell>15.9</cell><cell>6.2</cell><cell>11.7</cell><cell>3.4</cell><cell>2.3</cell></row></table><note coords="11,137.55,685.31,3.61,5.21;11,144.73,688.40,194.35,6.12"><p>a Concept scores not provided, only annotation decisions.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="15,126.58,402.40,549.58,83.25"><head></head><label></label><figDesc>Boxplots (also known as box-and-whiskers) for the test set of the per concept annotation F1-measures (in %) for all runs combined. The plots are ordered by the median performance. Concepts in red font are the ones not seen in development.</figDesc><table coords="15,126.58,402.40,546.26,72.29"><row><cell cols="7">/sunset lightning space underwater plant person fog tree cartoon sky water bus mountain galaxy building road pool cloud nebula tricycle desert boat aerial newspaper vehicle sea forest grass furniture flower moon protest sand snow logo sun bicycle traffic silhouette fire lake butterfly church bottle drink instrument horse violin chair food beach wagon castle portrait helicopter toy painting nighttime submarine river truck motorcycle harbor car reptile rainbow fish baby bridge sign diagram footwear guitar train book soil coast arthropod sculpture cat phone reflection table dog sport child garden rain highway spider airplane hat cityscape park outdoor drum bird teenager elder embroidery countryside poster shadow daytime smoke overcast indoor spectacles female male closeup cloudless monument rodent unpaved Concept Type WN 3.0 WN 3.0 Wikipedia article #images sense# offset dev. test aerial adj. 1 01380267 Aerial photography 39 72 airplane noun 1 02691156 Airplane 13 20 baby noun 1 09827683 Baby 9 29 beach noun 1 09217230 Beach 36 51 bicycle noun 1 02834778 Bicycle 17 15 bird noun 1 01503061 Bird 25 30 boat noun 1 02858304 Boat 59 83 book noun 2, 1 02870092, 06410904 Book 22 22 bridge noun 1 02898711 Bridge 35 44 building noun 1 02913152 Building 188 288 car noun 1 02958343 Car 47 86 cartoon noun 1 06780678 Cartoon 31 73 castle noun 2 02980441 Castle 18 20 cat noun 1 02121620 Cat 12 21 child noun 1 09917593 Child 23 60 church noun 2 03028079 Church (building) 12 16 cityscape noun 1 06209770 Cityscape 67 95 closeup noun 1 03049695 Closeup 15 56 cloud noun 2 09247410 Cloud 239 340 cloudless adj. 1 00460946 -99 159 coast noun 1 09428293 Coast 46 64 countryside noun 1 08645033 Countryside 43 74 daytime noun 1 15164957 Daytime (astronomy) 587 989 desert noun 1 08505573 Desert 19 27 diagram noun 1 03186399 Diagram 11 23 dog noun 1 02084071 Dog 28 34 drum noun 1 03249569 Drum 12 9 elder noun 1 10048218 Elderly 12 37 embroidery noun 2 03282933 Embroidery 10 14 female noun 2 09619168 Female 41 149 fire noun 3, 1 13480848, 07302836 Fire 28 34 firework noun 1 03348454 Firework 11 20 fish noun 1 02512053 Fish 17 33 flower noun 2 11669335 Flower 46 111 fog noun 2 14521648 Fog 17 39 food noun 2, 1 07555863, 00021265 Food 20 59 Fig. 3: A Concept List footwear noun 1, 2 03381126, 03380867 Footwear 19 40</cell></row><row><cell>forest</cell><cell>noun</cell><cell>1, 2</cell><cell>08438533, 09284015</cell><cell>Forest</cell><cell>96</cell><cell>129</cell></row><row><cell>furniture</cell><cell>noun</cell><cell>1</cell><cell>03405725</cell><cell>Furniture</cell><cell>52</cell><cell>120</cell></row><row><cell>garden</cell><cell>noun</cell><cell>1</cell><cell>03417345</cell><cell>Garden</cell><cell>14</cell><cell>21</cell></row><row><cell>grass</cell><cell>noun</cell><cell>1</cell><cell>12102133</cell><cell>Grass</cell><cell>162</cell><cell>253</cell></row><row><cell>guitar</cell><cell>noun</cell><cell>1</cell><cell>03467517</cell><cell>Guitar</cell><cell>7</cell><cell>13</cell></row><row><cell>harbor</cell><cell>noun</cell><cell>1</cell><cell>08639058</cell><cell>Harbor</cell><cell>20</cell><cell>35</cell></row><row><cell>helicopter</cell><cell>noun</cell><cell>1</cell><cell>03512147</cell><cell>Helicopter</cell><cell>8</cell><cell>14</cell></row><row><cell>highway</cell><cell>noun</cell><cell>1</cell><cell>03519981</cell><cell>Highway</cell><cell>15</cell><cell>16</cell></row><row><cell>horse</cell><cell>noun</cell><cell>1</cell><cell>02374451</cell><cell>Horse</cell><cell>18</cell><cell>44</cell></row><row><cell>indoor</cell><cell>adj.</cell><cell>1</cell><cell>01692786</cell><cell>-</cell><cell>87</cell><cell>218</cell></row><row><cell>instrument</cell><cell>noun</cell><cell>6</cell><cell>03800933</cell><cell>Musical instrument</cell><cell>34</cell><cell>58</cell></row><row><cell>lake</cell><cell>noun</cell><cell>1</cell><cell>09328904</cell><cell>Lake</cell><cell>43</cell><cell>65</cell></row><row><cell>lightning</cell><cell>noun</cell><cell>1, 2</cell><cell>11475279, 07412993</cell><cell>Lightning</cell><cell>10</cell><cell>16</cell></row><row><cell>logo</cell><cell>noun</cell><cell>1</cell><cell>07272084</cell><cell>Logo</cell><cell>15</cell><cell>35</cell></row><row><cell>male</cell><cell>noun</cell><cell>2</cell><cell>09624168</cell><cell>Male</cell><cell>53</cell><cell>115</cell></row><row><cell>monument</cell><cell>noun</cell><cell>1</cell><cell>03743902</cell><cell>Monument</cell><cell>8</cell><cell>19</cell></row><row><cell>moon</cell><cell>noun</cell><cell>1</cell><cell>09358358</cell><cell>Moon</cell><cell>7</cell><cell>31</cell></row><row><cell>motorcycle</cell><cell>noun</cell><cell>1</cell><cell>03790512</cell><cell>Motorcycle</cell><cell>12</cell><cell>20</cell></row><row><cell>mountain</cell><cell>noun</cell><cell>1</cell><cell>09359803</cell><cell>Mountain</cell><cell>100</cell><cell>181</cell></row><row><cell>newspaper</cell><cell>noun</cell><cell>3, 1</cell><cell>03822171, 06267145</cell><cell>Newspaper</cell><cell>9</cell><cell>9</cell></row><row><cell></cell><cell></cell><cell></cell><cell>continues in next page</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,657.44,61.20,7.47"><p>www.mturk.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,144.73,656.80,276.66,8.12"><p>Subtask website at http://imageclef.org/2013/photo/annotation</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,144.73,656.80,252.85,8.12"><p>Dataset available at http://risenet.iti.upv.es/webupv250k</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="5,144.73,656.80,224.62,7.86"><p>This textual data was identical to the 2012 edition<ref type="bibr" coords="5,352.45,656.80,13.52,7.86" target="#b19">[20]</ref>.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The authors are very grateful of the support of the <rs type="funder">CLEF campaign</rs> for the <rs type="projectName">ImageCLEF</rs> initiative. The research leading to these results has received funding from the <rs type="funder">European Union</rs>'s <rs type="programName">Seventh Framework Programme</rs> (<rs type="grantNumber">FP7/2007-2013</rs>) under the tranScriptorium project (#<rs type="grantNumber">600707</rs>), the <rs type="projectName">LiMoSINe</rs> project (#<rs type="grantNumber">288024</rs>), and from the <rs type="funder">Spanish MEC</rs> under the <rs type="projectName">STraDA</rs> project (<rs type="grantNumber">TIN2012-37475-C02-01</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_a74dE2m">
					<orgName type="project" subtype="full">ImageCLEF</orgName>
				</org>
				<org type="funding" xml:id="_awK5Dja">
					<idno type="grant-number">FP7/2007-2013</idno>
					<orgName type="program" subtype="full">Seventh Framework Programme</orgName>
				</org>
				<org type="funded-project" xml:id="_fANzUUZ">
					<idno type="grant-number">600707</idno>
					<orgName type="project" subtype="full">LiMoSINe</orgName>
				</org>
				<org type="funded-project" xml:id="_rwbRBer">
					<idno type="grant-number">288024</idno>
					<orgName type="project" subtype="full">STraDA</orgName>
				</org>
				<org type="funding" xml:id="_XP3NT2h">
					<idno type="grant-number">TIN2012-37475-C02-01</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="16,142.96,142.34,337.63,7.86;16,151.52,153.29,329.07,7.86;16,151.52,164.25,329.07,7.86;16,151.52,175.21,268.49,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="16,231.77,153.29,248.82,7.86;16,151.52,164.25,75.71,7.86">A multimedia IR-based system for the Photo Annotation Task at ImageCLEF2013</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Benavent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Castellanos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>De Ves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hernández-Aranda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Granados</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcia-Serrano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,248.26,164.25,232.34,7.86;16,151.52,175.21,36.52,7.86">CLEF 2013 Evaluation Labs and Workshop, Online Working Notes</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">September 23-26 2013</date>
		</imprint>
	</monogr>
	<note>Cited on page 9</note>
</biblStruct>

<biblStruct coords="16,142.96,186.12,337.64,7.86;16,151.52,197.08,329.07,7.86;16,151.52,208.04,290.27,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="16,318.92,186.12,161.67,7.86;16,151.52,197.08,106.02,7.86">CEA LIST@imageCLEF 2013: Scalable Concept Image Annotation</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">L</forename><surname>Borgne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Znaidia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,277.37,197.08,203.22,7.86;16,151.52,208.04,58.30,7.86">CLEF 2013 Evaluation Labs and Workshop, Online Working Notes</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">September 23-26 2013</date>
		</imprint>
	</monogr>
	<note>Cited on page 9</note>
</biblStruct>

<biblStruct coords="16,142.96,218.95,337.64,7.86;16,151.52,229.91,329.07,7.86;16,151.52,240.86,329.07,7.86;16,151.52,251.82,329.07,7.86;16,151.52,264.14,24.80,6.12" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="16,453.20,229.91,27.39,7.86;16,151.52,240.86,223.61,7.86">Image-CLEF 2013: the vision, the data and the open challenges</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zellhöfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Martínez-Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>García-Varea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cazorla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="16,395.08,240.86,85.51,7.86;16,151.52,251.82,82.63,7.86">CLEF. Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2013">September 23-26 2013</date>
			<publisher>Springer</publisher>
			<pubPlace>Valencia, Spain</pubPlace>
		</imprint>
	</monogr>
	<note>Cited on page 2</note>
</biblStruct>

<biblStruct coords="16,142.96,273.69,337.64,7.86;16,151.52,284.65,239.29,7.86" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="16,234.71,273.69,170.44,7.86">WordNet An Electronic Lexical Database</title>
		<editor>Fellbaum, C.</editor>
		<imprint>
			<date type="published" when="1998-05">May 1998</date>
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge, MA; London</pubPlace>
		</imprint>
	</monogr>
	<note>Cited on pages 4 and 5</note>
</biblStruct>

<biblStruct coords="16,142.96,295.56,337.64,7.86;16,151.52,306.52,329.07,7.86;16,151.52,317.48,329.07,7.86;16,151.52,328.43,159.65,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="16,151.52,306.52,282.16,7.86">UNIMORE at ImageCLEF 2013: Scalable Concept Image Annotation</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Grana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Manfredi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Martoglia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Mandreoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,455.89,306.52,24.70,7.86;16,151.52,317.48,255.85,7.86">CLEF 2013 Evaluation Labs and Workshop, Online Working Notes</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">September 23-26 2013</date>
		</imprint>
	</monogr>
	<note>Cited on page 9</note>
</biblStruct>

<biblStruct coords="16,142.96,339.34,337.63,7.86;16,151.52,350.30,329.07,7.86;16,151.52,361.26,253.13,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="16,294.56,339.34,186.03,7.86;16,151.52,350.30,70.44,7.86">MIL at ImageCLEF 2013: Scalable System for Image Annotation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hidaka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Gunji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,241.64,350.30,238.95,7.86;16,151.52,361.26,21.16,7.86">CLEF 2013 Evaluation Labs and Workshop, Online Working Notes</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">September 23-26 2013</date>
		</imprint>
	</monogr>
	<note>Cited on page 9</note>
</biblStruct>

<biblStruct coords="16,142.96,372.17,337.64,7.86;16,151.52,383.13,329.07,7.86;16,151.52,394.09,329.07,7.86;16,151.52,406.41,35.85,6.12" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="16,295.97,372.17,184.63,7.86;16,151.52,383.13,181.88,7.86">Combining textual and visual cues for contentbased image retrieval on the World Wide Web</title>
		<author>
			<persName coords=""><forename type="first">La</forename><surname>Cascia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,354.94,383.13,125.65,7.86;16,151.52,394.09,76.69,7.86;16,257.28,394.09,117.29,7.86">Content-Based Access of Image and Video Libraries</title>
		<imprint>
			<date type="published" when="1998">1998. 1998</date>
			<biblScope unit="page" from="24" to="28" />
		</imprint>
	</monogr>
	<note>Proceedings. IEEE Workshop. Cited on page 6</note>
</biblStruct>

<biblStruct coords="16,142.96,415.95,337.64,7.86;16,151.52,426.91,329.07,7.86;16,151.52,437.87,329.07,7.86;16,151.52,448.83,329.07,7.86;16,151.52,459.79,296.00,8.11" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="16,304.47,415.95,176.13,7.86;16,151.52,426.91,209.26,7.86">Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2006.68</idno>
		<ptr target="http://dx.doi.org/10.1109/CVPR.2006.68" />
	</analytic>
	<monogr>
		<title level="m" coord="16,383.06,426.91,97.53,7.86;16,151.52,437.87,329.07,7.86;16,262.90,448.83,40.20,7.86">Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
	<note>CVPR &apos;06</note>
</biblStruct>

<biblStruct coords="16,142.96,470.70,337.64,7.86;16,151.52,481.66,329.07,7.86;16,151.52,492.62,329.07,7.86;16,151.52,503.58,126.62,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="16,393.73,470.70,86.86,7.86;16,151.52,481.66,259.59,7.86">Renmin University of China at ImageCLEF 2013 Scalable Concept Image Annotation</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,433.82,481.66,46.77,7.86;16,151.52,492.62,220.41,7.86">CLEF 2013 Evaluation Labs and Workshop, Online Working Notes</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">September 23-26 2013</date>
		</imprint>
	</monogr>
	<note>Cited on page 9</note>
</biblStruct>

<biblStruct coords="16,142.61,514.48,337.98,7.86;16,151.52,525.44,329.07,7.86;16,151.52,536.40,329.07,7.86;16,151.52,547.36,194.28,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="16,311.08,514.48,169.51,7.86;16,151.52,525.44,125.50,7.86">The CLEF 2011 Photo Annotation and Concept-based Retrieval Tasks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liebetrau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,151.52,536.40,204.87,7.86">CLEF 2011 Labs and Workshop, Notebook Papers</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Petras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</editor>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-09">September 2011. 2011</date>
			<biblScope unit="page" from="19" to="22" />
		</imprint>
	</monogr>
	<note>Cited on page 1</note>
</biblStruct>

<biblStruct coords="16,142.61,558.27,337.98,7.86;16,151.52,569.23,329.07,8.12;16,151.52,580.83,233.34,7.47" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="16,245.36,558.27,235.23,7.86;16,151.52,569.23,91.62,7.86">Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1011139631724</idno>
		<ptr target="http://dx.doi.org/10.1023/A:1011139631724" />
	</analytic>
	<monogr>
		<title level="j" coord="16,250.33,569.23,90.88,7.86">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001-05">May 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.61,591.09,337.98,7.86;16,151.52,602.05,329.07,7.86;16,151.52,613.01,253.13,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="16,304.24,591.09,176.35,7.86;16,151.52,602.05,57.89,7.86">KDEVIR at ImageCLEF 2013 Image Annotation Subtask</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">A</forename><surname>Reshma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">Z</forename><surname>Ullah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Aono</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,231.82,602.05,248.77,7.86;16,151.52,613.01,21.16,7.86">CLEF 2013 Evaluation Labs and Workshop, Online Working Notes</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">September 23-26 2013</date>
		</imprint>
	</monogr>
	<note>Cited on page 9</note>
</biblStruct>

<biblStruct coords="16,142.61,623.92,337.97,7.86;16,151.52,634.88,329.07,7.86;16,151.52,645.84,329.07,7.86;16,151.52,656.80,185.24,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="16,195.46,623.92,285.13,7.86;16,151.52,634.88,323.74,7.86">CNRS -TELECOM ParisTech at ImageCLEF 2013 Scalable Concept Image Annotation Task: Winning Annotations with Context Dependent SVMs</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sahbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,165.76,645.84,271.02,7.86">CLEF 2013 Evaluation Labs and Workshop, Online Working Notes</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">September 23-26 2013</date>
		</imprint>
	</monogr>
	<note>Cited on page 9</note>
</biblStruct>

<biblStruct coords="17,142.62,119.67,337.97,7.86;17,151.52,130.63,329.07,7.86;17,151.52,141.59,329.07,7.86;17,151.52,152.55,213.92,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="17,257.47,130.63,223.12,7.86;17,151.52,141.59,17.24,7.86">URJC&amp;UNED at ImageCLEF 2013 Photo Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sánchez-Oro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Montalvo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Montemayor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Pantrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Fresno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Martínez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,190.13,141.59,269.38,7.86">CLEF 2013 Evaluation Labs and Workshop, Online Working Notes</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">September 23-26 2013</date>
		</imprint>
	</monogr>
	<note>Cited on page 9</note>
</biblStruct>

<biblStruct coords="17,142.62,163.51,337.97,7.86;17,151.52,174.47,329.07,7.86;17,151.52,185.43,197.51,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="17,332.92,163.51,147.67,7.86;17,151.52,174.47,107.30,7.86">Evaluating Color Descriptors for Object and Scene Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,266.09,174.47,214.50,7.86;17,151.52,185.43,45.57,7.86">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1582" to="1596" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>Cited on page 6</note>
</biblStruct>

<biblStruct coords="17,142.62,196.39,337.98,7.86;17,151.52,207.34,329.07,7.86;17,151.52,219.66,35.85,6.12" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="17,256.10,196.39,224.50,7.86;17,151.52,207.34,93.78,7.86">Overview of the ImageCLEF 2012 Flickr Photo Annotation and Retrieval Task</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,267.00,207.34,104.13,7.86">CLEF 2012 working notes</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>Cited on page 1</note>
</biblStruct>

<biblStruct coords="17,142.62,229.26,337.98,7.86;17,151.52,240.22,329.07,7.86;17,151.52,251.18,326.00,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="17,310.72,229.26,169.87,7.86;17,151.52,240.22,198.57,7.86">80 Million Tiny Images: A Large Data Set for Nonparametric Object and Scene Recognition</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,357.57,240.22,123.02,7.86;17,151.52,251.18,140.28,7.86">Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1958" to="1970" />
			<date type="published" when="2008-11">nov 2008</date>
		</imprint>
	</monogr>
	<note>IEEE Transactions on. Cited on page 2</note>
</biblStruct>

<biblStruct coords="17,142.62,262.14,337.97,7.86;17,151.52,273.10,329.07,7.86;17,151.52,284.06,290.27,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="17,358.18,262.14,122.41,7.86;17,151.52,273.10,105.10,7.86">KDEVIR at ImageCLEF 2013 Image Annotation Subtask</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Uricchio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bertini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ballan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">D</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,276.63,273.10,203.96,7.86;17,151.52,284.06,58.30,7.86">CLEF 2013 Evaluation Labs and Workshop, Online Working Notes</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">September 23-26 2013</date>
		</imprint>
	</monogr>
	<note>Cited on page 9</note>
</biblStruct>

<biblStruct coords="17,142.62,295.02,337.98,7.86;17,151.52,305.98,329.07,7.86;17,151.52,316.93,329.07,7.86;17,151.52,327.89,229.89,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="17,259.48,295.02,221.11,7.86;17,151.52,305.98,53.91,7.86">Image-Text Dataset Generation for Image Annotation and Retrieval</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,355.91,305.98,124.68,7.86;17,151.52,316.93,145.24,7.86">II Congreso Español de Recuperación de Información, CERI 2012</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Berlanga</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Rosso</surname></persName>
		</editor>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">June 18-19 2012</date>
			<biblScope unit="page" from="115" to="120" />
		</imprint>
		<respStmt>
			<orgName>Universidad Politécnica de Valencia</orgName>
		</respStmt>
	</monogr>
	<note>Cited on page 5</note>
</biblStruct>

<biblStruct coords="17,142.62,338.85,337.98,7.86;17,151.52,349.81,329.07,7.86;17,151.52,360.77,329.07,7.86;17,151.52,371.73,146.90,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="17,257.71,338.85,222.89,7.86;17,151.52,349.81,65.14,7.86">Overview of the ImageCLEF 2012 Scalable Web Image Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,435.38,349.81,45.21,7.86;17,151.52,360.77,223.39,7.86">CLEF 2012 Evaluation Labs and Workshop, Online Working Notes</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Karlgren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Womser-Hacker</surname></persName>
		</editor>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">September 17-20 2012</date>
		</imprint>
	</monogr>
	<note>Cited on pages 2, 4, and 5</note>
</biblStruct>

<biblStruct coords="17,142.62,382.69,337.98,7.86;17,151.52,393.65,329.07,7.86;17,151.52,404.61,305.20,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="17,369.13,382.69,111.47,7.86;17,151.52,393.65,146.67,7.86">ARISTA -image search to annotation on billions of web photos</title>
		<author>
			<persName coords=""><forename type="first">X</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,305.68,393.65,170.71,7.86">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="page" from="2987" to="2994" />
		</imprint>
	</monogr>
	<note>Cited on page 2</note>
</biblStruct>

<biblStruct coords="17,142.62,415.56,337.98,7.86;17,151.52,426.52,329.07,7.86;17,151.52,438.84,21.68,6.12" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="17,295.13,415.56,185.46,7.86;17,151.52,426.52,138.64,7.86">Large scale image annotation: learning to rank with joint word-image embeddings</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,298.26,426.52,72.85,7.86">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="21" to="35" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>Cited on page 2</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
