<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,216.24,116.95,182.93,12.62;1,218.93,134.89,177.56,12.62">MIL at ImageCLEF 2013: Personal Photo Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,227.52,173.36,73.44,8.74"><forename type="first">Masaru</forename><surname>Mizuochi</surname></persName>
							<email>mizuochi@mi.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Machine Intelligence Laboratory</orgName>
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,308.98,173.36,74.41,8.74"><forename type="first">Takayuki</forename><surname>Higuchi</surname></persName>
							<email>higuchi@mi.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Machine Intelligence Laboratory</orgName>
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,230.15,185.32,56.93,8.74"><forename type="first">Chie</forename><surname>Kamada</surname></persName>
							<email>kamada@mi.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Machine Intelligence Laboratory</orgName>
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,315.39,185.32,69.84,8.74"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
							<email>harada@mi.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Machine Intelligence Laboratory</orgName>
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,216.24,116.95,182.93,12.62;1,218.93,134.89,177.56,12.62">MIL at ImageCLEF 2013: Personal Photo Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">121A1C2EB24B966AA412D1619008A4B2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we describe our methods for ImageCLEF 2013 Personal Photo Retrieval Task. We devote our attention to making our system efficient in retrieving documents which have the similar topic with few query data. We train a ranking function using rankSVM. We extract Fisher Vectors (FVs) from several local descriptors as visual features, and use Bag-of-Words (BoW) as metadata features. The final similarity score is the weighted average of the visual similarities and the metadata similarities, where the weights are determined by relevance feedback. Results have shown that our system achieves the best performance in the Personal Photo Retrieval Task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper describes our methods for the ImageCLEF 2013 Personal Photo Retrieval Task. Our objective is to investigate efficient methods for retrieving documents that have the similar topic with few query data. The dataset is obtained such that it imitates actual browsing data. It contains query by example(QBE) and browsing data, though some of them are unavailable. We compare three methods to measure a visual similarity between a document and a topic: SVM, rankSVM, and a similarity between a query and the nearest sample. We use the distance metric like radial basis function (RBF) kernel for a metadata similarity. We extract Fisher Vectors (FVs) as visual features, and use Bag-of-Words (BoW) as metadata features. The final similarity score is the weighted average of the visual similarities from several local descriptors and the metadata similarity, where the weights are determined by relevance feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Feature Extraction</head><p>In this section, we describe the features we use in the task. We extract FVs as visual features and BoW of EXIF tags as metadata features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Visual Features</head><p>Fisher Vectors. As a visual feature, we use Fisher Vectors (FVs) computed from four kinds of local descriptors: SIFT, C-SIFT, LBP, and GIST. Actually, GIST is usually used to describe a whole image, but we use it as a local descriptor. All local descriptors are reduced to 64 dimensions using Principal Component Analysis (PCA). Local descriptors are densely extracted from five scales of patches on a regular grid every six pixels and learn a Gaussian Mixture Model (GMM) with 256 components, which have a diagonal matrix as its covariance matrix. To use spatial information, we divide images into 1 × 1, 2 × 2, and 3 × 1 cells. Then FVs are calculated over each region as follows.</p><p>Let</p><formula xml:id="formula_0" coords="2,167.53,245.43,96.70,9.68">X = {x 1 , x 2 , • • • , x N }</formula><p>be a set of N local descriptors extracted from an image, and w i , µ i , Σ i be the mixture weight, mean vector, covariance matrix of the i-th Gaussian, respectively. Then we difine,</p><formula xml:id="formula_1" coords="2,173.64,287.50,268.08,65.04">u i = 1 N √ w i N ∑ n=1 γ n (i)Σ -1 2 i (x n -µ i ) , v i = 1 N √ 2w i N ∑ n=1 γ n (i) [ Σ -1 i diag((x n -µ i )(x n -µ i ) T ) -1 ] ,</formula><p>where 1 is a column vector whose components are all 1 and diag(X) for matrix X is a column vector which is composed of diagonal components of X. γ n (i) is the soft assignment of x n to i-th Gaussian as</p><formula xml:id="formula_2" coords="2,253.71,402.94,98.40,24.78">γ n (i) = w i u i (x n ) Σ K j=1 w j u j (x n )</formula><p>, where u i is the i-th Gaussian, and it is also known as the posterior probability. The FV representation is therefore given as</p><formula xml:id="formula_3" coords="2,250.37,460.95,114.62,19.36">G = [ u T 1 v T 1 . . . u T K v T K ] T ,</formula><p>where K is the number of GMM components. Following <ref type="bibr" coords="2,194.02,500.99,9.97,8.74" target="#b1">[2]</ref>, we apply power normalization and L2 normalization to each of the extracted FVs. Power normalization is done by applying the function,</p><formula xml:id="formula_4" coords="2,266.97,530.22,81.42,10.81">g(z) = sign(z)|z| a ,</formula><p>to each component of FVs, where a is a parameter and is set to 1/2 in this work. After normalization, we concatenate them into a single vector. The dimension of our FVs is 262144. In the following sections, we represent visual features as x v .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Metadata Features</head><p>As metadata features, we use Bag-of-Words (BoW) representation, which is based on the idea that each word in a text appears independently. BoW is obtained by counting the occurrence frequency of words in a text. In our method, each EXIF datum is represented as a component of the histogram separately. We use 10 Exif data in xml files as metadata features. Each feature and its dimension are shown in Table <ref type="table" coords="3,267.76,143.90,3.88,8.74" target="#tab_0">1</ref>. We represent metadata features as x m . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Retrieving Methods</head><p>This section describes retrieving methods that we applied in the task. The dataset I for the task is composed of 5,555 images with metadata. Therefore, each image i ∈ I has visual information and meta information. For a topic t, we represent the query by example(QBE) as q t , and browsing data as</p><formula xml:id="formula_5" coords="3,134.76,429.99,345.05,21.61">B t = {b 1 , b 2 , ...} ∈ I in which b i is browsed later than b j (i &gt; j).</formula><p>QBE is an image which is admitted as the similar image for the topic. Browsing data is obtained while seeking QBE.</p><p>A n-dimensional feature vector of image i is represented as x v i ∈ R n . In our approach, we use rankSVM to ascertain the similarity between visual features, and use the distance metric as the similarity between metadata features. Finally, we obtain a similarity score of the document by summing the weighted similarity scores (rankSVM score for visual features + similarity score for metadata features). We use relevance feedback to obtain the weights for each score. Relevance is based on the variance between QBE and browsing data. For a comparison, we also adopt Nearest Neighbor and Support Vector Machine for visual features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Nearest Neighbor</head><p>We first adopt simple Nearest Neighbor method. For a topic t, s v t,i is a visual similarity score of an image i, and calculated as:</p><formula xml:id="formula_6" coords="3,240.92,638.90,129.56,29.01">s v t,i = 1 1 + min x v ∈{qt}∪Bt d(x v , x v i )</formula><p>, where d(x v , x v i ) is a distance between x v and x v i , e.g. a Euclidean distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Support Vector Machine</head><p>Support Vector Machine (SVM) is a popular learning method that applies a maximum margin manner for binary classification. s v t,i is a similarity score of an image i for topic t, and calculated as:</p><formula xml:id="formula_7" coords="4,265.32,222.23,84.72,12.70">s v t,i = w t • x v i -h t .</formula><p>For topic t, to train a linear model (w t , h t ), SVM needs training data</p><formula xml:id="formula_8" coords="4,134.76,247.66,345.83,24.83">D t = {(x v i , y v,t i )} |I| i=1 .</formula><p>We choose a label of image i for topic t as:</p><formula xml:id="formula_9" coords="4,245.70,276.47,122.77,34.54">y v,t i = { 1 (i ∈ {q t } ∪ B t ) -1 (otherwise).</formula><p>Here, SVM is formulated as following optimization problem:</p><formula xml:id="formula_10" coords="4,201.92,349.28,209.86,61.68">min wt,{ξi} |D t | i=1 1 2 ||w t || 2 + C |Dt| ∑ i=1 ξ i , s.t. y v,t i (w t • x v i -h t ) ≥ 1 -ξ i (i = 1, • • • , |D t |), ξ i ≥ 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">RankSVM</head><p>To learn a model for each topic from the QBE, browsing data, and other images, we applied rankSVM proposed by T. Joachims <ref type="bibr" coords="4,331.97,470.29,13.72,8.74" target="#b0">[1]</ref> for search engine optimization. s v t,i is a similarity score of an image i for topic t, and calculate as:</p><formula xml:id="formula_11" coords="4,277.69,506.98,59.98,12.69">s v t,i = w t • x v i .</formula><p>RankSVM is a pairwise learning method that applies a large margin principle used in SVM for ranking learning. The QBE has the strongest presence for the topic in the given image set. The browsing data have stronger presence than images except the QBE and other browsing data. Later browsed images are regarded as higher ranked than earlier ones. P t is a pair set of topic t, and defined as:</p><formula xml:id="formula_12" coords="4,223.04,610.21,167.62,57.53">P t = O t 1 ∪ O t 2 ∪ O t 3 , O t 1 = {(i, j)|i = q t , j ∈ I -{q t }}, O t 2 = {(i, j)|i ∈ B t , j ∈ I -B t -{q t }}, O t 3 = {(b i , b j )|b i , b j ∈ B t , i &gt; j}.</formula><p>Here, rankSVM is formulated as following optimization problem: min wt,{ξi}</p><formula xml:id="formula_13" coords="5,215.57,141.09,182.56,60.08">|P t | i=1 1 2 ||w t || 2 + C |Pt| ∑ i=1 ξ i , s.t. w t • (x v j -x v k ) ≥ 1 -ξ j (∀(j, k) ∈ P t ), ξ i ≥ 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Distance Metric between Metadata Features</head><p>Similarity between metadata features is calculated by the distance metric of BoW feature vectors as:</p><formula xml:id="formula_14" coords="5,237.13,281.82,139.43,71.68">d(x m i , x m j ) = 1 -e -τ ||x m i -x m j || 2 , c i,j = 1 1 + d(x m i , x m j ) , s m t,i = ∑ k∈Bt∪qt c k,i ,</formula><p>where x m i and x m j are feature vectors i, j ∈ I. d(x m i , x m j ) is the distance between two metadata features. In this method, we use 1 as τ . Here, c i,j is a similarity between x m i and x m j . s m t,i is a similarity score of metadata of image i for topic t, and is obtained by summing the scores with all of the query browsed images. Then we normalize the scores to zero mean and unit variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Relevance Feedback</head><p>Relevance feedback is a technique of information retrieval that improves a user's query and facilitates retrieval of information that a user needs. In this task, which feature is important for retrieval varies by topics. Therefore, the system must recognize the criteria for retrieval automatically from the query such as QBE document and browsing data. We calculate the weights of each feature. The final similarity score is the weighted average of the visual similarities and the metadata similarity.</p><p>In this method, the weights are calculated utilizing the process of the creation of the query data. Each query image is browsed sequentially. The weights of features are updated by chosen images. For example, the feature of which variance in query images is small should be important. We expand the weight formula as:</p><formula xml:id="formula_15" coords="5,231.94,624.76,151.48,42.98">ω new l,t = σ I l σ Bt l , ω l,t = α × ω new l,t + (1 -α) × ω old l,t ,</formula><p>where α is the updating weight. ω l,t is the weight used in summing all of the similarity scores. σ I l is the variance of images about feature l. σ Bt l is the variance of images in B t . An initial value of ω l,t is 1, and as the element count of Q t increases by one, ω l,t is recalculated.</p><p>Finally, the score of image i is calculated by summing visual feature scores and metadata feature scores which are weighted with relevance feedback. The final score s f in t,i is calculated as:</p><formula xml:id="formula_16" coords="6,213.66,209.12,186.38,86.76">s f in t,i = ∑ l∈L v γ v l ω l,t s v l,t,i + ∑ l∈L m γ m l ω l,t s m l,t,i , γ v l = λ v l N v l , γ m l = λ m l N m l ,</formula><p>where s v l,t,i and s m l,t,i are the visual and metadata scores of the image i for topic t. L v and L m are all of the visual and metadata features respectively. N v l and N m l represent the number of combinations of the visual features and the metadata features respectively. In this paper, we set N m l = 10. N v l is 1, 2, 3, and 4. λ v l and λ m l are the parameters to change ratio of the weight between visual and metadata features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>This section describes details of the comparison of learning methods and the result of visual feature's combinations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Retrieving method comparison</head><p>In our experiment, four kinds of local descriptors were extracted from each image: SIFT, C-SIFT, LBP, and GIST. Each descriptor was sampled densely on a regular grids (every six pixels). The dimensionalities of SIFT, C-SIFT, LBP, and GIST were 128, 384, 1024, and 960 respectively. They were coded into two state-of-the-art global feature representations (4×2 = 8 visual features in total). One is FV, as explained in the previous section. First, the mixture model of 256 Gaussians was trained using a standard EM-algorithm. To use spatial information, FVs were calculated respectively over 1 × 1, 2 × 2, and 3 × 1 cells. Thereby, we obtained FVs of which the dimensionality was 64 × 256 × 8 × 2 = 262, 144. The other is Locality-constrained Linear Coding (LLC) <ref type="bibr" coords="6,375.64,597.34,9.97,8.74" target="#b2">[3]</ref>, which describes each local descriptor as a linear weighted sum of a few nearest codewords. In our experiment, 1,024 codewords were generated with the k-means algorithm, and then each local descriptor was approximated using 5-NN of the descriptor. The images were divided into 1 × 1, 1 × 3 and 3 × 1 spatial grids differently from FV. Therefore, the dimensionality was 1024 × 7 = 7168.</p><p>In this experiment, we examined which learning method is effective to train a ranking function for the task using only visual features. We investigated which visual feature should be combined to achieve the best performance through the next experiment. We use NN, SVM, and rankSVM to train the ranking function. Therefore, we examine 8 × 3 = 24 experiments in total. We use trec eval system published for evaluation. We use a ndcg cut 100 value in the evaluation of average users. The ndcg cut 100 value indicates the rate of compliance with the top 100 documents of correct data. Table <ref type="table" coords="7,177.81,389.12,4.98,8.74" target="#tab_1">2</ref> shows the trec eval result of 12 runs. Results show that rankSVM achieves superior performance among the learning methods we use. Therefore, we apply only rankSVM for next experiment of the feature combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Feature combination</head><p>This experiment addresses a combination of visual features and metadata features. First, we evaluated only metadata features for retrieval, then we got a score of 0.6228. With only visual features, the best score is 0.3861 using FV, LBP, and rankSVM.</p><p>The combination formula is given as:</p><formula xml:id="formula_17" coords="7,213.66,524.56,186.38,86.77">s f in t,i = ∑ l∈L v γ v l ω l,t s v l,t,i + ∑ l∈L m γ m l ω l,t s m l,t,i , γ v l = λ v l N v l , γ m l = λ m l N m l ,</formula><p>where λ v l and λ m l are the parameters to change ratio of the weight between visual and metadata features. We use λ v l = 3 and λ m l = 1. Finally, we present the top six combinations of four visual features in two circumstances where 10 metadata features are used and not used respectively. We</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,244.98,174.83,122.33,140.49"><head>Table 1 .</head><label>1</label><figDesc>Metadata Features</figDesc><table coords="3,244.98,193.89,122.33,121.43"><row><cell cols="2">EXIF data name dimension</cell></row><row><cell>Make</cell><cell>20</cell></row><row><cell>Model</cell><cell>38</cell></row><row><cell>Flash</cell><cell>13</cell></row><row><cell>SceneCaptureType</cell><cell>4</cell></row><row><cell>DateTime</cell><cell>10</cell></row><row><cell>GPS Altitude</cell><cell>41</cell></row><row><cell>GPS Latitude Ref</cell><cell>2</cell></row><row><cell>GPS Latitude</cell><cell>143</cell></row><row><cell>GPS Longitude Ref</cell><cell>4</cell></row><row><cell>GPS Longitude</cell><cell>151</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,194.29,235.28,225.38,116.33"><head>Table 2 .</head><label>2</label><figDesc>Retrieval method comparison.</figDesc><table coords="7,194.29,255.68,225.38,95.93"><row><cell></cell><cell>NN</cell><cell>SVM</cell><cell>rankSVM</cell></row><row><cell>LLC SIFT</cell><cell>0.2946</cell><cell>0.3066</cell><cell>0.3308</cell></row><row><cell>LLC C-SIFT</cell><cell>0.2856</cell><cell>0.2967</cell><cell>0.3257</cell></row><row><cell>LLC LBP</cell><cell>0.3043</cell><cell>0.3199</cell><cell>0.3385</cell></row><row><cell>LLC GIST</cell><cell>0.2796</cell><cell>0.2943</cell><cell>0.3175</cell></row><row><cell>FV SIFT</cell><cell>0.3135</cell><cell>0.3278</cell><cell>0.3357</cell></row><row><cell>FV C-SIFT</cell><cell>0.3492</cell><cell>0.3486</cell><cell>0.3696</cell></row><row><cell>FV LBP</cell><cell>0.3636</cell><cell>0.3363</cell><cell>0.3861</cell></row><row><cell>FV GIST</cell><cell>0.3376</cell><cell>0.3145</cell><cell>0.3572</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>obtained the scores shown in Table <ref type="table" coords="8,292.11,362.50,4.98,8.74">3</ref> and<ref type="table" coords="8,320.14,362.50,31.45,8.74">Table 4</ref>. Without metadata features, the combination of three descriptors (C-SIFT, LBP, and GIST) got the best score among them. Additionally when metadata features were combined, the combination of all of the features (SIFT, C-SIFT, LBP, GIST, and Metadata) got the best score. We could not use this evaluation system before the submission. Therefore, we submitted the score of the experiment which uses four features (C-SIFT, LBP, GIST, and Metadata) we considered the best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>This paper explained our approach for ImageCLEF 2013 Personal Photo Retrieval Task. Results of comparative experiments show FVs as useful for visual representation and BoWs as metadata features. Retrieval is performed using rankSVM with visual features and the distance metric between metadata features. Using combinations of FVs from various local descriptors (C-SIFT, LBP, and GIST) and metadata features, we achieved the top score among all teams.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="8,138.34,595.02,342.17,7.86;8,146.91,605.98,119.35,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,211.20,595.02,219.26,7.86">Optimizing search engines using clickthrough data</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,458.75,595.02,21.77,7.86;8,146.91,605.98,39.47,7.86">ACM SIGKDD</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.34,616.34,342.14,7.86;8,146.91,627.29,206.00,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,312.22,616.34,168.26,7.86;8,146.91,627.29,76.39,7.86">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,244.00,627.29,23.23,7.86">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="143" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.34,637.65,342.14,7.86;8,146.91,648.61,258.88,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,373.50,637.65,106.98,7.86;8,146.91,648.61,119.92,7.86">Locality-constrained linear coding for image classification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,287.53,648.61,29.17,7.86">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3360" to="3367" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
