<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,129.00,152.87,337.19,12.58;1,222.18,170.87,150.86,12.58">A multimedia IR-based system for the Photo Annotation Task at ImageCLEF2013</title>
				<funder ref="#_agVdAGu">
					<orgName type="full">Regional Government of Madrid under Research Network MA2VIRMR</orgName>
				</funder>
				<funder ref="#_rQ6kTNg #_49dY78v">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,129.54,208.21,50.08,11.09"><forename type="first">X</forename><surname>Benavent</surname></persName>
							<email>xaro.benavent@uv.es</email>
							<affiliation key="aff1">
								<orgName type="institution">Universitat de Valéncia</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,187.78,208.21,58.44,11.09"><forename type="first">A</forename><surname>Castellanos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universidad Nacional de Educación a Distancia</orgName>
								<orgName type="institution" key="instit2">UNED</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,254.40,208.21,38.65,11.09"><forename type="first">E</forename><surname>De Ves</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Universitat de Valéncia</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,301.20,208.21,88.32,11.09"><forename type="first">D</forename><surname>Hernández-Aranda</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universidad Nacional de Educación a Distancia</orgName>
								<orgName type="institution" key="instit2">UNED</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,397.78,208.21,50.07,11.09"><forename type="first">R</forename><surname>Granados</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universidad Nacional de Educación a Distancia</orgName>
								<orgName type="institution" key="instit2">UNED</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,456.00,208.21,4.85,11.09;1,265.50,220.15,61.11,11.09"><forename type="first">A</forename><surname>Garcia-Serrano</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universidad Nacional de Educación a Distancia</orgName>
								<orgName type="institution" key="instit2">UNED</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,129.00,152.87,337.19,12.58;1,222.18,170.87,150.86,12.58">A multimedia IR-based system for the Photo Annotation Task at ImageCLEF2013</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">25F013ECD5F3282F1E37D1AF71C51A6E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Text-Based Information Retrieval</term>
					<term>Content-Based Information Retrieval</term>
					<term>Multimedia Fusion</term>
					<term>Logistic regression relevance algorithm</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The UNED-UV group at the ImageCLEF2013 Campaign have participated in the Scalable Concept Image Annotation subtask. We present a multimedia IR-based system for the annotation task. In this collection, the images do not have any textual description associated, so we have downloaded and preprocessed the web pages which contain the images. Regarding the concepts, we expanded their textual description with additional information from external resources as Wikipedia or WordNet and we generate a KLD concept model using recovered textual information. The multimedia IR-based system uses a logistic relevance algorithm to get a model for each of the concepts to be trained using visual image features. Finally, the fusion subsystem merges textual and visual scores for a certain image to belong a concept, and decides the presence of the concept in the images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The UNED-UV is a research group with researchers from two universities in Spain, the Universidad Nacional de Educación a Distancia (UNED) and the Valencia University (UV). The group is working together since ImageCLEF08 edition. At this 2013 ImageCLEF Campaign <ref type="bibr" coords="1,290.89,562.21,10.63,11.09" target="#b1">[2]</ref>, we participate at the Photo Annotation and Retrieval Task <ref type="bibr" coords="1,204.47,574.21,11.70,11.09" target="#b5">[6]</ref> in the Scalable Concept Image Annotation subtask. The motivation for this edition is focused on the development of image annotation systems that address the scalability problem in such a way that the annotation systems have to be able to adapt their behavior to take into account new concepts that can appear in the images to be annotated. There were two datasets to evaluate the systems, one containing the same concepts used for training (95 concepts) and a second one containing these 95 concepts and 21 additional ones.</p><p>As the classification-based systems, traditionally used for image annotation, are not suitable for this task, we use a multimedia IR-based annotation methodology that produces a concept model that predicts the probability that a certain concept belongs to an image. A merging algorithm fuses textual and visual probabilities, and this final score is used to decide the presence of a certain concept in an image.</p><p>Section 2 describes the system overview and the annotation methodology used for the two approaches submitted using the textual and the multimodal information. After that, section 3 shows the submitted runs and section 4 analyze the results obtained. Finally, in section 5 we extract conclusions and outlines possible future research lines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Description</head><p>The global system (shown at Fig. <ref type="figure" coords="2,270.27,265.21,4.16,11.09" target="#fig_0">1</ref>) is divided into three main subsystems: TBIR (Text-Based Image Retrieval), CBIR (Content-Based Image Retrieval) and the Merging module. The TBIR subsystem is in charge of annotating the images using only textual information selected from the web pages where they were downloaded.</p><p>As the list of concepts does not include example images to train each one of the concepts, the TBIR subsystem is in charge of generating a training set of images for each concept for the Multimedia approaches. These images are taken from the so called 3k collection images (Devel + Test). The CBIR subsystem generates a model for each of the concepts with the generated training set images. These concept models are used to generate the lists of relevant images for each concept. Finally, these lists are combined with the fusion subsystem following a late fusion approach based on the OWA operator <ref type="bibr" coords="2,186.90,397.21,10.64,11.09" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Annotation using textual information</head><p>TBIR subsystem is in charge of the textual annotation of images in the collection. In this collection, the images do not have any textual description associated, so the first step is to obtain the textual information for describing them. For this task, we have downloaded and pre-processed the web pages which contain the images. Regarding the concepts, we expanded their textual description with additional information from external resources as Wikipedia or WordNet.</p><p>As an image may be annotated by several concepts, the annotation strategy is based on an information retrieval approach, which indexes the concepts and uses each image as query. The result of the retrieval process is a ranked concepts list for each image, ordered by the textual similarity or score ( ).</p><p>The modules of the TBIR subsystem have the following functionalities:  Image Expansion. This module is in charge of downloading the web pages that contain the images in the collection. Then, we extract the textual information directly related to each image taking into account the text contained in the following HTML attributes: "title" and "alt" of &lt;img&gt; tag; and the &lt;a&gt; tag if the image is within a link. An image may be contained in several web pages, so we recover the textual information of every web page. Moreover, we take into account the image name that appears in its URL.</p><p>Concept Expansion. In order to obtain additional information to describe the concepts, we use Wikipedia and WordNet as external resources, in the following way:  Wikipedia. We extract the textual information from fields &lt;text&gt; and &lt;categories&gt; contained in the corresponding Wikipedia pages of the concepts.  WordNet. Lexical and semantic information about the concepts is extracted: definition, synonyms, hypernyms, hyponyms and related concepts.</p><p>Additionally, we have modelled the raw text obtained from the Wikipedia concepts description to identify a list with the most representative terms (so-called the Wikipedia-KLD list). For this, we applied a divergence-based approach (Kullback Leibler Divergence or KLD <ref type="bibr" coords="3,209.65,593.65,11.27,11.09" target="#b3">[4]</ref>) to identify not only the representative terminology but also the terminology that better differentiate each concept from the rest. KLD weights each term according to their occurrence in a given content and their occurrence in the rest of the contents following the formulation in (1):</p><formula xml:id="formula_0" coords="3,256.01,651.92,214.54,13.12">, • ln<label>(1)</label></formula><p>where is the probability of each term within a document (frequency of divided by the whole of terms in the document ) and is the probability of the same term within the collection (frequency of divided by the number of terms in the collection ).</p><p>Pre-processing. Textual information is pre-processed: 1) deletion of characters with no statistical meaning, like punctuation marks or blanks; 2) deletion of semantic empty words in English language (stopwords), 3) reduction of words to their base form by stemming, and 4) conversion of all words into lower case.</p><p>Indexing. The indexing process is carried out using Lucene. The images are indexed using only one field with the text associated to each image. The concepts are indexed using three fields, depending on the information used for expansion: Wikipedia, WordNET and Wikipedia_KLD.</p><p>Searching. This module is in charge of launching the queries against a concrete index in order to obtain the corresponding textual results (Txt Results). When using images as queries, a concepts list will be obtained; and when the queries are the concepts, an images list will be generated. The latter is used to fuse these textual results with the visual ones obtained from the CBIR subsystem. The applied ranking function is BM25 and its extension for structured documents BM25F <ref type="bibr" coords="4,382.40,352.21,10.61,11.09" target="#b4">[5]</ref>, using the default parameters.</p><p>For the concept description several approaches were tested. Finally, we have represented (and indexed) each concept by: Several experiments were performed in order to compare the use of the previous alternatives for the textual description of the images when using as queries to retrieve concepts from the concepts index of the Devel collection. The best result was obtained using only the img field as a query; so this is the field to be used in the runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Annotation using Multimedia information</head><p>For the Multimedia approaches, the TBIR subsystem generates a training set for each concept to be annotated. These images are taken from the 3k collection images (Devel + Test). The CBIR subsystem generates a model or predictor for each of the concepts with the Logistic Regression Relevance Model algorithm <ref type="bibr" coords="5,371.50,204.19,10.64,11.09" target="#b2">[3]</ref>. Once, the concepts models are trained, these models are used to predict the probability that a given image belongs to a certain concept (Si). Both probabilities (St,Si) are combined by the Fusion subsystem that finally decides if a certain concept is present or not to be annotated.</p><p>For the Logistic Regression Relevance algorithm, each of the concept models needs two sets to be trained: a set of images that have the concept, being the relevant or positive images, and a set of images that not belongs to a concept, being , the set of non-relevant images or negative images. Each image is represented by a Kdimensional low-level features vector , . . , , . . , . The relevance probability for a certain concept for a given image will be represented as . A logistic regression model can estimate these probabilities. Let us consider for a binary Y, and k explanatory variables , … , , the model for π(x) = P(Y=1| X ) (probability Y=1) for the x values ⋯ , where logit (π(x))=ln(π(x) / (1-π(x)). The model parameters are obtained by maximizing the likelihood estimator (MLE) of the parameter vector β by using an iterative method.</p><p>The positive o relevant images (I set), is given by the Text-Based Information sub-system using the 3k collection images. This initial list is tailored up to the tenth top images, and the final selection is human supervised. The non-relevant images, the I set, is selected from the images that do not have the required concept and this list is also tailored up to the twentieth top images, being the selection also human supervised. A good selection of the images that represent a certain concept is very important to make the estimator good and robust. For this reason, we have considered important the human supervision for the training sets. Furthermore, these sets are generated only once for training, and could be used to annotate any other collection with these concepts.</p><p>The explanatory variables x x , … , x to train the model are the visual lowlevel features based on colour and texture information that are given by the organization <ref type="bibr" coords="5,143.54,543.25,10.82,11.09" target="#b5">[6]</ref>: colour histograms and GIFT shape descriptor that describes the shape in an image by calculating the Gabor transform. We have a low-level features vector of 544 components: 64 for the colour histograms and 480 for the GIFT descriptor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multimedia Fusion</head><p>To merge the textual and the visual information, we have followed a late fusion approach which combines the two monomodal results lists at decision level. The applied late fusion algorithm tested in previous works <ref type="bibr" coords="5,318.02,641.29,11.69,11.09" target="#b0">[1]</ref> is based on the Mathematical aggregation operator OWA <ref type="bibr" coords="5,230.22,653.29,10.61,11.09" target="#b6">[7]</ref>. The OWA transforms a finite number of inputs into a single output without associating weights to any particular input; instead, the relative magnitude of the inputs decides which weight corresponds to each input. In our appli-cation, the inputs are the textual and image scores ( and ), and this property is very interesting because we do not know, a priori, which subsystem will provide us the best information. The aggregation weights used for our experiments correspond to an 0.3, which means that a weight of 0.3 is given to the higher probability value and a weight of 0.7 to the lower one.</p><p>Once the final fused list is obtained (containing, for each image, a ranked list of associated concepts), we have to decide how many concepts will be used to annotate each image. We consider two options: 1) select a fixed number of concepts; and 2) calculate a relevance threshold that decides whether or not an image is annotated by a concept. Both options have been evaluated with the Devel collection, since it has ground truth.</p><p>Relative to the first option, Fig. <ref type="figure" coords="6,276.31,280.21,5.01,11.09">2</ref> shows the evolution of evaluation measures MAP, mFsamp (by image) and mFcnpt (by concept) depending on the number of annotated concepts (values between 1 and 20). We can see how the MAP value is higher as the number of concepts is increased, while the value of the rest of measures decreases, being the cut-point between 5 and 6. On the other hand, the mean number of concept annotation per image is 6.345 (calculated from Devel ground truth), so, taking into account both factors, we decide to select 7 as fixed number of concepts to annotate images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2. Evolution of the evaluation measures depending on number of concepts</head><p>For the second option, the threshold calculation is based on the percentage of the maximum score by image. This option is more flexible, since an image is not annotated with a fixed number of concepts. Fig. <ref type="figure" coords="6,302.99,610.87,5.01,11.09" target="#fig_2">3</ref> shows the evolution of the evaluation measures considering the percentages from 10% to 100%. We can see that the more restrictive is the threshold (low values of percentage), the more MAP value increases and the rest of measures decrease.</p><p>The cut-off point is between 70 and 80%. If the number of concepts with which an image is annotated is considered, for 70% the mean is 6.622 while for 80% is 3.356. Therefore, we have selected the percentage of 70% as a threshold, since its mean concept number is similar to the mean calculated from Devel ground truth (6.345). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we present the two approaches developed for the image annotation subtask: the monomodal approach (TBIR-based annotation) and the multimodal approach (TBIR and CBIR based annotation). The five runs submitted are:</p><p> UNEDUV_1: Monomodal Approach. Query with the img field against the KLD_WP concept representation indexed.  UNEDUV_2: Monomodal approach. Query with the img field against the KLD_WP+WN concept representation indexed.  UNEDUV_3: Monomodal approach. Query with the img field against the WN concept representation indexed.  UNEDUV_4: Multimodal approach. UNEDUV_2 textual run is merged with the visual run by OWA algorithm, using the seventh most representative concepts to annotate every image.  UNEDUV_5: Multimodal approach. UNEDUV_2 textual run is merged with the visual run by OWA algorithm, using the concepts with 70% of representative concept score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">UNED-UV Results</head><p>In this section, we describe the results obtained with the submitted runs using the development and the test collections, measured according the mean F-measure for both the samples (MF-samples) and the concepts (MF-concepts); and the mean average precision for the samples (MAP-samples). The test set results is also evaluated with a fourth measure (MF-unseen), the mean F-measure for the concepts that are not in the development set. The values between the square brackets correspond to the 95% confidence intervals computed using Wilson's method. Table <ref type="table" coords="8,394.11,172.21,5.01,11.09" target="#tab_1">1</ref> shows the results for the development set, divided into textual and multimodal runs, and Table <ref type="table" coords="8,437.82,184.21,5.01,11.09" target="#tab_2">2</ref> shows the same results for the test set. The best obtained value for each of the measures it is also included in the table, together with the average over all the presented experiments by the rest of participants.</p><p>All our submitted runs are beyond the baseline results for both the development set and for the test set according to all measures (see the overall results at the Im-ageCLEF webpage <ref type="bibr" coords="8,206.66,256.21,10.51,11.09" target="#b5">[6]</ref>). Looking into the overall participant's results list, our best runs are at positions 21, 16 and 27 ordered by the MF-Samples, MF-Concepts and MAP-samples respectively for the development set, and at positions 24, 11 and 26 for the test set. It means that our best runs are at the first third top results.</p><p>Focusing on textual runs, UNEDUV_1 and 2 offer similar results according to all the measures; however, UNEDUV_3 results, which are based only in WordNET annotation, differs: values based on sample results are fewer than values of the other two approaches, and the value based on concepts (MF-concepts) improves these results (4-5 points higher). This behavior is observed in the development and in the test set.</p><p>On the other hand, the multimodal-based approach, UNEDUV_4 and UNEDUV_5 runs, improve the textual based run, the UNEDUV_2, in almost all measures, being this improvement higher in the UNEDUV_5. This means that the merging strategy of using a relevance threshold to decide if a concept should be or not annotated performs better than the one that uses a static number of concepts to be annotated. Similar behavior is observed for the two sets, development and test. An important issue to highlight for the test set results at Table <ref type="table" coords="8,392.09,619.21,5.01,11.09" target="#tab_2">2</ref> is the MF-unseen values. In general all our submitted approaches offer satisfactory values; especially the WordNET-based approach (UNEDUV_3) value, that is the 3rd best overall value, and the Multimedia approach (UNEDUV_5) at the 5th position. It highlights a good generalization capacity for our systems to annotate unseen concepts. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Concluding Remarks</head><p>Our best runs are at the first third top results for the different measurements. This means that the multimedia IR-based system presented has obtained quite good results regarding the current state of the art.</p><p>For the textual approaches, the WordNET-based approach for concept expansion is the one that has a better performance. As this textual baseline was not the one used for the submitted multimodal approaches, it is going to be tested in the multimodal IRbased system.</p><p>The multimedia approaches slightly outperform its textual baseline, although not in all measurements, being this behaviour needed to be further analysed. The fusion subsystem has proved that a relevance threshold to decide the annotation of a concept achieves better results than selecting a fixed number of concepts per image. It is important to highlight the good generalization capacity of our system to annotate unseen concepts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,253.68,356.24,90.30,9.97"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. System Overview</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,124.69,407.23,153.59,12.26;4,124.69,419.47,345.99,12.26;4,136.03,432.61,146.41,11.09;4,124.69,443.65,346.05,12.26;4,136.03,456.85,334.62,11.09;4,136.03,468.85,292.69,11.09;4,124.69,479.89,345.96,12.26;4,136.03,493.03,334.47,11.09;4,136.03,505.03,131.56,11.09;4,124.69,529.03,329.96,11.09;4,124.69,540.07,346.00,12.26;4,136.03,555.36,88.69,9.02;4,124.69,564.31,345.91,12.26;4,136.03,577.51,256.07,11.10;4,124.69,588.55,208.74,12.26;4,124.69,600.73,277.33,12.26;4,124.69,612.97,235.45,12.26"><head></head><label></label><figDesc>Concept: The name of the concept.  WP_Description: Contains the raw text of the concept Wikipedia page, plus the Wikipedia categories of the concept.  WP_KLD_Description: As we have modelled each concept using the raw Wikipedia text and the Wikipedia categories (WP_Desccription), the 50 most representative terms, according KLD weighting, are indexed for each concept.  WN_Description: The textual information obtained for the concept at WordNet is the one included at the &lt;definition&gt;, &lt;forms&gt;, &lt;hypernyms&gt;,&lt;hyponyms&gt; and &lt;related&gt; WordNet components. The different textual information describing the images is (store in the five fields):  Img: The textual description initially associated to the image: img_title, img_alt, img_link e img_name.  Webpage: Includes the general description about the webpage containing the image: webpage_title, webpage_description y webpage_keywords.  img+webpage: The two previous fields together.  text: The whole webpage text (text element) containing the image.  img+webpage+text: The three previous fields together.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,154.80,352.22,285.75,9.97;7,144.00,182.76,307.20,156.84"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Evolution of the evaluation measures depending on relevance threshold.</figDesc><graphic coords="7,144.00,182.76,307.20,156.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="6,150.36,380.76,294.84,170.88"><head></head><label></label><figDesc></figDesc><graphic coords="6,150.36,380.76,294.84,170.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,130.92,448.22,330.66,155.46"><head>Table 1 .</head><label>1</label><figDesc>Development Set Results</figDesc><table coords="8,130.92,471.15,330.66,132.53"><row><cell>Run</cell><cell>Mode</cell><cell>MFsamples</cell><cell>MFconcepts</cell><cell>MAPsamples</cell></row><row><cell>UNEDUV_1</cell><cell>Textual</cell><cell>25.0 [23.8-26.4]</cell><cell>27.5 [24.5-32.2]</cell><cell>32.8 [31.4-34.4]</cell></row><row><cell>UNEDUV_2</cell><cell>Textual</cell><cell>24.4 [23.2-25.7]</cell><cell>26.1 [23.6-30.4]</cell><cell>32.4 [31.0-33.9]</cell></row><row><cell>UNEDUV_3</cell><cell>Textual</cell><cell>22.5 [21.2-23.9]</cell><cell>31.5 [27.9-36.5]</cell><cell>27.1 [25.9-28.5]</cell></row><row><cell>UNEDUV_4</cell><cell>Multimodal</cell><cell>29.9 [28.7-31.3]</cell><cell>26.3 [23.7-30.7]</cell><cell>31.0 [29.8-32.3]</cell></row><row><cell>UNEDUV_5</cell><cell>Multimodal</cell><cell>27.6 [26.8-28.6]</cell><cell>31.7 [28.3-36.5]</cell><cell>35.5 [34.1-36.9]</cell></row><row><cell>Best</cell><cell>-</cell><cell>51.3</cell><cell>45.0</cell><cell>50.4</cell></row><row><cell>Average</cell><cell>-</cell><cell>27.1</cell><cell>24.7</cell><cell>34.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,126.84,148.22,343.44,154.14"><head>Table 2 .</head><label>2</label><figDesc>Test set Results [22.1-23.9] 25.0 [22.8-28.7] 31.7 [25.0-44.7] 30.3 [29.3-31.4] UNEDUV_2 Textual 22.9 [22.0-23.8] 24.0 [22.3-27.5] 30.6 [24.7-43.1] 30.6 [29.6-31.7] UNEDUV_3 Textual 23.1 [22.0-24.2] 31.3 [28.1-35.8] 43.2 [33.1-55.7] 26.6 [25.6-27.7] [23.8-25.1] 29.2 [26.7-33.1] 35.4 [27.7-48.2] 33.2 [32.2-34.3]</figDesc><table coords="9,126.84,167.43,343.44,134.93"><row><cell>Run</cell><cell>Mode</cell><cell>MF-samples</cell><cell>MF-concepts</cell><cell>MF-unseen</cell><cell>MAP-samples</cell></row><row><cell cols="6">UNEDUV_1 Textual 23.0 UNEDUV_4 Multi-30.0 [29.0-31.1] 22.8 [20.9-26.5] 24.6 [19.5-38.5] 29.8 [28.9-30.9] modal UNEDUV_5 Multi-modal 24.4 Best -42.6 34.1 45.3 45.6</cell></row><row><cell>Average</cell><cell>-</cell><cell>23.7</cell><cell>21.7</cell><cell>22.1</cell><cell>30.69</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This work has been partially supported for <rs type="funder">Regional Government of Madrid under Research Network MA2VIRMR</rs> (<rs type="grantNumber">S2009/TIC-1542</rs>), <rs type="projectName">HOLOPEDIA</rs> (<rs type="grantNumber">TIN 2010-21128-C02</rs>) and by project <rs type="grantNumber">MCYT TEC2009-12980</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_agVdAGu">
					<idno type="grant-number">S2009/TIC-1542</idno>
					<orgName type="project" subtype="full">HOLOPEDIA</orgName>
				</org>
				<org type="funding" xml:id="_rQ6kTNg">
					<idno type="grant-number">TIN 2010-21128-C02</idno>
				</org>
				<org type="funding" xml:id="_49dY78v">
					<idno type="grant-number">MCYT TEC2009-12980</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,132.68,597.38,338.03,9.96;9,141.72,608.36,328.90,9.96;9,141.72,619.34,22.00,9.96;9,179.10,619.34,22.48,9.96;9,216.91,619.34,39.74,9.96;9,271.99,619.34,19.53,9.96;9,306.83,619.34,45.99,9.96;9,368.19,619.34,9.01,9.96;9,392.57,619.34,44.27,9.96;9,452.21,619.34,18.53,9.96;9,141.72,630.38,109.50,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,414.73,597.38,55.97,9.96;9,141.72,608.36,328.90,9.96;9,141.72,619.34,22.00,9.96;9,179.10,619.34,22.48,9.96;9,216.91,619.34,36.13,9.96">Multimedia Information Retrieval based on Late Semantic Fusion Approaches: Experiments on a Wikipedia Image Collection</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Benavent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García-Serrano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Granados</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Benavent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Ves</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMM.2013.2267726</idno>
	</analytic>
	<monogr>
		<title level="j" coord="9,271.99,619.34,19.53,9.96;9,306.83,619.34,45.99,9.96;9,368.19,619.34,9.01,9.96;9,392.57,619.34,40.25,9.96">IEEE Transactions on Multimedia</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.68,641.36,338.04,9.96;9,141.72,652.34,329.01,9.96;9,141.72,663.38,233.81,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,447.10,652.34,23.63,9.96;9,141.72,663.38,134.86,9.96">the vision, the data and the open challenges</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zellhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">Martinez</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Varea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cazorla</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Imageclef</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,282.50,663.38,43.00,9.96">Proc. CLEF</title>
		<meeting>CLEF</meeting>
		<imprint>
			<publisher>LNCS</publisher>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.68,148.22,337.95,9.96;10,141.72,159.20,320.02,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,357.36,148.22,113.27,9.96;10,141.72,159.20,164.08,9.96">Applying logistic regression to relevance feedback in image retrieval systems</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zuccarello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ayala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>De Ves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Domingo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,311.92,159.20,71.65,9.96">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">2621</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.68,170.18,338.10,9.96;10,141.72,181.22,61.26,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,239.47,170.18,112.52,9.96">On information and sufficiency</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kullback</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A</forename><surname>Leibler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,357.95,170.18,112.83,9.96;10,141.72,181.22,11.79,9.96">Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1951">1951</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.68,192.20,338.02,9.96;10,141.72,203.18,328.95,9.96;10,141.72,214.22,258.89,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,244.80,192.20,225.90,9.96;10,141.72,203.18,126.19,9.96">Some simple effective approximations to the 2-Poisson model for probabilistic weighted retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,284.41,203.18,107.21,9.96">Proceedings of the SIGIR &apos;94</title>
		<editor>
			<persName><forename type="first">W</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</editor>
		<meeting>the SIGIR &apos;94<address><addrLine>NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="232" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.68,225.20,337.96,9.96;10,141.72,236.18,282.51,9.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,278.25,225.20,192.39,9.96;10,141.72,236.18,94.42,9.96">Overview of the ImageCLEF 2013 Scalable Concept Image Annotation Subtask</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,242.26,236.18,94.43,9.96">CLEF 2013 working notes</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.68,247.22,338.08,9.96;10,141.72,258.20,300.76,9.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,180.13,247.22,290.63,9.96;10,141.72,258.20,25.09,9.96">On ordered weighted averaging aggregation operators in multi criteria decision making</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Yager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,173.28,258.20,179.30,9.96">IEEE Transactions Systems Man and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="183" to="190" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
