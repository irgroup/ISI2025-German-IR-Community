<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,137.64,115.96,340.07,12.62;1,243.56,133.89,128.23,12.62">CEA LIST@imageCLEF 2013: Scalable Concept Image Annotation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,191.03,174.84,71.63,8.74"><forename type="first">Hervé</forename><surname>Le Borgne</surname></persName>
						</author>
						<author>
							<persName coords="1,270.79,174.84,67.10,8.74"><forename type="first">Adrian</forename><surname>Popescu</surname></persName>
						</author>
						<author>
							<persName coords="1,365.38,174.84,58.94,8.74"><forename type="first">Amel</forename><surname>Znaidia</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CEA</orgName>
								<orgName type="institution" key="instit2">LIST</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Vision &amp; Content Engineering Gif-sur-Yvettes</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,137.64,115.96,340.07,12.62;1,243.56,133.89,128.23,12.62">CEA LIST@imageCLEF 2013: Scalable Concept Image Annotation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C14BE78D0B295F262292897BAEC95FBD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We report the participation of the CEA LIST to the Scalable Concept Image Annotation Subtask of ImageCLEF 2013. The full system is based on both textual and visual similarity to each concept, that are merged by late fusion. Each image is visually represented with a bag of visterm, computed from a dense grid of SIFT every 3 pixels, that a locally soft coded and max pooled on a codebook of size 1024 and spatially extended with a pyramid 1 × 1 + 3 × 1 + 2 × 2, resulting into a vector of size 8192. The visual neighbors of a query are given by the L1 distance to the images of the learning database. The similarity of a query to one of the 95/116 concepts to identify is the sum of the similarity of each neighbor to the concept. The decision is set at 1 for all concepts above one standard deviation from the average similarity to all concepts for the considered query.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The similarity between a training image and a concept is computed from an intermediate vectorial representation of its tags. We tested several spaces of representation for the tags, including wikipedia concepts sorted according to their popularity or characterized according FlickR data. As well, the size of space was pruned at several values, from 5000 to 200, 000. The tag representation are max-pooled to make the training image vector, such that the resulting vector contain the maximal similarity to each concept of the intermediate space. The 96/116 concepts to identify are represented in the same space and their similarity to the training image is the cosine between the intermediate space representation.</p><p>As well, we ranked all training images to each concept to identify in order to learn visual classifiers (linear SVM). We tested several strategies to select positive and negative examples, including the visual coherency, but the simplest strategies was finally the most efficient. It consisted in setting the 100 most similar images as positive and the 500 least similar as negative.</p><p>Finally, a simple weighted late fusion of the visual and textual similarity scores appeared to be more efficient than more sophisticated strategies, resulting to 0.4 MAP on the development query and 0.34 on the testing ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Scalable Concept Image Annotation Subtask of ImageCLEF 2013 <ref type="bibr" coords="2,440.50,139.67,13.05,8.74" target="#b0">[1]</ref> is described in detail in <ref type="bibr" coords="2,220.78,151.62,9.96,8.74" target="#b6">[7]</ref>. The system we propose rely on both visual and textual cues. We conducted many preliminary experiments in order to iteratively improve the provided baseline system (see section 1.1). These experiments dealt with visual features to find image neighbors of queries (section 2), several models of tag (section 3 to 5), the way we learnt visual models (section 6) and finally the decision process (section 7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">baseline system</head><p>A baseline system based on the co-occurence of concepts and tags of the visual neighbors of each query is provided <ref type="bibr" coords="2,291.19,266.69,12.74,8.74" target="#b6">[7]</ref>. Each image I of the development set has to be annotated according to concepts C p , p = 1...95. Such an image has K v neighbors in the train test, according to a visual descriptor (csift BoV provided). Each of these neighbors (k = 1 . . . K v ) has T k tags with a given score</p><formula xml:id="formula_0" coords="2,134.77,314.51,345.33,22.28">(t k,1 , s k,1 . . . t k,i , s k,i . . . t k,T k , s k,T k ). Each of these tags is described with N k,i weighted concepts (C k,i,1 , W k,i,1 . . . C k,i,j , W k,i,j . . . C k,i,N k,i , W k,i,N k,i ).</formula><p>Thus, the score of concept C p for image I is:</p><formula xml:id="formula_1" coords="2,222.45,355.98,258.14,30.66">Score I (C p ) = 1 K v Kv k=1 T k i=1 s k,i W k,i,Cp T k i=1 s k,i<label>(1)</label></formula><p>In practice, each tag is described by the same number K concepts of concepts (default: 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Searching visual neighbors</head><p>In the original system, visual neighbors are provided and said to be found with a C-SIFT based descriptor. We tested several alternative methods. Descriptors are bag of visterms. SIFT local descriptors are densely extracted every 3 pixels. The bag are coded using local soft coding <ref type="bibr" coords="2,389.47,490.44,10.52,8.74" target="#b2">[3]</ref> and max pooling. Then two different spatial pyramid matching scheme <ref type="bibr" coords="2,363.39,502.40,10.52,8.74" target="#b1">[2]</ref> are used: 1×1+3+2×2 for BoV 1 and 1×1+2×24×4 for BoV 2 . Further details on bag-of-visterm design can be found in <ref type="bibr" coords="2,208.81,526.31,9.96,8.74" target="#b5">[6]</ref>. Several distances were tested on these vectors to find the neighbours. The histogram intersection (HI) distance implemented as:</p><formula xml:id="formula_2" coords="2,221.04,555.47,259.55,30.32">Dist HI (x -y) = 1 - 1 D D i=1 min(x i , y i ) max(x i , y i )<label>(2)</label></formula><p>and the classical L1 distance:</p><formula xml:id="formula_3" coords="2,238.75,607.17,241.84,30.32">Dist L1 (x -y) = 1 D D i=1 |x i -y i |<label>(3)</label></formula><p>Results are shown in table <ref type="table" coords="2,256.40,644.16,3.87,8.74" target="#tab_0">1</ref>, showing a non-significant improvement with the BoV 1 signature and the L1 distance. We used a FlickR-based tag model built from the selection of the 95 concepts (F lickr 95 ) and another one built from 30, 000 wikipedia concept (F lickr 30k ). See <ref type="bibr" coords="3,134.77,311.21,10.52,8.74" target="#b4">[5,</ref><ref type="bibr" coords="3,146.95,311.21,7.75,8.74" target="#b7">8]</ref> for details about the way similarities are computed for these models.Note that both models were built from the FlickR tags. The F lickr 95 tag model was injected into the system provided, in conjunction with two different visual models. The mAP is reported in table 2. Note this performance measure should be independant from the parameter K concepts that was fixed to 6. The F-measure only uses the annotation decisions and is computed in two ways, one by analyzing each of the testing samples and the other by analyzing each of the concepts. Results are reported on table 3 and 4.</p><formula xml:id="formula_4" coords="3,256.40,394.05,7.80,7.86">K</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Window-restricted FlickR-based tag models</head><p>The tag model of each training document is built with a restriction of the wordimage distance. In the original web page a training image has been found, the method consists in taking into account words that are less than a given distance from the considered image. Moreover, we considered a lemmatized and nonlemmatized version of the models.</p><p>Results are comparable to those obtained with F lickr 30k (around 0.31) but no improvement is actually observed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Wikipedia-based tag models</head><p>Similarly to the FlickR-based tag models, tags are projected onto 1187980 wikipedia concepts. The concepts being ranked to the numbers of their incoming links, the representation can be pruned to a lower dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Learning visual models</head><p>For a given tag-model, training images are sorted according to their score for each concept. Then we select positive and negative examples according to different strategies to learn linear SVM models from the BoV 1 signatures. Text model used was F lickr w 30k 0 i.e the FlickR tags projected on the 30k wikipedia concepts, with restriction a window of size 0. The simplest strategy consisted in setting the 100 most similar images as positive and the 500 least similar as negative. It leaded to a mAP of 0.219 on the devel queries.</p><p>A second strategy consisted to select as positive all images above a given score (0.8) and as negative all those below a smaller threshold (0.1). Given that classes were then strongly ubalanced, we limited negative image to nine times the positive on for each SVM model, leading to a mAP of 0.207. When the number of negative samples are forced to be equal to the positive ones, the mAP is 0.212.</p><p>A last strategy was tested, for wich the choice of the images was based on the visual coherency <ref type="bibr" coords="5,231.54,538.01,9.96,8.74" target="#b3">[4]</ref>. The 1000 most similar images to each concept are resorted according to their VCscore and the 100 best are thus selective as positive examples. Negative examples are chosen as the 1000 least similar images to the concept. Although promising, this approach leaded to a mAP of 0.209 only.</p><p>We thus finally decided to keep the first and simplest strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Decision</head><p>The decision value (0/1) is taken independently on each query, according to its similarity to the 95 or 116 concepts. We compute the average (µ) and the standard deviation (σ) of the scores and set the decision to 1 for all concepts having a score above µ + σ.</p><p>8 Participation to the campaign Results are quite close from each other and around 10 points (in term of mAP) below the best run of the campaign.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,134.77,117.78,345.82,152.90"><head>Table 1 .</head><label>1</label><figDesc>Result of the baseline system with several methods to search the K = 32 visual neighbors 3 Using a FlickR-based tag model</figDesc><table coords="3,255.75,117.78,103.85,85.37"><row><cell>System</cell><cell>mAP</cell></row><row><cell>K</cell><cell>32</cell></row><row><cell cols="2">Provided baseline 24.235</cell></row><row><cell cols="2">Random neighbors 17.878</cell></row><row><cell>BoV1 HI</cell><cell>23.830</cell></row><row><cell>BoV2 HI</cell><cell>23.468</cell></row><row><cell>BoV1 L1</cell><cell>24.305</cell></row><row><cell>BoV2 L1</cell><cell>23.229</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,134.77,394.05,345.83,118.70"><head>Table 2 .</head><label>2</label><figDesc>Result (mAP) of the system with different tag models and method to search the visual neighbors. (*) the mAP grows with Kconcepts here; result reported with Kconcepts = 6</figDesc><table coords="3,191.89,394.05,231.59,85.86"><row><cell></cell><cell>visual</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>64 128</cell></row><row><cell>Tag</cell><cell>Visual</cell><cell></cell><cell></cell><cell></cell></row><row><cell>co-occurrence</cell><cell>csift</cell><cell cols="4">24.71(*) 24.77 24.24 23.63 23.10</cell></row><row><cell cols="6">co-occurrence BoV1 + L1 25.01(*) 25.08 24.31 23.60 22.80</cell></row><row><cell>F lickR95</cell><cell>csift</cell><cell cols="4">25.08 25.92 26.61 27.33 27.51</cell></row><row><cell cols="6">F lickR95 BoV1 + L1 25.96 27.30 28.16 28.18 27.67</cell></row><row><cell>F lickr 30k</cell><cell>csift</cell><cell cols="4">30.25 29.46 29.23 28.80 28.44</cell></row><row><cell cols="6">F lickr 30k BoV1 + L1 31.05 30.25 29.50 29.07 28.48</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,134.77,117.78,345.82,263.95"><head>Table 3 .</head><label>3</label><figDesc>Result (mFsamp) of the system with different tag models and method to search the visual neighbors.</figDesc><table coords="4,177.90,117.78,259.55,241.59"><row><cell></cell><cell></cell><cell>K visual</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>64 128</cell></row><row><cell>Tag</cell><cell cols="2">Visual Kconcepts</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>2</cell><cell cols="3">11.92 12.49 11.54 10.81 10.32</cell></row><row><cell>co-occurrence</cell><cell>csift</cell><cell>4 6</cell><cell cols="3">15.53 16.48 16.60 16.37 15.77 17.90 18.96 18.60 17.88 17.54</cell></row><row><cell></cell><cell></cell><cell>8</cell><cell cols="3">19.27 19.71 19.61 19.03 18.65</cell></row><row><cell></cell><cell></cell><cell>10</cell><cell cols="3">20.09 20.14 19.86 19.59 19.23</cell></row><row><cell></cell><cell></cell><cell>2</cell><cell cols="3">12.46 13.00 12.20 11.64 10.47</cell></row><row><cell>co-occurrence</cell><cell>BoV1 + L1</cell><cell>4 6</cell><cell cols="3">16.21 16.78 16.33 15.48 14.96 17.76 18.43 18.20 17.81 17.38</cell></row><row><cell></cell><cell></cell><cell>8</cell><cell cols="3">19.07 19.60 19.29 19.06 18.85</cell></row><row><cell></cell><cell></cell><cell>10</cell><cell cols="3">19.83 20.19 19.96 19.63 18.97</cell></row><row><cell></cell><cell></cell><cell>2</cell><cell cols="3">13.96 14.82 15.31 16.39 15.76</cell></row><row><cell>F lickR95</cell><cell>csift</cell><cell>4 6</cell><cell cols="3">16.56 17.47 18.38 18.98 18.93 17.67 18.49 19.67 19.98 19.98</cell></row><row><cell></cell><cell></cell><cell>8</cell><cell cols="3">18.19 19.17 19.82 20.48 20.72</cell></row><row><cell></cell><cell></cell><cell>10</cell><cell cols="3">18.65 19.36 19.96 20.61 21.29</cell></row><row><cell></cell><cell></cell><cell>2</cell><cell cols="3">14.83 15.72 16.66 16.25 15.80</cell></row><row><cell>F lickR95</cell><cell>BoV1 + L1</cell><cell>4 6</cell><cell cols="3">17.48 18.68 19.36 19.44 19.07 18.64 20.04 20.73 21.01 20.90</cell></row><row><cell></cell><cell></cell><cell>8</cell><cell cols="3">19.28 20.33 21.19 21.37 21.18</cell></row><row><cell></cell><cell></cell><cell>10</cell><cell cols="3">19.38 20.51 21.20 21.68 21.68</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,134.77,117.78,345.83,263.95"><head>Table 4 .</head><label>4</label><figDesc>Result (mFcnpt) of the system with different tag models and method to search the visual neighbors.</figDesc><table coords="5,177.90,117.78,259.55,241.59"><row><cell></cell><cell></cell><cell>K visual</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>64 128</cell></row><row><cell>Tag</cell><cell cols="2">Visual Kconcepts</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>2</cell><cell cols="3">7.25 5.49 4.33 3.60 2.91</cell></row><row><cell>co-occurrence</cell><cell>csift</cell><cell>4 6</cell><cell cols="3">11.33 9.13 8.47 7.81 6.11 13.98 12.48 10.67 9.41 8.00</cell></row><row><cell></cell><cell></cell><cell>8</cell><cell cols="3">15.34 13.17 11.87 10.46 9.39</cell></row><row><cell></cell><cell></cell><cell>10</cell><cell cols="3">16.18 14.15 12.67 11.72 10.43</cell></row><row><cell></cell><cell></cell><cell>2</cell><cell cols="3">8.28 7.95 6.01 5.24 3.00</cell></row><row><cell>co-occurrence</cell><cell>BoV1 + L1</cell><cell>4 6</cell><cell cols="3">12.26 11.49 9.15 7.87 7.12 14.40 13.14 11.62 10.38 8.95</cell></row><row><cell></cell><cell></cell><cell>8</cell><cell cols="3">15.42 14.45 13.72 11.93 10.34</cell></row><row><cell></cell><cell></cell><cell>10</cell><cell cols="3">16.21 15.70 14.40 13.03 11.18</cell></row><row><cell></cell><cell></cell><cell>2</cell><cell cols="3">15.11 15.74 15.35 15.39 13.79</cell></row><row><cell></cell><cell></cell><cell>4</cell><cell cols="3">16.10 17.20 17.95 18.13 17.26</cell></row><row><cell></cell><cell></cell><cell>6</cell><cell cols="3">16.30 17.54 18.91 19.34 18.78</cell></row><row><cell></cell><cell></cell><cell>8</cell><cell cols="3">16.26 17.75 18.41 19.43 19.89</cell></row><row><cell></cell><cell></cell><cell>10</cell><cell cols="3">16.35 17.61 17.96 19.16 20.53</cell></row><row><cell></cell><cell></cell><cell>2</cell><cell cols="3">17.00 17.17 17.15 15.61 14.42</cell></row><row><cell>F lickR95</cell><cell>BoV1 + L1</cell><cell>4 6</cell><cell cols="3">18.64 19.08 19.16 17.98 17.05 18.62 19.28 20.47 19.56 18.85</cell></row><row><cell></cell><cell></cell><cell>8</cell><cell cols="3">18.25 19.33 20.56 20.00 18.85</cell></row><row><cell></cell><cell></cell><cell>10</cell><cell cols="3">17.88 18.99 20.34 19.97 20.06</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,134.77,187.94,345.83,296.29"><head>Table 5 .</head><label>5</label><figDesc>Run 1: we computed the FlickR-based tag model and merged it with the visual similarities. The weights were respectively 0.8 and 0.2.Run 2: we added to Run 1 a Wikipedia-based tag model with a representation pruned to 5, 000 dimensions.Run 3: we added to Run 2 a FlickR-based tag model with a representation pruned to 50, 000 dimensions.Run 4: similar to Run 3 with a visual model selected on scores Run 5: similar to Run 4 with a FlickR-based tag model with a representation pruned to 200, 000 dimensions. Result of our five runs to the campaign.</figDesc><table coords="6,134.77,187.94,345.82,285.38"><row><cell cols="2">8.1 Submitted runs</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">We submitted five runs to the campaign, based on the obsvation made during</cell></row><row><cell cols="2">preliminary experiments:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>8.2 Results</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>devel set</cell><cell></cell><cell></cell><cell>test set</cell><cell></cell></row><row><cell cols="6">mAP MF-sample MF-concepts mAP MF-sample MF-concepts</cell></row><row><cell>Run 1 34.6</cell><cell>28.7</cell><cell>23.6</cell><cell>29.4</cell><cell>23.0</cell><cell>19.0</cell></row><row><cell>Run 2 39.6</cell><cell>30.2</cell><cell>24.6</cell><cell>33.6</cell><cell>24.2</cell><cell>20.1</cell></row><row><cell>Run 3 40.4</cell><cell>31.8</cell><cell>25.3</cell><cell>34.1</cell><cell>25.2</cell><cell>20.2</cell></row><row><cell>Run 4 40.3</cell><cell>32.2</cell><cell>26.1</cell><cell>34.2</cell><cell>26.0</cell><cell>21.2</cell></row><row><cell>Run 5 39.2</cell><cell>31.6</cell><cell>25.4</cell><cell>33.6</cell><cell>25.7</cell><cell>21.0</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,138.35,590.92,342.24,7.86;6,146.91,601.88,333.68,7.86;6,146.91,612.84,333.68,7.86;6,146.91,623.80,20.99,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,442.70,601.88,37.89,7.86;6,146.91,612.84,207.40,7.86">Imageclef 2013: the vision, the data and the open challenges</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zellhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">Martinez</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Varea</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Cazorla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,377.11,612.84,68.16,7.86">Proc CLEF 2013</title>
		<meeting>CLEF 2013</meeting>
		<imprint>
			<publisher>LNCS</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,634.88,342.24,7.86;6,146.91,645.84,333.68,7.86;6,146.91,656.80,329.54,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,375.51,634.88,105.08,7.86;6,146.91,645.84,285.90,7.86">Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories</title>
		<author>
			<persName coords=""><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,458.31,645.84,22.28,7.86;6,146.91,656.80,229.20,7.86">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,119.67,342.24,7.86;7,146.91,130.63,249.47,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,325.71,119.67,150.39,7.86">In Defense of Soft-assignment Coding</title>
		<author>
			<persName coords=""><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xinwang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,158.43,130.63,209.79,7.86">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,141.59,342.24,7.86;7,146.91,152.55,333.68,7.86;7,146.91,163.51,333.68,7.86;7,146.91,174.47,273.05,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,461.39,141.59,19.20,7.86;7,146.91,152.55,187.15,7.86">Multimodal image retrieval over a large database</title>
		<author>
			<persName coords=""><forename type="first">Débora</forename><surname>Myoupo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hervé</forename><surname>Le Borgne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre-Alain</forename><surname>Moëllic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,358.30,152.55,122.29,7.86;7,146.91,163.51,333.68,7.86;7,146.91,174.47,34.16,7.86">Proceedings of the 10th international conference on Cross-language evaluation forum: multimedia experiments, CLEF&apos;09</title>
		<meeting>the 10th international conference on Cross-language evaluation forum: multimedia experiments, CLEF&apos;09<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="177" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,185.43,342.24,7.86;7,146.91,196.39,320.20,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,323.67,185.43,140.85,7.86">Social media driven image retrieval</title>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gregory</forename><surname>Grefenstette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,146.91,196.39,224.17,7.86">ACM International Conference on Multimedia Retrieval</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,207.35,342.24,7.86;7,146.91,218.30,333.68,7.86;7,146.91,229.26,178.51,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,307.95,207.35,172.64,7.86;7,146.91,218.30,141.31,7.86">Locality-constrained and spatially regularized coding for scene categorization</title>
		<author>
			<persName coords=""><forename type="first">Aymen</forename><surname>Shabou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hervé</forename><surname>Le Borgne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,307.58,218.30,173.01,7.86;7,146.91,229.26,78.16,7.86">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3618" to="3625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,240.22,342.25,7.86;7,146.91,251.18,333.68,7.86;7,146.91,262.14,20.99,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,375.66,240.22,104.94,7.86;7,146.91,251.18,197.86,7.86">Overview of the imageclef 2013 scalable concept image annotation subtask</title>
		<author>
			<persName coords=""><forename type="first">Mauricio</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roberto</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bart</forename><surname>Thomee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,369.31,251.18,107.37,7.86">CLEF 2013 working notes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,273.10,342.24,7.86;7,146.91,284.06,333.68,7.86;7,146.91,295.02,333.68,7.86;7,146.91,305.98,151.16,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,164.29,284.06,300.24,7.86">Multimodal feature generation framework for semantic image classification</title>
		<author>
			<persName coords=""><forename type="first">Amel</forename><surname>Znaidia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aymen</forename><surname>Shabou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hervé</forename><surname>Le Borgne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Céline</forename><surname>Hudelot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,146.91,295.02,278.18,7.86">ICMR, International Conference on Multimedia Retrieval, ICMR &apos;12</title>
		<meeting><address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">June 5-8, 2012. 2012</date>
			<biblScope unit="page">38</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
