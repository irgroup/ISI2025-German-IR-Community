<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,216.24,116.95,182.87,12.62;1,171.66,134.89,272.03,12.62">MIL at ImageCLEF 2013: Scalable System for Image Annotation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,186.06,172.56,75.90,8.74"><forename type="first">Masatoshi</forename><surname>Hidaka</surname></persName>
							<email>hidaka@mi.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Machine Intelligence Lab</orgName>
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,270.02,172.56,62.19,8.74"><forename type="first">Naoyuki</forename><surname>Gunji</surname></persName>
							<email>gunji@mi.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Machine Intelligence Lab</orgName>
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,359.49,172.56,69.82,8.74"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
							<email>harada@mi.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Machine Intelligence Lab</orgName>
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,216.24,116.95,182.87,12.62;1,171.66,134.89,272.03,12.62">MIL at ImageCLEF 2013: Scalable System for Image Annotation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1CEE085A0A5674DBEB3BD0E10E49DDBB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ImageCLEF</term>
					<term>Textual Feature</term>
					<term>WordNet</term>
					<term>Annotation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We give details of our methods in the ImageCLEF 2013 Scalable Concept Image Annotation task. For the textual feature, we propose a method for selecting text closely related to an image from its webpage. In addition, to consider the meaning of the concept, we propose to use WordNet for getting words related to the concept. For visual features, we use Fisher Vector (FV), which is regarded as an extension of the Bagof-Visual-Words representation. We trained linear classifiers by Passive-Aggressive with Averaged Pairwise Loss (PAAPL), an online multilabel learning method based on Passive-Aggressive. Since PAAPL is computationally efficient and able to cope with multilabel data appropriately, it is suitable for this task. Results show that our annotation pipeline is simple but works well in this task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In ImageCLEF 2013 Scalable Concept Image Annotation, our task is multi-label image annotation <ref type="bibr" coords="1,214.89,465.84,9.96,8.74" target="#b0">[1]</ref>. The dataset is extracted from general webpages, so that the costs in collecting data are low <ref type="bibr" coords="1,283.77,477.79,9.96,8.74" target="#b1">[2]</ref>. However, collected images have no explicit labels. Therefore, we need to extract correct labels of corresponding images from webpages. As for the extraction of labels from websites, the simplest solution is that concept labels which exist in webpages are assigned to the images. However, this method often fails to get correct labels because it does not consider meanings of concepts. Furthermore texts of webpages are not necessarily related to the images. Therefore, we try some methods to get more accurate labels. To achieve it, we use information from WordNet <ref type="bibr" coords="1,297.53,561.48,10.52,8.74" target="#b2">[3]</ref> to get words related to the concepts. In addition, limitation of text extraction range is adopted to omit text not related to the images. For visual features, we adopt Fisher Vector (FV) <ref type="bibr" coords="1,410.24,585.39,13.13,8.74" target="#b3">[4]</ref>, which is an improved method of Bag-of-Visual-Words (BoVW). We use linear classifiers for each concept because they are computationally efficient and suitable for largescale data. When training classifiers, because labels assigned to the images are not ground-truth labels, they must be regarded as noisy. Therefore, we devote attention to robustness for noise in the training data. In order to train linear classifiers, we use PAAPL <ref type="bibr" coords="1,254.36,657.12,9.96,8.74" target="#b4">[5]</ref>, an online multilabel learning method based on Passive-Aggressive <ref type="bibr" coords="2,212.98,119.99,13.04,8.74" target="#b5">[6]</ref>. PAAPL shows faster convergence than PA and has the same feature of robustness to the noise as PA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Feature Extraction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Visual Feature</head><p>As a visual feature, we use the Fisher Vector (FV). Because it can achieve a good classification performance with a linear classifier, it is often used for large-scale visual categorization. Indeed, in the ImageNet Large-Scale Visual Recognition Challenge 2012 (ILSVRC2012), four out of seven teams used FVs to represent images. We use four local descriptors: SIFT, C-SIFT, LBP, and GIST. Actually, GIST is usually used to describe a whole image, but we use it as a local descriptor. All local descriptors are reduced to 64 dimensions using Principal Component Analysis (PCA). Local descriptors are densely extracted from five scales of patches on a regular grid every six pixels and learn a Gaussian Mixture Model (GMM) with 256 components, which have a diagonal matrix as its covariance matrix. To use spatial information, we divide images into 1 × 1, 2 × 2, and 3 × 1 cells. Then FVs are calculated over each region as follows.</p><p>Let X = {x 1 , x 2 , • • • , x N } be a set of N local descriptors extracted from an image, and w i , µ i , Σ i be the mixture weight, mean vector, covariance matrix of the i-th Gaussian, respectively. Then we difine,</p><formula xml:id="formula_0" coords="2,173.64,381.14,268.08,65.03">u i = 1 N √ w i N ∑ n=1 γ n (i)Σ -1 2 i (x n -µ i ) , v i = 1 N √ 2w i N ∑ n=1 γ n (i) [ Σ -1 i diag((x n -µ i )(x n -µ i ) T ) -1 ] ,</formula><p>where 1 is a column vector whose components are all 1 and diag(X) for matrix X is a column vector which is composed of diagonal components of X. γ n (i) is the soft assignment of x n to i-th Gaussian as</p><formula xml:id="formula_1" coords="2,253.71,493.13,98.39,24.78">γ n (i) = w i u i (x n ) Σ K j=1 w j u j (x n )</formula><p>, where u i is the i-th Gaussian, and it is also known as the posterior probability. The FV representation is therefore given as</p><formula xml:id="formula_2" coords="2,250.37,547.69,114.62,19.36">G = [ u T 1 v T 1 . . . u T K v T K ] T ,</formula><p>where K is the number of GMM components. Following <ref type="bibr" coords="2,194.01,586.01,9.96,8.74" target="#b3">[4]</ref>, we apply power normalization and L2 normalization to each of the extracted FVs. Power normalization is done by applying the function,</p><formula xml:id="formula_3" coords="2,266.97,613.51,81.42,10.81">g(z) = sign(z)|z| a ,</formula><p>to each component of FVs, where a is a parameter and is set to 1/2 in this work. After normalization, we concatenate them into a single vector. The dimension of our FVs is 262144. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Textual Feature</head><p>To assign correct labels to images, we take two steps. First we extract text closely related to an image from its webpage. Then if a concept word exists in the extracted text, the concept label is assigned to the image. The pipeline is presented in Fig. <ref type="figure" coords="3,211.36,314.34,3.87,8.74" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text Extraction.</head><p>To extract text closely related to an image, we consider three types of texts in the webpage: text around image, img tag attributes (src, alt, title), page title. First, we parse the xml file of the webpage and extract page title, text, img tag. Then we select some of them and split them into a set of single words T . For the text around the image, we consider the distance from the image (img tag position) because the entire webpage does not necessarily focuses on one image. Then we use max distance from an image as a parameter. We use words which are within the max distance. To normalize words, we singularize nouns.</p><p>Label Assignment. To assign labels to the image, first we collect words related to the each concept C given in the task. We denote a set of collected words by W C . For W C , collecting synonyms and hyponyms of C is considered.</p><formula xml:id="formula_4" coords="3,222.24,506.36,165.30,9.66">W C = {C, synonym(C), hyponym(C)}</formula><p>For example, given a target concept "bird", we get</p><formula xml:id="formula_5" coords="3,235.16,543.11,145.04,9.65">W bird = {bird, parrot, pigeon, ...}.</formula><p>For collecting synonyms and hyponyms, we use WordNet. To make implementation simpler, we use no compound words. Hyponyms are hierarchized. Therefore, we collect words of all depths recursively. Words which have multiple meanings are omitted. Determination is done by checking whether the word appears in multiple entries in WordNet.</p><p>Then if the extracted text T contains any of the concept-related words W C (concept word, synonyms and hyponyms), we assign those concept labels C to the image. Consequently, we obtain a training dataset in which some images have multiple labels, and some images have no label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multilabel Annotation</head><p>In this section, we describe the method of training of the classifiers and annotating of the test images. We use linear classifier for each concept label considering the scalability. With linear classifier, the annotation for test images is performed by computing score of labels as product of the visual feature and the weight vector of labels, and assigning the top 5 scored labels.</p><p>To learn the models for each concept label from various images, requirements are not only compatibility of scalability for the data amount and accuracy for label estimation, but also noise tolerability.</p><p>For that reason, we use Passive-Aggressive with Averaged Pairwise Loss (PAAPL). PAAPL is based on Passive-Aggressive (PA) method, which is known to be tolerant to the noise in training samples.</p><p>First, we describe the model update rule of PA. Given the t-th training sample, we denote the visual feature by f t , the set of concept labels assigned to the sample by Y t , the set of concept labels not assigned to the sample by Ȳt , the current model (weight vector) corresponding to concept label C by µ C t . In our setting, the dimension of f t is 262144 (Fisher Vector) + 1 (bias).</p><p>1. Fetch t-th training sample, compute scores for each label using current models. 2. Find a label r t ∈ Y t associated with the sample and a label s t ∈ Ȳt not associated with the sample as follows.</p><formula xml:id="formula_6" coords="4,272.79,407.13,86.21,41.06">r t = arg min r∈Yt µ r t • f t s t = arg max s∈ Ȳt µ s t • f t</formula><p>Given these labels, compute the hinge-loss l from the current model. The hinge-loss l is given as</p><formula xml:id="formula_7" coords="4,161.67,480.93,302.17,35.87">l(µ rt t , µ st t ; (f t , Y t )) = { 0 µ rt t • f t -µ st t • f t &gt; 1 1 -(µ rt t • f t -µ st t • f t ) otherwise</formula><p>3. Update models with the update rule below.</p><formula xml:id="formula_8" coords="4,257.26,544.87,117.28,54.06">µ rt t+1 = µ rt t + l 2|f t | 2 + 1 D f t µ st t+1 = µ st t - l 2|f t | 2 + 1 D f t</formula><p>D is a parameter which controls the sensitivity to label prediciton mistakes.</p><p>Then we describe the PAAPL method.</p><p>1. Fetch t-th training sample, compute scores for each label using current models.</p><p>2. For all combinations of label r t ∈ Y t associated with the sample and label s t ∈ Ȳt not associated with the sample, compute the hinge-loss as PA. 3. For all combinations for which the hinge-loss is not 0, update the model corresponding to the update rule of PA.</p><p>In PA, only a pair of models is updated for one sample. In PAAPL, on the other hand, all pairs of models are updated for one sample, which reduces the number of training iterations and score computation process, which is timeconsuming. Therefore, the models converge faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Using the visual feature and the textual feature stated in the previous section, the image classifier was trained by PAAPL. The number of training iterations was 5.</p><p>First, we determined whether we should use synonyms and hyponyms of concept for assigning labels to the image. For extracting text from a webpage, we used 10 words of text around the image, img tag attributes and page title. The visual feature is provided BoVW representations of C-SIFT.As a result, we chose to use synonyms and hyponyms. The result is presented in Table <ref type="table" coords="5,447.71,354.27,3.87,8.74" target="#tab_0">1</ref>. Second, we conducted a grid search for the text extraction conditions on the length of words around the image should be considered, the necessity of using the img tag attributes and the page title. The visual feature is the same as in first step. Results show that using only img tag attributes was the best. The text far from the image decreased label assignment accuracy notably. The result is shown in Table <ref type="table" coords="5,216.51,549.10,3.87,8.74" target="#tab_1">2</ref>. The number of images which have at least one label and the average number of labels assigned to one image was also shown in the result. Because of the property of PAAPL, only images which have at least one label are used for training. It is worth noting that in the best condition, the number of images used and the average number of labels are both lowest.</p><p>After this optimization, we tried a previous evaluation (of whether we should use synonyms and hyponyms) again, but the result was the same.</p><p>Finally, using the condition of the textual feature extraction stated above, we trained the weight vectors corresponding to each visual feature (Fisher Vector). It took 2 hr to learn for each visual feature. The final score of each test </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,234.48,230.49,146.40,7.89"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Pipeline of label assignment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,183.75,386.18,247.85,74.89"><head>Table 1 .</head><label>1</label><figDesc>Comparison of the use of synonyms and hyponyms.</figDesc><table coords="5,242.31,406.98,130.73,54.09"><row><cell cols="3">Synonym Hyponym MF-samples</cell></row><row><cell>-</cell><cell>-</cell><cell>0.234</cell></row><row><cell></cell><cell>-</cell><cell>0.232</cell></row><row><cell>-</cell><cell></cell><cell>0.261</cell></row><row><cell></cell><cell></cell><cell>0.266</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,146.66,116.91,263.74,51.65"><head>Table 2 .</head><label>2</label><figDesc>Comparison of the text extraction range.</figDesc><table coords="6,146.66,140.25,102.50,28.31"><row><cell>Text around</cell><cell></cell></row><row><cell>image</cell><cell>Img tag</cell></row><row><cell>(max distance)</cell><cell>attributes</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>image is calculated by summing the scores of all the classifiers (C-SIFT+FV, GIST+FV, LBP+FV, and SIFT+FV). Final results are presented in Table <ref type="table" coords="6,472.86,373.76,3.87,8.74">3</ref>.</p><p>The evaluation with provided C-SIFT + Bag-of-Visual-Words is also shown in the table. Fisher vector exhibited much higher performance than Bag-of-Visual-Words. We performed learning and annotation for the test set with the top 5 ranked combinations.</p><p>According to the results presented from the task organizers, we have achieved the second score among all teams with our best run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this working note, our methods to annotate images in ImageCLEF 2013 Scalable Concept Image Annotation task are described, with particular emphasis on extracting labels for images from websites. Results show that, using concepts' synonyms and hyponyms from WordNet was useful and limiting text range of website was also shown to be important. For visual features, we applied Fisher Vector, a state-of-the-art coding method. Four local descriptors for FV were tried. The combination of C-SIFT, GIST and SIFT showed superior performance. Our textual and visual features are simple but we can achieve a good performance. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="6,138.34,646.84,342.24,7.86;6,146.91,657.80,283.37,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,310.79,646.84,169.80,7.86;6,146.91,657.80,142.92,7.86">Overview of the ImageCLEF 2013 Scalable Concept Image Annotation Subtask</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,298.24,657.80,104.06,7.86">CLEF 2013 working notes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.34,350.43,342.24,7.86;7,146.91,361.38,110.53,7.86" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="7,262.25,350.43,218.33,7.86;7,146.91,361.38,52.85,7.86">Image-Text Dataset Generation for Image Annotation and Retrieval</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
		<respStmt>
			<orgName>CERI</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.34,372.34,307.74,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="7,202.84,372.34,165.52,7.86">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.34,383.30,342.24,7.86;7,146.91,394.26,281.02,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,316.82,383.30,163.76,7.86;7,146.91,394.26,76.41,7.86">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,230.97,394.26,168.80,7.86">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.34,405.22,342.23,7.86;7,146.91,416.18,273.59,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,317.17,405.22,163.41,7.86;7,146.91,416.18,77.18,7.86">Efficient image annotation for automatic sentence generation</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Kuniyoshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,232.23,416.18,159.89,7.86">International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.34,427.14,342.24,7.86;7,146.91,438.10,333.68,7.86;7,146.91,449.06,58.87,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,417.99,427.14,62.60,7.86;7,146.91,438.10,89.82,7.86">Online Passive-Aggressive Algorithms</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Keshet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,248.94,438.10,178.59,7.86">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="551" to="585" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
