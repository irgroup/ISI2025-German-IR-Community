<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,157.56,115.82,300.14,12.93;1,248.64,133.82,118.07,12.93">URJC&amp;UNED at ImageCLEF 2013 Photo Annotation Task</title>
				<funder ref="#_9TpxKfN #_xa9sgaG">
					<orgName type="full">&quot;Ministerio de Ciencia e Innovación&quot;</orgName>
				</funder>
				<funder ref="#_gnNsSQh #_rBacGF7 #_eWBGJ3t #_2FFEcYC #_CX69aD9">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_mkB8U5B">
					<orgName type="full">Education Council of the Regional Government of Madrid</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,151.20,171.41,81.06,9.62"><forename type="first">Jesús</forename><surname>Sánchez-Oro</surname></persName>
							<email>jesus.sanchezoro@urjc.es</email>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Rey Juan Carlos</orgName>
								<address>
									<settlement>Móstoles</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,242.64,171.41,63.78,9.62"><forename type="first">Soto</forename><surname>Montalvo</surname></persName>
							<email>soto.montalvo@urjc.es</email>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Rey Juan Carlos</orgName>
								<address>
									<settlement>Móstoles</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,316.80,171.41,104.57,9.62"><forename type="first">Antonio</forename><forename type="middle">S</forename><surname>Montemayor</surname></persName>
							<email>antonio.sanz@urjc.es</email>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Rey Juan Carlos</orgName>
								<address>
									<settlement>Móstoles</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,431.76,171.41,28.44,9.62;1,158.16,183.29,37.50,9.62"><forename type="first">Juan</forename><forename type="middle">J</forename><surname>Pantrigo</surname></persName>
							<email>juanjose.pantrigo@urjc.es</email>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Rey Juan Carlos</orgName>
								<address>
									<settlement>Móstoles</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,206.04,183.29,74.26,9.62"><forename type="first">Abraham</forename><surname>Duarte</surname></persName>
							<email>abraham.duarte@urjc.es</email>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Rey Juan Carlos</orgName>
								<address>
									<settlement>Móstoles</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,290.88,183.29,59.22,9.62"><forename type="first">Víctor</forename><surname>Fresno</surname></persName>
							<email>vfresno@lsi.uned.es</email>
							<affiliation key="aff1">
								<orgName type="institution">Universidad Nacional de Educación a Distancia</orgName>
								<address>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,380.04,183.29,72.82,9.62"><forename type="first">Raquel</forename><surname>Martínez</surname></persName>
							<email>raquel@lsi.uned.es</email>
							<affiliation key="aff1">
								<orgName type="institution">Universidad Nacional de Educación a Distancia</orgName>
								<address>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,157.56,115.82,300.14,12.93;1,248.64,133.82,118.07,12.93">URJC&amp;UNED at ImageCLEF 2013 Photo Annotation Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D9C6302CDC1AEEC9C983F87760C9041E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>imageclef</term>
					<term>image classification</term>
					<term>visual features</term>
					<term>textual features</term>
					<term>automatic image annotation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work we describe the URJC&amp;UNED participation in the ImageCLEF 2013 Photo Annotation Task. We use visual information to find similar images and textual information extracted from the training set to label the test images. We propose two additional visual features apart from the provided by the organization and a method to expand the textual information available. The new visual features proposed define the images in terms of color and texture, and the textual method uses WordNet to obtain synonyms and hyperonyms of the textual information provided. The score of each concept is obtained by using a co-ocurrence matrix that matches concepts and textual information of the training images. The experimental results show that the proposal is able to obtain competitive results in all the performance measures used.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This work proposes a new image annotation algorithm used in the Scalable Concept Image Annotation task of ImageCLEF 2013, which is one of the labs of the CLEF 2013 <ref type="bibr" coords="1,222.36,524.33,9.99,9.62" target="#b3">[4]</ref>. In this task we receive a set of training images with some visual and textual features associated to each one. The goal is the labeling of a new set of images that only contains visual features, without any textual information. We use visual information to find similar images within the training set, and then we use their associated textual information in order to annotate the test images.</p><p>Visual features provided with the images are based on the ones used in previous years, namely, GIST, Color Histograms, SIFT, C-SIFT, RGB-SIFT and OPPONENT-SIFT, all of them saved in a bag-of-words representation. The textual data of the training images contains information about the source of the images, a list of words related to the image and the words used to find the images in three different search engines. More details on the task can be found in <ref type="bibr" coords="1,460.32,655.85,15.60,9.62" target="#b9">[10]</ref> In this work we only use the C-SIFT feature, the words related to the images and the words used to find the images in the search engines. We also propose the use of different new visual features in order to increase the performance of the annotation algorithm, as well as a procedure that extends the textual information provided with the training images.</p><p>The rest of the paper is organized as follows: Section 2 describes our proposed visual features and the algorithm to expand the textual information. The algorithm for concept annotation is described in Section 3 and in Section 4 we analyze the results obtained by our proposal. Finally, Section 5 draws the conclusions of the work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Features</head><p>The algorithm proposed in this work uses only the C-SIFT visual feature provided by the organization and the list of words related to the image and words used to find it as textual features. In addition to these features we propose two more visual features and a procedure to expand the textual information provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Additional Visual Features</head><p>With the new visual features proposed in this work we try to increase the set of features used, that seems to be the same in the last years (SIFT with a bag-ofwords representation).</p><p>The first visual feature proposed is a color histogram in the HSV space. It is common to use the color histogram of an image when we are trying to find similar images, but those histograms are usually obtained from the RGB space. HSV color space is robust against shadows in the images or changes in the lighting conditions, while in RGB color space a shadow can abruptly change the color value of an area. Specifically, we use the Hue and Saturation channels of the HSV space, and discard the Value channel, because it only stores information about the brightness, which is not useful in this task.</p><p>Figure <ref type="figure" coords="2,181.44,500.69,4.98,9.62" target="#fig_0">1</ref> shows the result obtained extracting the channels Hue, Saturation and Value from an image of the training set. In the Hue image it is easy to see that the regions which belongs to the same or similar color are quite uniform, while the Saturation image gives information about how light or dark is the color. The Value image contains details about brightness and shadows in the image, and it is discarded in our proposal. To compare two HSV color histograms we use the Bhattacharyya distance, which is commonly used in histogram comparisons, defined as follows:</p><formula xml:id="formula_0" coords="2,233.04,606.06,247.59,31.34">B(h1, h2) = i=|h1| i=1 h1[i] • h2[i] h1 + h2<label>(1)</label></formula><p>where h1, h2 are the HSV color histograms of the images that are being compared. The second additional feature proposed is the Local Binary Patterns method (LBP). It was proposed in <ref type="bibr" coords="3,258.12,362.93,10.56,9.62" target="#b6">[7]</ref> and uses information about the texture of the image. The feature is based on the comparison of each pixel to its neighborhood. To analyze the local binary pattern of one pixel, we take the pixel as the center and then we threshold each pixel in the neighborhood against it, obtaining a 1 value if the intensity of the neighbor is higher or equal than the pixel in the center and a 0 value otherwise. Then we concatenate the binary values of the resulting neighbors to obtain a binary chain of 8 elements (3x3 neighborhood). That binary chain is then converted into a [0 -255] decimal value, which represents the local binary pattern of the center pixel. shows an example of the evaluation of the LBP code of a pixel. The intensity of the center pixel is used as a threshold for the neighborhood, so values 2, 1 and 4 are converted to 0 and 8, 9 and 6 are converted to 1. The result is read clockwise starting from the upper left corner, obtaining the binary chain 00010101, which is converted in the decimal number 21. Then the intensity value of the center pixel in the new LBP image will be 21. The algorithm to obtain this new intensity value is really efficient because it is only based on threshold evaluations, and it has been successfully used in the detection and tracking of objects, biometrics, biomedical applications, video analysis, etc. <ref type="bibr" coords="4,415.91,178.49,9.99,9.62" target="#b8">[9]</ref>. Although the resulting image is not easy to interpret, it clearly separates the different areas of the image. For instance, it is easy to see that the texture of the face is quite different from the texture of the hair or the background. The important details of pictures are usually in the center of the image, so we remove the 15% of the image frame in order to evaluate only the center part of the image. Once the LBP image has been obtained, we calculate the histogram of resulting image, and we use it as the descriptor of the feature. Then, to compare two LBP histograms we use the Chi-Square distance, as recommended in most LBP relevant works <ref type="bibr" coords="4,224.16,568.97,10.56,9.62" target="#b0">[1,</ref><ref type="bibr" coords="4,236.28,568.97,11.70,9.62" target="#b10">11]</ref>, defined as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original LBP</head><formula xml:id="formula_1" coords="4,233.76,604.38,246.87,31.34">χ 2 (h1, h2) = i=|h1| i=1 (h1[i] -h2[i]) 2 h1[i] + h2[i]<label>(2)</label></formula><p>where h1, h2 are the LBP histograms of the images that are being compared.</p><p>V</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Additional Textual Features</head><p>Several sets of textual features are provided, but we only use two of them. We use the text from the webpages where the images appear and the words used to find each image in the search engines. The webpages which referenced the images and the URLs of the images are not used as explicitly mentioned by the organization.</p><p>The processed text extracted from the webpages near where the images appeared are provided joined a score per word, which is derived taking into account the term frequency, the document object model attributes, and the word distance to the image. We use these scores and only select the 95% of words with the highest values. On the other hand, we select all the keywords used to find the images when querying image search engines, independently of the position given to the image in the ranking list.</p><p>We build a text representation from the textual features using the lexical database WordNet <ref type="bibr" coords="5,217.80,296.69,10.56,9.62" target="#b4">[5,</ref><ref type="bibr" coords="5,230.04,296.69,7.04,9.62" target="#b7">8]</ref>. WordNet is used to enrich the keywords with synonyms and hyperonyms, because we think these type of textual features could be closer to the meaning of the images than other textual features.</p><p>We represent the images by means of a co-occurrence matrix, including stop words removal and stemming for both concepts and words of selected textual features. In the matrix the columns are the concepts to be tagged, and the rows are the different words selected as textual features. Formally, the co-occurrence matrix of an image is a N × M matrix, where N corresponds to the number of unique words in the set of textual features for the image, and M corresponds to the number of concepts. A cell m ij contains the number of times word w i co-occurs with concept c j within the set of textual features of the image. The task of building the co-occurrence matrix is quite common in corpus linguistics and provides the starting point to the algorithm to annotate the image concepts.</p><p>Usually, it can be not easy to find the suitable words to search images in a search engine. For this reason expanding the keywords with synonyms allows us to found more co-occurrences with the concepts. Different works confirm that expand with synonyms are useful for different tasks. For instance, <ref type="bibr" coords="5,420.72,488.81,10.56,9.62" target="#b2">[3]</ref> shows that indexing with Wordnet synonyms may improve retrieval results, and <ref type="bibr" coords="5,429.96,500.69,10.56,9.62" target="#b1">[2]</ref> proposes an expansion with WordNet synonyms for the task of document retrieval in Question Answering. Also, in the last edition of ImageCLEF Photo Annotation task several proposal used WordNet and synonyms to enrich the representation ( <ref type="bibr" coords="5,138.36,548.57,10.80,9.62" target="#b5">[6,</ref><ref type="bibr" coords="5,150.84,548.57,11.66,9.62" target="#b11">12]</ref>). In the same way, expanding the keywords with hyperonyms allows us to find more concepts for the images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Concept annotation</head><p>The annotation of concepts is performed in two stages. Given a test image, the first stage finds the most similar images among all the images in the training set using only visual features, while the second stage uses textual information of the training images to annotate the test image.</p><p>The first stage has been implemented using a k-nearest neighbor algorithm (KNN), with k = 50. We have trained three different KNN, one for each visual feature. The training has been carried out by measuring the distance from each test image (from test and devel sets) to each training image. It results in an ordered list for each test image, in which the first element is the most similar training image and the last element is the least similar training image. In order to make the labeling efficient, we saved the training for each feature in a file. With this training, to find the most similar images, we only have to read a file, instead of evaluating the distance among images for each run.</p><p>These training files contains, for each test image, the 200 most similar training images for each visual feature (i.e., C-SIFT, HSV and LBP). Then, in the first stage of the concept annotation algorithm we obtain the 50 most similar images from each feature, resulting in the union of the three features, giving the same importance to each feature. Specifically,</p><formula xml:id="formula_2" coords="6,197.52,300.55,283.11,34.21">S(image test ) = KN N (50, C -SIF T, image test ) KN N (50, HSV, image test ) KN N (50, LBP, image test )<label>(3)</label></formula><p>where s(image test ) is the set of the most similar images that is going to be used in the second stage. The distance measure used depends on the visual feature used. The C-SIFT feature uses the Euclidean distance, the LBP feature uses the Chi-Square distance and, finally, the HSV feature uses the Bhattacharyya distance.</p><p>The second stage is based on the textual features of the training images. For each image train ∈ s(image test ) we extract its co-occurrence matrix, as described in Section 2.2. Then, we sum the concept co-ocurrences (each column), constructing a vector with size equal to n = |concepts| where position i contains the number of occurrences of concept i in the image textual features. Finally, we normalize this vector in the range [0, 1] and use it as the output of the algorithm scores. If the score of a concept exceeds a predefined threshold, then the concept is labeled with 1, else it is labeled with 0.</p><p>We have submitted three runs, that differs in the textual features used, as described below:</p><p>-Run 1: Keywords and words near to the image in the website (selecting only the 95% words with the highest score) of the training images. -Run 2: We add the synonyms of the keywords to the textual features of Run 1. -Run 3: We add the hyperonyms of the keywords to the textual features of Run 2.</p><p>As can be seen, the textual features used grows incrementally with each run, with the aim of controlling if synonyms of hyperonyms are useful to improve the results.</p><p>This section reports the computational experiments that we have performed to obtain the visual and textual features, train the k-nearest neighbor algorithms and finally execute each submitted run. The visual features extraction has been implemented in C++ using the Open Computer Vision 2.4.5 (OpenCV) library and the textual features, KNN training and runs has been implemented in Java 7. The experiments have been performed in an Intel Core i7 2600 CPU (3.4 GHz) and 4 GB RAM. The performance measures used to evaluate the runs are: mean F-measure for the samples (MF-samples), mean F-measure for the concepts (MF-concepts) and the mean average precision for the samples (MAP-samples).</p><p>The first experiment is oriented to evaluate the quality of the KNN training algorithm. We do not have ground truth to evaluate this experiment, so the evaluation is only qualitative. The aim of this experiment is to check whether the images obtained by the KNN training are similar to the test images or not. Figure <ref type="figure" coords="7,166.20,304.37,4.98,9.62" target="#fig_3">4</ref> shows an example of the images extracted from each KNN training. As can be seen in Figure <ref type="figure" coords="7,261.36,643.97,3.90,9.62" target="#fig_3">4</ref>, the images of the KNN training do not overlap, because the information used by each feature is quite different. For that reason we use these three visual features, each one focused in one aspect of the image: color (HSV), shape (C-SIFT) and texture (LBP).</p><p>In the second experiment we compare the three textual features proposed to evaluate which one is able to obtain a higher performance. Table <ref type="table" coords="8,428.40,154.61,4.98,9.62" target="#tab_0">1</ref> shows the results of the three runs using different textual features over the development set. Run 3 is the best run when comparing all the performance measures, but the difference is not big enough to discard Run 1 and Run 2. Although Run 3 is slightly better than Run 1 and Run 2 in all the performance measures used, we decided to include the three runs because there is another test set with different concepts in which Run 1 and Run 2 may be better than Run 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MF-samples MF-samples MAP-samples</head><p>Finally, we analyze the results obtained by the three runs over the development and test set, comparing them with the other participants. Tables <ref type="table" coords="8,455.40,370.13,4.98,9.62" target="#tab_1">2</ref> and<ref type="table" coords="8,134.76,382.13,4.98,9.62" target="#tab_2">3</ref> shows the final performance of our proposal. The values between brackets indicates the ranking of the run among all the 58 participants. The results show that our runs are in the middle of the ranking if we analyze the MF-samples performance measure and in the third quarter if we analyze the other measures. As expected, Run 3 is our best run in all the measures, and there are not important differences between Run 1 and Run 2 in both development and test set. The union of the visual features have resulted in a good method to look for similar images. The results also show that the method to expand textual information has improved the results obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper we describe our participation in the ImageCLEF 2013 Photo Annotation Task. The algorithm proposed is divided in two stage. The first stage uses only visual features while the second stage take advantage of the available textual information. We propose two additional visual features apart from the C-SIFT information that are able to analyze the image focusing in different features: color, shape and texture. We also propose a method to expand the available textual information with synonyms and hyperonyms, compare that information with the concepts and give a score for each concept depending on the comparison. The results show that the best run (Run 3) takes advantage of the visual features proposed and the method to expand the textual information to improve the results of the annotation. Our submissions are in the middle of the ranking analyzing the MF-samples measure and in the third quarter analyzing the other measures. The main aim of future works is the improvement the annotation algorithm, as well as the addition of new visual and textual features that lead us to improve our performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,134.76,305.68,345.76,8.66;3,134.76,316.60,38.38,8.66;3,362.09,206.09,103.02,77.22"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example of the extraction of Hue, Saturation and Value channels of a training image (a)</figDesc><graphic coords="3,362.09,206.09,103.02,77.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,141.00,598.00,333.31,8.66"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Example of the LBP evaluation of a pixel with respect to its neighborhood</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,183.96,421.24,247.22,8.66;4,175.39,218.19,129.72,172.82"><head>Fig. 3 .Figure 3</head><label>33</label><figDesc>Fig. 3. Example of the LBP evaluation over a training image</figDesc><graphic coords="4,175.39,218.19,129.72,172.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,134.76,596.68,345.87,8.66;7,134.76,607.60,46.06,8.66"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Test image and the five most similar images using each visual feature (C-SIFT, LBP, HSV)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,173.88,234.52,267.46,41.54"><head>Table 1 .</head><label>1</label><figDesc>Preliminary results of the runs over the development set</figDesc><table coords="8,214.32,234.52,166.84,30.62"><row><cell>Run 1</cell><cell>27.4</cell><cell>19.2</cell><cell>32.0</cell></row><row><cell>Run 2</cell><cell>27.7</cell><cell>19.7</cell><cell>32.2</cell></row><row><cell>Run 3</cell><cell>27.9</cell><cell>19.8</cell><cell>32.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,166.08,426.76,283.12,149.78"><head>Table 2 .</head><label>2</label><figDesc>Performance measures of the runs over the development set.</figDesc><table coords="8,175.80,426.76,263.67,149.78"><row><cell cols="5">MF-samples MF-concepts MAP-samples</cell></row><row><cell cols="2">Run 1 27.4 (29)</cell><cell>19.2 (41)</cell><cell>32.0 (35)</cell></row><row><cell cols="2">Run 2 27.7 (27)</cell><cell>19.7 (40)</cell><cell>32.2 (34)</cell></row><row><cell cols="2">Run 3 27.9 (26)</cell><cell>19.8 (39)</cell><cell>32.6 (32)</cell></row><row><cell cols="5">MF-samples MF-concepts MF-new concepts MAP-samples</cell></row><row><cell>Run 1 23.7 (29)</cell><cell>17.1 (41)</cell><cell cols="2">14.6 (45)</cell><cell>27.6 (36)</cell></row><row><cell>Run 2 23.8 (28)</cell><cell>17.2 (40)</cell><cell cols="2">14.6 (44)</cell><cell>27.6 (35)</cell></row><row><cell>Run 3 24.1 (27)</cell><cell>17.3 (38)</cell><cell cols="2">14.8 (43)</cell><cell>28.1 (33)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,183.84,578.80,247.60,8.66"><head>Table 3 .</head><label>3</label><figDesc>Performance measures of the runs over the test set.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work has been part-funded by the <rs type="funder">Education Council of the Regional Government of Madrid</rs>, <rs type="projectName">MA2VICMR</rs> (<rs type="grantNumber">S-2009/TIC-1542</rs>), and the research projects <rs type="projectName">Holopedia</rs>, <rs type="projectName">HiSCoP</rs>, <rs type="projectName">DIETHA</rs> and <rs type="projectName">SAMOA3D</rs> funded by the <rs type="funder">"Ministerio de Ciencia e Innovación"</rs> under grants <rs type="grantNumber">TIN2010-21128-C02</rs>, <rs type="grantNumber">TIN2008-06890-C02</rs>, <rs type="grantNumber">TIN2012-35632</rs> and <rs type="grantNumber">TIN2011-28151</rs> respectively.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_mkB8U5B">
					<idno type="grant-number">S-2009/TIC-1542</idno>
					<orgName type="project" subtype="full">MA2VICMR</orgName>
				</org>
				<org type="funded-project" xml:id="_gnNsSQh">
					<orgName type="project" subtype="full">Holopedia</orgName>
				</org>
				<org type="funded-project" xml:id="_rBacGF7">
					<orgName type="project" subtype="full">HiSCoP</orgName>
				</org>
				<org type="funded-project" xml:id="_eWBGJ3t">
					<orgName type="project" subtype="full">DIETHA</orgName>
				</org>
				<org type="funded-project" xml:id="_9TpxKfN">
					<idno type="grant-number">TIN2010-21128-C02</idno>
					<orgName type="project" subtype="full">SAMOA3D</orgName>
				</org>
				<org type="funding" xml:id="_xa9sgaG">
					<idno type="grant-number">TIN2008-06890-C02</idno>
				</org>
				<org type="funding" xml:id="_2FFEcYC">
					<idno type="grant-number">TIN2012-35632</idno>
				</org>
				<org type="funding" xml:id="_CX69aD9">
					<idno type="grant-number">TIN2011-28151</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,138.32,525.04,342.23,8.66;9,146.88,536.08,333.67,8.66;9,146.88,547.00,186.22,8.66" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,314.04,525.04,166.51,8.66;9,146.88,536.08,154.51,8.66">Face Description with Local Binary Patterns: Application to Face Recognition</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,304.32,536.08,176.23,8.66;9,146.88,547.00,81.18,8.66">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2037" to="2041" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.32,557.92,342.35,8.66;9,146.88,568.96,333.52,8.66;9,146.88,579.88,106.60,8.66" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,207.72,557.92,272.95,8.66;9,146.88,568.96,50.07,8.66">Query expansion based on pseudo relevance feedback from definition clusters</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bernhard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,200.76,568.96,279.64,8.66;9,146.88,579.88,76.48,8.66">Proceedings of the 23rd International Conference on Computational Linguistics: Posters</title>
		<meeting>the 23rd International Conference on Computational Linguistics: Posters</meeting>
		<imprint>
			<biblScope unit="page" from="54" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.32,590.80,342.23,8.66;9,146.88,601.84,333.67,8.66;9,146.88,612.76,301.84,8.66" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,240.36,590.80,240.19,8.66;9,146.88,601.84,17.53,8.66">Indexing with wordnet synonyms may improve retrieval results</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Buscaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,171.24,601.84,309.31,8.66;9,146.88,612.76,262.54,8.66">Proceedings of the 10th cross-language evaluation forum conference on Multilingual information access evaluation: text retrieval experiments</title>
		<meeting>the 10th cross-language evaluation forum conference on Multilingual information access evaluation: text retrieval experiments</meeting>
		<imprint>
			<biblScope unit="page" from="128" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.32,623.68,342.20,8.66;9,146.88,634.72,333.67,8.66;9,146.88,645.64,333.67,8.66;9,146.88,656.56,25.42,8.66" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,453.24,634.72,27.31,8.66;9,146.88,645.64,228.36,8.66">Image-CLEF 2013: the vision, the data and the open challenges</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zellhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Martínez Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">García</forename><surname>Varea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Cazorla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,382.44,645.64,66.32,8.66">Proc CLEF 2013</title>
		<meeting>CLEF 2013</meeting>
		<imprint>
			<publisher>LNCS</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.32,119.44,342.20,8.66;10,146.88,130.48,25.42,8.66" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="10,199.80,119.44,165.86,8.66">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.32,141.40,342.20,8.66;10,146.88,152.32,288.10,8.66" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,211.32,141.40,269.20,8.66;10,146.88,152.32,66.17,8.66">The participation of IntermidiaLab at the ImageCLEF 2012 Photo Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Manzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,220.44,152.32,185.83,8.66">CLEF Online Working Notes/Labs/Workshop</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.32,163.36,342.20,8.66;10,146.88,174.28,333.67,8.66;10,146.88,185.20,333.74,8.66;10,146.88,196.24,94.54,8.66" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,291.72,163.36,188.80,8.66;10,146.88,174.28,333.67,8.66;10,146.88,185.20,62.54,8.66">Face Recognition with Local Binary Patterns, Spatial Pyramid Histograms and Naive Bayes Nearest Neighbor Classification Proceedings of the</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mery</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Soto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,236.76,185.20,243.86,8.66;10,146.88,196.24,26.91,8.66">International Conference of the Chilean Computer Science Society</title>
		<imprint>
			<biblScope unit="page" from="125" to="132" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.32,207.16,300.82,8.66;10,456.24,207.16,24.28,8.66;10,146.88,218.08,79.78,8.66" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="10,196.80,207.16,242.34,8.66;10,456.24,207.16,18.21,8.66">WordNet: A Lexical Database for English Communications of ACM</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="39" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.32,229.12,3.56,8.66;10,194.64,229.12,285.88,8.66;10,146.88,240.04,130.06,8.66" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename></persName>
		</author>
		<title level="m" coord="10,357.24,229.12,123.28,8.66;10,146.88,240.04,101.52,8.66">Computer Vision Using Local Binary Patterns Springer</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.54,250.96,337.95,8.66;10,146.88,262.00,333.67,8.66;10,146.88,272.92,25.42,8.66" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,308.28,250.96,172.22,8.66;10,146.88,262.00,145.22,8.66">Overview of the ImageCLEF 2013 Scalable Concept Image Annotation Subtask</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,300.12,262.00,109.69,8.66">CLEF 2013 Working Notes</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.54,283.84,338.01,8.66;10,146.88,294.88,333.67,8.66;10,146.88,305.80,81.82,8.66" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,356.40,283.84,124.15,8.66;10,146.88,294.88,307.03,8.66">Boosting Local Binary Pattern (LBP)-Based Face Recognition Advances in Biometric Person Authentication</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Xiangsheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,456.12,294.88,24.43,8.66">LNCS</title>
		<imprint>
			<biblScope unit="volume">3338</biblScope>
			<biblScope unit="page" from="179" to="186" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.54,316.72,337.77,8.66;10,146.88,327.64,333.76,8.66;10,146.88,338.68,119.98,8.66" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,372.84,316.72,107.47,8.66;10,146.88,327.64,207.97,8.66">CEA LIST&apos;s Participation to the Concept Annotation Task of ImageCLEF</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Znaidia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Shabou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">L</forename><surname>Borgne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,384.36,327.64,96.28,8.66;10,146.88,338.68,91.27,8.66">CLEF Online Working Notes/Labs/Workshop</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
