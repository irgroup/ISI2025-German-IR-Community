<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,137.88,115.96,339.59,12.62;1,291.37,133.89,32.61,12.62">Overview of the ImageCLEF 2013 Robot Vision Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,137.88,171.75,98.59,8.74"><forename type="first">Jesus</forename><surname>Martinez-Gomez</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Castilla-La Mancha Albacete</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,247.02,171.75,88.47,8.74"><forename type="first">Ismael</forename><surname>Garcia-Varea</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Castilla-La Mancha Albacete</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,346.04,171.75,66.18,8.74"><forename type="first">Miguel</forename><surname>Cazorla</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Alicante Alicante</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,442.14,171.75,35.34,8.74;1,281.89,183.71,32.10,8.74"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
							<email>bcaputo@idiap.ch</email>
							<affiliation key="aff2">
								<orgName type="institution">Idiap Research Institute</orgName>
								<address>
									<settlement>Martigny</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Rome La Sapienza</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,137.88,115.96,339.59,12.62;1,291.37,133.89,32.61,12.62">Overview of the ImageCLEF 2013 Robot Vision Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">62B64D0C8627F0488138E2E444158190</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article describes the RobotVision@ImageCLEF 2013 challenge, which addresses two problems: place classification and object recognition. Participants of the challenge were asked to classify rooms on the basis of image sequences captured by cameras mounted on a mobile robot. They were also asked to detect the appearance or lack of several objects. The proposals of the participants had to answer two questions: "where are you?" (I am in the elevator, in the toilet, etc.) and "which objects can you see?" (I can see a table and a chair but not a computer) when presented with a test sequence. The number of times a specific set of object appears in a frame was not considered but if they appeared or not in it. The test sequence was acquired within the same environment but with different lighting conditions than the training sequences. The main novelty of the 2013 edition of the task is the object recognition problem. For both problems: place classification and object recognition, depth and visual images were provided. Moreover, participants were allowed to take advantage from the temporal continuity of the test sequence. The winner of the 2013 edition of the Robot Vision task was the MIAR ICT group, from China.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper describes the ImageCLEF 2013 Robot Vision challenge <ref type="bibr" coords="1,429.48,569.46,14.61,8.74" target="#b11">[12]</ref>, a competition that started in 2009 within the ImageCLEF 1 <ref type="bibr" coords="1,378.40,581.42,10.52,8.74" target="#b1">[2]</ref> as part of the Cross This work was supported by the SNSF project MULTI (B. C.), and by the European Social Fund (FEDER), the Spanish Ministerio de Ciencia e Innovacion (MICINN), and the Spanish "Junta de Comunidades de Castilla-La Mancha" (MIPRCV Consolider Ingenio 2010 CSD2007-00018, TIN2010-20900-C04-03, PBI08-0210-7127 and PPII11-0309-6935 projects, J. M.-G. and I. G.-V.) 1 http://imageclef.org/ Language Evaluation Forum (CLEF) Initiative<ref type="foot" coords="2,348.66,117.42,3.97,6.12" target="#foot_0">2</ref> . Since its origin, the Robot Vision task has been addressing the problem of place classification for mobile robot localization.</p><p>The 2009@ImageCLEF edition of the task <ref type="bibr" coords="2,337.18,155.31,14.61,8.74" target="#b12">[13]</ref>, with 7 participating groups, defined some details that have been maintained for all the following editions. Participants were given training data consisting of sequences of frames recorded in indoor environments. These training frames were labelled with the name of the rooms they were acquired from. The task consisted on building a system capable to classify test frames using as class the name of the rooms previously seen. Moreover, the system could refrain from making a decision in the case of lack of confidence. Two different subtasks were then proposed: obligatory and optional. The difference between both subtasks was that the temporal continuity of the test sequence could only be exploited in the optional task. The score for each participant submission was computed as the sum of the frames that were correctly labelled minus a penalty that was applied to the frames that were misclassified. No penalties were applied for frames not classified.</p><p>In 2010, two editions of the challenge took place. The second edition of the task, 2010@ICPR <ref type="bibr" coords="2,215.91,323.13,15.50,8.74" target="#b9">[10]</ref> was held in conjunction with ICPR 2010 conference. In that edition, where 9 groups participated, the use of stereo images and two types of different training sequences (easy and hard), that had to be used separately, were introduced. The 2010@ImageCLEF edition <ref type="bibr" coords="2,374.23,358.99,14.61,8.74" target="#b10">[11]</ref>, with 7 participating groups, was focused on generalization: several areas could belong to the same semantic category.</p><p>In 2012, stereo images were replaced by images acquired using two types of camera: a perspective camera for visual images and a depth camera (the Microsoft Kinect sensor) for range images. Therefore, each frame consisted of two types of images and the challenge become a problem of multimodal (place) classification. In addition to the use of depth images, the optional task contained kidnappings and unknown rooms (not previously seen in training sequences) not appeared in the test sequences. Moreover, several techniques for features extraction and cue integration were proposed to the participants.</p><p>For the ImageCLEF 2013 Robot Vision challenge we changed the visual data, providing the traditional RGB images and its corresponding point cloud information. The main difference from 2012 edition was that no depth image was provided but the point cloud itself. The purpose of that was to encourage the participants to make use of 3D image processing techniques, in addition to visual ones, with the aim to obtain better classification results. Furthermore, for some specific rooms, we provided completely dark images for which the use of the 3D information had to be used in order to classify such a room.</p><p>Regarding the participation, in this edition, we received a total of 16 runs, from 6 different participant groups. The best result was obtained by the MIART ICT research group from Beijing, China.</p><p>The rest of the paper details the challenge and is organized as follows: Section 2 describes the 2013 ImageCLEF edition of the RobotVision task. Section 3 presents all the participants groups, while the results are reported in Section 4. Finally, in Section 5, the main conclusions are drawn and some ideas for future editions are outlined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The RobotVision Task</head><p>This section describes the details concerning the setup of the ImageCLEF 2013 Robot Vision task. In Section 2.1 a description of training, validation and test sequences is provided. In Section 2.2 the performance evaluation criteria is detailed. Finally, in Section 2.3 a brief description of the baseline visual place classification system provided by the organizers, as well as other relevant details concerning the task are presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Description</head><p>The fifth edition of the Robot Vision challenge was focused on the problem of multi-modal information retrieval from indoor scenes. Participants had to detect, for each test image, the presence or lack of a set of objects. They also had to determine the kind of room where the image was acquired from. All the images were captured by a perspective camera (visual images) and a Kinect device (depth images) mounted on a mobile robot (see Fig. <ref type="figure" coords="3,400.26,365.50,4.43,8.74">1</ref>) within an office environment. In one hand, and as opposite to previous editions of the challenge, a single task was considered this time, therefore no sub-tasks were defined. All the room and object categories included in the test sequence were previously seen during training. On the other hand, the use of the temporal continuity of the test sequence was allowed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Data</head><p>In the 2013 edition of the RobotVision challenge the O-VIDA Robot Vision dataset was used. This dataset consists of different training, validation and test sequences of depth and visual images acquired within an indoor environment: a department building at the University of Alicante. Visual images were stored in PNG format and depth ones in PCD. Every image in the dataset was manually labelled with its corresponding room category/class and with a list of eight different objects to appear or not within it. The 10 different room categories are: corridor, hall, professorOffice, studentOffice, technicalRoom, toilet, secretary, visioconference, elevator area and warehouse. The 8 different objects are: extinguisher, computer, chair, printer, urinal, screen, trash and fridge. From this dataset two different labelled sequences were selected for training, one labelled sequence for validation, and one unlabelled sequence for testing. The frequency distribution for room categories in the training, validation and test sequences are depicted in Table <ref type="table" coords="4,275.48,607.95,3.87,8.74" target="#tab_0">1</ref>.</p><p>It can be observed that in all sequences, Corridor is the class with higher number of frames. This is because most of the space of the University of Alicante building, suitable for robot navigation, belongs to several corridors. This  The differences between all the room categories can be observed in Figure <ref type="figure" coords="5,472.84,644.16,3.87,8.74">3</ref>, where a single visual image for each of the 10 room categories is shown. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Performance Evaluation</head><p>The runs submitted for each participant were compared and sorted according the score assigned to each submission. Every submission consisted of the room category assigned to each test image and the corresponding list of the 8 detected/nondetected objects within that image. As we already mentioned above, the number of times a specific object appears in an image was not relevant to compute the score. The score was computed using the rules shown in Table <ref type="table" coords="7,404.69,201.30,3.87,8.74" target="#tab_2">3</ref>. Due to the fact that wrong room classifications and/or wrong object detections account negatively to the score, participants were allowed to not providing such information, in which case the score is not affected. The final score was computed as the sum of the score obtained for each individual test frame. According to the test set released the maximum score to be obtained was 7030 points. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Additional information provided by the organization</head><p>As in the previous edition <ref type="bibr" coords="7,250.25,471.46,9.96,8.74" target="#b4">[5]</ref>, we proposed the use of several techniques for features extraction (PHOG and NARF) and cue integration (OBSCURE). Thanks to the use of these techniques, participants could focus on the development of new features while using the proposed method for cue integration or vice versa. Information about the point cloud library <ref type="bibr" coords="7,320.96,519.28,15.50,8.74" target="#b13">[14]</ref> and a basic technique for taking advantage of the temporal continuity<ref type="foot" coords="7,296.45,529.66,3.97,6.12" target="#foot_1">3</ref> was also provided. In this regard, and in order to evaluate the performance of the baseline classification system (which was built using uniquely the provided techniques, briefly described below) we submitted a single runs. The results obtained with such proposal <ref type="bibr" coords="7,417.76,567.10,10.52,8.74" target="#b3">[4]</ref> can be considered as baseline results, which all the participants were expected to improve.</p><p>Visual Features PHOG features are histogram-based global features that combine structural and statistical approaches. Other descriptors similar to PHOG that could also be used are: Sift-based Pyramid Histogram Of visual Words (PHOW) <ref type="bibr" coords="8,179.16,118.99,9.96,8.74" target="#b0">[1]</ref>, Pyramid histogram of Local Binary Patterns (PLBP) <ref type="bibr" coords="8,443.52,118.99,9.96,8.74" target="#b5">[6]</ref>, Self-Similarity-based PHOW (SS-PHOW) <ref type="bibr" coords="8,303.86,130.95,14.61,8.74" target="#b14">[15]</ref>, and Compose Receptive Field Histogram (CRFH) <ref type="bibr" coords="8,208.69,142.90,9.96,8.74" target="#b2">[3]</ref>.</p><p>Depth Features NARF features is a novel descriptor technique that has been included in the point cloud library <ref type="bibr" coords="8,289.27,187.57,14.61,8.74" target="#b13">[14]</ref>. The number of descriptors that can be extracted from a range image is not fixed, in the same manner as SIFT points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cue Integration</head><p>The algorithm proposed for cue integration was the Online-Batch Strongly Convex mUlti keRnel lEarning (OBSCURE) <ref type="bibr" coords="8,412.57,244.19,9.96,8.74" target="#b8">[9]</ref>. This SVMbased multi-class learning algorithm obtains state-of-the-art performance in a considerably lower training time. Other algorithm that could be used was the Online Independent Support Vector Machines <ref type="bibr" coords="8,327.62,280.05,10.52,8.74" target="#b7">[8]</ref> that, in comparison with SVM, dramatically reduces learning time and space requirements at the price of a negligible loss in accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Participation</head><p>In 2013, 39 participants registered to the Robot Vision task but only 6 submitted, at least, one run accounting for a total of 16 different runs. These participants were:</p><p>-NUDT: National University of Defense Technology, Changsha, China.</p><p>-MIAR ICT: Beijing, China.</p><p>-MICA: Hanoi university of Science and Technology, Hanoi, Vietnam -REGIM: University of Sfax National School of Engineers, Tunisia -GRAM:University of Alcala de Henares, Spain -SIMD: University of Castilla-La Mancha, Albacete, Spain.</p><p>• Out of competition organizers contribution using proposed techniques</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>This section presents the results of the Robot Vision task of ImageCLEF 2013.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overall Results</head><p>The scores obtained by all the submitted runs are shown in Table <ref type="table" coords="8,426.89,608.30,3.87,8.74" target="#tab_3">4</ref>. The maximum score that could be achieved was 7030 and the winner (MIAR ICT) obtained a score of 6033.5 points. NUDT and SIMD teams ranked second and third respectively and their score was higher than 71% of the maximum score (the one obtained with the baseline system, SIMD result in the table). * SIMD organizers submission was out-of-competition, it was provided to be considered a baseline score. The organizers only used the techniques proposed in the webpage of the challenge<ref type="foot" coords="9,306.68,380.01,3.97,6.12" target="#foot_2">4</ref> . Concretely, PHOW features were extracted from visual images and then, a Support Vector Machine was trained using DOGMA <ref type="bibr" coords="9,203.94,405.50,9.96,8.74" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Detailed Results</head><p>Here we present a deeper analysis of the best submission for each participant group. We have computed separately the score for the class classification and the recognition sub-problems. All these results can be seen in Table <ref type="table" coords="9,411.81,481.04,4.98,8.74" target="#tab_4">5</ref> and Fig. <ref type="figure" coords="9,458.55,481.04,3.87,8.74" target="#fig_2">5</ref>. As it can be observed, in one hand, that MIAR ICT and NUDT groups obtained similar scores, with better results for room classification than for object recognition. On the other hand, REGIM and MICA proposals ranked better for object recognition than for room classification.</p><p>We have also analysed the specific performance for the different room categories and objects. For each room class and object considered, we have computed the percentage of right and wrong classifications. We also have computed the percentage of times of not providing information for room classes or objects. All these data can be observed in Fig. <ref type="figure" coords="9,292.86,588.81,4.98,8.74" target="#fig_3">6</ref> and Fig. <ref type="figure" coords="9,343.04,588.81,4.98,8.74" target="#fig_4">7</ref> for room classes and objects, respectively.</p><p>From the reported results, we can state that Hall and Elevator Area are the most challenging rooms, while Corridor is the easiest one. The number of training frames containing these classes can be one of the most important reasons for this  It can be noticed that all the objects are managed properly by the participant proposals. Urinal was the object that obtained the highest percentage of right detections, while Trash obtained the lowest one. It should be pointed out that, for all the objects (see Table <ref type="table" coords="10,292.45,469.49,3.87,8.74" target="#tab_1">2</ref>), the appearance ratio in less than 30%. Classifying all test frames as "there are no objects in the scene" would obtain a high positive score, especially for Urinal, Fridge or Screen. Chair and Trash could be considered the most challenging test objects because their appearance ratio is higher than for the rest of the objects. There are two possible reasons explaining that participants obtained better results for Chair than for Trash: (1) trashes are considerably smaller than chairs, and (2) trashes can appear in most of the room categories while chairs are only present in 6 rooms (TechnicalRoom, ProfessorOffice, StudentOffice, Secretary, VisioConference and Warehouse).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>In this paper the overview of the 2013 edition of the Robot Vision task at ImageCLEF has been presented. We have described the task, which had slightly variations from previous editions, and a detailed analysis of the results obtained for each run submitted by the participants.  As a novelty for this edition, we have introduced the additional problem to recognize specific objects that can appear within an image. That provides and additional component to the classical place classification problem, turning it into a multimodal classification problem.</p><p>According to the obtained results we can conclude that the introduction of the object recognition task was not as challenging as we expected: most of the participants were able to identify those objects properly. With respect to the scores obtained the different runs, almost half of them improved the baseline results provided by the organizers, obtaining score higher than the 80% of the maximum score.</p><p>For future editions we plan to continue in the direction of including new challenging variations to the problem of scene classification problem. In particular, as the next step forward we will focus on providing the number of occurrences a specific object appears in an image.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,194.71,596.81,225.94,7.89;3,276.56,406.61,62.25,175.42"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>Fig. 1. Mobile robot platform used for data acquisition.</figDesc><graphic coords="3,276.56,406.61,62.25,175.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,134.77,356.85,345.83,7.89;6,134.77,367.84,144.47,7.86;6,222.95,279.78,83.00,62.25"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig. 3. Examples of visual images (one for of the 10 different categories) from the O-VIDA Robot Vision 2013 dataset</figDesc><graphic coords="6,222.95,279.78,83.00,62.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="10,217.66,376.32,176.97,7.89;10,152.06,224.46,311.24,137.09"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Best results obtained for each group</figDesc><graphic coords="10,152.06,224.46,311.24,137.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="11,168.09,265.05,279.19,7.89;11,134.77,115.83,345.83,134.44"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Percentage of hits, fails and unknowns for all room categories</figDesc><graphic coords="11,134.77,115.83,345.83,134.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="11,185.52,444.84,244.33,7.89;11,134.77,295.96,345.83,134.11"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Percentage of hits, fails and unknowns for all objects</figDesc><graphic coords="11,134.77,295.96,345.83,134.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,134.77,115.91,345.83,320.04"><head>Table 1 .</head><label>1</label><figDesc>Frequency distribution of room categories for the training, validation and test sequences.</figDesc><table coords="5,195.78,147.67,220.73,140.57"><row><cell></cell><cell></cell><cell cols="2">Number of frames</cell><cell></cell></row><row><cell cols="5">Room Category Training 1 Training 2 Validation Test</cell></row><row><cell>Corridor</cell><cell>891</cell><cell>1262</cell><cell>764</cell><cell>1317</cell></row><row><cell>Hall</cell><cell>103</cell><cell>228</cell><cell>000</cell><cell>297</cell></row><row><cell>ProfessorOffice</cell><cell>124</cell><cell>192</cell><cell>200</cell><cell>222</cell></row><row><cell>StudentOffice</cell><cell>155</cell><cell>276</cell><cell>282</cell><cell>318</cell></row><row><cell>TechnicalRoom</cell><cell>136</cell><cell>281</cell><cell>214</cell><cell>240</cell></row><row><cell>Toilet</cell><cell>121</cell><cell>242</cell><cell>188</cell><cell>198</cell></row><row><cell>Secretary</cell><cell>098</cell><cell>195</cell><cell>181</cell><cell>201</cell></row><row><cell>VisioConference</cell><cell>149</cell><cell>300</cell><cell>000</cell><cell>306</cell></row><row><cell>Warehouse</cell><cell>070</cell><cell>166</cell><cell>000</cell><cell>127</cell></row><row><cell>ElevatorArea</cell><cell>100</cell><cell>174</cell><cell>040</cell><cell>289</cell></row><row><cell>All</cell><cell>1947</cell><cell>3316</cell><cell>1869</cell><cell>3515</cell></row></table><note coords="5,134.77,319.62,345.83,8.74;5,134.77,331.58,345.83,8.74;5,134.77,343.53,345.83,8.74;5,134.77,355.49,345.83,8.74;5,134.77,367.44,345.82,8.74;5,134.77,379.40,345.82,8.74;5,134.77,391.35,345.82,8.74;5,134.77,403.31,345.82,8.74;5,134.77,415.26,345.83,8.74;5,134.77,427.22,86.95,8.74"><p><p><p>situation makes it easier the classification of test frames as Corridor while other classes as Warehouse or Toilet are more challenging. The validation sequence was released for providing participants an additional sequence for testing their preliminary proposals. It was also released for preventing the extreme lighting conditions present in the test sequence. The validation sequence was acquired just in the first floor of the building and it does not contains any frame for three rooms: Hall, VisioConference and Warehouse. The frequency distribution for object categories in the training, validation and test sequences are depicted in Table</p>2</p>, where can be observed that there are no presence of Screens in the validation sequence.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,134.77,462.32,345.82,150.40"><head>Table 2 .</head><label>2</label><figDesc>Frequency distribution of object presences or lacks for the training, validation and test sequences.</figDesc><table coords="5,163.51,494.07,285.27,118.65"><row><cell></cell><cell></cell><cell cols="2">Number of presences / lacks</cell></row><row><cell cols="2">Room Category Training 1</cell><cell>Training 2</cell><cell>Validation</cell><cell>Test</cell></row><row><cell>Extinguisher</cell><cell cols="4">259 / 1688 529 / 2787 286 / 1583 520 / 2995</cell></row><row><cell>Computer</cell><cell cols="4">289 / 1658 466 / 2850 416 / 1453 473 / 3042</cell></row><row><cell>Chair</cell><cell cols="4">470 / 1477 767 / 2549 567 / 1302 889 / 2626</cell></row><row><cell>Printer</cell><cell cols="4">210 / 1737 292 / 3024 255 / 1614 279 / 3236</cell></row><row><cell>Urinal</cell><cell cols="4">054 / 1893 110 / 3206 070 / 1799 090 / 3425</cell></row><row><cell>Screen</cell><cell cols="4">081 / 1866 190 / 3126 000 / 1869 151 / 3364</cell></row><row><cell>Trash</cell><cell cols="4">406 / 1541 451 / 2865 253 / 1616 662 / 2853</cell></row><row><cell>Fridge</cell><cell cols="4">057 / 1890 104 / 3212 099 / 1770 114 / 3401</cell></row><row><cell>All</cell><cell cols="4">1826 / 13750 2909 / 23610 1946 / 13006 3178 / 24942</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,172.01,293.61,268.25,106.42"><head>Table 3 .</head><label>3</label><figDesc>Rules used to calculate the final score for a test frame</figDesc><table coords="7,172.01,312.67,268.25,87.37"><row><cell>Class / Room Category</cell><cell></cell></row><row><cell>Room class/category correctly classified</cell><cell>+1.0 points</cell></row><row><cell>Room class/category wrongly classified</cell><cell>-0.5 points</cell></row><row><cell>Room class/category not classified</cell><cell>+0.0 points</cell></row><row><cell>Object Recognition</cell><cell></cell></row><row><cell cols="2">For each correctly classified object whitin the frame +0.125 points</cell></row><row><cell>For each misclassified object whitin the frame</cell><cell>-0.125 points</cell></row><row><cell>For each not classified object whitin the frame</cell><cell>+0.000 points</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,134.77,115.91,345.82,213.62"><head>Table 4 .</head><label>4</label><figDesc>Overall ranking of the runs submitted by the participant groups to the 2013 Robot Vision task</figDesc><table coords="9,219.33,145.93,173.63,183.60"><row><cell cols="4">Rank Group Name Score % Max. Score</cell></row><row><cell>1</cell><cell cols="2">MIAR ICT 6033.500</cell><cell>85.83</cell></row><row><cell>2</cell><cell cols="2">MIAR ICT 5924.250</cell><cell>84.27</cell></row><row><cell>3</cell><cell cols="2">MIAR ICT 5924.250</cell><cell>84.27</cell></row><row><cell>4</cell><cell cols="2">MIAR ICT 5867.500</cell><cell>83.46</cell></row><row><cell>5</cell><cell cols="2">MIAR ICT 5867.000</cell><cell>83.46</cell></row><row><cell>6</cell><cell>NUDT</cell><cell>5722.500</cell><cell>81.40</cell></row><row><cell>7</cell><cell>SIMD*</cell><cell>5004.750</cell><cell>71.19</cell></row><row><cell>8</cell><cell cols="2">REGIM 4368.250</cell><cell>65.98</cell></row><row><cell>9</cell><cell>MICA</cell><cell>4479.875</cell><cell>63.73</cell></row><row><cell>10</cell><cell cols="2">REGIM 3763.750</cell><cell>53.54</cell></row><row><cell>11</cell><cell>MICA</cell><cell>3316.125</cell><cell>47.17</cell></row><row><cell>12</cell><cell>MICA</cell><cell>2680.625</cell><cell>38.13</cell></row><row><cell>13</cell><cell>GRAM</cell><cell>-487.000</cell><cell>&lt;0.00</cell></row><row><cell>14</cell><cell>GRAM</cell><cell>-497.000</cell><cell>&lt;0.00</cell></row><row><cell>15</cell><cell>GRAM</cell><cell>-497.000</cell><cell>&lt;0.00</cell></row><row><cell>16</cell><cell>NUDT</cell><cell>-866.250</cell><cell>&lt;0.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,191.29,115.91,229.71,94.81"><head>Table 5 .</head><label>5</label><figDesc>Detailled ranking of the best runs submitted</figDesc><table coords="10,191.29,136.71,229.71,74.02"><row><cell cols="5">Rank Group Name Score Class Score Object Score Total</cell></row><row><cell>1</cell><cell>MIAR ICT</cell><cell>3168.5</cell><cell>2865.000</cell><cell>6033.500</cell></row><row><cell>2</cell><cell>NUDT</cell><cell>3002.0</cell><cell>2720.500</cell><cell>5722.500</cell></row><row><cell>3</cell><cell>SIMD*</cell><cell>1988.0</cell><cell>3016.750</cell><cell>5004.750</cell></row><row><cell>4</cell><cell>REGIM</cell><cell>2223.5</cell><cell>2414.750</cell><cell>4368.250</cell></row><row><cell>5</cell><cell>MICA</cell><cell>2063.0</cell><cell>2416.875</cell><cell>4479.875</cell></row><row><cell>6</cell><cell>GRAM</cell><cell>-487.0</cell><cell>0.000</cell><cell>-487.000</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="2,144.73,656.80,124.93,7.86"><p>http://www.clef-initiative.eu//</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="7,144.73,656.80,130.86,7.86"><p>http://imageclef.org/2012/robot</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="9,144.73,656.80,153.39,7.86"><p>http://www.imageclef.org/2013/robot</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="12,142.96,142.59,337.64,7.86;12,151.52,153.55,329.07,7.86;12,151.52,164.51,20.99,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,315.86,142.59,164.74,7.86;12,151.52,153.55,36.36,7.86">Image classification using random forests and ferns</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Munoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,208.09,153.55,185.11,7.86">International Conference on Computer Vision</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,175.46,337.63,7.86;12,151.52,186.42,329.07,7.86;12,151.52,197.38,320.88,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,373.24,186.42,107.36,7.86;12,151.52,197.38,131.85,7.86">Imageclef 2013: the vision, the data and the open challenges</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zellhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">Martinez</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Varea</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Cazorla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,302.87,197.38,58.51,7.86">Springer LNCS</title>
		<meeting><address><addrLine>CLEF</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,208.34,337.64,7.86;12,151.52,219.30,273.89,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,274.81,208.34,205.78,7.86;12,151.52,219.30,142.30,7.86">Object recognition using composed receptive field histograms of higher dimensionality</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Linde</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,313.49,219.30,83.98,7.86">Proc. ICPR. Citeseer</title>
		<meeting>ICPR. Citeseer</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,230.26,337.64,7.86;12,151.52,241.22,329.07,7.86;12,151.52,252.18,150.62,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,426.10,230.26,54.49,7.86;12,151.52,241.22,213.23,7.86">Baseline multimodal place classifier for the 2012 robot vision task</title>
		<author>
			<persName coords=""><forename type="first">Jesus</forename><surname>Martinez-Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ismael</forename><surname>Garcia-Varea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,384.45,241.22,96.13,7.86;12,151.52,252.18,91.65,7.86">CLEF (Online Working Notes/Labs/Workshop)</title>
		<imprint>
			<publisher>CLEF</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,263.14,337.64,7.86;12,151.52,274.09,188.47,7.86;12,358.92,274.09,121.67,7.86;12,151.52,285.05,120.30,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,442.93,263.14,37.66,7.86;12,151.52,274.09,184.62,7.86">Overview of the imageclef 2012 robot vision task</title>
		<author>
			<persName coords=""><forename type="first">Jesus</forename><surname>Martinez-Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ismael</forename><surname>Garcia-Varea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,375.42,274.09,105.17,7.86;12,151.52,285.05,91.65,7.86">CLEF (Online Working Notes/Labs/Workshop)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,296.01,337.64,7.86;12,151.52,306.97,329.07,7.86;12,151.52,317.93,84.02,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,341.38,296.01,139.21,7.86;12,151.52,306.97,193.68,7.86">Gray scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mäenpää</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,355.77,306.97,99.38,7.86">Computer Vision-ECCV</title>
		<imprint>
			<date type="published" when="2000">2000. 2000</date>
			<biblScope unit="page" from="404" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,328.89,337.63,7.86;12,151.52,339.85,148.71,7.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="12,204.71,328.89,182.14,7.86">Dogma: a matlab toolbox for online learning</title>
		<author>
			<persName coords=""><surname>Orabona</surname></persName>
		</author>
		<ptr target="http://dogma.sourceforge.net" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,350.81,337.64,7.86;12,151.52,361.77,329.07,7.86;12,151.52,372.72,20.99,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,397.14,350.81,83.45,7.86;12,151.52,361.77,215.61,7.86">Indoor place recognition using online independent support vector machines</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Orabona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Castellini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sandini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,385.57,361.77,48.09,7.86">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,383.68,337.64,7.86;12,151.52,394.64,317.69,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,302.91,383.68,177.68,7.86;12,151.52,394.64,33.81,7.86">Online-Batch Strongly Convex Multi Kernel Learning</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Orabona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,205.16,394.64,203.69,7.86">Proc. of Computer Vision and Pattern Recognition</title>
		<meeting>of Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,405.60,337.97,7.86;12,151.52,416.56,329.07,7.86;12,151.52,427.52,84.02,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,332.33,405.60,148.26,7.86;12,151.52,416.56,71.85,7.86">Overview of the imageclef@ icpr 2010 robot vision track</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pronobis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,233.32,416.56,243.11,7.86">Recognizing Patterns in Signals, Speech, Images and Videos</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="171" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,438.48,337.98,7.86;12,151.52,449.44,250.33,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,390.39,438.48,90.21,7.86;12,151.52,449.44,68.25,7.86">The robot vision track at imageclef 2010</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pronobis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fornoni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Hi Christensesn</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,228.06,449.44,120.53,7.86">Working Notes of ImageCLEF</title>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,460.40,337.98,7.86;12,151.52,471.35,329.07,7.86;12,151.52,482.31,329.07,7.86;12,151.52,493.27,56.09,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,312.67,460.40,84.39,7.86">The robot vision task</title>
		<author>
			<persName coords=""><forename type="first">Andrzej</forename><surname>Pronobis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,410.83,471.35,69.76,7.86;12,151.52,482.31,176.60,7.86">ImageCLEF, volume 32 of The Information Retrieval Series</title>
		<editor>
			<persName><forename type="first">Henning</forename><surname>Muller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="185" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,504.23,337.98,7.86;12,151.52,515.19,329.07,7.86;12,151.52,526.15,329.07,7.86;12,151.52,537.11,329.07,7.86;12,151.52,548.07,329.07,7.86;12,151.52,559.03,20.99,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,353.64,504.23,126.95,7.86;12,151.52,515.19,47.30,7.86">Overview of the clef 2009 robot vision track</title>
		<author>
			<persName coords=""><forename type="first">Andrzej</forename><surname>Pronobis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,461.75,526.15,18.84,7.86;12,151.52,537.11,271.48,7.86">Multilingual Information Access Evaluation II. Multimedia Experiments</title>
		<title level="s" coord="12,161.10,548.07,137.15,7.86">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gareth</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jayashree</forename><surname>Kalpathy-Cramer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Theodora</forename><surname>Tsikrika</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin / Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6242</biblScope>
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,569.98,337.98,7.86;12,151.52,580.94,329.07,7.86;12,151.52,591.90,68.22,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,318.01,569.98,144.96,7.86">3d is here: Point cloud library (pcl)</title>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><surname>Radu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steve</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Cousins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,151.52,580.94,300.04,7.86">Robotics and Automation (ICRA), 2011 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,602.86,337.97,7.86;12,151.52,613.82,329.07,7.86;12,151.52,624.78,109.84,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,276.68,602.86,203.91,7.86;12,151.52,613.82,23.52,7.86">Matching local self-similarities across images and videos</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,195.28,613.82,256.15,7.86;12,151.52,624.78,36.03,7.86">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>CVPR&apos;07</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
