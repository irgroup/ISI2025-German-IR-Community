<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,161.88,115.96,291.60,12.62">NovaSearch on medical ImageCLEF 2013</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,194.53,153.67,61.29,8.74"><forename type="first">André</forename><surname>Mourão</surname></persName>
							<email>a.mourao@campus.fct.unl.pt</email>
							<affiliation key="aff0">
								<orgName type="department">Faculdade de Ciências e Tecnologia</orgName>
								<orgName type="institution">Universidade Nova de Lisboa</orgName>
								<address>
									<settlement>Caparica</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,264.32,153.67,64.71,8.74"><forename type="first">Flávio</forename><surname>Martins</surname></persName>
							<email>flaviomartins@acm.org</email>
							<affiliation key="aff0">
								<orgName type="department">Faculdade de Ciências e Tecnologia</orgName>
								<orgName type="institution">Universidade Nova de Lisboa</orgName>
								<address>
									<settlement>Caparica</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,351.72,153.67,69.11,8.74"><forename type="first">João</forename><surname>Magalhães</surname></persName>
							<email>jm.magalhaes@fct.unl.pt</email>
							<affiliation key="aff0">
								<orgName type="department">Faculdade de Ciências e Tecnologia</orgName>
								<orgName type="institution">Universidade Nova de Lisboa</orgName>
								<address>
									<settlement>Caparica</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,161.88,115.96,291.60,12.62">NovaSearch on medical ImageCLEF 2013</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">83427289CB3B4BED4D97CB2D5334FFE6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>medical retrieval</term>
					<term>case-based retrieval</term>
					<term>multimodal fusion</term>
					<term>medical modality classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article presents the participation of the Center of Informatics and Information Technology group CITI in medical ImageCLEF 2013. This is our first participation and we submitted runs on the modality classification task, the ad-hoc image retrieval task and case retrieval task. We are developing a system to integrate textual and visual retrieval into a framework for multimodal retrieval. Our approach for multimodal case retrieval achieved best global performance using Segmented (6×6 grid) Local Binary Pattern (LBP) histograms and Segmented HSV histograms for images and Lucene with query expansion (using the first top 3 results). In modality classification we achieved one of the largest MAP gains in the multimodal classification task, resulting in the third best team result.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>ImageCLEF 2013 AMIA: Medical task is the CLEF benchmark focused on the retrieval and classification of medical images and articles from PubMed. This is our first participation on ImageCLEF and the Medical track in particular, so we tried to design a system to participate on every task (excluding Compound figure separation). However we delved more in the case-based retrieval task and results lists fusion.</p><p>Our system is divided in image retrieval, textual retrieval and results fusion. For image retrieval, we focused on top performing features from previous editions <ref type="bibr" coords="1,172.31,554.08,15.50,8.74" target="#b9">[10]</ref> (CEDD <ref type="bibr" coords="1,228.33,554.08,9.96,8.74" target="#b2">[3]</ref>, FCTH <ref type="bibr" coords="1,277.15,554.08,9.96,8.74" target="#b3">[4]</ref>, Local Binary Pattern histograms <ref type="bibr" coords="1,442.52,554.08,9.96,8.74" target="#b0">[1]</ref>, color histograms).</p><p>For text retrieval, we used Lucene<ref type="foot" coords="1,296.86,576.46,3.97,6.12" target="#foot_0">1</ref> from the Apache project paired with our implementation of the BM25L <ref type="bibr" coords="1,275.33,589.99,10.52,8.74" target="#b7">[8]</ref> ranking function for Lucene. For modality classification, we combined text and image descriptors (early fusion) and performed classification using Vowpal Wabbit<ref type="foot" coords="1,319.25,612.32,3.97,6.12" target="#foot_1">2</ref> . Our training dataset included only the images provided on the 2013 edition <ref type="bibr" coords="1,319.04,625.85,9.96,8.74" target="#b6">[7]</ref>. We did not perform any type of dataset augmentation. On the case-based retrieval task, we also tested a variety of late fusion approaches, and came up with a variant of Reciprocal Rank (RR) <ref type="bibr" coords="2,134.77,142.90,15.50,8.74" target="#b10">[11]</ref> we named Inverse Square Rank (ISR). Our fusion method provided the best performance for our case-based image and textual runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Techniques</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Text retrieval</head><p>In text retrieval, article text is indexed using Lucene and retrieved using the BM25L <ref type="bibr" coords="2,170.36,231.26,10.52,8.74" target="#b7">[8]</ref> retrieval function. We expanded the initial query with preferred and alternative terms sourced from a SKOS formatted version of MeSH using Lucene-SKOS. This improves precision metrics at the top ranks, so we then exploit these top documents to perform pseudo-relevance feedback.</p><p>The indexed fields depend on the task: For image retrieval, we achieved good results indexing and searching only on the image captions, title and abstract. For case retrieval, we indexed and searched on the full text (all chapters including image captions), abstract and title. We ran pseudo-relevance feedback using the top 3 results retrieved using the initial query. We added a maximum of 25 new query terms to the initial query. We purged any candidate words that did not appear in a minimum of 5 documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Visual retrieval</head><p>For visual retrieval, we extracted a set of descriptors effective in medical images retrieval (CEDD, FCTH, Local Binary Pattern (LBP) histograms and color histograms). We used this image features in pairs. For LPB and color histograms, we extracted both descriptors for 6×6 image grid and concatenated the results. For CEDD and FCTH, we extracted and concatenated both feature descriptors.</p><p>CEDD (Color and Edge Directivity Descriptor) expresses color and edge directivity information from the image into a compact descriptor (54 bytes per image). It combines "real color" histograms from the HSV color space (e.g. bins Light Magenta, Dark Green) with MPEG-7 Edge Histogram Descriptor. FCTH (Fuzzy Color and Texture Histogram) combines the same color histograms as CEDD with texture information extracted with multiple energy wavelets using fuzzy logic techniques. Color histograms are histograms of the individual components of the HSV color space. Local Binary Patters are texture descriptors based on thresholding a pixel with its neighbors to detect texture variations. The thresholded values are concatenated into an histogram to represent image texture. The features of all images in the corpus are stored in a FLANN <ref type="bibr" coords="2,455.86,572.43,10.52,8.74" target="#b8">[9]</ref> L 2 index.</p><p>The image retrieval results are sorted by their similarity, with the score being the normalized inverse of the L 2 distances between the query image and the indexed result images. For case-base retrieval, an additional step must be performed: the image id (IRI), must be converted into a document id (DOI) ( </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Results fusion</head><p>Result fusion aims at combining ranked lists from multiple sources into a single combined ranked list. Consider these two use cases: combine the results from queries with multiple images and combine the results from text and images queries.</p><p>There are two main approaches for late fusion: score based and rank based. Score based approaches (CombSUM, CombMAX and CombMNZ) combine the normalized scores given by the individual searches (e.g. visual and textual) as a basis to the create the new ranked list. The studied variant that achieves the best performance <ref type="bibr" coords="3,192.43,388.47,10.52,8.74" target="#b1">[2]</ref> is CombMNZ, but ranked based fusion is gaining momentum, and can outperform score based fusion under most conditions <ref type="bibr" coords="3,415.61,400.43,10.52,8.74" target="#b4">[5,</ref><ref type="bibr" coords="3,427.79,400.43,7.01,8.74" target="#b5">6]</ref>. For each document i, the score after fusion can be computed as:</p><formula xml:id="formula_0" coords="3,233.31,430.21,247.29,48.08">combSUM(i) = N (i) k=1 S k (i),<label>(1) combMAX</label></formula><formula xml:id="formula_1" coords="3,274.06,469.55,206.53,9.65">(i) = max(S), ∀S ⊂ D i ,<label>(2)</label></formula><formula xml:id="formula_2" coords="3,226.03,485.48,254.56,8.74">combMNZ(i) = N (i) × combSUM(i),<label>(3)</label></formula><p>where S k (i) is the score of the i document on the k result list. N(i) refers to the number of times a document appears on a results list. A result list k does not contain all documents. Documents with a zero score or a very high rank can be safely ignored. Thus, N(i) varies between 0 (the document i does not appear on any list) and the total number of results list (the document i appears on all lists). For example, in our experiments, there are two results lists: one for visual search and other for textual search, limited to 1000 results each.</p><p>Rank based fusion methods consider the inverse of the rank of each document in each one of the individual lists as the score. Reciprocal Rank and Reciprocal Rank Fusion are the two methods we evaluated:</p><formula xml:id="formula_3" coords="3,264.83,636.85,215.76,31.41">RR(i) = N (i) k=1 1 R k (i) ,<label>(4)</label></formula><formula xml:id="formula_4" coords="4,225.39,117.30,106.16,31.41">RRF(i) = N (i) k=1 1 h + R k (i)</formula><p>, with h = 60. <ref type="bibr" coords="4,467.86,128.57,12.73,8.74" target="#b4">(5)</ref> where R k (i) is the rank of document i on the k rank.</p><p>After analyzing both score and rank based approaches, we combined elements from both to improve precision. Inverted Squared Rank (ISR) combines the inverse rank approaches of RR and RRF (using the squared rank to improve precision at top results) with the frequency component of combMNZ (results that appear on multiple lists are boosted):</p><formula xml:id="formula_5" coords="4,245.47,237.30,230.88,31.41">ISR(i) = N (i) × N (i) k=1 1 R k (i) 2 . (<label>6</label></formula><formula xml:id="formula_6" coords="4,476.35,248.57,4.24,8.74">)</formula><p>3 Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Modality classification</head><p>In the modality classification task, we used the CEDD and FCTH descriptors and (stemmed) text from the corresponding caption and article title. In the image and textual runs, we used the corresponding descriptors from all images in the provided training dataset to create a model based on stochastic gradient descent with Vowpal Wabbit. These runs correspond to the CEDD FCTH for images and words from text in Table <ref type="table" coords="4,298.88,392.99,3.87,8.74" target="#tab_0">1</ref>.</p><p>The multimodal run concatenates the descriptors described above and performs the stochastic gradient descent with Vowpal Wabbit with the combined descriptors (CEDD, FCTH, caption and title words). This run correspond to the All in Table <ref type="table" coords="4,190.88,440.85,3.87,8.74" target="#tab_0">1</ref>.</p><p>The dataset provided as training data for modality classification was unbalanced in the quantity of images per class. For example, the "Compound or multipane images" (COMP) category contains over 1,105 training images while the "PET" (DRPE) category contains 16 training images . Also, images in the COMP may not be visually distinct; they consist on the combination of images from other categories. Thus, we have decided to send runs where we ignored the COMP in training and classification. These run correspond to the runs ended by noComb in Table <ref type="table" coords="4,227.21,536.53,3.87,8.74" target="#tab_0">1</ref>.</p><p>Regarding the performance of our algorithm, we were able to be the 3 rd best team overall with our multimodal All run, classifying 72.92% of the images correctly. This is only 8.76% bellow the best run. Our visual run classified 57.62% of the images correctly (23.17% behind the best run) and our best textual run classified 62.35% of the images correctly (1.82% behind the best run). The most interesting fact is the big improvement of the results from single modality runs in the multimodal run. We improved our best single modality run by over 10%, while the best team only improved their best run by 0.89% in the multimodal approach. Our noComb approaches did not perform well, as the testing dataset contained a lot of COMP images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Case retrieval</head><p>For case retrieval, we submitted one visual run (with segmented LPB Histograms and HSV Color Histograms) and two textual runs (one with MeSH expansion and one without expansion). The run with MeSH expansion (with MSH on the run name) outperformed the run without expansion in all metrics, with a special emphasis on MAP (12% increase) and P@10 (11% increase). We achieved our best result in the expanded textual run, with a MAP 22%, GM-MAP 11.8% and P@10 26%, very close to the best textual run (MAP: 24%, GM-MAP: 11.6% and P@10: 27%). The visual run achieved the best result in class, outperforming the second best result by a factor of 10. Our run achieved a MAP of 3% and P@10 of 4%. Our best multimodal run (using ISR) also achieved the best result in class with a MAP of 16% and P@10 of 18%. The combMNZ based run achieved much worse results (MAP: 8% and P@10: 14%), following our idea that rank-based fusion is better that score-based fusion in multimodal retrieval. Although these results are worse that the textual only results, our rank-based fusion algorithms improved existing algorithm by a small margin. Fusion In addition to the submitted runs, we compared the performance of the fusion algorithms using the best textual and visual runs for case-based retrieval. Performance was evaluated using trec eval and the provided relevance judgments (Table <ref type="table" coords="6,166.04,154.86,3.87,8.74" target="#tab_2">3</ref>). With our data, rank-based approaches outperformed score based approaches by a factor of 2. One of the reasons is the scoring differencs between text and images. Even though both visual and text scores have the same normalization (the [0...1] interval), the distribution of the results in the score space is different. Rank based approaches can handle multi-modality better, because the scores are ignored.</p><p>Regarding the differences between RR, RRF and ISR: ISR performed as well or better in our experiments (Table <ref type="table" coords="6,289.17,251.53,4.98,8.74" target="#tab_2">3</ref> and<ref type="table" coords="6,315.58,251.53,4.43,8.74" target="#tab_3">4</ref>) in most measures, with a significant performance boost on P@10 for case-based retrieval. The polynomial component promotes top ranking results to the top of the list, offering a better precision at top results (e.g. P@10). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Image retrieval</head><p>In ad-hoc image retrieval, we submitted one visual run (with segmented LPB Histograms and HSV Color Histograms) and two textual runs (one with MeSH expansion and one without expansion). As with case retrieval, the run with MeSH expansion (FCT SOLR BM25L MSH) outperformed the run without expansion (FCT SOLR BM25L) in all metrics, although to a lesser degree: MAP increased 5% and P@10 increased 10%. We achieved our best result in the expanded textual run, with a MAP of 23% and P@10 of 30%, well behind the best textual (and best overall) run with a MAP of 32% and P@10 of 39%. As expected, our visual runs performed worst with a MAP of 0.7% and P@10 of 3%, about half of the performance of the best visual run (MAP: 1.9% and P@10: 6%).</p><p>After the competition, we tested the same fusion algorithms as with casebased retrieval for multimodal retrieval. Our conclusion are similar to the ones discussed in the case-based retrieval section: rank based approaches are much better for our data, with ISR and RR being the best for fusion. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>We would like to emphasize our results in the visual and multimodal case retrieval tracks, where we achieved the best MAP and bpref. Our fusion algorithm (ISR) achieved slightly better performance than existing fusion algorithms and helped us achieving the best result on the multimodal case retrieval track.</p><p>Our results in the modality classification are also noteworthy. In the multimodal run, we increased the best single modality result by 17%, ending with the third best team result.</p><p>We were not able to submit all the desired combination of features and fusion algorithms due to limited time. We hope that next year we can submit all the desired runs. We will also focus on improving multimodal fusion using other fusion approaches (e.g early fusion in case and image based retrieval) and algorithms. Other technique we will study is the integration of modality as a feature in the retrieval tasks.</p><p>Overall, we are satisfied with our first participation in ImageCLEF and hope that we improve our performance in the following editions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,134.77,222.60,345.83,7.89;3,134.77,233.58,345.82,7.86;3,134.77,244.54,171.93,7.86"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Case based retrieval step. (a) get document id (DOI) from image id (IRI); (b) combine multiple document results into one (unique) document list. The example uses CombMAX fusion for simple visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,134.77,115.91,345.83,139.45"><head>Table 1 .</head><label>1</label><figDesc>Modality classification performance comparison by retrieval type. All our runs and the best runs are present. If our run is the best, the second best is present</figDesc><table coords="5,209.20,147.67,196.94,107.69"><row><cell>Run Name</cell><cell cols="2">Type Correctly classified</cell></row><row><cell>IBM modality run1</cell><cell>T</cell><cell>64.17%</cell></row><row><cell>words</cell><cell>T</cell><cell>62.35%</cell></row><row><cell>words noComb</cell><cell>T</cell><cell>32.80%</cell></row><row><cell>IBM modality run4</cell><cell>V</cell><cell>80.79%</cell></row><row><cell>CEDD FCTH</cell><cell>V</cell><cell>57.62%</cell></row><row><cell cols="2">CEDD FCTH NoComb V</cell><cell>32.49%</cell></row><row><cell>IBM modality run8</cell><cell>M</cell><cell>81.68%</cell></row><row><cell>All</cell><cell>M</cell><cell>72.92%</cell></row><row><cell>All NoComb</cell><cell>M</cell><cell>44.61%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,134.77,509.09,345.83,128.49"><head>Table 2 .</head><label>2</label><figDesc>Case based runs performance comparison by retrieval type. All our runs and the best runs are present. If ours is the best in the modality, the second best is present</figDesc><table coords="5,137.60,540.85,337.09,96.73"><row><cell>Run Name</cell><cell>Type MAP GM-MAP bpref P@10 P@30</cell></row><row><cell>SNUMedinfo9</cell><cell>T 0.2429 0.1163 0.2417 0.2657 0.1981</cell></row><row><cell cols="2">FCT LUCENE BM25L MSH PRF T 0.2233 0.1177 0.2000 0.2600 0.1800</cell></row><row><cell>FCT LUCENE BM25L PRF</cell><cell>T 0.1992 0.0964 0.1874 0.2343 0.1781</cell></row><row><cell>FCT SEGHIST 6x6 LBP</cell><cell>V 0.0281 0.0009 0.0300 0.0429 0.0238</cell></row><row><cell>medgift visual nofilter casebased</cell><cell>V 0.0029 0.0001 0.0035 0.0086 0.0067</cell></row><row><cell>FCT CB MM rComb</cell><cell>M 0.1608 0.0779 0.1400 0.1800 0.1257</cell></row><row><cell cols="2">medgift mixed nofilter casebased M 0.1467 0.0883 0.1318 0.1971 0.1457</cell></row><row><cell>FCT CB MM MNZ</cell><cell>M 0.0794 0.0035 0.0800 0.1371 0.0810</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,151.25,320.47,310.07,94.81"><head>Table 3 .</head><label>3</label><figDesc>Fusion comparison for the medical ImageCLEF case based queries</figDesc><table coords="6,151.25,341.27,309.78,74.02"><row><cell>Run Name</cell><cell>Comb</cell><cell>MAP GM-MAP bpref P@10 P@30</cell></row><row><cell>FCT CB MM rComb</cell><cell>ISR</cell><cell>0.1608 0.0779 0.14 0.1800 0.1257</cell></row><row><cell>(not submitted)</cell><cell>RRF</cell><cell>0.1597 0.0787 0.13 0.1571 0.1248</cell></row><row><cell>(not submitted)</cell><cell>RR</cell><cell>0.1582 0.0779 0.14 0.1771 0.1238</cell></row><row><cell>(not submitted)</cell><cell cols="2">combSUM 0.0804 0.0039 0.09 0.1429 0.0790</cell></row><row><cell cols="3">FCT CB MM MNZ combMNZ 0.0794 0.0035 0.08 0.1371 0.0810</cell></row><row><cell>(not submitted)</cell><cell cols="2">combMAX 0.0292 0.0013 0.03 0.0457 0.0248</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,136.16,464.20,374.11,84.25"><head>Table 4 .</head><label>4</label><figDesc>Ad-hoc image retrieval fusion performance.</figDesc><table coords="6,136.16,485.00,374.11,63.46"><row><cell>Run Name</cell><cell>Comb</cell><cell>MAP GM-MAP bpref P@10 P@30</cell></row><row><cell cols="3">nlm-se-image-based-mixed (best global result) 0.3196 0.1018 0.2983 0.3886 0.2686</cell></row><row><cell>(not submitted)</cell><cell>RRF</cell><cell>0.1496 0.0331 0.1549 0.2200 0.1495</cell></row><row><cell>(not submitted)</cell><cell>ISR</cell><cell>0.1457 0.0329 0.1504 0.2057 0.1476</cell></row><row><cell>(not submitted)</cell><cell>RR</cell><cell>0.1448 0.0326 0.1492 0.2086 0.1457</cell></row><row><cell>(not submitted)</cell><cell>combMMZ</cell><cell>0.0488 0.0013 0.0735 0.1457 0.0752</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,134.77,281.56,345.83,95.21"><head>Table 5 .</head><label>5</label><figDesc>Ad-hoc image based runs performance comparison by retrieval type. All our runs and the best runs are present.</figDesc><table coords="7,136.16,313.31,305.68,63.46"><row><cell>Run Name</cell><cell cols="4">Type MAP GM-MAP bpref P@10 P@30</cell></row><row><cell cols="5">nlm-se-image-based-textual T 0.3196 0.1018 0.2982 0.3886 0.2686</cell></row><row><cell>FCT SOLR BM25L MSH</cell><cell cols="4">T 0.2305 0.0482 0.2316 0.2971 0.2181</cell></row><row><cell>FCT SOLR BM25L</cell><cell>T</cell><cell>0.22</cell><cell>0.0476</cell><cell>0.228 0.2657 0.2114</cell></row><row><cell>DEMIR4</cell><cell cols="4">V 0.0185 0.0005 0.0361 0.0629 0.0581</cell></row><row><cell>FCT SEGHIST 6x6 LBP</cell><cell cols="4">V 0.0072 0.0001 0.0151 0.0343 0.0267</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,646.48,141.22,7.47"><p>http://lucene.apache.org/core/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,144.73,657.44,235.37,7.47"><p>https://github.com/JohnLangford/vowpal_wabbit/wiki</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,142.96,142.59,337.63,7.86;8,151.52,153.55,329.07,7.86;8,151.52,164.51,213.04,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,338.53,142.59,142.06,7.86;8,151.52,153.55,162.49,7.86">Face description with local binary patterns: application to face recognition</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,324.07,153.55,156.53,7.86;8,151.52,164.51,98.41,7.86">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2037" to="2041" />
			<date type="published" when="2006-12">Dec. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,175.46,337.63,7.86;8,151.52,186.42,329.07,7.86;8,151.52,197.38,103.16,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,371.72,175.46,108.87,7.86;8,151.52,186.42,226.72,7.86">Combining the evidence of multiple query representations for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Kantor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,388.87,186.42,87.13,7.86">Inf. Process. Manage</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="431" to="448" />
			<date type="published" when="1995-05">May 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,208.34,337.64,7.86;8,151.52,219.30,329.07,7.86;8,151.52,230.26,329.07,7.86;8,151.52,241.22,167.92,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,318.62,208.34,161.97,7.86;8,151.52,219.30,235.81,7.86">Cedd: color and edge directivity descriptor: a compact descriptor for image indexing and retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Boutalis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,407.93,219.30,72.66,7.86;8,151.52,230.26,262.72,7.86">Proceedings of the 6th international conference on Computer vision systems, ICVS&apos;08</title>
		<meeting>the 6th international conference on Computer vision systems, ICVS&apos;08<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="312" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,252.18,337.63,7.86;8,151.52,263.14,329.07,7.86;8,151.52,274.09,329.07,7.86;8,151.52,285.05,182.40,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,457.30,252.18,23.29,7.86;8,151.52,263.14,329.07,7.86;8,151.52,274.09,85.79,7.86">Accurate Image Retrieval Based on Compact Composite Descriptors and Relevance Feedback Information</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zagoris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Boutalis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Papamarkos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,245.11,274.09,235.48,7.86;8,151.52,285.05,86.11,7.86">International Journal of Pattern Recognition and Artificial Intelligence (IJPRAI)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="244" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,296.01,337.63,7.86;8,151.52,306.97,329.07,7.86;8,151.52,317.93,175.87,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,368.70,296.01,111.90,7.86;8,151.52,306.97,235.00,7.86">Reciprocal rank fusion outperforms condorcet and individual rank learning methods</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Buettcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,409.78,306.97,41.20,7.86">SIGIR &apos;09</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="758" to="759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,328.89,337.64,7.86;8,151.52,339.85,290.41,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,268.51,328.89,212.08,7.86;8,151.52,339.85,140.68,7.86">Comparing rank and score combination methods for data fusion in information retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">Frank</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Taksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,299.85,339.85,36.66,7.86">Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="449" to="480" />
			<date type="published" when="2005-05">May 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,350.81,337.63,7.86;8,151.52,361.77,329.07,7.86;8,151.52,372.72,84.17,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,216.90,361.77,183.94,7.86">Overview of the imageclef 2013 medical tasks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcia Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,421.77,361.77,58.82,7.86;8,151.52,372.72,55.83,7.86">Working notes of CLEF 2013</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,383.68,337.64,7.86;8,151.52,394.64,329.07,7.86;8,151.52,405.60,213.43,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,237.29,383.68,175.97,7.86">When documents are very long, bm25 fails</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,434.71,383.68,45.88,7.86;8,151.52,394.64,329.07,7.86;8,151.52,405.60,85.91,7.86">Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval</title>
		<meeting>the 34th international ACM SIGIR conference on Research and development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1103" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,416.56,337.64,7.86;8,151.52,427.52,329.07,7.86;8,151.52,438.48,314.46,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,248.77,416.56,231.83,7.86;8,151.52,427.52,76.71,7.86">Fast approximate nearest neighbors with automatic algorithm configuration</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Muja</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,247.06,427.52,233.53,7.86;8,151.52,438.48,102.68,7.86">International Conference on Computer Vision Theory and Application VISSAPP&apos;09)</title>
		<imprint>
			<publisher>INSTICC Press</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">340</biblScope>
			<biblScope unit="page" from="331" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.61,449.44,337.98,7.86;8,151.52,460.40,329.07,7.86;8,151.52,471.35,242.31,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,254.76,460.40,225.84,7.86;8,151.52,471.35,90.85,7.86">Overview of the imageclef 2012 medical image retrieval and classification tasks</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,261.79,471.35,104.06,7.86">CLEF 2012 working notes</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.61,482.31,337.97,7.86;8,151.52,493.27,329.07,7.86;8,151.52,504.23,329.07,7.86;8,151.52,515.19,119.54,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,435.61,482.31,44.98,7.86;8,151.52,493.27,329.07,7.86;8,151.52,504.23,70.93,7.86">Expansionbased technologies in finding relevant and new information: Thu trec2002 novelty track experiments</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,244.62,504.23,235.97,7.86;8,151.52,515.19,27.04,7.86">the Proceedings of the Eleventh Text Retrieval Conference (TREC</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="586" to="590" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
