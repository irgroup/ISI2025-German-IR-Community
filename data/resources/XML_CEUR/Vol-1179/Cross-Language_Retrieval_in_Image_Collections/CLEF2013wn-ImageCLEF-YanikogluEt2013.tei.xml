<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,164.57,116.95,286.21,12.62;1,192.27,134.89,230.82,12.62">Sabanci-Okan System at ImageClef 2013 Plant Identification Competition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,176.11,172.56,74.08,8.74"><forename type="first">Berrin</forename><surname>Yanikoglu</surname></persName>
							<email>berrin@sabanciuniv.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Sabanci University</orgName>
								<address>
									<postCode>34956</postCode>
									<settlement>Istanbul</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,260.75,172.56,69.36,8.74"><forename type="first">Erchan</forename><surname>Aptoula</surname></persName>
							<email>erchan.aptoula@okan.edu.tr</email>
							<affiliation key="aff1">
								<orgName type="institution">Okan University</orgName>
								<address>
									<postCode>34959</postCode>
									<settlement>Istanbul</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,360.03,172.56,35.70,8.74"><forename type="first">S</forename><surname>Tolga</surname></persName>
							<email>stolgay@sabanciuniv.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Sabanci University</orgName>
								<address>
									<postCode>34956</postCode>
									<settlement>Istanbul</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,164.57,116.95,286.21,12.62;1,192.27,134.89,230.82,12.62">Sabanci-Okan System at ImageClef 2013 Plant Identification Competition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">81258A59CE964C21D145246EDF836806</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Plant identification</term>
					<term>mathematical morphology</term>
					<term>support vector machines</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe our participation in the plant identification task of ImageClef 2013. We submitted one fully automatic run that uses different features for the uniform background (isolated leaves) and natural background (unconstrained photos) categories. Besides the category information, meta-data was only used in the natural background category. Our approach employs a variety of shape, texture and color descriptors. As in the previous years, we used shape and texture only for isolated leaves and observed them to be very effective. Our system obtained the best results in this category with a score of 0.607 which is the inverse rank of the retrieved class, averaged over all queried photos and users. As for the natural background category, we used a limited approach using a restricted set of features that were extracted globally due to lack of time, and obtained a score of 0.181.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ImageCLEF plant identification competition is organized every year since 2011 and aims to benchmark progress in the area of plant identification from photographs <ref type="bibr" coords="1,193.65,501.70,10.52,8.74" target="#b2">[3,</ref><ref type="bibr" coords="1,205.82,501.70,7.75,8.74" target="#b3">4,</ref><ref type="bibr" coords="1,215.23,501.70,7.01,8.74" target="#b1">2]</ref>. Similar to the previous years, the competition in 2013 consisted of identifying images of plants that were captured by different means: isolated leaves that were scanned or photographed on a uniform background comprised the SheetAsBackground category. Parts or full images of a plant taken on a natural background formed the NaturalBackground category. This category was further sub-divided as flower, fruit, entire, leaf and stem categories.</p><p>The organizers collected a large set of data from 250 different plant species over the course of several years. Part of this data formed the training set that was distributed to the participants along with the corresponding groundtruth. The remaining data was shared with the participants in order to collect their systems' responses, while the corresponding groundtruth was kept sequestered.</p><p>Submitted systems were scored in terms of the inverse average rank of the correct class for each submitted query. The details of this competition are described in <ref type="bibr" coords="1,190.47,657.11,9.96,8.74" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview of the System</head><p>As a collaboration from two universities in Istanbul, we submitted a single fully automatic run (Sabanci-Okan-Run1) that uses different features for the uniform background (isolated leaves) category and natural background (unconstrained photos) sub-categories. The category information was obtained from the metadata of the query image. This handling of queries in different categories was done to select the appropriate feature set for each group, but it also helped with the handling of this large task.</p><p>As in the previous years, we used shape and texture only for isolated leaves and observed them to be very effective. We had the best average score overall last year in both the automatic and manual categories <ref type="bibr" coords="2,409.05,248.55,10.52,8.74" target="#b3">[4]</ref> and this year we obtained the best score on the isolated leaf (uniform background) category.</p><p>For the natural background category, we used texture and color features for the flower, fruit and entire sub-categories; shape and texture for the leaf category; and only texture features for the stem category. The feature group selection was done based on our previous experiences in this problem and in order to increase generalization performance; it also helped reduce the time spent in feature extraction.</p><p>Meta-data was used only in the natural background category; specifically the month information was used to narrow down successfully the alternatives for fruit and flower categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Segmentation</head><p>Although segmentation is of crucial significance for content description, it has been used in our system only for isolated leaves and stems. In contrast, segmentation of photographs with a natural background is either not meaningful (i.e. the whole picture contains some part of the plant) or not an easy problem even though the background is well-defined (e.g. a plant photographed with the forest ground). In ImageCLEF 2012, we had used an approach where photos were aggressively segmented to leave only a single leaf in the image, in order to channel photographs to our successful isolated leaf recognition system <ref type="bibr" coords="2,438.72,501.70,9.96,8.74" target="#b3">[4]</ref>. While we believe that this is an interesting and complementary approach to one based on local invariants, it is limited in its potential as much information is discarded. This approach was skipped altogether this year due to lack of time.</p><p>Isolated leaves usually possess an uniform background, often with uneven illumination and sometimes shadow. Their segmentation has been conducted as in the past, using edge preserving morphological simplification by means of area attribute filters, followed by an adaptive threshold <ref type="bibr" coords="2,365.40,585.38,9.96,8.74" target="#b8">[9]</ref>. Moreover, contrary to flowers and fruit, it has been observed that the stem category contains mostly vertical or horizontal tree trunks that often occupy the majority of the image surface's center. Hence, in order to reliably obtain a background-free sub-image, we first determined the stem's orientation by controlling the horizontal and vertical derivatives' maxima, followed by cropping the corresponding central two third's of the image surface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Preprocessing</head><p>Preprocessing stages were present only for the isolated leaves, in the form of size and orientation normalization. Specifically, we align the leaves' major axis with the vertical and normalize their height to 600 pixels, preserving the aspect ratio. Orientation normalization is realized through principal component analysis, with additional correction coming from the leaf petiole's location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Features</head><p>Given the high visual variability of this year's dataset categories as well as the number of classes, feature extraction has become more challenging than ever before. Consequently, a large spectrum of descriptors has been evaluated, including shape, texture, color and local invariants. Moreover, considering the strong relation between seasons and image categories such as fruit and flowers, meta-data have also been exploited with great success. Here we summarize only the new descriptors, while the others have been explained in detail at the previous working notes <ref type="bibr" coords="3,198.50,329.40,10.52,8.74" target="#b7">[8,</ref><ref type="bibr" coords="3,210.67,329.40,7.01,8.74" target="#b8">9]</ref>.</p><p>In particular, following the success of our past systems with scan and scan-like data (isolated leaves), it has been chosen not to greatly modify their descriptor set; instead we mainly optimized their parameters in order to cope with the higher class count. In addition, only one new descriptor was included in the feature extraction set: the edge background/foreground histogram. It is computed on the binary mask of its input and it consists in calculating the ratio of background to foreground pixels in a subwindow centered on each edge pixel. The normalized histogram of the said ratios constitutes the end feature vector.</p><p>As far as photographs are concerned, given the extreme variation of viewpoint and scale (especially w.r.t. the category "entire"), we resorted to using rather traditional, yet still reliable color descriptors. In particular, we employed the color autocorrelogram <ref type="bibr" coords="3,252.94,472.86,9.96,8.74" target="#b5">[6]</ref>, computed in the LSH color space after a nonuniform subquantization to 63 colors (7 levels for hue, 3 for saturation and 3 for luminance). The color autocorrelogram describes the spatial correlation of colors. It consists of a table where the entry (i, j) denotes the probability of encountering two pixels of color i at a distance of j pixels.</p><p>We further employed the saturation-weighted hue histogram <ref type="bibr" coords="3,422.85,532.64,9.96,8.74" target="#b4">[5]</ref>, where the total value of each bin W θ , θ ∈ [0, 360] is calculated as:</p><formula xml:id="formula_0" coords="3,270.54,567.76,210.05,19.61">W θ = x S x δ θHx<label>(1)</label></formula><p>where H x and S x are the hue and saturation values at position x and δ ij the Kronecker delta function. As far as the color space is concerned, we have used LSH <ref type="bibr" coords="3,157.29,621.25,10.52,8.74" target="#b0">[1]</ref> since it provides a saturation representation independent of luminance. And last, in order to exploit the effect of seasons on fruit and flowers, it has been decided to use the meta-data accompanying the visual samples, and specifically the month of acquisition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Classifier Training and Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Data</head><p>The competition data consisted of a training set which was made available to all the participants, along with the corresponding groundtruth files, and a test set whose groundtruth was kept sequestered. The distribution of the data in each category and in each of these sets is shown in Table <ref type="table" coords="4,363.93,198.83,3.87,8.74" target="#tab_0">1</ref>. We split the available training data shown in Table <ref type="table" coords="4,406.67,352.34,4.98,8.74" target="#tab_0">1</ref> into train and validation subsets. The training set was used in training the corresponding classifier and the validation set was used as our internal test data for evaluating different features and algorithms. In order to help with the generalization capability, we tried to avoid having very similar images in the train and validation splits. Specifically, pictures from an individual plant were put in either the train or validation subset. The selection of the samples was done as described in <ref type="bibr" coords="4,467.31,424.07,9.96,8.74" target="#b8">[9]</ref>. As a result of this split, we obtained the train/validation subsets as shown in Table <ref type="table" coords="4,162.16,447.98,3.87,8.74" target="#tab_1">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Classifiers</head><p>We used shape and texture only for isolated leaves in the SheetAsBackground category and observed them to be very effective. The length of the feature vector was 156 for this case, consisting of Fourier descriptors (50 of them), in addition to various area and contour-based shape descriptors, and texture descriptors (106 altogether), many of them used in our previous system <ref type="bibr" coords="5,454.44,131.95,9.96,8.74" target="#b8">[9]</ref>. In the NaturalBackground category, we only used color features for the flower, fruit and entire sub-categories (autocorrelogram, saturation-weighted hue histogram and the month the picture was taken, for a total of 265 dimensions); shape and texture for the leaf category (same classifier as for isolated leaves); and only texture features for the stem category.</p><p>Feature extraction was done from the whole picture, except for the case of leaf images in the SheetAsBackground and the NaturalBackground categories, where segmentation step preceded feature extraction. The approach of using global features or using only color features is clearly not sufficient for unconstrained photos (e.g. flower, fruit, entire categories), however we did not have time to incorporate other methods based on local features.</p><p>The classifiers used for different categories were all trained with the training portion of the available data shown in Table <ref type="table" coords="5,383.97,296.70,3.87,8.74" target="#tab_1">2</ref>, except for the leaf sub-category of NaturalBackground photographs. For this group, we used the same system developed for recognizing the SheetAsBackground category, after a simple segmentation of the image.</p><p>As classifier, we used a Support Vector Machine (SVM) classifier based on their good performance in many object recognition problem and used the SMO classifier inside the WEKA toolbox. The parameters for the SVM was set asC = 10 and a polynomial kernel of degree 2 after some limited tests with the validation set.</p><p>In Table <ref type="table" coords="5,204.39,413.63,3.87,8.74" target="#tab_2">3</ref>, we give the cross-validation accuracy obtained while training a classifier using 10-fold cross-validation, as well as the accuracy of the same classifier on the validation subset. In the last column of this table, we also include the average inverse rank results published by the competition organizers for each category <ref type="bibr" coords="5,174.80,461.45,9.96,8.74" target="#b1">[2]</ref>. Here, a score of 1 indicates that all queries return the correct class as the top guess, while a score near 0 means the correct class is returned much later in rank. This year we streamlined this process a little better and concentrated on what we could accomplish the best. For that reason, we worked on isolated leaves the most, while some categories received minimal attention (e.g. leaves under the NaturalBackground category).</p><p>As the official results indicate, we obtained the best results in recognizing isolated leaves (SheetAsBackground category), with an average inverse rank of 0.607. This score roughly indicates that that the correct class was returned as top-1 or top-2 alternative for the majority of queries, which is a promising result for the plant retrieval problem. In recognizing the unconstrained photographs in the NaturalBackground category, we started working on a system based on SIFT features <ref type="bibr" coords="6,171.22,323.23,9.96,8.74" target="#b6">[7]</ref>; although the initial results have been encouraging, the allocated time has not been sufficient for finalizing this module before the submission.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,186.53,224.56,239.22,108.67"><head>Table 1 :</head><label>1</label><figDesc>Train and test dataset sizes.</figDesc><table coords="4,186.53,236.49,239.22,96.73"><row><cell>Category</cell><cell cols="2">Train Test</cell></row><row><cell>SheetAsBackground (Isolated leaves)</cell><cell cols="2">9,781 1,250</cell></row><row><cell cols="3">NaturalBackground (Unconstrained photos) 11,204 3,842</cell></row><row><cell>Flower</cell><cell cols="2">3,522 1,233</cell></row><row><cell>Leaf</cell><cell>2,080</cell><cell>790</cell></row><row><cell>Entire</cell><cell>1,455</cell><cell>594</cell></row><row><cell>Fruit</cell><cell>1,387</cell><cell>520</cell></row><row><cell>Stem</cell><cell>1,337</cell><cell>605</cell></row><row><cell>All</cell><cell cols="2">20,985 5,092</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,164.52,474.60,286.31,103.38"><head>Table 2 :</head><label>2</label><figDesc>Train and validation splits of the available training data.</figDesc><table coords="4,175.44,492.21,261.40,85.77"><row><cell>Category</cell><cell>Train</cell><cell>Validation</cell></row><row><cell>SheetAsBackground (Isolated leaves)</cell><cell>7,867</cell><cell>1914</cell></row><row><cell cols="2">NaturalBackground (Unconstrained photos) 7,865</cell><cell>2,562</cell></row><row><cell>Flower</cell><cell>2,325</cell><cell>1197</cell></row><row><cell>Entire</cell><cell>1,455</cell><cell>594</cell></row><row><cell>Fruit</cell><cell>960</cell><cell>495</cell></row><row><cell>Stem</cell><cell>1,045</cell><cell>276</cell></row><row><cell>All</cell><cell>15,732</cell><cell>4,476</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,134.77,524.37,348.84,109.27"><head>Table 3 :</head><label>3</label><figDesc>Cross-validation and validation set accuracies, along with the official test scores obtained by our system. Participation into the ImageCLEF Plant Identification competition is an arduous task, especially when done in collaboration, with different people working in different parts of the problem. Last year we had to transfer partial results back and forth, since alternating steps of segmentation, preprocessing, feature extraction, and classification were done by different people in our small group.</figDesc><table coords="5,136.16,548.26,347.44,85.37"><row><cell>Category</cell><cell>Features</cell><cell>Cross-Val.</cell><cell>Validation</cell><cell>Inverse Rank</cell></row><row><cell cols="2">UniformBackground Shape, texture</cell><cell>93.77%</cell><cell>70.64%</cell><cell>0.607</cell></row><row><cell>NaturalBackground</cell><cell></cell><cell></cell><cell></cell><cell>0.181</cell></row><row><cell>Flower</cell><cell cols="2">Texture, color, month 40.20%</cell><cell>34.50%</cell><cell>0.223</cell></row><row><cell>Fruit</cell><cell cols="2">Texture, color, month 51.33%</cell><cell>43.64%</cell><cell>0.194</cell></row><row><cell>Entire</cell><cell cols="2">Texture, color, month 34.23%</cell><cell>29.50%</cell><cell>0.174</cell></row><row><cell>Stem</cell><cell>Texture</cell><cell>-</cell><cell>9.30%</cell><cell>0.106</cell></row><row><cell>Leaf</cell><cell>Shape, texture</cell><cell>-</cell><cell>-</cell><cell>0.049</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,138.35,390.66,342.24,7.86;6,146.91,401.62,202.84,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,264.75,390.66,162.11,7.86">On the morphological processing of hue</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Aptoula</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lefèvre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,437.54,390.66,43.06,7.86;6,146.91,401.62,71.32,7.86">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1394" to="1401" />
			<date type="published" when="2009-08">August 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,412.58,342.24,7.86;6,146.91,423.53,333.68,7.86;6,146.91,434.49,333.68,7.86;6,146.91,445.45,20.99,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,442.70,423.53,37.89,7.86;6,146.91,434.49,206.17,7.86">Imageclef 2013: the vision, the data and the open challenges</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zellhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">Martinez</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Varea</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Cazorla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,375.26,434.49,70.38,7.86">Proc. CLEF 2013</title>
		<meeting>CLEF 2013</meeting>
		<imprint>
			<publisher>LNCS</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,456.41,342.25,7.86;6,146.91,467.37,333.68,7.86;6,146.91,478.33,229.59,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,264.31,467.37,173.55,7.86">The clef 2011 plant image classification task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Barthelemy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Birnbaum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mouysset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,455.89,467.37,24.70,7.86;6,146.91,478.33,76.01,7.86">CLEF 2011 working notes</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,489.29,342.24,7.86;6,146.91,500.25,333.68,7.86;6,146.91,511.21,100.27,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,194.36,500.25,181.93,7.86">The ImageClef 2012 plant identification task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Barthelemy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Molino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,398.28,500.25,82.31,7.86;6,146.91,511.21,19.67,7.86">CLEF 2011 working notes</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,522.16,342.24,7.86;6,146.91,533.12,261.28,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,198.07,522.16,166.63,7.86">Circular statistics applied to colour images</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,382.20,522.16,98.39,7.86;6,146.91,533.12,38.06,7.86">Computer Vision Winter Workshop</title>
		<meeting><address><addrLine>Valtice, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-02">February 2003</date>
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,544.08,342.24,7.86;6,146.91,555.04,301.83,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,395.09,544.08,85.50,7.86;6,146.91,555.04,68.60,7.86">Image indexing using color correlogram</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,235.30,555.04,23.34,7.86">CVPR</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="762" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,566.00,342.24,7.86;6,146.91,576.96,93.23,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,217.01,566.00,215.70,7.86">Object recognition from local scale-invariant features</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,454.48,566.00,20.89,7.86">ICCV</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,587.92,342.25,7.86;6,146.91,598.88,314.84,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,316.24,587.92,164.36,7.86;6,146.91,598.88,95.91,7.86">Sabanci-Okan system at ImageClef 2011: Plant identification task</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Yanikoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Aptoula</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Tirkaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,262.28,598.88,170.78,7.86">CLEF (Notebook Papers/Labs/Workshop)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,609.84,342.25,7.86;6,146.91,620.79,333.68,7.86;6,146.91,631.75,126.14,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,316.24,609.84,164.36,7.86;6,146.91,620.79,237.41,7.86">Sabanci-Okan system at ImageClef 2012: Combining features and classifiers for plant identification</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Yanikoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Aptoula</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Tirkaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,408.97,620.79,71.62,7.86;6,146.91,631.75,97.44,7.86">CLEF (Notebook Papers/Labs/Workshop)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
