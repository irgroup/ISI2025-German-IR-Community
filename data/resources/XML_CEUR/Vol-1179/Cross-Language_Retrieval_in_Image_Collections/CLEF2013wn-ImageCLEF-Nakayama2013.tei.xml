<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,184.23,116.85,246.98,12.93;1,220.22,134.79,174.98,12.93">NLab-UTokyo at ImageCLEF 2013 Plant Identification Task</title>
				<funder>
					<orgName type="full">Nakajima Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,269.35,171.99,76.64,9.96"><forename type="first">Hideki</forename><surname>Nakayama</surname></persName>
							<email>nakayama@ci.i.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Grad. School of Information Science and Technology</orgName>
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<addrLine>7-3-1, Hongo, Bunkyo-ku</addrLine>
									<settlement>Tokyo</settlement>
									<country key="JP">JAPAN</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,184.23,116.85,246.98,12.93;1,220.22,134.79,174.98,12.93">NLab-UTokyo at ImageCLEF 2013 Plant Identification Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EB69A90127131F0E3CE8D9D5E668F6B2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Fine-grained Visual Categorization</term>
					<term>Multiple Local Descriptors</term>
					<term>Polynomial Embedding</term>
					<term>Fisher Vector</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe our system at the ImageCLEF 2013 plant identification task. Plant identification is extremely challenging because target classes are often visually quite similar. To distinguish them, we need to extract highly informative visual features. We believe that the key to achieving this is to enhance the discriminative power of local descriptors. We employed multiple local features with our polynomial embedding technique to boost the performance. Further, they were encoded into the sophisticated Fisher Vector representation which enables accurate classification with linear classifiers. Our system achieved promising performance, and got the first place in NaturalBackground and the third place in SheetAsBackground tasks, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this report, we describe our contributions submitted to the ImageCLEF 2013 plant identification task <ref type="bibr" coords="1,245.50,477.21,10.51,9.96" target="#b2">[3,</ref><ref type="bibr" coords="1,257.68,477.21,7.01,9.96" target="#b7">8]</ref>. The system is based on our recently proposed method designed for fine-grained visual categorization (FGVC) <ref type="bibr" coords="1,410.65,489.18,14.60,9.96" target="#b12">[13]</ref>. The goal of FGVC is to categorize conceptually (and thus visually) similar classes such as plant and animal species <ref type="bibr" coords="1,246.55,513.09,10.51,9.96" target="#b5">[6,</ref><ref type="bibr" coords="1,258.73,513.09,12.72,9.96" target="#b17">18,</ref><ref type="bibr" coords="1,273.12,513.09,12.72,9.96" target="#b13">14,</ref><ref type="bibr" coords="1,287.51,513.09,11.62,9.96" target="#b9">10]</ref>, and thus naturally includes the concept of this challenge. However, FGVC is regarded to be extremely difficult because of its high intra-class and low inter-class variations <ref type="bibr" coords="1,359.63,536.99,9.96,9.96" target="#b5">[6]</ref>.</p><p>To distinguish very similar categories, we need to extract highly informative visual features. We believe that the key to achieving this is to enhance the discriminative power of local descriptors. Our method can efficiently improve the discriminative performance of arbitrary local descriptors for bag-of-words <ref type="bibr" coords="1,134.77,596.77,10.51,9.96" target="#b4">[5]</ref> based systems with a simple supervised dimensionality reduction method. Using polynomials of a descriptor and its neighbors, we can efficiently exploit local spatial co-occurrence patterns.</p><p>We implemented our method with standard object recognition pipelines using the state-of-the-art Fisher Vector coding <ref type="bibr" coords="1,314.95,644.59,14.60,9.96" target="#b14">[15]</ref>. Our submitted runs achieved the first place in NaturalBackground and the third place in SheetAsBackground tasks, respectively. Moreover, we achieved the first place in four of five subcategories in NaturalBackground task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview</head><p>Figure <ref type="figure" coords="2,166.63,384.82,4.98,9.96" target="#fig_0">1</ref> illustrates the entire pipeline of our system. We first extract multiple local descriptors from images. For each type of descriptors, we first compute augmented latent descriptors in a supervised learning framework <ref type="bibr" coords="2,423.16,408.73,14.60,9.96" target="#b12">[13]</ref>, which is our core contribution. Then we encode them into a global feature vector using the Fisher Vector <ref type="bibr" coords="2,196.03,432.64,15.49,9.96" target="#b14">[15]</ref> framework. As a classifier, we train a linear logistic regression model. Classifiers are trained independently for each descriptor and combined in late-fusion approach. The output of logistic regression is a probability and can be easily integrated when fusing multiple classifiers. Specifically, we simply take the average log-likelihood of posterior probability for each classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Polynomial Embedding</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Augmenting Descriptors</head><p>We densely extract local features v ∈ R d from images. Each patch at position (x, y) is described by v (x,y) . We augment this by explicitly including the polynomials<ref type="foot" coords="2,171.79,564.70,3.97,6.97" target="#foot_0">1</ref> of its elements. Let p c (x,y) denote the augmented descriptor, where c is the number of neighbors considered. When no neighbor is considered,</p><formula xml:id="formula_0" coords="2,225.02,598.87,255.57,41.02">p 0 (x,y) =    v (x,y) upperV ec v (x,y) v T (x,y)    ,<label>(1)</label></formula><p>where, upperV ec() is the flattened vector of the components in the upper triangular part of a symmetric matrix. Moreover, we can efficiently exploit local spatial information by taking the polynomials between neighboring descriptors. When considering two neighbors (left side and right side),</p><formula xml:id="formula_1" coords="3,225.02,192.95,255.57,83.48">p 2 (x,y) =           v (x,y) upperV ec v (x,y) v T (x,y) V ec v (x,y) v T (x-δ,y) V ec v (x,y) v T (x+δ,y)           ,<label>(2)</label></formula><p>where, V ec() is the flattened vector of the components of a matrix, and δ is an offset parameter for defining neighbors.</p><p>In this work, we considered at most two neighbors, although our previous work suggests that using more neighbors could have resulted in better performance <ref type="bibr" coords="3,165.75,340.40,14.60,9.96" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervised Dimensionality Reduction</head><p>We apply canonical correlation analysis (CCA) <ref type="bibr" coords="3,340.26,388.33,10.51,9.96" target="#b8">[9]</ref> to the pairs of the augmented descriptor p and corresponding label vector l. In this work, we use the imagelevel label vector<ref type="foot" coords="3,209.93,410.87,3.97,6.97" target="#foot_1">2</ref> for descriptor compression. That is, all p within an image are coupled with the same label vector for supervised dimensionality reduction <ref type="foot" coords="3,464.97,422.83,3.97,6.97" target="#foot_2">3</ref> .</p><p>CCA finds the linear projections s = A T p and t = B T l that maximize the correlation between the projected vectors s and t. We randomly sample {p (x,y) , l (x,y) } pairs from the entire training dataset, and let C = C pp C pl C lp C ll denote their covariance matrices. Namely,</p><formula xml:id="formula_2" coords="3,241.25,505.84,239.33,23.09">C pp = 1 N (p -p)(p -p) T ,<label>(3)</label></formula><formula xml:id="formula_3" coords="3,244.34,529.82,236.25,23.09">C ll = 1 N (l -l)(l -l) T ,<label>(4)</label></formula><formula xml:id="formula_4" coords="3,242.80,553.81,237.79,23.09">C pl = 1 N (p -p)(l -l) T ,<label>(5)</label></formula><formula xml:id="formula_5" coords="3,242.80,578.62,237.79,12.98">C lp = C T pl ,<label>(6)</label></formula><p>where, N is the number of sampled pairs, and p and l are their means. The solution of CCA can be obtained by solving the following eigenvalue problem.</p><formula xml:id="formula_6" coords="4,215.19,151.43,265.40,13.77">C pl C -1 ll C lp A = C pp AΛ 2 (A T C pp A = I m ),<label>(7)</label></formula><formula xml:id="formula_7" coords="4,214.60,166.37,261.74,13.23">C lp C -1 pp C pl B = C ll BΛ 2 (B T C ll B = I m ), (<label>8</label></formula><formula xml:id="formula_8" coords="4,476.35,168.24,4.24,9.96">)</formula><p>where Λ is the diagonal matrix of the first m canonical correlations, and m is the dimension of the canonical elements. The parameter m corresponds to the dimension of the embedded descriptor, and needs to be tuned manually. One problem is that m can be at most the dimension of the label vector because of the rank problem. If we need more features, we can project p into the orthogonal subspace and iteratively apply CCA to further extract discriminative components.</p><p>Using the projections obtained by CCA, we get a compact vector s that embeds a high-dimensional augmented vector, which we call the latent descriptor.</p><formula xml:id="formula_9" coords="4,287.14,306.12,193.45,11.58">s = A T p.<label>(9)</label></formula><p>Once the latent descriptor is computed, it can be used in the exact same manner as widely-used raw descriptors such as SIFT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global Feature Vector</head><p>We encode the latent descriptors into a global feature vector using the Fisher Vector framework <ref type="bibr" coords="4,217.29,395.44,14.60,9.96" target="#b14">[15]</ref>, which is a recently established state-of-the-art variant of bag-of-words encoding. Since the dimensionality of Fisher Vector is in proportional to that of local descriptors, compactness of the latent descriptor is essentially important to utilize this representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Plant Identification Task</head><p>The goal of the challenge is to identify 250 species of plants from their photos. There are two main subtasks: SheetAsBackground and NaturalBackground (Fig. <ref type="figure" coords="4,134.77,511.05,3.87,9.96">2</ref>). While the objective of the former is to recognize leaves spread on white background, the latter targets more organs and generic background. Therefore, NaturalBackground task has more generic nature like typical FGVC problems and thought to be challenging. The performance is evaluated in terms of the rank of the correct species in the list of retrieved species. The score is normalized by the numbers of content owners, individual plants, and pictures taken from the same plant. For more information, refer to <ref type="bibr" coords="4,323.03,582.78,9.96,9.96" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Details of the system</head><p>We used several standard local descriptors in our system, such as SIFT <ref type="bibr" coords="4,462.34,632.64,14.60,9.96" target="#b11">[12]</ref>, C-SIFT <ref type="bibr" coords="4,171.64,644.59,9.96,9.96" target="#b1">[2]</ref>, Opponent-SIFT <ref type="bibr" coords="4,261.32,644.59,14.60,9.96" target="#b15">[16]</ref>, HSV-SIFT <ref type="bibr" coords="4,333.27,644.59,9.96,9.96" target="#b0">[1]</ref>, and the self-similarity (SSIM) <ref type="bibr" coords="4,134.77,656.55,15.49,9.96" target="#b16">[17]</ref> descriptors. The dimension of SSIM is 40 in our system (4 radial bins and 10 angle bins). All these local features are extracted in a dense sampling approach without rotation invariance <ref type="bibr" coords="5,258.73,539.14,14.60,9.96" target="#b10">[11]</ref>. We extract local features from 24x24 patches on regular grids spacing five pixels. They are compressed into 64 dimensions via PCA, except for SSIM. Finally, we apply our polynomial embedding (PE) method with CCA and obtain 64-dimensional latent descriptor (m = 64). We fix the offset parameter δ = 20 for defining neighbors. For implementing Fisher Vectors, we use 64 Gaussians for estimating a Gaussian mixture model and concatenate feature vectors from an entire image and three horizontal regions.</p><p>We used the feature extraction software provided by the authors of <ref type="bibr" coords="5,445.79,632.64,15.49,9.96" target="#b15">[16]</ref> and <ref type="bibr" coords="5,134.77,644.59,10.51,9.96" target="#b3">[4]</ref> for computing SIFT (including its variants) and SSIM, respectively. Also, we used the LIBLINEAR <ref type="bibr" coords="5,233.75,656.55,10.51,9.96" target="#b6">[7]</ref> package for the implementation of our classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Our Runs</head><p>We submitted three runs under different configurations. Each run consists of some classifiers independently trained for a certain domain as in Fig. <ref type="figure" coords="6,444.65,151.24,3.87,9.96">2</ref>. Information for identifying the domain of testing samples can be drawn from the corresponding xml files<ref type="foot" coords="6,238.43,173.77,3.97,6.97" target="#foot_3">4</ref> .</p><p>-Run 1 (D1): A multi-class classifier is trained on the whole dataset of 250 classes without distinguishing SheetAsBackground and NaturalBackground categories (and its sub-categories). -Run 2 (D2, D3): Two classifiers are trained independently for SheetAs-Background and NaturalBackground categories, respectively. We do not distinguish their sub-categories. -Run 3 (D2, D4-8): Classifiers are trained independently for SheetAsBackground and NaturalBackground categories. The former is the same one used in Run 2 (D2). For the latter, we train classifiers independently for each of five sub-categories (D4-8).</p><p>To tune our system, we take roughly 10% of the individual plants in the provided training dataset for validation. Table <ref type="table" coords="6,322.59,338.33,4.98,9.96">1</ref> summarizes the number of samples for each domain. For simplicity, we evaluate the classification accuracy on the validation dataset without distinguishing individual plants and owners. After optimizing the parameters, classifiers are trained again on the original training dataset and applied to testing data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Validation Results</head><p>For various domain and feature combinations, we tuned our system and validated their effectiveness. Table <ref type="table" coords="6,270.67,447.79,4.98,9.96">2</ref> shows the results. "PCA64" denotes the Fisher Vector using 64-dimensional descriptors compressed via PCA <ref type="foot" coords="6,401.54,458.37,3.97,6.97" target="#foot_4">5</ref> . This is a typical implementation of Fisher Vector coding and serves as the baseline. For most of the trials, PE reasonably improves the performance of the original descriptors. Also, the relative improvement seems more significant in NaturalBackground domains.</p><p>Based on the results, we selected the features for final submissions. In Shee-tAsBackground (D2) domain, we used only gray SIFT because we found color descriptors were not effective. As for NaturalBackground domain, we chose color SIFTs + SSIM combination considering its good performance in all subcategories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Test Results</head><p>Based on the validation results, we submitted three runs. Figure <ref type="figure" coords="6,421.59,617.03,4.98,9.96">3</ref> summarizes the performance of submitted runs from all participants. Not surprisingly, Run </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SheetAsBackground NaturalBackground</head><p>Fig. <ref type="figure" coords="8,216.73,249.96,4.13,8.08">3</ref>. Scores of all submitted runs. See <ref type="bibr" coords="8,362.62,249.52,9.72,8.97" target="#b7">[8]</ref> for details.</p><p>3 achieved the best in three runs on NaturalBackground task since it consists of multiple classifiers tuned for each sub-category. However, interestingly, the difference in performance is not large compared to the result of Run 2. Moreover, D1 classifier got better performance than D2 classifier on SheetAsBackground task. We noticed that some plants share similar appearance in different subcategories (e.g. 'Flower' and 'Entire'). In such a case, universal classifier might result in better performance than specific ones for each sub-category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>During this challenge, we bet on implementing powerful image features, rather than classification algorithms and systems. We employed multiple local features with our polynomial embedding technique to boost the performance. They are further encoded into the powerful Fisher Vector representation. Our system achieved promising performance, and got the first place in NaturalBackground and the third place in SheetAsBackground tasks. On the other hand, our learning and classification algorithms are very simple and could be improved. Although some individual plants have multiple images of different organs, our system treats them independently and loses co-occurrence information. It would be interesting to develop classification methods utilizing them in an integrated manner.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,243.36,253.67,128.59,8.97"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of our system.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.73,646.33,335.87,8.97;2,144.73,657.30,266.05,8.97"><p>We use at most the second-order polynomials in this paper considering the computational cost, although our framework supports higher-order ones.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,144.73,602.50,335.87,8.97;3,144.73,613.46,230.20,8.97"><p>The dimension of the vector is the number of categories. If the image belongs to the category wi, the i-th element is one; otherwise, it is zero.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,144.73,624.42,335.86,8.97;3,144.73,635.37,335.86,8.97;3,144.73,646.33,335.83,8.97;3,144.73,657.30,71.82,8.97"><p>Obviously, this is a rather rough approach, since not all local features within an image are actually related to the image-level labels. Nevertheless, we note that this assumption is justified somewhat for FGVC problems, since objects are often closely targeted by users.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="6,144.73,646.33,303.87,8.97"><p>Note that this is not interpreted as a manual intervention in this challenge.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="6,144.73,657.30,227.85,8.97"><p>We use the raw SSIM descriptor without applying PCA.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This work is partially supported by the <rs type="funder">Nakajima Foundation</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="8,142.96,646.33,337.63,8.97;8,151.52,657.30,289.64,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,308.37,646.33,172.22,8.97;8,151.52,657.30,113.73,8.97">Scene classification using a hybrid generative/discriminative approach</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Muñoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,272.67,657.30,77.86,8.97">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="712" to="727" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,120.17,337.62,8.97;9,151.52,131.13,284.85,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,297.70,120.17,182.88,8.97;9,151.52,131.13,15.40,8.97">Performance evaluation of local colour invariants</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J</forename><surname>Burghouts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Geusebroek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,173.84,131.13,176.51,8.97">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="48" to="62" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,142.09,337.59,8.97;9,151.52,153.04,329.05,8.97;9,151.52,164.01,327.36,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,453.19,153.04,27.39,8.97;9,151.52,164.01,228.49,8.97">Image-CLEF 2013: the vision, the data and the open challenges</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zellhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Martinez Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Garcia Varea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cazorla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,401.06,164.01,49.17,8.97">Proc. CLEF</title>
		<meeting>CLEF</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,174.97,337.63,8.97;9,151.52,185.92,329.07,8.97;9,151.52,196.88,202.19,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,323.85,174.97,156.73,8.97;9,151.52,185.92,134.27,8.97">Efficient retrieval of deformable shape classes using local self-similarities</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,306.60,185.92,173.99,8.97;9,151.52,196.88,173.51,8.97">IEEE ICCV Workshop on Non-rigid Shape Analysis and Deformable Image Alignment</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,207.85,337.63,8.97;9,151.52,218.80,329.06,8.97;9,151.52,229.76,97.59,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,407.59,207.85,73.00,8.97;9,151.52,218.80,108.03,8.97">Visual categorization with bags of keypoints</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Willamowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,280.74,218.80,199.85,8.97;9,151.52,229.76,68.92,8.97">Proc. ECCV Workshop on Statistical Learning in Computer Vision</title>
		<meeting>ECCV Workshop on Statistical Learning in Computer Vision</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,240.71,337.64,8.97;9,151.52,251.68,239.46,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,314.44,240.71,166.15,8.97;9,151.52,251.68,98.15,8.97">What does classifying more than 10,000 image categories tell us?</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,266.82,251.68,53.51,8.97">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="71" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,262.64,337.62,8.97;9,151.52,273.59,329.07,8.97;9,151.52,284.55,47.11,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,404.14,262.64,76.43,8.97;9,151.52,273.59,134.72,8.97">LIBLINEAR: A library for large linear classification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><forename type="middle">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,292.34,273.59,152.93,8.97">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,295.51,51.56,8.97;9,231.67,295.51,248.89,8.97;9,151.52,306.47,329.01,8.97;9,151.52,317.43,51.51,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,174.18,306.47,196.89,8.97">The ImageCLEF 2013 Plant Identification Task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bakić</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Barthelemy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Boujemaa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,394.87,306.47,85.67,8.97;9,151.52,317.43,22.83,8.97">CLEF 2013 Working Notes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,328.39,337.65,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,206.86,328.39,147.72,8.97">Relations between two sets of variants</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hotelling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,360.49,328.39,44.15,8.97">Biometrika</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="321" to="377" />
			<date type="published" when="1936">1936</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,339.34,337.97,8.97;9,151.52,350.31,329.05,8.97;9,151.52,361.27,152.57,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,361.16,339.34,119.43,8.97;9,151.52,350.31,143.21,8.97">Novel dataset for fine-grained image categorization: Stanford dogs</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Jayadevaprakash</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,316.35,350.31,164.22,8.97;9,151.52,361.27,123.90,8.97">Proc. CVPR Workshop on Fine-Grained Visual Categorization (FGVC)</title>
		<meeting>CVPR Workshop on Fine-Grained Visual Categorization (FGVC)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,372.22,337.96,8.97;9,151.52,383.18,329.05,8.97;9,151.52,394.14,86.02,8.97" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,307.26,372.22,173.32,8.97;9,151.52,383.18,199.10,8.97">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,372.42,383.18,73.86,8.97">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,405.10,337.96,8.97;9,151.52,416.06,52.22,8.97" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,194.44,405.10,216.35,8.97">Object recognition from local scale-invariant features</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,433.25,405.10,47.33,8.97;9,151.52,416.06,23.56,8.97">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,427.02,337.97,8.97;9,151.52,437.97,213.84,8.97" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,213.08,427.02,267.50,8.97;9,151.52,437.97,89.05,8.97">Augmenting descriptors for fine-grained visual categorization using polynomial embedding</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Nakayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,262.33,437.97,74.36,8.97">Proc. IEEE ICME</title>
		<meeting>IEEE ICME</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,448.94,337.97,8.97;9,151.52,459.90,329.06,8.97;9,151.52,470.85,124.91,8.97" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,275.25,448.94,205.34,8.97;9,151.52,459.90,36.68,8.97">Automated flower classification over a large number of classes</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,210.93,459.90,269.66,8.97;9,151.52,470.85,40.94,8.97">Proc. Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<meeting>Indian Conference on Computer Vision, Graphics &amp; Image essing</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,481.81,337.95,8.97;9,151.52,492.78,176.71,8.97" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="9,312.44,481.81,168.13,8.97;9,151.52,492.78,76.38,8.97">Improving the Fisher kernel for large-scale image classification</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,248.61,492.78,50.96,8.97">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,503.73,337.97,8.97;9,151.52,514.69,329.06,8.97;9,151.52,525.65,136.19,8.97" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="9,354.70,503.73,125.89,8.97;9,151.52,514.69,111.46,8.97">Evaluating color descriptors for object and scene recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,269.43,514.69,211.16,8.97;9,151.52,525.65,45.55,8.97">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1582" to="1596" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,536.61,337.94,8.97;9,151.52,547.57,119.03,8.97" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="9,255.09,536.61,221.55,8.97">Matching local self-similarities across images and videos</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,165.60,547.57,76.29,8.97">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,558.53,337.95,8.97;9,151.52,569.48,315.42,8.97" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<title level="m" coord="9,151.52,569.48,97.72,8.97">Caltech-UCSD birds 200</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. rep.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
