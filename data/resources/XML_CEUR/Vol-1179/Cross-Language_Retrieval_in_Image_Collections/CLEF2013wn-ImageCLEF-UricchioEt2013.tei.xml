<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,154.14,115.96,307.08,12.62;1,211.64,133.89,192.08,12.62">MICC-UNIFI at ImageCLEF 2013 Scalable Concept Image Annotation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,144.25,171.56,69.63,8.74"><forename type="first">Tiberio</forename><surname>Uricchio</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Media Integration and Communication Center (MICC</orgName>
								<orgName type="institution">Università degli Studi di Firenze</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,221.52,171.56,59.70,8.74"><forename type="first">Marco</forename><surname>Bertini</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Media Integration and Communication Center (MICC</orgName>
								<orgName type="institution">Università degli Studi di Firenze</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,288.68,171.56,72.00,8.74"><forename type="first">Lamberto</forename><surname>Ballan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Media Integration and Communication Center (MICC</orgName>
								<orgName type="institution">Università degli Studi di Firenze</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,387.78,171.56,83.32,8.74"><forename type="first">Alberto</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Media Integration and Communication Center (MICC</orgName>
								<orgName type="institution">Università degli Studi di Firenze</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,154.14,115.96,307.08,12.62;1,211.64,133.89,192.08,12.62">MICC-UNIFI at ImageCLEF 2013 Scalable Concept Image Annotation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FE1402B3AEA8ED658DB16BA9E48F04A4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image annotation</term>
					<term>image tagging</term>
					<term>social media</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we report on MICC participation to the Scalable Concept Image Annotation subtask of the ImageCLEF Photo Annotation and Retrieval competition <ref type="bibr" coords="1,308.15,269.54,13.52,7.86" target="#b12">[13]</ref>. Our goal has been to investigate the applicability of data-driven methods that have obtained good results in the field of social image annotation and retrieval to web images. These methods have been applied typically to tasks such as tag ranking, tag suggestion and refinement. Since they do not require a training stage they can be applied in cases in which the set of annotation keywords can vary greatly over time or when the set of images to be analysed is very large.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper describes our participation in the Scalable Concept Image Annotation subtask of the 2013 ImageCLEF competition <ref type="bibr" coords="1,330.97,428.97,9.96,8.74" target="#b1">[2]</ref>. It is a standardized benchmark for systems that automatically annotate images based on a varying vocabulary and a large corpus of web images with their corresponding web pages <ref type="bibr" coords="1,446.09,452.88,14.61,8.74" target="#b12">[13]</ref>. No annotated ground truth data is available, except for a small dataset which is used exclusively to test the system during its development. We submitted five runs using an unsupervised scalable approach based on nearest-neighbors by experimenting with several parameters.</p><p>Recently, data-driven approaches have shown to be able to deal with very large scale scenarios, and have been applied to tag ranking for social image retrieval, tag refinement for social image annotation <ref type="bibr" coords="1,350.91,536.57,10.52,8.74" target="#b8">[9,</ref><ref type="bibr" coords="1,363.09,536.57,7.75,8.74" target="#b6">7,</ref><ref type="bibr" coords="1,372.50,536.57,7.75,8.74" target="#b3">4,</ref><ref type="bibr" coords="1,381.91,536.57,11.62,8.74" target="#b11">12]</ref>. In order to address the problem of large-scale collections and maintaining an efficient approach, we choose to evaluate the use of such nearest-neighbor approaches also in the context of web images annotation.</p><p>Our approach, described in section 3, computes a visual distance between test images and train images and then obtains a score for several words in WordNet by performing a simple density estimation. Afterwards, a final score for each of the concepts from the requested vocabulary is obtained by evaluating several semantic similarities. Section 3 describes in more detail the various steps of the method; Section 4 reports the experimental setup used, while description of runs and results are reported in Section 5. Conclusions are drawn in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method Overview</head><p>Given a set of training images I with their respective web pages, a set of test images I T and a vocabulary of words V the goal is to get a relevance value r(i, w) ∀w ∈ V, ∀i ∈ I T and to choose a set of final annotations to be assigned. The latter ones can be simply obtained by using a threshold on relevance values or by enumerating the first fixed N words, where N has been determined empirically. Our method is comprised of four steps:</p><p>1. Building a set of artificial tags for every image in the provided training set.</p><p>This casts the problem as a tag refinement task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Features extraction from training and test images and computation of an</head><p>image -and corresponding web pages -neighborhood for every test image. 3. Construction of a candidate words set by image annotation, based on TagRelevance method over text features. 4. Filtering of stop-words and re-ranking of words by using several semantic metrics, defined on WordNet and Wikipedia ontologies.</p><p>The obtained scores correspond to the final relevance value assigned. Particular attention is given to the issue of scalability: our approach can scale up to utilize as many features and data as possible. Note that we are also assuming an openworld vocabulary which comprises potentially every possible word used on the web. However, as English WordNet and English Wikipedia are more mature, we consider only words which are contained in these two ontologies. This limitation can be possibly overcame by exploiting a system for automatic translation like Google Translate or Bing Translator; however, we have not used this approach in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A Nearest Neighbor approach</head><p>The basic idea of nearest-neighbor methods is to select a set of visually similar images and then to select a set of relevant associated words based on a word transfer procedure. Images selected as visually similar must be tagged with a set of words possibly related to the content. This type of methods has also been applied to different tasks such as tag suggestion and tag ranking/relevance, applied to the context of social media <ref type="bibr" coords="2,296.32,536.57,10.52,8.74" target="#b6">[7,</ref><ref type="bibr" coords="2,308.50,536.57,7.75,8.74" target="#b3">4,</ref><ref type="bibr" coords="2,317.91,536.57,11.62,8.74" target="#b11">12]</ref>. There is no need to use an explicit training of a model as it is implicitly made by the choosing of distance and space. In this ImageCLEF task, however, training images don't have any words (or tags) associated, instead they have one or more web pages with natural language text content. To overcame this issue, the first step is to build a training set of artificially labeled images to be used as a source of neighbors. Text, metadata and URLs from web pages are transformed in a set of tags for every image. It is not required to have perfect annotations as nearest neighbors method can make use of bigger training set, by simply using a bigger visual neighborhood sample to better estimate the specific tag distribution. A source of noise is related to the kind of relationship between the images and corresponding web pages: some images can possibly be completely disassociated to the content described in the web page. As there's no easy way to determine if this is the case, some images can possibly be artificially tagged with completely unrelated tags. In our experiments we directly employed a set of text features provided.</p><p>3.1 Learning Tag Relevance from Visual Neighbors: Li et al. <ref type="bibr" coords="3,448.47,182.98,12.09,8.77" target="#b6">[7]</ref> Li et al. have proposed a tag relevance measure for image retrieval based on the consideration, originally proposed in <ref type="bibr" coords="3,298.25,213.14,9.96,8.74" target="#b4">[5]</ref>, that if different persons label visually similar images using the same tags, then these tags are more likely to reflect objective aspects of the visual content. Therefore it can be assumed that the more frequently the tag occurs in the set of images that form the visual neighborhood of the image to be annotated, the more relevant it might be. However, some frequently occurring tags are unlikely to be relevant to the majority of images. To account for this fact the proposed tag relevance measurement takes into account both the distribution of a tag t in the neighbor set for an image I and in the entire collection:</p><formula xml:id="formula_0" coords="3,194.09,328.53,286.51,9.65">tagRelevance(t, I, K) := n t [N k (I, K)] -P rior(t, K)<label>(1)</label></formula><p>where n t is an operator counting the occurrences of t in the neighborhood N k (I, K) of K similar images, and P rior(t, K) is the occurrence frequency of t in the entire collection. In order to reduce user bias, only one image per different user is considered when computing the visual neighborhood. As the neighborhood increases in size, it can be proved that tags selected by TagRelevance yields to an ideal image ranking <ref type="bibr" coords="3,248.69,408.04,9.96,8.74" target="#b6">[7]</ref>, provided that probability to sample visually similar image is greater than sampling random images. The method has been tested for image retrieval on a proprietary Flickr dataset with 20,000 manually checked images and for image auto-annotation using a subset of 331 images. Recently it has been applied to a bigger social media dataset named NUSWIDE-240K <ref type="bibr" coords="3,462.32,455.86,14.61,8.74" target="#b11">[12]</ref>,</p><p>showing considerable performance for image retagging. Differently from the original approach of <ref type="bibr" coords="3,336.51,479.77,10.52,8.74" target="#b6">[7]</ref> we weight the occurrences of t with the distance: considering the setup of the auto-annotation experiment, we estimate tagRelevance for each candidate tag and then rank the tags in descending order by tagRelevance. Given a test image I the procedure used for tag refinement is:</p><p>1. Estimation of the distribution of each tag t of I in N k (I, K). 2. Computation of tagRelevance of each tag t subtracting P rior(t, K) from the distribution of t in N k (I, K). 3. Ranking of the tags according to their tagRelevance score. 4. Transfer the n highest ranking tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>The core of a working nearest neighbor approach is given by the space where images are represented and by the selection of a good distance measure. In our experiments we use only a visual space derived directly from the visual features provided with the ImageCLEF datasets. An early fusion is made by concatenating all the features provided (global color histogram, getlf, CSIFT, GIST, opponent SIFT, RGB-SIFT, SIFT) resulting in a 21,312 dimension space. All features are singularly normalized using 2 norm. In our implementation the distance between images is computed using a Gaussian kernel:</p><formula xml:id="formula_1" coords="4,265.72,198.96,214.87,23.89">d(I i , I k ) = e ||fi-f k || σ 2<label>(2)</label></formula><p>where I i is the visual neighbor in the i position, with N features f i = (f 1 i , . . . , f N i ), and σ is set as the median value of all the distances. The size of Development and Test sets, together, is only of 3, 000 images and the training set is comprised of 250, 000 images, which constitute a total of 3, 000 × 250, 000 = 750, 000, 000 distances. Given the relatively low number, we directly computed all distances exhaustively. The process took about three hours on a medium spec computer. In alternative, as the training set increases, one can use an approximate technique like LSH <ref type="bibr" coords="4,193.41,317.15,30.56,8.74">[3][11]</ref>, without losing too much precision. Assuming to measure distance in double-precision floats, a matrix of this dimension needs 7, 5 * 10 8 * 8 = 6 * 10 9 ∼ 6 GB of RAM, a relatively big size for a medium spec machine. To ease working, we precomputed distances between every image (either from development or test set) to all training images, retaining only the 4, 999 nearest. This results in two distance matrices of respectively 1, 000 × 4, 999 and 2, 000 × 4, 999 for development and test set of about 115 MB.</p><p>After some initial experiments we have used the square of the distance also to weight the occurrences of a tag t in the neighborhood of an image I k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Text Features</head><p>We used three kind of provided features: the score features, the triplets used to get images in search engines and the training URLs to create the set of tags associated with the images. The URLs were processed to extract the words that composed them by means of regular expressions and by checking their presence in WordNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Semantic Augmentation</head><p>To cope with the fact that the tag refinement approach used is applied to images that are associated with a set of textual features that could be different from the set of keywords to be used for annotation, we have tested some approaches to perform simple semantic augmentation of the tags resulting from the process described in Section 3.1.</p><p>Initially we have tried to add WordNet synonyms to the list of selected tags. However this approach has resulted in very limited improvement. A second approach has provided some steady improvement when using the Test dataset and therefore has been used also in all the runs on the Development dataset: 10 tags with the highest tag relevance score are selected, then the ImageCLEF keywords that have an overall strong semantic similarity with them are added to the list of candidate words used for annotation. This selection is performed by computing the average semantic distance between all the tags, considering the best semantic relatedness based on Wikipedia article internal links structure <ref type="bibr" coords="5,420.81,166.81,14.61,8.74" target="#b9">[10]</ref>, and then selecting the ImageCLEF keywords with a lower average distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Fusion Methods</head><p>Nearest neighbor approaches have proven to be able to use several distances by fusing the results of more than one system <ref type="bibr" coords="5,324.10,232.60,9.96,8.74" target="#b7">[8]</ref>. Inspired by this we tried to fuse several runs where parameters were different in number of neighbors (from 50 to 4,999), text features selected (several combinations of all features) and different distances ( 1 , 2 , χ 2 ). Following <ref type="bibr" coords="5,306.39,268.47,9.96,8.74" target="#b5">[6]</ref>, given the result of several classifiers X 1 , X 2 , . . . , X k ∈ R I×C , where I is the number of images and C is the number of categories, we employed simple fusion techniques Y = operator(X 1 , X 2 , . . . , X k ) without learned parameters, for completely unsupervised classifiers. We tried average, multiplication, max and min, followed or preceded by a soft-max operation. Another technique we tried is that of Borda count <ref type="bibr" coords="5,384.54,328.24,9.96,8.74" target="#b0">[1]</ref>, a well-known rank aggregation algorithm. However, none of the combinations resulted in more than very limited improvement, ranging from losing 3 -5% to improving 0.5% at the cost of several runs of executions of all the single modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Description of Runs and Discussion</head><p>We submitted five runs, using both Development and Test datasets. In all the runs we used all the pre-computed features to evaluate the visual neighborhood, score features were used as image tags and tag relevance was computed weighting the presence of tags using the squared visual distance between the image to be annotated and its visual neighborhood.</p><p>Only two parameters were varied during the runs: the number of tags assigned to each image and the size of the visual neighborhood.  <ref type="table" coords="5,281.06,572.43,4.98,8.74" target="#tab_1">1</ref> and 2, respectively. It can be observed that the larger the number of visual neighbors the better the performance. The improvement is much reduced on the Test dataset probably due to the over fitting induced by using the same set of images from the Training dataset to compute the visual neighborhood in both experiments.</p><p>The system has been completely developed in Python, without attempting to implement any particular optimization. Running all the experiments on a portable PC with 2.53 GHz Intel Core i5 processor takes about 2.5 hours. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper we have presented our system for web images annotation based on a data-driven approach that has been used for tag reranking in the context of social media. Thanks to its simplicity and the fact that it requires no training or supervision, the system can be executed on mid level PCs and can be easily applied to other datasets. The system has also just two main parameters that have to be adjusted: the number of images used to create the visual neighborhood of the images to be annotated and the number of tags to be selected for annotation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,160.32,117.78,294.72,188.72"><head>Table 1 .</head><label>1</label><figDesc>Experimental results of the 5 runs on the Development dataset</figDesc><table coords="6,213.20,117.78,188.95,188.72"><row><cell></cell><cell cols="4">Run F1 micro F1 macro MAP</cell></row><row><cell></cell><cell>1</cell><cell>20.4</cell><cell>20.3</cell><cell>28.7</cell></row><row><cell></cell><cell>2</cell><cell>23.3</cell><cell>20.7</cell><cell>29.0</cell></row><row><cell></cell><cell>3</cell><cell>22.3</cell><cell>21.0</cell><cell>29.0</cell></row><row><cell></cell><cell>4</cell><cell>22.4</cell><cell>21.0</cell><cell>29.2</cell></row><row><cell></cell><cell>5</cell><cell>22.7</cell><cell>21.4</cell><cell>29.1</cell></row><row><cell cols="4">Run F1 micro F1 macro MAP</cell><cell>F1 macro</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>unseen concepts</cell></row><row><cell>1</cell><cell>18.7</cell><cell>17.3</cell><cell>25.9</cell><cell>17.6</cell></row><row><cell>2</cell><cell>20.4</cell><cell>17.5</cell><cell>26.1</cell><cell>17.0</cell></row><row><cell>3</cell><cell>20.0</cell><cell>18.1</cell><cell>26.1</cell><cell>18.5</cell></row><row><cell>4</cell><cell>20.0</cell><cell>18.0</cell><cell>26.1</cell><cell>18.6</cell></row><row><cell>5</cell><cell>20.0</cell><cell>18.0</cell><cell>26.2</cell><cell>18.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,134.77,319.47,345.83,20.59"><head>Table 2 .</head><label>2</label><figDesc>Experimental results of the 5 runs on the Test dataset. The 5 th column reports results on the set of concepts that were not part of the Development set.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,142.96,536.65,337.64,7.86;6,151.52,547.61,329.07,7.86;6,151.52,558.57,185.14,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,269.88,536.65,90.39,7.86">Models for metasearch</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montague</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,383.10,536.65,97.49,7.86;6,151.52,547.61,329.07,7.86;6,151.52,558.57,74.32,7.86">Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 24th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="276" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,569.40,337.64,7.86;6,151.52,580.35,329.07,7.86;6,151.52,591.31,316.15,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,407.88,580.35,72.71,7.86;6,151.52,591.31,176.70,7.86">ImageCLEF 2013: the vision, the data and the open challenges</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zellhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">G</forename><surname>Varea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cazorla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,349.26,591.31,89.74,7.86">Proc. of CLEF, LNCS</title>
		<meeting>of CLEF, LNCS</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,602.14,337.64,7.86;6,151.52,613.10,329.07,7.86;6,151.52,624.06,76.80,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,320.69,602.14,159.91,7.86;6,151.52,613.10,29.16,7.86">Similarity search in high dimensions via hashing</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,202.27,613.10,274.38,7.86">Proceedings of the international conference on very large data bases</title>
		<meeting>the international conference on very large data bases</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="518" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,634.88,337.64,7.86;6,151.52,645.84,329.07,7.86;6,151.52,656.80,52.21,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,382.08,634.88,98.51,7.86;6,151.52,645.84,276.74,7.86">Tagprop: Discriminative metric learning in nearest neighbor models for image auto-annotation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,449.00,645.84,31.59,7.86;6,151.52,656.80,23.55,7.86">Proc. of ICCV</title>
		<meeting>of ICCV</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,119.67,337.64,7.86;7,151.52,130.63,329.07,7.86;7,151.52,141.59,326.40,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,331.85,119.67,148.74,7.86;7,151.52,130.63,285.54,7.86">Reliable tags using image similarity: mining specificity and expertise from large-scale multimedia databases</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">S</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Slaney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,459.20,130.63,21.40,7.86;7,151.52,141.59,232.20,7.86">Proc. of ACM-MM Workshop on Web-Scale Multimedia Corpus</title>
		<meeting>of ACM-MM Workshop on Web-Scale Multimedia Corpus<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,152.55,337.63,7.86;7,151.52,163.51,75.85,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="7,217.64,152.55,228.32,7.86">Combining Pattern Classifiers: Methods and Algorithms</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">I</forename><surname>Kuncheva</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Wiley-Interscience</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,174.47,337.64,7.86;7,151.52,185.43,269.03,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,307.78,174.47,172.81,7.86;7,151.52,185.43,23.69,7.86">Learning social tag relevance by neighbor voting</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,182.24,185.43,138.48,7.86">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1310" to="1322" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,196.39,337.64,7.86;7,151.52,207.34,266.39,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,310.69,196.39,169.90,7.86;7,151.52,207.34,133.54,7.86">Unsupervised multi-feature tag relevance learning for social image retrieval</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,305.77,207.34,83.46,7.86">Proc. of ACM CIVR</title>
		<meeting>of ACM CIVR</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,218.30,337.64,7.86;7,151.52,229.26,90.12,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,311.99,218.30,149.68,7.86">A new baseline for image annotation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Makadia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,151.52,229.26,61.46,7.86">Proc. of ECCV</title>
		<meeting>of ECCV</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.62,240.22,337.98,7.86;7,151.52,251.18,230.09,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,255.30,240.22,225.29,7.86;7,151.52,251.18,121.70,7.86">An effective, low-cost measure of semantic relatedness obtained from Wikipedia links</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Milne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,293.92,251.18,59.02,7.86">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.62,262.14,337.98,7.86;7,151.52,273.10,329.07,7.86;7,151.52,284.06,256.23,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,247.56,262.14,233.03,7.86;7,151.52,273.10,76.60,7.86">Fast approximate nearest neighbors with automatic algorithm configuration</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Muja</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,248.48,273.10,232.11,7.86;7,151.52,284.06,102.94,7.86">International Conference on Computer Vision Theory and Application VISSAPP&apos;09)</title>
		<imprint>
			<publisher>INSTICC Press</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="331" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.62,295.02,337.98,7.86;7,151.52,305.98,255.36,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,375.46,295.02,105.14,7.86;7,151.52,305.98,145.81,7.86">An evaluation of nearestneighbor methods for tag refinement</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Uricchio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ballan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bertini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Del Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,318.55,305.98,59.66,7.86">Proc. of ICME</title>
		<meeting>of ICME</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.62,316.93,337.98,7.86;7,151.52,327.89,329.07,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,315.61,316.93,164.99,7.86;7,151.52,327.89,133.56,7.86">Overview of the imageclef 2013 scalable concept image annotation subtask</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,304.75,327.89,80.65,7.86">CLEF working notes</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
