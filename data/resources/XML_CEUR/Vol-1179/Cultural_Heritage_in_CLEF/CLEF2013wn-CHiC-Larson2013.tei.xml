<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,150.42,115.96,314.53,12.62;1,284.77,133.89,45.81,12.62">Pseudo-Relevance Feedback for CLEF-CHiC Adhoc</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,275.88,171.56,63.60,8.74"><forename type="first">Ray</forename><forename type="middle">R</forename><surname>Larson</surname></persName>
							<email>ray@sims.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,150.42,115.96,314.53,12.62;1,284.77,133.89,45.81,12.62">Pseudo-Relevance Feedback for CLEF-CHiC Adhoc</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B1F601A2ED9DB5B3F54A2B3705E48240</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we will briefly describe the approaches taken by the Cheshire (Berkeley) Group for the CLEF CHiC Adhoc tasks (Monolingual, Bilingual and Multilingual retrieval for English, French and German). We used multiple translations of the topics for searching each of the CHiC Europeana English, French and German subcollections, employing Google Translate as our translation system. In addition we combined the original topics for various multilingual runs. Once again this year our approach was to use probabilistic text retrieval based on logistic regression and incorporating pseudo relevance feedback for all of the runs. The results overall, when viewed using the multilingual qrels based on the entire set of languages for the CHiC collection, were not good, while the individual monolingual runs using only the collection-specific qrels, appear to have performed reasonably well. There is some question about the qrels for the the entire multilingual collection, since there appear to be no relevant documents at all from the English collection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The collections used for the Cultural Heritage in CLEF (CHiC) track are derived from the Europeana Digital Library and contain metadata describing materials ranging from text documents to photographs, museum objects, historic locations and persons.</p><p>Each the collections used in the CLEF Adhoc CHiC track are considered to be "mainly" in a particular language (at least in the English, French, and German collections), according to the language codes specified the in the language attribute of the metadata tag, however records also included descriptive metadata in virtually all other languages as part of the "enrichment:concept label" elements. Thus, and English record would also include concept names in other languages as well. This overlap of languages presents an interesting multilingual search problem, and we explored it by using tranlations of topics into each of the other languages (we used only English, French and German for this) and combining those translations with the original topics in some of our submissions.</p><p>In this paper we review the retrieval algorithms and evaluation results for Berkeley's official submissions for the Adhoc-CHiC 2013 track. All of the runs were automatic without manual intervention in the queries (or translations). We submitted ten Multilingual runs (using various combinations of query languages and searching the English, French and German collections), six Bilingual runs (two for each target language German, English and French) and three monolingual runs for the three target languages.</p><p>This paper first describes the retrieval algorithms used for our submissions, followed by a discussion of the processing used for the runs. We then examine the results obtained for our official runs, and finally present conclusions and future directions for Adhoc-CHiC participation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Retrieval Algorithms</head><p>Note that this section is virtually identical to one that appears in our papers from previous CLEF participation and appears here for reference only <ref type="bibr" coords="2,436.71,283.50,12.55,8.74" target="#b8">[8,</ref><ref type="bibr" coords="2,450.96,283.50,8.15,8.74" target="#b7">7]</ref> The basic form and variables of the Logistic Regression (LR) algorithm used for all of our submissions was originally developed by Cooper, et al. <ref type="bibr" coords="2,407.91,307.41,9.96,8.74" target="#b5">[5]</ref>. As originally formulated, the LR model of probabilistic IR attempts to estimate the probability of relevance for each document based on a set of statistics about a document collection and a set of queries in combination with a set of weighting coefficients for those statistics. The statistics to be used and the values of the coefficients are obtained from regression analysis of a sample of a collection (or similar test collection) for some set of queries where relevance and non-relevance has been determined. More formally, given a particular query and a particular document in a collection P (R | Q, D) is calculated and the documents or components are presented to the user ranked in order of decreasing values of that probability. To avoid invalid probability values, the usual calculation of P (R | Q, D) uses the "log odds" of relevance given a set of S statistics, s i , derived from the query and database, such that:</p><formula xml:id="formula_0" coords="2,241.30,471.78,239.29,30.32">log O(R | Q, D) = b 0 + S i=1 b i s i (1)</formula><p>where b 0 is the intercept term and the b i are the coefficients obtained from the regression analysis of the sample collection and relevance judgements. The final ranking is determined by the conversion of the log odds form to probabilities:</p><formula xml:id="formula_1" coords="2,238.57,557.30,242.02,24.06">P (R | Q, D) = e log O(R|Q,D) 1 + e log O(R|Q,D)<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">TREC2 Logistic Regression Algorithm</head><p>For Adhoc-CHiC we used a version the Logistic Regression (LR) algorithm that has been used very successfully in Cross-Language IR by Berkeley researchers for a number of years <ref type="bibr" coords="2,222.65,644.16,11.83,8.74" target="#b3">[3]</ref>. The formal definition of the TREC2 Logistic Regression algorithm used is:</p><formula xml:id="formula_2" coords="3,197.47,138.27,283.12,146.86">log O(R|C, Q) = log p(R|C, Q) 1 -p(R|C, Q) = log p(R|C, Q) p(R|C, Q) = c 0 + c 1 * 1 |Q c | + 1 |Qc| i=1 qtf i ql + 35 + c 2 * 1 |Q c | + 1 |Qc| i=1 log tf i cl + 80 (3) -c 3 * 1 |Q c | + 1 |Qc| i=1 log ctf i N t + c 4 * |Q c |</formula><p>where C denotes a document component (i.e., an indexed part of a document which may be the entire document) and Q a query, R is a relevance variable,</p><formula xml:id="formula_3" coords="3,134.77,331.82,345.83,58.32">p(R|C, Q) is the probability that document component C is relevant to query Q, p(R|C, Q) the probability that document component C is not relevant to query Q, which is 1.0 -p(R|C, Q) |Q c |</formula><p>is the number of matching terms between a document component and a query, qtf i is the within-query frequency of the ith matching term, tf i is the within-document frequency of the ith matching term, ctf i is the occurrence frequency in a collection of the ith matching term, ql is query length (i.e., number of terms in a query like |Q| for non-feedback situations), cl is component length (i.e., number of terms in a component), and N t is collection length (i.e., number of terms in a test collection). c k are the k coefficients obtained though the regression analysis.</p><p>If stopwords are removed from indexing, then ql, cl, and N t are the query length, document length, and collection length, respectively. If the query terms are re-weighted (in feedback, for example), then qtf i is no longer the original term frequency, but the new weight, and ql is the sum of the new weight values for the query terms. Note that, unlike the document and collection lengths, query length is the "optimized" relative frequency without first taking the log over the matching terms.</p><p>The coefficients were determined by fitting the logistic regression model specified in log O(R|C, Q) to TREC training data using a statistical software package. The coefficients, c k , used for our official runs are the same as those described by Chen <ref type="bibr" coords="3,167.46,632.21,13.49,8.74" target="#b0">[1]</ref>. These were: c 0 = -3.51, c 1 = 37.4, c 2 = 0.330, c 3 = 0.1937 and c 4 = 0.0929. Further details on the TREC2 version of the Logistic Regression algorithm may be found in Cooper et al. <ref type="bibr" coords="3,315.26,656.12,9.96,8.74" target="#b4">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pseudo Relevance Feedback</head><p>Instead of performing direct retrieval of documents using the TREC2 logistic regression algorithm described above, we have implmented a "pseudo relevance feedback" step as part of the search processing. The algorithm used for pseudo relevance feedback was by Chen <ref type="bibr" coords="4,278.40,174.39,10.52,8.74" target="#b2">[2]</ref> for an earlier search system used in CLEF. Pseudo relevance feedback has become well-known in the information retrieval community primarily because of its apparent ability to provide consistent improvements over initial search results as seen in TREC, CLEF and other retrieval evaluations <ref type="bibr" coords="4,186.83,222.21,9.96,8.74" target="#b6">[6]</ref>. While the most commonly used algorithm for pseudo relevance feedback is the Rocchio algorithm originally developed for the SMART system, we have adopted a probabilistic approach based on the probabilistic term relevance weighting formula developed by Robertson and Sparck Jones <ref type="bibr" coords="4,431.07,258.07,9.96,8.74" target="#b9">[9]</ref>.</p><p>Pseudo relevance feedback is typically performed in two stages. First, an initial search using the original topic statement is performed, after which a number of terms are selected from some number of the top-ranked documents (which are presumed to be relevant). The selected terms are then weighted and then merged with the initial query to formulate a new query. Finally the reweighted and expanded query is submitted against the same collection to produce a final ranked list of documents. Obviously there are important choices to be made regarding the number of top-ranked documents to consider, and the number of terms to extract from those documents. For CHiC this year, we chose to use the top 10 terms from 10 top-ranked documents, since these parameters have worked well in similar evaluations. The terms were chosen by extracting the document vectors for each of the 10 and computing the Robertson and Sparck Jones term relevance weight for each document. This weight is based on a contingency table where the counts of 4 different conditions for combinations of (assumed) relevance and whether or not the term is, or is not in a document. Table <ref type="table" coords="4,446.52,437.40,4.98,8.74" target="#tab_0">1</ref> shows this contingency table. </p><formula xml:id="formula_4" coords="4,214.26,498.24,186.33,30.58">Nt -Rt Nt Not in doc R -Rt N -Nt -R + Rt N -Nt R N -R N</formula><p>The relevance weight is calculated using the assumption that the first 10 documents are relevant and all others are not. For each term in these documents the following weight is calculated:</p><formula xml:id="formula_5" coords="4,261.48,595.51,219.11,29.71">w t = log Rt R-Rt Nt-Rt N -Nt-R+Rt (4)</formula><p>The 10 terms (including those that appeared in the original query) with the highest w t are selected and added to the original query terms. For the terms not in the original query, the new "term frequency" (qtf i in main LR equation above) is set to 0.5. Terms that were in the original query, but are not in the top 10 terms are left with their original qtf i . For terms in the top 10 and in the original query the new qtf i is set to 1.5 times the original qtf i for the query. The new query is then processed using the same LR algorithm as shown in Equation <ref type="formula" coords="5,134.77,166.81,4.98,8.74">4</ref>and the ranked results returned as the response for that topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approaches for CHiC Adhoc</head><p>In this section we describe the specific approaches taken for our submitted runs for the CHiC Adhoc task. First we describe the indexing and term extraction methods used, and then the search features we used for the submitted runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Indexing and Term Extraction</head><p>The Cheshire II system uses the XML structure of the documents to extract selected portions for indexing and retrieval. Any combination of tags can be used to define the index contents. Table <ref type="table" coords="5,178.40,499.50,4.98,8.74" target="#tab_1">2</ref> lists the indexes created by the Cheshire II system for the three Adhoc-CHiC databases and the document elements from which the contents of those indexes were extracted. The "Used" column in Table <ref type="table" coords="5,386.43,523.41,4.98,8.74" target="#tab_1">2</ref> indicates whether or not a particular index was used in the submitted Adhoc-CHiC runs. As the table shows we used only the topic index, which contains most of the content-bearing parts of records, for all of our submitted runs. In addition to the databases for the individual language collection, we used a Cheshire II feature that allows searching across multiple databases and merging the ranked results for searches of all three languages.</p><p>For all indexing we used language-specific stoplists to exclude function words and very common words from the indexing and searching. The German language runs did not use decompounding in the indexing and querying processes to generate simple word forms from compounds. The Snowball stemmer was used by Cheshire for language-specific stemming. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Search Processing</head><p>Searching the Adhoc-CHiC collection using the Cheshire II system involved using TCL scripts to parse the topics and submit the title element from the topics.</p><p>For monolingual search tasks we used the topics in the appropriate language (English, German, and French) and searched the specific language collection. For bilingual tasks the topics were translated from the source language to the target language using Google Translate (this involved converting the original topics to HTML (as if the set of topics was a web page) and then asking Google Translate to translate the web page, then the translated page was converted back to the topic schema, again the matching language database for the translated topics was searched.</p><p>For multilingual search tasks we used the original topics, but searched that topic (without translation) in all three (EN, FR, DE) databases and combined the results based on MINMAX normalized scores. The topics used include additional languages other than the main language of the database (e.g., Dutch, Finnish and Italian) without translation, and combinations of multiple languages for a given topic. The</p><p>The scripts for each run submitted the topic elements as they appeared in the topic or expanded topic to the system for TREC2 logistic regression searching with pseudo feedback. Only the "title" topic element was used, and where appropriate multiple titles in different languages were combined into a single probabilistic query and searched using the "topic" index as described in Table <ref type="table" coords="6,472.84,656.12,3.87,8.74" target="#tab_1">2</ref>.  <ref type="table" coords="7,268.51,428.97,3.87,8.74" target="#tab_2">3</ref>. In this table each column represents the MAP results for a particular set of the task qrels. The EN column uses only qrels for the English subcollection, similarly for German (DE) and French (FR). The ENFRDE column combines the qrels for the three languages and while the ML column is the official MAP calculated using all language collections. The Recall-Precision curves for these runs are also shown in Figures 1 (for monolingual) and 2 (for bilingual) and 3 (for Multilingual). In Figures <ref type="figure" coords="7,447.44,500.70,4.98,8.74" target="#fig_0">1</ref> and<ref type="figure" coords="7,475.61,500.70,4.98,8.74" target="#fig_1">2</ref> the names for the individual runs represent the language codes, which can easily be compared with full names and descriptions in Table <ref type="table" coords="7,381.97,524.61,4.98,8.74" target="#tab_2">3</ref> (since each language combination has only a single run).</p><p>Table <ref type="table" coords="7,177.03,548.52,4.98,8.74" target="#tab_2">3</ref> indicates runs that had the highest overall MAP for the task within a given qrel set and subtask (monolingual, translated bilingual and multilingual) by asterisks next to the scores.</p><p>The results Table <ref type="table" coords="7,240.95,584.39,4.98,8.74" target="#tab_2">3</ref> show, as might be expected, the best results are usually the pure monolingual approach for a single language subcollection. We did have the interesting case again (which has happened before in other CLEF and NTCIR evaluations for us) that an bilingual version of a query in another language translated to English outperforms the "native" English query on the same collection. In this case the bilingual German to English translation did better in searching the English collection than the original English topic. Also not very surprising was the fact that particular languages used in searching the collections of different languages typically ended up with a MAP of zero or very near zero. What is rather strange however, is that it appears that no results were obtained from the English collections for all multilingual searches (which were supposed to be searching the English, German and French collections). Since MINMAX normalization of scores was used, it seems unlikely that this was the result of a scoring difference. It may be, however, that some system error that is causing the system to fail to combine all of the result for all three language collections. We will be checking into this later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>Our overall results this year compared poorly with the best performing systems, based on the track summary data on the DIRECT system. However, even our best runs included only the results from the English, French, and German subcollections, and no other languages, this alone apparently was enough to drastically lower the results. Interestingly, our monolingual run against only the German collection was our best performing run (when evaluated using the full multillingual qrels), which seems to indicate a predominance of German documents in the overall relevant. What was also interesting was that single language searching the three collections using languages outside of the main content of the collections including Dutch, Finnish, Italian and Spanish, provided non-zero results from each of the qrel sets, except for English. This would seem to indicated </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,236.61,354.62,142.13,7.89;7,149.17,115.84,317.02,224.02"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Berkeley Monolingual Runs</figDesc><graphic coords="7,149.17,115.84,317.02,224.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,243.46,353.62,128.45,7.89;8,147.92,115.84,319.52,223.02"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Berkeley Bilingual Runs</figDesc><graphic coords="8,147.92,115.84,319.52,223.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,236.87,353.62,141.62,7.89;9,148.42,115.84,318.52,223.02"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Berkeley Multilingual Runs</figDesc><graphic coords="9,148.42,115.84,318.52,223.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,192.30,466.48,230.77,39.62"><head>Table 1 .</head><label>1</label><figDesc>Contingency table for term relevance weighting</figDesc><table coords="4,214.26,486.88,136.65,19.22"><row><cell></cell><cell>Relevant Not Relevant</cell></row><row><cell>In doc</cell><cell>Rt</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,183.16,361.94,249.03,107.62"><head>Table 2 .</head><label>2</label><figDesc>Cheshire II Indexes for Adhoc-CHiC 2006</figDesc><table coords="5,183.16,380.99,249.03,88.56"><row><cell>Name</cell><cell>Description</cell><cell>Content Tags</cell><cell>Used</cell></row><row><cell>URI</cell><cell cols="2">Europeana URI europeana:uri</cell><cell>no</cell></row><row><cell></cell><cell></cell><cell>dc:identifier</cell><cell></cell></row><row><cell cols="3">contributor Contributing Inst. dc:contributor</cell><cell>no</cell></row><row><cell>subject</cell><cell>Topics</cell><cell>dc:subject</cell><cell>no</cell></row><row><cell>title</cell><cell>Title</cell><cell>dc:title</cell><cell>no</cell></row><row><cell></cell><cell></cell><cell>dc:subject, dc:description</cell><cell></cell></row><row><cell>topic</cell><cell>Entire record</cell><cell>ims:fields</cell><cell>yes</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,169.06,115.91,277.24,250.43"><head>Table 3 .</head><label>3</label><figDesc>Mean Average Precision for Different QRels</figDesc><table coords="6,169.06,136.71,277.24,229.63"><row><cell>RunID</cell><cell>EN</cell><cell>FR</cell><cell>DE</cell><cell cols="2">ENFRDE ML</cell></row><row><cell>BerkMonoEN01</cell><cell cols="4">0.1842* 0.0000 0.0000 0.0645</cell><cell>0.0375</cell></row><row><cell>BerkMonoFR02</cell><cell cols="4">0.0000 0.2014* 0.0000 0.0648</cell><cell>0.0325</cell></row><row><cell>BerkMonoDE03</cell><cell cols="5">0.0000 0.0000 0.1757* 0.0857* 0.0440*</cell></row><row><cell>BerkBiDEEN04</cell><cell cols="4">0.1942* 0.0000 0.0000 0.0641</cell><cell>0.0371*</cell></row><row><cell>BerkBiFREN05</cell><cell cols="4">0.1744 0.0000 0.0000 0.0579</cell><cell>0.0333</cell></row><row><cell>BerkBiFRDE06</cell><cell cols="4">0.0000 0.0000 0.1474 0.0679</cell><cell>0.0330</cell></row><row><cell>BerkBiENFR07</cell><cell cols="4">0.0000 0.1608* 0.0000 0.0555</cell><cell>0.0284</cell></row><row><cell>BerkBiDEFR08</cell><cell cols="4">0.0000 0.1296 0.0000 0.0391</cell><cell>0.0217</cell></row><row><cell>BerkBiENDE09</cell><cell cols="5">0.0000 0.0000 0.1785* 0.0726* 0.0331</cell></row><row><cell>BerkMLEN10</cell><cell cols="4">0.0000 0.0274 0.0379 0.0350</cell><cell>0.0166</cell></row><row><cell>BerkMLFR11</cell><cell cols="4">0.0000 0.0896* 0.0185 0.0474</cell><cell>0.0231</cell></row><row><cell>BerkMLDE12</cell><cell cols="4">0.0000 0.0205 0.1260* 0.0734</cell><cell>0.0385</cell></row><row><cell>BerkMLDU13</cell><cell cols="4">0.0000 0.0244 0.0368 0.0337</cell><cell>0.0198</cell></row><row><cell>BerkMLFI14</cell><cell cols="4">0.0000 0.0369 0.0336 0.0406</cell><cell>0.0273</cell></row><row><cell>BerkMLIT15</cell><cell cols="4">0.0000 0.0222 0.0196 0.0207</cell><cell>0.0123</cell></row><row><cell>BerkMLSP16</cell><cell cols="4">0.0000 0.0200 0.0254 0.0260</cell><cell>0.0148</cell></row><row><cell>BerkMLALL17</cell><cell cols="4">0.0000 0.0485 0.0758 0.0736</cell><cell>0.0357</cell></row><row><cell cols="5">BerkMLSPENFRDEIT18 0.0000 0.0597 0.0673 0.0737</cell><cell>0.0353</cell></row><row><cell>BerkMLENFRDE19</cell><cell cols="5">0.0000 0.0642 0.0763 0.0803* 0.0393*</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,138.35,449.13,342.24,7.86" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="9,202.53,449.13,274.25,7.86">Multilingual information retrieval using english and chinese queries</title>
		<author>
			<persName coords=""><forename type="first">Aitao</forename><surname>Chen</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,146.91,460.09,333.68,7.86;9,146.91,471.05,333.68,7.86;9,146.91,482.01,333.67,7.86;9,146.91,492.97,289.72,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,459.76,460.09,20.83,7.86;9,146.91,471.05,333.68,7.86;9,146.91,482.01,137.99,7.86">Evaluation of Cross-Language Information Retrieval Systems: Second Workshop of the Cross-Language Evaluation Forum</title>
	</analytic>
	<monogr>
		<title level="m" coord="9,293.60,482.01,44.59,7.86">CLEF-2001</title>
		<title level="s" coord="9,225.18,492.97,163.33,7.86">Springer Computer Scinece Series LNCS</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Martin</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><surname>Kluck</surname></persName>
		</editor>
		<meeting><address><addrLine>Darmstadt, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-09">September 2001. 2002</date>
			<biblScope unit="volume">2406</biblScope>
			<biblScope unit="page" from="44" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,503.82,342.24,7.86;9,146.91,514.77,124.32,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="9,203.07,503.82,196.72,7.86">Cross-Language Retrieval Experiments at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Aitao</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002. 2003</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">2785</biblScope>
			<biblScope unit="page" from="28" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,525.62,342.24,7.86;9,146.91,536.58,333.68,7.86;9,146.91,547.54,40.45,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,283.26,525.62,197.33,7.86;9,146.91,536.58,205.98,7.86">Multilingual information retrieval using machine translation, relevance feedback and decompounding</title>
		<author>
			<persName coords=""><forename type="first">Aitao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fredric</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,361.98,536.58,86.00,7.86">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="149" to="182" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,558.39,342.25,7.86;9,146.91,569.35,333.68,7.86;9,146.91,580.31,169.73,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,316.43,558.39,164.17,7.86;9,146.91,569.35,248.82,7.86">Full Text Retrieval based on Probabilistic Equations with Coefficients fitted by Logistic Regression</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">S</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,420.47,569.35,60.12,7.86;9,146.91,580.31,86.94,7.86">Text REtrieval Conference (TREC-2)</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,591.15,342.24,7.86;9,146.91,602.11,333.68,7.86;9,146.91,613.07,333.68,7.86;9,146.91,624.03,233.67,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,393.07,591.15,87.53,7.86;9,146.91,602.11,134.98,7.86">Probabilistic retrieval based on staged logistic regression</title>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">S</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fredric</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><forename type="middle">P</forename><surname>Dabney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,300.15,602.11,180.45,7.86;9,146.91,613.07,252.76,7.86">15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>Copenhagen, Denmark; New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1992">June 21-24. 1992</date>
			<biblScope unit="page" from="198" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,634.88,342.24,7.86;9,146.91,645.84,333.68,7.86;9,146.91,656.80,110.97,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,217.11,634.88,263.48,7.86;9,146.91,645.84,55.64,7.86">Probabilistic retrieval, component fusion and blind feedback for XML retrieval</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Larson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,220.12,645.84,43.31,7.86">INEX 2005</title>
		<title level="s" coord="9,372.51,645.84,108.08,7.86;9,146.91,656.80,59.26,7.86">Lecture Notes in Computer Science, LNCS</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">3977</biblScope>
			<biblScope unit="page" from="225" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,119.67,342.24,7.86;10,146.91,130.63,333.68,7.86;10,146.91,141.59,333.68,7.86;10,146.91,152.55,169.44,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,219.33,119.67,257.46,7.86">Cheshire at GeoCLEF 2007: Retesting text retrieval baselines</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Larson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,158.46,130.63,275.54,7.86">8th Workshop of the Cross-Language Evaluation Forum, CLEF 2007</title>
		<meeting><address><addrLine>Budapest, Hungary; Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09-21">September 19-21, 2007. September 2008</date>
			<biblScope unit="volume">5152</biblScope>
			<biblScope unit="page" from="811" to="814" />
		</imprint>
	</monogr>
	<note>Revised Selected Papers</note>
</biblStruct>

<biblStruct coords="10,138.35,163.51,342.24,7.86;10,146.91,174.47,333.68,7.86;10,146.91,185.43,333.68,7.86;10,146.91,196.39,331.86,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,216.27,163.51,264.32,7.86;10,146.91,174.47,169.85,7.86">Experiments in classification clustering and thesaurus expansion for domain specific cross-language retrieval</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Larson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,334.49,174.47,146.10,7.86;10,146.91,185.43,121.43,7.86">8th Workshop of the Cross-Language Evaluation Forum, CLEF 2007</title>
		<meeting><address><addrLine>Budapest, Hungary; Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09-21">September 19-21, 2007. September 2008</date>
			<biblScope unit="volume">5152</biblScope>
			<biblScope unit="page" from="188" to="195" />
		</imprint>
	</monogr>
	<note>Revised Selected Papers</note>
</biblStruct>

<biblStruct coords="10,138.35,207.34,342.24,7.86;10,146.91,218.30,329.46,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,300.44,207.34,142.01,7.86">Relevance weighting of search terms</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Sparck</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,450.00,207.34,30.59,7.86;10,146.91,218.30,193.93,7.86">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="page" from="129" to="146" />
			<date type="published" when="1976-06">May-June 1976</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
