<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,135.68,115.90,343.99,12.90;1,223.43,135.75,168.50,10.75">INAOE&apos;s participation at PAN&apos;13: Author Profiling task Notebook for PAN at CLEF 2013</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,202.52,172.15,97.30,8.64"><forename type="first">A</forename><surname>Pastor López-Monroy</surname></persName>
						</author>
						<author>
							<persName coords="1,307.14,172.15,100.84,8.64"><forename type="first">Manuel</forename><surname>Montes-Y-Gómez</surname></persName>
						</author>
						<author>
							<persName coords="1,168.30,184.10,78.10,8.64"><forename type="first">Hugo</forename><forename type="middle">Jair</forename><surname>Escalante</surname></persName>
						</author>
						<author>
							<persName coords="1,252.96,184.10,89.75,8.64"><forename type="first">Luis</forename><surname>Villaseñor-Pineda</surname></persName>
						</author>
						<author>
							<persName coords="1,366.17,184.10,80.89,8.64"><forename type="first">Esaú</forename><surname>Villatoro-Tello</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Óptica y Electrónica</orgName>
								<orgName type="institution">Instituto Nacional de Astrofísica</orgName>
								<address>
									<country key="MX">México</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Information Technologies Department</orgName>
								<orgName type="institution">Universidad Autónoma Metropolitana-Cuajimalpa</orgName>
								<address>
									<country key="MX">México</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,135.68,115.90,343.99,12.90;1,223.43,135.75,168.50,10.75">INAOE&apos;s participation at PAN&apos;13: Author Profiling task Notebook for PAN at CLEF 2013</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">76F86B54A81A2FDEC74E8620B4475B52</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of the Laboratory of Language Technologies of INAOE at PAN 2013 evaluation lab. We adopted second order representations for facing the problem of Author Profiling (AP). This representation tackles two shortcomings of the typical Bag-of-Terms: i) the sparsity and high dimensionality of document representations, and ii) the assumption of total independence between terms in documents. In order to overcome these problems the proposed representation builds document vectors in a space of the different profiles, which represent the relationships of each document with the different profiles (say, age and gender). In order to evaluate our approach, we compare the proposed representation against a standard Bag-of-Terms representation using the PAN 2013 corpus for AP. We found that the second order attributes using a low computational cost, show evidence of being useful to determine genre and age profile.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Author Profiling (AP) task consists in knowing as much as possible about an unknown author, just by analyzing a given text <ref type="bibr" coords="1,315.51,493.53,10.58,8.64" target="#b1">[2]</ref>. The interest in AP task is growing in recent years, this is due, in part, to the huge amount of information in plain text available on internet. In this context, several applications related to AP are emerging, some of them have to do with business intelligence, computer forensics and security. One way to address the AP task is to approach it as a single-label multiclass classification problem, where profiles represent the classes to discriminate.</p><p>The representation of documents is a key procedure for AP. Currently, one of the most common approaches for document representation is the Bag of Terms (BOT). BOT representation builds feature vectors of documents, taking each term in the vocabulary as an attribute. However, BOT like representations have some drawbacks:</p><p>1. Terms are considered independent of other elements in the problem: We believe that valuable information that may help to deal with the AP problem is being ignored.</p><p>In this context, we propose taking into account relationships between profiles and terms.</p><p>2. High dimensionality and high sparsity of vectors: both affect the representation and the performance of the classification algorithms, and could be impractical in some situations. In this way, we focus in a representation based in second order attributes rich in representativeness, which represents relations with each profile.</p><p>In summary, to overcome the above issues we propose to use a low dimensional representation with high level of representativeness. In this way, in our proposal we follow some ideas from Concise Semantic Analysis (CSA) <ref type="bibr" coords="2,339.31,201.69,11.62,8.64" target="#b2">[3]</ref> to achieve relationships between documents and profiles. Thus, our approach intends to exploit the use of second order attributes for the AP task. For this, we use: i) the term frequency tf weighting scheme in order to capture the use of the stylistic terms (e.g., stopwords, puntuation marks, etc.), and ii) we provide an effective normalization to deal with the high imbalanced data.</p><p>In summary, The rest of this paper is organized as follows: Section 2 introduces the proposed representation, Section 3 explains how we performed the experiments and the results we obtained, finally Section 4 shows our conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Second order attributes for Author Profiling</head><p>From a general point of view, the proposed representation is built through two main stages: i) To build term vectors in a space of profiles, and ii) To build document vectors in a space of profiles. The rest of this section explains both steps in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Term Representation</head><p>Estimating the relationships between each term and profiles is the first step to get the second order attributes. In this way, it is necessary to construct a vector representation for each term. Terms could be any textual unit used as document feature, for example, words, n-grams, punctuation marks, etc.</p><p>The main idea behind this first step is to capture the relation that each term maintains with different profiles. In other words, we compute a value that shows how a term t j is used in each profile p i . Let {t 1 , . . . , t m } denote the vocabulary in the collection, and {p 1 , . . . , p n } be the set of profiles to be analyzed. For each term t j in the vocabulary, we build a term vector t j = tp 1j , . . . , tp nj , where tp ij is a real value representing the relationship of the term t j with the profile p i . For computing tp ij we mainly take into account those documents that belong to the profile p i . The relationship of a term with a profile considers the relative term frequency just in the documents of this profile. Thus, high frequencies will show more preference for the term in a given set of documents. Equation 1 follows the above idea and computes a relative weight as:</p><formula xml:id="formula_0" coords="2,233.69,604.06,246.90,28.49">w ij = k:d k ∈Pi log 2 1 + tf kj len(d k )<label>(1)</label></formula><p>where P i is the set of documents that belong to profile p i , tf kj is the number of occurrences of the term t j in the document d k , and len(d k ) is the length of the document d k . The function in equation 1 is to soften the most frequent terms of the corpus. Finally, we apply a simple normalization (Equation <ref type="formula" coords="3,330.91,131.27,3.94,8.64" target="#formula_1">2</ref>.1) for computing tp ij . Note that this normalization takes into account the weights computed for other profiles, causing each weight being relative to all profiles.</p><p>(2.1)</p><formula xml:id="formula_1" coords="3,211.96,172.03,268.63,43.77">tp ij = w ij T ERM S i=1 w ij (2.2) tp ij = w ij P ROF ILES i=1 w ij<label>(2)</label></formula><p>It is worth noting that, until this step, the second order attributes are sensible to highly unbalanced data (as the PAN13 corpus). That is, the relation of each term with each profile attribute will be higher for larger classes just because those classes have more documents and then more occurrences of certain terms. For that reason, we apply a simple but effective normalization over each profile in order to consider the proportion of the term in each profile. Equation <ref type="formula" coords="3,286.16,282.03,4.15,8.64" target="#formula_1">2</ref>.2 shows the latter idea. Note that this normalization takes into account the weights computed for other terms, causing each weight being relative to all terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Document Representation</head><p>After computing term vectors in a space of profiles, we build relationships between documents and profiles; these are the second-order attributes. We compute these adding term vectors of the terms contained in the documents. In this way, we will have documents represented as d k = dp 1k , . . . , dp nk , where n is the total number of existing profiles, and dp ik is a real value representing the relationship of the document d k with the profile p i . Additionally, each term vector, before being added, is weighted by the relative frequency of the term t j in the document d k . Equation (3) shows the above ideas.</p><formula xml:id="formula_2" coords="3,254.21,467.08,226.39,27.47">d k = tj D k tf kj len(d k ) × t j<label>(3)</label></formula><p>where D k is the set of terms that belongs to document d k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head><p>We have approached the AP problem as a single-labeled six class classification problem. This means, that we have six age-genre profiling classes: 10s-female, 10s-male, 20s-female, 20s-male, 30s-female, 30s-male. Given this context, we use for each experiment the following configuration: i) a stratified 10 cross fold validation using the training PAN13 corpus, ii) the most 50,000 frequent terms, and iii) a LibLINEAR classifier <ref type="bibr" coords="3,156.70,620.57,10.58,8.64" target="#b0">[1]</ref>. For terms we use words, contractions, words with hyphens, punctuation marks and a set of common emoticons. From Table <ref type="table" coords="3,313.31,632.53,4.98,8.64">1</ref> it can be seen how the Second Order Attributes (SOA) outperforms the BOT representation using the PAN 13 corpus, which is an imbalanced dataset (a realistic scenario). We believe this is because the second order attributes provides a different document perspective beyond the isolated word frequencies. In Table <ref type="table" coords="4,191.98,131.27,4.98,8.64">1</ref> we also show the detailed results for predicting Age and Genre in the test dataset, and a summary of the averaged results for all participants at PAN 2013.  <ref type="table" coords="4,159.19,261.09,3.36,8.06">1</ref>. Experiments using the Second Order Attributes (SOA) and BOT computed over the 50,000 most frequent terms on the datasets. We denote in bold our best outcomes for each dataset.</p><p>Results in Table <ref type="table" coords="4,216.91,314.87,4.98,8.64">1</ref> demonstrate the performance of our proposal, which overcomes the conventional BOT and holds the first position for both languages (averaged accuracy), and second position for each one. We think this is because SOA are less sensitive to the high dimensionality problem, the scarce data, and the imbalanced classes. Moreover, it is worth knowing that our approach took only 0.22% of the time required by the method in one position below for english corpus (based in accuracy), which means that our proposal was one of the most efficient and effective approaches at PAN 2013.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this paper we have explored a new document representation for AP task. To the best of our knowledge, this is the first time that AP is addressed using attributes that represent relationships with profiles. We found that, with very low computational cost our proposal can build discriminative low dimensional dense vectors for AP. Using these vectors, the classifier can keep good classification rates, even for imbalanced data. We think that this is due to the relations among terms and profiles, which provides few but high predictive attributes. We also presented experimental results that show better performance of the proposed approach against the standard BOT. We further believe that the proposed representation is a feasible and stable representation, quite practical in situations where it is necessary to represent and classify fast and effectively.</p></div>		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,138.13,594.78,340.97,7.77;4,146.47,605.73,291.10,7.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,361.72,594.78,117.37,7.77;4,146.47,605.73,68.45,7.77">LIBLINEAR: A library for large linear classification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><forename type="middle">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,220.43,605.73,139.44,7.77">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,138.13,616.07,340.79,7.77;4,146.47,627.02,237.34,7.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,298.13,616.07,180.78,7.77;4,146.47,627.02,22.41,7.77">Automatically categorizing written texts by author gender</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Argamon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Shimoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,174.86,627.02,125.28,7.77">Literary and Linguistic Computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="401" to="412" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,138.13,637.36,335.27,7.77;4,146.47,648.32,215.53,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,305.21,637.36,168.19,7.77;4,146.47,648.32,27.67,7.77">Fast text categorization using concise semantic analysis</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,179.84,648.32,98.49,7.77">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="441" to="448" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
