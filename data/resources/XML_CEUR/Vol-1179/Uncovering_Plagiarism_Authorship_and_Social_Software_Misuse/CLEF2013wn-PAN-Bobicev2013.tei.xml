<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,188.21,150.42,230.18,14.09;1,211.37,170.49,172.58,10.60">Authorship Detection with PPM Notebook for PAN at CLEF 2013</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,269.11,207.48,68.62,8.88"><forename type="first">Victoria</forename><surname>Bobicev</surname></persName>
							<email>victoria_bobicev@rol.md</email>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Moldova</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,188.21,150.42,230.18,14.09;1,211.37,170.49,172.58,10.60">Authorship Detection with PPM Notebook for PAN at CLEF 2013</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7DE60720C19841DBD66B06A46B4E2113</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper reports on our work in the PAN 2013 author identification task. The task is to automatically detect the author of the given text having small training sets with known authors. The task was solved by a system that used the PPM (Prediction by Partial Matching) compression algorithm based on an n-gram statistical model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the emergence of user-generated web content, text author profiling is being increasingly studied by the NLP community. Various works describe experiments aiming to automatically discover hidden attributes of text which reveal author's gender, age, personality and others. Authorship identification is an important problem in many areas including information retrieval and computational linguistics.</p><p>While a great number of works have presented investigations in this area there is need for a common ground to evaluate different author recognition techniques. PAN 2013 as part of the CLEF campaigns aims to provide the common conditions and data for this task.</p><p>We participated in this shared task with the PPM (Prediction by Partial Matching) compression algorithm based on a character-based n-gram statistical model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model description</head><p>Authorship identification can be viewed as a type of classification task. Such tasks are solved using learning methods. There are different types of text classification. Authorship attribution, spam filtering, dialect identification are just several of the purposes of text categorization. It is natural that for different types of categorization different methods are pertinent. The most common type is the content-based categorization which classifies texts by their topic and requires the most common classification methods based on classical set of features. More specific methods are necessary in cases when classification criterions are not so obvious, for example, in the case of author identification.</p><p>In this paper the application of the PPM (Prediction by Partial Matching) model for automatic text classification is explored. Prediction by partial matching (PPM) is an adaptive finite-context method for text compression that is a back-off smoothing technique for finite-order Markov models <ref type="bibr" coords="2,304.65,184.20,10.69,8.88" target="#b0">[1]</ref>. It obtains all information from the original data, without feature engineering, is easy to implement and relatively fast. PPM produces a language model and can be used in a probabilistic text classifier.</p><p>PPM is based on conditional probabilities of the upcoming symbol given several previous symbols <ref type="bibr" coords="2,199.13,230.16,10.69,8.88" target="#b1">[2]</ref>. The PPM technique uses character context models to build an overall probability distribution for predicting upcoming characters in the text. A blending strategy for combining context predictions is to assign a weight to each context model, and then calculate the weighted sum of the probabilities:</p><formula xml:id="formula_0" coords="2,265.49,275.64,202.08,25.69">m P(x) = Σ λ i p i (x),<label>(1)</label></formula><p>i=1 where λ i and p i are weights and probabilities assigned to each order i (i=1…m).</p><p>For example, the probability of character 'm' in context of the word 'algorithm' is calculated as a sum of conditional probabilities dependent on different context lengths up to the limited maximal length:</p><formula xml:id="formula_1" coords="2,136.22,365.74,331.95,28.76">P PPM ('m') = λ 5 ⋅ P( 'm' | 'orith') + λ 4 ⋅ P( 'm' | 'rith') + λ 3 ⋅ P( 'm' | 'ith') + + λ 2 ⋅ P( 'm' | 'th') + λ 1 ⋅ P( 'm' | 'h') + + λ 0 ⋅ P( 'm' ) + λ -1 ⋅ P( 'esc' ),<label>(2)</label></formula><p>where</p><formula xml:id="formula_2" coords="2,153.86,406.07,43.31,13.07">λ i (i = 1…5</formula><p>) is the normalization weight; 5 is the maximal length of the context; P( 'esc' ) -'escape' probability, the probability of an unknown character.</p><p>PPM is a special case of the general blending strategy. The PPM models use an escape mechanism to combine the predictions of all character contexts of length m, where m is the maximum model order; the order 0 model predicts symbols based on their unconditioned probabilities, the default order -1 model ensures that a finite probability (however small) is assigned to all possible symbols. The PPM escape mechanism is more practical to implement than weighted blending. There are several versions of the PPM algorithm depending on the way the escape probability is estimated. In our implementation, we used the escape method C <ref type="bibr" coords="2,393.70,524.33,10.69,8.88" target="#b2">[3]</ref>, named PPMC.</p><p>Treating a text as a string of characters, a character-based PPM avoids defining word boundaries; it deals with different types of documents in a uniform way. It can work with texts in any language and be applied to diverse types of classification; more details can be found in <ref type="bibr" coords="2,228.18,570.41,10.69,8.88" target="#b3">[4]</ref>. Our utility function for text classification was crossentropy of the test document: Usually, the cross-entropy is greater than the entropy, because the probabilities of symbols in diverse texts are different. The cross-entropy can be used as a measure for document similarity; the lower cross-entropy for two texts is, the more similar they are. Hence, if several statistical models had been created using documents that belong to different classes and cross-entropies are calculated for an unknown text on the basis of each model, the lowest value of cross-entropy will indicate the class of the unknown text. In this way cross-entropy is used for text classification.</p><formula xml:id="formula_3" coords="2,238.13,592.85,229.32,25.69">n H d m -= Σ p m (x i ) log p m (x i ),<label>(3)</label></formula><p>On the training step, we created PPM models for each class of documents; on the testing step, we evaluated cross-entropy of previously unseen texts using models for each class. Thus, cross-entropy was used as similarity metrics, the lowest value of cross-entropy indicated the class of the unknown text.</p><p>The maximal length of a context equal to 5 in PPM model was proven to be optimal for text compression <ref type="bibr" coords="3,248.23,287.67,10.69,8.88" target="#b5">[6]</ref>. In all our experiments with character-based PPM model we used maximal length of a context equal to 5; thus our method is PPMC5.</p><p>The character-based PPM models were used for spam detection, source-based text classification and classification of multi-modal data streams that included texts. In <ref type="bibr" coords="3,456.48,322.23,10.67,8.88" target="#b0">[1]</ref>, the character-based PPM models were used for spam detection. In <ref type="bibr" coords="3,414.71,333.75,10.69,8.88" target="#b4">[5]</ref>, the PPM algorithm was applied to text categorization in two ways: on the basis of characters and on the basis of words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method description</head><p>In the previous author identification experiments the standard PPM classification methodology was applied to the large set of forum posts written by tens of forum participants. More exactly, we experimented with 30 authors, 100 posts for each authors; approximately length of posts was 150-200 words. The unknown unit was one post. The task was to classify test posts having these 30 classesauthors. The classification accuracy was surprisingly high: F-measure was around 0.8. Further experiments showed that the test text length is the most influencing factor and the maximal accuracy we reached with test texts of 300 words length. The F-measure was 0.97 and did not grow for the longer test texts.</p><p>The current task had several differences and we could not use the standard classification methodology directly. First, the task was to make a decision about only one test text whether it belonged to the same author as the several known texts which were written by one author. Thus, we actually had only one class and could not compare entropies of the test text for several classes in order to select one. Second, in this task training and test data were extremely small; in some cases test text was larger than the whole training set. It should be mentioned that the volume of test and especially training data affected the classification results for our method; it tended to attribute texts to the class with the larger training set. We applied a special normalization procedure to normalize entropies of larger and smaller training texts for better classification in the experiments with forum posts.</p><p>For the data in this experiment we needed a number of known and unknown texts; thus we divided all given known and unknown texts in small fragments and compared their entropies calculated on the basis of known texts models and on the basis of unknown text models. If the entropies were considerably different we considered that texts were written by two different authors. The following algorithm presents our methodology in more details.</p><p>The o calculate entropy of this fragment on the basis of the created model. -compare these four lists of the entropies as the statistical variables using t-test for the null hypothesis that these variables are equivalent. If the null hypothesis was accepted we considered that the unknown text was written by the same author as known texts, otherwise we decided that the known and unknown texts were written by two different authors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,276.43,619.37,9.36,6.09;2,136.22,627.92,24.32,8.88;2,158.90,639.44,153.14,8.88;2,146.30,650.99,10.44,9.53;2,158.42,648.73,4.68,5.67;2,165.65,647.56,177.88,13.07;2,148.82,662.51,4.98,8.72;2,153.86,660.25,4.68,5.67;2,158.54,662.36,7.81,8.88;2,166.37,666.37,1.80,5.67;2,168.17,662.36,124.10,9.68;2,294.79,662.36,50.01,8.88;2,146.30,674.03,10.44,9.53;2,158.42,671.77,4.68,5.67;2,165.65,673.88,266.69,8.88"><head></head><label></label><figDesc>number of symbols in a text d, H d mentropy of the text d obtained by model m, p m (x i ) is a probability of a symbol x i in the text d. H d m was estimated by the modelling part of the compression algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,138.62,184.20,331.17,169.83"><head></head><label></label><figDesc>algorithm: -join all known texts into one; -divide this known text in fragments with the similar length (about 350 words); -divide unknown text in fragments with the similar length (about 350 words); -for each known fragment in turn: o create PPM statistical model on the rest of known texts; o calculate entropy of this fragment on the basis of the created model. -for each unknown fragment in turn: o create PPM statistical model on the rest of unknown fragments; o calculate entropy of this fragment on the basis of the created model. -create PPM statistical model on all unknown texts; -for each known fragment in turn: o calculate entropy of this fragment on the basis of the created model. -create PPM statistical model on all known texts; -for each unknown fragment in turn:</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,128.58,479.45,342.06,8.88;4,138.98,490.97,331.70,8.88;4,138.98,502.49,26.75,8.88" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="4,232.49,479.45,172.60,8.88">Spam Filtering Using Compression Models</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bratko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Filipic</surname></persName>
		</author>
		<idno>IJS-DP-9227</idno>
		<imprint>
			<date type="published" when="2005">2005</date>
			<pubPlace>Ljubljana, Slovenia</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Intelligent Systems ; Jozef Stefan Institute</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="4,128.58,514.01,341.96,8.88;4,138.98,522.25,331.82,13.07;4,138.98,537.05,26.75,8.88" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,225.89,514.01,244.65,8.88;4,138.98,525.53,36.16,8.88">Data Compression Using Adaptive Coding and Partial String Matching</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cleary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,183.17,525.53,159.44,8.88">IEEE Transactions on Communications</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="396" to="402" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,128.58,548.45,342.07,8.88;4,138.98,559.97,141.43,8.88" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,295.29,548.45,122.86,8.88">Modeling for text compression</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">C</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">G</forename><surname>Cleary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,425.74,548.45,44.92,8.88;4,138.98,559.97,30.20,8.88">Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="557" to="591" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,128.58,571.49,342.01,8.88;4,138.98,583.04,323.65,8.88" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,196.73,571.49,270.04,8.88">Comparison of Word-based and Letter-based Text Classification</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Bobicev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,138.98,583.04,207.48,8.88">Recent Advances in Natural Language Processing V</title>
		<meeting><address><addrLine>Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="76" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,128.58,594.56,341.89,8.88;4,138.98,606.08,331.33,8.88;4,138.98,617.48,188.97,8.88" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,265.49,594.56,204.98,8.88;4,138.98,606.08,51.53,8.88">An effective and robust method for short text classification</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Bobicev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sokolova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,202.50,606.08,267.81,8.88;4,138.98,617.48,52.44,8.88">Proceedings of the 23rd national conference on Artificial intelligence</title>
		<meeting>the 23rd national conference on Artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1444" to="1445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,128.58,629.00,342.00,8.88;4,138.98,640.52,61.43,8.88" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="4,194.34,629.00,94.65,8.88">Modelling English text</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Teahan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<pubPlace>New Zealand</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Waikato</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
