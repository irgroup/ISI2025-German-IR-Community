<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,137.61,115.90,340.15,12.90;1,258.67,133.83,98.01,12.90;1,223.43,152.26,168.50,10.75">Ensemble-based classification for author profiling using various features Notebook for PAN at CLEF 2013</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,143.24,187.24,56.17,8.64"><forename type="first">Michał</forename><surname>Meina</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Mathematics and Computer Science</orgName>
								<orgName type="institution">Nicolaus Copernicus University</orgName>
								<address>
									<settlement>Toruń</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,208.86,187.24,82.73,8.64"><forename type="first">Karolina</forename><surname>Brodzińska</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Physics, Astronomy and Informatics</orgName>
								<orgName type="institution">Nicolaus Copernicus University</orgName>
								<address>
									<settlement>Toruń</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,301.04,187.24,62.26,8.64"><forename type="first">Bartosz</forename><surname>Celmer</surname></persName>
							<email>bcelmer@mat.umk.pl</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Mathematics and Computer Science</orgName>
								<orgName type="institution">Nicolaus Copernicus University</orgName>
								<address>
									<settlement>Toruń</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,372.74,187.24,56.17,8.64"><forename type="first">Maja</forename><surname>Czoków</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Mathematics and Computer Science</orgName>
								<orgName type="institution">Nicolaus Copernicus University</orgName>
								<address>
									<settlement>Toruń</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,438.36,187.24,33.75,8.64;1,218.94,199.20,24.75,8.64"><forename type="first">Martyna</forename><surname>Patera</surname></persName>
							<email>patera@mat.umk.pl</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Mathematics and Computer Science</orgName>
								<orgName type="institution">Nicolaus Copernicus University</orgName>
								<address>
									<settlement>Toruń</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,253.14,199.20,56.72,8.64"><forename type="first">Jakub</forename><surname>Pezacki</surname></persName>
							<email>kpezacki@mat.umk.pl</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Mathematics and Computer Science</orgName>
								<orgName type="institution">Nicolaus Copernicus University</orgName>
								<address>
									<settlement>Toruń</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,336.18,199.20,55.77,8.64"><forename type="first">Mateusz</forename><surname>Wilk</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Mathematics and Computer Science</orgName>
								<orgName type="institution">Nicolaus Copernicus University</orgName>
								<address>
									<settlement>Toruń</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,137.61,115.90,340.15,12.90;1,258.67,133.83,98.01,12.90;1,223.43,152.26,168.50,10.75">Ensemble-based classification for author profiling using various features Notebook for PAN at CLEF 2013</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BBDFAD08F60ED791C51A43E53EFA2504</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper summarize our approach to author profiling task -a part of evaluation lab PAN'13. We have used ensemble-based classification on large features set. All the features are roughly described and experimental section provides evaluation of different methods and classification approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Main goal of authorship analysis is to retrieve information carried by the text about specified characteristics of its author. Characteristics may relate to demography, culture, nationality, personality, etc. Such analysis, for example, may be applied to discover the relation between profile of a person and his/her opinion on particular subject. It may also help in recognition of criminal and terrorist activities. In this paper we describe our approach to Author Profiling task, which was a part of the PAN 2013 competition <ref type="foot" coords="1,456.39,466.83,3.49,6.05" target="#foot_0">3</ref> . The goal was to determine gender and age of given chat conversations' authors.</p><p>We decided to apply ensemble-based classification methods because of their potential for more effective recognition of complex patterns. Ensemble methods combines several weak classifiers into one classifier, which is more effective than the individual ones. The features were created on the base of structure, stylometry and semantics of the text. The diversity of explored properties of the dataset assures high independence of the features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data</head><p>The dataset was divided into two language groups (English and Spanish). Each group included conversations stored in XML files (grouped by author). There were 236 000 files (564 413 conversations, 180 809 187 words) in English and 75 900 files (126 453 conversations, 21 824 198 words) in Spanish.</p><p>In the data preprocessing phase we employed standard techniques for text cleaning and tokenization. Regular expressions was used to strip out most of html tags leaving special tokens for hyperlinks and embedded images. Some of feature extraction procedures required word and sentence tokenization. Word tokenization is easy task and can be addressed by regular expressions. For sentence splitting we have used unsupervised algorithm <ref type="bibr" coords="2,175.73,179.09,16.60,8.64" target="#b7">[11]</ref> to build a model for abbreviation words, collocations, and words that are at the beginning of sentences.</p><p>We noticed that many conversations in dataset seemed to be a spam (probably produced by some chatterbots used for commercial purposes). It is reasonable to assume that age and gender characteristics in spam-like text have different rationale. One can say, for example, that advertisement of certain products is more oriented towards one gender group than the other one. Also mixing together chatter bots and humans conversations can introduce unnecessary noise in phase of learning of a classifier. Because of that we put extra effort to discriminate spam-like over human-like chats.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Feature Engineering</head><p>With each document we associated a collection of features, which were employed by classification algorithms to identify the age and gender of the document's author. In the final versions of our analytic dataset (that was used for classifier training) there are 311 and 476 features, respectively for the texts written in English and Spanish.</p><p>These features can be divided into groups roughly described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Structural Features</head><p>Structure of a document is proven to be important feature in various classification problems regarding text mining and authorship profiling. There were not much previous work, however, that can be easily adopted in our approach. Therefore we have engineered very simple features, that not necessarily carry information about authors profile but can be used to group the documents into similarly structured conversations (eg. long and shorts ones). This approach enables us to discover more subtle high-level features in various document groups. Some examples of such structural features are: the number of conversations, paragraphs, sentences and words per sentences, number of special characters, etc. If the person conducted more than one conversation we measure minimum, maximum and average conversation length. The usage (absolute count and ratio) of hyperlinks and images, and whether they were used at the beginning or at the end of the conversation. After more thorough investigation we discovered that this particular feature can be also useful in spam detection. We also noticed that chatterbots seem to perform very similar conversations therefore we measured the Jaccard similarity coefficient of individual conversations and enclosed this average edit distance into an analytic dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Parts of Speech</head><p>Argamon et.al <ref type="bibr" coords="2,194.94,644.48,11.62,8.64" target="#b1">[5]</ref> reports that usage of particular part of speech in some cases can be exploited effectively in gender detection of texts' authors. Therefore we have measured relative frequencies of particular parts of speech in whole conversation of each author. In order to measure these proportions we have used part-of-speech tagger (pos-tagger) available in nltk toolkit. For English texts we used pre-trained tagger (more details in <ref type="bibr" coords="3,145.74,155.18,11.20,8.64" target="#b2">[6]</ref>) and for Spanish texts we trained trigram tagger (with respectively bigram and unigram tagger as backoffs) over annotated corpora <ref type="bibr" coords="3,342.44,167.13,15.27,8.64" target="#b8">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Exploration of Sequences of Parts of Speech</head><p>We employ n-gram language model (separate for age and gender problem) to create a vector of features. An n-gram in our case is a contiguous sequence of n pos-tagged words from a given sequence. In the learning phase for each class C we selected all files ascribe to it and we count the number of occurrences of each n-gram and (n-1)gram in chosen files. Next, on the basis of determined values, for the given class C, the probabilities P C (x i |x i-n+1 , ..., x i-1 ) are estimated, for more details see <ref type="bibr" coords="3,427.10,269.05,10.58,8.64" target="#b3">[7]</ref>.</p><p>Obtained probabilities are employed to generate vector of features for a given conversation. To this end for each sentence x -n+2 , ..x l+1 in the conversation and for each class C is calculated a vector of the posterior probabilities P(C|x -n+2 , ..x l+1 ). In order to calculate this value, the probability of occurrence of the sentence in the files ascribe to the class C is calculated:</p><formula xml:id="formula_0" coords="3,204.52,343.01,276.07,30.32">P C (x -n+2 , ..x l+1 ) = l+1 i=1 P C (x i |x i-n+1 , ..., x i-1 ).<label>(1)</label></formula><p>In the equation 1 we make a (n -1) th order Markov assumption. Next, the parameter</p><formula xml:id="formula_1" coords="3,134.77,394.52,72.40,9.65">P C (x -n+2 , ..x l+1</formula><p>) is multiplied by the prior class probability P(C). Values obtained in this way for all classes and normalized to sum up to 1, create a vector of the posterior probabilities P(C|x -n+2 , ..x l+1 ). We sum these vectors from all sentences in the conversation. The obtained vector is again normalized to sum up to 1 and is returned as the final feature vector.</p><p>For both languages we employed 3 models for n ∈ {4, 5, 6}. So, for age we have 2 • 3 = 6 features and for gender 3 • 3 = 9. A n-gram language model has a very large number of parameters. For both languages, we applied part-of-speech taggers, which maps words to the set of 46 different symbols, what results in 46 n different possible n-grams. Even with a huge set of training sentences, many of the n-grams do not occur in the training set. It is serious problem, since some of these sequences appear in conversations, which we want to classify. We apply discounting method <ref type="bibr" coords="3,134.77,536.89,11.62,8.64" target="#b3">[7]</ref> to smooth date for 4-gram model. This method slightly decreases values of nonzero probabilities P C (x i |x i-3 , ..., x i-1 ) and ascribes positive, close to 0 values to all P C (x i |x i-3 , ..., x i-1 ), for which x i-3 , ..., x i-1 , x i does not occur in the training set. This procedure is not employed for models with n = {5, 6} since keeping their all parameters in memory is too expensive. For them we estimate parameters on the basis of parameters of models with lower n. If for n-gram model P C (x i |x i-n+1 , ..., x i-1 ) is missing, we estimate it by δ • P C (x i |x i-n+2 , ..., x i-1 ), where P C (x i |x i-n+2 , ..., x i-1 ) is taken from (n-1)-gram model. If P C (x i |x i-n+2 , ..., x i-1 ) is also missing, we apply δ • λ • P C (x i |x i-n+3 , ..., x i-1 ). For English conversations δ = 0.2, λ = 0.5 for gender and δ = 0.9, λ = 0.9 for age. For Spanish conversations δ = 0.2, λ = 0.5 for gender and δ = 0.7, λ = 0.9 for age. These values were chosen experimentally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Text difficulty &amp; readability</head><p>In order to determine text difficulties we applied several readability tests for the documents: Flesch Reading Ease, Flesch-Kincaid Grade Level and Dale-Chall readability formula. They are based on the number of words, sentences, syllables and difficult words (there is Dale-Chall list of 3 000 familiar words [4] and thus, words, which are not on that list, are considered as difficult). For details see <ref type="bibr" coords="4,366.77,185.64,11.62,8.64" target="#b5">[9]</ref> and <ref type="bibr" coords="4,397.76,185.64,10.58,8.64" target="#b4">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Dictionary-based Features</head><p>We wanted to examine the intensity of words and expressions of particular types. In each document we counted number of abbreviations, emoticons and badwords. We useed NodeBox [1] to count the number of basic emotion words (anger, disgust, fear, joy, sadness, surprise), connective words (nevertheless, whatever, secondly, etc. and words like I, the, own, him which have little semantical value) and persuasive words (you, money, save, new, results, health, easy, etc.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Errors</head><p>Numbers of errors and language mistakes is determined by using LanguageTool [2] in accordance with the list of 27 standardized ISO 27 error' types that can be found in <ref type="bibr" coords="4,466.48,351.33,10.58,8.64" target="#b0">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Topic Specific</head><p>By topic specific features we understand coefficients corresponding to the representation of the document as a linear combination of 150 (for each language) "statistical topics" estimated using Latent Semantic Analysis (LSA) technique [?]. In short, crucial point of LSA is k-rank approximation of singular value decomposition of term-doc matrix:</p><formula xml:id="formula_2" coords="4,276.73,457.77,203.86,9.65">M ≈ U k Σ k V k ,<label>(2)</label></formula><p>where M is tf-idf weighted term-doc matrix, U k and V k can be interpreted as term-topic matrix and topic-document matrix both in low rank approximation. We have computed this decomposition for English and Spanish corpus separately using rank k = 150 (chosen experimentally). Next, previously unseen document can be represented in latent (topic specific) space by "fold-in" operation:</p><formula xml:id="formula_3" coords="4,276.47,543.22,204.12,13.38">d = Σ -1 k U T k d,<label>(3)</label></formula><p>where d is vector (bag-of-words) representation of a document and d = w i is k-length vector in which |w i | indicates how this document contribute to i-th topic. We enclosed those values in analytic dataset avoiding classifier overfitting using 10-folds to obtain those topic specific features. Each 10% of documents was treated as unseen and folded into latent representation estimated with 90% rest of corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">Structural and Topic Specific Centroids</head><p>We employed a cluster analysis on two subsets of features: structural and topic specific (defined in previous section). This is used to differentiate behavioral profiles of authors. The basic behavior profile can be perceived as a preliminary authorship analysis; most significantly we can distinguish human from chatter bots (similarly to Gianvecchio et. al. <ref type="bibr" coords="5,146.89,182.97,16.60,8.64" target="#b6">[10]</ref> but using different features). Centroid of a cluster either structural {C 1 , ..., C 4 } or topical S 1 , ..., S m (m = 30 for English and m = 17 for Spanish) is used as typical behavior or conversation topic therefore euclidean distance to each cluster is enclosed into analytic dataset. We obtained clusters using K-Means algorithm with Silhouette score as criterion for estimation the number of clusters. Below table depicts structural centroids. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.9">Natural Language Model</head><p>N-grams on words were used in order to preserve language model. We estimated top-n n-grams for each gender-age group, next we merged the results and computed Mutual Information (equation below) to measure how much information every ngrams carry about each group.</p><formula xml:id="formula_4" coords="5,169.35,514.31,311.25,26.53">I(C, T ) = c∈0,1 t∈0,1 P(C = c, T = t)log 2 P (C = c, T = t) P (C = c) P (T = t) ,<label>(4)</label></formula><p>where P(C = 0) represents the probability that randomly selected author is a member of particular age-gender group and P(C = 1) represents probability that it isn't. Similarly, P(T = 1) represents the probability that a randomly selected chatter contains a given n-gram, and P(T = 0) represents the probability that it doesn't.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>We tested how individual classifiers (Naive Bayes, Random Tree, SVM) as well as ensemble methods (Random Forest, Classifiers Committees) work with our features set. The random forest method gives the best results and those are presented in Table <ref type="table" coords="5,473.11,656.44,3.74,8.64" target="#tab_1">2</ref>. Experiment was conducted using k-cross validation with (k = 10). The final Random Forest classifier was trained on 12-core machine using about 30GB of RAM. Training took 45 minutes. Parameters of the classifier were estimated by trial-and-error: minimum samples per leaf = 5, size of feature set for each tree was equal to √ n_f eatures. The classification accuracy convergence for the number of tree in the forest larger than 1000, but due to memory constraints on test machine we lowered this parameter to 666, which surprisingly fits the memory almost exactly. Also because memory constraints we did not used a n-gram model from subsection 3.9.</p><p>Among individual classifiers we tested also popular classifiers such as kNN (k = 5), linear SVM, SVM with RBF and Naive Bayes. We also conducted experiments for two simple ensembles composed of those classifiers: majority and weighted committee. Due to the time constraint we did not performed k-fold cross-validation. All classifiers were trained on 99% of feature vectors and tested on the remaining 1%. The date set was divided into training and test set by means of stratified cross-validation. The experiments were conducted on 48-core machine with 200 GB of RAM available. The obtained results are presented in Table <ref type="table" coords="6,290.01,380.53,3.74,8.64" target="#tab_2">3</ref>. We also performed similar tests on the subsets of features. The description of each of 9 subset can be found in Sect. 3 -"Feature engineering". We tested all of the four classifiers: kNN, Linear SVM, SVM with RBF and Naive Bayes. Further, for each subset of features we chose the best classifier and built committee from all 9 thus obtained classifiers. The results that we achieved are presented in Table <ref type="table" coords="6,384.51,618.47,3.74,8.64" target="#tab_3">4</ref>.</p><p>The subsets of features: (I) Structural features, (II) Parts of speech, (III) Exploration of sequences of parts of speech, (IV)) Test difficulty, (V) Dictionary-based features, (VI) Errors, (VII) Topical features, (VIII) Topical centroids, (IX) Structural centroids. Due to the long time of execution the tests for English language were not performed. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,137.92,271.31,333.95,122.43"><head>Table 1 .</head><label>1</label><figDesc>Classification accuracy</figDesc><table coords="5,137.92,295.02,333.95,98.72"><row><cell cols="8">centroid href_count sentence_count word_count href_word_ratio avg_conv_len new_line_count tab_count</cell></row><row><cell cols="2">English corpora</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>C1</cell><cell>0.820</cell><cell>6.372</cell><cell>119.764</cell><cell>0.027</cell><cell>395.533</cell><cell>12.103</cell><cell>7.460</cell></row><row><cell>C2</cell><cell>3.354</cell><cell>99.882</cell><cell>2419.265</cell><cell>0.000</cell><cell>11429.932</cell><cell>91.313</cell><cell>7.083</cell></row><row><cell>C3</cell><cell>23.879</cell><cell>45.204</cell><cell>921.405</cell><cell>0.009</cell><cell>1306.874</cell><cell>93.641</cell><cell>47.736</cell></row><row><cell>C4</cell><cell>3.712</cell><cell>43.678</cell><cell>962.547</cell><cell>0.000</cell><cell>3315.166</cell><cell>29.639</cell><cell>8.439</cell></row><row><cell cols="2">Spanish corpora</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>C1</cell><cell>0.146</cell><cell>3.839</cell><cell>98.389</cell><cell>0.002</cell><cell>385.496</cell><cell>6.427</cell><cell>7.766</cell></row><row><cell>C2</cell><cell>3.745</cell><cell>1.203</cell><cell>4.152</cell><cell>0.992</cell><cell>27.819</cell><cell>6.0677</cell><cell>5.186</cell></row><row><cell>C3</cell><cell>0.850</cell><cell>46.452</cell><cell>1183.494</cell><cell>0.000</cell><cell>2542.832</cell><cell>19.344</cell><cell>78.775</cell></row><row><cell>C4</cell><cell>1.317</cell><cell>250.837</cell><cell>5945.458</cell><cell>0.000</cell><cell>25741.812</cell><cell>19.375</cell><cell>197.689</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,194.81,115.83,225.73,75.16"><head>Table 2 .</head><label>2</label><figDesc>Classification accuracy</figDesc><table coords="6,194.81,140.33,225.73,50.65"><row><cell>gender</cell><cell>age</cell><cell>gender + age</cell></row><row><cell cols="3">English 0.632 ± 0.0019 0.611 ± 0.0019 0.653 ± 0.0019</cell></row><row><cell cols="3">Spanish 0.611 ± 0.0071 0.596 ± 0.0089 0.626 ± 0.0091</cell></row><row><cell>baseline 0.1650</cell><cell>0.5</cell><cell>0.3333</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,211.50,411.84,192.35,118.99"><head>Table 3 .</head><label>3</label><figDesc>Classification accuracy. Spanish language.</figDesc><table coords="6,211.50,436.34,192.35,94.49"><row><cell></cell><cell>gender age</cell><cell>gender + age</cell></row><row><cell>kNN</cell><cell cols="2">0.534 0.535 0.263</cell></row><row><cell>Naive Bayes</cell><cell cols="2">0.553 0.520 0.016</cell></row><row><cell>Linear SVM</cell><cell cols="2">0.6123 0.595 0.357</cell></row><row><cell>SVM with RBF</cell><cell cols="2">0.529 0.573 0.279</cell></row><row><cell cols="3">Majority committee 0.584 0.552 0.264</cell></row><row><cell cols="3">Weighted committee 0.573 0.552 0.242</cell></row><row><cell>baseline</cell><cell>0.1650 0.5</cell><cell>0.3333</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,180.79,115.83,253.77,135.40"><head>Table 4 .</head><label>4</label><figDesc>Classification accuracy. Subset of features. Spanish language.</figDesc><table coords="7,180.79,139.54,253.77,111.69"><row><cell></cell><cell>gender</cell><cell>age</cell><cell>gender + age</cell></row><row><cell>I.</cell><cell cols="3">0.541 (Naive Bayes) 0.564 (Linear SVM) 0.306 (Naive Bayes)</cell></row><row><cell>II.</cell><cell cols="3">0.556 (Linear SVM) 0.562 (Linear SVM) 0.305 (Linear SVM)</cell></row><row><cell>III.</cell><cell cols="3">0.581 (Linear SVM) 0.597 (Linear SVM) 0.346 (Liear SVM)</cell></row><row><cell>IV.</cell><cell cols="3">0.514 (Linear SVM) 0.561 (Linear SVM) 0.289 (Linear SVM)</cell></row><row><cell>V.</cell><cell cols="3">0.536 (Linear SVM) 0.561 (Linear SVM) 0.3014 (Linear SVM)</cell></row><row><cell>VI.</cell><cell>0.568 (kNN)</cell><cell cols="2">0.582 (Naive Bayes) 0.315 (Naive Bayes)</cell></row><row><cell>VII.</cell><cell cols="3">0.529 (Naive Bayes) 0.562 (Linear SVM) 0.363 (kNN)</cell></row><row><cell>VIII.</cell><cell cols="3">0.541 (Naive Bayes) 0.565 (Linear SVM) 0.284 (SVM)</cell></row><row><cell>IX.</cell><cell cols="3">0.541 (Linear SVM) 0.561 (Linear SVM) 0.280 (Linear SVM)</cell></row><row><cell cols="2">Majority committee 0.604</cell><cell>0.561</cell><cell>0.308</cell></row><row><cell cols="2">Weighted committee 0.664</cell><cell>0.405</cell><cell>0.069</cell></row><row><cell>baseline</cell><cell>0.1650</cell><cell>0.5</cell><cell>0.3333</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,136.16,115.83,343.68,597.81"><head>Table 5 .</head><label>5</label><figDesc>The table lists 50 features with the highest Information Gain ratio</figDesc><table coords="8,136.16,146.16,343.68,567.47"><row><cell>English</cell><cell></cell><cell>Spanish</cell><cell></cell></row><row><cell>Feature</cell><cell>Inf. gain</cell><cell>Feature</cell><cell>Inf. gain</cell></row><row><cell>1 min_conv_len</cell><cell>0.0653 gram_n4_30s</cell><cell></cell><cell>0.0416</cell></row><row><cell cols="2">2 total_connective_words/total_sents 0.0653 gram_n5_30s</cell><cell></cell><cell>0.0363</cell></row><row><cell>3 avg_conv_len_words</cell><cell>0.0647 gram_n4_20s</cell><cell></cell><cell>0.0337</cell></row><row><cell>4 avg_conv_len</cell><cell>0.0644 gram_n5_20s</cell><cell></cell><cell>0.0246</cell></row><row><cell>5 total_abbreviations/total_sents</cell><cell>0.0642 gram_n4_male</cell><cell></cell><cell>0.0228</cell></row><row><cell>6 C1</cell><cell cols="2">0.0635 gram_n4_female</cell><cell>0.0228</cell></row><row><cell>7 gram_n6_20s</cell><cell cols="3">0.0631 total_uncategorized_errors/total_sents 0.0209</cell></row><row><cell>8 max_conv_len</cell><cell>0.0625 gram_n4_age</cell><cell></cell><cell>0.0207</cell></row><row><cell>9 C0</cell><cell>0.0624 gram_n5_age</cell><cell></cell><cell>0.0201</cell></row><row><cell>10 gram_n5_20s</cell><cell cols="2">0.0622 total_errors/total_sents</cell><cell>0.0197</cell></row><row><cell>11 gram_n6_age</cell><cell cols="3">0.0612 total_typographical_errors/total_sents 0.0177</cell></row><row><cell>12 total_badwords/total_sents</cell><cell cols="2">0.0604 new_line_count/sentence_count</cell><cell>0.0172</cell></row><row><cell>13 C3</cell><cell cols="2">0.0559 gram_n4_gender</cell><cell>0.0169</cell></row><row><cell>14 gram_n4_20s</cell><cell cols="2">0.0539 gram_n5_female</cell><cell>0.0163</cell></row><row><cell>15 gram_n6_30s</cell><cell>0.0524 gram_n5_male</cell><cell></cell><cell>0.0163</cell></row><row><cell>16 gram_n5_30s</cell><cell>0.0523 gram_n4_10s</cell><cell></cell><cell>0.0134</cell></row><row><cell>17 gram_n5_age</cell><cell cols="2">0.0518 gram_n5_gender</cell><cell>0.0127</cell></row><row><cell>18 total_abbreviations</cell><cell>0.0514 Fc_n</cell><cell></cell><cell>0.0107</cell></row><row><cell>19 word_count</cell><cell>0.0508 sps00_n</cell><cell></cell><cell>0.0107</cell></row><row><cell>20 gram_n4_30s</cell><cell>0.0503 gram_n5_10s</cell><cell></cell><cell>0.0100</cell></row><row><cell>21 total_badwords</cell><cell>0.0478 href_count</cell><cell></cell><cell>0.0095</cell></row><row><cell cols="3">22 total_persuasive_words/total_sents 0.0458 sentence_count</cell><cell>0.0090</cell></row><row><cell>23 sentence_count</cell><cell cols="3">0.0430 total_connective_words/total_words 0.0087</cell></row><row><cell>24 new_line_count/word_count</cell><cell>0.0404 Fp_n</cell><cell></cell><cell>0.0086</cell></row><row><cell>25 href_count</cell><cell>0.0397 UNK_n</cell><cell></cell><cell>0.0077</cell></row><row><cell>26 new_line_count/sentence_count</cell><cell cols="2">0.0385 href_word_ratio</cell><cell>0.0073</cell></row><row><cell>27 gram_n4_age</cell><cell cols="2">0.0380 new_line_count</cell><cell>0.0071</cell></row><row><cell>28 gram_n6_female</cell><cell>0.0369 word_count</cell><cell></cell><cell>0.0067</cell></row><row><cell>29 gram_n6_male</cell><cell>0.0369 rn_n</cell><cell></cell><cell>0.0066</cell></row><row><cell>30 gram_n4_male</cell><cell>0.0345 Fat_n</cell><cell></cell><cell>0.0061</cell></row><row><cell>31 gram_n4_female</cell><cell>0.0345 C2</cell><cell></cell><cell>0.0061</cell></row><row><cell>32 gram_n5_female</cell><cell>0.0344 Fs_n</cell><cell></cell><cell>0.0060</cell></row><row><cell>33 gram_n5_male</cell><cell cols="2">0.0344 avg_conv_len_words</cell><cell>0.0059</cell></row><row><cell>34 C2</cell><cell cols="2">0.0308 total_difficult_words/total_words</cell><cell>0.0057</cell></row><row><cell cols="2">35 total_difficult_words/total_words 0.0284 max_conv_len</cell><cell></cell><cell>0.0056</cell></row><row><cell>36 total_syllables/total_words</cell><cell>0.0283 C1</cell><cell></cell><cell>0.0055</cell></row><row><cell>37 gram_n4_10s</cell><cell cols="2">0.0268 new_line_count/word_count</cell><cell>0.0055</cell></row><row><cell>38 gram_n5_10s</cell><cell cols="2">0.0265 total_abbreviations</cell><cell>0.0054</cell></row><row><cell>39 gram_n6_gender</cell><cell>0.0252 C3</cell><cell></cell><cell>0.0053</cell></row><row><cell>40 gram_n6_10s</cell><cell>0.0250 ncmp000_n</cell><cell></cell><cell>0.0053</cell></row><row><cell>41 flasch_reading_easy</cell><cell>0.0241 avg_conv_len</cell><cell></cell><cell>0.0053</cell></row><row><cell>42 gram_n5_gender</cell><cell>0.0230 vmip1s0_n</cell><cell></cell><cell>0.0053</cell></row><row><cell>43 gram_n4_gender</cell><cell>0.0227 topic-85</cell><cell></cell><cell>0.0052</cell></row><row><cell>44 dale_chall_readability_formula</cell><cell>0.0216 topic-55</cell><cell></cell><cell>0.0050</cell></row><row><cell>45 total_badwords/total_words</cell><cell>0.0210 topic-17</cell><cell></cell><cell>0.0050</cell></row><row><cell>46 flesch_kincaid_grade_level</cell><cell>0.0209 topic-116</cell><cell></cell><cell>0.0049</cell></row><row><cell>47 total_emoticons/total_words</cell><cell>0.0206 topic-8</cell><cell></cell><cell>0.0049</cell></row><row><cell>48 total_emoticons</cell><cell>0.0202 topic-147</cell><cell></cell><cell>0.0048</cell></row><row><cell>49 total_abbreviations/total_words</cell><cell>0.0187 topic-36</cell><cell></cell><cell>0.0048</cell></row><row><cell>50 total_emoticons/total_sents</cell><cell>0.0180 pp1cs000_n</cell><cell></cell><cell>0.0048</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="1,144.73,657.08,70.74,7.77"><p>http://pan.webis.de/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,142.61,336.92,322.16,7.77;7,150.95,349.83,75.81,4.91;7,139.25,358.84,341.34,7.77;7,150.95,371.75,214.39,4.91" xml:id="b0">
	<monogr>
		<ptr target="http://www.usingenglish.com/resources/wordcheck/list-dale-chall+list+of+simple+words.html" />
		<title level="m" coord="7,193.75,358.84,119.02,7.77">Dale-chall list of simple words</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,380.75,337.98,7.77;7,150.95,391.71,145.06,7.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,331.42,380.75,149.17,7.77;7,150.95,391.71,43.51,7.77">Gender, genre, and writing style in formal written texts</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Argamon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Shimoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,199.88,391.71,22.91,7.77">TEXT</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="321" to="346" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,402.67,337.98,7.77;7,150.95,413.63,321.02,7.77" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="7,263.43,402.67,217.16,7.77;7,150.95,413.63,122.55,7.77">Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Loper</surname></persName>
		</author>
		<ptr target="http://www.nltk.org/book" />
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>O&apos;Reilly</publisher>
			<pubPlace>Beijing</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,424.59,337.98,7.77;7,150.95,435.55,329.64,7.77;7,150.95,446.51,124.17,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,244.09,424.59,236.50,7.77;7,150.95,435.55,10.28,7.77">An empirical study of smoothing techniques for language modeling</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,179.54,435.55,301.06,7.77;7,150.95,446.51,11.76,7.77">Proceedings of the 34th annual meeting on Association for Computational Linguistics</title>
		<meeting>the 34th annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page">96</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,457.47,337.98,7.77;7,150.95,468.43,99.94,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,230.71,457.47,131.84,7.77">A formula for predicting readability</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Chall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,369.25,457.47,111.34,7.77">Educational Research Bulletin</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="20" />
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,479.38,337.98,7.77;7,150.95,490.34,42.58,7.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,192.94,479.38,99.56,7.77">A new readability yardstick</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Flesch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,298.81,479.38,113.02,7.77">Journal of Applied Psychology</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="221" to="233" />
			<date type="published" when="1948-06">June 1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.24,501.30,338.35,7.77;7,150.95,512.26,329.64,7.77;7,150.95,523.22,39.09,7.77" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,319.22,501.30,161.37,7.77;7,150.95,512.26,161.42,7.77">Humans and bots in internet chat: measurement, analysis, and automated classification</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gianvecchio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,319.15,512.26,92.42,7.77">IEEE/ACM Trans. Netw</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1557" to="1571" />
			<date type="published" when="2011-10">Oct 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.24,534.18,338.35,7.77;7,150.95,545.14,120.03,7.77" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,224.09,534.18,200.77,7.77">Unsupervised multilingual sentence boundary detection</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Strunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,431.19,534.18,49.40,7.77;7,150.95,545.14,19.68,7.77">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="485" to="525" />
			<date type="published" when="2006-12">Dec 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.24,556.10,338.35,7.77;7,150.95,567.06,263.69,7.77" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="7,336.17,556.10,144.42,7.77;7,150.95,567.06,61.01,7.77">Cess-ece: A multilingual and multilevel annotated corpus</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Martí</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Taulé</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Márquez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bertran</surname></persName>
		</author>
		<ptr target="http://www.lsi.upc.edu/~mbertran/cess-ece" />
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
