<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,153.38,115.90,308.60,12.90;1,153.00,133.83,309.36,12.90;1,223.43,153.68,168.50,10.75">Experiments with SMS Translation and Stochastic Gradient Descent in Spanish Text Author Profiling Notebook for PAN at CLEF 2013</title>
				<funder ref="#_6wXQvQA">
					<orgName type="full">Ministry of Economy and Competitiveness and the Center for Industrial Technological Development (CDTI)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,179.81,190.08,63.36,8.64"><forename type="first">Andrés</forename><surname>Alfonso</surname></persName>
						</author>
						<author role="corresp">
							<persName coords="1,245.67,190.08,52.28,8.64"><forename type="first">Caurcel</forename><surname>Díaz</surname></persName>
							<email>aarcaurcel@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Politécnica de Madrid</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,321.78,190.08,43.44,8.64"><forename type="first">José</forename><surname>María</surname></persName>
						</author>
						<author>
							<persName coords="1,367.71,190.08,63.36,8.64"><forename type="first">Gómez</forename><surname>Hidalgo</surname></persName>
							<affiliation key="aff1">
								<address>
									<addrLine>Optenet</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,153.38,115.90,308.60,12.90;1,153.00,133.83,309.36,12.90;1,223.43,153.68,168.50,10.75">Experiments with SMS Translation and Stochastic Gradient Descent in Spanish Text Author Profiling Notebook for PAN at CLEF 2013</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">ED933BA0023D3F0A0E10598D351FB0C8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Inspired by our ongoing work in the project WENDY, which addresses age detection in social networks by linguistic processing (among other methods), we have built a system that makes use of a number of linguistic resources (a Spanish dictionary, and a SMS-language dictionary) and algorithms (custom text utterances tokenization, SMS to standard Spanish translation, and a number of normalization rules) in order to apply a learning-based approach using a custom Stochastic Gradient Descent algorithm adapted to text, to the Spanish Author Profiling task at PAN'2013. We believe the results obtained in internal testing on a validation set extracted from training dataset do validate our approach in WENDY, while the results obtained in the PAN task are not as good as expected.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Age Detection has multiple applications, including marketing and security. In Optenet, we face Age Detection as an intermediate step in the overall process of protecting children in the Internet <ref type="bibr" coords="1,213.34,451.72,10.58,8.64" target="#b0">[1]</ref>. In fact, we are interested on two particular cases:</p><p>-Children posing as adults to be members of a social network. In most European countries, users aged below a certain threshold (typically 14) must not join a social network without explicit parent consent. Most social network operators just react at users claims, but in order to protect minors, a more proactive approach is required. We seek to detect youngsters or children below 14 (either in the case they have omitted their age, or they have entered a false, over 14 one), in order to provide this function as a service for parents and social network operators. -Adults posing as children in order to establish contact with minors, and sometimes, to perform harassment or sexual predation. In this case, we would be interested on monitor "fake" children (users over 18 but with an underage profile) to check which children they communicate, and to alert their parents or the social networks operators about the contact or when detecting inappropriate behavior.</p><p>In order to detect these cases, Optenet research team has been developing a project named "WENDY: WEb-access coNfidence for chilDren and Young" 1 in collaboration with the Group of Biometrics, Biosignals and Security from the Polytechnical University of Madrid. The project adopts a multi-biometric approach based on monitoring user profiles, communications (posts, chats, etc.) and pictures, and classify the users into three age ranges: [0 -13, 14 -17, 17-). Classification is performed by a multimodal classifier which combines several individual classifiers based on the analysis of different kinds of data. For instance, the output of specialized classifiers for the profile information, the posts and the pictures is combined into a single prediction by a second level classifier that has been trained on the outputs for a validation user dataset.</p><p>While the age ranges are different from those addressed in the PAN Authorship Profiling task, and we do not address gender recognition in the project WENDY, most of the text-based techniques used in this project can be applied to the task. In fact, we restrict ourselves to the Spanish language case as this is the one addressed in WENDY, and our linguistic resources are language-dependent. In consequence, we take our participation in this task as an opportunity to validate WENDY's text analysis approach in a different domain (from social networks to chats but within the same language).</p><p>The general approach followed in several WENDY's mono-modal classifiers is learning a content-based text classifier on a training collection, and it is inspired by previous work by <ref type="bibr" coords="2,207.11,322.55,79.70,8.64">Tam and Martell [3]</ref>. This is just the approach we follow in the PAN Author profiling tasks, and it consists of two phases: text analysis and representation, and classifier learning. We discuss these phases in the next sections, along with the results we have obtained in our experiments on the training collections and the lines of future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Text Analysis</head><p>As we face very noisy text, plagued with typos, abbreviations, emoticons and so on, we have designed a pipeline of language analyzers aimed at reduce this noise. The overall process consists of the following steps:</p><p>1. Text tokenization via a specific tokenizer designed for noisy Spanish language texts. 2. Token normalization according to a set of linguistic rules designed for noisy Spanish text. 3. SMS word-by-word language translation to standard Spanish by using two languagespecific dictionaries.</p><p>We discuss each process in the next subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Text Tokenization</head><p>The usage of specific language codes and chat and SMS-like messages is a major trend in electronic communications. This fact makes Natural Language Processing quite hard, even at the simplest step for text message tokenization, due to the widespread usage of non-alphanumeric symbols, frequent typos and non-standard word separators. Aiming at recognizing this type of language, we have developed a tokenizer that features two steps. The first consists of splitting the original text string into candidate tokens separated by white spaces. For every candidate token, we consider three possibilities:</p><p>-It is a sequence of alphanumeric characters, thus most likely making a proper word.</p><p>-It is a sequence of punctuation and non-alphanumeric characters, possibly a smiley.</p><p>-It is a mixture of both.</p><p>In the first two cases, we consider that the character sequences are already words themselves. This implies that non-alphanumeric character sequences, which in particular include punctuation symbols isolated tokens are considered relevant. In the third case, we proceed to a second tokenization step by splitting the candidate token into all sub-sequences of continuous alphanumeric and non-alphanumeric characters. For example, given the Spanish string "Hola:-)ketal"<ref type="foot" coords="3,330.44,219.01,3.49,6.05" target="#foot_1">2</ref> , this step would separate it into following tokens: "Hola", ":-)", and "ketal".</p><p>As we make use of the learning package WEKA <ref type="foot" coords="3,349.72,242.92,3.49,6.05" target="#foot_2">3</ref> , we have implemented this tokenization algorithm in Java, based on the previously existing tokenizer NgramTokenizer <ref type="foot" coords="3,159.10,266.83,3.49,6.05" target="#foot_3">4</ref> in WEKA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Token Normalization</head><p>The informal Spanish language used in forums and social networks has motivated the development of analyzers able to deal with phenomena such as the following ones:</p><p>-Contamination with typical SMS language abbreviations (like e.g. "tkm" -"I love you", "bss" -"kisses", "dsp" -"after", etc.). -Alternating lower and upper case letters (e.g.: "ThIS iS An ExAMplE").</p><p>-Repeating letters (e.g.: "heeeeeelllooooooo!").</p><p>-Omission of the letter "u" in the syllables "que" or "qui", or replacement og "qu-" by "k" (e.g.: "qiero", "kiero"). -Replacing the syllable "ca" or the letter "c" with the letter "k" (e.g.: "kompra kfe"</p><p>-"buy kofee"). -Intentional misspellings as "soi" ("I am"), "voi" ("I go"), "i" instead of "y", etc.</p><p>These phenomena and others are recognized and analyzed by the Deflogger normalization tool<ref type="foot" coords="3,195.31,465.51,3.49,6.05" target="#foot_4">5</ref> , which is a system developed to standardize the language used in the social network Fotolog<ref type="foot" coords="3,227.39,477.46,3.49,6.05" target="#foot_5">6</ref> . We apply this system to the token sequences obtained in the previous step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">SMS Language Translation</head><p>We have developed a system for translating textual elements using a number of existing linguistic resources, namely: a dictionary of SMS-like language for Spanish<ref type="foot" coords="3,447.87,546.35,3.49,6.05" target="#foot_6">7</ref> , and a Spanish language dictionary<ref type="foot" coords="3,247.66,558.30,3.49,6.05" target="#foot_7">8</ref> .</p><p>The translation system operates as follows:</p><p>1. Given a target word (possibly an expression in SMS), we search for it in the standard Spanish dictionary. 2. If the word is present in the dictionary, the process is terminated. If the word is not present in the dictionary, then we search for it the SMS language dictionary. 3. If the word is present in the SMS language dictionary, we select the most common or popular meaning or translation. If the word is not present in the SMS dictionary, we leave the token unchanged.</p><p>It should be emphasized that the SMS dictionary includes not only popular expressions as "xq" (the abbreviation of "porque", i.e. "because of"), but it also contains a significant amount of emoticons (like e.g. ":-)" etc.). The previous tokenization system is able to preserve these symbols, allowing us to translate them when possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Classifier Description</head><p>We follow a text learning approach in which we build a classifier using the Stochastic Gradient Descent for Text (SGDtext) algorithm<ref type="foot" coords="4,341.34,321.89,3.49,6.05" target="#foot_8">9</ref> as implemented in WEKA. This method employs the Stochastic Gradient Descent (SGD) <ref type="bibr" coords="4,364.74,335.52,11.62,8.64" target="#b3">[4]</ref> algorithm combined with a the WEKA String To Word Vector (STWV) filter in order to build the dictionary and update it in successive iterations. Two particular characteristics of the SGDtext algorithm is that it is specifically designed for working on text problems, and that it is update-able; that is, it builds the learning model incrementally, which allows us to train it in successive instance batches.</p><p>The SGD algorithm works as a batch learning method approaching the space defined by feature vectors using a loss function to converge. The vectors used in SGDtext are obtained from filtering plain text instances with the STWV filter, that transforms those text strings into term-weight vectors according to the Vector Space Model <ref type="bibr" coords="4,447.07,443.55,10.58,8.64" target="#b1">[2]</ref>. The STWV filter transforms a string attribute in a series of numeric attributes, corresponding with all words in the string and their frequencies.</p><p>The options used in the SGDtext classifier are: support vector machines as the loss function, a learning rate of 0.01, a regularization constant of 0.01, 500 iterations without pruning the dictionary, 3 as minimum word frequency, default normalization and no transformation to lower case the input instances. The tokenization of text strings is performed with the tokenizer described previously as a parameter. As the tokenizer is a version of the existing NgramTokenizer, it is possible to build n-grams instead of isolated tokens as text representation terms. We have configured the tokenizer to build unigrams, bigrams and trigrams in order to capture meaningful word sequences.</p><p>Regarding the training process, we have divided the original Spanish into four parts. We have accumulated the 30% of longest text files for training an initial model. Then we have randomly built two additional instance batches containing 30% of the instances for incremental learning, and an additional 10% batch kept for testing. In terms of absolut numbers, the original 56,126 have been divided in three batches of 17,176 instances and one batch for testing containing 4,597 instances.</p><p>Due to time constraints and computational limits, we have only been able to train the models on the original batch for both (age and gender prediction) problems, and updated the age prediction classifier with the second learning batch. Thus, the results we have obtained are limited and they do not show the full power of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Analysis</head><p>In the table 1 we show the results of the age prediction classifier obtained with our method when evaluated on the test batch we have produced. In the first three columns we show the contingency matrix (with decisions in columns while actual classes in rows), while in the two latest columns we show the precision and recall for each class. The results obtained for the age prediction problem are not good in general for the particular setup of the PAN'2013 author profiling task, but they are completely aligned with the kind of setup we face in the WENDY project. While precision for the (very under-represented) 10s class is low, the recall is very high -that is exactly what we have tried to achieve in WENDY, where it is very important to detect all children below 14 because they should not be in a social network without explicit (and legal) parent permission. As in WENDY, we have detected here that an important issue of our approach is that it is not able to discriminate between ages over 18 -which is not a problem, as far as our goal in WENDY is to detect people over 18 posing as minors.</p><p>In the table <ref type="table" coords="5,199.88,474.55,4.98,8.64">2</ref> we show the results for the gender prediction classifier on our test batch. The table is similar to the previous one except for the fact that there are only two classes. male female Prec. Rec. male 87 2211 0,509 0,0379 female 84 2215 0,500 0,963 Table <ref type="table" coords="5,231.88,562.83,3.36,8.06">2</ref>. Test results for the gender prediction problem.</p><p>The results obtained for gender prediction in our internal tests show a major trend of classifying every instance as "female", i.e. the classifier approaches the trivial rejector. As it is a first approach obtained by reusing exactly the same method used for age prediction, and it is clearly under-trained, those results can be expected but anyway they are far from good.</p><p>As an additional validation of the approach we have followed in the WENDY project for a close (but not identical problem), the results we have obtained in our internal experiments fulfill our expectations. In this sense, we must note that PAN Author Profiling and WENDY tasks face different problems. On one side, in PAN we work with individual blog posts while WENDY's goal is to detect age problems on a continuous stream of social network updates and profile information, which can include pictures and other data types as well. On the other, classes and problem statements are clearly different in both tasks; in WENDY, recall is extremely important for children below 14, while precision is a priority for adults (over 18) as they are a majority in the social network Tuenti. However, in PAN all classes are identically important in terms of accuracy metrics.</p><p>In the near future, we plan to finish the full experiment on the available training data, and compare it with the same setup without using linguistic resources. We want to extend both experiments to the English collection as well -for this task, specific linguistic resources have to be acquired, and a set of linguistic rules similar to Deflogger ones must be derived for this language.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,214.52,278.64,186.32,52.71"><head>Table 1 .</head><label>1</label><figDesc>Test results for the age prediction problem.</figDesc><table coords="5,254.13,278.64,107.10,41.85"><row><cell cols="2">10 20 30 Prec. Rec.</cell></row><row><cell>10s 78</cell><cell>2 1 0,299 0,963</cell></row><row><cell cols="2">20s 104 2367 14 0,552 0,552</cell></row><row><cell cols="2">30s 79 1919 34 0,694 0,017</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,657.08,153.09,7.77"><p>See: http://wendy.optenet.com, in Spanish.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,144.73,590.35,206.46,7.77"><p>Approximate translation into English: "Hello:-)whatsup".</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,144.73,601.50,157.22,7.77"><p>See: http://www.cs.waikato.ac.nz/ml/weka/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="3,144.73,612.65,309.98,7.77"><p>See: http://weka.sourceforge.net/doc.dev/weka/core/tokenizers/NGramTokenizer.html.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="3,144.73,623.80,136.99,7.77"><p>See: http://code.google.com/p/deflog/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="3,144.73,634.95,106.02,7.77"><p>See: http://www.fotolog.com.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="3,144.73,645.94,178.24,7.77"><p>See: http://www.diccionariosms.com/contenidos/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="3,144.73,657.08,324.53,7.77"><p>The one included in the linguistic analysis system Freeling: http://nlp.lsi.upc.edu/freeling/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8" coords="4,144.73,657.08,298.19,7.77"><p>See: http://weka.sourceforge.net/doc.dev/weka/classifiers/functions/SGDText.html.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work has been performed with partial support from the <rs type="funder">Ministry of Economy and Competitiveness and the Center for Industrial Technological Development (CDTI)</rs> under the granted project "<rs type="projectName">WENDY: WEb-access coNfidence for chilDren and Young</rs>" (<rs type="grantNumber">TSI-020100-2010-452</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_6wXQvQA">
					<idno type="grant-number">TSI-020100-2010-452</idno>
					<orgName type="project" subtype="full">WENDY: WEb-access coNfidence for chilDren and Young</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,138.13,457.69,342.46,7.77;6,146.47,468.65,203.82,7.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,288.85,457.69,191.74,7.77;6,146.47,468.65,87.75,7.77">Avances tecnol&apos;ogicos en la protecci&apos;on del menor en redes sociales. Nov&apos;atica</title>
		<author>
			<persName coords=""><forename type="first">G'omez</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Caurcel D'iaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,240.21,468.65,62.28,7.77">Revista de la ATI</title>
		<imprint>
			<biblScope unit="volume">218</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.13,479.61,340.80,7.77;6,146.47,490.57,44.08,7.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,199.05,479.61,178.98,7.77">Machine learning in automated text categorization</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,383.81,479.61,71.22,7.77">Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">34</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.13,501.53,341.07,7.77;6,146.47,512.49,302.52,7.77;6,146.47,523.45,108.12,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,229.43,501.53,75.25,7.77">Age detection in chat</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>Martell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,322.57,501.53,156.63,7.77;6,146.47,512.49,128.73,7.77;6,322.05,512.49,32.82,7.77">Proceedings of the 2009 IEEE International Conference on Semantic Computing</title>
		<meeting>the 2009 IEEE International Conference on Semantic Computing<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="33" to="39" />
		</imprint>
	</monogr>
	<note>ICSC &apos;09</note>
</biblStruct>

<biblStruct coords="6,138.13,534.41,332.50,7.77;6,146.47,545.37,333.19,7.77;6,146.47,556.32,202.73,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,185.65,534.41,284.98,7.77;6,146.47,545.37,36.91,7.77">Solving large scale linear prediction problems using stochastic gradient descent algorithms</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,201.52,545.37,274.63,7.77;6,182.34,556.32,35.30,7.77">Proceedings of the twenty-first international conference on Machine learning</title>
		<meeting>the twenty-first international conference on Machine learning<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">116</biblScope>
		</imprint>
	</monogr>
	<note>ICML &apos;04</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
