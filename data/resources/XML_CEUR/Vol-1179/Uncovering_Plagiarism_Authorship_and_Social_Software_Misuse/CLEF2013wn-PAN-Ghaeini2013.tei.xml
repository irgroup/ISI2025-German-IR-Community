<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,135.34,115.90,344.68,12.90;1,291.74,133.83,31.88,12.90;1,223.43,153.68,168.50,10.75">Intrinsic Author Identification Using Modified Weighted KNN Notebook for PAN at CLEF 2013</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,280.42,190.08,54.51,8.64"><forename type="first">M</forename><forename type="middle">R</forename><surname>Ghaeini</surname></persName>
							<email>mr.ghaeini@aut.ac.ir</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Engineering and Information Technology</orgName>
								<orgName type="institution">Amirkabir University of Technology</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,135.34,115.90,344.68,12.90;1,291.74,133.83,31.88,12.90;1,223.43,153.68,168.50,10.75">Intrinsic Author Identification Using Modified Weighted KNN Notebook for PAN at CLEF 2013</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">48F199A24B2E7F2B1A30C70656232D28</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Author Attribution</term>
					<term>Identification</term>
					<term>Intrinsic</term>
					<term>Weighted KNN</term>
					<term>N_gram</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we describe our approach and task for the PAN 2013 Author Identification. Best thing for an intelligent application is a large corpus, but when it is small, it would be helpful to extract many features from dataset and study on them. So, we extract many features from documents like: word bag, stop word bag, punctuation bag, part of speech (POS) bag and etc. It is so important that you select proper features for your learning algorithm; therefore, we decide to learn proper features too. Next, we identify author using a modified weighted KNN since it can have a good performance for a small corpus.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Authorship attribution supported by statistical or computational methods has a long history starting from 19th century and marked by the seminal study of Mosteller and Wallace (1964) on the authorship of the disputed Federalist Papers. During the last decade, this scientific field has been developed substantially taking advantage of research advances in areas such as machine learning, information retrieval, and natural language processing <ref type="bibr" coords="1,179.98,489.06,10.58,8.64" target="#b0">[1]</ref>. Authorship attribution is an important issue and it can be helpful in applied areas such as history, law and etc. Where knowing documents author is important. Human readers can realize that a document belongs to a specific author or not by comparison between document styles. It is the main idea of intrinsic author identification. In Intrinsic algorithms we try to learn author's writing style and it can be hard, so we can use so many different features to recognize author's style; for example we can use stylometric features like lexical features; syntactic features and semantic features. After features extraction, we must calculate document styles similarity (or distance). There are different methods for this calculation like cosine similarity, Minkowski distance, etc. Intrinsic algorithms can be so helpful when we don't have a large corpus. Corpus is so important in learning of intelligent applications. In this task (Author Identification) we have a small dataset for learning in three languages and it makes that a little hard to reach a proper result. KNN is a good method in this situation since we have a small dataset, we can make decision locally and use whole of our information as good as possible. But we must use weighted KNN to balance the effect of each feature; otherwise, its result can be unacceptable and awful. Also, it is so important to avoid wrong announcement, so we must use some methods to decrease wrong announcement chance, therefore we can use some decision maker to reach to a final decision. Some related works have been done in this area before, and we will describe some of them briefly in blow: Usually, there are extremely few training texts at least for some of the candidate authors or there is a significant variation in the text-length among the available training texts of the candidate authors. Moreover, in this task usually there is no similarity between the distribution of training and test texts over the classes, that is, a basic assumption of inductive learning does not apply. It presents methods to handle imbalanced multi-class textual datasets <ref type="bibr" coords="2,198.26,250.82,10.58,8.64" target="#b1">[2]</ref>. Text representation is a necessary procedure for text categorization tasks. Currently, bag of words (BOW) is the most widely used text representation method but it suffers from two drawbacks. First, the quantity of words is huge; second, it is not feasible to calculate the relationship between words. Semantic analysis techniques help BOW overcome these two drawbacks by interpreting words and documents in a space of concepts. They propose a concise semantic analysis technique for text categorization tasks <ref type="bibr" coords="2,434.39,322.55,10.58,8.64" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Feature Selection and Extraction</head><p>It is so important that you have proper and enough features for learning something and making decision about it. In text analyzing, we have different parts like: paragraph, sentence and word. We can extract some features from them; also we can consider syntactic or grammatical properties as a feature. Below we describe some features, which we use them in our application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Lexical Features</head><p>Paragraphs. Authors have their habits about writing, some authors write many paragraphs, but their paragraphs are short. Others don't write many paragraphs, but their paragraphs are long, or other styles. We can consider paragraph number and average of paragraph length as a feature.</p><p>Sentences. Sentence is like paragraph in our observation. We can consider number and average length of them as a feature too. It really can be helpful, since number and length of sentences related to writer's mind.</p><p>Words. Words are so useful in our approach. But they are too much and if we use them without any preprocessing, our features can be specific and it isn't good at all. So we use Stemming to reach more general words in our approach; since there are different words which all have the same stem and concept. Besides, stop words aren't important and they are so common between authors. So we ignore them in word processing to make words more meaningful.</p><p>In word processing, we consider words average length, unique word number average in a sentence and word number average in a sentence as a feature (in this section, we process on document words without stemming or removing stop words. Moreover, we make up bag of words in two different ways. In First way, value of a word equals to its frequency in documents, and in second way, value of a word equals to its TF.IDF amount.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Syntactic Features</head><p>One reason that syntactic features perform well is because they are topic-independent. A person's preferred syntactic constructions can also be cues to his/her authorship <ref type="bibr" coords="3,464.02,241.18,10.58,8.64" target="#b3">[4]</ref>.</p><p>Parts-of-Speech scoring by n-gram. A simple and proper way to capture syntactic construction of an author is parts-of-speech tagging. In this case, we have construction of a document. Also it is important that we generalize our POS types, for example we can classify every type of noun like plural, as noun. Now, we use n-gram to capture author style. We use trigrams, 5-grams and 7-grams to reach a proper result. Moreover, we make up a bag for each of them (like bag of words) by using frequency of them in a document, but in this case we use specific type of them, since we look at the whole document (not sequence like n-gram) and we don't need to generalize them anymore and their specific types can be helpful, for example an author maybe use passive verbs too much, and if we generalize that, we lose an important knowledge about him/her. Punctuation habits. For unedited texts, we can identify an author based on frequency of distinctive punctuation habits <ref type="bibr" coords="3,266.81,432.79,10.58,8.64" target="#b4">[5]</ref>. We inspect some popular punctuation and obtain frequency of them. It can help us in author identification.</p><p>Stop words. Stop words describe relationships between content words, they are meaningless and we can remove them in word processing, but their frequencies can help us in author identification, for example some authors use "the" too much. Authors can use different stop words in their document, for example we can use "thus" instead of "so" in many situations. Furthermore, there are different stop words with the same meaning but noticeable different length, like: "furthermore" and "also" or "consequently" and "so". We consider stop words average length and stop words average number in a sentence as a feature. Moreover, we made up a bag of stop words (like bag of words) by usage frequency of them in a document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Other Features language.</head><p>In this task, we must study on three languages (English, Spanish and Greek), so language can be a feature in our task, since every language can has its properties. We use this feature in decision making, since maybe it isn't good to use train result of a language to identify author with another language without any observation. known documents number. In this competition we have some sub folder and we must recognize that unknown document belongs to known document(s) author or not. But we have varying number of known documents in them. Number of known documents can be a feature because if it was low, we would need more probability to making decision about unknown document; therefore, we use this feature in making decision too.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Algorithm</head><p>We can divide our approach into 3 main part: PreProcssing, Comparison Step and Decision Making. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">PreProcessing</head><p>We extract documents features in preprocessing step. We described this step in section 2 of this paper. This step done for all known and unknown author documents in a folder (every document which related to a problem). At the end of this step, we prepare all features that we need, like: bag of word, stop word, parts-of-speech and etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison Step</head><p>In this step we have two types of work. First calculating distance of our bags (like bag of word, stop word, etc.) from unknown document bags; second, calculating parts-ofspeech score for unknown document.</p><p>Distance Calculation. We have at least 4 bags (you see them in Figure1) for each document, and if we want to use them in learning, we must calculate their distances from corresponding unknown document bags. For doing that, we use cosine similarity to determine distance or similarity between two arrays or bags (Equation <ref type="formula" coords="5,427.13,155.18,3.60,8.64" target="#formula_0">1</ref>).</p><formula xml:id="formula_0" coords="5,227.45,167.14,249.27,29.66">similarity = cos(θ) = - → A . - → B - → A . - → B (<label>1</label></formula><formula xml:id="formula_1" coords="5,476.72,180.01,3.87,8.64">)</formula><p>Cosine similarity is a popular vector based on similarity measure in text mining and information retrieval. Cosine similarity is a good method for determining distance of two vectors, because length of vectors don't affect the similarity of them, thus we can simply ignore difference in document length by this feature.</p><p>In many problems we have more than one known document, thus we must determine a way to reach one amount for distance between unknown document and known documents. There are many ways (like multiplying) to reach that, but we use average amount of them. For more explanation, after calculating POS distance for each known document from unknown document, we set average of them as POS distance of known documents from unknown document.</p><p>Parts-of-speech Score. When we are processing known documents, we capture frequency of each POS n-gram (trigram, 5-gram and 7-gram separately), which exists in them (and at last we normalize their frequencies). Next, in processing unknown document, we search each POS n-gram, which exists in that in our POS n-gram list. If we find that in our list, we add frequency of that to POS score of unknown document, and if we can't find that, we subtract 1 point from unknown document POS score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Weighted KNN</head><p>We use KNN algorithm for our application. KNN is a proper method in this task because we have a small corpus. We can make decision locally and use all of our information as good as possible by KNN. But we must use weighted KNN to balance the effect of each feature; otherwise, its result can be unacceptable and awful. Thus, we use some coefficients when we want to calculate distance between test data and train data (Equation <ref type="formula" coords="5,134.77,500.17,3.60,8.64" target="#formula_2">2</ref>).</p><formula xml:id="formula_2" coords="5,177.26,514.10,303.34,30.55">distance j = Dimensions k=0 ((testData k -trainData jk ) 2 * w j )<label>(2)</label></formula><p>At last, if test data, and train data j belong to different languages, we add a constant to distance of them. But it isn't enough yet, determining weights is so significant; therefore, we learn them too. We use gradient descent approach to find best weights, which we can. In this case we just need to calculate cost (Equation <ref type="formula" coords="5,299.36,598.07,4.15,8.64">3</ref>) partial derivation based on weight of each dimension (Equation <ref type="formula" coords="5,220.93,610.02,3.60,8.64" target="#formula_3">4</ref>). It is noticeable that y * means real and true result of train data (label of data) in equations.</p><formula xml:id="formula_3" coords="5,250.08,637.94,230.51,30.32">Cost = testsetsize i=0 (y i -y * i ) 2 (3) ∂Cost ∂w j = testsetsize i=0 (2 * (y i -y * i ) * ∂y i ∂w j )<label>(4)</label></formula><p>Calculation of partial derivation of cost function is a little long; therefore, we can't explain it here. Moreover, we set K equals to 3, in the other hand we use 3NN, because we want to search more locally in our dataset. But we change decision-making method in our KNN (or 3NN) too. Distance affects on our decision too and we don't only look at the label of neighbors (Equation <ref type="formula" coords="6,229.12,217.84,3.60,8.64" target="#formula_4">5</ref>).</p><formula xml:id="formula_4" coords="6,259.92,230.56,220.67,35.21">y i = K j=0 ( y * j distanceij ) K j=0 ( 1 distanceij )<label>(5)</label></formula><p>In Equation <ref type="formula" coords="6,184.56,277.96,3.74,8.64" target="#formula_4">5</ref>, we assumed that arrays has been sorted ascending based on distance ij (distance between test data i from train data j). So, distance i0 is minimum distance of test data i from our dataset, and y * 0 is its label. In this case, y i equals to probability that unknown document belongs to known document(s) author.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Decision Making</head><p>It is so important to avoid wrong announcement in our application. We learned feature weights by our learning method and in best case, we reached to %85.71 hit (30 problems of 35 problems identified correctly) with different weights and different fails (we couldn't identify different 5 problems correctly). Accordingly, we use 5 decision makers with different weights to reach better and more robust result. We select 5 decision makers with different weights that can cover their fails as good as possible. We try to cover each miss with at least 3 hit to make sure that our algorithm can identify better in that decision area. Weight series were so different. We saw that some features got 0 weight in some weight series, thus we tried to select different weight series with different attention to features to reach best result that we could. At last, we consider average of decision maker probability as probability of unknown document. Next we make final decision based on known document number. For each range of known document number, we determine a threshold to announce our result, since known document number affects to certitude of our result (if we have more known documents, we can capture author style and features better and with more probability). If the probability doesn't reach to test data threshold, we don't announce any thing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Resources</head><p>We use three APIs for POS tagging: "Stanford-postagger" for English tagging, "Tree Tagger" for Spanish tagging and "AUEB_POS_tagger" for Greek tagging. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>Unfortunately, we didn't get excellent results, because of two mistakes in our implementation. First, we set thresholds without any evidence, so it affected recall and precision, since we use a useless high certitude. Second, we didn't select proper weight series for decision-making step, since we forgot effect of probability in decision-making step. About runtime; we print many strings in our algorithm, to it be traceable and print command is one of the slowest commands in programming; consequently, our runtime is more than the actual runtime of our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>In our task, we must study on three languages: English, Greek and Spanish, but unfortunately, we were not so familiar with Greek and Spanish at all. Especially Greek, and we got poor result in this language. We just treated with them like English and we only used their POS tagger, their stop words and their stemmer (nothing else is different between them in our application). More important, we set thresholds without any evidence and it really affected result of our application. We must learned proper thresholds too, or at least we must use some different tests set to reach proper thresholds.</p><p>Another mistake is that we tried to reach good weights for features without observing author language. Importance of our features (weights) can be change in different languages, thus we must learn weights for each languages separately. Moreover, when we selected our making decision weights series, we didn't pay attention to probability of their responses, but we use average of their responses probability to make final decision (if one of them returns a wrong probability, which is far from the right answer, it badly affects our final decision and also can make us give a wrong answer). And when we ran our application again for testing this hypothesis, we saw that it is true (unfortunately). Furthermore, our data set was too short and we didn't use any train data more than competition train dataset, but we could do that and if we did we could absolutely get a better result.</p><p>According to Discussion section, we can say that the presented algorithm in this paper is a good and proper algorithm for author identification based on competition task, since when we don't have a large corpus, each train data can be important. Also, we can have some local decision areas; therefore, we can't use linear learning algorithms, thus our algorithm can be so good and proper in this situation (Moreover, if we can use linear learning algorithm, we can use this algorithm too without loosing anything). Some of KNN algorithm problems are: 1-KNN can take a long time to make decision and find K nearest neighbors. But in this task, we didn't have large dataset, so it takes a short time to find them; moreover, we can use some tree structures like BST to reduce this time.</p><p>2-If we have (a) non-related and helpful feature(s) among our features, KNN result can be bad and make unacceptable result. But we learn weights and importance of features, thus we don't have any problem about non-related features. Moreover, when we use weights for effect of features, we reach a good and meaningful distance for test data and it helps us to make a good decision very much.</p><p>3-If we only pay attention to K nearest neighbor without considering their distribution, it is possible that our distance don't show real distance of test data from their neighbors. But we pay attention to it too.</p><p>As you see, we don't have these problems in our algorithm, so it can be so good for this task.</p><p>But unfortunately, we have some mistakes in our implementation; therefore, we couldn't get an expected and proper result (top three) in competition.</p><p>Next time, we will absolutely use more train data and never use anything without acceptable evidence. Our ideas were good but we didn't use them in a correct way, we think that if we used them correctly, we could reach a proper result for sure.</p><p>Moreover, we will study on language properties and we will use different weights series for each language to get a better result. Besides we will find good thresholds for our algorithm by learning. Furthermore, we can use some decision maker, but when we want to make decision, we use some weights for combining their result to reach proper result and paying attention to their responses probability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,229.32,470.44,156.71,8.12;4,134.77,268.61,349.86,187.09"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Abstract diagram of our approach</figDesc><graphic coords="4,134.77,268.61,349.86,187.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,216.70,115.83,181.96,89.07"><head>Table 1 .</head><label>1</label><figDesc>Preliminary performances on the test data</figDesc><table coords="7,239.46,136.86,136.29,68.04"><row><cell cols="3">Language F1 P recision Recall</cell></row><row><cell>All</cell><cell>0.606 0.671</cell><cell>0.553</cell></row><row><cell cols="2">English 0.691 0.760</cell><cell>0.633</cell></row><row><cell cols="2">Greek 0.461 0.545</cell><cell>0.400</cell></row><row><cell cols="2">Spanish 0.667 0.696</cell><cell>0.640</cell></row><row><cell></cell><cell>Runtime 125655 ms</cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,138.13,544.48,342.46,7.77;8,146.47,555.44,291.48,7.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,205.15,544.48,187.99,7.77">A Survey of Modern Authorship Attribution Methods</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,403.58,544.48,77.01,7.77;8,146.47,555.44,130.27,7.77">American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="538" to="556" />
			<date type="published" when="2009-03">March 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.13,565.41,342.46,7.77;8,146.47,576.37,329.23,7.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,203.73,565.41,276.85,7.77;8,146.47,576.37,11.61,7.77">Author identification: Using text sampling to handle the class imbalance problem</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,168.06,576.37,145.95,7.77">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="790" to="799" />
			<date type="published" when="2008-03">March 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.13,586.34,342.46,7.77;8,146.47,597.30,334.12,7.77;8,146.47,608.26,54.78,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,382.03,586.34,98.56,7.77;8,146.47,597.30,107.35,7.77">Fast text categorization using concise semantic analysis</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhixing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhongyang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yufang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Chunyong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,263.87,597.30,99.04,7.77">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="441" to="448" />
			<date type="published" when="2011-02">February 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.13,618.23,342.46,7.77;8,146.47,629.19,140.69,7.77" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="8,183.72,618.23,265.29,7.77">Authorship Attribution. Foundations and Trends in Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Juola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006-12">December 2006</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="233" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.13,639.16,342.46,7.77;8,146.47,650.12,145.78,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,199.25,639.16,222.71,7.77">Empirical evaluations of language-based author identification</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">E</forename><surname>Chaski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,431.51,639.16,49.08,7.77;8,146.47,650.12,26.35,7.77">Forensic Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="65" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
