<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,136.94,148.76,332.74,15.96;1,133.46,166.16,328.49,15.96;1,270.17,183.56,55.08,15.96;1,211.37,204.04,172.60,12.00">Plagiarism Candidate Retrieval Using Selective Query Formulation and Discriminative Query Scoring Notebook for PAN at CLEF 2013</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,224.33,242.12,61.20,9.05"><forename type="first">Osama</forename><surname>Haggag</surname></persName>
							<email>osama.haggag@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Informatics Science</orgName>
								<orgName type="laboratory">Text Mining Research Group</orgName>
								<orgName type="institution">Nile University</orgName>
								<address>
									<settlement>Cairo</settlement>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,305.09,242.12,77.46,9.05"><forename type="first">Samhaa</forename><surname>El-Beltagy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Informatics Science</orgName>
								<orgName type="laboratory">Text Mining Research Group</orgName>
								<orgName type="institution">Nile University</orgName>
								<address>
									<settlement>Cairo</settlement>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,136.94,148.76,332.74,15.96;1,133.46,166.16,328.49,15.96;1,270.17,183.56,55.08,15.96;1,211.37,204.04,172.60,12.00">Plagiarism Candidate Retrieval Using Selective Query Formulation and Discriminative Query Scoring Notebook for PAN at CLEF 2013</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">658CA8C3776D8E48387840E25397C676</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper details the approach of implementing an English plagiarism source retrieval system to be presented at PAN 2013. The system uses the TextTiling algorithm to break a given document into segments that are centered around certain topics within the document. From these segments, keyphrases are generated using the KPMiner keyphrase extraction system. These keyphrases and segments are then used in generating queries indicative of the segment, and consequently the document. The queries are submitted to ChatNoir for finding plagiarism sources in the ClueWeb09 corpus from which the pan13 dataset is plagiarized. The target is to lessen the overall search effort while maximizing the performance by scoring unconsumed queries against the already downloaded candidate sources. Comparison to other PAN 2013 submissions for the same task, show the presented system to be one of the top performers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Plagiarism is the act of copying or using other people's data without permission or giving credit. It poses an unfortunate problem in modern academia as figures, charts, and even text from research papers and proposals can be copied without permission. For example, when asked to write a study on a certain topic, most students would start by searching the internet, or looking up Wikipedia. Some would then proceed to copy data on the subject, be it charts figures, tables or text, while omitting references to the sources. Such activities are deemed dishonorable and unacceptable in the academic community, and are considered as theft. It is often up to the reviewers of the material to detect the plagiarism through experience, and properly act upon it by proving and reporting it. Many tools have been developed recently for checking papers for plagiarism examples of which include Turnitin's OriginalityCheck <ref type="bibr" coords="1,433.71,634.48,12.81,9.05" target="#b0">[1]</ref> and iThenticate <ref type="bibr" coords="1,168.18,645.88,11.83,9.05">[2]</ref>. These tools check submitted papers for plagiarism by comparing them against a vast collection of documents in the form of webpages, papers, etc. The increasing demand for these tools serves to prove how severe the problem of plagiarism is becoming.</p><p>In this paper, we present an approach for generating discriminative queries from documents suspected of plagiarism. These queries are then used to search for possible sources of a suspect document at hand. This is done by dissecting the given document into related subtopics that are then condensed into characteristic keyphrases. The keyphrases are used to direct the focus of the search towards the possible sources of the aforementioned subtopics to avoid looking up as many irrelevant documents as possible.</p><p>The rest of this paper is organized as follows; Section 2 describes the problem and explores its various dimensions. Section 3 provides the details of the implementation. Section 4 presents the performance of the system in comparison to other PAN 2013 system submissions. Section 5 concludes the paper, and discusses possible future improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem and Task Descriptions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Description</head><p>Text plagiarism is a very common form of plagiarism, and is one of the prime focuses of PAN <ref type="bibr" coords="2,183.64,375.83,14.66,9.05" target="#b1">[3]</ref>. Unfortunately, detecting this kind of plagiarism can be a difficult task. The difficulty in detecting such plagiarism stems from the fact that it is relatively easy for one to conceal the plagiarism in a number of ways. One could obfuscate, paraphrase, or simply rearrange words, rendering it much harder for a machine, or even a human to detect the plagiarism. However, most plagiarists do not exert a lot of effort in their plagiarism as they simply copy and paste data.</p><p>There are two types of plagiarism detection, extrinsic and intrinsic <ref type="bibr" coords="2,418.76,445.01,11.52,8.89" target="#b2">[4]</ref>; intrinsic detection is the detection of deviations in the overall document style given only the document, thus identifying possible non-authentic bodies of text. Extrinsic detection is the task of identifying possible plagiarism instances in the document with relation to other documents. Extrinsic plagiarism detection is the focus of this work. Extrinsic plagiarism detection as a task is comprised of two sub-tasks: source retrieval, and text alignment. Source retrieval is the process of locating as many as possible source documents from which a suspect document has been plagiarized. Text alignment is the process of mapping passages within a suspect document to passages from which they have been copied in the source document. Source retrieval is the focus of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">PAN task Description</head><p>Participants in the source retrieval task were given a plagiarized dataset <ref type="bibr" coords="2,439.61,605.92,11.83,9.05" target="#b3">[5]</ref> that consists of suspicious documents. Each document addresses a certain topic and is plagiarized from web pages on that topic from the ClueWeb09 <ref type="bibr" coords="2,374.90,628.84,14.87,9.05" target="#b4">[6]</ref> corpus. Analysis of the dataset revealed that there is little to no obfuscation in the documents. Some small passages and headlines are authentic and not plagiarized and the documents are well written, punctuated, and organized into paragraphs focusing on certain aspects.</p><p>The task requires processing the suspect documents to formulate queries that are indicative and characteristic of their content. These queries should then be used to search for the plagiarism sources in the ClueWeb09 corpus using one of two provided search engines: ChatNoir <ref type="bibr" coords="3,224.24,184.04,13.34,9.05" target="#b5">[7]</ref> or Indri <ref type="bibr" coords="3,269.69,184.04,11.23,9.05" target="#b6">[8]</ref>.</p><p>Four different measures are used to assess any source retrieval system: Retrieval performance represented by Precision, Recall, and the F1 score, the workload represented by the number of queries and document downloaded, time till first detection, represented by the number of queries and documents downloaded before an actual source is located, and system runtime. The goal is to maintain a good balance between those four measures while aiming to maximize retrieval performance and to minimize workload and system runtime. With these constraints in mind, keeping a keen eye on downloads is determined to be the core of the problem. Downloading irrelevant documents would lead to more searching, possibly retrieving more documents that are irrelevant, and damaging the performance. Whereas downloading relevant documents would help minimize the search effort and sharpen the system's precision. As such, it was concluded that the strategy to be adopted must be devised to control the number of downloads.</p><p>Another factor that should be taken into consideration is that the ClueWeb09 corpus contains "spam" pages, and noisy documents such as huge site directories and listings. These could be wrongfully flagged as candidate sources, due to these pages having a huge variety of unrelated words that could trick the system into flagging them as a source of a certain query slowing the time to the first actual correct result, while dealing a blow to the precision of the system. Thus, we have also concluded that after retrieving candidate sources, and to keep the number of downloads and precision in check, there needs to be some kind of relation between the current downloaded candidates and the queries that have not been utilized yet. It would be favorable to score the unutilized queries against the already downloaded candidates to prevent the system from over-searching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Implementation</head><p>The slight obfuscation in the dataset was determined not to affect the general outcome of the plagiarism detection process as initial experimentation showed that it did not hinder the searching process from retrieving documents that are relevant to the topic at hand. We have chosen ChatNoir as the search engine to use for searching for possible document sources. Within the proposed system, a number of phases were implemented. In the following subsections, each phase is detailed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preparing the Data</head><p>Given an input suspect plagiarized document, it is preprocessed with regular expressions to remove all non-English characters (including numbers, symbols, punctuation, etc) since the dataset is already known to be in English. It is then tokenized on whitespace, and the frequency distribution of the document is calculated. Then using the TextTiling <ref type="bibr" coords="3,231.12,674.80,12.58,9.05" target="#b7">[9]</ref> algorithm (provided by Python's NLTK <ref type="bibr" coords="3,405.34,674.80,21.86,9.05" target="#b8">[10]</ref> platform), the document is divided into topically related segments/subdocuments. This step is tuned to produce a relatively small number of large segments, as explained in subsection 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Formulating the Queries</head><p>Keyphrases are extracted for each of the document's generated segments using the KPMiner <ref type="bibr" coords="4,159.13,230.00,19.61,9.05" target="#b9">[11]</ref> keyphrase extraction system, which returns the topmost keyphrase of the segment. This keyphrase (which is a phrase of 1 to 3 words on average) is said to be characteristic of the segment. The segment is then divided into sentences, and every four sentences are grouped into a chunk. A query is then generated for each chunk.</p><p>Each chunk is preprocessed exactly like the main document, and tokenized on whitespace. English stopwords are removed, and the unique words (words with a frequency of 1 over the document) are identified. These unique words are removed from the chunk, and added to the chunk's query. The chunk now contains no stopwords, or unique words. The remaining non-unique words in the chunk are then sorted by ascending frequency, and moved into the query in their sorted order.</p><p>The query is now organized in such a way that the unique words are at the beginning, and then the rest of the words in ascending frequency. If the length of the query is greater than 10 words, the query is trimmed down to 10 keywords taking only the first 10 terms that appear in the query. This is due to ChatNoir's keyword limit of 10. The segment's keyphrase (which could be of length n-words) is then added in place of the last n words in the query if the query does not already contain the keyphrase's terms. The queries are stored as a list of strings per document. An overview of the process of query generation is shown in Fig. <ref type="figure" coords="4,367.04,437.05,3.91,9.05" target="#fig_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Searching</head><p>ChatNoir allows for requesting snippets of the search results of a query. A snippet request returns a number of characters (500 characters) from the search result around the terms of the submitted query. Snippets do not count as "downloads" when calculating performance. For each document, its list of queries is traversed as follows:</p><p>The first query is submitted to ChatNoir for a snippet request. The query is scored against the snippet through simple token matching; if 50% or more of the query's tokens are found in the 500-character snippet's tokens, then the search result (the snippet's origin) is considered a candidate source. This candidate is then downloaded and added to a list of downloaded documents related to the document at hand.</p><p>Each of the subsequent queries is then checked against the list of downloaded documents through simple token matching as well. If 60% or more of the query's tokens are found in any of the downloaded documents' tokens, then this document is considered a source for this query, and this query will not be used for searching. If the query fails to score 60% against all the downloaded documents, it is then used for a new snippet request, and possibly to download a new candidate source as above. The algorithm for scoring the queries is illustrated in pseudo-code below: When downloading documents, ChatNoir provides an "oracle" function. Given the id of the suspicious document in the training set that you are searching for, and the id of the page you are trying to download, the oracle would inform you if the downloaded page is a source for the suspicious document or not. This is used to calculate precision, recall, and the time to the first actual result.</p><formula xml:id="formula_0" coords="5,170.18,264.40,57.65,6.44">For i = 1 to</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Tunable Parameters</head><p>The plagiarism detection system detailed above is affected by a number of tunable parameters. Due to the time cost of the runs over the dataset, as a typical run takes around 2-3 hours, it was decided that determining the optimum factors through iteration over all the different combinations of the parameters would take a lot of time. As a result, a different approach was used to obtain values that would be good enough, but are not necessarily globally optimal. Through human intuition and common sense, and a small number of experiments we tried to find an optimum value for each of the parameters. Below, we go through the reasoning behind the various configurations. The row in bold in the tables denotes the submitted configuration.</p><p>TextTiling parameters: TextTiling offers control over how large the subdocuments/segments can be, by changing two parameters w, and k; TextTiling is done by tokenizing the text into pseudo-sentences of a fixed size w, where k is the size (in sentences) of the block used in the block comparison method <ref type="bibr" coords="5,400.47,587.44,18.05,9.05" target="#b10">[12]</ref>. The size of the pseudo-sentences control to a large degree the number of segments generated.</p><p>Through experimentation, it was found that tuning the segmentation for a large number of topics of small size leads to higher recall but lesser precision. This is logical due to there being a lot of variance in the queries due to the large number of topics and their different keyphrases; Many queries would fail to score 60+% on the downloaded documents due to the variety in the downloads. This leads the system to carry out more searching and downloading, which in the end damages the precision, without much gain in recall as shown in Table <ref type="table" coords="5,312.41,679.48,3.78,9.05" target="#tab_1">1</ref>.</p><p>It is determined that setting the segmentation to collect larger topics (an average of 15 segments per document) is best for both precision and recall. This is obtained by setting w = 50, and k = 5 as can be seen in Table <ref type="table" coords="6,322.63,172.52,3.83,9.05" target="#tab_1">1</ref>. Chunk size selection: there is a choice as to how large the sentence chunk size could be. Setting the chunk size to 1 (one sentence per chunk, i.e., one sentence per query) leads to more searching, and higher recall at a loss for precision. The choice of four sentences was determined by running a number of experiments to determine the best performing chunk size and as can be seen from table 2, a chunk size of 4 was best for both precision and recall. There is also the choice of the frequency threshold that identifies "unique" words. Several values were also tested in the same manner and a threshold of 1 was found to be best as shown in Table <ref type="table" coords="6,342.67,345.71,3.77,9.05" target="#tab_2">2</ref>. Search parameters: one can retrieve more than one result for a certain query. Doing so does not benefit the performance, as the first result is often the most correct one. There are also the two scoring functions: one that scores queries against snippets, and queries against candidate downloaded documents. The snippet score is chosen to be 50%. On trying higher values, more snippets would fail to pass the check due to their limited number of characters, damaging both the precision and the recall. For lower values most snippets would pass the check flagging more documents for downloads, damaging the precision. The same rationale goes for scoring queries against the candidate documents as in shown Table <ref type="table" coords="6,331.99,557.29,3.77,9.05" target="#tab_3">3</ref>. In bold are the metrics in which each system performed better compared to the others</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No. of Topics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>This paper has presented a system that can retrieve plagiarism sources while minimizing the workload. The system is capable of achieving its goal by careful formulation and elimination of queries to be submitted for search. The system utilizes two algorithms in generating the queries, namely the TextTiling algorithm, and the KPMiner keyphrase extraction algorithm as well as a set of heuristics for employing those. The system's performance was shown to be among the best this year, as shown in Table <ref type="table" coords="7,160.43,586.36,3.76,9.05">4</ref>.</p><p>There is however room for improvement on the current system. For example, the values of the used parameters could be further optimized. Measures could also be taken to counter the obfuscation without adding too much complexity to the system. More use of ChatNoir's additional functionality can be made. This includes for example, making use of ChatNoir's batch query service where you can request search results for more than one query in the same request. This can be used on the document level, by coming up with a method of speeding up the searching process by using more than one query at the same time, while maintaining the scoring scheme. It can also be used on a dataset level basis by processing a number of documents simultaneously. Moreover, ChatNoir provides some advanced parameters for searching, such as defining spam rank, page rank limits, and a proximity factor between queries and the results they return. All these could be used to refine the search results, filtering out unwanted result pages. The scoring functions could take into consideration the context of the query while scoring queries against candidate documents instead of simple token matching. The code to our implementation will be available under the MIT license <ref type="bibr" coords="8,252.57,230.00,16.20,9.05" target="#b11">[13]</ref> for whoever wants to extend or use our system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,244.01,575.06,118.74,7.31"><head>Fig. 1 ,</head><label>1</label><figDesc>Fig. 1, Overview of query generation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,132.86,195.12,334.56,56.23"><head>Table 1 ,</head><label>1</label><figDesc>TextTiling parameters w, k at a Snippet score of 50%, and a Query score of 60%, Chunk Size of 4, Frequency Threshold of 1. First Detection is in queries. All numbers are averages over the dataset</figDesc><table coords="6,139.70,195.12,315.94,36.31"><row><cell></cell><cell></cell><cell>Precision</cell><cell>Recall</cell><cell>No. Qrs</cell><cell>No. Dlds</cell><cell>1 st Detection</cell></row><row><cell>w=20, k=5</cell><cell>35</cell><cell>0.67</cell><cell>0.44</cell><cell>42.9</cell><cell>7.85</cell><cell>12.55</cell></row><row><cell>w=50, k=5</cell><cell>15</cell><cell>0.72</cell><cell>0.44</cell><cell>36.33</cell><cell>7.4</cell><cell>9.775</cell></row><row><cell>w=80, k=30</cell><cell>11</cell><cell>0.69</cell><cell>0.43</cell><cell>32.15</cell><cell>7.45</cell><cell>11.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,126.74,368.31,341.88,85.41"><head>Table 2 ,</head><label>2</label><figDesc>Choice of sentence chunk size (CS) and the frequency threshold (FT) at w=50, k=5, Snippet Score of 50%, Query score of 60%. First Detection is in queries. All numbers are averages over the dataset</figDesc><table coords="6,161.54,368.31,266.74,65.49"><row><cell></cell><cell>Precision</cell><cell>Recall</cell><cell>No. Qrs</cell><cell>No. Dlds</cell><cell>1 st Detection</cell></row><row><cell>CS=1, FT=1</cell><cell>0.45</cell><cell>0.5</cell><cell>53</cell><cell>13</cell><cell>12</cell></row><row><cell>CS=3, FT=1</cell><cell>0.65</cell><cell>0.46</cell><cell>37.85</cell><cell>8.8</cell><cell>9.7</cell></row><row><cell>CS=4, FT=1</cell><cell>0.72</cell><cell>0.44</cell><cell>36.33</cell><cell>7.4</cell><cell>9.775</cell></row><row><cell>CS=5, FT=1</cell><cell>0.71</cell><cell>0.41</cell><cell>28.6</cell><cell>6.7</cell><cell>8.7</cell></row><row><cell>CS=4, FT=3</cell><cell>0.73</cell><cell>0.4</cell><cell>30.45</cell><cell>7.0</cell><cell>8.95</cell></row><row><cell>CS=4, FT=5</cell><cell>0.63</cell><cell>0.37</cell><cell>27.25</cell><cell>6.85</cell><cell>11.35</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,140.18,580.22,326.38,84.98"><head>Table 3 ,</head><label>3</label><figDesc>Choice of Snippet and Query scores at w=50, and k=5, chunk size of 4, Frequency Threshold of 1. First Detection is in queries. All numbers are averages over the dataset</figDesc><table coords="6,155.54,580.22,284.46,65.06"><row><cell>Snippet</cell><cell>Query</cell><cell>Precision</cell><cell>Recall</cell><cell>No. Qrs</cell><cell>No. Dlds</cell><cell>1 st Detection</cell></row><row><cell>Score</cell><cell>Score</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>40</cell><cell>60</cell><cell>0.71</cell><cell>0.45</cell><cell>35.675</cell><cell>7.5</cell><cell>9.55</cell></row><row><cell>50</cell><cell>60</cell><cell>0.72</cell><cell>0.44</cell><cell>36.33</cell><cell>7.4</cell><cell>9.775</cell></row><row><cell>60</cell><cell>60</cell><cell>0.72</cell><cell>0.42</cell><cell>37.65</cell><cell>7.05</cell><cell>10.425</cell></row><row><cell>50</cell><cell>40</cell><cell>0.75</cell><cell>0.4</cell><cell>29.65</cell><cell>6.175</cell><cell>9.625</cell></row><row><cell>50</cell><cell>70</cell><cell>0.72</cell><cell>0.45</cell><cell>39.5</cell><cell>7.525</cell><cell>9.75</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,126.26,444.62,342.90,16.55"><head>Table 4a (</head><label>4a</label><figDesc></figDesc><table /><note coords="7,159.05,444.62,310.11,7.31;7,135.38,453.86,49.37,7.31"><p><p>top), Performance comparison with the top three participants.</p>Table 4b (bottom), Performance of all participants.</p></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>The presented system was evaluated using the four measures described in section 2.2 in addition to the "No Detection" metric. Our system was determined to be one of the top three submissions to PAN'13 in the source retrieval task. Among the top three participants (shown in Table <ref type="table" coords="7,245.32,211.76,7.73,9.05">4a</ref>), our system has the highest precision, the lightest workload and the fastest runtime. Our system is also the fastest system to download the first source. Overall, our system has the highest number of top performance indicators. In addition, our system has the second-highest recall and F1 score. Table <ref type="table" coords="7,178.58,257.87,10.02,9.05">4b</ref> provides a comparison between the performance of our system and the other participants of PAN13. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="8,128.58,300.35,3.77,9.05;8,156.86,300.35,280.61,9.05" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Originalitycheck</surname></persName>
		</author>
		<ptr target="http://turnitin.com/en_us/products/originalitycheck" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,128.58,343.43,3.77,9.05;8,156.86,343.43,128.69,9.05" xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Pan</surname></persName>
		</author>
		<ptr target="http://pan.webis.de/" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,128.58,364.91,3.77,9.05;8,156.86,364.91,281.56,9.05;8,156.86,376.31,311.35,9.05" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,304.41,364.91,134.01,9.05;8,156.86,376.31,147.28,9.05">Overview of the 4th International Competition on Plagiarism Detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,310.80,376.31,91.67,9.05">Pamela Forner, Jussi …</title>
		<imprint>
			<biblScope unit="page" from="17" to="20" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,128.58,397.91,3.77,9.05;8,156.86,397.91,297.95,9.05;8,156.86,409.31,299.20,9.05;8,156.86,420.85,240.20,9.05" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,347.43,397.91,107.38,9.05;8,156.86,409.31,181.52,9.05">Crowdsourcing Interaction Logs to Understand Text Reuse from the Web</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Völske</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,346.27,409.31,109.79,9.05;8,156.86,420.85,208.57,9.05">51st Annual Meeting of the Association of Computational Linguistics (ACL 13)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,128.58,442.45,3.77,9.05;8,156.86,442.45,227.28,9.05" xml:id="b4">
	<monogr>
		<ptr target="http://lemurproject.org/clueweb09/" />
		<title level="m" coord="8,156.86,442.45,78.55,9.05">ClueWeb09 Dataset</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,128.58,463.93,3.77,9.05;8,156.86,463.93,308.70,9.05;8,156.86,475.33,285.91,9.05;8,156.86,486.85,309.21,9.05;8,156.86,498.37,295.77,9.05;8,156.86,509.89,139.65,9.05" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,206.25,475.33,218.49,9.05">ChatNoir: A Search Engine for the ClueWeb09 Corpus</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Graßegger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tippmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Welsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,415.13,486.85,50.95,9.05;8,156.86,498.37,295.77,9.05;8,156.86,509.89,43.79,9.05">International ACM Conference on Research and Development in Information Retrieval (SIGIR 12)</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Hersh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Callan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Maarek</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">1004</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,128.58,531.37,3.77,9.05;8,156.86,531.37,166.10,9.05" xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Indri</surname></persName>
		</author>
		<ptr target="http://www.lemurproject.org/indri/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,128.58,552.85,3.77,9.05;8,156.86,552.85,282.43,9.05;8,156.86,564.37,219.79,9.05" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,204.56,552.85,234.73,9.05;8,156.86,564.37,33.24,9.05">TextTiling: Segmenting text into multi-paragraph subtopic passages</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,196.79,564.37,101.76,9.05">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="33" to="64" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,133.20,585.88,4.19,9.05;8,156.86,585.88,169.14,9.05" xml:id="b8">
	<monogr>
		<ptr target="http://nltk.org/" />
		<title level="m" coord="8,156.86,585.88,102.23,9.05">Natural Language Toolkit</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,133.20,607.36,4.19,9.05;8,156.86,607.36,296.59,9.05;8,156.86,618.88,298.38,9.05" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,270.50,607.36,182.95,9.05;8,156.86,618.88,120.78,9.05">KP-Miner: A keyphrase extraction system for English and Arabic documents</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">R</forename><surname>El-Beltagy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rafea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,284.75,618.88,81.45,9.05">Information Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="132" to="144" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,133.20,640.36,4.19,9.05;8,156.86,640.36,179.25,9.05;8,156.86,651.88,213.85,9.05" xml:id="b10">
	<monogr>
		<ptr target="http://nltk.org/_modules/nltk/tokenize/texttiling.html" />
		<title level="m" coord="8,156.86,640.36,174.67,9.05">NLTK&apos;s TextTiling Module Documentation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,133.20,673.36,4.19,9.05;8,156.86,673.36,280.73,9.05;8,156.86,684.88,298.54,9.05" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="8,278.50,673.36,154.55,9.05">Plagiarism Candidate Retrieval System</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Haggag</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">R</forename><surname>El-Beltagy</surname></persName>
		</author>
		<ptr target="https://github.com/osama-haggag/Plagiarism-Source-Retrieval-for-PAN13" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
