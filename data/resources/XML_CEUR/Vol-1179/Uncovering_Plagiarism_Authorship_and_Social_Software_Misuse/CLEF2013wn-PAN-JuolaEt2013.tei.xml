<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,147.56,148.84,311.48,16.02;1,253.16,166.30,88.99,16.02">Overview of the Author Identification Task at PAN 2013</title>
				<funder ref="#_hU42BMe">
					<orgName type="full">FP7 Marie Curie</orgName>
				</funder>
				<funder ref="#_DUDN7K6">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,221.48,207.46,51.97,9.11"><forename type="first">Patrick</forename><surname>Juola</surname></persName>
							<email>juola@mathcs.duq.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Duquesne University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,295.87,207.46,86.31,9.11"><forename type="first">Efstathios</forename><surname>Stamatatos</surname></persName>
							<email>stamatatos@aegean.gr</email>
							<affiliation key="aff1">
								<orgName type="institution">University of the Aegean</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,147.56,148.84,311.48,16.02;1,253.16,166.30,88.99,16.02">Overview of the Author Identification Task at PAN 2013</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AEE28F17CF07A50811448F492D8D3B18</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The author identification task at PAN-2013 focuses on author verification where given a set of documents by a single author and a questioned document, the problem is to determine if the questioned document was written by that particular author or not. In this paper we present the evaluation setup, the performance measures, the new corpus we built for this task covering three languages and the evaluation results of the 18 participant teams that submitted their software. Moreover, we survey the characteristics of the submitted approaches and show that a very effective meta-model can be formed based on the output of the participant methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.38" lry="841.98"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.38" lry="841.98"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.38" lry="841.98"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.38" lry="841.98"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.38" lry="841.98"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.38" lry="841.98"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.38" lry="841.98"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.38" lry="841.98"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.38" lry="841.98"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.38" lry="841.98"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.38" lry="841.98"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.38" lry="841.98"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.38" lry="841.98"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.38" lry="841.98"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.38" lry="841.98"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.38" lry="841.98"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.38" lry="841.98"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.38" lry="841.98"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.38" lry="841.98"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.38" lry="841.98"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Authorship attribution is an important problem in many areas including information retrieval and computational linguistics, but also in applied areas such as law and journalism where knowing the author of a document (such as a ransom note) may be crucial to save lives. The most common framework for testing candidate algorithms is a closed-set text classification problem: given known sample documents from a small, finite set of candidate authors, which if any wrote a questioned document of unknown authorship? <ref type="bibr" coords="1,178.10,522.66,15.83,9.11" target="#b15">[16,</ref><ref type="bibr" coords="1,200.59,522.66,13.37,9.11" target="#b32">33]</ref> It has been commented, however, that this may be an unreasonably easy task <ref type="bibr" coords="1,220.46,534.18,15.35,9.11" target="#b21">[22]</ref>. A more demanding problem is author verification where given a set of documents by a single author and a questioned document, the problem is to determine if the questioned document was written by that particular author or not <ref type="bibr" coords="1,124.76,568.68,15.35,9.11" target="#b23">[24]</ref>. This may more accurately reflect real life in the experiences of professional forensic linguists, who are often called upon to answer this kind of question. Interestingly, every author identification problem with multiple candidate authors can be transformed to a set of author verification problems.</p><p>The author identification task at PAN 2013 introduced several new aspects this year. The problem was framed differently this year, using the idea of the "fundamental problem of authorship attribution" as framed by Koppel et al. <ref type="bibr" coords="1,443.38,637.62,15.35,9.11" target="#b22">[23]</ref>, a reframing that supported a new software submission paradigm. The corpus incorporated a substantial multilingual element, including both resource-rich (English, Spanish) and resource-poor (Greek) natural languages. Despite this new framework, participation remained robust, with 18 participants and 16 notebook submissions, as detailed in the following sections. This year represents a departure from this precedent, as we focus on authorship verification, or what Koppel et al. <ref type="bibr" coords="3,273.50,161.14,16.72,9.11" target="#b22">[23]</ref> have called the "fundamental problem" in authorship attribution: Given two documents, are they by the same author? There is an elegance about this formulation, but it also represents possibly the most difficult formulation of the problem as it contains the minimum extra information upon which an analysis can rely. Discussions of this issue at and after the Authorship Attribution Workshop at Brooklyn Law School (October, 2012) suggested that this framing may be too difficult to solve at present technologies, especially at with realistic amounts of training data. For this reason, we focused on a variant of the fundamental problem: Given a set of documents (no more than 10, possibly only one) by the same author, is an additional (out-of-set) document also by that author?</p><p>This framework has several advantages, most notably that evaluation is relatively straightforward as each "problem" has a simple yes/no answer and that each problem can be represented relatively simply in a computational framework. This made it easier to incorporate the second major innovation of this iteration of the evaluation, the use of software-only submissions. In contrast to previous years, participants were asked to submit executable programs conforming to a simple command-line interface and output in a specific format that can be automatically evaluated. (Readers familiar with the ACM International Collegiate Programming Contest will be familiar with this paradigm). Submitted programs were run and evaluated in the TIRA<ref type="foot" coords="3,429.28,367.92,3.00,5.45" target="#foot_0">1</ref> platform <ref type="bibr" coords="3,124.76,379.66,15.35,9.11" target="#b9">[10]</ref>. Among other advantages, this enables us to "keep the contest open"; if someone has a brilliant idea in 2015, we hope they will be able to use the identical setup to submit and test that idea, hopefully outperforming 2013's winner.</p><p>Beyond the binary yes/no answers, it was also possible to leave some problems unanswered. In addition, the participants could optionally produce a confidence score, namely a real number in the set [0,1] inclusive where 1.0 means that it is absolutely sure that the questioned document was written by the examined author and 0 means the opposite.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation Corpus</head><p>The corpus we built for the author identification task of PAN-2013 covers three languages: English, Greek, and Spanish. For each language there is a set of problems, where one problem comprises a set of documents of known authorship by the same author and exactly one document of questioned authorship. All the documents within a problem are in the same language and placed in a separate folder. The language information was encoded in the problem label (i.e., folder name) so that it is possible to apply appropriate models per language without the need of language identification techniques.</p><p>The training corpus comprised 10 problems in English, 20 problems in Greek and 5 problems in Spanish. On the other hand, the evaluation corpus was balanced over the three languages comprising 30 problems in English, 30 problems in Greek and 25 problems in Spanish. A part of the latter was used in the early-bird evaluation stage, that is 20 problems in English, 20 problems in Greek and 15 problems in Spanish. In all cases, the distribution of positive and negative problems in each corpus (and every language-specific sub-corpus) was balanced.</p><p>The English part of the corpus (collected by Patrick Brennan of Juola &amp; Associates) consists of extracts from published textbooks on computer science and related disciplines, culled from an on-line repository. This particular genre was chosen in part because it represents a relatively controlled universe of discourse and  also a relatively unstudied genre compared with more commonly analyzed genres such as fiction or news reportage. A pool of 16 authors was selected and their works were collected. Each test and training document was around 1,000 words each and collected by hand from the larger works. Formulas and computer code was removed. Beyond the overall genre of "textbooks regarding IT or computer science", some of the paired documents are members of a very narrow genre (e.g. textbooks regarding Java programming) while others are more divergent (e.g. Cyber Crime vs. Digital Systems Design); the intention was to make the task more difficult and to curb a simple reading of the documents for content in order to guess authorship.</p><p>The Greek part of the corpus comprises newspaper articles published in the Greek weekly newspaper TO BHMA<ref type="foot" coords="5,248.48,264.42,3.00,5.45" target="#foot_1">2</ref> from 1996 to 2012. Initially, a pool of more than 800 opinion articles by about 100 authors was downloaded. The length of each article is at least 1,000 words. All HTML tags, scripts etc. as well as the title/subtitles of the article and author names were removed semi-automatically. Based on this collection of documents, a set of author verification problems was formed. In each problem, we included texts that had strong thematic similarities indicated by the occurrence of certain keywords. In addition, to make the task more challenging, we applied a stylometric analysis based on a character 3-gram representation and the dissimilarity measure d 1 proposed in <ref type="bibr" coords="5,226.77,356.62,16.70,9.11" target="#b31">[32]</ref> to detect stylistically similar or dissimilar documents. Hence, in problems where the true answer is positive (the questioned document was written by the author of the known documents) the unknown document was selected to have relatively high dissimilarity from the other known documents. On the other hand, in problems where the true answer is negative the unknown document (by a certain author) was selected to have relatively low dissimilarity from the known documents (by another author). Therefore, beyond similarities in genre, theme, and date of writing, there also stylistic relationships in within-problem documents of the Greek sub-corpus. This makes the Greek part of the evaluation corpus more challenging especially for verification methods based on CNG and variants <ref type="bibr" coords="5,428.00,460.14,15.37,9.11" target="#b32">[33]</ref>.</p><p>The Spanish part of the corpus (collected in part by Sheila Queralt of Universitat Pompeu Fabra and by Angela Melendez of Duquesne University) consisted of excerpts from newspaper editorials and short fiction.</p><p>Figures <ref type="figure" coords="5,168.68,506.16,5.01,9.11" target="#fig_0">1</ref> and<ref type="figure" coords="5,193.22,506.16,5.01,9.11" target="#fig_1">2</ref> show the distribution of known authorship documents per language for the training and evaluation corpora, respectively. In the training corpus, the English and Spanish parts include problems with no more than 5 known documents. On the other hand the Greek part covers the range of 1-10 known documents in a balanced way. In the evaluation corpus, the English and Spanish parts include problems with no more 6 known documents. The majority of the problems comprise 4-5 known documents for English and 2-3 known documents for Spanish. The Greek part again covers the range 1-10 of known documents while the majority of problems include 2-5 known documents.</p><p>From another perspective, Figures <ref type="figure" coords="6,278.41,609.12,5.01,9.11" target="#fig_2">3</ref> and<ref type="figure" coords="6,304.19,609.12,5.01,9.11" target="#fig_3">4</ref> show the distribution of text-length (in terms of words) over the evaluation documents per language for the training and evaluation corpora, respectively. In both cases, the majority of the documents comprise 1,000-1,500 words while some longer documents are included in the Greek part and some shorter documents in the Spanish part. The distribution of English and Greek parts is similar in training and evaluation corpora. The Spanish part of the evaluation corpus includes longer texts in comparison to the training corpus.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Performance Measures</head><p>PAN-2013 participants were asked to provide a simple "yes/no" binary answer for each problem of the author identification task. In case their method was not confident enough for some problems, they could leave the problem unanswered. To evaluate the output of their software, we used the following measures:</p><formula xml:id="formula_0" coords="7,214.04,229.47,167.30,26.58">Recall = #correct_answers / #problems Precision = #correct_answers / #answers</formula><p>Note that in case a participant answers all the problems, these two measures are equal. The final ranking was computed by combining these measures via F 1 for the whole evaluation corpus comprising all three languages. That way, a method that can only deal with a certain language will be ranked very low.</p><p>In addition, to evaluate the participants that also submitted a score (a real number in the set [0,1] inclusive where 1 indicates a confident positive answer and 0 indicates a confident negative answer) we used Receiver-Operating Characteristic (ROC) curves and the area under the curve (AUC) as a single measure. ROC curves provide a more detailed picture over the ability of the author verification methods to assign appropriate scores to their answers <ref type="bibr" coords="7,279.02,367.96,10.61,9.11" target="#b5">[6]</ref>. For the calculation of ROC curves, any missing answers were assumed to be wrong answers. Again, those participants that can only handle documents of a certain language will produce low AUC scores.</p><p>Finally, since we locally run the software submissions, it is possible for first time to have some comparative results between author verification methods with respect to their runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation Results</head><p>In total, 18 teams submitted their author verification software. The final evaluation results and the ranking of the participants according to the overall F 1 score as well as their runtime are shown in Table <ref type="table" coords="7,271.91,513.78,3.76,9.11" target="#tab_0">1</ref>. Evaluation results by each one of the three examined languages can be seen in Tables <ref type="table" coords="7,296.60,525.24,7.51,9.11" target="#tab_1">2,</ref><ref type="table" coords="7,306.62,525.24,3.76,9.11">3</ref>, and 4.</p><p>Most of the submissions answered all the problems in the evaluation corpus. Hence, the recall and precision measures are equal. Only two participants <ref type="bibr" coords="7,433.80,548.28,36.70,9.11;7,124.76,559.74,11.73,9.11">(Bobicev [2]</ref> and Ghaeini <ref type="bibr" coords="7,190.22,559.74,11.27,9.11" target="#b8">[9]</ref>) used the "I don't know" option in some problems. Moreover, two participants (Veenman&amp;Li <ref type="bibr" coords="7,235.04,571.26,16.72,9.11" target="#b34">[35]</ref> and Sorin) provided answers only for the English part of the corpus.</p><p>The winning submission <ref type="bibr" coords="7,238.38,594.24,16.73,9.11" target="#b30">[31]</ref> is a modification of the recently proposed Impostors method <ref type="bibr" coords="7,158.10,605.76,15.36,9.11" target="#b19">[20]</ref>. It achieved remarkable performance on English and Greek parts of the corpus. On the other hand, its performance on the Spanish part was moderate. Veenman&amp;Li <ref type="bibr" coords="7,183.49,628.74,16.72,9.11" target="#b34">[35]</ref> submitted another very effective approach for English only. The submissions of Halvani et al., <ref type="bibr" coords="7,248.72,640.26,15.40,9.11" target="#b11">[12]</ref>, Layton et al. <ref type="bibr" coords="7,325.76,640.26,15.35,9.11" target="#b24">[25]</ref>, and Petmanson <ref type="bibr" coords="7,412.28,640.26,16.74,9.11" target="#b28">[29]</ref> were also noticeable. Beyond their good performance, the former two required very low runtime. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Submission F 1 Precision Recall</head><p>Seidman <ref type="bibr" coords="8,198.14,482.08,16.69,9.02" target="#b30">[31]</ref> 0.800 0.800 0.800 Veenman&amp;Li <ref type="bibr" coords="8,218.12,493.60,16.70,9.02" target="#b34">[35]</ref> 0.800 0.800 0.800 Layton et al. <ref type="bibr" coords="8,213.98,505.12,16.70,9.02" target="#b24">[25]</ref> 0.767 0.767 0.767 Moreau&amp;Vogel <ref type="bibr" coords="8,226.46,516.58,16.69,9.02" target="#b26">[27]</ref> 0.767 0.767 0.767 Jankowska et al. <ref type="bibr" coords="8,229.52,528.10,16.70,9.02" target="#b12">[13]</ref> 0.733 0.733 0.733 Vilariño et al. <ref type="bibr" coords="8,218.96,539.62,16.69,9.02" target="#b35">[36]</ref> 0.733 0.733 0.733 Halvani et al. <ref type="bibr" coords="8,217.34,551.08,16.70,9.02" target="#b11">[12]</ref> 0.700 0.700 0.700 Feng&amp;Hirst <ref type="bibr" coords="8,210.92,562.60,11.68,9.02" target="#b6">[7]</ref> 0.700 0.700 0.700 Ghaeini <ref type="bibr" coords="8,194.84,574.12,11.64,9.02" target="#b8">[9]</ref> 0.691 0.760 0.633 Petmanson <ref type="bibr" coords="8,207.08,585.58,16.58,9.02" target="#b28">[29]</ref> 0.667 0.667 0.667 Bobicev <ref type="bibr" coords="8,196.52,597.10,11.64,9.02" target="#b1">[2]</ref> 0.644 0.655 0.633 Sorin 0.633 0.633 0.633 van Dam <ref type="bibr" coords="8,199.52,620.08,11.73,9.02" target="#b4">[5]</ref> 0.600 0.600 0.600 Jayapal&amp;Goswami <ref type="bibr" coords="8,239.18,631.60,16.76,9.02" target="#b13">[14]</ref> 0.600 0.600 0.600 Kern <ref type="bibr" coords="8,183.14,643.10,16.73,9.04" target="#b18">[19]</ref> 0.533 0.533 0.533 BASELINE 0.500 0.500 0.500 Vartapetiance&amp;Gillam <ref type="bibr" coords="8,254.18,666.12,16.69,9.02" target="#b33">[34]</ref> 0.500 0.500 0.500 Ledesma et al. <ref type="bibr" coords="8,221.72,677.58,16.69,9.02" target="#b25">[26]</ref> 0.467 0.467 0.467 Grozea 0.400 0.400 0.400 Table <ref type="table" coords="9,209.90,163.16,3.76,8.95">3</ref>. Results on the Greek part of the evaluation corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Submission F 1 Precision Recall</head><p>Seidman <ref type="bibr" coords="9,199.76,203.84,16.69,9.02" target="#b30">[31]</ref> 0.833 0.833 0.833 Bobicev <ref type="bibr" coords="9,198.14,215.36,11.64,9.02" target="#b1">[2]</ref> 0.712 0.724 0.700 Vilariño et al. <ref type="bibr" coords="9,220.58,226.88,16.69,9.02" target="#b35">[36]</ref> 0.667 0.667 0.667 Ledesma et al. <ref type="bibr" coords="9,223.34,238.34,16.69,9.02" target="#b25">[26]</ref> 0.667 0.667 0.667 Halvani et al. <ref type="bibr" coords="9,218.96,249.86,16.70,9.02" target="#b11">[12]</ref> 0.633 0.633 0.633 Jayapal&amp;Goswami <ref type="bibr" coords="9,240.80,261.38,16.76,9.02" target="#b13">[14]</ref> 0.633 0.633 0.633 Grozea 0.600 0.600 0.600 Jankowska et al. <ref type="bibr" coords="9,231.14,284.36,16.70,9.02" target="#b12">[13]</ref> 0.600 0.600 0.600 Feng&amp;Hirst <ref type="bibr" coords="9,212.54,295.88,11.68,9.02" target="#b6">[7]</ref> 0.567 0.567 0.567 Petmanson <ref type="bibr" coords="9,208.70,307.34,16.58,9.02" target="#b28">[29]</ref> 0.567 0.567 0.567 Vartapetiance&amp;Gillam <ref type="bibr" coords="9,255.80,318.86,16.69,9.02" target="#b33">[34]</ref> 0.533 0.533 0.533 BASELINE 0.500 0.500 0.500 Kern <ref type="bibr" coords="9,184.76,341.84,16.69,9.02" target="#b18">[19]</ref> 0.500 0.500 0.500 Layton et al. <ref type="bibr" coords="9,215.60,353.36,16.70,9.02" target="#b24">[25]</ref> 0.500 0.500 0.500 van Dam <ref type="bibr" coords="9,201.14,364.82,11.73,9.02" target="#b4">[5]</ref> 0.467 0.467 0.467 Ghaeini <ref type="bibr" coords="9,196.46,376.32,11.67,9.04" target="#b8">[9]</ref> 0.461 0.545 0.400 Moreau&amp;Vogel <ref type="bibr" coords="9,228.08,387.88,16.69,9.02" target="#b26">[27]</ref> 0.433 0.433 0.433 Sorin ---Veenman&amp;Li <ref type="bibr" coords="9,219.74,410.86,16.70,9.02" target="#b34">[35]</ref> ---Table <ref type="table" coords="9,206.30,439.18,3.76,8.95">4</ref>. Results on the Spanish part of the evaluation corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Submission F 1 Precision Recall</head><p>Halvani et al. <ref type="bibr" coords="9,219.14,479.86,16.70,9.02" target="#b11">[12]</ref> 0.840 0.840 0.840 Petmanson <ref type="bibr" coords="9,208.88,491.38,16.58,9.02" target="#b28">[29]</ref> 0.800 0.800 0.800 Layton et al. <ref type="bibr" coords="9,215.78,502.84,16.70,9.02" target="#b24">[25]</ref> 0.760 0.760 0.760 van Dam <ref type="bibr" coords="9,201.32,514.36,11.73,9.02" target="#b4">[5]</ref> 0.760 0.760 0.760 Ledesma et al. <ref type="bibr" coords="9,223.52,525.88,16.69,9.02" target="#b25">[26]</ref> 0.720 0.720 0.720 Grozea 0.680 0.680 0.680 Feng&amp;Hirst <ref type="bibr" coords="9,212.72,548.86,11.68,9.02" target="#b6">[7]</ref> 0.680 0.680 0.680 Ghaeini <ref type="bibr" coords="9,196.64,560.32,11.64,9.02" target="#b8">[9]</ref> 0.667 0.696 0.640 Jankowska et al. <ref type="bibr" coords="9,231.32,571.84,16.70,9.02" target="#b12">[13]</ref> 0.640 0.640 0.640 Bobicev <ref type="bibr" coords="9,198.32,583.36,11.64,9.02" target="#b1">[2]</ref> 0.600 0.600 0.600 Moreau&amp;Vogel <ref type="bibr" coords="9,228.26,594.82,16.69,9.02" target="#b26">[27]</ref> 0.600 0.600 0.600 Seidman <ref type="bibr" coords="9,199.94,606.34,16.69,9.02" target="#b30">[31]</ref> 0.600 0.600 0.600 Vartapetiance&amp;Gillam <ref type="bibr" coords="9,255.98,617.86,16.69,9.02" target="#b33">[34]</ref> 0.600 0.600 0.600 Kern <ref type="bibr" coords="9,184.94,629.32,16.69,9.02" target="#b18">[19]</ref> 0.560 0.560 0.560 Vilariño et al. <ref type="bibr" coords="9,220.76,640.82,16.73,9.04" target="#b35">[36]</ref> 0.560 0.560 0.560 BASELINE 0.500 0.500 0.500 Jayapal&amp;Goswami <ref type="bibr" coords="9,240.98,663.84,16.76,9.02" target="#b13">[14]</ref> 0.480 0.480 0.480 Sorin ---Veenman&amp;Li <ref type="bibr" coords="9,219.92,686.88,16.72,9.02" target="#b34">[35]</ref> ---</p><p>Although the best performance on the English part of the corpus is lower than the best performances on the Greek and Spanish parts, the average performance on the Greek part is lower than the other two parts. Moreover, more submissions are below the baseline for the Greek part of the corpus than the other two parts. We may conclude therefore that the Greek part is more difficult in comparison to the English and the Spanish parts of the corpus.</p><p>Table <ref type="table" coords="10,163.70,162.20,3.76,8.95">5</ref>. Evaluation of real scores (AUC) for the whole corpus and per language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rank Submission</head><p>Overall English Greek Spanish    More than half of the participants (i.e., 10 out of 18) have also submitted real scores (i.e., in the set [0,1] inclusive) together with their binary answers. This allowed us to compute the ROC curves and the corresponding AUC values for those participants. The results of this evaluation procedure are shown in Table <ref type="table" coords="12,416.38,450.84,3.74,9.11">5</ref>.</p><p>The submission of Jankowska et al. <ref type="bibr" coords="12,290.42,462.30,16.72,9.11" target="#b12">[13]</ref> managed to be equally effective in all three languages. The approach of Seidman <ref type="bibr" coords="12,301.76,473.82,16.70,9.11" target="#b30">[31]</ref> was strong in the English and Greek parts but very weak in the Spanish part of the evaluation corpus. The method of Ghaeini <ref type="bibr" coords="12,159.32,496.80,11.74,9.11" target="#b8">[9]</ref> was quite remarkable for the Spanish part, strong for the English part but very weak for the Greek part. On the other hand, the submission of Layton et al. <ref type="bibr" coords="12,453.94,508.32,16.72,9.11" target="#b24">[25]</ref> produces very low AUC scores despite its very good performance using binary answers. A closer examination of the output of their submission indicated that most likely they assign absolute confidence scores in each problem (i.e., assigning 1.0 to a problem they are confident no matter if it is positive or negative) rather than indicating confident positive and confident negative answers as requested.</p><p>In more detail, the convex hull of the ROC curves of all the participants on the entire evaluation corpus is depicted in Figure <ref type="figure" coords="12,309.04,588.84,3.76,9.11" target="#fig_4">5</ref>. The best performing submissions that form part of the convex hull are also depicted. The corresponding curves per language can be seen in Figures <ref type="figure" coords="12,216.14,611.82,17.50,9.11" target="#fig_6">6, 7,</ref> and<ref type="figure" coords="12,253.07,611.82,3.76,9.11" target="#fig_7">8</ref>.</p><p>The approach of Jankowska et al. <ref type="bibr" coords="12,273.26,623.34,16.78,9.11" target="#b12">[13]</ref> seems to be more effective for low values of FPR while the approaches of Ghaeini <ref type="bibr" coords="12,289.52,634.80,10.61,9.11" target="#b8">[9]</ref>, Feng&amp;Hirst <ref type="bibr" coords="12,360.62,634.80,11.70,9.11" target="#b6">[7]</ref> and Bobicev <ref type="bibr" coords="12,433.84,634.80,11.74,9.11" target="#b1">[2]</ref> work better for high values of FPR. The submission of Seidman <ref type="bibr" coords="12,375.58,646.32,16.72,9.11" target="#b30">[31]</ref> seems to be more balanced at least for the English and Greek parts of the corpus. The Spanish part is dominated by the performance of Ghaeini <ref type="bibr" coords="12,294.74,669.32,10.56,9.11" target="#b8">[9]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Early-bird evaluation:</head><p>To help participants build their approaches in time we allowed them to submit early versions of their models to be tested using a part of the evaluation corpus. That way, they could identify bugs in their software and fix them and also have an idea of the effectiveness of their models based on real evaluation problems. In total, 8 teams used this option. Table <ref type="table" coords="13,332.36,442.08,5.01,9.11">6</ref> presents the results of the earlybird and final evaluation phases for these teams. Surprisingly, most of the teams participated in the early-bird evaluation phase performed worse in the final evaluation corpus. On the other hand, the submissions of Halvani et al. <ref type="bibr" coords="13,390.27,476.58,16.70,9.11" target="#b11">[12]</ref> and especially Petmanson <ref type="bibr" coords="13,171.20,488.10,16.60,9.11" target="#b28">[29]</ref> took full advantage of this procedure to improve their effectiveness.</p><p>Combining the submitted approaches: Having access to the output of all the submitted approaches, we attempted to combine them all into a meta-model. This was inspired by a similar idea applied to the PAN-2010 competition on Wikipedia vandalism detection <ref type="bibr" coords="13,207.59,545.58,15.37,9.11" target="#b29">[30]</ref>. Hence, we built a simple meta-classifier based on the binary output of the 18 submitted models. When the majority of the binary answers is Y/N then a positive/negative answer is produced. In ties, a "I don't know" answer is given. Moreover, a real score is generated corresponding to the ratio of the number of positive answers to the number of all the answers. The results of this simple metamodel can be seen in Table <ref type="table" coords="13,237.68,603.06,3.77,9.11" target="#tab_4">7</ref>. By comparing these results with those of the individual submissions, we conclude that the meta-model is in general more effective. It is beaten only by the approach of Seidman <ref type="bibr" coords="13,298.04,626.04,16.72,9.11" target="#b30">[31]</ref> for the Greek part of the corpus. As concerns the real confidence scores, again the meta-model is very effective improving the overall performance. However, it is beaten by the approaches of Jankowska et al. <ref type="bibr" coords="13,124.76,660.56,16.69,9.11" target="#b12">[13]</ref> and Ghaeini <ref type="bibr" coords="13,196.16,660.56,11.68,9.11" target="#b8">[9]</ref> in the English part and by Seidman <ref type="bibr" coords="13,356.24,660.56,16.80,9.11" target="#b30">[31]</ref> in the Greek part of the corpus. It is remarkable that in the Spanish part the meta-model managed to equal the excellent performance of Ghaeini <ref type="bibr" coords="13,263.12,683.60,10.65,9.11" target="#b8">[9]</ref>. In addition, Figure <ref type="figure" coords="13,358.76,683.60,5.01,9.11" target="#fig_8">9</ref> shows the ROC curves of Table <ref type="table" coords="13,190.70,154.04,3.77,8.95">6</ref>. Comparison of early-bird and final evaluation results (F 1 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Submission</head><p>Overall English Greek Spanish Evaluation </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Survey of the Submitted Approaches</head><p>Out of 18 participants, 16 submitted a notebook describing their approach. Here we try to review these approaches. In the following, we use the term training corpus to  refer to the collection of verification problems released before the evaluation phase and not the documents of known authorship within a verification problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text representation:</head><p>The features used by the participants include character, lexical, syntactic, and semantic features <ref type="bibr" coords="15,256.50,195.64,15.36,9.11" target="#b32">[33]</ref>. The most popular character features were letter frequencies <ref type="bibr" coords="15,175.01,207.16,10.90,9.11" target="#b6">[7,</ref><ref type="bibr" coords="15,190.03,207.16,11.87,9.11" target="#b11">12]</ref>, punctuation mark frequencies <ref type="bibr" coords="15,336.37,207.16,10.88,9.11" target="#b8">[9,</ref><ref type="bibr" coords="15,351.43,207.16,12.53,9.11" target="#b11">12,</ref><ref type="bibr" coords="15,368.08,207.16,12.53,9.11" target="#b25">26,</ref><ref type="bibr" coords="15,384.76,207.16,12.53,9.11" target="#b28">29,</ref><ref type="bibr" coords="15,401.43,207.16,11.92,9.11" target="#b35">36]</ref>, character ngrams <ref type="bibr" coords="15,152.88,218.68,10.90,9.11" target="#b4">[5,</ref><ref type="bibr" coords="15,167.41,218.68,12.53,9.11" target="#b11">12,</ref><ref type="bibr" coords="15,183.62,218.68,12.53,9.11" target="#b13">14,</ref><ref type="bibr" coords="15,199.81,218.68,12.53,9.11" target="#b24">25,</ref><ref type="bibr" coords="15,216.02,218.68,12.53,9.11" target="#b26">27,</ref><ref type="bibr" coords="15,232.22,218.68,11.82,9.11" target="#b30">31]</ref>, and common prefixes-suffices of words <ref type="bibr" coords="15,419.02,218.68,15.88,9.11" target="#b11">[12,</ref><ref type="bibr" coords="15,438.58,218.68,12.53,9.11" target="#b18">19,</ref><ref type="bibr" coords="15,454.77,218.68,11.87,9.11" target="#b35">36]</ref>. Submissions based on compression models are also utilize character sequence patterns <ref type="bibr" coords="15,160.31,241.66,10.84,9.11" target="#b1">[2,</ref><ref type="bibr" coords="15,175.01,241.66,11.89,9.11" target="#b34">35]</ref>. The most widely used lexical features were word frequencies <ref type="bibr" coords="15,454.81,241.66,15.85,9.11" target="#b25">[26,</ref><ref type="bibr" coords="15,124.76,253.18,11.91,9.11" target="#b30">31]</ref>, word n-grams <ref type="bibr" coords="15,202.01,253.18,15.97,9.11" target="#b11">[12,</ref><ref type="bibr" coords="15,220.64,253.18,11.91,9.11" target="#b25">26]</ref>, function words <ref type="bibr" coords="15,302.37,253.18,10.92,9.11" target="#b6">[7,</ref><ref type="bibr" coords="15,315.97,253.18,7.51,9.11" target="#b8">9,</ref><ref type="bibr" coords="15,326.20,253.18,12.56,9.11" target="#b11">12,</ref><ref type="bibr" coords="15,341.42,253.18,11.91,9.11" target="#b35">36]</ref>, function word n-grams <ref type="bibr" coords="15,454.77,253.18,15.89,9.11" target="#b11">[12,</ref><ref type="bibr" coords="15,124.76,264.64,11.91,9.11" target="#b33">34]</ref>, hapax legomena <ref type="bibr" coords="15,213.96,264.64,10.93,9.11" target="#b6">[7,</ref><ref type="bibr" coords="15,228.44,264.64,11.87,9.11" target="#b18">19]</ref>, morphological information (lemma, stem, case, mood, etc.) <ref type="bibr" coords="15,145.57,276.16,10.85,9.11" target="#b8">[9,</ref><ref type="bibr" coords="15,159.68,276.16,11.91,9.11" target="#b28">29]</ref>, word, sentence and paragraph length <ref type="bibr" coords="15,332.12,276.16,10.96,9.11" target="#b6">[7,</ref><ref type="bibr" coords="15,346.34,276.16,7.54,9.11" target="#b8">9,</ref><ref type="bibr" coords="15,357.14,276.16,11.88,9.11" target="#b25">26]</ref>, grammatical errors and slang words <ref type="bibr" coords="15,181.67,287.62,15.37,9.11" target="#b18">[19]</ref>. Some participants used NLP tools to extract more complex, syntactic and semantic features. POS n-grams are the most popular features of this category <ref type="bibr" coords="15,162.68,310.66,10.82,9.11" target="#b6">[7,</ref><ref type="bibr" coords="15,176.95,310.66,12.53,9.11" target="#b26">27,</ref><ref type="bibr" coords="15,192.90,310.66,12.54,9.11" target="#b28">29,</ref><ref type="bibr" coords="15,208.88,310.66,11.87,9.11" target="#b35">36]</ref>. The approach of Vilariño et al. <ref type="bibr" coords="15,360.20,310.66,16.74,9.11" target="#b35">[36]</ref> build graphs based on POS sequences and then extract sub-graph patterns. Feng&amp;Hirst <ref type="bibr" coords="15,387.85,322.12,11.72,9.11" target="#b6">[7]</ref> use POS entropy and more advanced coherence features as discourse-level authorship information by using an NLP tool able to extract entities from and resolve coreferences in English texts. To analyze Greek and Spanish texts, they first translate them to English. In general, the use of NLP tools considerably increases the computational cost <ref type="bibr" coords="15,429.69,368.14,10.94,9.11" target="#b6">[7,</ref><ref type="bibr" coords="15,443.14,368.14,12.53,9.11" target="#b26">27,</ref><ref type="bibr" coords="15,458.13,368.14,12.54,9.11" target="#b28">29,</ref><ref type="bibr" coords="15,124.76,379.66,11.91,9.11" target="#b35">36]</ref>. Some participants combine different types of features in their models <ref type="bibr" coords="15,428.54,379.66,11.08,9.11" target="#b8">[9,</ref><ref type="bibr" coords="15,442.65,379.66,12.53,9.11" target="#b11">12,</ref><ref type="bibr" coords="15,458.20,379.66,12.46,9.11" target="#b35">36,</ref><ref type="bibr" coords="15,124.76,391.14,12.53,9.11" target="#b26">27,</ref><ref type="bibr" coords="15,140.48,391.14,13.36,9.11" target="#b28">29]</ref> while others use a single type of features <ref type="bibr" coords="15,328.03,391.14,11.00,9.11" target="#b4">[5,</ref><ref type="bibr" coords="15,342.23,391.14,12.53,9.11" target="#b13">14,</ref><ref type="bibr" coords="15,357.88,391.14,12.58,9.11" target="#b24">25,</ref><ref type="bibr" coords="15,373.66,391.14,11.87,9.11" target="#b33">34]</ref>. Similarly, Seidman <ref type="bibr" coords="15,124.76,402.66,16.72,9.11" target="#b30">[31]</ref> selects the most appropriate feature type per language.</p><p>Classification models: The submitted approaches fall in two main categories: intrinsic and extrinsic verification models. Intrinsic models are only based on the set of documents of known authorship and the document of unknown authorship to make their decision. Examples of this category are the approaches of Layton et al. <ref type="bibr" coords="15,453.93,460.14,16.69,9.11" target="#b28">[29]</ref> Halvani et al. <ref type="bibr" coords="15,185.41,471.66,15.35,9.11" target="#b11">[12]</ref>, Jankowska et al. <ref type="bibr" coords="15,281.29,471.66,15.35,9.11" target="#b12">[13]</ref>, and Feng&amp;Hirst <ref type="bibr" coords="15,374.16,471.66,10.64,9.11" target="#b6">[7]</ref>. On the other hand, extrinsic models use external resources, that is additional documents by other authors taken from the training corpus or downloaded from the web. Usually extrinsic models attempt to transform the one-class classification problem to a binary or multi-class classification problem. The winning submission <ref type="bibr" coords="15,333.30,517.62,16.80,9.11" target="#b30">[31]</ref> follows this approach. The submission of Veenman&amp;Li <ref type="bibr" coords="15,246.37,529.14,16.69,9.11" target="#b34">[35]</ref> that is very effective on the English part of the corpus also collects documents of similar genre from the web and builds a two-class classifier. Vilariño et al. <ref type="bibr" coords="15,233.78,552.12,16.69,9.11" target="#b35">[36]</ref> build a multi-class classifier based on the training corpus and an additional class formed by the documents of known authorship per problem. Moreover, van Dam <ref type="bibr" coords="15,249.14,575.16,11.74,9.11" target="#b4">[5]</ref> uses information from the training corpus (i.e., the average distance between the test document and the unknown documents) to decide about a given problem. In addition, the training corpus for English was extended by using additional documents of other authors. In both intrinsic and extrinsic methods, ensemble classification models are very popular and effective <ref type="bibr" coords="15,382.02,621.12,11.03,9.11" target="#b8">[9,</ref><ref type="bibr" coords="15,396.55,621.12,12.55,9.11" target="#b11">12,</ref><ref type="bibr" coords="15,412.54,621.12,12.53,9.11" target="#b24">25,</ref><ref type="bibr" coords="15,428.56,621.12,11.87,9.11" target="#b30">31]</ref>. Other popular models are modifications of the CNG method <ref type="bibr" coords="15,352.75,632.64,11.01,9.11" target="#b4">[5,</ref><ref type="bibr" coords="15,367.45,632.64,12.53,9.11" target="#b12">13,</ref><ref type="bibr" coords="15,383.64,632.64,11.94,9.11" target="#b24">25]</ref>, variations of the unmasking method <ref type="bibr" coords="15,208.43,644.16,10.84,9.11" target="#b6">[7,</ref><ref type="bibr" coords="15,224.41,644.16,11.88,9.11" target="#b26">27]</ref>, and compression-based approaches <ref type="bibr" coords="15,396.94,644.16,10.81,9.11" target="#b1">[2,</ref><ref type="bibr" coords="15,412.88,644.16,11.84,9.11" target="#b34">35]</ref>. The vast majority of the participants follow the instance-based paradigm <ref type="bibr" coords="15,401.27,655.64,16.83,9.11" target="#b32">[33]</ref> where each document of known authorship is treated separately. In some cases the documents of known authorship are first concatenated and then split into fragments of equal size <ref type="bibr" coords="15,459.53,678.68,11.08,9.11" target="#b1">[2,</ref><ref type="bibr" coords="16,124.76,149.68,11.91,9.11" target="#b11">12]</ref>. Some methods require at least two documents of known authorship, hence in case there is only one such document, they split it into two parts <ref type="bibr" coords="16,378.82,161.14,16.00,9.11" target="#b12">[13,</ref><ref type="bibr" coords="16,398.55,161.14,11.92,9.11" target="#b28">29]</ref>. On the other hand, only the approach of van Dam <ref type="bibr" coords="16,274.70,172.66,11.74,9.11" target="#b4">[5]</ref> follows the profile-based paradigm where all known documents are treated cumulatively.</p><p>Parameter tuning: One basic question is how to optimize the parameter values required by every verification method. In addition, since the evaluation corpus comprises problems in three languages, language-dependent parameter settings should be defined. Some participants avoid this problem by using global parameter settings <ref type="bibr" coords="16,124.76,253.18,10.87,9.11" target="#b8">[9,</ref><ref type="bibr" coords="16,139.69,253.18,12.53,9.11" target="#b11">12,</ref><ref type="bibr" coords="16,156.31,253.18,12.53,9.11" target="#b13">14,</ref><ref type="bibr" coords="16,172.92,253.18,11.84,9.11" target="#b25">26]</ref>. However, the majority of the participants used the training corpus sometimes enhanced by external documents found in the web or from other collections to better estimate the appropriate parameter values per language <ref type="bibr" coords="16,438.80,276.16,15.84,9.11" target="#b12">[13,</ref><ref type="bibr" coords="16,458.07,276.16,12.53,9.11" target="#b28">29,</ref><ref type="bibr" coords="16,124.76,287.62,11.91,9.11" target="#b30">31]</ref>. On the other hand, Layton et al. <ref type="bibr" coords="16,276.79,287.62,16.69,9.11" target="#b24">[25]</ref> take advantage of this problem by building an ensemble model where each base classifier corresponds to a different configuration of the parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text normalization:</head><p>The majority of the approaches did not perform any kind of text preprocessing. They just used the original textual data as found in the set of known documents and the unknown document. Some participants performed simple transformations like the removal of diacritics <ref type="bibr" coords="16,318.37,368.14,10.95,9.11" target="#b4">[5,</ref><ref type="bibr" coords="16,333.59,368.14,11.93,9.11" target="#b11">12]</ref>, substitution of digits with a special symbol <ref type="bibr" coords="16,190.90,379.66,10.68,9.11" target="#b4">[5]</ref>, or conversion of the text to lowercase <ref type="bibr" coords="16,376.56,379.66,10.73,9.11" target="#b4">[5]</ref>. More importantly, several participants attempted to normalize the text-length of the documents. Halvani et al. <ref type="bibr" coords="16,148.93,402.66,16.69,9.11" target="#b11">[12]</ref> and Bobicev <ref type="bibr" coords="16,223.40,402.66,11.74,9.11" target="#b1">[2]</ref> first concatenate all known documents and then segment them into equal-size fragments. Jankowska et al. <ref type="bibr" coords="16,325.33,414.18,16.69,9.11" target="#b12">[13]</ref> reduces all documents within a problem to the same size to produce equal-size representation profiles. This process seems to be crucial especially for methods based on character representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion</head><p>The author identification task at PAN-2013 introduced a number of novelties. First, it required software submissions, therefore enabling reproduction of the results and comparison of runtimes. In addition, the submitted approaches can now easily be applied to any corpus of similar properties and thus it will be possible to be compared with future models. Second novelty is the task definition itself. The problem of having a few documents of known authorship and one document of questioned authorship can model any given author identification task (i.e., multi-class, closed-set, or openset cases). So, this is a fundamental problem in authorship attribution research <ref type="bibr" coords="16,451.48,582.96,15.35,9.11" target="#b22">[23]</ref>. Third, the corpus built in the framework of this task includes verification problems in three natural languages and genres. It tested the ability of the submitted approaches to handle resource-rich languages and resource-poor languages. In addition, the task indirectly posed the question how to appropriately tune a certain method for a given genre/language. The participation in this task was more than satisfactory. In total, 18 teams from 14 countries have submitted their software. We are aware that certain teams with mainly a linguistic background develop semi-automated approaches to author identification and therefore had difficulties to submit their methods to this fully-automated evaluation campaign. To enable their participation, we offered an alternative option to such teams so that they have access to the evaluation corpus after the deadline of software submissions and then submit their results to be ranked in a separate list. However, finally there was no such participation. We hope to attract more teams with linguistic background in future evaluation campaigns since our ultimate goal is to provide a common forum for all researchers working on author identification.</p><p>The vast majority of the participants answered all the problems of the evaluation corpus. Only two teams used the "I don't know" option. Given the nature of the author verification applications, it is crucial for verification models to only provide the answers they are quasi-certain about. Unfortunately, the performance measures we used in this task do not give enough weight to verification problems left unanswered. In future evaluation campaigns, the performance measures should be better selected towards this direction. For example, the c@1 measure <ref type="bibr" coords="17,362.24,299.14,16.79,9.11" target="#b27">[28]</ref> used in the question answering community could be useful. Moreover, the submission of real scores indicating the confidence of the provided answers should be mandatory since ROC curves offer a very detailed picture of the submitted models. Additionally, ROC curves are independent of the distribution of positive/negative problems in the evaluation corpus <ref type="bibr" coords="17,198.40,356.62,11.72,9.11" target="#b5">[6]</ref> and therefore the conclusions drawn from this analysis are more general.</p><p>The most successful submitted approaches follow the extrinsic verification paradigm where the one-class problem is transformed to a multi-class classification problem, one class formed by the documents of known authorship and the other classes formed by documents of other authors found in external resources <ref type="bibr" coords="17,434.96,414.18,16.09,9.11" target="#b30">[31,</ref><ref type="bibr" coords="17,454.78,414.18,11.87,9.11" target="#b34">35]</ref>. Moreover, methods based on complicated features extracted by specialized NLP tools do not seem to have any advantage over simpler methods based on character and lexical information. The latter require very low computational cost.</p><p>The meta-model combining the output of all the submissions proved to be very effective and in average better than any individual method. The combination of heterogeneous models has not attracted much attention so far in authorship attribution research and certainly needs to be examined thoroughly. To this end, it is crucial to increase the publicly-available implementations of certain author identification methods.</p><p>It is also important to consider what, if any, changes should be made to future similar evaluations. In our opinion, the same basic verification framework should be retained at least for the next few iterations of PAN/CLEF or similar conferences. This will enable researchers to concentrate their efforts on incremental improvements of the analysis technology itself instead of on meeting changes in the problem specifications. At the same time, in light of the importance of authorship attribution as a forensic problem <ref type="bibr" coords="17,205.25,598.14,10.91,9.11" target="#b3">[4,</ref><ref type="bibr" coords="17,219.71,598.14,13.38,9.11" target="#b10">11]</ref> as well as the emerging need for accuracy standards and "solid linguistic research" into "reliable markers of authorship" <ref type="bibr" coords="17,397.22,609.66,11.01,9.11" target="#b2">[3,</ref><ref type="bibr" coords="17,412.77,609.66,11.83,9.11" target="#b17">18]</ref>, it is also important to consider what type of problems to incorporate. Many real-world problems do not have substantial "external resources," whether because they are in less commonly studied languages, historical dialects, or simply unusual genres such as ransom notes. Put simply, what, if any, real-world applications of authorship attribution should be modeled, and how best should the modeling happen? How can PAN frame the problem in order to continue to attract a wide variety of participants, including not merely computational approaches, but also approaches that use human expertise and high level linguistic information, a feature largely absent from the high scoring participants in this round?</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,130.16,352.72,335.42,9.11;4,130.16,364.18,29.19,9.11;4,130.20,143.16,345.54,207.24"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Distribution of known documents over the problems of the training corpus.</figDesc><graphic coords="4,130.20,143.16,345.54,207.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,130.16,583.08,335.49,9.11;4,130.16,594.78,29.19,9.11;4,130.20,373.50,345.54,207.24"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Distribution of known documents over the problems of the evaluation corpus.</figDesc><graphic coords="4,130.20,373.50,345.54,207.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,183.08,355.06,226.92,9.11;6,128.76,145.50,345.54,207.24"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Text-length distribution of the training corpus.</figDesc><graphic coords="6,128.76,145.50,345.54,207.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,177.86,585.42,237.45,9.11;6,128.76,375.84,345.54,207.30"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Text-length distribution of the evaluation corpus.</figDesc><graphic coords="6,128.76,375.84,345.54,207.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="10,133.40,565.68,331.36,9.11;10,133.40,577.14,116.35,9.11;10,133.44,332.34,345.78,231.00"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: ROC curves of the best performing submissions on the evaluation corpus and their convex hull.</figDesc><graphic coords="10,133.44,332.34,345.78,231.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="11,135.14,393.60,333.41,9.11;11,135.14,405.06,175.23,9.11;11,135.12,171.60,345.78,219.66"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: ROC curves of the best performing submissions on the English part of the evaluation corpus and their convex hull.</figDesc><graphic coords="11,135.12,171.60,345.78,219.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="11,135.14,659.36,333.36,9.11;11,135.14,670.82,158.13,9.11;11,135.12,437.34,345.78,219.66"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: ROC curves of the best performing submissions on the Greek part of the evaluation corpus and their convex hull</figDesc><graphic coords="11,135.12,437.34,345.78,219.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="12,132.14,384.60,333.41,9.11;12,132.14,396.06,172.78,9.11;12,132.18,162.60,345.78,219.66"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: ROC curves of the best performing submissions on the Spanish part of the evaluation corpus and their convex hull</figDesc><graphic coords="12,132.18,162.60,345.78,219.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="14,135.26,518.70,323.31,9.11;14,196.64,530.22,200.54,9.11;14,130.86,285.36,345.78,231.06"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Comparison of the ROC curves of the meta-model and the convex hall of the participants on the entire evaluation corpus.</figDesc><graphic coords="14,130.86,285.36,345.78,231.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="13,117.36,151.68,362.28,238.14"><head></head><label></label><figDesc></figDesc><graphic coords="13,117.36,151.68,362.28,238.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,127.04,165.04,341.96,256.82"><head>Table 1 .</head><label>1</label><figDesc>Overall results, runtime, and ranking of submissions.</figDesc><table coords="8,127.04,188.00,341.96,233.86"><row><cell cols="2">Rank Submission</cell><cell>F 1</cell><cell>Precision</cell><cell>Recall</cell><cell>Runtime</cell></row><row><cell>1</cell><cell>Seidman [31]</cell><cell>0.753</cell><cell>0.753</cell><cell>0.753</cell></row><row><cell>2</cell><cell>Halvani et al. [12]</cell><cell>0.718</cell><cell>0.718</cell><cell>0.718</cell></row><row><cell>3</cell><cell>Layton et al. [25]</cell><cell>0.671</cell><cell>0.671</cell><cell>0.671</cell></row><row><cell>3</cell><cell>Petmanson [29]</cell><cell>0.671</cell><cell>0.671</cell><cell>0.671</cell></row><row><cell>5</cell><cell>Jankowska et al. [13]</cell><cell>0.659</cell><cell>0.659</cell><cell>0.659</cell></row><row><cell>5</cell><cell>Vilariño et al. [36]</cell><cell>0.659</cell><cell>0.659</cell><cell>0.659</cell></row><row><cell>7</cell><cell>Bobicev [2]</cell><cell>0.655</cell><cell>0.663</cell><cell>0.647</cell></row><row><cell>8</cell><cell>Feng&amp;Hirst [7]</cell><cell>0.647</cell><cell>0.647</cell><cell>0.647</cell></row><row><cell>9</cell><cell>Ledesma et al. [26]</cell><cell>0.612</cell><cell>0.612</cell><cell>0.612</cell></row><row><cell>10</cell><cell>Ghaeini [9]</cell><cell>0.606</cell><cell>0.671</cell><cell>0.553</cell></row><row><cell>11</cell><cell>van Dam [5]</cell><cell>0.600</cell><cell>0.600</cell><cell>0.600</cell></row><row><cell>11</cell><cell>Moreau&amp;Vogel [27]</cell><cell>0.600</cell><cell>0.600</cell><cell>0.600</cell></row><row><cell>13</cell><cell>Jayapal&amp;Goswami [14]</cell><cell>0.576</cell><cell>0.576</cell><cell>0.576</cell></row><row><cell>14</cell><cell>Grozea</cell><cell>0.553</cell><cell>0.553</cell><cell>0.553</cell></row><row><cell>15</cell><cell>Vartapetiance&amp;Gillam [34]</cell><cell>0.541</cell><cell>0.541</cell><cell>0.541</cell></row><row><cell>16</cell><cell>Kern [19]</cell><cell>0.529</cell><cell>0.529</cell><cell>0.529</cell></row><row><cell></cell><cell>BASELINE</cell><cell>0.500</cell><cell>0.500</cell><cell>0.500</cell></row><row><cell>17</cell><cell>Veenman&amp;Li [35]</cell><cell>0.417</cell><cell>0.800</cell><cell>0.282</cell></row><row><cell>18</cell><cell>Sorin</cell><cell>0.331</cell><cell>0.633</cell><cell>0.224</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,178.22,441.00,246.00,9.11"><head>Table 2 .</head><label>2</label><figDesc>Results on the English part of the evaluation corpus.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="14,137.60,164.14,318.47,95.82"><head>Table 7 .</head><label>7</label><figDesc>The performance of the meta-classifier combining the output of all the submissions.</figDesc><table coords="14,183.92,198.56,221.41,61.40"><row><cell></cell><cell>F1</cell><cell>Precision</cell><cell>Recall</cell><cell>AUC</cell></row><row><cell>Overall</cell><cell>0.814</cell><cell>0.829</cell><cell>0.800</cell><cell>0.841</cell></row><row><cell>English</cell><cell>0.867</cell><cell>0.867</cell><cell>0.867</cell><cell>0.821</cell></row><row><cell>Greek</cell><cell>0.690</cell><cell>0.714</cell><cell>0.667</cell><cell>0.756</cell></row><row><cell>Spanish</cell><cell>0.898</cell><cell>0.917</cell><cell>0.880</cell><cell>0.926</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,130.04,686.31,67.44,8.10"><p>http://tira.webis.de</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="5,130.04,686.22,77.95,8.18"><p>http://www.tovima.gr</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This work was partially supported by the <rs type="programName">WIQ-EI IRSES</rs> project (Grant No. <rs type="grantNumber">269180</rs>) within the <rs type="funder">FP7 Marie Curie</rs> action. This material is based on work supported by the <rs type="funder">National Science Foundation</rs> under grant no. <rs type="grantNumber">OCI-1032683</rs>. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the <rs type="funder">National Science Foundation</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_hU42BMe">
					<idno type="grant-number">269180</idno>
					<orgName type="program" subtype="full">WIQ-EI IRSES</orgName>
				</org>
				<org type="funding" xml:id="_DUDN7K6">
					<idno type="grant-number">OCI-1032683</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="18,146.06,349.24,324.56,9.11;18,146.06,360.76,324.43,9.11;18,146.06,372.21,214.21,9.12" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="18,280.52,349.24,190.10,9.11;18,146.06,360.76,163.30,9.11">Overview of the International Authorship Identification Competition at PAN-2011</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Argamon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Juola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,171.08,372.21,159.61,9.12">CLEF Notebook Papers/Labs/Workshop</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Petras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,146.06,383.74,324.54,9.11;18,146.06,395.27,101.70,9.12" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="18,200.60,383.74,270.00,9.11">Authorship Detection with PPM -Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Bobicev</surname></persName>
		</author>
		<editor>Forner et al.</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,146.06,406.73,304.71,9.12" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="18,200.81,406.74,146.03,9.11">Ethics, Best Practices, and Standards</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R</forename><surname>Butters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,364.42,406.73,56.74,9.12">Proc. IAFL&apos;11</title>
		<meeting>IAFL&apos;11</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,146.06,418.25,324.54,9.12;18,146.06,429.78,42.53,9.11" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="18,205.28,418.26,141.61,9.11">On Admissible Linguistic Evidence</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Coulthard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="18,353.96,418.25,71.23,9.12">J. Law and Policy</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="441" to="466" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,146.06,441.24,324.59,9.11;18,146.06,452.75,216.96,9.12" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="18,202.52,441.24,268.13,9.11;18,146.06,452.76,112.79,9.11">A Basic Character N-gram Approach to Authorship Verification -Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Van Dam</surname></persName>
		</author>
		<editor>Forner et al.</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,146.06,464.27,324.54,9.12;18,146.06,475.74,87.76,9.11" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="18,198.98,464.28,143.61,9.11">An Introduction to ROC Analysis</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Fawcett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="18,351.86,464.27,114.96,9.12">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="861" to="874" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,146.06,487.26,324.43,9.11;18,146.06,498.77,324.59,9.12;18,146.06,510.23,27.00,9.12" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="18,256.16,487.26,214.33,9.11;18,146.06,498.78,248.73,9.11">Authorship Verification with Entity Coherence and Other Rich Linguistic Features -Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hirst</surname></persName>
		</author>
		<editor>Forner et al.</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,146.06,521.75,324.52,9.12;18,146.06,533.27,166.13,9.12" xml:id="b7">
	<monogr>
		<title level="m" coord="18,329.48,521.75,141.10,9.12;18,146.06,533.27,136.76,9.12">CLEF 2013 Evaluation Labs and Workshop -Working Notes Papers</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Navigli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Tufis</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,146.06,544.74,324.59,9.11;18,146.06,556.25,216.95,9.12" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="18,206.90,544.74,263.75,9.11;18,146.06,556.26,112.79,9.11">Intrinsic Author Identification Using Modified Weighted KNN -Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Ghaeini</surname></persName>
		</author>
		<editor>Forner et al.</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,146.06,567.78,324.41,9.11;18,146.06,579.24,324.37,9.11;18,146.06,590.76,70.30,9.11" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="18,343.65,567.78,126.81,9.11;18,146.06,579.24,229.14,9.11">TIRA: Configuring, Executing, and Disseminating Information Retrieval Experiments</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Burrows</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hoppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,400.00,579.24,70.43,9.11;18,146.06,590.76,38.18,9.11">Proc. of TIR at DEXA&apos;12</title>
		<meeting>of TIR at DEXA&apos;12</meeting>
		<imprint>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,146.06,602.22,324.51,9.11;18,146.06,613.73,256.39,9.12" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="18,187.04,602.22,283.53,9.11;18,146.06,613.74,91.02,9.11">TXT 4N6: Method, Consistency, and Distinctiveness of the Analysis of SMS Text Messages</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Grant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="18,244.16,613.73,70.68,9.12">J. Law and Policy</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="2" to="467" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,146.06,625.26,324.54,9.11;18,146.06,636.71,324.59,9.12;18,146.06,648.23,27.00,9.12" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="18,348.20,625.26,122.40,9.11;18,146.06,636.72,246.34,9.11">Authorship Verification via k-Nearest Neighbor Estimation -Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Halvani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Steinebach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zimmermann</surname></persName>
		</author>
		<editor>Forner et al.</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,146.06,659.78,324.57,9.11;18,146.06,671.24,324.58,9.11;18,146.06,682.75,246.37,9.12" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="18,351.68,659.78,118.95,9.11;18,146.06,671.24,324.58,9.11;18,146.06,682.76,142.22,9.11">Proximity based One-class Classification with Common N-Gram Dissimilarity for Authorship Verification Task -Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jankowska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kešelj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Milios</surname></persName>
		</author>
		<editor>Forner et al.</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,146.06,149.68,324.60,9.11;19,146.06,161.13,311.66,9.12" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="19,276.68,149.68,193.98,9.11;19,146.06,161.14,207.47,9.11">Vector Space Model and Overlap Metric for Author Identification -Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jayapal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Goswami</surname></persName>
		</author>
		<editor>Forner et al.</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,146.06,172.65,303.96,9.12" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="19,182.73,172.66,175.08,9.11">Ad-hoc Authorship Attribution Competition</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Juola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,375.58,172.65,69.48,9.12">Proc. of ALLC&apos;04</title>
		<meeting>of ALLC&apos;04</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="19,146.06,184.17,324.63,9.12;19,146.06,195.64,22.55,9.11" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="19,187.19,184.18,92.77,9.11">Authorship Attribution</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Juola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="19,288.62,184.17,128.05,9.12">Foundations and Trends in IR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="234" to="334" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,146.06,207.16,324.53,9.11;19,146.06,218.67,75.04,9.12" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="19,186.59,207.16,267.03,9.11">An Overview of the Traditional Authorship Attribution Subtask</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Juola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,146.06,218.67,70.01,9.12">Proc. of CLEF&apos;12</title>
		<meeting>of CLEF&apos;12</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="19,146.06,230.14,324.42,9.11;19,146.06,241.65,97.21,9.12" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="19,183.84,230.14,282.35,9.11">A Critical Analysis of the Ceglia/Zuckerberg Email Authorship Study</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Juola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,156.92,241.65,56.75,9.12">Proc. IAFL&apos;13</title>
		<meeting>IAFL&apos;13</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,146.06,253.18,324.52,9.11;19,146.06,264.63,262.46,9.12" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="19,189.02,253.18,281.56,9.11;19,146.06,264.64,158.33,9.11">Grammar Checker Features for Author Identification and Author Profiling -Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kern</surname></persName>
		</author>
		<editor>Forner et al.</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,146.06,276.16,324.34,9.11;19,146.06,287.61,324.38,9.12;19,146.06,299.13,91.15,9.12" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="19,262.96,276.16,207.43,9.11;19,146.06,287.62,26.44,9.11">Determining if Two Documents are by the Same Author</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Winter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="19,184.70,287.61,285.74,9.12;19,146.06,299.13,44.71,9.12">Journal of the American Society for Information Science and Technology</title>
		<imprint/>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="19,146.06,310.66,324.59,9.11;19,146.06,322.11,324.49,9.12;19,146.06,333.63,121.96,9.12" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="19,312.28,310.66,158.36,9.11;19,146.06,322.12,43.07,9.11">Computational Methods in Authorship Attribution</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Argamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="19,199.10,322.11,271.45,9.12;19,146.06,333.63,44.71,9.12">Journal of the American Society for information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="26" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,146.06,345.16,324.40,9.11;19,146.06,356.61,217.79,9.12" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="19,319.64,345.16,146.31,9.11">Authorship Attribution in the Wild</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Argamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="19,146.06,356.61,145.77,9.12">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="83" to="94" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,146.06,368.14,324.36,9.11;19,146.06,379.65,262.51,9.12" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="19,353.93,368.14,116.48,9.11;19,146.06,379.66,101.38,9.11">The &quot;Fundamental Problem&quot; of Authorship Attribution</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Argamon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Winter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="19,253.88,379.65,60.50,9.12">English Studies</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="284" to="291" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,146.06,391.14,324.44,9.11;19,146.06,402.65,324.60,9.12;19,146.06,414.18,80.29,9.11" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="19,354.79,391.14,115.70,9.11;19,146.06,402.66,146.42,9.11">Measuring Differentiability: Unmasking Pseudonymous Authors</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Bonchek-Dokow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="19,301.58,402.65,164.66,9.12">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1261" to="1276" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,146.06,425.64,324.45,9.11;19,146.06,437.15,224.46,9.12" xml:id="b24">
	<monogr>
		<title level="m" type="main" coord="19,308.60,425.64,161.91,9.11;19,146.06,437.16,120.30,9.11">Local n-grams for Author Identification -Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Layton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Dazeley</surname></persName>
		</author>
		<editor>Forner et al.</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,146.06,448.68,324.53,9.11;19,146.06,460.14,305.52,9.11" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="19,382.72,448.68,87.87,9.11;19,146.06,460.14,201.98,9.11">Distance Learning for Author Verification -Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ledesma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Fuentes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Jasso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Toledo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Meza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="19,383.87,460.14,48.65,9.11">Forner et al</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,146.06,471.66,324.58,9.11;19,146.06,483.17,224.46,9.12" xml:id="b26">
	<monogr>
		<title level="m" type="main" coord="19,251.60,471.66,219.04,9.11;19,146.06,483.18,120.30,9.11">Style-based Distance Features for Author Verification -Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Vogel</surname></persName>
		</author>
		<editor>Forner et al.</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,146.06,494.63,324.56,9.12;19,146.06,506.15,324.54,9.12;19,146.06,517.62,95.86,9.11" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="19,251.71,494.64,168.30,9.11">A Simple Measure to Assess Nonresponse</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rodrigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,438.22,494.63,32.40,9.12;19,146.06,506.15,300.10,9.12">Proc. of the 49th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>of the 49th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1415" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,146.06,529.14,324.54,9.11;19,146.06,540.65,260.78,9.12" xml:id="b28">
	<monogr>
		<title level="m" type="main" coord="19,216.32,529.14,254.28,9.11;19,146.06,540.66,156.65,9.11">Authorship Identification Using Correlations of Frequent Features -Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Petmanson</surname></persName>
		</author>
		<editor>Forner et al.</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,146.06,552.12,262.74,9.11;19,408.94,550.07,4.32,5.89;19,419.56,552.12,51.08,9.11;19,146.06,563.64,324.45,9.11;19,146.06,575.15,292.53,9.12" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="19,324.81,552.12,83.99,9.11;19,408.94,550.07,4.32,5.89;19,419.56,552.12,51.08,9.11;19,146.06,563.64,207.02,9.11">Overview of the 1 st International Competition on Wikipedia Vandalism Detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Holfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,205.76,575.15,203.15,9.12">Notebook Papers of CLEF 10 Labs and Workshops</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,146.06,586.62,324.62,9.11;19,146.06,598.13,175.02,9.12" xml:id="b30">
	<monogr>
		<title level="m" type="main" coord="19,199.22,586.62,271.46,9.11;19,146.06,598.14,70.84,9.11">Authorship Verification Using the Impostors Method -Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Seidman</surname></persName>
		</author>
		<editor>Forner et al.</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,146.06,609.66,324.50,9.11;19,146.06,621.11,324.58,9.12;19,146.06,632.63,64.15,9.12" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="19,210.14,609.66,260.42,9.11;19,146.06,621.12,20.64,9.11">Author Identification Using Imbalanced and Limited Training Texts</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,187.52,621.11,283.12,9.12;19,146.06,632.63,35.25,9.12">Proc. of the 4th International Workshop on Text-based Information Retrieval</title>
		<meeting>of the 4th International Workshop on Text-based Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,146.06,644.15,324.53,9.12;19,146.06,655.63,324.63,9.12;19,146.06,667.16,22.55,9.11" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="19,206.64,644.16,214.66,9.11">A Survey of Modern Authorship Attribution Methods</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="19,428.92,644.15,41.67,9.12;19,146.06,655.63,264.86,9.12">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="538" to="556" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,146.06,149.68,324.43,9.11;20,146.06,161.14,324.51,9.11;20,146.06,172.65,36.72,9.12" xml:id="b33">
	<monogr>
		<title level="m" type="main" coord="20,283.46,149.68,187.03,9.11;20,146.06,161.14,258.08,9.11">A Textual Modus Operandi: Surrey&apos;s Simple System for Author Identification -Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vartapetiance</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gillam</surname></persName>
		</author>
		<editor>Forner et al.</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,146.06,184.18,324.59,9.11;20,146.06,195.63,216.96,9.12" xml:id="b34">
	<monogr>
		<title level="m" type="main" coord="20,251.18,184.18,219.47,9.11;20,146.06,195.64,112.79,9.11">Authorship Verification with Compression Features -Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Veenman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<editor>Forner et al.</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,146.06,207.16,324.59,9.11;20,146.06,218.68,324.56,9.11;20,146.06,230.13,101.70,9.12" xml:id="b35">
	<monogr>
		<title level="m" type="main" coord="20,383.08,207.16,87.57,9.11;20,146.06,218.68,324.56,9.11">Lexical-Syntactic and Graph-Based Features for Authorship Verification -Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Vilariño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>León</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Castillo</surname></persName>
		</author>
		<editor>Forner et al.</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
