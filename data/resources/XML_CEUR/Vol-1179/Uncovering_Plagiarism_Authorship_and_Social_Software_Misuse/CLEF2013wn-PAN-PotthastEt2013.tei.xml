<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,155.71,115.90,303.93,12.90;1,243.73,133.83,127.89,12.90">Overview of the 5th International Competition on Plagiarism Detection</title>
				<funder ref="#_YQAUtnw">
					<orgName type="full">FP7 Marie Curie</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,169.16,171.88,60.37,8.64"><forename type="first">Martin</forename><surname>Potthast</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Web Technology &amp; Information Systems</orgName>
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,240.33,171.88,61.11,8.64"><forename type="first">Matthias</forename><surname>Hagen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Web Technology &amp; Information Systems</orgName>
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,313.14,171.88,44.61,8.64"><forename type="first">Tim</forename><surname>Gollub</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Web Technology &amp; Information Systems</orgName>
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,369.02,171.88,67.92,8.64"><forename type="first">Martin</forename><surname>Tippmann</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Web Technology &amp; Information Systems</orgName>
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,158.37,183.83,62.97,8.64"><forename type="first">Johannes</forename><surname>Kiesel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Web Technology &amp; Information Systems</orgName>
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,232.29,183.83,47.41,8.64"><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Natural Language Engineering Lab</orgName>
								<orgName type="institution" key="instit1">ELiRF</orgName>
								<orgName type="institution" key="instit2">Universitat Politècnica de València</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,291.14,183.83,84.36,8.64"><forename type="first">Efstathios</forename><surname>Stamatatos</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Dept. of Information &amp; Communication Systems Engineering</orgName>
								<orgName type="institution">University of the Aegean</orgName>
								<address>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,403.53,183.83,48.99,8.64"><forename type="first">Benno</forename><surname>Stein</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Web Technology &amp; Information Systems</orgName>
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,155.71,115.90,303.93,12.90;1,243.73,133.83,127.89,12.90">Overview of the 5th International Competition on Plagiarism Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CFC968B88D0A78BA359EF49C53FBAF77</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper overviews 18 plagiarism detectors that have been evaluated within the fifth international competition on plagiarism detection at PAN 2013. We report on their performances for the two tasks source retrieval and text alignment of external plagiarism detection. Furthermore, we continue last year's initiative to invite software submissions instead of run submissions, and, re-evaluate this year's submissions on last year's evaluation corpora and vice versa, thus demonstrating the benefits of software submissions in terms of reproducibility.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="29" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="30" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="31" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text is reused in many ways, such as quotations, translations, paraphrases, summaries, and boilerplate text. Under the right circumstances, all of these kinds of text reuse may also be considered plagiarism <ref type="bibr" coords="1,260.90,436.23,15.27,8.64" target="#b24">[25]</ref>. A frequent research topic concerning text reuse and plagiarism is algorithms that detect them. Particularly the detection of plagiarism has received considerable attention in terms of publications over the past two decades. Our focus is on the evaluation of such algorithms with respect to their retrieval performance; since 2009 we have been organizing four annual competitions on plagiarism detection <ref type="bibr" coords="1,173.78,496.00,15.77,8.64" target="#b25">[26,</ref><ref type="bibr" coords="1,192.04,496.00,12.45,8.64" target="#b26">27,</ref><ref type="bibr" coords="1,206.99,496.00,12.45,8.64" target="#b28">29,</ref><ref type="bibr" coords="1,221.93,496.00,13.28,8.64" target="#b29">30]</ref> and this paper reports on the results of the fifth edition. <ref type="foot" coords="1,456.58,494.33,3.49,6.05" target="#foot_0">1</ref>During the first three editions of our lab we developed the first standardized evaluation framework for plagiarism detection <ref type="bibr" coords="1,302.11,519.91,15.27,8.64" target="#b27">[28]</ref>. A total of 32 teams of researchers took part in these evaluations, nine of whom more than once, and the framework has been adopted by the research community since. While evaluation frameworks should accurately emulate the real world around a given computational task in a controlled laboratory environment, most frameworks do so only to some extent, since they typically rest on design choices that affect their generalizability. This is also true for our framework, which has been shown to exert a number of shortcomings due to its semiautomatic construction that render it less realistic and sometimes lead to impractical algorithm design. As of last year, we started developments on a new, more realistic evaluation framework . Generic retrieval process to detect plagiarism <ref type="bibr" coords="2,399.14,256.35,13.74,7.77" target="#b39">[40]</ref>.</p><p>that consists of entirely manually generated text reuse and plagiarism <ref type="bibr" coords="2,418.47,299.28,15.27,8.64" target="#b31">[32]</ref>. This year, we employ the new framework for the second time to evaluate a total of 18 plagiarism detectors, revising it along the way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Plagiarism Detection and its Step-wise Evaluation</head><p>Figure <ref type="figure" coords="2,162.71,371.08,4.98,8.64" target="#fig_0">1</ref> shows a generic retrieval process to detect plagiarism in a given suspicious document d plg , when also given a (very large) document collection D of potential source documents. This process is also referred to as external plagiarism detection since plagiarism in d plg is detected by searching for text passages in D that are highly similar to text passages in d plg . <ref type="foot" coords="2,201.30,417.23,3.49,6.05" target="#foot_1">2</ref> The process is divided into three basic steps, which are typically implemented in most plagiarism detectors. First, source retrieval, which identifies a small set of candidate documents D src ⊆ D that are likely sources for plagiarism regarding d plg . Second, text alignment, where each candidate document d src ∈ D src is compared to d plg , extracting all passages of text that are highly similar. Third, knowledgebased post-processing, where the extracted passage pairs are cleaned, filtered, and possibly visualized for later presentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Contributions</head><p>Since last year, we evaluate plagiarism detectors step-wise instead of as a whole. Our focus is on the source retrieval task and the text alignment task, and we research and develop new evaluation frameworks for each of them. In the following two sections, we detail the evaluations of both tasks. Our contributions are as follows:</p><p>1. Software Submissions. For the second time, we asked participants to submit their software instead of outputs of software runs. The submitted softwares were run and evaluated at our site using the TIRA experimentation platform <ref type="bibr" coords="3,406.10,119.31,10.58,8.64" target="#b8">[9]</ref>. This improves the sustainability of our evaluations because submitted softwares are maintained in executable state so that they can be run against new corpora later on. 2. Text Alignment Evaluation Across Years. Since software submissions were introduced last year for the text alignment task, this puts us in the position to re-evaluate last year's submissions against this year's evaluation corpora and vice versa. We report on the results of this cross-year evaluation and present a combined ranking, demonstrating the benefits of software submissions in terms or reproducibility. 3. Survey of Retrieval Approaches. We survey the 18 submitted softwares as described by their authors, analyze how they work, and organize them into generic retrieval processes for both of the two tasks source retrieval and text alignment. 4. Performance Measures for Source Retrieval. Regarding the source retrieval task, we shed light onto measuring the performance of a source retrieval algorithm. In particular, we show that near-duplicate retrieval results should be discounted when measuring source retrieval performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Source Retrieval</head><p>In source retrieval, given a suspicious document and a web search engine, the task is to retrieve all source documents from which text has been reused whilst minimizing retrieval costs. The cost-effectiveness of plagiarism detectors in this task is important since using existing search engines is perhaps the only feasible way for researchers as well as small and medium-sized businesses to implement plagiarism detection against the web, whereas search companies charge considerable fees for automatic usage.</p><p>In what follows, we describe the building blocks of our evaluation setup, provide details about the evaluation corpus and how it was constructed, discuss performance measures, survey the submitted softwares, and finally, report on the evaluation of these softwares.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Evaluation Setup</head><p>For the evaluation of source retrieval from the web, we consider the real-world scenario of an author who uses a web search engine to retrieve documents in order to reuse text from them in a document. A plagiarism detector typically uses a search engine, too, to find reused sources of a given document. Therefore, to evaluate a plagiarism detector, an evaluator must collect realistic samples of documents that contain reused text and feed them into the detector while keeping the web environment under full control. To do so in a reproducible manner, the web environment must be representative, yet static, so that evaluations of different detectors yield comparable results when done asynchronously. Meeting both constraints at the same time poses a significant engineering challenge. Over the past years, we assembled the necessary building blocks to allow for a meaningful evaluation of source retrieval algorithms; Figure <ref type="figure" coords="3,395.86,604.56,4.98,8.64" target="#fig_2">2</ref> shows how they are connected:</p><p>-A large-scale web corpus (in our case, the ClueWeb09) that can be readily served and browsed as if it were the real web (i.e., links of delivered web pages are rewritten so that they point to the servers hosting the web corpus instead of the real web).  -Web search engines (Indri and ChatNoir) that index the web corpus using two different retrieval models and that offer both a user interface as well as an API for automatic usage. -An evaluation corpus (Webis-TRC-12) that consists of long, manually written documents from authors who searched for sources in the web corpus using one of the search engines and then reused text from sources retrieved. -An experimentation platform (TIRA) that allows participants to submit their plagiarism detector for evaluation and for conservation in an executable state for future evaluations. -A search proxy API that, for later analysis, monitors and logs a participant's use of the search engines as well as web page downloads from the corpus. -Performance measures that measure the retrieval quality and cost-effectiveness of a plagiarism detector. -A source oracle that provides feedback to a detector about whether or not a web page the detector decided to download is a true positive detection (i.e., a source document, or a duplicate thereof, that was actually used).</p><p>This setup has been used for both corpus construction as well as for evaluation runs, both of which are detailed in the following sections. Beforehand, we briefly describe the underlying infrastructure which must be kept operational for both purposes and maintained for future evaluations.</p><p>The TIRA experimentation platform The TIRA experimentation platform is developed alongside our evaluation lab at our site <ref type="bibr" coords="5,297.20,167.13,10.58,8.64" target="#b8">[9]</ref>. It facilitates information retrieval experiments by providing a unified execution environment that allows for local instantiation of experiment software, web dissemination of evaluation results, platform independent software development, and result retrieval and visualization. TIRA itself consists of a number of building blocks the full description of which is beyond the scope of this report. Suffice it to say that one of them, depicted in Figure <ref type="figure" coords="5,383.00,226.91,4.98,8.64" target="#fig_2">2</ref> bottom left, facilitates both platform independent software development and software submissions at the same time by its capability to create and remote control virtual machines on which our lab's participants deploy their plagiarism detectors.</p><p>The ClueWeb, Indri, and ChatNoir The currently most widely adopted web crawl that is regularly used for large-scale web search-related evaluations is the ClueWeb corpus 2009 (ClueWeb09). <ref type="foot" coords="5,230.93,308.93,3.49,6.05" target="#foot_2">3</ref> The corpus consists of about one billion web pages, half of which are English ones. It has been successfully employed to evaluate retrieval models and search engines within the annual TREC evaluation conference <ref type="foot" coords="5,403.82,332.84,3.49,6.05" target="#foot_3">4</ref> and has therefore been widely adopted. As of this year, an updated version of the corpus has been released, <ref type="foot" coords="5,162.15,356.75,3.49,6.05" target="#foot_4">5</ref> however, our evaluation is still based on the 2009 version. Indri <ref type="foot" coords="5,169.08,368.70,3.49,6.05" target="#foot_5">6</ref> and ChatNoir <ref type="bibr" coords="5,234.72,370.37,16.60,8.64" target="#b30">[31]</ref> are currently the only publicly available search engines that index the ClueWeb09 corpus. Indri's retrieval model combines language modeling and inference networks <ref type="bibr" coords="5,251.44,394.28,15.27,8.64" target="#b19">[20]</ref>, whereas ChatNoir implements the classic BM25F model <ref type="bibr" coords="5,163.30,406.24,16.60,8.64" target="#b32">[33]</ref> and incorporates PageRank and SpamRank scores. This way, two of the most widespread retrieval models are available within our evaluation setup. For developer convenience, we also provide a proxy server which unifies the APIs of the search engines. At the same time, the proxy server logs all accesses to the search engines for later performance analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Evaluation Corpus</head><p>The evaluation corpus employed for source retrieval is based on the Webis text reuse corpus 2012 (Webis-TRC-2012) <ref type="bibr" coords="5,266.55,515.83,15.27,8.64" target="#b31">[32]</ref>. The corpus consists of 297 documents that have been written by 27 writers who worked with our setup as shown in the first row of Figure <ref type="figure" coords="5,164.21,539.74,3.88,8.64" target="#fig_2">2</ref>: given a topic, a writer used ChatNoir to search for source material on that topic while preparing a document of 5700 words length on average, reusing text from the found sources. The writers were instructed to modify their text reuse as much as they deemed necessary to avoid automatic detection. To do so, writers applied two general strategies, namely paraphrasing of a reused passage and interleaving of two or more reused passages from different source documents. Some writers made only superficial modifications while others made many, so that a spectrum of cases ranging from no paraphrasing and no interleaving to much paraphrasing and much interleaving can be observed.</p><p>Last year, we sampled 40 documents from the Webis-TRC-2012 as training and test documents. This year, these documents were provided for training, and another 58 documents were sampled as test documents. The remainder of the corpus will be used within future labs on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Performance Measures</head><p>Given a suspicious document d plg that contains passages of text that have been reused from a set of source documents D src , we measure the retrieval performance of a source retrieval algorithm in retrieving D src in terms of precision and recall. Let D ret denote the set of documents that are retrieved by a source retrieval algorithm when given d plg , then it may seem straightforward to define precision as prec = |D ret ∩ D src |/|D ret | and recall as r ec = |D ret ∩ D src |/|D src |. However, this definition turns out to be overly simplistic since it disregards an important side-effect of using a large-scale web corpus for evaluation, namely near-duplicate web documents.</p><p>The web is rife with documents that are near-duplicates of each other <ref type="bibr" coords="6,428.24,336.50,10.58,8.64" target="#b3">[4]</ref>. A source retrieval algorithm may therefore retrieve a document d ret ∈ D ret that is almost the same as a source document d src ∈ D src in terms of its main contents, but not exactly the same. Because of this, d ret would be counted falsely as a negative detection despite the fact that a human assessor would consider it a true positive one. To relax this constraint, we employ a near-duplicate detector to judge whether a retrieved document d ret is a true positive detection; i.e., whether a document d src exists in d plg 's set of source documents D src that is a near-duplicate of d ret . If one of the following conditions holds, we say that d ret is a true positive detection for a given pair of d src and d plg : This way of determining whether a retrieved document d ret is a true positive detection inherently entails inaccuracies, so that not all near-duplicates of a source document d src will be considered true positive detections. While there is no straightforward way to solve this problem, this error source affect all detectors at the same time, still allowing for relative comparisons. Let d dup denote a near-duplicate of a given d src that would be considered a true positive detection according to the above conditions if it was retrieved by a source retrieval algorithm. Note that every d src may have more than one near-duplicate and every d dup may be a near-duplicate of more than one source document. Further, let D dup denote the set of all near-duplicates of a given set of source documents D src of d plg and let D ret denote the subset of D src that have at least one corresponding true positive detection in D ret : Based on these sets, we define precision and recall of D ret regarding D src and d plg as follows:</p><formula xml:id="formula_0" coords="7,136.71,176.78,154.98,9.65">D dup = {d dup | ∃d src ∈ D src : d dup is</formula><formula xml:id="formula_1" coords="7,192.30,234.18,230.76,24.18">precision = |D ret ∩ D dup | |D ret | , recall = |D ret ∩ D src | |D ret | .</formula><p>Rationale for this definition is the fact that, retrieving more than one near-duplicate of a source document does not decrease precision, but it does not increase recall, either, since no additional information is obtained. Finally, to measure the cost-effectiveness of a source retrieval algorithm in retrieving D ret , we count the numbers of queries and downloads made and compute the workload in terms of queries and downloads until the first true positive detection is made.</p><p>The Source Oracle As described at the outset, the task subsequent to source retrieval is text alignment, where the candidate sources found are compared in closer detail with a given suspicious document (see Figure <ref type="figure" coords="7,292.88,371.89,3.60,8.64" target="#fig_0">1</ref>). Because of this connection, one problem of implementing and evaluating a source retrieval algorithm is that a working text alignment algorithm is required a priori. Since such an algorithm is not at hand, we decouple the source retrieval task from the text alignment task by means of a source oracle so as to allow for participation without the need to develop a text alignment algorithm. The oracle automatically enriches a downloaded document with information about whether or not it is considered a true positive source for the given suspicious document. Note that the oracle employs the aforementioned conditions to determine whether a document is a true positive detection. However, the oracle does not, yet, tell for which part of a suspicious document a downloaded document is a true positive detection detection. Hence, applying a custom text alignment strategy can still be beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Survey of Retrieval Approaches</head><p>Nine of the 14 participants submitted runs for the source retrieval task, eight of whom also submitted a notebook describing their approach. An analysis of these descriptions reveals the same building blocks that were commonly used in last year's source retrieval algorithms: (1) chunking, (2) keyphrase extraction, (3) query formulation, (4) search control, and (5) download filtering. Some participants simply reused their previous approach; in what follows, we describe the new or changed ideas in detail.</p><p>Chunking Given a suspicious document, it is divided into (possibly overlapping) passages of text. Each chunk of text is then processed individually. Rationale for chunking the suspicious document is to evenly distribute "attention" over a suspicious document so that algorithms employed in subsequent steps are less susceptible to unexpected characteristics of the suspicious document.</p><p>The chunking strategies employed by the participants are no chunking (i.e., the whole document as one chunk) <ref type="bibr" coords="8,270.49,155.18,10.79,8.64" target="#b2">[3,</ref><ref type="bibr" coords="8,285.62,155.18,12.45,8.64" target="#b17">18,</ref><ref type="bibr" coords="8,302.42,155.18,11.83,8.64" target="#b41">42]</ref>, 50-line chunks <ref type="bibr" coords="8,387.68,155.18,10.58,8.64" target="#b2">[3]</ref>, TextTiling <ref type="bibr" coords="8,451.89,155.18,16.60,8.64" target="#b13">[14]</ref> to identify the topically related passage and therein 4-sentence chunking <ref type="bibr" coords="8,418.77,167.13,15.27,8.64" target="#b12">[13]</ref>, paragraph chunking <ref type="bibr" coords="8,174.54,179.09,15.77,8.64" target="#b18">[19,</ref><ref type="bibr" coords="8,192.99,179.09,11.83,8.64" target="#b41">42]</ref>, anomaly sections based on intrinsic plagiarism detection <ref type="bibr" coords="8,440.56,179.09,15.27,8.64" target="#b20">[21]</ref>, 100word chunks <ref type="bibr" coords="8,188.25,191.04,15.27,8.64" target="#b42">[43]</ref>, 5-sentence chunks <ref type="bibr" coords="8,285.51,191.04,15.27,8.64" target="#b43">[44]</ref>, and combinations thereof. Note that chunks typically are stated as non-overlapping. An interesting question could be to identify the potential of overlapping chunks (except maybe in the cases of the whole-document and TextTiling chunks). Typical plagiarism cases have no fixed length and overlapping chunks would reduce the risk of, for instance, having more than one source in one chunk of 50 lines or 100 words, etc. Furthermore, relying on the given document structure (e.g., chunking by lines or paragraphs) bears the risk of failing for some unseen documents that are not as well-formatted as the ones in our evaluation corpus.</p><p>Keyphrase Extraction Given a chunk, keyphrases are extracted from it in order to formulate queries with them. Rationale for keyphrase extraction is to select only those phrases (or words) which maximize the chance of retrieving source documents matching the suspicious document. Keyphrase extraction may also serve as a means to limit the amount of queries formulated, thus reducing the overall costs of using a search engine. This step is perhaps the most important one of a source retrieval algorithm since the decisions made here directly affect the overall performance: the fewer keywords are extracted, the better the choice must be or recall is irrevocably lost.</p><p>Phrasal search was provided by the Indri search engine. However, only Lee et al. <ref type="bibr" coords="8,134.77,412.21,16.60,8.64" target="#b18">[19]</ref> made use of Indri. Some participants use single keywords while others extract whole phrases. Most of the participants preprocessed the suspicious document by removing stop words before the actual keyphrase extraction. In particular, Elizalde <ref type="bibr" coords="8,468.97,436.12,11.62,8.64" target="#b2">[3]</ref> applies three different strategies. The first approach generates one query per 50-lines chunk containing the top-10 words scored by tf • idf values; a word's document frequency is obtained from the external Brown corpus. The second approach uses ten queries, each of which is formed by one of the ten longest named entities extracted from the whole document using the Python Natural Language Toolkit NLTK. The third approach uses 15 queries, each of which is formed by one of the top-15 noun phrases extracted via Barker and Cornacchia's head noun phrase extractor <ref type="bibr" coords="8,400.92,519.81,10.58,8.64" target="#b0">[1]</ref>. Haggag and El-Beltagy <ref type="bibr" coords="8,169.21,531.77,16.60,8.64" target="#b12">[13]</ref> use KPMiner <ref type="bibr" coords="8,246.50,531.77,11.62,8.64" target="#b1">[2]</ref> to extract one keyphrase per topically related passage and combine this phrase with the rarest word (frequency on document level) of each 4-sentence chunk within the passage until the query contains ten keywords. Kong et al. <ref type="bibr" coords="8,134.77,567.63,16.60,8.64" target="#b17">[18]</ref> apply two different strategies. First, they use the top-20 words scored by tf • idf values from the whole document; a word's document frequency is obtained from the external Wall Street Journal corpus. Second, they use a Pat Tree <ref type="bibr" coords="8,403.97,591.54,16.60,8.64" target="#b9">[10]</ref> to extract one 2-gram, one 3-gram, two 4-grams, and forty 5-grams with highest tf • idf scores that contain at least one of the top-10 tf • idf terms. Lee et al. <ref type="bibr" coords="8,377.66,615.45,16.60,8.64" target="#b18">[19]</ref> use the most unique 8-gram from each paragraph (uniqueness determined via the Google Books n-grams) that starts with a word that is not contained in any keyphrase obtained for a previous paragraph. Nourian <ref type="bibr" coords="8,215.35,651.32,16.60,8.64" target="#b20">[21]</ref> use the first ten keywords of a chunk as one phrase (stopping or keyword extraction method not specified any further). Suchomel et al. <ref type="bibr" coords="9,438.31,119.31,16.60,8.64" target="#b41">[42]</ref> apply three different strategies. First, they use the top-5 words scored by tf •idf values from the whole document; a word's document frequency is obtained from an external web crawl. These top-5 keywords are then also combined with their most frequent two or three term collocations. Second they use a "representative" sentence of at least 6 words length from passages in the document that an intrinsic plagiarism detector identified as differing according to writing style; however, what makes a sentence representative is not explained. Third, for a paragraph they extract the longest sentence. Veselý et al. <ref type="bibr" coords="9,134.77,214.95,16.60,8.64" target="#b42">[43]</ref> use all the longest non-overlapping n-grams (n ≥ 5, n is odd) that return less than 300 but more than 0 results. To determine these keyphrases they basically employ the open end query formulation <ref type="bibr" coords="9,250.16,238.86,16.60,8.64" target="#b35">[36]</ref> with a query-level version of the User-over-Ranking hypothesis <ref type="bibr" coords="9,179.72,250.82,15.77,8.64" target="#b38">[39,</ref><ref type="bibr" coords="9,197.87,250.82,11.83,8.64" target="#b10">11]</ref>. Williams et al. <ref type="bibr" coords="9,276.16,250.82,16.60,8.64" target="#b43">[44]</ref> use a very similar and simplistic keyphrase extraction strategy: the first three disjunct sequential 10-grams of each "reduced" chunk (only nouns, adjectives and verbs) form the keyphrases. Note that this is very similar to the winning approach from 2012, where Jayapal <ref type="bibr" coords="9,334.30,286.69,16.60,8.64" target="#b14">[15]</ref> used the first such 10-gram per chunk only (also allowing pronouns).</p><p>Altogether, the participants' approaches to keyphrase extraction can basically be divided into four different categories. (1) Rather simplistic strategies that identify keyphrases by chunking the whole document into some longer n-grams. This probably conforms with the folklore human strategy of identifying some suspicious n-gram in a suspicious document and submitting this n-gram to a search engine. Using all longer n-grams probably also "hits" parts of the n-grams a human would have chosen. Thus, it is interesting to analyze the final performance of approaches that use this kind of keyphrases (cf. Section 2.5). ( <ref type="formula" coords="9,257.21,394.28,3.87,8.64">2</ref>) Another very common strategy is to use the tf • idfwise highest scoring words or phrases. (3) Notably, this year, for the first time, two participants also use keyphrase extraction schemes obtained from the research community around this topic. (4) Some participants do not rely on one strategy alone but combine different approaches for keyphrase extraction. This way, just as with chunking, the risk of algorithm error is further diminished and it becomes possible to exploit potentially different sources of information that complement each other.</p><p>Query Formulation Given sets of keywords extracted from chunks, queries are formulated which are tailored to the API of the search engine used. Rationale for this is to adhere to restrictions imposed by the search engine and to exploit search features that go beyond basic keyword search (e.g., Indri's phrasal search). The maximum number of search terms enforced by ChatNoir is 10 keywords per query while Indri allows for longer queries. Interestingly, most of the participants hardly combine keyphrases into one query apart from merging, for instance, the top-5 tf • idf terms, then the next five etc. This way, most participants explicitly try to formulate non-overlapping queries (i.e., they do not use the same keyword in more than one query) except for some of the participants that basically use all the longer n-grams in the suspicious document. This non-overlap is in line with many query-by-document strategies but in contrast to previous source retrieval strategies that were shown to better identify highly related documents than non-overlapping queries <ref type="bibr" coords="9,302.09,639.36,15.27,8.64" target="#b11">[12]</ref>. Also note that none of the participants made use of advanced search operators offered by Indri or ChatNoir, such as the facet to search for web pages of at least 300 words of text, and the facet to filter search results by readability.</p><p>Search Control Given a set of queries, the search controller schedules their submission to the search engine and directs the download of search results. Rationale for this is to dynamically adjust the search based on the results of each query, which may include dropping queries, reformulating existing ones, or formulating new ones based on the relevance feedback obtained from search results. Some teams did not implement a search controller and simply submit all formulated queries. The ones who implemented search contral applied the following ideas.</p><p>Haggag and El-Beltagy <ref type="bibr" coords="10,249.12,244.41,16.60,8.64" target="#b12">[13]</ref> drop a query when more than 60% of its terms are contained in a previous downloaded document. <ref type="bibr" coords="10,327.07,256.37,56.96,8.64">Lee et al. [19]</ref> stop submitting queries when most of the suspicious document is found as plagiarized or the number of queries exceeds the number of paragraphs in the suspicious document. However, it remains unclear what "most of a document" means and why paragraphs are a good upper bound on the query budget. In practice, not all documents are well-formatted. A document without paragraph breaks would then allow for a single query only. Suchomel et al. <ref type="bibr" coords="10,134.77,328.10,16.60,8.64" target="#b41">[42]</ref> schedule queries dependent on the keyphrase extractor which extracted the words: the order of precedence corresponds to the order in which they have been explained above. Whenever later queries were formulated for portions of the suspicious document that were already mapped to a source, these queries are not submitted and discarded from the list of open queries.</p><p>Note that none of the teams did try to reformulate existing queries or formulating new ones based on the available number of search results, the search snippets, or the downloaded documents, which leaves significant room for improvement. Download Filtering Given a set of downloaded documents, a download filter removes all documents that are probably not worthwhile being compared in detail with the suspicious document. Rationale for this is to further reduce the set of candidates and to save invocations of the subsequent detailed comparison step.</p><p>In particular, Elizalde <ref type="bibr" coords="10,238.82,489.06,11.62,8.64" target="#b2">[3]</ref> focuses on the top-10 results of a query and downloads a result document when at least 90% of the words in a 160-character snippet are contained in the suspicious document. Haggag and El-Beltagy <ref type="bibr" coords="10,346.09,512.97,16.60,8.64" target="#b12">[13]</ref> only consider the top-ranked result and download it when at least 50% of the query terms are contained in a 500 character snippet. It remains unclear whether checking not single words but phrases from the snippets could be beneficial. Kong et al. <ref type="bibr" coords="10,308.74,548.84,16.60,8.64" target="#b17">[18]</ref> download a document when the cosine similarity of the snippet (presumably 500 characters long) and the suspicious document exceeds some threshold (not specified in the paper). Taking into account that Chat-Noir's snippets are centered around the passage of a search result that contains most of the query's terms, the cosine similarity should typically be rather high for all search results (the query terms in the snippet are also contained in the suspicious document). Lee et al. <ref type="bibr" coords="10,173.84,620.57,16.60,8.64" target="#b18">[19]</ref> download the top-k results of a query (k is not specified) and documents that appear frequently in the results (frequency threshold not specified). Suchomel et al. <ref type="bibr" coords="10,134.77,644.48,16.60,8.64" target="#b41">[42]</ref> download documents when more than 20% of the word 2-grams in the 500 character snippet also appear in the suspicious document. Veselý et al. <ref type="bibr" coords="10,407.72,656.44,16.60,8.64" target="#b42">[43]</ref> first compute Table <ref type="table" coords="11,164.97,115.83,3.36,8.06">1</ref>. Source retrieval results with respect to retrieval performance and cost-effectiveness. queries without submitting them and submit all queries computed when 20% of the words in the suspicious document are contained in the queries without computing any new query afterwards. The retrieval scores of ChatNoir are used to compute the sum of these scores for a document over all queries and in the end the 15 documents with highest sum are downloaded (however, this is not consistent with the overall workload we measured, which is around 31 downloads per suspicious document). Williams et al. <ref type="bibr" coords="11,134.77,363.76,16.60,8.64" target="#b43">[44]</ref> download the top-3 documents whose snippets share at least five word 5-grams with the suspicious document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Team</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Evaluation Results</head><p>Table <ref type="table" coords="11,160.77,425.53,4.98,8.64">1</ref> shows the performances of the nine plagiarism detectors that implemented source retrieval. Since there is currently no formula to organize retrieval performance and cost-effectiveness into an absolute order, the detectors are ordered alphabetically, whereas the best performance value for each metric is highlighted. As can be seen, there is no single detector that performs best on all accounts. Rather, different detectors have different characteristics. The detector of Williams et al. <ref type="bibr" coords="11,394.65,485.30,16.60,8.64" target="#b43">[44]</ref> achieves the best trade-off between precision and recall and therefore the best F 1 value. This detector is followed closely by that of Haggag and El-Beltagy <ref type="bibr" coords="11,340.40,509.21,15.27,8.64" target="#b12">[13]</ref>, which achieves best precision but mediocre recall, whereas the detector of Kong et al. <ref type="bibr" coords="11,369.90,521.17,16.60,8.64" target="#b17">[18]</ref> achieves best recall at the cost of poor precision. It is not easy to decide which of these detectors solves the task best, since each of them may have their justification in practice. For example, the detector of Interestingly, the ensemble of all submitted approaches would achieve an average recall of 0.82 retrieving all sources for 23 topics. Only for eight topics the recall is below 0.65 (which is the best individual average recall). For just three topics none of the detectors detects a source (one of which actually has none).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Text Alignment</head><p>In text alignment, given a pair of documents, the task is to identify all contiguous passages of reused text between them. The challenge with this task is to identify passages of text that have been obfuscated, sometimes to the extent that, apart from stop words, little lexical similarity remains between an original passage and its plagiarized counterpart. Consequently, for evaluators, the challenge is to provide a representative corpus of documents that emulate this situation. To study this task, we employ a corpus construction methodology similar to that which has been used in previous evaluations of this task, while fixing some of its deficiencies. We evaluate the performance of plagiarism detectors based on the traditionally employed measures. Finally, we exploit the benefits of software submissions for the first time and evaluate the detectors that have been submitted last year on this year's evaluation corpus and vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Evaluation Corpus</head><p>The evaluation corpus for text alignment is also based on the aforementioned Webis-TRC-13. But instead of employing the documents of that corpus directly, pairs of documents that comprise reused passages have been constructed automatically, as was done in previous years <ref type="bibr" coords="12,206.34,392.29,15.27,8.64" target="#b27">[28]</ref>. One frequent point of criticism about automatically generating plagiarism is that it is difficult to ensure that documents between which text is plagiarized are about the same topic, so that plagiarism could be simply detected by analyzing topic drift <ref type="bibr" coords="12,177.44,428.15,15.27,8.64" target="#b31">[32]</ref>. Using the documents that have been retrieved manually as sources for the documents of the Webis-TRC-13 as a basis for constructing plagiarism cases, however, allows us to mitigate this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corpus Construction</head><p>The corpus is constructed within eight steps:</p><p>1. Documents. The documents used for our corpus are web documents obtained from the ClueWeb 2009 corpus. We have compiled a set of documents on 145 topics which have been manually searched, browsed, and found relevant to a given topic.</p><p>For each topic, we collected a set D topic that contains between 1 and 270 documents for a total of 10 630 documents. The documents have been judged by the writers who wrote documents on these topics for the aforementioned Webis-TRC-2012. 2. Pre-Processing. The HTML documents were converted to plain text, extracting their main content using the BoilerPipe library. Passages of text with eight words or less were discarded as well as documents with less than 100 words. In total, 6500 documents remained after pre-processing, divided into 144 remaining topics with at least two and up to 170 documents. 3. Withheld Documents. For each topic, one document is withheld in order not to be used for plagiarism.</p><p>4. Source Set Formation. Each of the suspicious documents that are generated in a later step has a set of source documents D src . In this step, these sets are formed by randomly choosing a topic from which to draw documents, a targeted size |D src | between 5 and 75 documents, and documents from D topic until the targeted size is reached or no further documents are available: in each iteration, a document is chosen at random from D topic and added to D src , unless a duplicate of the chosen document is already present in D src . In this connection, we consider two documents duplicates if their n-gram cosine similarity is above 0.6 for n = 1, above 0.25 for n = 3, and above 0 for n = 8. These thresholds were experimentally determined with respect to the documents used. <ref type="foot" coords="13,320.53,225.24,3.49,6.05" target="#foot_6">7</ref> A total of 520 source sets were created this way. 5. Withheld Source Sets. In addition to the above source sets, for each topic one source set was created similarly but ensuring that no sentence of a source document has a duplicate sentence in the withheld document of that topic chosen in Step 3. Two sentences are considered duplicates if their 1-gram cosine similarity is above 0.9. 6. Passage Extraction. Assuming a log-normal distribution of document lengths, we estimate its parameters based on the documents withheld in Step 3. For a given source set D src created in Step 4, we extract a set of passages from its documents, so that each passage is at least 50 words long, and every source document contributes at least one passage. Given these constraints, we sample the number of passages to be extracted per source document from a Poisson distribution with λ = 3, favoring lesser but larger passages. Further, passages are drawn so they are no duplicates of previously drawn passages (1-gram cosine similarity above 0.9), so they are not adjacent to previously drawn passages, and so that at least one passage is drawn per document. The resulting 520 passage sets contained between 5 and 134 passages. 7. Obfuscation. Every passage from the passage sets extracted in Step 6 is obfuscated in order to emulate plagiarist behavior based on the following four strategies: no obfuscation, random obfuscation, and, for the first time, cyclic translation obfuscation and summary obfuscation. With the exception of summary obfuscation which was constructed independently of the rest of the corpus, the strategies are applied uniformly distributed ensuring that only one strategy is applied in a suspicious document. Details about these strategies can be found below. 8. Suspicious Document Generation. In this step, a suspicious document d plg is generated by randomly concatenating an (obfuscated) passage set. Special care is taken with regard to their formatting so that paragraph breaks are no easy predictor of boundaries of reused passages.</p><p>Finally, the resulting suspicious documents are paired with their respective source documents, and the withheld documents are paired with the withheld source sets from Step 5 to form examples of document pairs that do not contain plagiarism. The corpus contains in total 3653 suspicious documents and 4774 source documents, which are grouped into 10000 pairs, so that there are 6000 pairs containing plagiarism (i.e., 2000 for each of the mentioned obfuscation strategies), 2000 containing unobfuscated plagiarism, and 2000 without plagiarism. Half of this corpus has been released as training corpus, and the other half was used as test corpus.</p><p>Random Obfuscation Random obfuscation is a naïve approach to obfuscation in that the resulting passages are not human-readable and bear no semantics. The purpose of this type of obfuscation is to test whether text alignment algorithms are capable of identifying reused passages from a bag-of-words model point of view. The obfuscation strategy itself is a sequence of random text operations such as shuffling, adding, deleting, and replacing words or short phrases at random. Replacing words is done based on a synonym database such as WordNet, and phrases are shuffled while maintaining the original part-of-speech sequence. Moreover, some sentences may be shuffled randomly.</p><p>The longer the sequence of random operations, the more an obfuscated passage differs from its original, and presumably the more difficult it is to identify them automatically. This kind of obfuscation has been used in all previous evaluation corpora that have been employed to evaluate this task.</p><p>Cyclic Translation Obfuscation A new kind of obfuscation strategy we introduce this year is cyclic translation obfuscation. Here, a plagiarized passage of text is run through a sequence of translations, so that the output of one translation forms the input of the next one while the last language of the sequence is the same as the passage's original language. Rationale of this strategy is to exploit the fact that translating a text inherently involves paraphrasing it, so that translating a text back and forth between languages is a way ob obtaining alternative versions of a text without changing its semantics. We employ the APIs of three different translation web services for this task, namely Google Translate, <ref type="foot" coords="14,205.50,424.89,3.49,6.05" target="#foot_7">8</ref> Microsoft Translator, <ref type="foot" coords="14,295.86,424.89,3.49,6.05" target="#foot_8">9</ref> and MyMemory. <ref type="foot" coords="14,368.61,424.89,6.97,6.05" target="#foot_9">10</ref> In every cyclic translation sequence, all three services are employed, since preliminary experiments revealed that employing only one of the services yield little to no difference of the obfuscated text to its unobfuscated counterpart. An explanation for this may be found in the fact that the language models used for translation appear to deterministically favor one alternative translation over others, even across many languages, while different translation services are based on independently trained models which introduce more variation in a cyclic translation sequence.</p><p>A cyclic translation sequence is constructed randomly. The intermediate languages of a sequence-start and end are always English-are drawn at random from two sets of languages, namely the Indo-European languages French, German, Italian, Spanish, and Swedish, and a mixture of different language families such as Arabic, Chinese, Hebrew, Hindi, and Japanese. First, one of the two sets is chosen and then up to three intermediate languages are employed as intermediate languages.</p><p>Summary Obfuscation Another new kind of obfuscation strategy we introduce this year is summary obfuscation. Its rationale is that including an unattributed summary of another's document in one's own text can be considered a case of plagiarism since the main ideas of the document are maintained in condensed form. Presuming that the summary is not based on a simple concatenation of some sentences from the original document, the lexical and syntactic similarities between summary and original document may be very restricted. Actually, summary obfuscation can be viewed as a form of plagiarism of ideas rather than the simple case of reusing exact phrases or sentences.</p><p>To build a corpus of plagiarism cases based on this idea, we used existing resources from the research field of automatic text summarization. In more detail, the set of original documents was taken from the Document Understanding Conference (DUC) 2001 corpus for text summarization. <ref type="foot" coords="15,260.93,225.24,6.97,6.05" target="#foot_10">11</ref> These are newswire stories and newspaper articles originally published in Wall Street Journal, Associated Press, San Jose Mercury News, Financial times, LA Times, and FBIS. For each such document there are two summaries of approximately 100 words created by human assessors. To produce plagiarism cases, these summaries were planted into documents of the DUC 2006 text summarization corpus. <ref type="foot" coords="15,197.12,285.02,6.97,6.05" target="#foot_11">12</ref> This corpus also comprises newswire stories and newspaper articles originally published in Associated Press, New York Times, and Xinhua News Agency. Hence, for each original document, we produced two plagiarism cases based on summary obfuscation. In addition, for each original document, eight more documents from DUC 2006 corpus were used as suspicious documents.</p><p>Given the genre of the DUC 2001 and DUC 2006 corpora, the similarity between an original document and its summary may be easily identified by using named-entity occurrences. That is, both the original document and its summary will talk about the same persons, locations, organizations, etc. To weaken this kind of similarity, we introduced some noise in the DUC 2006 documents by replacing some of their named-entities with the named-entities of the original document or the named-entities of the summaries. In particular, we randomly selected some parts of the suspicious documents of similar length to the summaries (e.g., 100 words) and transformed them to noisy areas by replacing their named-entities with those of the original text. That way, a suspicious document appears similar to the original since it refers to the same proper names, locations, organizations, etc. The popular Stanford Named Entity Recognizer 13 was used to detect time, location, organization, person, money, percent, and date entities in original and suspicious documents. In total, there are 237 original documents and 2607 suspicious ones in this part of our corpus. 474 of the suspicious documents contain plagiarized summaries, and 1896 are noisy documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Performance Measures</head><p>To assess the performance of the submitted detailed comparison approaches, we employ the performance measures used in previous evaluations. For this paper to be selfcontained, we summarize the definition found in <ref type="bibr" coords="15,335.75,587.56,15.49,8.64" target="#b27">[28]</ref>: let S denote the set of plagiarism cases in the corpus, and let R denote the set of detections reported by a plagiarism detector for the suspicious documents. To simplify notation, a plagiarism case s = s plg , d plg , s src , d src , s ∈ S, is represented as a set s of references to the characters of d plg and d src , specifying the passages s plg and s src . Likewise, a plagiarism detection r ∈ R is represented as r. Based on this notation, precision and recall of R under S can be measured as follows:</p><formula xml:id="formula_2" coords="16,145.38,171.89,324.60,27.47">prec(S, R) = 1 |R| r∈R | s∈S (s r)| |r| , r ec(S, R) = 1 |S| s∈S | r∈R (s r)| |s| ,</formula><p>where s r = s ∩ r if r detects s, ∅ otherwise.</p><p>Observe that neither precision nor recall account for the fact that plagiarism detectors sometimes report overlapping or multiple detections for a single plagiarism case. This is undesirable, and to address this deficit also a detector's granularity is quantified as follows:</p><formula xml:id="formula_3" coords="16,245.48,282.37,124.39,27.42">gran(S, R) = 1 |S R | s∈S R |R s |,</formula><p>where S R ⊆ S are cases detected by detections in R, and R s ⊆ R are detections of s; i.e., S R = {s | s ∈ S ∧ ∃r ∈ R : r detects s} and R s = {r | r ∈ R ∧ r detects s}. Note further that the above three measures alone do not allow for a unique ranking among detection approaches. Therefore, the measures are combined into a single overall score as follows:</p><formula xml:id="formula_4" coords="16,224.30,371.64,162.79,24.17">plagdet(S, R) = F 1 log 2 (1 + gran(S, R))</formula><p>,</p><p>where F 1 is the equally weighted harmonic mean of precision and recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Survey of Text Alignment Approaches</head><p>Nine of the 18 submitted detectors implement text alignment, and for six of them also a notebook describing their approach has been submitted. An analysis of these notebooks reveals a number of building blocks that are commonly used to build text alignment algorithms: (1) seeding, (2) extension, and (3) filtering. Text alignment is closely related to gene sequence alignment in bioinformatics, of which the terminology is borrowed: all of this year's approaches to text alignment implement the so-called seed and extendparadigm which is frequently applied in gene sequence alignment. In what follows, we describe them in detail.</p><p>Seeding Given a suspicious document and a source document, matches (so-called "seeds") between the two documents are identified using some seed heuristic. Seed heuristics either identify exact matches or create matches by changing the underlying texts in a domain-specific or linguistically motivated way. Rationale for this is to pinpoint substrings that altogether make up for the perceived similarity between suspicious and source document. By coming up with as many reasonable seeds as possible, the subsequent step of extending them into aligned passages of text becomes a lot easier. A number of seed heuristics have been applied by this year's participants: Rodríguez Torrejón and Martín Ramos <ref type="bibr" coords="16,247.00,656.44,16.60,8.64" target="#b33">[34]</ref> use sorted word 3-grams and two kinds of sorted word 1-skip-3-grams. Kong et al. <ref type="bibr" coords="17,246.69,119.31,16.60,8.64" target="#b17">[18]</ref> use sentence pairs as seeds which exceed a given similarity threshold. Suchomel et al. <ref type="bibr" coords="17,267.48,131.27,16.60,8.64" target="#b41">[42]</ref> use sorted word 4-grams and unsorted stop word 8-grams (the latter having been introduced in <ref type="bibr" coords="17,318.18,143.22,14.94,8.64" target="#b37">[38]</ref>). Shrestha and Solorio <ref type="bibr" coords="17,429.32,143.22,16.60,8.64" target="#b36">[37]</ref> also use stop word 8-grams in addition to named entity 5-grams (i.e., word 5-grams that contain at least one named entity) as well as all other word 5-grams. The latter, however, are processed separately from the former, since otherwise they would subsume the named entitiy n-grams. Also, at the expense of runtime, they introduce inexact n-gram matching (i.e., the n-grams need not overlap entirely but their Jaccard similarity must be above a given threshold). Palkovskii and Belov <ref type="bibr" coords="17,297.04,214.95,16.60,8.64" target="#b23">[24]</ref> use sorted word 5-grams. Before computing seeds, some participants choose to collapse whitespace, reduce cases, remove stop words, and stem the remaining words, if applicable to their respective seed heuristics.</p><p>Extension Given seed matches identified between a suspicious document and a source document, they are extended into aligned text passages between the two documents of maximal length, which are then reported as plagiarism detections. Rationale for merging seed matches is to determine whether a document contains plagiarized passages at all rather than just seeds matching by chance, and to identify a plagiarized passage as a whole rather than only its fragments.</p><p>Most of the participants' extension heuristics are rule-based, merging seeds into aligned passages if they are adjacent in both suspicious and source document and the size of the gap between them is below some threshold. The exact rule depends on the seeds used, and instead of using just one rule, many participants develop sets of constraints that have to be fulfilled by aligned passages in order to be reported as plagiarism detections. Since the rules are usually highly involved with their respective setup, we exemplify only one rule set here in order to give an idea of what they may look like: Suchomel et al. <ref type="bibr" coords="17,200.26,423.74,16.60,8.64" target="#b41">[42]</ref> employ a 2-step merge heuristic, where in the first step, adjacent seed matches that are no more than 4000 chars apart are merged. The resulting passages from the first step are then merged again, considering pairs of adjacent passages in turn, and checking if the gap between them contains at least four seeds so that there is at least one seed per 10 000 chars of gap length between them. To be merged, adjacent passages further have to fulfill the constraints that their gap is smaller than 30 000 chars, that their combined size is bigger than twice the gap size, and that the ratio of seeds per chars of the adjacent passages does not drop by a factor of more than three in the potentially merged passage. The only participants who go beyond rule-based merging are Palkovskii and Belov <ref type="bibr" coords="17,221.95,531.34,15.27,8.64" target="#b23">[24]</ref>, who employ clustering for unsupervised merging.</p><p>Filtering Given a set of aligned passages, a passage filter removes all aligned passages that do not meet certain criteria. Rationale for this is mainly to deal with overlapping passages and to discard extremely short passages.</p><p>Kong et al. <ref type="bibr" coords="17,195.64,596.66,16.60,8.64" target="#b16">[17]</ref> discard passages whose word overlap under a modified Jaccard coefficient is below a threshold. Suchomel et al. <ref type="bibr" coords="17,315.45,608.62,16.60,8.64" target="#b40">[41]</ref> discard overlapping passages that are shorter than 300 chars, and keep only the passages longer than 300 chars. Palkovskii and Belov <ref type="bibr" coords="17,178.91,632.53,16.60,8.64" target="#b22">[23]</ref> discard passages shorter than 190 chars. Gillam et al. <ref type="bibr" coords="17,417.50,632.53,11.62,8.64" target="#b7">[8]</ref> discard passages shorter than 50 words that have less than 0.75 cosine similarity under a vector space model. Other participants do not apply passage filtering. Remarks Since six of this year's participants took part in previous years as well, many of them simply reuse their earlier solutions. While there is no problem with doing so, innovative ideas become less frequent compared to parameter tuning and small adjustments to an algorithm. However, this year's best performing approach submitted by Rodríguez Torrejón and Martín Ramos <ref type="bibr" coords="18,291.73,328.35,15.27,8.64" target="#b33">[34]</ref>, has been evaluated for the fourth time in a row, showing that persistent development may eventually yield good results. A number of new ideas could be observed:</p><p>-Palkovskii and Belov <ref type="bibr" coords="18,240.64,370.96,16.60,8.64" target="#b23">[24]</ref> continue their development of obfuscation-specific detection approaches by targeting summary obfuscation in particular. -Suchomel et al. <ref type="bibr" coords="18,214.96,394.62,15.27,8.64" target="#b41">[42]</ref>, Rodríguez Torrejón and Martín Ramos <ref type="bibr" coords="18,391.88,394.62,15.27,8.64" target="#b33">[34]</ref>, and Shrestha and Solorio <ref type="bibr" coords="18,183.06,406.58,16.60,8.64" target="#b36">[37]</ref> employ more than one seed heuristic at the same time. In particular, the seed heuristic of Stamatatos <ref type="bibr" coords="18,265.70,418.54,16.60,8.64" target="#b37">[38]</ref> based on stop word n-grams is used more often. -Shrestha and Solorio <ref type="bibr" coords="18,239.61,430.24,16.60,8.64" target="#b36">[37]</ref> employ inexact seed matching (i.e., in order for a pair of seeds to be linked across documents, they need not be exactly equal but only approximately equal according to some similarity measure). This approach may lead to more relaxed seeding heuristics, but also introduces runtime overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Evaluation Results</head><p>In this section, we report on the evaluation of this year's submissions on the aforementioned evaluation corpus. Moreover, we conduct the first cross-year evaluation of all softwares submitted last year and this year on the evaluation corpora of both years. We further differentiate performance with regard to obfuscation strategies to provide insights into how the softwares deal with different strengths of obfuscation. In addition to that, we reveal how the changes made to softwares that have been submitted in different versions in both years affect performance, and whether or not they improved. We also shed light on the question of corpus difficulty and find that last year's evaluation corpus was more difficult than this year's corpus.</p><p>Overall Results of 2013 Table <ref type="table" coords="18,258.02,632.53,4.98,8.64" target="#tab_2">2</ref> shows the overall performances of the nine plagiarism detectors that implement text alignment and were submitted this year. The overall best performing approach is that of Rodríguez Torrejón and Martín Ramos <ref type="bibr" coords="18,429.41,656.44,15.27,8.64" target="#b33">[34]</ref>, closely followed by that of Kong et al. <ref type="bibr" coords="19,268.64,119.31,15.27,8.64" target="#b17">[18]</ref>. The former detector has unbalanced precision and recall, while the latter does, but with worse granularity. The three new approaches submitted this year from Saremi and Yaghmaee <ref type="bibr" coords="19,334.45,143.22,15.27,8.64" target="#b34">[35]</ref>, Shrestha and Solorio <ref type="bibr" coords="19,446.00,143.22,16.60,8.64" target="#b36">[37]</ref> and Nourian <ref type="bibr" coords="19,171.34,155.18,16.60,8.64" target="#b20">[21]</ref> achieve mid-range performances. Two detectors' performances do not exceed the baseline. In terms of precision and granularity, almost all detectors perform well, whereas recall sets them apart. In terms of runtime, all detectors are in the range of minutes, one requiring only 1.2 minutes, while two others lag far behind because they employ resource-intensive named entity recognition algorithms.</p><p>Cross-Year Evaluation of 2012 and 2013 Tables <ref type="table" coords="19,335.53,226.05,23.98,8.64" target="#tab_5">3 to 6</ref> show the performances of all 18 plagiarism detectors submitted last year and this year that implement text alignment on both years' respective evaluation corpora. The overall performance of the detectors with regard to the plagdet score can be found in Table <ref type="table" coords="19,356.24,261.92,3.74,8.64">3</ref>. As can be seen, the best performing detectors across both years are those of Oberreuter et al. <ref type="bibr" coords="19,399.48,273.87,16.60,8.64" target="#b21">[22]</ref> and Kong et al. <ref type="bibr" coords="19,134.77,285.83,15.27,8.64" target="#b16">[17]</ref>, both of which have been first evaluated in 2012. This year's best performing detectors from Rodríguez Torrejón and Martín Ramos <ref type="bibr" coords="19,341.02,297.78,16.60,8.64" target="#b33">[34]</ref> comes close to them, however, only when evaluated on the 2013 evaluation corpus. On the 2012 corpus it is far off, which suggest this detector may be overfitted to the 2013 corpus.</p><p>Regarding different obfuscation strategies, it appears the detectors' performances on the 2012 corpus correlate mostly with their overall performance, but on the 2013 corpus this is not the case. Especially for summary obfuscation, the two best performing detectors are from Suchomel et al. <ref type="bibr" coords="19,280.32,369.51,15.77,8.64" target="#b40">[41,</ref><ref type="bibr" coords="19,299.65,369.51,13.28,8.64" target="#b41">42]</ref> which otherwise achieve mid-range performance only. The detector of Oberreuter et al. <ref type="bibr" coords="19,331.77,381.47,16.60,8.64" target="#b21">[22]</ref> fails on summarized plagiarism, while that of Kong et al. <ref type="bibr" coords="19,235.13,393.42,16.60,8.64" target="#b16">[17]</ref> achieves third-best performance on this kind of obfuscation. It is unfortunate that the detector of Palkovskii and Belov <ref type="bibr" coords="19,385.73,405.38,15.27,8.64" target="#b23">[24]</ref>, which implements a detection approach that targets summary obfuscation, does not compete with the others. Regarding unobfuscated plagiarism (column "None" in the tables), it is interesting to observe that many detectors detect this kind of plagiarism with scores above 0.9 on the 2012 corpus but less so in the 2013 corpus where the scores are mostly below that number. It is unclear why this is the case, since unobfuscated plagiarism is not changed when inserted into a suspicious document with the exception of text formatting.</p><p>Table <ref type="table" coords="19,175.58,489.06,4.98,8.64" target="#tab_3">4</ref> shows the detectors' performances with regard to precision. In general, achieving a high precision appears to be less of a problem compared to achieving a high recall. This is underpinned by the fact that our basic baseline approach outperforms almost all detectors in precision. However, the detectors that perform best in precision typically have deficiencies in terms of recall, but not the other way around: the aforementioned overall best performing detectors achieve mid-range precision. The only obfuscation strategy that poses a comparably higher challenge in terms of precision is random high obfuscation, which has been adjusted to emulate extreme obfuscation. Table <ref type="table" coords="19,159.73,584.71,4.98,8.64" target="#tab_4">5</ref> shows the detectors' performances with regard to recall. The best performing detectors are the two versions submitted by Kong et al. <ref type="bibr" coords="19,367.17,596.66,15.77,8.64" target="#b16">[17,</ref><ref type="bibr" coords="19,386.57,596.66,11.83,8.64" target="#b17">18]</ref>. They dominate all others in terms of recall, but not each other; the 2013 version performs best on the 2012 corpus and vice versa. By contrast, this year's best performing detector by Rodríguez Torrejón and Martín Ramos <ref type="bibr" coords="19,281.03,632.53,16.60,8.64" target="#b33">[34]</ref> achieves only mid-range recall. The best performing approaches with regard to precision from Gillam et al. <ref type="bibr" coords="19,381.93,644.48,10.79,8.64" target="#b7">[8,</ref><ref type="bibr" coords="19,394.62,644.48,8.30,8.64" target="#b6">7]</ref> performs poor with regard to recall, suggesting that the implemented approach is too conservative for this Table <ref type="table" coords="20,158.62,129.93,3.36,8.06">3</ref>. Cross-year evaluation of text alignment software submissions for 2012 and 2013 with respect to plagdet. The darker a cell, the better the performance compared to the entire column. task; the authors reveal their primary concerns up to now has been near-duplicate texts instead of paraphrases. In general, a visible correlation of recall performance on the entire 2012 corpus with the performances for each obfuscation strategy can be observed, and to a lesser extent on the 2013 corpus. Table <ref type="table" coords="24,332.69,155.18,4.98,8.64" target="#tab_5">6</ref> shows the detectors' performances with regard to granularity. Many detectors achieve perfect granularity on almost all obfuscation strategies, which may indicate that the problem of fragmented detections of a contiguous plagiarism case is under control, however, especially obfuscated plagiarism naturally poses a higher challenge with regard to this performance measure, which can be seen looking at random obfuscation as well as summary obfuscation. The only approaches that apparently do not do anything about granularity appear to be the ones of Jayapal <ref type="bibr" coords="24,167.06,238.86,15.77,8.64" target="#b14">[15,</ref><ref type="bibr" coords="24,185.25,238.86,11.83,8.64" target="#b15">16]</ref>. In general, however, these numbers must be taken with a grain of salt, since participants often resort to post-retrieval filtering in order to optimize granularity only for the sake of achieving a good ranking instead, while some admit that they would not do this in practice.</p><p>Comparing Detector Versions between 2012 and 2013 Since six of nine teams submitted versions of their detectors in both 2012 and 2013, this allows for an analysis of performance changes across versions of the same detector, and whether the adjustments made pay off in terms of improved performance. Such analyses are a novelty for evaluation labs such as ours and they yield insights into task design. Figure <ref type="figure" coords="24,415.42,346.03,4.98,8.64" target="#fig_5">3</ref> shows the performance differences of each of the six detector pairs when subtracting their respective 2012 performance values from their 2013 ones for each of our four performance measures and both years' evaluation corpora. Regarding plagdet, half of the participants achieve a performance improvement and the other half decreased the performance of their detectors. The highest performance gain of about 0.08 plagdet performance was achieved by Jayapal and Goswami <ref type="bibr" coords="24,275.26,417.76,15.27,8.64" target="#b15">[16]</ref>, but only on the 2013 corpus, while Palkovskii and Belov <ref type="bibr" coords="24,177.34,429.72,16.60,8.64" target="#b23">[24]</ref> suffer a significant performance loss of at least 0.2 plagdet on both corpora. Clearly, the modifications made on the latter detector should be carefully reviewed or even reverted. Precision and recall are related measures in that one can typically be traded for the other. Regarding them, it can be seen that all but one detector decrease in precision performance, but only Jayapal and Goswami <ref type="bibr" coords="24,316.10,489.49,16.60,8.64" target="#b15">[16]</ref> and Suchomel et al. <ref type="bibr" coords="24,417.08,489.49,16.60,8.64" target="#b41">[42]</ref> materialize a return in terms of increased recall. The modifications made by Gillam <ref type="bibr" coords="24,427.03,501.45,11.62,8.64" target="#b6">[7]</ref> and Kong et al. <ref type="bibr" coords="24,155.70,513.40,16.60,8.64" target="#b17">[18]</ref> result in a slight decrease of recall, and those of Palkovskii and Belov <ref type="bibr" coords="24,447.77,513.40,16.60,8.64" target="#b23">[24]</ref> in a big loss of recall. The detector of Rodríguez Torrejón and Martín Ramos <ref type="bibr" coords="24,424.86,525.36,16.60,8.64" target="#b33">[34]</ref> improves a lot in terms of precision but only slightly in terms of recall. Finally, all but one detector suffer losses in terms of granularity. Most of these losses are rather small, except for that of Jayapal and Goswami <ref type="bibr" coords="24,252.81,561.22,16.60,8.64" target="#b15">[16]</ref> which amounts to more than 10 granularity loss.</p><p>Comparing Corpus Versions between 2012 and 2013 Software submissions not only allow for more sustainable evaluations and assessing software versions in terms of performance changes, but also for measuring the difficulty of different evaluation corpora. By evaluating every detector on the evaluation corpora of both 2012 and 2013, a distribution of comparable performance values is obtained that sheds light on how difficult it is to identify cases of plagiarism in the two corpora relative to each other. Figure <ref type="figure" coords="24,449.06,644.48,4.98,8.64" target="#fig_6">4</ref> shows the performances of all detectors on both corpora for each of our four performance mea- sures, ordered from high to low performance. In terms of plagdet, the 2013 evaluation corpus is consistently easier than the 2012 corpus. While the recall curve difference is comparable to that of the plagdet curves, the precision curves are closer to each other, which indicates that precision difficulty is similar across both corpora. In terms of granularity, more than half of the detectors achieve almost equal performance. The remainder perform better on the 2013 corpus because their granularity values are smaller. As a result, our revised corpus construction process outlined above yields plagiarism cases that are more easily detected in general. Besides these differences, the obfuscation-specific performances shown in Tables <ref type="table" coords="25,475.61,551.67,4.98,8.64;25,134.77,563.63,16.51,8.64" target="#tab_5">3  to 6</ref> show that the random obfuscation strategy employed in 2013 compares to that of random low obfuscation of 2012; despite other intentions, we did not accomplish to hit the middle ground between random low and random high obfuscation, which contributes to the 2013 corpus being less difficult. The cyclic translation obfuscation appears to be on a level of difficulty similar to that of random (low) obfuscation, since most detectors achieve similar performances on them. The most difficult portions of the 2012 corpus is random high obfuscation, and that of the 2013 corpus is summary obfuscation, which can be seen particularly when considering recall performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Outlook</head><p>With this fifth international competition on plagiarism detection at PAN 2013 we introduced a number of improvements to the evaluation methodology for plagiarism detectors. (1) By calling for software submissions instead of run submissions, we further automated the organization of evaluation labs in general. Similarly, we improved both the reproducibility and the comparability of the evaluation results. (2) We extended the evaluation setup for plagiarism source retrieval, which now is a task built on top of two search engines that index the ClueWeb corpus, a search proxy API, and a source oracle service, all of which are running on a cluster computer at our site. Moreover, by introducing performance measures that are robust against near-duplicate retrieval results we improved the task at a conceptual level. (3) For the plagiarism text alignment task we presented a new evaluation corpus that is based on manually written essays so as to further increase its realism. (4) We introduced two new forms of plagiarism obfuscation strategies, which implement new paradigms of emulating a real plagiarist's behavior when modifying a copied passage. The two strategies are: cyclic translations, which provide for more realistic automatic paraphrasing compared to previously employed methods, and summaries, which have been obtained from a third-party data source.</p><p>This year, we collected a total of 18 plagiarism detectors from 14 teams, half of which implement source retrieval and the other half text alignment. For the task of text alignment, this is the second year in which we ask for software submissions instead of run submissions, which gives us the opportunity to conduct a cross-year evaluation of all detectors submitted in both years on all evaluation corpora available. Our crossyear evaluation reveals that this year's best performing detector cannot keep up with the best performing detectors of last year. Moreover, considering the six detectors that have been submitted in both years, we analyzed whether their retrieval performance has been improved. In fact, for three of the detectors this is not the case; i.e., the adjustments should be carefully reviewed or even reverted. Finally, we used the submitted softwares to compare the difficulty of our corpora, and we found out that this year's evaluation corpus is significantly easier than the last year's corpus in terms of detecting the contained plagiarism cases.</p><p>Altogether, we see a lot of room for further improvement with respect to both the methodology of plagiarism detection evaluation and the organization paradigm of software submissions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,199.04,256.00,217.27,8.12"><head>Figure 1</head><label>1</label><figDesc>Figure1. Generic retrieval process to detect plagiarism<ref type="bibr" coords="2,399.14,256.35,13.74,7.77" target="#b39">[40]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,134.77,376.23,345.83,8.12;4,134.77,387.54,345.82,7.77;4,134.77,398.50,345.83,7.77;4,134.77,409.46,343.29,7.77"><head>Figure 2 .</head><label>2</label><figDesc>Figure2. Overview of the building blocks used in the evaluation of plagiarism detectors that implement source retrieval. The components are organized by the two activities corpus construction and evaluation runs (top two rows). Both activities are based on a static evaluation infrastructure (bottom row) consisting of an experimentation platform, web search engines, and a web corpus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,139.25,451.75,319.18,9.65;6,139.25,463.70,341.35,9.65;6,151.70,475.66,328.89,8.96;6,151.70,487.61,255.79,8.96;6,139.25,499.57,341.34,9.65;6,151.70,511.52,328.89,9.65;6,151.70,523.48,328.89,9.65;6,151.70,535.43,328.89,8.96;6,151.70,547.39,40.84,8.96"><head>1 .</head><label>1</label><figDesc>Equality. The document d ret is a true positive detection of d src if d ret = d src . 2. Similarity. The document d ret is a true positive detection of d src if their n-gram Jaccard similarity is above 0.8 for n = 3, above 0.5 for n = 5, and above 0 for n = 8. The thresholds have been determined experimentally. 3. Containment. The document d ret is a true positive detection of d src if the passages in d plg known to be reused from d src are contained in d ret . Containment is measured as asymmetrical set overlap of the passages' set of n-grams regarding that of d ret , so that the overlap is above 0.8 for n = 3, above 0.5 for n = 5, and above 0 for n = 8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,294.18,176.78,131.59,9.65;7,140.14,191.72,8.25,8.74;7,148.39,196.23,9.77,6.12;7,161.45,191.72,317.20,9.65"><head></head><label></label><figDesc>a true positive detection of d src }, D ret = {d src | d src ∈ D src and ∃d ret ∈ D ret : d ret is a true positive detection of d src }.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="25,134.77,389.87,345.83,8.12;25,134.77,401.18,345.82,7.77;25,134.77,412.14,344.09,7.77"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Performance differences of detectors of which two versions have been submitted, one last year and one this year. The differences reveal the performance changes which result from further development on these detectors both on the 2012 evaluation corpus and the 2013 corpus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="26,134.77,405.22,345.83,8.12;26,134.77,416.52,345.82,7.77;26,134.77,427.48,345.82,7.77;26,134.77,438.44,79.68,7.77"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Performance value distribution on both the 2013 evaluation corpus and the 2012 corpus. The differences in performance across the resulting curves indicate corpus difficulty relative to each other. The bigger-smaller, in case of granularity-the area under a curve, the easier the corresponding corpus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="11,134.77,557.04,345.82,68.41"><head></head><label></label><figDesc>Haggag and  El-Beltagy downloads only about six documents on average per suspicious document and minimizes the time to first detection. Despite the excellent trade-off of Williams et al.'s detector, it incurs the second-highest costs in terms of queries on average, which is more than thrice as much as the other mentioned detectors. Kong et al.'s detector has highest download costs, but one may argue that downloads are much cheaper than queries, and that in this task recall is more important than precision.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="18,179.33,115.83,256.71,136.52"><head>Table 2 .</head><label>2</label><figDesc>Text alignment results with retrieval performance and runtime.</figDesc><table coords="18,179.98,138.62,255.39,113.73"><row><cell>Team</cell><cell cols="3">PlagDet Recall Precision Granularity Runtime</cell></row><row><cell cols="2">R. Torrejón 0.82220 0.76190 0.89484</cell><cell>1.00141</cell><cell>1.2 m</cell></row><row><cell>Kong</cell><cell>0.81896 0.81344 0.82859</cell><cell>1.00336</cell><cell>6.1 m</cell></row><row><cell>Suchomel</cell><cell>0.74482 0.76593 0.72514</cell><cell>1.00028</cell><cell>28.0 m</cell></row><row><cell>Saremi</cell><cell>0.69913 0.77123 0.86509</cell><cell>1.24450</cell><cell>446.0 m</cell></row><row><cell>Shrestha</cell><cell>0.69551 0.73814 0.87461</cell><cell>1.22084</cell><cell>684.5 m</cell></row><row><cell cols="2">Palkovskii 0.61523 0.53561 0.81699</cell><cell>1.07295</cell><cell>6.5 m</cell></row><row><cell>Nourian</cell><cell>0.57716 0.43381 0.94707</cell><cell>1.04343</cell><cell>40.1 m</cell></row><row><cell>Baseline</cell><cell>0.42191 0.34223 0.92939</cell><cell>1.27473</cell><cell>30.5 m</cell></row><row><cell>Gillam</cell><cell>0.40059 0.25890 0.88487</cell><cell>1.00000</cell><cell>21.3 m</cell></row><row><cell>Jayapal</cell><cell>0.27081 0.38187 0.87901</cell><cell>2.90698</cell><cell>4.8 m</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="20,134.89,163.73,345.57,481.10"><head>Table 4 .</head><label>4</label><figDesc>Cross-year evaluation of text alignment software submissions for 2012 and 2013 with respect to precision. The darker a cell, the better the performance compared to the entire column.</figDesc><table coords="20,134.89,163.73,345.57,481.10"><row><cell cols="2">Software Submission</cell><cell cols="5">Obfuscation Strategies of the 2012 Evaluation Corpus</cell><cell>Entire Corpus</cell></row><row><cell>Team</cell><cell>Year</cell><cell>None</cell><cell cols="2">Random low Random high</cell><cell cols="2">Translation Man. paraphrase</cell><cell></cell></row><row><cell>Oberreuter</cell><cell>2012</cell><cell>0.92552</cell><cell>0.84416</cell><cell>0.40671</cell><cell>0.78128</cell><cell>0.71728</cell><cell>0.74575</cell></row><row><cell>Kong</cell><cell>2012</cell><cell>0.89899</cell><cell>0.82258</cell><cell>0.39652</cell><cell>0.77121</cell><cell>0.75884</cell><cell>0.73846</cell></row><row><cell>Kong</cell><cell>2013</cell><cell>0.87815</cell><cell>0.81645</cell><cell>0.39764</cell><cell>0.77576</cell><cell>0.73973</cell><cell>0.72576</cell></row><row><cell>Suchomel</cell><cell>2013</cell><cell>0.88169</cell><cell>0.81143</cell><cell>0.33546</cell><cell>0.71113</cell><cell>0.64713</cell><cell>0.68542</cell></row><row><cell>R. Torrejón</cell><cell>2012</cell><cell>0.93248</cell><cell>0.71393</cell><cell>0.12761</cell><cell>0.69631</cell><cell>0.67367</cell><cell>0.67078</cell></row><row><cell>R. Torrejón</cell><cell>2013</cell><cell>0.94516</cell><cell>0.71251</cell><cell>0.14697</cell><cell>0.73955</cell><cell>0.68047</cell><cell>0.66816</cell></row><row><cell>Suchomel</cell><cell>2012</cell><cell>0.93875</cell><cell>0.80181</cell><cell>0.15376</cell><cell>0.63538</cell><cell>0.61104</cell><cell>0.66532</cell></row><row><cell>Palkovskii</cell><cell>2012</cell><cell>0.82442</cell><cell>0.76691</cell><cell>0.31273</cell><cell>0.73679</cell><cell>0.62232</cell><cell>0.64630</cell></row><row><cell>Nourian</cell><cell>2013</cell><cell>0.86535</cell><cell>0.51824</cell><cell>0.06977</cell><cell>0.55013</cell><cell>0.49950</cell><cell>0.53270</cell></row><row><cell>Kueppers</cell><cell>2012</cell><cell>0.80950</cell><cell>0.26609</cell><cell>0.02645</cell><cell>0.48362</cell><cell>0.29958</cell><cell>0.40494</cell></row><row><cell>Palkovskii</cell><cell>2013</cell><cell>0.54280</cell><cell>0.33422</cell><cell>0.10561</cell><cell>0.46256</cell><cell>0.42080</cell><cell>0.38150</cell></row><row><cell>Gillam</cell><cell>2012</cell><cell>0.92933</cell><cell>0.04741</cell><cell>0.00917</cell><cell>0.00050</cell><cell>0.12409</cell><cell>0.31109</cell></row><row><cell>Gillam</cell><cell>2013</cell><cell>0.92736</cell><cell>0.04735</cell><cell>0.00917</cell><cell>0.00050</cell><cell>0.12253</cell><cell>0.31034</cell></row><row><cell cols="2">Sánchez-Vega 2012</cell><cell>0.60305</cell><cell>0.25388</cell><cell>0.04202</cell><cell>0.39758</cell><cell>0.26400</cell><cell>0.30857</cell></row><row><cell>Baseline</cell><cell></cell><cell>0.87712</cell><cell>0.06382</cell><cell>0.00023</cell><cell>0.04440</cell><cell>0.06137</cell><cell>0.20210</cell></row><row><cell>Jayapal</cell><cell>2013</cell><cell>0.11483</cell><cell>0.06265</cell><cell>0.01075</cell><cell>0.07336</cell><cell>0.02950</cell><cell>0.05753</cell></row><row><cell>Jayapal</cell><cell>2012</cell><cell>0.10272</cell><cell>0.05059</cell><cell>0.01349</cell><cell>0.05211</cell><cell>0.04394</cell><cell>0.05085</cell></row><row><cell cols="2">Software Submission</cell><cell cols="5">Obfuscation Strategies of the 2013 Evaluation Corpus</cell><cell>Entire Corpus</cell></row><row><cell>Team</cell><cell>Year</cell><cell>None</cell><cell>Random</cell><cell cols="2">Cyclic translation</cell><cell>Summary</cell><cell></cell></row><row><cell>Kong</cell><cell>2012</cell><cell>0.87249</cell><cell>0.83242</cell><cell></cell><cell>0.85212</cell><cell>0.43635</cell><cell>0.83679</cell></row><row><cell>Oberreuter</cell><cell>2012</cell><cell>0.94170</cell><cell>0.74955</cell><cell></cell><cell>0.84618</cell><cell>0.13208</cell><cell>0.82678</cell></row><row><cell>R. Torrejón</cell><cell>2013</cell><cell>0.92586</cell><cell>0.74711</cell><cell></cell><cell>0.85113</cell><cell>0.34131</cell><cell>0.82220</cell></row><row><cell>Kong</cell><cell>2013</cell><cell>0.82740</cell><cell>0.82281</cell><cell></cell><cell>0.85181</cell><cell>0.43399</cell><cell>0.81896</cell></row><row><cell>Palkovskii</cell><cell>2012</cell><cell>0.88161</cell><cell>0.79692</cell><cell></cell><cell>0.74032</cell><cell>0.27507</cell><cell>0.79155</cell></row><row><cell>R. Torrejón</cell><cell>2012</cell><cell>0.88222</cell><cell>0.70151</cell><cell></cell><cell>0.80112</cell><cell>0.44184</cell><cell>0.78767</cell></row><row><cell>Suchomel</cell><cell>2013</cell><cell>0.81761</cell><cell>0.75276</cell><cell></cell><cell>0.67544</cell><cell>0.61011</cell><cell>0.74482</cell></row><row><cell>Suchomel</cell><cell>2012</cell><cell>0.89848</cell><cell>0.65213</cell><cell></cell><cell>0.63088</cell><cell>0.50087</cell><cell>0.73224</cell></row><row><cell>Saremi</cell><cell>2013</cell><cell>0.84963</cell><cell>0.65668</cell><cell></cell><cell>0.70903</cell><cell>0.11116</cell><cell>0.69913</cell></row><row><cell>Shrestha</cell><cell>2013</cell><cell>0.89369</cell><cell>0.66714</cell><cell></cell><cell>0.62719</cell><cell>0.11860</cell><cell>0.69551</cell></row><row><cell>Kueppers</cell><cell>2012</cell><cell>0.81977</cell><cell>0.51602</cell><cell></cell><cell>0.56932</cell><cell>0.13848</cell><cell>0.62772</cell></row><row><cell>Palkovskii</cell><cell>2013</cell><cell>0.82431</cell><cell>0.49959</cell><cell></cell><cell>0.60694</cell><cell>0.09943</cell><cell>0.61523</cell></row><row><cell>Nourian</cell><cell>2013</cell><cell>0.90136</cell><cell>0.35076</cell><cell></cell><cell>0.43864</cell><cell>0.11535</cell><cell>0.57716</cell></row><row><cell>Sánchez-Vega</cell><cell>2012</cell><cell>0.52179</cell><cell>0.45598</cell><cell></cell><cell>0.44323</cell><cell>0.28807</cell><cell>0.45923</cell></row><row><cell>Baseline</cell><cell></cell><cell>0.93404</cell><cell>0.07123</cell><cell></cell><cell>0.10630</cell><cell>0.04462</cell><cell>0.42191</cell></row><row><cell>Gillam</cell><cell>2012</cell><cell>0.87655</cell><cell>0.04723</cell><cell></cell><cell>0.01225</cell><cell>0.00218</cell><cell>0.41373</cell></row><row><cell>Gillam</cell><cell>2013</cell><cell>0.85884</cell><cell>0.04191</cell><cell></cell><cell>0.01224</cell><cell>0.00218</cell><cell>0.40059</cell></row><row><cell>Jayapal</cell><cell>2013</cell><cell>0.38780</cell><cell>0.18148</cell><cell></cell><cell>0.18181</cell><cell>0.05940</cell><cell>0.27081</cell></row><row><cell>Jayapal</cell><cell>2012</cell><cell>0.34758</cell><cell>0.12049</cell><cell></cell><cell>0.10504</cell><cell>0.04541</cell><cell>0.20169</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="22,134.77,129.76,345.82,515.24"><head>Table 5 .</head><label>5</label><figDesc>Cross-year evaluation of text alignment software submissions for 2012 and 2013 with respect to recall. The darker a cell, the better the performance compared to the entire column.</figDesc><table coords="22,134.89,163.56,345.57,481.44"><row><cell cols="2">Software Submission</cell><cell cols="5">Obfuscation Strategies of the 2012 Evaluation Corpus</cell><cell>Entire Corpus</cell></row><row><cell>Team</cell><cell>Year</cell><cell>None</cell><cell cols="2">Random low Random high</cell><cell cols="2">Translation Man. paraphrase</cell><cell></cell></row><row><cell>Kong</cell><cell>2013</cell><cell>0.97212</cell><cell>0.77942</cell><cell>0.28378</cell><cell>0.74257</cell><cell>0.65001</cell><cell>0.67971</cell></row><row><cell>Kong</cell><cell>2012</cell><cell>0.97376</cell><cell>0.77716</cell><cell>0.27710</cell><cell>0.72727</cell><cell>0.65768</cell><cell>0.67877</cell></row><row><cell>Oberreuter</cell><cell>2012</cell><cell>0.99994</cell><cell>0.78029</cell><cell>0.26646</cell><cell>0.73028</cell><cell>0.59226</cell><cell>0.66072</cell></row><row><cell>Suchomel</cell><cell>2013</cell><cell>0.99582</cell><cell>0.79682</cell><cell>0.21922</cell><cell>0.64758</cell><cell>0.56048</cell><cell>0.63205</cell></row><row><cell>Palkovskii</cell><cell>2012</cell><cell>0.99897</cell><cell>0.75036</cell><cell>0.20687</cell><cell>0.66084</cell><cell>0.54329</cell><cell>0.61864</cell></row><row><cell>R. Torrejón</cell><cell>2013</cell><cell>0.97983</cell><cell>0.61697</cell><cell>0.08888</cell><cell>0.65729</cell><cell>0.52734</cell><cell>0.56609</cell></row><row><cell>R. Torrejón</cell><cell>2012</cell><cell>0.96883</cell><cell>0.59335</cell><cell>0.06961</cell><cell>0.59719</cell><cell>0.57758</cell><cell>0.56468</cell></row><row><cell>Suchomel</cell><cell>2012</cell><cell>0.99912</cell><cell>0.69438</cell><cell>0.08513</cell><cell>0.51053</cell><cell>0.45850</cell><cell>0.53778</cell></row><row><cell>Palkovskii</cell><cell>2013</cell><cell>0.96086</cell><cell>0.51128</cell><cell>0.08881</cell><cell>0.49045</cell><cell>0.44202</cell><cell>0.49151</cell></row><row><cell>Nourian</cell><cell>2013</cell><cell>0.95057</cell><cell>0.47997</cell><cell>0.04437</cell><cell>0.39671</cell><cell>0.36408</cell><cell>0.43694</cell></row><row><cell cols="2">Sánchez-Vega 2012</cell><cell>0.84717</cell><cell>0.28239</cell><cell>0.03033</cell><cell>0.43377</cell><cell>0.24522</cell><cell>0.34798</cell></row><row><cell>Kueppers</cell><cell>2012</cell><cell>0.93472</cell><cell>0.19246</cell><cell>0.01389</cell><cell>0.43943</cell><cell>0.24407</cell><cell>0.34535</cell></row><row><cell>Baseline</cell><cell></cell><cell>0.99888</cell><cell>0.04571</cell><cell>0.00011</cell><cell>0.03894</cell><cell>0.08176</cell><cell>0.21593</cell></row><row><cell>Gillam</cell><cell>2012</cell><cell>0.93503</cell><cell>0.02785</cell><cell>0.00709</cell><cell>0.00025</cell><cell>0.06656</cell><cell>0.19210</cell></row><row><cell>Gillam</cell><cell>2013</cell><cell>0.93562</cell><cell>0.02782</cell><cell>0.00709</cell><cell>0.00025</cell><cell>0.06566</cell><cell>0.19190</cell></row><row><cell>Jayapal</cell><cell>2013</cell><cell>0.55421</cell><cell>0.10837</cell><cell>0.00998</cell><cell>0.10898</cell><cell>0.05298</cell><cell>0.15126</cell></row><row><cell>Jayapal</cell><cell>2012</cell><cell>0.25459</cell><cell>0.06208</cell><cell>0.01148</cell><cell>0.05322</cell><cell>0.04942</cell><cell>0.08162</cell></row><row><cell cols="2">Software Submission</cell><cell cols="5">Obfuscation Strategies of the 2013 Evaluation Corpus</cell><cell>Entire Corpus</cell></row><row><cell>Team</cell><cell>Year</cell><cell>None</cell><cell>Random</cell><cell cols="2">Cyclic translation</cell><cell>Summary</cell><cell></cell></row><row><cell>Kong</cell><cell>2012</cell><cell>0.94836</cell><cell>0.77903</cell><cell></cell><cell>0.85003</cell><cell>0.29892</cell><cell>0.82449</cell></row><row><cell>Kong</cell><cell>2013</cell><cell>0.90682</cell><cell>0.78682</cell><cell></cell><cell>0.84626</cell><cell>0.30017</cell><cell>0.81344</cell></row><row><cell>Saremi</cell><cell>2013</cell><cell>0.95416</cell><cell>0.68877</cell><cell></cell><cell>0.80473</cell><cell>0.10209</cell><cell>0.77123</cell></row><row><cell>Oberreuter</cell><cell>2012</cell><cell>0.99932</cell><cell>0.65322</cell><cell></cell><cell>0.79587</cell><cell>0.07076</cell><cell>0.76864</cell></row><row><cell>Suchomel</cell><cell>2013</cell><cell>0.99637</cell><cell>0.68886</cell><cell></cell><cell>0.66621</cell><cell>0.56296</cell><cell>0.76593</cell></row><row><cell>R. Torrejón</cell><cell>2013</cell><cell>0.95256</cell><cell>0.63370</cell><cell></cell><cell>0.81124</cell><cell>0.21593</cell><cell>0.76190</cell></row><row><cell>Palkovskii</cell><cell>2012</cell><cell>0.99379</cell><cell>0.75130</cell><cell></cell><cell>0.66672</cell><cell>0.16089</cell><cell>0.76181</cell></row><row><cell>R. Torrejón</cell><cell>2012</cell><cell>0.96414</cell><cell>0.60283</cell><cell></cell><cell>0.79092</cell><cell>0.29007</cell><cell>0.75324</cell></row><row><cell>Shrestha</cell><cell>2013</cell><cell>0.99902</cell><cell>0.71461</cell><cell></cell><cell>0.63618</cell><cell>0.09897</cell><cell>0.73814</cell></row><row><cell>Suchomel</cell><cell>2012</cell><cell>0.99835</cell><cell>0.51946</cell><cell></cell><cell>0.50106</cell><cell>0.35305</cell><cell>0.64667</cell></row><row><cell>Sánchez-Vega</cell><cell>2012</cell><cell>0.74452</cell><cell>0.43502</cell><cell></cell><cell>0.58133</cell><cell>0.22161</cell><cell>0.56225</cell></row><row><cell>Palkovskii</cell><cell>2013</cell><cell>0.85048</cell><cell>0.36420</cell><cell></cell><cell>0.49667</cell><cell>0.08082</cell><cell>0.53561</cell></row><row><cell>Kueppers</cell><cell>2012</cell><cell>0.83854</cell><cell>0.36865</cell><cell></cell><cell>0.42427</cell><cell>0.09265</cell><cell>0.51074</cell></row><row><cell>Nourian</cell><cell>2013</cell><cell>0.87626</cell><cell>0.23609</cell><cell></cell><cell>0.28568</cell><cell>0.07622</cell><cell>0.43381</cell></row><row><cell>Jayapal</cell><cell>2013</cell><cell>0.86040</cell><cell>0.18182</cell><cell></cell><cell>0.19411</cell><cell>0.07236</cell><cell>0.38187</cell></row><row><cell>Baseline</cell><cell></cell><cell>0.99960</cell><cell>0.04181</cell><cell></cell><cell>0.08804</cell><cell>0.03649</cell><cell>0.34223</cell></row><row><cell>Gillam</cell><cell>2012</cell><cell>0.87187</cell><cell>0.02422</cell><cell></cell><cell>0.00616</cell><cell>0.00109</cell><cell>0.26994</cell></row><row><cell>Gillam</cell><cell>2013</cell><cell>0.83788</cell><cell>0.02142</cell><cell></cell><cell>0.00616</cell><cell>0.00109</cell><cell>0.25890</cell></row><row><cell>Jayapal</cell><cell>2012</cell><cell>0.51885</cell><cell>0.11148</cell><cell></cell><cell>0.09195</cell><cell>0.04574</cell><cell>0.22287</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="23,134.77,130.22,345.82,514.32"><head>Table 6 .</head><label>6</label><figDesc>Cross-year evaluation of text alignment software submissions for 2012 and 2013 with respect to granularity. The darker a cell, the better the performance compared to the entire column.</figDesc><table coords="23,134.89,164.03,345.57,480.52"><row><cell cols="2">Software Submission</cell><cell cols="5">Obfuscation Strategies of the 2012 Evaluation Corpus</cell><cell>Entire Corpus</cell></row><row><cell>Team</cell><cell>Year</cell><cell>None</cell><cell cols="2">Random low Random high</cell><cell cols="2">Translation Man. paraphrase</cell><cell></cell></row><row><cell>Suchomel</cell><cell>2012</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00000</cell></row><row><cell>Suchomel</cell><cell>2013</cell><cell>1.00000</cell><cell>1.00190</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00040</cell></row><row><cell>R. Torrejón</cell><cell>2012</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00249</cell><cell>1.00084</cell></row><row><cell>Oberreuter</cell><cell>2012</cell><cell>1.00000</cell><cell>1.02602</cell><cell>1.00763</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00610</cell></row><row><cell>Kong</cell><cell>2012</cell><cell>1.00000</cell><cell>1.04024</cell><cell>1.02899</cell><cell>1.00000</cell><cell>1.00251</cell><cell>1.01105</cell></row><row><cell>Kong</cell><cell>2013</cell><cell>1.00000</cell><cell>1.04192</cell><cell>1.03318</cell><cell>1.00000</cell><cell>1.00248</cell><cell>1.01172</cell></row><row><cell>Palkovskii</cell><cell>2012</cell><cell>1.01005</cell><cell>1.06526</cell><cell>1.02232</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.01809</cell></row><row><cell>R. Torrejón</cell><cell>2013</cell><cell>1.00000</cell><cell>1.07692</cell><cell>1.05556</cell><cell>1.00000</cell><cell>1.00828</cell><cell>1.02050</cell></row><row><cell>Gillam</cell><cell>2013</cell><cell>1.00177</cell><cell>1.20455</cell><cell>1.88889</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.02429</cell></row><row><cell>Gillam</cell><cell>2012</cell><cell>1.00178</cell><cell>1.20455</cell><cell>1.88889</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.02436</cell></row><row><cell>Nourian</cell><cell>2013</cell><cell>1.00175</cell><cell>1.30997</cell><cell>1.23636</cell><cell>1.00000</cell><cell>1.07590</cell><cell>1.09426</cell></row><row><cell>Kueppers</cell><cell>2012</cell><cell>1.10980</cell><cell>1.29514</cell><cell>1.00000</cell><cell>1.28818</cell><cell>1.46228</cell><cell>1.27635</cell></row><row><cell cols="2">Sánchez-Vega 2012</cell><cell>1.22826</cell><cell>1.81558</cell><cell>1.36471</cell><cell>1.71117</cell><cell>1.71099</cell><cell>1.58308</cell></row><row><cell>Palkovskii</cell><cell>2013</cell><cell>1.01386</cell><cell>2.84501</cell><cell>1.81579</cell><cell>1.51816</cell><cell>1.37062</cell><cell>1.63842</cell></row><row><cell>Baseline</cell><cell></cell><cell>1.01340</cell><cell>1.57021</cell><cell>1.00000</cell><cell>2.19824</cell><cell>4.51313</cell><cell>2.27432</cell></row><row><cell>Jayapal</cell><cell>2012</cell><cell>14.36813</cell><cell>3.59490</cell><cell>2.08333</cell><cell>2.74925</cell><cell>3.11826</cell><cell>6.28323</cell></row><row><cell>Jayapal</cell><cell>2013</cell><cell>49.36929</cell><cell>5.89810</cell><cell>2.37129</cell><cell>4.99491</cell><cell>6.98626</cell><cell>16.78476</cell></row><row><cell cols="2">Software Submission</cell><cell cols="5">Obfuscation Strategies of the 2013 Evaluation Corpus</cell><cell>Entire Corpus</cell></row><row><cell>Team</cell><cell>Year</cell><cell>None</cell><cell>Random</cell><cell cols="2">Cyclic translation</cell><cell>Summary</cell><cell></cell></row><row><cell>Gillam</cell><cell>2012</cell><cell>1.00000</cell><cell>1.00000</cell><cell></cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00000</cell></row><row><cell>Gillam</cell><cell>2013</cell><cell>1.00000</cell><cell>1.00000</cell><cell></cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00000</cell></row><row><cell>Oberreuter</cell><cell>2012</cell><cell>1.00000</cell><cell>1.00000</cell><cell></cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00000</cell></row><row><cell>Palkovskii</cell><cell>2012</cell><cell>1.00000</cell><cell>1.00000</cell><cell></cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00000</cell></row><row><cell>R. Torrejón</cell><cell>2012</cell><cell>1.00000</cell><cell>1.00000</cell><cell></cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00000</cell></row><row><cell>Suchomel</cell><cell>2013</cell><cell>1.00000</cell><cell>1.00000</cell><cell></cell><cell>1.00000</cell><cell>1.00476</cell><cell>1.00028</cell></row><row><cell>Suchomel</cell><cell>2012</cell><cell>1.00000</cell><cell>1.00000</cell><cell></cell><cell>1.00000</cell><cell>1.00610</cell><cell>1.00032</cell></row><row><cell>R. Torrejón</cell><cell>2013</cell><cell>1.00000</cell><cell>1.00000</cell><cell></cell><cell>1.00000</cell><cell>1.03086</cell><cell>1.00141</cell></row><row><cell>Kong</cell><cell>2012</cell><cell>1.00000</cell><cell>1.00000</cell><cell></cell><cell>1.00000</cell><cell>1.06452</cell><cell>1.00282</cell></row><row><cell>Kong</cell><cell>2013</cell><cell>1.00000</cell><cell>1.00000</cell><cell></cell><cell>1.00000</cell><cell>1.07742</cell><cell>1.00336</cell></row><row><cell>Sánchez-Vega</cell><cell>2012</cell><cell>1.00394</cell><cell>1.02200</cell><cell></cell><cell>1.03533</cell><cell>1.04523</cell><cell>1.02196</cell></row><row><cell>Kueppers</cell><cell>2012</cell><cell>1.02687</cell><cell>1.01847</cell><cell></cell><cell>1.01794</cell><cell>1.31061</cell><cell>1.03497</cell></row><row><cell>Nourian</cell><cell>2013</cell><cell>1.00092</cell><cell>1.11558</cell><cell></cell><cell>1.00485</cell><cell>1.34234</cell><cell>1.04343</cell></row><row><cell>Palkovskii</cell><cell>2013</cell><cell>1.00000</cell><cell>1.06785</cell><cell></cell><cell>1.02825</cell><cell>1.73596</cell><cell>1.07295</cell></row><row><cell>Shrestha</cell><cell>2013</cell><cell>1.00083</cell><cell>1.30962</cell><cell></cell><cell>1.26184</cell><cell>1.83696</cell><cell>1.22084</cell></row><row><cell>Saremi</cell><cell>2013</cell><cell>1.06007</cell><cell>1.29511</cell><cell></cell><cell>1.24204</cell><cell>2.15556</cell><cell>1.24450</cell></row><row><cell>Baseline</cell><cell></cell><cell>1.00912</cell><cell>1.18239</cell><cell></cell><cell>1.86726</cell><cell>1.97436</cell><cell>1.27473</cell></row><row><cell>Jayapal</cell><cell>2012</cell><cell>2.87916</cell><cell>2.15530</cell><cell></cell><cell>2.00578</cell><cell>2.75743</cell><cell>2.45403</cell></row><row><cell>Jayapal</cell><cell>2013</cell><cell>3.90017</cell><cell>2.19096</cell><cell></cell><cell>2.34218</cell><cell>3.60987</cell><cell>2.90698</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,635.17,335.85,7.77;1,144.73,646.13,335.85,7.77;1,144.73,657.09,118.88,7.77"><p>Some of the concepts found in this paper have been described earlier, so that, because of the inherently incremental nature of evaluations, and in order for this paper to be self-contained, we reuse text from these sources.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,144.73,624.05,335.87,7.93;2,144.73,635.17,335.86,7.77;2,144.73,646.13,335.86,7.77;2,144.73,657.08,75.21,7.77"><p>Another approach to detect plagiarism is called intrinsic plagiarism detection, where detectors are given only a suspicious document and are supposed to identify text passages in them which deviate in their style from the remainder of the document. This year, we focus on external plagiarism detection.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="5,144.73,619.14,122.39,7.77"><p>http://lemurproject.org/clueweb09</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="5,144.73,630.29,65.63,7.77"><p>http://trec.nist.gov</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="5,144.73,641.44,122.39,7.77"><p>http://lemurproject.org/clueweb12</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="5,144.73,652.59,195.23,7.77"><p>http://lemurproject.org/clueweb09/index.php#Services</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="13,144.73,613.01,335.86,7.77;13,144.73,623.96,335.86,7.77;13,144.73,634.92,335.86,7.77;13,144.73,645.88,335.86,7.77;13,144.73,656.84,105.35,7.77"><p>Note that the duplicate detector differs from that applied in source retrieval. In source retrieval, we adjusted the duplicate detector to maximize precision and therefore false positive detections, whereas in text alignment, we maximize recall for the same reason (i.e., to make sure that no unintended duplication is found between pairs of documents that may lead text alignment algorithms astray).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="14,144.73,634.79,96.14,7.77"><p>http://translate.google.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8" coords="14,144.73,645.94,130.92,7.77"><p>http://www.microsoft.com/translator</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9" coords="14,144.73,657.08,112.49,7.77"><p>http://mymemory.translated.net</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10" coords="15,144.73,631.10,213.32,7.77"><p>http://www-nlpir.nist.gov/projects/duc/data/2001_data.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_11" coords="15,144.73,642.25,213.32,7.77"><p>http://www-nlpir.nist.gov/projects/duc/data/2006_data.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank the participating teams of this task for their devoted work. This paper was partially supported by the <rs type="programName">WIQ-EI IRSES project</rs> (Grant No. <rs type="grantNumber">269180</rs>) within the <rs type="funder">FP7 Marie Curie</rs> action.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_YQAUtnw">
					<idno type="grant-number">269180</idno>
					<orgName type="program" subtype="full">WIQ-EI IRSES project</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="27,156.34,455.36,324.25,8.64;27,156.34,467.14,312.56,8.82;27,156.34,479.09,317.89,8.59;27,156.34,491.05,273.45,8.59;27,156.34,503.00,318.02,8.82;27,156.34,515.14,152.34,8.64" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="27,299.26,455.36,181.34,8.64;27,156.34,467.32,42.28,8.64">Using noun phrase heads to extract document keyphrases</title>
		<author>
			<persName coords=""><forename type="first">Ken</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nadia</forename><surname>Cornacchia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="27,330.76,467.14,138.15,8.59;27,156.34,479.09,317.89,8.59;27,156.34,491.05,81.64,8.59">Advances in Artificial Intelligence, 13th Biennial Conference of the Canadian Society for Computational Studies of Intelligence, AI 2000</title>
		<title level="s" coord="27,276.06,503.00,139.13,8.59">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Howard</forename><forename type="middle">J</forename><surname>Hamilton</surname></persName>
		</editor>
		<meeting><address><addrLine>Montréal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000">May 14-17, 2000. 2000</date>
			<biblScope unit="volume">1822</biblScope>
			<biblScope unit="page" from="40" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,156.34,526.55,317.50,8.64;27,156.34,538.33,316.82,8.82;27,156.34,550.46,22.42,8.64" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="27,337.84,526.55,136.00,8.64;27,156.34,538.51,160.34,8.64">Kp-miner: A keyphrase extraction system for english and arabic documents</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Samhaa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ahmed</forename><forename type="middle">A</forename><surname>El-Beltagy</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Rafea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="27,324.84,538.33,79.87,8.59">Information Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="132" to="144" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,156.34,561.88,264.21,8.64;27,156.34,573.83,268.16,8.64" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="27,229.61,561.88,190.94,8.64;27,156.34,573.83,164.01,8.64">Using Statistic and Semantic Analysis to Detect Plagiarism-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Victoria</forename><surname>Elizalde</surname></persName>
		</author>
		<editor>Forner et al.</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,156.34,585.25,318.45,8.64;27,156.34,597.02,307.37,8.82;27,156.34,608.98,257.80,8.82" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="27,360.43,585.25,114.37,8.64;27,156.34,597.20,116.31,8.64">On the Evolution of Clusters of Near-Duplicate Web Pages</title>
		<author>
			<persName coords=""><forename type="first">Dennis</forename><surname>Fetterly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Manasse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="27,291.30,597.02,172.41,8.59;27,156.34,608.98,35.34,8.59">Proceedings of the 1st Latin American Web Congress</title>
		<meeting>the 1st Latin American Web Congress<address><addrLine>LA-WEB</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,156.34,620.39,322.36,8.82;27,156.34,632.35,324.24,8.59;27,156.34,644.30,178.46,8.82;27,156.34,656.44,222.72,8.64" xml:id="b4">
	<monogr>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
		<title level="m" coord="27,431.93,620.39,46.77,8.59;27,156.34,632.35,221.01,8.59">CLEF 2012 Evaluation Labs and Workshop -Working Notes Papers</title>
		<editor>
			<persName><forename type="first">Pamela</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jussi</forename><surname>Karlgren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Christa</forename><surname>Womser-Hacker</surname></persName>
		</editor>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-09-20">17-20 September. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,156.34,119.13,319.09,8.82;28,156.34,131.09,318.26,8.59;28,156.34,143.22,271.14,8.64" xml:id="b5">
	<monogr>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
		<title level="m" coord="28,382.44,119.13,92.99,8.59;28,156.34,131.09,175.38,8.59">CLEF 2013 Evaluation Labs and Workshop -Working Notes Papers</title>
		<editor>
			<persName><forename type="first">Pamela</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Dan</forename><surname>Tufis</surname></persName>
		</editor>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-09">September. 2013</date>
			<biblScope unit="page" from="23" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,156.34,155.18,318.96,8.64;28,156.34,167.13,264.28,8.64" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="28,207.52,155.18,267.78,8.64;28,156.34,167.13,160.13,8.64">Guess Again and See if They Line Up: Surrey&apos;s Runs at Plagiarism Detection-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Lee</forename><surname>Gillam</surname></persName>
		</author>
		<editor>Forner et al.</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,156.34,179.09,304.84,8.64;28,156.34,191.04,317.23,8.64;28,156.34,203.00,290.56,8.64;28,156.34,214.95,104.03,8.64;28,156.34,226.91,222.72,8.64" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="28,335.84,179.09,125.34,8.64;28,156.34,191.04,317.23,8.64;28,156.34,203.00,160.13,8.64">Educated Guesses and Equality Judgements: Using Search Engines and Pairwise Match for External Plagiarism Detection-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Lee</forename><surname>Gillam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Neil</forename><surname>Newbold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Neil</forename><surname>Cooke</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
		<editor>Forner et al.</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,156.34,238.86,316.24,8.64;28,156.34,250.82,299.69,8.64;28,156.34,262.60,286.10,8.82;28,156.34,274.55,308.02,8.59;28,156.34,286.51,270.06,8.82;28,156.34,298.64,279.32,8.64" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="28,347.94,238.86,124.64,8.64;28,156.34,250.82,266.32,8.64">Ousting Ivory Tower Research: Towards a Web Framework for Providing Experiments as a Service</title>
		<author>
			<persName coords=""><forename type="first">Tim</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>Burrows</surname></persName>
		</author>
		<idno type="DOI">10.1145/2348283.2348501</idno>
		<ptr target="http://dx.doi.org/10.1145/2348283.2348501" />
	</analytic>
	<monogr>
		<title level="m" coord="28,156.34,274.55,308.02,8.59;28,156.34,286.51,81.40,8.59">International ACM Conference on Research and Development in Information Retrieval (SIGIR 12)</title>
		<editor>
			<persName><forename type="first">Bill</forename><surname>Hersh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yoelle</forename><surname>Maarek</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mark</forename><surname>Sanderson</surname></persName>
		</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012-08">August 2012</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1125" to="1126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,156.34,310.60,308.28,8.64;28,156.34,322.37,294.69,8.82;28,156.34,334.33,126.74,8.82" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="28,401.24,310.60,63.38,8.64;28,156.34,322.55,110.97,8.64">New indices for text: Pat trees and pat arrays</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Gaston</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ricardo</forename><forename type="middle">A</forename><surname>Gonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tim</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Snider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="28,285.52,322.37,165.52,8.59;28,156.34,334.33,42.52,8.59">Information Retrieval: Data Structures &amp; Algorithms</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="66" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,156.34,346.46,316.12,8.64;28,156.34,358.24,289.60,8.82;28,156.34,370.19,306.64,8.59;28,156.34,382.15,301.05,8.82;28,156.34,394.28,172.23,8.64;28,156.34,406.24,197.97,8.64" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="28,294.14,346.46,178.33,8.64;28,156.34,358.42,84.93,8.64">Applying the User-over-Ranking Hypothesis to Query Formulation</title>
		<author>
			<persName coords=""><forename type="first">Matthias</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-23318-0_21</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-642-23318-0_21" />
	</analytic>
	<monogr>
		<title level="m" coord="28,259.95,358.24,185.99,8.59;28,156.34,370.19,302.69,8.59">Advances in Information Retrieval Theory. 3rd International Conference on the Theory of Information Retrieval (ICTIR 11)</title>
		<title level="s" coord="28,221.73,382.15,139.13,8.59">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">6931</biblScope>
			<biblScope unit="page" from="225" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,156.34,418.19,320.21,8.64;28,156.34,429.97,308.62,8.82;28,156.34,441.92,287.90,8.82;28,156.34,453.88,323.31,8.82;28,156.34,466.01,215.96,8.64" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="28,294.14,418.19,182.42,8.64;28,156.34,430.15,83.85,8.64">Candidate Document Retrieval for Web-Scale Text Reuse Detection</title>
		<author>
			<persName coords=""><forename type="first">Matthias</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-24583-1_35</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-642-24583-1_35" />
	</analytic>
	<monogr>
		<title level="m" coord="28,258.69,429.97,206.28,8.59;28,156.34,441.92,150.59,8.59">18th International Symposium on String Processing and Information Retrieval (SPIRE 11)</title>
		<title level="s" coord="28,378.75,441.92,65.49,8.59;28,156.34,453.88,71.14,8.59">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">7024</biblScope>
			<biblScope unit="page" from="356" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,156.34,477.97,308.98,8.64;28,156.34,489.92,317.54,8.64;28,156.34,501.88,159.69,8.64" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="28,315.01,477.97,150.32,8.64;28,156.34,489.92,317.54,8.64;28,156.34,501.88,55.54,8.64">Plagiarism Candidate Retrieval Using Selective Query Formulation and Discriminative Query Scoring-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Osama</forename><surname>Haggag</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Smhaa</forename><surname>El-Beltagy</surname></persName>
		</author>
		<editor>Forner et al.</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,156.34,513.83,301.23,8.64;28,156.34,525.61,282.25,8.82;28,156.34,537.74,246.83,8.64" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="28,225.23,513.83,232.35,8.64;28,156.34,525.79,33.20,8.64">TextTiling: Segmenting text into multi-paragraph subtopic passages</title>
		<author>
			<persName coords=""><forename type="first">Marti</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
		<ptr target="http://acl.ldc.upenn.edu/J/J97/J97-1003.pdf" />
	</analytic>
	<monogr>
		<title level="j" coord="28,197.28,525.61,104.65,8.59">Computational Linguistics</title>
		<idno type="ISSN">0891-2017</idno>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="64" />
			<date type="published" when="1997-03">March 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,156.34,549.70,323.43,8.64;28,156.34,561.65,317.98,8.64;28,156.34,573.61,146.91,8.64;28,156.34,585.56,222.72,8.64" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="28,243.21,549.70,236.56,8.64;28,156.34,561.65,230.42,8.64">Similarity Overlap Metric and Greedy String Tiling at PAN 2012: Plagiarism Detection-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Arun</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jayapal</forename></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
		<editor>Forner et al.</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,156.34,597.52,322.11,8.64;28,156.34,609.47,151.10,8.64;28,156.34,621.43,227.50,8.64;28,156.34,633.39,280.82,8.64;28,156.34,645.34,95.48,8.64" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Arun</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jayapal</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Binayak</forename><surname>Goswami</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
		<title level="m" coord="28,338.42,597.52,140.03,8.64;28,156.34,609.47,146.98,8.64">Submission to the 5th International Competition on Plagiarism Detection</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Nuance Communications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="29,156.34,119.31,304.67,8.64;29,156.34,131.27,324.24,8.64;29,156.34,143.22,309.39,8.64;29,156.34,155.18,129.21,8.64;29,156.34,167.13,222.72,8.64" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="29,178.89,131.27,301.70,8.64;29,156.34,143.22,205.24,8.64">Approaches for Candidate Document Retrieval and Detailed Comparison of Plagiarism Detection-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Leilei</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haoliang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cuixia</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Suhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yong</forename><surname>Han</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
		<editor>Forner et al.</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="29,156.34,179.09,306.28,8.64;29,156.34,191.04,268.15,8.64;29,156.34,203.00,264.28,8.64" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="29,156.34,191.04,268.15,8.64;29,156.34,203.00,160.13,8.64">Approaches for Source Retrieval and Text Alignment of Plagiarism Detection-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Leilei</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haoliang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cuixia</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mingxing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhongyuan</forename><surname>Han</surname></persName>
		</author>
		<editor>Forner et al.</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="29,156.34,214.95,312.32,8.64;29,156.34,226.91,307.61,8.64;29,156.34,238.86,263.72,8.64" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="29,417.28,214.95,51.38,8.64;29,156.34,226.91,307.61,8.64;29,156.34,238.86,159.57,8.64">CopyCaptor: Plagiarized Source Retrieval System using Global Word frequency and Local Feedback-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Taemin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeongmin</forename><surname>Chae</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kinam</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Soonyoung</forename><surname>Jung</surname></persName>
		</author>
		<editor>Forner et al.</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="29,156.34,250.82,291.27,8.64;29,156.34,262.60,324.25,8.82;29,156.34,274.73,273.12,8.64" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="29,305.37,250.82,142.24,8.64;29,156.34,262.77,163.75,8.64">Combining the language model and inference network approaches to retrieval</title>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ipm.2004.05.001</idno>
	</analytic>
	<monogr>
		<title level="j" coord="29,327.18,262.60,84.57,8.59">Inf. Process. Manage</title>
		<idno type="ISSN">0306-4573</idno>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="735" to="750" />
			<date type="published" when="2004-09">September 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="29,156.34,286.69,320.45,8.64;29,156.34,298.64,272.31,8.64;29,156.34,310.60,281.09,8.64;29,156.34,322.55,154.68,8.64" xml:id="b20">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Alireza</forename><surname>Nourian</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
		<title level="m" coord="29,226.88,286.69,249.91,8.64;29,156.34,298.64,37.10,8.64">Submission to the 5th International Competition on Plagiarism Detection</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
		<respStmt>
			<orgName>University of Science and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="29,156.34,334.51,304.84,8.64;29,156.34,346.46,296.38,8.64;29,156.34,358.42,322.12,8.64;29,156.34,370.37,284.98,8.64;29,156.34,382.33,262.36,8.64" xml:id="b21">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Gabriel</forename><surname>Oberreuter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Carrillo-Cisneros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Isaac</forename><forename type="middle">D</forename><surname>Scherson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Juan</forename><forename type="middle">D</forename><surname>Velásquez</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
		<title level="m" coord="29,202.81,346.46,249.91,8.64;29,156.34,358.42,37.10,8.64">Submission to the 4th International Competition on Plagiarism Detection</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint/>
		<respStmt>
			<orgName>University of Chile, Chile, and the University of California</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="29,156.34,394.28,288.79,8.64;29,156.34,406.24,299.73,8.64;29,156.34,418.19,307.18,8.64;29,156.34,430.15,168.77,8.64;29,156.34,442.10,222.72,8.64" xml:id="b22">
	<monogr>
		<title level="m" type="main" coord="29,297.63,394.28,147.50,8.64;29,156.34,406.24,299.73,8.64;29,156.34,418.19,241.48,8.64">Applying Specific Clusterization and Fingerprint Density Distribution with Genetic Algorithm Overall Tuning in External Plagiarism Detection-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Yurii</forename><surname>Palkovskii</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexei</forename><surname>Belov</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
		<editor>Forner et al.</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="29,156.34,454.06,289.62,8.64;29,156.34,466.01,309.39,8.64" xml:id="b23">
	<monogr>
		<title level="m" type="main" coord="29,297.63,454.06,148.33,8.64;29,156.34,466.01,205.24,8.64">Using Hybrid Similarity Methods for Plagiarism Detection-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Yurii</forename><surname>Palkovskii</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexei</forename><surname>Belov</surname></persName>
		</author>
		<editor>Forner et al.</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="29,156.34,477.79,298.52,8.82;29,156.34,489.92,209.22,8.64;29,156.34,501.88,316.00,8.64" xml:id="b24">
	<monogr>
		<title level="m" type="main" coord="29,224.14,477.79,171.68,8.59">Technologies for Reusing Text from the Web</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<ptr target="http://nbn-resolving.de/urn/resolver.pl?urn:nbn:de:gbv:wim2-20120217-15663" />
		<imprint>
			<date type="published" when="2011-12">December 2011</date>
		</imprint>
		<respStmt>
			<orgName>Bauhaus-Universität Weimar</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Dissertation</note>
</biblStruct>

<biblStruct coords="29,156.34,513.83,322.78,8.64;29,156.34,525.79,315.88,8.64;29,156.34,537.74,315.98,8.64;29,156.34,549.52,324.24,8.82;29,156.34,561.48,315.67,8.82;29,156.34,573.61,130.33,8.64" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="29,186.78,525.79,281.33,8.64">Overview of the 1st International Competition on Plagiarism Detection</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andreas</forename><surname>Eiselt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-502" />
	</analytic>
	<monogr>
		<title level="m" coord="29,219.42,549.52,261.17,8.59;29,156.34,561.48,131.22,8.59">SEPLN 09 Workshop on Uncovering Plagiarism, Authorship, and Social Software Misuse (PAN 09)</title>
		<editor>
			<persName><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Efstathios</forename><surname>Stamatatos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Moshe</forename><surname>Koppel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2009-09">September 2009</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="29,156.34,585.56,322.78,8.64;29,156.34,597.52,319.20,8.64;29,156.34,609.30,303.87,8.82;29,156.34,621.25,293.55,8.82;29,156.34,633.39,104.03,8.64;29,156.34,645.34,222.72,8.64" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="29,186.78,597.52,284.64,8.64">Overview of the 2nd International Competition on Plagiarism Detection</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andreas</forename><surname>Eiselt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
	</analytic>
	<monogr>
		<title level="m" coord="29,426.82,609.30,33.39,8.59;29,156.34,621.25,192.88,8.59">Working Notes Papers of the CLEF 2010 Evaluation Labs</title>
		<editor>
			<persName><forename type="first">Martin</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Donna</forename><surname>Harman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Emanuele</forename><surname>Pianta</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2010-09">September 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,156.34,119.31,304.52,8.64;30,156.34,131.27,309.29,8.64;30,156.34,143.04,312.56,8.82;30,156.34,155.00,295.47,8.82;30,156.34,167.13,172.13,8.64" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="30,448.69,119.31,12.17,8.64;30,156.34,131.27,189.80,8.64">An Evaluation Framework for Plagiarism Detection</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="30,247.32,143.04,221.59,8.59;30,156.34,155.00,54.17,8.59">International Conference on Computational Linguistics (COLING 10)</title>
		<editor>
			<persName><forename type="first">Chu-Ren</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</editor>
		<meeting><address><addrLine>Stroudsburg, Pennsylvania</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010-08">August 2010</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="997" to="1005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,156.34,179.09,322.78,8.64;30,156.34,191.04,317.54,8.64;30,156.34,202.82,305.65,8.82;30,156.34,214.78,268.36,8.82;30,156.34,226.91,104.03,8.64;30,156.34,238.86,222.72,8.64" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="30,186.78,191.04,282.98,8.64">Overview of the 3rd International Competition on Plagiarism Detection</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andreas</forename><surname>Eiselt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
	</analytic>
	<monogr>
		<title level="m" coord="30,403.42,202.82,58.58,8.59;30,156.34,214.78,167.70,8.59">Working Notes Papers of the CLEF 2011 Evaluation Labs</title>
		<editor>
			<persName><forename type="first">Vivien</forename><surname>Petras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pamela</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2011-09">September 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,156.34,250.82,315.65,8.64;30,156.34,262.77,260.18,8.64;30,156.34,274.73,324.24,8.64;30,156.34,286.69,300.92,8.64;30,156.34,298.46,303.26,8.82;30,156.34,310.42,320.16,8.82;30,156.34,322.55,222.72,8.64" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="30,401.21,274.73,79.37,8.64;30,156.34,286.69,200.38,8.64">Overview of the 4th International Competition on Plagiarism Detection</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tim</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthias</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jan</forename><surname>Graßegger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maximilian</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arnd</forename><surname>Oberländer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Tippmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Parth</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
	</analytic>
	<monogr>
		<title level="m" coord="30,346.30,298.46,113.30,8.59;30,156.34,310.42,112.97,8.59">Working Notes Papers of the CLEF 2012 Evaluation Labs</title>
		<editor>
			<persName><forename type="first">Pamela</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jussi</forename><surname>Karlgren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Christa</forename><surname>Womser-Hacker</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2012-09">September 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,156.34,334.51,297.75,8.64;30,156.34,346.46,320.65,8.64;30,156.34,358.42,314.36,8.64;30,156.34,370.19,291.57,8.82;30,156.34,382.15,305.33,8.82;30,156.34,394.28,150.79,8.64;30,156.34,406.24,177.22,8.64" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="30,353.04,346.46,123.95,8.64;30,156.34,358.42,90.75,8.64">ChatNoir: A Search Engine for the ClueWeb09 Corpus</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthias</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jan</forename><surname>Graßegger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maximilian</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Tippmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Clement</forename><surname>Welsch</surname></persName>
		</author>
		<idno type="DOI">10.1145/2348283.2348429</idno>
		<ptr target="http://dx.doi.org/10.1145/2348283.2348429" />
	</analytic>
	<monogr>
		<title level="m" coord="30,255.13,370.19,192.79,8.59;30,156.34,382.15,196.63,8.59">International ACM Conference on Research and Development in Information Retrieval (SIGIR 12)</title>
		<editor>
			<persName><forename type="first">Bill</forename><surname>Hersh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yoelle</forename><surname>Maarek</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mark</forename><surname>Sanderson</surname></persName>
		</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012-08">August 2012</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">1004</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,156.34,418.19,271.64,8.64;30,156.34,429.97,324.25,8.82;30,156.34,441.92,309.67,8.59;30,156.34,453.88,208.91,8.82" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="30,156.34,430.15,288.37,8.64">Crowdsourcing Interaction Logs to Understand Text Reuse from the Web</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthias</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Völske</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="http://dx.doi.org/" />
	</analytic>
	<monogr>
		<title level="m" coord="30,463.98,429.97,16.61,8.59;30,156.34,441.92,309.67,8.59;30,156.34,453.88,29.78,8.59">51st Annual Meeting of the Association of Computational Linguistics (ACL 13) (to appear)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013-08">August 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,156.34,466.01,306.62,8.64;30,156.34,477.79,311.24,8.82;30,156.34,489.74,284.63,8.59;30,156.34,501.70,262.69,8.82" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="30,406.77,466.01,56.19,8.64;30,156.34,477.97,146.13,8.64">Simple BM25 extension to multiple weighted fields</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="30,320.29,477.79,147.30,8.59;30,156.34,489.74,279.69,8.59">Proceedings of the 2004 ACM CIKM International Conference on Information and Knowledge Management</title>
		<meeting>the 2004 ACM CIKM International Conference on Information and Knowledge Management<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">November 8-13, 2004. 2004</date>
			<biblScope unit="page" from="42" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,156.34,513.83,314.75,8.64;30,156.34,525.79,323.21,8.64;30,156.34,537.74,75.66,8.64" xml:id="b33">
	<monogr>
		<title level="m" type="main" coord="30,408.57,513.83,62.52,8.64;30,156.34,525.79,298.30,8.64">Text Alignment Module in CoReMo 2.1 Plagiarism Detector-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Diego</forename><forename type="middle">A</forename><surname>Rodríguez Torrejón</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">José</forename><surname>Manuel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martín</forename><surname>Ramos</surname></persName>
		</author>
		<editor>Forner et al.</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,156.34,549.70,294.92,8.64;30,156.34,561.65,151.10,8.64;30,156.34,573.61,227.50,8.64;30,156.34,585.56,282.49,8.64;30,156.34,597.52,64.53,8.64" xml:id="b34">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Mehrin</forename><surname>Saremi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Farzin</forename><surname>Yaghmaee</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
		<title level="m" coord="30,311.23,549.70,140.03,8.64;30,156.34,561.65,146.98,8.64">Submission to the 5th International Competition on Plagiarism Detection</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
		<respStmt>
			<orgName>Semnan University, Iran</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="30,156.34,609.47,315.71,8.64;30,156.34,621.25,309.28,8.82;30,156.34,633.21,315.62,8.59;30,156.34,645.16,164.53,8.82" xml:id="b35">
	<analytic>
		<title level="a" type="main" coord="30,280.04,609.47,192.02,8.64;30,156.34,621.43,193.95,8.64">Constructing web search queries from the user&apos;s information need expressed in a natural language</title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Isak</forename><surname>Taksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="30,368.95,621.25,96.68,8.59;30,156.34,633.21,185.92,8.59">Proceedings of the 2003 ACM Symposium on Applied Computing (SAC)</title>
		<meeting>the 2003 ACM Symposium on Applied Computing (SAC)<address><addrLine>Melbourne, FL, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">March 9-12, 2003. 2003</date>
			<biblScope unit="page" from="1157" to="1162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,156.34,119.31,290.51,8.64;31,156.34,131.27,316.77,8.64;31,156.34,143.22,75.66,8.64" xml:id="b36">
	<monogr>
		<title level="m" type="main" coord="31,307.96,119.31,138.89,8.64;31,156.34,131.27,291.86,8.64">Using a Variety of n-Grams for the Detection of Different Kinds of Plagiarism-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Prasha</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</author>
		<editor>Forner et al.</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,156.34,155.00,318.26,8.82;31,156.34,167.13,266.60,8.64" xml:id="b37">
	<analytic>
		<title level="a" type="main" coord="31,248.49,155.00,186.44,8.82">Plagiarism Detection Using Stopword n-Grams</title>
		<author>
			<persName coords=""><forename type="first">Efstathios</forename><surname>Stamatatos</surname></persName>
		</author>
		<idno type="DOI">10.1002/asi.21630</idno>
		<ptr target="http://dx.doi.org/10.1002/asi.21630" />
	</analytic>
	<monogr>
		<title level="j" coord="31,443.18,155.00,26.93,8.59">JASIST</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2512" to="2527" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,156.34,179.09,277.65,8.64;31,156.34,190.86,324.24,8.82;31,156.34,202.82,301.35,8.82;31,156.34,214.95,288.18,8.64;31,156.34,226.91,197.97,8.64" xml:id="b38">
	<analytic>
		<title level="a" type="main" coord="31,294.14,179.09,139.85,8.64;31,156.34,191.04,43.02,8.64">Introducing the User-over-Ranking Hypothesis</title>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthias</forename><surname>Hagen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-20161-5_50</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-642-20161-5_50" />
	</analytic>
	<monogr>
		<title level="m" coord="31,218.01,190.86,262.57,8.59;31,156.34,202.82,86.29,8.59">Advances in Information Retrieval. 33rd European Conference on IR Resarch (ECIR 11)</title>
		<title level="s" coord="31,314.45,202.82,139.13,8.59">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011-04">April 2011</date>
			<biblScope unit="volume">6611</biblScope>
			<biblScope unit="page" from="503" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,156.34,238.86,282.62,8.64;31,156.34,250.82,303.23,8.64;31,156.34,262.60,309.91,8.82;31,156.34,274.55,317.56,8.59;31,156.34,286.69,307.60,8.64;31,156.34,298.64,177.22,8.64" xml:id="b39">
	<analytic>
		<title level="a" type="main" coord="31,385.71,238.86,53.25,8.64;31,156.34,250.82,134.70,8.64">Strategies for Retrieving Plagiarized Documents</title>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Meyer Zu Eißen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<idno type="DOI">10.1145/1277741.1277928</idno>
		<ptr target="http://dx.doi.org/10.1145/1277741.1277928" />
	</analytic>
	<monogr>
		<title level="m" coord="31,370.25,262.60,96.01,8.59;31,156.34,274.55,313.61,8.59">30th International ACM Conference on Research and Development in Information Retrieval (SIGIR 07)</title>
		<editor>
			<persName><forename type="first">Charles</forename><surname>Clarke</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Norbert</forename><surname>Fuhr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Noriko</forename><surname>Kando</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Wessel</forename><surname>Kraaij</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Arjen</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007-07">July 2007</date>
			<biblScope unit="page" from="825" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,156.34,310.60,324.24,8.64;31,156.34,322.55,261.21,8.64;31,156.34,334.51,290.56,8.64;31,156.34,346.46,104.03,8.64;31,156.34,358.42,222.72,8.64" xml:id="b40">
	<monogr>
		<title level="m" type="main" coord="31,376.71,310.60,103.88,8.64;31,156.34,322.55,261.21,8.64;31,156.34,334.51,160.13,8.64">Three Way Search Engine Queries with Multi-feature Document Comparison for Plagiarism Detection-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Šimon</forename><surname>Suchomel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jan</forename><surname>Kasprzak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michal</forename><surname>Brandejs</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
		<editor>Forner et al.</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,156.34,370.37,301.74,8.64;31,156.34,382.33,315.37,8.64;31,156.34,394.28,101.66,8.64" xml:id="b41">
	<monogr>
		<title level="m" type="main" coord="31,377.15,370.37,80.94,8.64;31,156.34,382.33,315.37,8.64">Diverse Queries and Feature Type Selection for Plagiarism Discovery-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Šimon</forename><surname>Suchomel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jan</forename><surname>Kasprzak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michal</forename><surname>Brandejs</surname></persName>
		</author>
		<editor>Forner et al.</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,156.34,406.24,309.13,8.64;31,156.34,418.19,318.80,8.64;31,156.34,430.15,75.66,8.64" xml:id="b42">
	<monogr>
		<title level="m" type="main" coord="31,358.49,406.24,106.98,8.64;31,156.34,418.19,293.90,8.64">Source Retrieval via Naïve Approach and Passage Selection Heuristics-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Ondřej</forename><surname>Veselý</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tomáš</forename><surname>Foltýnek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiří</forename><surname>Rybička</surname></persName>
		</author>
		<editor>Forner et al.</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,156.34,442.10,311.42,8.64;31,156.34,454.06,313.47,8.64;31,156.34,466.01,128.51,8.64" xml:id="b43">
	<monogr>
		<title level="m" type="main" coord="31,156.34,454.06,313.47,8.64;31,156.34,466.01,24.36,8.64">Unsupervised Ranking for Plagiarism Source Retrieval-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Kyle</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hung-Hsuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sagnik</forename><surname>Ray Chowdhury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">Lee</forename><surname>Giles</surname></persName>
		</author>
		<editor>Forner et al.</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
