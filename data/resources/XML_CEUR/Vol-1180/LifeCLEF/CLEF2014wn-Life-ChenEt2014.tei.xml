<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,156.64,116.95,302.07,12.62;1,220.22,134.89,174.92,12.62">IBM Research Australia at LifeCLEF2014: Plant Identification Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,192.64,172.56,49.70,8.74"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research Australia</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,250.77,172.56,58.18,8.74"><forename type="first">Mani</forename><surname>Abedini</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research Australia</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,316.91,172.56,59.72,8.74"><forename type="first">Rahil</forename><surname>Garnavi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research Australia</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,384.67,172.56,38.05,8.74"><forename type="first">Xi</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research Australia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,156.64,116.95,302.07,12.62;1,220.22,134.89,174.92,12.62">IBM Research Australia at LifeCLEF2014: Plant Identification Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">83AD780F77C5E94BDDF6C5D0485E84A2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Fine-grained object recognition</term>
					<term>plant recognition</term>
					<term>deep learning</term>
					<term>feature coding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present the system and learning strategies that were applied by the IBM Research team to the plant identification task of LifeCLEF 2014. Plant identification is one of the most popular fine-grained categorization tasks. To ensure high classification accuracy, we have utilised strong visual features together with fusion of robust machine learning techniques. Our proposed system involves automatic delineation of the region of interest (e.g. plant's leaf, flower, etc.) in the given image, followed by extracting multiple complementary low level features. The features have been then encoded into the sophisticated Fisher Vector representation which enables accurate classification with linear classifiers. We have also applied the recent development of deep learning. More importantly our system combines multiple source of information, i.e. integrates organ annotation with image data, and adopts fusion of classifiers which has led to great results. The extensive experiments demonstrate the effectiveness of the proposed system, where three (out of four) of our submissions outperforms all submissions by other teams, therefore the team achieves the first place in LifeCLEF 2014 Plant task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The fine-grained classification, i.e. classification among categories which are both visually and semantically very similar, is a very difficult task. It is even challenging for humans without careful training, and is critical for establishing a more detailed understanding of the visual world <ref type="bibr" coords="1,322.22,537.56,15.16,8.74" target="#b10">[11]</ref> <ref type="bibr" coords="1,337.38,537.56,15.16,8.74" target="#b17">[18]</ref> <ref type="bibr" coords="1,352.54,537.56,15.16,8.74" target="#b9">[10]</ref>. Identifying plant species is essential for successful agricultural development and conservation of biodiversity. However, it is a very difficult task even for professionals (e.g. farmers, wood exploiters, botanists). To evaluate recent advances of information retrieval and computer vision on this challenging task, the CLEF Cross Language Evaluation forum has been organizing yearly competition on plant identification since 2011. Following the success of the three previous ImageCLEF Plant identification tasks, LifeCLEF 2014 plant task consists of 500 plant species dedicated to botanical data. The task has focused on tree, herbs and ferns species identification based on different types of images. Main novelties compared to the last years are the following:</p><p>-Multi-image query: The motivation of the task is to fit better with a real scenario where one user tries to identify a plant by observing its different organs, such as it has been demonstrated in <ref type="bibr" coords="2,343.85,143.90,53.84,8.74">[MAED2012]</ref>. Indeed, botanists usually observe simultaneously several organs, e.g. the leaves and the fruits or the flowers in order to identify species which could be mystified if only one organ were observed. -Observation-based evaluation: Unlike previous years, the species identification task is not image-centered but observation-centered. The aim of the task is to produce a list of relevant species for each observation of a plant of the test dataset, i.e. one or a set of several pictures related to a same event, where a same person photographs several detailed views on various organs the same day with the same device with the same lightening conditions observing the same plant. -Large number of species: The number of species is about 500, which is an important step towards covering the entire flora of a given region.</p><p>This paper presents the system and learning strategies that were applied by the IBM Research team to the plant identification task of LifeCLEF 2014. The rest of the paper is organised as follows: Section 2 describes the main components of the system, involving segmentation, feature extraction and learning, and methodologies applied at each module. Section 3 discusses the details of experiments and obtained results on validation and test sets. Section 4 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head><p>In this section we first briefly describe our submission to the LifeCLEF 2014 competition. Then various components of the system, including segmentation (delineation of the region of interest), feature coding and deep convolutional nerual network are explained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">IBM Research Australia Runs</head><p>We have submitted four different runs to LifeCLEF <ref type="bibr" coords="2,375.06,518.64,15.50,8.74" target="#b21">[22]</ref> Plant identification task <ref type="bibr" coords="2,156.13,530.59,14.61,8.74" target="#b22">[23]</ref>.  <ref type="bibr" coords="2,194.74,633.20,14.76,8.74" target="#b16">[17]</ref>. We first extract dense feature, e.g. SIFT <ref type="bibr" coords="2,401.00,633.20,10.52,8.74" target="#b1">[2]</ref> and Color Moment from raw images. Each feature is modeled with Mixture of Gaussian (GMMs) and forms the Fisher Vector representation. Then we learn a linear SVM <ref type="bibr" coords="3,177.95,119.99,15.50,8.74" target="#b13">[14]</ref> for each feature. The final submission is the average score from the two features. 3. Run3-Fusion of Run1 and Run2: We have applied an empirical fusion method, where each of the components are fed into the fusion module and a weight has been tuned for each component. 4. Run4-Segmentation and Fusion of Run2 and Run3: We have applied the feature encoding method on images with ROI extracted. We then fuse this result with Run3. We also made a few improvements, e.g. SVM averaging.</p><p>The details of the methods used above are explained in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Segmentation</head><p>In most images, using a region of interest (ROI) which encloses the main object is sufficient to determine their class label. In fact, segmentation of the main objects and extracting the ROI often results in removing the irrelevant background which could introduce noise to a supervised classier. In categories of flower, fruit, leaf, leafScan and stem, we apply different segmentation methods to remove the background. For the two categories of wholeTree and branch, however, we have observed that the whole image contains useful information. Therefore, we choose not to extract any ROI from these views, instead the original image is used in the next modules of the system, i.e. feature extraction and learning.</p><p>For the flower and fruit categories, we apply a similar segmentation method to extract the ROI that contains flower or a fruit, as follows: assuming that a flower or fruit is usually "more red" compared to leaves, we locate the regions of which their red channel is larger than the green channel. Each color image is converted to a gray-value image, and the localized "red" regions are then employed as initial masks in segmenting the flower or fruit in the gray-value image using a active contour method <ref type="bibr" coords="3,234.88,450.78,14.61,8.74" target="#b20">[21]</ref>. In the end, we compute the minimum bounding box of the flower or fruit mask as the ROI. Figure <ref type="figure" coords="3,334.22,462.73,4.98,8.74" target="#fig_0">1</ref> shows ROI extraction results on some samples on flower pictures and Figure <ref type="figure" coords="3,328.11,474.69,4.98,8.74" target="#fig_0">1</ref> shows those on fruit pictures. For the leafScan category, every image typically includes a leaf with a light background. However, there are variations among the background colors. We therefore normalize the background with a consistent white color which can potentially improve the accuracy of the classification. We convert the color image into a gray-value image and then apply Otsu method <ref type="bibr" coords="4,364.92,231.16,10.52,8.74" target="#b0">[1]</ref> to compute a threshold. The pixels in the gray-value image that are smaller than the threshold are labeled as background, and all background pixels in the corresponding color image are assigned with a while color. Some examples are shown in Figure <ref type="figure" coords="4,417.77,267.03,3.87,8.74" target="#fig_2">3</ref>. In images of the leaf category, we observed that a leaf is usually located in the center area of the picture and there are typically some margin between the leaf boundary and the picture border. We therefore extract the object that is located in the center of the picture. In pre-processing, we convert the color picture to a gray-value image and apply a Gaussian filter on the image with σ = 3. We then extract an rectangular ROI in the image as an initial ROI that contain the leaf. The ROI is defined by its left top and bottom right coordination in the image. Let (r, l) is the number of rows and columns in the image, the left top coordination is ( r 6 , l 6 ) and the right bottom coordination is ( 5r 6 , 5l 6 ). We then apply active contour on the gray-value image using the rectangular ROI as the initial mask. The boundary box of the segmented leaf is the final ROI and some examples are shown in Figure <ref type="figure" coords="5,267.63,131.95,3.87,8.74" target="#fig_3">4</ref>. In the stem category, the stem in usually located in the center of the image. For segmenting the stem, we convert the color image to a gray-value image and creating a central mask on the image by cropping %25 from left and %25 from right. Active contour is then applied on the gray-value image using the mask. The bounding box of the resulting area is the ROI for the stem image. This method can effectively remove most of the background for vertical stems without branches. Some examples are shown in Figure <ref type="figure" coords="5,375.37,385.69,3.87,8.74" target="#fig_4">5</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Advanced Feature Coding</head><p>Feature coding is the generalization of the popular BoW model <ref type="bibr" coords="5,407.93,608.92,10.20,8.74" target="#b7">[8]</ref> <ref type="bibr" coords="5,418.13,608.92,10.20,8.74" target="#b2">[3]</ref>. The recent evaluation <ref type="bibr" coords="5,183.27,620.88,10.52,8.74" target="#b3">[4]</ref> shows that the FisherKernel <ref type="bibr" coords="5,326.83,620.88,14.87,8.74" target="#b19">[20]</ref>[5] feature coding achieves best results in most of the cases. We introduce this coding method in this section.</p><p>Suppose we have a probability density function u λ (x) which models a generative process in feature space. Let X = {x 1 , • • • , x N } be the set of N local features extracted from an image. Then the image can be described by the gradient vector of log likelihood with respect to the model parameters λ:</p><formula xml:id="formula_0" coords="6,254.71,149.95,225.89,22.31">G X λ = 1 N ∇ λ log u λ (X).<label>(1)</label></formula><p>A natural kernel on these gradients is</p><formula xml:id="formula_1" coords="6,134.77,180.33,345.83,24.50">K(X, Y ) = G X λ F -1 λ G Y λ where F λ is the Fisher information matrix of u λ : F λ = E x∼u λ [∇ λ log u λ (x)∇ λ log u λ (x) ].</formula><p>As F λ is symmetric and positive definite, it can be decomposed as F λ = L λ L λ , and the kernel K(X, Y ) can be expressed as a dot-product between normalized vectors G X λ = L λ G X λ called Fisher vectors. We stress that learning a kernel classifier using the Fisher kernel is equivalent to learning a linear classier on the Fisher vectors G X λ . As been recognized widely, linear classifiers offer significant advantages in terms of efficiency both for training and testing.</p><p>Fisher Vector encoding utilizes a Gaussian mixture model (GMM), u λ (x) = K k=1 π k u k (x) trained on local features of a large image set using Maximum Likelihood (ML) estimation. The parameters of the trained GMM are denoted as</p><formula xml:id="formula_2" coords="6,134.77,315.37,129.92,9.65">λ = {π k , µ k , Σ k , k = 1, • • • , K}</formula><p>, where {π, µ, Σ} are the prior probability, mean vector and diagonal covariance matrix of the Gaussian mixture respectively. This GMM is used for description of low level features. Then for a set of low level features X = {x 1 , • • • , x N } extracted from an image y, the soft assignments of the descriptor x i to the kth Gaussian component γ ik is computed by:</p><formula xml:id="formula_3" coords="6,258.06,382.24,222.53,26.56">γ ik = π k u k (x i ) K k=1 π k u k (x i )<label>(2)</label></formula><p>And the Fisher vector (FV) for X is denoted as</p><formula xml:id="formula_4" coords="6,134.77,417.10,345.83,64.15">φ(X) = {G X µ1 , G X σ1 , • • • , G X µ K , G X σ K } where G µ k and G σ k is defined as: G X µ k = N i=1 1 N √ π k γ ik x i -µ k σ k ,<label>(3)</label></formula><formula xml:id="formula_5" coords="6,225.21,485.88,255.38,30.32">G X σ k = N i=1 1 N √ 2π k γ ik [ (x i -µ k ) 2 σ 2 k -1],<label>(4)</label></formula><p>Where σ k is the square root of the diagonal values of Σ k . The FV has several good properties: (a) Fisher Vector encoding is not limited to computing visual word occurrences. It also encodes the distribution information of the feature points, which will perform more stable when encoding a single feature point. (b) it can naturally separate the image specific information from the noisy local features. (c) we can use a linear model for this representation.</p><p>Power Normalization and L2 Normalization: It is easy to observe that as the number of Gaussians increases, Fisher vectors become sparser, and the distribution of features in a given dimension becomes more peaky around zero. This issue is addressed by a combination of power normalization and l2 normalization for each Fisher vector descriptor. Suppose z is one dimension of the fisher vector φ, the power normalization is defined as f (z) = sign(z)|z| α where 0 ≤ α ≤ 1 is a parameter of the normalization and we choose α = 0.5 in all the experiments. Subsequently, the Fisher vectors are l2 normalized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Deep Convolutional Neural Network</head><p>Fig. <ref type="figure" coords="7,172.38,304.02,4.13,7.89">6</ref>. A deep convolutional neural network with similar configuration in <ref type="bibr" coords="7,452.87,304.05,9.73,7.86" target="#b5">[6]</ref> .</p><p>We also applied a deep convolutional nerual network (CNN) for the plant identification task. We use the training data from the plant task only, which is relative small scale data for this deep model. We also tried a pretrained CNN using ImageNet <ref type="bibr" coords="7,208.87,382.34,11.15,8.74" target="#b6">[7]</ref>[13] dataset, as this usage of extra data is not allowed in the LifeCLEF challenge, we didn't submit the result but only evaluated on our internal validation set.</p><p>Our CNN has around 60 million parameters. it consists of five convolutional layers, some of which are followed by max-pooling layers, and three fullyconnected layers with a final softmax layer. To make training faster, we have used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called dropout that proved to be very effective. We also use the data augmentation as in <ref type="bibr" coords="7,350.24,489.93,9.96,8.74" target="#b5">[6]</ref>. In order to incorporate the information of the "Organ" annotation, we propose two methods: (1) we train the CNN with two objective function which targets the label accuracy and view accuracy at the same time. ( <ref type="formula" coords="7,256.40,525.80,4.24,8.74" target="#formula_3">2</ref>) we train the CNN with one objective function but we set the class label as the enumerate of the species and view annotation. These two implementation turns out providing similar performance in our validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Fusing of Multiple Systems</head><p>Until now we have presented the sub models of the system, we then fuse the results at the late stage. Each sub model k provides a confidence score s k i,j for each observationID/imagei and each category j. We optimize to get the final confident score as the weighted sum: S i,j = k w k * s k i,j . The optimization is performed on the validation set. Each time, we select on sub model with best accuracy and tune the weight to get the best fused accuracy. The same set of the weight parameters have been used to get the confidence score on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section we discusses the details of experiments and obtained results on validation and test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental setting</head><p>Dataset The PlantCLEF dataset focuses on 500 herb, tree and fern species centered on France (some plants observations are from neighboring countries). It contains more than 60000 pictures belonging to one of the seven types of view reported into the meta-data, in a xml file (one per image) with explicit tags. A part of the dataset has been provided as training data and the remaining part will be used later as test data. Test observation will be chosen by randomly sampling 1/3 of the observations of each species.</p><p>The training data finally results in 47815 images, including 1987 of "Branch", 6356 photographs of "Entire", 13164 of "Flower", 3753 "Fruit", 7754 of "Leaf", 3466 "Stem" and 11335 scans and scan-like pictures of "leaf". The test data contains 8163 plant-observation-queries. These queries are based on 13146 images; 731 of "Branch", 2983 photographs of "Entire", 4559 of "Flower", 1184 "Fruit", 2058 of "Leaf", 935 "Stem" and 696 scans and scan-like pictures of "leaf".</p><p>Validation Set In order to verify the effective of each components in our system, we split the "training data" provided by PlantCLEF into two parts: a train set and a validation set. The validation set is roughly 1/5 of the total training data. We split the training data according to the observation id which is critical since the final evaluation is based on observation id. In the following section, we will report the results on both the validation set and the testing set.</p><p>Implementation details As previously mentioned, in the feature coding approach, we extract two types of local features: dense SIFT feature and Color moment feature. The dimension of each local feature has been reduced to 64 using PCA <ref type="bibr" coords="8,184.55,571.91,14.61,8.74" target="#b15">[16]</ref>. Then for each type of feature, we generate a GMM model which has 512 components.</p><p>For deep convolutional neural network, we follow the pipeline of Alex <ref type="bibr" coords="8,447.58,596.58,9.96,8.74" target="#b5">[6]</ref>. The filter size and filter number is the same as Alex's. We restrict the node number of the fully connection network to 2048 as this number is far more enough to model the plant images.</p><p>We use open source libraries, e.g. SIFT from VL-feat <ref type="bibr" coords="8,393.80,645.16,10.52,8.74" target="#b8">[9]</ref> and SVM solver from LibSVM <ref type="bibr" coords="8,198.30,657.11,15.50,8.74" target="#b14">[15]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results on Validation Set</head><p>For proper assessment of each components of our system, we perform diagnosis evaluation on the validation set. The evaluation results are shown in Table <ref type="table" coords="9,475.63,149.46,4.98,8.74" target="#tab_0">1</ref> based on two metrics as proposed by PlantCLEF organiser, i.e. the Accuracy w.r.t. image (Acc image) and Accuracy w.r.t. observation id (Acc observ). Based on result shown in Table <ref type="table" coords="9,289.80,281.67,3.87,8.74" target="#tab_0">1</ref>, we can observe the following:</p><p>1. The feature coding pipeline (by combing two type of features, i.e. SIFT and color moment) achieves stable results. As reported in the literature, the Fisher Kernel encoding performs best among many of the encoding methods in visual recognition. 2. Segmentation (ROI extraction) improves the pipeline of feature coding and increase the classification accuracy. 3. The CNN method results in lower classification accuracy, compared to feature coding. We believe the effect is the due to limited number of training data. Deep learning has been demonstrated great success when using large scale dataset. The PlantCLEF dataset is a middle scale dataset and the deep model can be easily overfitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results on Test Set</head><p>In this subsection, we present our final results submitted to LifeCLEF2014 plant identification task. Results of each run is shown in  -RUN1 is the result of using deep convolutional nerual network. It obtains reasonable result but inferior to other runs. The main problem is that the deep model is very easy to overfitting on the training set with so many parameters.</p><p>-RUN2 is the raw result with feature coding pipeline which gives quite good result. It demonstrates that the feature coding pipeline is very mature and easily adaptable to many tasks in visual recognition. -RUN3 is the result from combing RUN2 and RUN1. It shows that the CNN framework is complementary to the traditional feature coding framework. -RUN4 is the result from combing RUN3 and feature coding pipeline applied on segmented images. It shows the correct delineation of ROI improves the classification results.</p><p>Further evaluation results:</p><p>1. Overall performance: Our submissions achieve the top three results when comparing with other teams as shown in Figure <ref type="figure" coords="10,363.60,277.27,4.98,8.74" target="#fig_5">7</ref>   2. Results for each organ (view): Figure <ref type="figure" coords="10,336.89,509.29,4.98,8.74" target="#fig_6">8</ref> shows the result for each organ (view). Our classification results again leads other teams in most of cases.</p><p>It is worth noting that our system's performance on LeafScan and Flower is better than 50%. Considering such fine-grained categroization task, we believe this result has practical value for real system. 3. Observation-based evaluation: We observe a notable improvement from image-based evaluation to observation-based evaluation on the validation set, which shows the ability of our system in using multiple source of image data and perform a more accurate classification. Unfortunately, the trend is not that obvious on the test set. This is caused by dissimilarity between the training and testing set in terms of average number of images per observation id, i.e. there is around 4.5 images per observation id in the training set, while we only have around 1.6 images per observation id in the test set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we described the system and learning methodology applied by the IBM Australia Research team to the plant identification task of LifeCLEF 2014. We utilized the advanced feature coding method with automatic ROI extraction. We also applied the recent development of deep learning and achieved great result on the validation set. The most important contribution of this work is in effective fusion of various learning schemes and proper use of multiple source of information (annotation data and image data). The extensive experiments demonstrated the effectiveness of the proposed system and the final submitted run achieved the first place in LifeCLEF 2014 Plant task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,154.37,587.25,306.61,7.89;3,180.30,505.83,100.00,66.60"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Original flower image (left) and automatically extracted ROI (right)</figDesc><graphic coords="3,180.30,505.83,100.00,66.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,157.31,198.25,300.72,7.89;4,169.70,127.23,100.00,56.20"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Original fruit image (left) and automatically extracted ROI (right)</figDesc><graphic coords="4,169.70,127.23,100.00,56.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,189.52,515.89,236.30,7.89;4,153.36,401.07,75.00,100.00"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Original LeafScan image (left) and the ROI (right)</figDesc><graphic coords="4,153.36,401.07,75.00,100.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,159.11,278.93,297.12,7.89;5,157.76,197.71,100.00,66.40"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Original leaf image (left) and automatically extracted ROI (right)</figDesc><graphic coords="5,157.76,197.71,100.00,66.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="5,155.91,532.67,303.52,7.89;5,160.36,417.85,75.00,100.00"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Original Stem image (left) and automatically extracted ROI (right)</figDesc><graphic coords="5,160.36,417.85,75.00,100.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="10,134.77,457.27,345.81,7.89;10,134.77,468.26,335.50,7.86;10,150.52,347.46,155.62,95.04"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Official Test results for the submitted Plant Identification runs, based on observation id (left) and image id (right). IBM runs achieved top three performances.</figDesc><graphic coords="10,150.52,347.46,155.62,95.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="11,162.52,258.32,290.30,7.89;11,203.93,116.83,207.49,126.71"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. The final results evaluated based on each image for each organs.</figDesc><graphic coords="11,203.93,116.83,207.49,126.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,138.97,549.65,341.64,92.29"><head>1 .</head><label>1</label><figDesc>Run1-Deep convolutional nerual network: In this run, we utilize the deep convolutional nerual network model explained in Sec.2.4 with five layers of convolutional network, three layers of fully connection and cost layer of logistic regression. The CNN is trained on the plant training data provided by LifeClef 2014, which is relative small scale data for this deep model.</figDesc><table coords="2,138.97,609.26,341.64,32.68"><row><cell>2. Run2-Advanced feature encoding: In this run, we apply the advanced</cell></row><row><cell>feature encoding methods explained in Sec2.3 using Fisher Kernel encod-</cell></row><row><cell>ing [20][5]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,172.59,204.21,270.17,51.38"><head>Table 1 .</head><label>1</label><figDesc>Results for each components on validation set.</figDesc><table coords="9,172.59,224.99,270.17,30.60"><row><cell cols="5">ValidationSet FeatureCoding FeatureCoding+ROI DNN Combine</cell></row><row><cell>Acc image</cell><cell>0.445</cell><cell>0.458</cell><cell>0.32</cell><cell>0.483</cell></row><row><cell>Acc observ</cell><cell>0.624</cell><cell>0.63</cell><cell>0.44</cell><cell>0.656</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,134.77,473.87,345.84,32.65"><head>Table 2</head><label>2</label><figDesc></figDesc><table /><note coords="9,392.00,473.87,88.61,8.74;9,134.77,485.83,345.84,8.74;9,134.77,497.78,58.57,8.74"><p>, based on two metrics: the Accuracy w.r.t. image (Acc image) and Accuracy w.r.t. observation id (Acc observ).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,149.71,528.63,244.34,84.45"><head>Table 2 .</head><label>2</label><figDesc>Results for each runs on test set.</figDesc><table coords="9,149.71,547.65,237.85,65.42"><row><cell>TestSet RUN1 RUN2 RUN3 RUN4</cell></row><row><cell>Acc image 0.263 0.438 0.446 0.456</cell></row><row><cell>Acc observ 0.271 0.454 0.459 0.471</cell></row><row><cell>From Table 2 we observe:</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,147.56,462.44,333.01,7.86;11,156.13,473.40,139.25,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,228.11,462.44,228.30,7.86">A threshold selection method from gray-level histograms</title>
		<author>
			<persName coords=""><forename type="first">Nobuyuki</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,465.48,462.44,15.10,7.86;11,156.13,473.40,35.33,7.86">Automatica</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="23" to="27" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,147.56,484.09,333.03,7.86;11,156.13,495.05,197.21,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,198.49,484.09,222.73,7.86">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName coords=""><surname>Dg Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,428.00,484.09,52.59,7.86;11,156.13,495.05,112.22,7.86">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,147.56,505.73,333.01,7.86;11,156.13,516.69,254.16,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,256.50,505.73,224.08,7.86;11,156.13,516.69,38.24,7.86">A bayesian hierarchical model for learning natural scene categories</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,213.79,516.69,168.39,7.86">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,147.56,527.38,333.01,7.86;11,156.13,538.34,324.46,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,323.86,527.38,156.72,7.86;11,156.13,538.34,138.10,7.86">The devil is in the details: an evaluation of recent feature encoding methods</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,313.41,538.34,139.24,7.86">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,147.56,549.02,333.01,7.86;11,156.13,559.98,324.46,7.86;11,156.13,570.94,52.76,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,394.42,549.02,86.15,7.86;11,156.13,559.98,169.07,7.86">Improving the Fisher Kernel for Large-Scale Image Classification</title>
		<author>
			<persName coords=""><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jorge</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,342.17,559.98,138.42,7.86;11,156.13,570.94,24.59,7.86">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,147.56,581.63,333.01,7.86;11,156.13,592.59,324.45,7.86;11,156.13,603.55,58.62,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,342.54,581.63,138.04,7.86;11,156.13,592.59,121.11,7.86">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,301.83,592.59,178.75,7.86;11,156.13,603.55,30.23,7.86">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,147.56,614.23,333.01,7.86;11,156.13,625.19,324.46,7.86;11,156.13,636.15,72.62,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,439.11,614.23,41.46,7.86;11,156.13,625.19,171.70,7.86">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,356.11,625.19,124.47,7.86;11,156.13,636.15,44.51,7.86">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,147.56,646.84,333.01,7.86;11,156.13,657.80,306.37,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,271.55,646.84,209.03,7.86;11,156.13,657.80,74.46,7.86">Video Google: a text retrieval approach to object matching in videos</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,250.12,657.80,184.21,7.86">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,147.56,120.67,333.01,7.86;12,156.13,131.63,205.81,8.11" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="12,273.28,120.67,207.30,7.86;12,156.13,131.63,67.43,7.86">VLFeat: An open and portable library of computer vision algorithms</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Fulkerson</surname></persName>
		</author>
		<ptr target="http://www.vlfeat.org/" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,147.22,142.59,333.35,7.86;12,156.13,153.55,324.45,7.86;12,156.13,164.51,20.99,7.86" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="12,156.13,153.55,99.53,7.86">Caltech-UCSD birds 200</title>
		<author>
			<persName coords=""><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="12,147.22,175.47,333.35,7.86;12,156.13,186.42,216.38,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,292.95,175.47,187.63,7.86;12,156.13,186.42,73.56,7.86">Automated Flower Classification over a Large Number of Classes</title>
		<author>
			<persName coords=""><forename type="first">M-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,249.22,186.42,31.04,7.86">ICVGIP</title>
		<imprint>
			<biblScope unit="page" from="722" to="729" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,147.22,197.38,333.35,7.86;12,156.13,208.34,324.45,7.86;12,156.13,219.30,305.84,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,316.03,197.38,164.55,7.86;12,156.13,208.34,324.45,7.86;12,156.13,219.30,38.24,7.86">Learning generative visual models from few training examples: An incremental Bayesian approach tested on 101 object categories</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,202.27,219.30,174.45,7.86">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="70" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,147.22,230.26,333.35,7.86;12,156.13,241.22,321.54,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,342.00,230.26,138.57,7.86;12,156.13,241.22,102.48,7.86">Hierarchical semantic indexing for large scale image retrieval</title>
		<author>
			<persName coords=""><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,281.17,241.22,168.39,7.86">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,147.22,252.18,333.37,7.86;12,156.13,263.14,114.17,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,210.19,252.18,243.75,7.86">A tutorial on support vector machines for pattern recognition</title>
		<author>
			<persName coords=""><surname>Cjc Burges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,461.18,252.18,19.42,7.86;12,156.13,263.14,85.98,7.86">Data Min Knowl Discovery</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,147.22,274.10,333.35,7.86;12,156.13,285.05,324.46,7.86;12,156.13,296.01,54.77,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,322.72,274.10,157.86,7.86;12,156.13,285.05,34.86,7.86">LIBSVM: a library for support vector machines</title>
		<author>
			<persName coords=""><forename type="first">Chih-Chung</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,203.35,285.05,272.68,7.86">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,147.22,306.97,311.68,7.86" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="12,205.40,306.97,120.65,7.86">Principal Component Analysis</title>
		<author>
			<persName coords=""><forename type="first">I T</forename><surname>Jolliffe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002-10">October 2002</date>
			<publisher>Springer Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,147.22,317.93,333.35,7.86;12,156.13,328.89,324.46,7.86;12,156.13,339.85,20.99,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="12,325.03,317.93,155.54,7.86;12,156.13,328.89,132.34,7.86">Towards Good Practice in Large-Scale Learning for Image Classification</title>
		<author>
			<persName coords=""><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Akata</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Harchaoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,307.99,328.89,168.55,7.86">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,147.22,350.81,333.35,7.86;12,156.13,361.77,324.45,7.86;12,156.13,372.73,324.46,7.86;12,156.13,383.68,95.69,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="12,352.46,361.77,128.11,7.86;12,156.13,372.73,205.57,7.86">Searching the World&apos;s Herbaria: A System for Visual Identification of Plant Species</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Feiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kress</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sheorey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,382.18,372.73,98.41,7.86;12,156.13,383.68,67.52,7.86">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,147.22,394.64,333.35,7.86;12,156.13,405.60,279.21,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="12,244.94,394.64,231.63,7.86">Improving Image Classication using Semantic Attributes</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,156.13,405.60,168.11,7.86">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="77" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,147.22,416.56,333.35,7.86;12,156.13,427.52,273.56,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="12,279.42,416.56,201.16,7.86;12,156.13,427.52,57.36,7.86">Fisher Kernels on Visual Vocabularies for Image Categorization</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,233.19,427.52,168.39,7.86">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,147.22,438.48,333.37,7.86;12,156.13,449.44,167.63,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="12,290.49,438.48,120.83,7.86">Active contours without edges</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">F</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Vese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,419.76,438.48,60.83,7.86;12,156.13,449.44,103.72,7.86">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">266277</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,147.22,460.40,333.35,7.86;12,156.13,471.36,324.44,7.86;12,156.13,482.32,324.45,7.86;12,156.13,493.27,141.17,7.86" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="12,311.75,482.32,168.83,7.86;12,156.13,493.27,22.87,7.86">multimedia life species identification challenges</title>
		<author>
			<persName coords=""><forename type="first">Alexis</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Henning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hervé</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hervé</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Concetto</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andreas</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Willem-Pier And</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bob</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lifeclef</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,186.90,493.27,86.33,7.86">Proceedings of CLEF</title>
		<meeting>CLEF</meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,147.22,504.23,333.35,7.86;12,156.13,515.19,324.45,7.86;12,156.13,526.15,132.92,7.86" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="12,303.25,515.19,177.33,7.86">Nozha LifeCLEF Plant Identification Task</title>
		<author>
			<persName coords=""><forename type="first">Hervé</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexis</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jean-François And</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Barthélémy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Boujemaa</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,178.65,526.15,86.33,7.86">Proceedings of CLEF</title>
		<meeting>CLEF</meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
