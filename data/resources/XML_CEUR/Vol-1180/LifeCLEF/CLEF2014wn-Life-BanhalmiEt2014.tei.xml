<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,138.89,115.96,337.58,12.62;1,220.22,133.89,174.92,12.62">Wlab of University of Szeged at LifeCLEF 2014 Plant Identification Task</title>
				<funder>
					<orgName type="full">European Union</orgName>
				</funder>
				<funder ref="#_8Q75n9B">
					<orgName type="full">European Social Fund</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,137.57,171.62,67.31,8.74"><forename type="first">Dénes</forename><surname>Paczolay</surname></persName>
							<affiliation key="aff1">
								<address>
									<addrLine>{pdenes</addrLine>
									<settlement>banhalmi</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,215.43,171.62,76.61,8.74"><forename type="first">András</forename><surname>Bánhalmi</surname></persName>
							<affiliation key="aff1">
								<address>
									<addrLine>{pdenes</addrLine>
									<settlement>banhalmi</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,302.59,171.62,65.58,8.74"><forename type="first">László</forename><forename type="middle">G</forename><surname>Nyúl</surname></persName>
							<email>nyul@inf.u-szeged.hu</email>
							<affiliation key="aff1">
								<address>
									<addrLine>{pdenes</addrLine>
									<settlement>banhalmi</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,378.72,171.62,61.08,8.74"><forename type="first">Vilmos</forename><surname>Bilicki</surname></persName>
							<email>bilickiv@inf.u-szeged.hu</email>
							<affiliation key="aff1">
								<address>
									<addrLine>{pdenes</addrLine>
									<settlement>banhalmi</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,451.60,169.11,26.18,8.74;1,292.40,183.58,26.10,8.74"><forename type="first">Árpád</forename><surname>Sárosi</surname></persName>
							<email>sarosi.arpad@gmail.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Szeged</orgName>
								<address>
									<settlement>Szeged</settlement>
									<country key="HU">Hungary</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,138.89,115.96,337.58,12.62;1,220.22,133.89,174.92,12.62">Wlab of University of Szeged at LifeCLEF 2014 Plant Identification Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FEC86EEFD4344F1A97280C6AE634DEE9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Plant identification</term>
					<term>sheet as background</term>
					<term>leaf classification</term>
					<term>contour and metric feature combination</term>
					<term>contour histograms</term>
					<term>gradient histograms in color channels</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our aim is to implement a plant identification application that can run on smartphones, and this shared task includes it. After the plant identification task of 2013 we concluded that the most frequent trees (e. g. in Hungary) can be identified well by a leaf, when there is a white paper background behind it at the time of photographing. This is why we want to classify more accurately this kind of pictures. Our other important goal is to develop a system which can be trained online while smartphone users take more and more photos. In the previous year there was a separate shared task for the 'Sheet as Background' photos, however this year the task is very complex, and supposes solutions for hard image processing problems, not just feature extraction and classification solutions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ImageCLEF Plant identification task <ref type="bibr" coords="1,315.78,500.70,10.52,8.74" target="#b2">[3]</ref> is becoming increasingly more complex year after year, while more and more photos are being taken of more and more plants. This year 500 different types of plant species need to be identified. Although there are currently 47815 photos, this number is still not sufficiently high, because for many specific classes there are only a few examples available, which means that the usual classification methods may not be trained well, and other solutions are required. Because the number of examples for the scan-like leaf images is the highest (11335), and because the preprocessing of these kinds of pictures seems to be the least complex, we focused on this task, and all the others were treated to get a better solution than a random or a most-is-best classifier. In addition, we will concentrate on scan-like images because our aim is to improve an initial training by online learning methods, which refines a basic model continuously, when the users of smartphones take new photos of leaves with a white background. Our idea is that when users take new photos, there will be a voting mechanism using human votes, which decides if a new photo belongs to one specific species or not. There is also a confidence value assigned to all the users, and this value is also updated when the smartphone application is used continuously for a long time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Experiments</head><p>The training dataset <ref type="bibr" coords="2,228.37,218.50,10.51,8.74" target="#b1">[2]</ref> was downloaded from the PlantCLEF webpage, and it contained 47815 images (1987 of "Branch", 6356 photographs of "Entire", 13164 of "Flower", 3753 "Fruit", 7754 of "Leaf", 3466 "Stem" and 11335 scans and scan-like pictures of leaf) with complete xml files associated to them. Table <ref type="table" coords="2,178.71,443.04,4.98,8.74" target="#tab_0">1</ref> lists some statistics concerning the photos for each category. As can be seen, in the case of "Branch", "Fruit", "Leaf" and "Stem" the majority of species are strongly underrepresented; very little data is available. In the case of "Flower", however, there are many more photos, and for almost the all species (483), from the 500 species. The scan-like pictures have the best example/species ratio, however there are many species for which scan-like leaf data is not available. From this point of view, we should surely focus on the classification using mainly the scan-like leaf and flower pictures, or when using the other categories, then at the score aggregation we should consider the number of examples in the training set for a category and species.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">SheetAsBackground (Scan-like Leaf, LeafScan) Category</head><p>This category contains the most examples for a species, and it seems to be the easiest to preprocess, namely to get a mask of a leaf. We tried to find the less complex methods for masking, feature extraction and classication, because our aim is a fast method that can be used on a smartphone without waiting long for the classification or ranking results.</p><p>Masking: Our masking method is rather simple. After taking the blue channel of the color picture, a 3x3 blur is appiled on it, and Otsu's thresholding method is applied. The result is obtained from this after the salt-and-pepper noise has been removed.</p><p>The masking method will fail in some cases, for example when the background has a similar intensity on the blue channel than the leaf, but in most cases it works well.</p><p>Features: In order to classify the species by their leaves, an important task is to extract features suitable for this. Since the leaves' orientation on the photo is not fixed, the appropriate features have to be scale and rotation invariant. Instead of starting from scratch, we based our features on two sets taken from the literature.</p><p>First, we implemented the vein and metric features introduced in <ref type="bibr" coords="3,438.42,284.87,9.97,8.74" target="#b3">[5]</ref>. These basic features contain area information of veins computed by an unsharp masking method. Vein density is computed at four levels with four different blur radii. Some of the metric features in <ref type="bibr" coords="3,309.43,320.73,10.52,8.74" target="#b3">[5]</ref> supposes, that the two endings of the leaves have been set by hand. We modified these features, such that our method finds the longest projection of a leaf, and computes the perpendicular projection, as well. This way, we don't need endings, and don't need a fixed leaf orientation. Metric features include area/perimeter 2 , perimeter/diameter, diameter/perpendiculardiameter, etc.</p><p>More complex features are based on the TAR and TSL descriptors defined in <ref type="bibr" coords="3,146.90,405.06,9.96,8.74">[4]</ref>, but in another way we used them to extract contour features. The idea behind it is that a given number of points are set uniformly on the contour of a leaf. At each point a given number of features can be computed from the data of triangles joining the neighbouring points. When the features are the ratio of side lengths, then these scale invariant measures are called TSL, and when the features are oriented angles of the triangles, then it is called TOA. For each of the N points on the leaf contour, a number of M rectangles are set, and each rectangle is described by two angles, or two side length ratios. This method gives a number of 2 • N • M values. The question is the way of representation for classification, because the feature vectors computed from the previous data have to be rotation invariant. In [4], the Locality Sensitive Hashing technique is proposed for the purpose of matching two vectors, and measuring similarity. Here we draw a simpler feature extraction method, which is also rotation invariant, as in <ref type="bibr" coords="3,159.89,560.48,9.97,8.74">[4]</ref>. In our solution only one angle is computed for a triangle, this being the angle at the actual control point for which the neighbouring triangles were set. The other two descriptors for a triangle are the ratios of side lengths, but in our representation the maximal side length is always the denominator, then the range of these values is between 0 and 1. After we compute the angle and the two side ratios for each triangle, we simply make three histograms, and since the histogram cumulates the values independently of the order of control points, the feature will be invariant under rotation. As the size of each histogram is 10, we will have 30 additional features.</p><p>Classification Method: We modeled the SheetAsBackground problem as a multi-class classification task. Many kinds of methods like these have been tried and tested from Weka and OpenCV, but at the end the best solution seemed to be the random forest implementation of OpenCV with the following parameters:</p><p>-Maximal tree depth=25 -Maximal iteration for termination=100, epsilon for termination=0.1 -It should also be mentioned that for the purpose of obtaining not just one class label, rather a ranked list, a voting scheme was implemented upon the basic classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Other Categories</head><p>The segmentation of interesting parts in the task of the other categories is not simple, and we think that it should be the first step for getting good descriptors. However, for the flower task we can make some assumptions, for example the color of the flowers is very important. This is why we collected color-gradient features for all these tasks.</p><p>Features: First, from a test photo, 9 new pictures are created corresponding to different color intervals, and one picture for the grey part. Fig. <ref type="figure" coords="4,429.09,351.96,4.98,8.74" target="#fig_0">1</ref> illustrates this step. After this decomposition, gradient-based features were computed on all the 9 picture channels, namely, an intensity histogram, a gradient magnitude histogram, and a gradient angle histogram. These histograms (containing 10 bins each) were also normalized before being used as features in the classification process.</p><p>Classification Method: Since in most cases very little training data is available per species, we used nearest neighbor classifier to generate test results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Own Results</head><p>For the experiments, we used 10-fold cross validation method to evaluate our models. As mentioned earlier, for scan-like leaf photos a random forest was trained, while for all the other problems nearest neighbour classifiers were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Our results on the training database:</head><p>On the scan-like leaves task we achieved an accuracy score of 55.84%, which is competitive with the results of the ImageCLEF task's performance from the previous year <ref type="bibr" coords="4,198.66,596.34,9.96,8.74" target="#b0">[1]</ref>. The accuracy scores attained on the other tasks are listed in Table <ref type="table" coords="4,177.01,608.30,3.88,8.74">2</ref>. These accuracy scores are much better than a trivial classifier's result would be, however, it should be taken into account, that when the cross validation method divided the data into training and test sets, many similar photos from the same plant taken at the same time may be placed into both sets, and this fact introduced a positive bias in accuracy score. Table <ref type="table" coords="5,163.47,386.87,4.13,7.89">2</ref>. Accuracies got for natural photos, using the training data and tenfold crossvalidation. However, with an observation oriented cross-validation, the accuracy values should be closer to the final results.</p><p>Stem Fruit Flower Branch Entire Leaf 39% 30% 28% 18% 20% 26%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Combination of The Scores</head><p>For the official evaluation, every participant had to aggregate the scores and ranks obtained for the different photos of the same plant. Our three aggregation strategies were very similar to each other, and they were all based on the following scheme:</p><p>-If there was a scan-like leaf photo, then only the result of this type was considered.</p><p>-If more than one scan-like image was available, then a weighted majority voting scheme was used to compute ranking scores. -Otherwise, a score aggregation method was made from the nearest neighbor classifier's results, using also some prior information.</p><p>The following prior information was used during the aggregating process:</p><p>-N(s,c): the number of training data for a species and a category (stem, flower, etc.) -R(s,c): the ROC AUC value got from the training database by applying cross-validation for a species and a category -(S1,f1,S2,f2,...)(s,c): the species with which a specific species is frequently confused, and the frequency of this mistake (for a species and a category).</p><p>After the official evaluation, it turned out, that our aggregating strategies achieved almost the same results, hence it is enough to present only the first aggregation method of the three similar methods here. The aggregation rule is the following:</p><p>-When the NN classifier of a category "c" assigns an S0 class label, then a score of 1 is assigned to S0. -Taking the prior information for S0 → (S1, f 1, S2, f 2, ...) contains the frequent mistakes, S1 gets a rank of 1/4, S2 gets a rank of 1/5, etc. -All the previous ranks are multiplied by a Trust (S0,c) factor, which is computed as: (R(S0, c) -0.5) * (1/(1 + exp(5 -N (S0, c)))). That means that the trust value is much higher when a ROC AUC value is much greater than 0.5, and the number of learning examples is much higher than 5. -This is done for all the photos taken of a plant, and a weighted voting is applied to get the final scores and ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Official Test Results</head><p>The results fit our expectations. Our main target was the scan-like leaf recognition, and the other categories were handled with a lower priority. Table <ref type="table" coords="6,475.61,411.29,4.98,8.74" target="#tab_1">3</ref> summarises our results on the different categories, Fig. <ref type="figure" coords="6,379.25,423.25,4.98,8.74" target="#fig_1">2</ref> illustrates our results on the complex recognition task, and Fig. <ref type="figure" coords="6,316.90,435.20,4.98,8.74">3</ref> shows our results for the category of LeafScan images, which are the most important for us. In the task of scan-like leaves we achieved a good result using features can be computed very efficiently and are described aerlier.  Fig. <ref type="figure" coords="7,154.40,505.17,4.13,7.89">3</ref>. Scan-like leaf task: since we were late to produce a bugless run, our result was not included in the official chart. However, our solution seems competitive (see the red bar at score=0.488).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Summary</head><p>Here, our goal was to define simple, but useful and powerful features for scan-like leaf images, and some features based on color and gradient for the other tasks, by which better results can be achieved than those for a trivial classifier. From the results of our experiments, we think that scan-like leaves can be classified quite well, and our aim is to improve the performance on this task by implementing an on-line learning solution based on a voting mechanism like intelligent crowdsourcing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,134.77,357.38,345.82,7.89;5,134.77,368.36,19.48,7.86;5,146.24,115.83,322.88,226.77"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Decomposing the original image into different color intervals, and into a grey part.</figDesc><graphic coords="5,146.24,115.83,322.88,226.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,134.77,300.69,345.83,7.89;7,168.43,320.32,278.50,170.08"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. Complex task: our solutions for the "non scanned leaf" tasks were very simple.</figDesc><graphic coords="7,168.43,320.32,278.50,170.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,134.77,287.99,345.83,126.20"><head>Table 1 .</head><label>1</label><figDesc>The number of species for which there is a photo in a given category, the number of species for which there is fewer than 10 examples, and the average example number. The scan-like leaf category has the best (image) example/species ratio, but with a lot of visually redundant near duplicate images, and with fewer informative images from the same observation. When observations are considered, LeafScan and Flower categories have mostly the same observation example/species ratio.</figDesc><table coords="2,136.16,367.07,329.38,47.12"><row><cell>category</cell><cell cols="4">branch entire flower fruit leaf stem scan-like leaf</cell></row><row><cell># of species</cell><cell>356</cell><cell cols="2">490 483 374 470 399</cell><cell>212</cell></row><row><cell cols="2"># of species with &lt; 10 examples 293</cell><cell>191</cell><cell>64 236 237 290</cell><cell>83</cell></row><row><cell>average examples/species</cell><cell>5.6</cell><cell cols="2">13.0 27.3 10.0 16.5 8.9</cell><cell>53.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,210.76,502.28,193.85,47.24"><head>Table 3 .</head><label>3</label><figDesc>Accuracies of the test run.</figDesc><table coords="6,210.76,524.32,193.85,25.20"><row><cell cols="2">LeafScan Stem Fruit Flower Branch Entire Leaf</cell></row><row><cell>48.8%</cell><cell>7.7% 2.8% 8.8% 4.8% 2.9% 6.4%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This work has been supported by the <rs type="funder">European Union</rs> and the <rs type="funder">European Social Fund</rs> through project <rs type="projectName">FuturICT</rs>.hu (grant no.: <rs type="grantNumber">TAMOP-4.2.2.C-11/1/KONV-2012-0013</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_8Q75n9B">
					<idno type="grant-number">TAMOP-4.2.2.C-11/1/KONV-2012-0013</idno>
					<orgName type="project" subtype="full">FuturICT</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,138.35,142.59,342.24,7.86;8,146.91,153.55,333.68,7.86;8,146.91,164.51,333.68,7.86;8,146.91,175.47,333.68,7.86;8,146.91,186.42,182.03,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,209.54,153.55,170.37,7.86">The imageclef plant identification task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Bakic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Barthélémy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Molino</surname></persName>
		</author>
		<idno type="DOI">10.1145/2509896.2509902</idno>
		<ptr target="http://doi.acm.org/10.1145/2509896.2509902" />
	</analytic>
	<monogr>
		<title level="m" coord="8,432.52,153.55,48.08,7.86;8,146.91,164.51,333.68,7.86;8,146.91,175.47,50.52,7.86;8,261.33,175.47,46.42,7.86">Proceedings of the 2Nd ACM International Workshop on Multimedia Analysis for Ecological Data</title>
		<meeting>the 2Nd ACM International Workshop on Multimedia Analysis for Ecological Data<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="23" to="28" />
		</imprint>
	</monogr>
	<note>MAED &apos;13</note>
</biblStruct>

<biblStruct coords="8,138.35,197.38,342.24,7.86;8,146.91,208.34,117.55,7.86" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="8,451.78,197.38,28.81,7.86;8,146.91,208.34,96.04,7.86">Lifeclef plant identification task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Barthélémy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,219.30,342.24,7.86;8,146.91,230.26,333.68,7.86;8,146.91,241.22,40.25,7.86;8,134.77,252.18,345.83,7.86;8,146.91,263.14,333.68,7.86;8,146.91,274.10,333.68,7.86;8,146.91,285.05,289.12,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,273.85,230.26,206.74,7.86;8,146.91,241.22,40.25,7.86;8,134.77,252.18,3.59,7.86;8,345.94,252.18,134.65,7.86;8,146.91,263.14,222.11,7.86">A shape-based approach for leaf classification using multiscale triangular representation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mouine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Verroust-Blondet</surname></persName>
		</author>
		<ptr target="http://hal.inria.fr/hal-00818115,pl@ntNet(Agropolisfondation" />
	</analytic>
	<monogr>
		<title level="m" coord="8,391.16,263.14,89.44,7.86;8,146.91,274.10,196.05,7.86">ICMR &apos;13 -3rd ACM International Conference on Multimedia Retrieval</title>
		<meeting><address><addrLine>Dallas, United States</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013-04">Apr 2013</date>
		</imprint>
	</monogr>
	<note>Lifeclef 2014: multimedia life species identification challenges 4</note>
</biblStruct>

<biblStruct coords="8,138.35,296.01,342.24,7.86;8,146.91,306.97,333.68,7.86;8,146.91,317.93,333.68,7.86;8,146.91,328.89,70.02,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,408.93,296.01,71.66,7.86;8,146.91,306.97,267.64,7.86">A leaf recognition algorithm for plant classification using probabilistic neural network</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">L</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,435.60,306.97,44.99,7.86;8,146.91,317.93,143.66,7.86">Signal Processing and Information Technology</title>
		<imprint>
			<date type="published" when="2007-12">2007. Dec 2007</date>
			<biblScope unit="page" from="11" to="16" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
