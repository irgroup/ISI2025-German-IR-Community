<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,148.56,116.95,318.24,12.62;1,195.15,134.89,225.06,12.62">Maestra at LifeCLEF 2014 Plant Task: Plant Identification using Visual Data</title>
				<funder ref="#_DyHfEXG">
					<orgName type="full">European Commission</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,145.82,172.56,75.50,8.74"><forename type="first">Ivica</forename><surname>Dimitrovski</surname></persName>
							<email>ivica.dimitrovski@finki.ukim.mk</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Ss Cyril</orgName>
								<address>
									<addrLine>Methodius Rugjer Boshkovikj 16</addrLine>
									<postCode>1000</postCode>
									<settlement>Skopje</settlement>
									<country key="MK">Macedonia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,231.87,172.56,75.68,8.74"><forename type="first">Gjorgji</forename><surname>Madjarov</surname></persName>
							<email>gjorgji.madjarov@finki.ukim.mk</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Ss Cyril</orgName>
								<address>
									<addrLine>Methodius Rugjer Boshkovikj 16</addrLine>
									<postCode>1000</postCode>
									<settlement>Skopje</settlement>
									<country key="MK">Macedonia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,318.10,172.56,62.36,8.74"><forename type="first">Petre</forename><surname>Lameski</surname></persName>
							<email>petre.lameski@finki.ukim.mk</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Ss Cyril</orgName>
								<address>
									<addrLine>Methodius Rugjer Boshkovikj 16</addrLine>
									<postCode>1000</postCode>
									<settlement>Skopje</settlement>
									<country key="MK">Macedonia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,410.39,172.56,54.69,8.74"><forename type="first">Dragi</forename><surname>Kocev</surname></persName>
							<email>dragi.kocev@ijs.si</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Knowledge Technologies</orgName>
								<orgName type="institution">Jo≈æef Stefan Institute Jamova</orgName>
								<address>
									<addrLine>39</addrLine>
									<postCode>1000</postCode>
									<settlement>Ljubljana</settlement>
									<country key="SI">Slovenia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,148.56,116.95,318.24,12.62;1,195.15,134.89,225.06,12.62">Maestra at LifeCLEF 2014 Plant Task: Plant Identification using Visual Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D1CDDC39D9B0713E91A0DCCF97BFCD8F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>plant identification</term>
					<term>opponent SIFT</term>
					<term>TSLA</term>
					<term>bag-of-visualwords</term>
					<term>approximate k-means</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we describe an approach to the automatic plant identification task of the LifeCLEF 2014 lab. The image descriptors for all submitted runs were obtained using the bag-of-visual-words method. For the leaf scans, we use multiscale triangular shape descriptor and for the other plant organs Opponent SIFT extracted around points of interest obtained using Harris-Laplace detector. We then use approximate k-means (AKM) algorithm to cluster these descriptors in large number of clusters/visual words (approximately 200K). Each image in the training and test dataset is represented as a sparse high-dimensional histogram of term (visual word) occurrences. The similarity between two images is defined as a L2 distance over the obtained histograms. We use the standard tf-idf weighting scheme, which reduces the contribution that commonly occurring, and therefore less discriminative, words make to the similarity. To obtain the predictions, we employ a late fusion scheme for combining the similarities/ranks from multiple ranked image lists build for each type of view. Overall the proposed methods performed well, we ranked fifth, out of 10 competing groups.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ImageCLEF plant identication competition is organized every year since 2011 and aims to benchmark the progress in the area of plant identication from images <ref type="bibr" coords="1,167.07,561.47,9.96,8.74" target="#b4">[5]</ref>. Similar to the previous years, the task in 2014 is evaluated as a plant species retrieval task based on multi-image plant observations queries. The goal is to retrieve the correct plant species among the top results of a ranked list of species returned by the evaluated system. The number of species in this year task is about 500, which is an important step towards covering the entire flora of a given region.</p><p>Contrary to previous plant identification benchmarks, queries are not defined as single images but as plant observations, meaning a set of 1 to 5 images depicting the same individual plant observed by the same person the same day.</p><p>Each image of a query observation is associated with a single view type (entire plant, branch, leaf, fruit, flower, stem or leaf scan) and with contextual metadata (data, location, author). The motivation of the task is to fit better with a real scenario where one user tries to identify a plant by observing its different organs. The details of this competition are described in <ref type="bibr" coords="2,379.44,167.81,9.96,8.74" target="#b5">[6]</ref>.</p><p>In this paper, we describe our approach and runs submitted to the LifeCLEF 2014 Plant Task. The approach is based on bag-of-visual-words representation. We are using Harris-Laplace detector to detect points of interest. From these points, local invariant descriptors are then extracted. We used Opponent SIFT as local descriptors <ref type="bibr" coords="2,219.02,227.59,14.61,8.74" target="#b12">[13]</ref>. For the leaf scans we use the multiscale triangular shape descriptor <ref type="bibr" coords="2,182.64,239.54,14.61,8.74" target="#b9">[10]</ref>. Approximate k-means (AKM) algorithm is applied to cluster these descriptors in large number of clusters/visual words (approximately 200K) <ref type="bibr" coords="2,134.77,263.45,14.61,8.74" target="#b11">[12]</ref>. In AKM, the exact nearest neighbor search is replaced with approximate nearest neighbor search in the assignment step when searching for the nearest cluster center for each point. Each image in the training and testing dataset is represented as a sparse high-dimensional histogram of term (visual word) occurrences. The similarity between each query/test image histogram and each histogram from the training set is measured by using a L 2 distance. We use the standard tf-idf weighting scheme <ref type="bibr" coords="2,300.56,335.18,9.96,8.74" target="#b0">[1]</ref>, which down-weights the contribution that commonly occurring, and therefore less discriminative, words make to the relevance score.</p><p>The remainder of this paper is organized as follows. Section 2 briefly presents the training and test dataset. The image processing and feature extraction algorithms are described in Section 3. Section 4 presents the information fusion and classification algorithms that we used to obtain the predictions. Section 5 presents the results from the experimental evaluation. Finally, the conclusions and a summary are given in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Training and Test Dataset</head><p>The Plant Identification task is based on the Pl@ntView dataset which focuses on 500 herb, tree and fern species centered on France (some plants observations are from neighboring countries) <ref type="bibr" coords="2,273.25,508.10,9.96,8.74" target="#b5">[6]</ref>. The complete dataset contains 60961 images belonging each to one of the 7 types of view reported into the meta-data, in a xml file (one per image) with explicit tags. The views are as follows: Scan (scan or scan-like pictures of leaf), photos of Flower, Fruit, Stem, Leaf, Branch and Entire views. On Figure <ref type="figure" coords="2,242.80,555.92,4.98,8.74">1</ref> example images from each type of view are shown.</p><p>The distribution of training and test data of the Pl@ntView dataset is depicted in Table <ref type="table" coords="2,204.59,579.84,3.87,8.74" target="#tab_0">1</ref>. As can be seen from the presented data, most of the images in the training and the test dataset are from the Flower view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Feature Extraction and Image Description</head><p>For image description, we used the bag-of-visual-words approach <ref type="bibr" coords="2,427.31,645.16,14.61,8.74" target="#b13">[14]</ref>, <ref type="bibr" coords="2,449.81,645.16,9.96,8.74" target="#b3">[4]</ref>, <ref type="bibr" coords="2,467.33,645.16,9.96,8.74" target="#b2">[3]</ref> similarity definition. The creation of the visual vocabulary starts with detection of interesting points in the images, and then proceeds with extracting local invariant descriptors from them. Finally, the visual codebook is obtained by clustering the large set of descriptors obtained from all of the images. The resulting clusters represent the visual words, while all the visual words comprise the visual codebook. The image description phase assigns all of the local image descriptors to the visual words from the visual codebook. Each image is then described with a high-dimensional histogram and each component from the histogram is the number of descriptors that are assigned to a given visual word. Finally, the images are ranked using term frequency inverse document frequency (tf-idf) scores which reduce the influence of visual words which occur in many images. In the reminder of this section, we explain the phases in more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Image Processing and Feature Extraction</head><p>The images can be categorized in two groups. The first group is represented by scan and scan-like images, and in the second group are images from plants organs in natural surroundings, like branch, leaf, fruit, stem, flower and images from the entire plant. Having this in mind, we used two different feature extraction algorithms for the given images.</p><p>For the first group of images (scans and scan-like images of leaf) we used the triangle side lengths and angle (TSLA) descriptor from <ref type="bibr" coords="4,397.12,131.95,14.61,8.74" target="#b9">[10]</ref>, <ref type="bibr" coords="4,418.84,131.95,9.96,8.74" target="#b1">[2]</ref>. TSLA is a multiscale triangular shape descriptor where the triangles are described by their lengths and an angle. Similar as in <ref type="bibr" coords="4,294.79,155.86,9.96,8.74" target="#b1">[2]</ref>, the leaf contour in our experiments is described by 400 sample points, each point is represented by 10 triangles, with a distance d=5 between the triangle points at two successive scales. The TSLA descriptors require a preliminary leaf boundary extraction/segmentation of the image. In our experiments, we performed the boundary detection with the Otsu thresholding method <ref type="bibr" coords="4,227.52,215.63,14.61,8.74" target="#b10">[11]</ref>. The resulting descriptor for each image is a set of 400 points, each point represented with 30 float values.</p><p>For the second group of images, we used Opponent SIFTs as local descriptors extracted over the area around points of interest <ref type="bibr" coords="4,347.68,251.82,14.61,8.74" target="#b12">[13]</ref>. First, we extracted points of interest in the images using a Harris-Laplace interest point detector <ref type="bibr" coords="4,446.71,263.77,9.96,8.74" target="#b8">[9]</ref>. The Harris-Laplace detector uses the Harris corner detector to find scale-invariant interesting points. It then selects a subset of these points for which the Laplacianof-Gaussians reaches a maximum over scale <ref type="bibr" coords="4,335.70,299.64,9.96,8.74" target="#b8">[9]</ref>. For the given set of images, especially (for the flowers and branch) more than 20000 points were sampled per image. In addition, a rhomboid-shaped mask was applied to the input image to minimize the effect of the cluttered background, and to reduce the number of points as in <ref type="bibr" coords="4,187.42,347.46,9.96,8.74" target="#b1">[2]</ref>. We kept only the points that were inside the applied mask. This assumption is justified because in most of the images the observed plant organ is placed in the center.</p><p>Secondly, over the area around points of interest, Opponent SIFT descriptors were extracted. Opponent SIFT describes all the channels in the opponent color space (eq. 1) using SIFT descriptors <ref type="bibr" coords="4,304.19,407.56,9.96,8.74" target="#b7">[8]</ref>. The information in the O 3 channel is equal to the intensity information, while the other channels (O 1 and O 2 ) describe the color information in the image. These other channels do contain some intensity information, but due to the normalization of the SIFT descriptor they are invariant to changes in light intensity. The R, G and B values in eq. 1 represent the channels of the RGB color space. The resulting descriptor for each image in this case is a set of 1000 points, each point represented with 384 integer values.</p><formula xml:id="formula_0" coords="4,258.92,500.01,221.68,42.82">Ô£´ Ô£≠ O 1 O 2 O 3 Ô£∂ Ô£∏ = Ô£´ Ô£¨ Ô£≠ R-G ‚àö 2 R+G-2B ‚àö 6 R+G+B ‚àö 3 Ô£∂ Ô£∑ Ô£∏<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Approximate k-means (AKM)</head><p>The construction of a visual codebook is an essential part of the bag-of-visualwords approach to image representation. For example, in our case, we are clustering more than 10M local descriptors into more than 200K clusters. Generating clusters from such a large quantity of data presents challenges to traditionally used algorithms like k-means <ref type="bibr" coords="4,261.00,632.88,14.61,8.74" target="#b11">[12]</ref>. As a alternative, we use approximate k-means.</p><p>In typical k-means, the vast majority of computation time is spent on calculating nearest neighbours between the points and cluster centers. We replace this exact computation by an approximate nearest neighbor method, and use a forest of 8 randomized k-d trees built over the cluster centers at the beginning of each iteration to increase speed. We use the implementation from Philbin et al. <ref type="bibr" coords="5,161.87,155.86,14.61,8.74" target="#b11">[12]</ref>. This implementation uses randomized k-d tree code, optimized for matching SIFT descriptors <ref type="bibr" coords="5,252.97,167.81,9.96,8.74" target="#b7">[8]</ref>. Usually in a k-d tree, each node splits the dataset using the dimension with the highest variance for all the data points falling into that node and the splitting value is found by taking the median value along that dimension (although the mean can also be used). In the randomized version, the splitting dimension is chosen at random from among a set of the dimensions with highest variance and the split value is randomly chosen a point close to the median.</p><p>The conjunction of these trees creates an overlapping partition of the feature space and helps to mitigate quantization effects, where features which fall close to a partition boundary are assigned to an incorrect nearest neighbour. This robustness is especially important in high-dimensions, where due to the "curse of dimensionality" <ref type="bibr" coords="5,205.84,302.78,14.61,8.74" target="#b11">[12]</ref>, points will be more likely to lie close to a boundary. A new data point is assigned to the (approximately) closest cluster center as follows. Initially, each tree is traversed to a leaf and the distances to the discriminating boundaries are recorded in a single priority queue for all trees. Then, iteratively the most promising branch from all trees is chosen and keep adding unseen nodes into the priority queue. The stop criteria is the exploration of a fixed number of tree paths. This way, more trees can be used without significantly increasing the search time.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Image Description</head><p>The complete pipeline for extracting the visual descriptors, creating the visual codebook and obtaining the image descriptors is presented in Figure <ref type="figure" coords="6,450.97,155.16,3.87,8.74" target="#fig_0">2</ref>. The proposed pipeline for obtaining the image descriptors is as follows. First, we apply Harris-Laplace detector on the images (training and test) that belong to the second group of images (leaf, flower, fruit, stem, entire and branch) and generate opponent SIFT local descriptors around the detected points. For the first group of images (scans of leaves), we generate TSLA descriptors. Next, we use the generated descriptors to construct the visual codebooks. Note that, different visual codebooks were created for the different views of the plants (seven in total, one for each view). We randomly select a subset of the local descriptors (TSLA and opponent SIFT) from the training images for each view separately.</p><p>The number of the local descriptors is varying from 6M for the scans to 10M for the flowers. We use these descriptors as a input to the approximate k-means algorithm to obtain the clusters/visual words that will constitute the visual codebooks. Finally, each image in the training and test dataset is represented as a sparse high-dimensional histogram of term (visual word) occurrences.</p><p>The similarity between two images is defined as a L 2 distance over the obtained histograms. We use the standard tf-idf weighting scheme <ref type="bibr" coords="6,410.34,347.10,9.96,8.74" target="#b0">[1]</ref>, which downweights the contribution that commonly occurring, and therefore less discriminative, words make to the similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Information Fusion and Classification</head><p>For each run, we used the fact that images in the test dataset are associated with plant observations to perform multiple image queries for all image organs and scans having the same ObservationID value <ref type="bibr" coords="6,326.58,457.28,9.96,8.74" target="#b1">[2]</ref>. The overall process is presented in Figure <ref type="figure" coords="6,177.83,469.24,3.87,8.74" target="#fig_1">3</ref>. More precisely, for each descriptor:</p><p>-We first grouped all the images I 1 , ..., I k coming from the same plant observation using the ObservationID in metadata. -Then, we computed similarity ranking lists of the retrieved images L 1 , ..., L k corresponding to the query images I 1 , ..., I k . -Finally, the 300 first image results were kept for each list and were merged into a final list L using a late fusion scheme.</p><p>We used three different late fusion schemes to obtain the final predictions.</p><p>1. Min. rank fusion: For this fusion scheme, we used the Leave Out algorithm (LO) <ref type="bibr" coords="6,204.43,609.29,9.96,8.74" target="#b6">[7]</ref>. Lists L 1 , ..., L k are merged by setting the rank of an image to the minimum of the ranks in each list. Thus, the best position of an image among the returned lists is kept. The minimal ranks of the classes associated to the corresponding images are considered as a final predictions of the observations.</p><p>2. Probability fusion: For this fusion scheme, first the classes associated to the images from the lists L 1 , ..., L k are ranked per organ (i.e. scans), according to the average L 2 distance between the corresponding query images and the images from their ranked lists L 1 , ..., L k . We took into account only the best two ranked images of one observation. The final predictions (per observation) are obtained by calculating the minimal ranks of the classes. 3. Mixed fusion: This fusion scheme is a combination of the previous two schemes. In particular, for this setup we used min. rank fusion for the scans images and probability fusion for the organ images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>We submitted three runs for the LifeCLEF 2014 Plant Task. As we stated previously, the three runs rely on the same visual descriptors but we used different fusion schemes to obtain the final predictions. The results from the runs are presented in Tabale 2. The table contains the scores by image and observation.</p><p>In our submitted runs, these two values for each run are the same. First we obtained the predictions for the observations and later on, we just apply these predictions for the images that are part of the corresponding observation.</p><p>The best performing run is the run named FINKI Run 1. This run is a combination of the other two runs. Namely, the predictions for the test images denoted with Leaf Scans were taken from the FINKI Run 2 and the predictions for the other images were taken from the run with name FINKI Run 3. We made this combination because in the validation phase, when we apply the algorithm on the ImageCLEF 2013 Plant Task, we obtained better results for the images denoted with Leaf Scan using the technique implemented in FINKI Run 1.</p><p>By comparing the second and third run in Table <ref type="table" coords="8,361.40,119.99,4.98,8.74" target="#tab_2">2</ref> we can conclude that taking into consideration the distribution of the images across the different species does help in boosting the predictive performance. The run named FINKI Run 3 has better score compared to the run with name FINKI Run 2. In Table <ref type="table" coords="8,191.42,286.66,3.87,8.74" target="#tab_3">3</ref>, we present the detailed scores obtained for each type of plant organs. The best results are obtained for the Leaf Scan images. This is to be expected because these images contain only leaves and are taken in very controlled environment, in most of the cases on a white sheet as a background. The second best score is obtained for the images with flowers. The lowest score is obtained for the images with branches and images that contain the entire plant. These images are most challenging in respect to the variant background and lightening conditions under which these images are taken. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Summary and Discussion</head><p>We submitted three runs on LifeCLEF 2014 Plant Task. The image descriptors for all three runs are obtained using the bag-of-visual-words approach. For the leaf scans we are using multiscale triangular shape descriptor and for the other plant organs we are using Opponent SIFT extracted around points of interest obtained using Harris-Laplace detector. We are using approximate k-means (AKM) algorithm to cluster these descriptors in large number of clusters/visual words (approximately 200K). Each image in the training and test dataset is represented as a sparse high-dimensional histogram of term (visual word) occurrences. The similarity between two images is defined as a L 2 distance over the obtained histograms. We use the standard tf-idf weighting scheme, which reduces the contribution that commonly occurring, and therefore less discriminative, words make to the similarity.</p><p>Applied on the LifeCLEF 2014 Plant Task our approach was ranked fifth, out of 10 competing groups. The approach we presented is general. We are planning to extend it with different image descriptors in order to tackle the different aspects of each plant organ/view. The inclusion of more image descriptors requires development of different and more complex weighting/fusion schemes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,183.03,629.89,249.29,7.89"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The pipeline used for obtaining the image descriptors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,134.77,410.40,345.82,8.37;7,134.77,421.38,314.02,7.86"><head>FusionFig. 3 .</head><label>3</label><figDesc>Fig. 3. Multiple image queries. I1, ...I k are leaf images associated to the same Obser-vationID and Img. D is the image descriptor either TSLA or Opponent SIFT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,134.77,645.16,345.84,20.69"><head>Table 1 .</head><label>1</label><figDesc>Example images from each plant view/organ: Scan (scan or scan-like pictures of leaf), photos of Flower, Fruit, Stem, Leaf, Branch and Entire. Distribution of the images in the Pl@ntView dataset.</figDesc><table coords="2,477.28,645.16,3.32,8.74"><row><cell>.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,134.77,187.71,345.82,71.15"><head>Table 2 .</head><label>2</label><figDesc>Scores per image and observation of the 3 runs submitted to the LifeCLEF 2014 Plant Task.</figDesc><table coords="8,147.49,217.72,317.28,41.14"><row><cell>Run name</cell><cell>Run filename</cell><cell cols="2">Score Image Score Observation</cell></row><row><cell cols="2">FINKI Run 1 run maestra per image mixed</cell><cell>0.205</cell><cell>0.205</cell></row><row><cell cols="2">FINKI Run 3 run maestra per image prob</cell><cell>0.204</cell><cell>0.204</cell></row><row><cell cols="2">FINKI Run 2 run maestra per image min rank</cell><cell>0.166</cell><cell>0.166</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,134.77,402.20,356.40,110.43"><head>Table 3 .</head><label>3</label><figDesc>Distribution of the images in Pl@ntView dataset. Run name Branch Entire Flower Fruit Leaf Leaf Scan Stem FINKI Run 1 0.088 0.117 0.255 0.177 0.160 0.400 0.157 FINKI Run 2 0.108 0.099 0.187 0.160 0.140 0.399 0.180 FINKI Run 3 0.088 0.117 0.255 0.177 0.162 0.360 0.159 Our best performing run was ranked fifth from 10 different participants/research group.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>We would like to acknowledge the support of the <rs type="funder">European Commission</rs> through the project <rs type="projectName">MAESTRA -Learning from Massive, Incompletely annotated, and Structured Data</rs> (Grant number <rs type="grantNumber">ICT-2013-612944</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_DyHfEXG">
					<idno type="grant-number">ICT-2013-612944</idno>
					<orgName type="project" subtype="full">MAESTRA -Learning from Massive, Incompletely annotated, and Structured Data</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,142.96,339.62,337.62,7.86;9,151.52,350.58,25.60,7.86" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="9,300.51,339.62,123.50,7.86">Modern Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>ACM Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,361.58,337.62,7.86;9,151.52,372.53,329.06,7.86;9,151.52,383.49,282.07,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,234.48,372.53,246.10,7.86;9,151.52,383.49,17.24,7.86">Inria&apos;s participation at ImageCLEF 2013 Plant Identification Task</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Bakic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mouine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ouertani-Litayem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Verroust-Blondet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Go√´au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,190.22,383.49,188.45,7.86">CLEF (Online Working Notes/Labs/Workshop</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">2013</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,394.49,337.62,7.86;9,151.52,405.45,327.87,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,381.91,394.49,98.67,7.86;9,151.52,405.45,162.81,7.86">Fast and scalable image retrieval using predictive clustering trees</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Dimitrovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kocev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Loskovska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dzeroski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,335.07,405.45,69.71,7.86">Discovery Science</title>
		<imprint>
			<biblScope unit="page" from="33" to="48" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,416.44,337.62,7.86;9,151.52,427.40,329.05,7.86;9,151.52,438.36,182.49,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,382.28,416.44,98.30,7.86;9,151.52,427.40,325.46,7.86">Fast and efficient visual codebook construction for multi-label annotation using predictive clustering trees</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Dimitrovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kocev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Loskovska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dzeroski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,151.52,438.36,112.86,7.86">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="38" to="45" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,449.36,337.62,7.86;9,151.52,460.32,329.06,7.86;9,151.52,471.27,139.04,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,205.05,460.32,171.64,7.86">The imageclef 2012 plant identification task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Go√´au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Barthelemy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Molino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,396.18,460.32,84.40,7.86;9,151.52,471.27,105.62,7.86">CLEF (Online Working Notes/Labs/Workshop</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,482.27,337.62,7.86;9,151.52,493.23,287.72,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,462.27,482.27,18.31,7.86;9,151.52,493.23,112.68,7.86">Lifeclef plant identification task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Go√´au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Barth√©l√©my</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,305.42,493.23,83.65,7.86">CLEF working notes</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page">2014</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,504.22,337.62,7.86;9,151.52,515.18,329.05,7.86;9,151.52,526.14,329.06,7.86;9,151.52,537.10,309.72,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,343.43,504.22,137.15,7.86;9,151.52,515.18,195.84,7.86">Image retrieval based on similarity score fusion from feature similarity ranking lists</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jovi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Hatakeyama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Hirota</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,238.67,526.14,165.94,7.86">Fuzzy Systems and Knowledge Discovery</title>
		<title level="s" coord="9,412.35,526.14,68.23,7.86;9,151.52,537.10,71.32,7.86">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Shi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">4223</biblScope>
			<biblScope unit="page" from="461" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,548.10,337.62,7.86;9,151.52,559.06,224.97,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,205.63,548.10,234.10,7.86">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,448.30,548.10,32.28,7.86;9,151.52,559.06,138.96,7.86">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,570.05,337.62,7.86;9,151.52,581.01,329.06,7.86;9,151.52,591.97,228.04,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,283.61,581.01,157.52,7.86">A comparison of affine region detectors</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Schaffalitzky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kadir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,448.30,581.01,32.28,7.86;9,151.52,591.97,138.96,7.86">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="43" to="72" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,602.96,337.96,7.86;9,151.52,613.92,329.05,7.86;9,151.52,624.88,329.06,7.86;9,151.52,635.84,88.18,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,347.77,602.96,132.81,7.86;9,151.52,613.92,217.04,7.86">A shape-based approach for leaf classification using multiscaletriangular representation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mouine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Verroust-Blondet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,389.71,613.92,90.86,7.86;9,151.52,624.88,287.14,7.86">Proceedings of the 3rd ACM Conference on International Conference on Multimedia Retrieval</title>
		<meeting>the 3rd ACM Conference on International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,646.84,337.96,7.86;9,151.52,657.80,245.97,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,192.50,646.84,228.94,7.86">A threshold selection method from gray-level histograms</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,429.07,646.84,51.50,7.86;9,151.52,657.80,169.17,7.86">IEEE Transactions on Systems, Man and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="66" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,120.67,337.96,7.86;10,151.52,131.63,329.05,7.86;10,151.52,142.59,191.79,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,394.43,120.67,86.14,7.86;10,151.52,131.63,178.69,7.86">Object retrieval with large vocabularies and fast spatial matching</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,352.94,131.63,127.63,7.86;10,151.52,142.59,126.17,7.86">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,153.55,337.96,7.86;10,151.52,164.51,329.06,7.86;10,151.52,175.47,96.76,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,311.95,153.55,168.62,7.86;10,151.52,164.51,66.38,7.86">Evaluating color descriptors for object and scene recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,223.97,164.51,256.61,7.86">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1582" to="1596" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,186.42,337.96,7.86;10,151.52,197.38,301.30,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,248.47,186.42,232.11,7.86;10,151.52,197.38,34.28,7.86">Video google: a text retrieval approach to object matching in videos</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,206.87,197.38,152.89,7.86">IEEE Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1470" to="1477" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
