<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,147.45,115.96,320.45,12.62">Bird Classification using Ensemble Classifiers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,170.66,153.75,34.73,8.74"><forename type="first">Yi</forename><surname>Leng</surname></persName>
							<email>yrleng@i2r.a-star.edu.sg</email>
						</author>
						<author>
							<persName coords="1,208.71,153.75,17.30,8.74"><surname>Ren</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Infocomm Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,236.56,153.75,111.59,8.74"><forename type="first">Jonathan</forename><forename type="middle">William</forename><surname>Dennis</surname></persName>
							<email>jonathan-dennis@i2r.a-star.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Infocomm Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,378.08,153.75,62.16,8.74"><forename type="first">Tran</forename><forename type="middle">Huy</forename><surname>Dat</surname></persName>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">STAR</orgName>
								<address>
									<settlement>Singapore</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,147.45,115.96,320.45,12.62">Bird Classification using Ensemble Classifiers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">450A974BF9EBD3B5D0925EF9675227E3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ensemble</term>
					<term>audio</term>
					<term>classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This working note summarizes our submission to the Life-CLEF 2014 Bird Task which combines the outputs from a Python and Matlab classification system. The features used for both systems include Mel-Frequency Cepstral Coefficients (MFCC), time-averaged spectrograms and the provided meta-data. The Python subsystem combines a large ensemble of different classifiers with different subsets of the features while the Matlab subsystem is an ensemble of the Random Forest and Linear Discriminant Analysis (LDA) classifiers using local spectral and meta features. By combining this disparate set of features and classifiers, we managed to achieve a Mean Average Precision (MAP) score that is far superior to what is possible with any single classifier.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>LifeCLEF 2014 Bird Task (BirdCLEF) <ref type="bibr" coords="1,301.85,428.73,14.82,8.74" target="#b0">[1]</ref> involves 14027 audio recordings containing 501 bird species. In comparison, the 9th annual MLSP competition <ref type="bibr" coords="1,467.26,440.69,13.34,8.74" target="#b2">[3]</ref> involves 645 recordings with 19 bird species while the Neural Information Processing Scaled for Bioacoustics (NIPS4B) bird song competition <ref type="bibr" coords="1,407.47,464.60,13.34,8.74" target="#b3">[4]</ref> involves 1000 recordings with 87 bird species. The greatly increased size in both the available data and the number of target classes for BirdCLEF introduces new challenges.</p><p>The total size of the audio database for both the MLSP and NIPS4B challenges amounts to around 1GB while the BirdCLEF data is over 20GB in size. The winning methods for the MLSP and NIPS4B challenges performed image segmentation on the audio spectrograms followed by template matching to derive class models. Based on the difference in data size and the number of classes, it can be extrapolated that using the same methods for BirdCLEF will require at least a hundred fold increase in computation time. Given that the time between the release of the training data and the final test submission is only around three months, it would not be efficient to try to reproduce the above methods given our limited computing resources.</p><p>All of our experiments on BirdCLEF are performed on standard 4-core desktop PCs with 8GB of RAM thus there are severe constraints on the possible machine learning methods that can be applied. Instead of attempting to build a strong learner using well-engineered features, we chose to work on building a number of weak learners with simple features and finding the optimal combination of such learners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Front-end</head><p>We make use of both audio and meta data for our feature front-end as they provide complementary information to the classifier. The total memory used in the computation is heavily dependent on the size of the feature used thus it is important to select a compact representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">MFCC</head><p>The MFCC features provided by the organizers consist of a single log energy coefficient and 15 cepstral coefficients, appended with the first and second derivatives for a total of 48 dimensions per time frame. For each audio clip, we perform a simple energy-based segmentation by dropping all the time frames where the value of the log energy coefficient is below the median value. The remaining frames are further reduced by taking the maximum value for each of the 48 dimensions to arrive at a final 48 dimension feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Spectral features</head><p>The spectral features we used are similar to the spectrum-summarizing features described in <ref type="bibr" coords="2,190.54,392.90,9.96,8.74" target="#b2">[3]</ref>. The audio clips are converted into spectrograms by windowing the audio into overlapping 200ms windows at 100ms intervals. The frequency dimension is binned into n dimensions using a simple mean. The time dimension is eliminated by taking the maximum value similar to the MFCC feature. Finally, the natural logarithm is applied to the n dimension spectral features to reduce its dynamic range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Meta features</head><p>In addition to the audio features described above, we also make use of the metadata represented by the following eight fields: Latitude, Longitude, Elevation, Y ear, M onth, M onth + Day, T ime, Author</p><p>The M onth + Day field combines the two-digit M onth and Day fields into a single four-digit field as comparing the Day fields across different months is illogical.</p><p>One of the challenges of BirdCLEF compared to MLSP which also provides meta-data in the form of a numerical location index <ref type="bibr" coords="2,358.35,596.34,12.77,8.74" target="#b2">[3]</ref> is the presence of unreliable and missing data. The completion of the meta-data fields by the BirdCLEF data contributors is optional with no enforced format. Unreliable information such as "afternoon" or "am" in the T ime field and completely missing entries have to be manually parsed into logical numeric values for use as features and their presence will affect the performance of the classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Back-end</head><p>Although the official labels for the audio data only gives a single bird species as the ground truth given by ClassId, the BackgroundSpecies field often list additional bird species. However, like the meta-data, this field is unreliable as there is no indication if no additional species are present or identified, or if the author simply did not complete this particular field. We can only assume that an empty BackgroundSpecies field indicates that only the single species given by ClassId is present.</p><p>Another problem that arises due to the BackgroundSpecies field is the presence of bird species that are outside of the 501 species to be classified. For the given training data, there are over three hundred additional species and we chose to retain the sixteen extra species that have over eleven examples each for a total of 517 target species. The remaining labels are ignored since there are insufficient examples to train proper models for them. By making use of the extra information from BackgroundSpecies, the number of examples for the 501 original target classes is also increased and the task becomes a multi-label classification problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Python</head><p>The classifiers used in the Python subsystem are built from the scikit-learn toolkit (sklearn) <ref type="bibr" coords="3,205.20,385.52,12.70,8.74" target="#b4">[5]</ref>. In order to build an ensemble of classifiers, we require the classifiers to output probability estimates for each of the target classes so that the final output is simply a mean of the individual classifier outputs. The subsequent computation of the Mean Average Precision (MAP) is also straightforward with such class probability outputs as opposed to max voting. Out of the available classifiers in the toolkit, we selected the following list to evaluate: ExtraTreesClassifier, RandomForestClassifier, KNeighborsClassifier, LogisticRegression, SGDClassifier, AdaBoostClassifier, GradientBoostingClassifier, SVC, GaussianNB, BernoulliNB, LDA From the above list, only ExtraT reesClassif ier, RandomF orestClassif ier and KN eighborsClassif ier are able to handle multi-label classification by default. The OneV sRestClassif ier is used to transform the remaining binary classifiers into multi-label multi-class classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Matlab</head><p>Two classifiers are used in the Matlab subsystem: Random Forests using the T reeBagger function and LDA using the Classif icationDiscriminant.f it function, both of which were trained in a one-vs-rest configuration. Both of these classifiers output probability estimates through the predict function and the final output is the mean of the individual classifier outputs. The T reeBagger function was modified such that each tree in the forest was trained with a balanced amount of negative data, as this was found to considerably improve the speed of the classification without affecting performance. In particular, each tree was trained with all of the positive class data, and a random subset of the negative data containing twenty times as much as the positive data. The number of variables to sample at each node was set to 3, while the number of trees was set to 200 for the final evaluation run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment Setup</head><p>To evaluate the performance of a feature and classifier combination, 10-fold cross-validation is used on the given training data. The objective is to maximize the MAP score of the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Python subsystem</head><p>From the list of feature front-ends and the selected classifiers from sklearn, combinations of feature and classifier pairs are evaluated. This step took up the bulk of our time as it involves the optimizing of feature settings, classifier settings and the combinations of the two to build the final ensemble classifier. Out of the available classifiers, the following six were selected:</p><p>1. Linear Discriminant Analysis (LDA), LDA in sklearn 2. Logistic Regression (LR), LogisticRegression in sklearn 3. Support Vector Machines (SVM) with Radial Basis Function (RBF) kernel, SV C in sklearn 4. AdaBoost (AdaB), AdaBoostClassif ier in sklearn 5. K-Nearest Neighbours (KNN), KN eighborsClassif ier in sklearn 6. Random Forests (RanF), RandomF orestClassif ier in sklearn For the feature front-end, three combinations were selected:</p><p>1. M eta + M F CC + ∆M F CC + Spec10 + Spec100 30 , used with LDA, LR and SVM 2. M eta + M F CC + Spec100 30 , used with AdaB and KNN 3. M eta, used with RanF M F CC refers to the first 16 dimensions of the described 48 dimension feature while ∆M F CC are the next 16 dimensions (indices 17 to 32) which represents the first derivatives or deltas. Spec10 refers to the spectral features with n = 10 and Spec100 30 are the first 30 dimensions of the spectral features with n = 100. M eta refers to the eight dimension meta features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Matlab subsystem</head><p>Only one feature configuration is used with the Matlab subsystem. Using the same terminology as above, the features can be denoted M eta + Spec40, with n = 40 dimensions used for the spectral feature concatenated with the eight dimensions of the meta features. However, instead of taking the maximum over the whole time frame, as with the Python subsystem, local spectral features are generated by taking the maximum within selected local windows. Each window is 0.5 seconds long, and the step is equivalent to the frame increment. The energy of each window is calculated simply as the average log-energy over the window, and is then used as a metric for selecting the windows for classification. Only those windows that are represent local maxima in the energy function are retained, with the remaining windows discarded. The idea is that the windows containing local maxima in the energy will correspond to local instances of the bird song in the clip, hence provide a complimentary source of information to the global features used in the Python subsystem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>Although we train the classifiers on 517 target species, the results presented only consider the actual 501 target species (MAP with k = 501).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Python subsystem</head><p>The results shown in Table <ref type="table" coords="5,256.79,424.98,4.98,8.74" target="#tab_0">1</ref> give a rough breakdown of the individual components of the Python subsystem. For the sake of brevity, the other combinations of the six selected classifiers are not shown. The Audio and M eta columns show how each classifier performs when the complementary meta or audio features are removed. For all but the KNN classifier, combining both audio and meta features is shown to improve the classifier performance. Although the KNN classifier is shown to give a better MAP score with the meta feature alone, when combined with the other classifiers in an ensemble, adding the audio features is found to result in a better score overall. It is interesting to note that using the 8-dimension meta features alone with the chosen classifiers can give a reasonable score of 0.200, highlighting the value of using meta-data for classification tasks.</p><p>Table <ref type="table" coords="6,176.69,190.73,4.98,8.74" target="#tab_1">2</ref> shows the approximate computation time for the Python subsystem. One of our objectives is to design a system with a short turnaround time as the search space for the system parameters is huge. With each new dataset, it is likely that the optimal system configuration in terms of feature and classifier choices will be different thus it will be necessary to retune the entire system from scratch. There is a large discrepancy in the computation time required for each of the classifiers chosen that is partly attributed to the different feature combinations used. The strength of using an ensemble of classifiers is the option to mix and match different combinations of the components to suit particular demands for performance and computation cost. The three fastest classifiers (LDA, KNN and RanF) combine to give an ensemble which runs in less than 15 minutes with a respectable score of 0.290. In contrast, removing RanF from the six classifier ensemble only saves 8 minutes but reduces the score from 0.313 to 0.274. There are no general rules determining the performance of each specific combination, especially if the dataset is changed, thus it is usually necessary to perform an exhaustive search to find the optimal system. Another strength of the ensemble classifier is how the individual classifiers complement each other. For most of the classifiers tested, it is possible to improve the individual score at the cost of computation time by changing parameters such as increasing the number of iterations or the number of neighbours to query. However, these improvements do not necessarily transfer to the ensemble classifier when they are combined as the individual gains are made obsolete through the contributions of the other classifiers. Depending on the classifiers used, it is often possible to replicate a computation-intensive performance gain from a classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Matlab subsystem</head><p>The results shown in Table <ref type="table" coords="6,260.40,632.21,4.98,8.74" target="#tab_2">3</ref> give a breakdown of the individual components of the Matlab subsystem using the combined Audio + M eta features. It can be seen that the LDA classifier is performing relatively poorly, perhaps due the Due to time constraints, we did not manage to evaluate the performance of the combined Python and Matlab system through cross-validation. Table <ref type="table" coords="7,448.04,449.84,4.98,8.74" target="#tab_3">4</ref> shows the final results for our proposed systems on the evaluation data. Compared to Table <ref type="table" coords="7,163.49,473.75,3.87,8.74" target="#tab_0">1</ref>, the Python subsystem shows a 10% relative loss with the inclusion of BackgroundSpecies. This might be due to the use of a lower k value when evaluating the MAP score or a mismatch between the training and evaluation data. The combined system is found to improve on the Python subsystem score slightly despite the poor performance of the Matlab subsystem.</p><p>Our system was trained as a multi-label classifier and this is reflected by the lower score when the background species are removed (MAP 2). Interestingly, we are the only group that has a lower MAP 2 score than MAP 1, indicating that we are the only group that focussed only identifying the background species as well as the dominant species.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>The challenges presented by BirdCLEF include the large amount of provided data, the large number of target species to be classified, incomplete or inaccurate multi-class labels and missing and unreliable meta-data. The limited time between the release of the training data and the final submission and the limited computing resources available to our group are additional obstacles we faced. With these limitations in mind, we designed an ensemble classifier that combines the output from a number of individually weak learners using simple features that are fast to train and test. The final system is shown to provide greatly improved results over the component classifiers and the fast turnaround time allows the setup to be quickly calibrated for new tasks and datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,155.95,505.35,303.44,152.14"><head>Table 1 .</head><label>1</label><figDesc>Selected results for 10-fold cross-validation for Python subsystem</figDesc><table coords="5,173.80,527.68,264.68,129.81"><row><cell cols="9">LDA LogR SVM AdaB KNN RanF Audio Meta Both</cell></row><row><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">0.191 0.075 0.220</cell></row><row><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">0.197 0.076 0.232</cell></row><row><cell></cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell cols="3">0.199 0.085 0.236</cell></row><row><cell></cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell></cell><cell cols="3">0.085 0.110 0.133</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell></cell><cell cols="3">0.130 0.177 0.164</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell>-</cell><cell>0.192</cell><cell>-</cell></row><row><cell>x</cell><cell>x</cell><cell>x</cell><cell>x</cell><cell>x</cell><cell></cell><cell cols="3">0.220 0.195 0.274</cell></row><row><cell>x</cell><cell></cell><cell></cell><cell></cell><cell>x</cell><cell>x</cell><cell>-</cell><cell cols="2">0.195 0.290</cell></row><row><cell>x</cell><cell>x</cell><cell>x</cell><cell>x</cell><cell>x</cell><cell>x</cell><cell>-</cell><cell cols="2">0.200 0.313</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,134.77,281.86,345.81,54.71"><head>Table 2 .</head><label>2</label><figDesc>Approximate computation time for 10-fold cross-validation for Python sub-</figDesc><table coords="6,134.77,292.84,301.07,43.72"><row><cell>system components</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LDA LogR</cell><cell>SVM</cell><cell>AdaB KNN RanF</cell><cell>Total</cell></row><row><cell cols="4">5 min 15 min 1h 10 min 11 min 17 sec 8 min 1h 45 min</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,134.77,115.91,345.84,207.55"><head>Table 3 .</head><label>3</label><figDesc>Classification results for 10-fold cross-validation for the Matlab subsystem number of false positives in the features. These false positives arise from using local features that are extracted from local maxima in the energy, and hence may represent noise or other information that is not related to the target bird species. On the other hand, the random forest classifier is able to select the features that best separate the target classes, and this allows it to handle a large number of false positives in the labelled data. Combining the two classifiers together increases the score, giving an MAP of 0.222 on the cross-validation.</figDesc><table coords="7,247.47,138.25,117.35,48.51"><row><cell cols="3">LDA RanF Audio + Meta</cell></row><row><cell>x</cell><cell></cell><cell>0.119</cell></row><row><cell></cell><cell>x</cell><cell>0.216</cell></row><row><cell>x</cell><cell>x</cell><cell>0.222</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,185.96,350.49,240.35,57.29"><head>Table 4 .</head><label>4</label><figDesc>Final results for proposed system</figDesc><table coords="7,185.96,372.82,240.35,34.96"><row><cell></cell><cell cols="3">Python Matlab Combined</cell></row><row><cell>with BackgroundSpecies</cell><cell>0.284</cell><cell>0.166</cell><cell>0.289</cell></row><row><cell cols="2">without BackgroundSpecies 0.267</cell><cell>0.159</cell><cell>0.272</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,138.35,246.20,342.22,7.86;8,142.31,257.16,151.27,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,362.99,246.20,117.58,7.86;8,142.31,257.16,18.99,7.86">LifeCLEF Bird Identification Task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rauber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,188.43,257.16,83.65,7.86">CLEF working notes</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,268.12,342.23,7.86;8,142.31,279.08,338.27,7.86;8,142.31,290.04,229.97,7.86" xml:id="b1">
	<analytic>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Halvey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-1180/" />
	</analytic>
	<monogr>
		<title level="m" coord="8,433.96,268.12,46.62,7.86;8,142.31,279.08,85.98,7.86">CLEF 2014 Labs and Workshops</title>
		<title level="s" coord="8,238.04,279.08,242.54,7.86;8,142.31,290.04,9.03,7.86">Notebook Papers. CEUR Workshop Proceedings (CEUR-WS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,301.00,342.23,7.86;8,142.31,311.95,338.27,7.86;8,142.31,322.91,338.27,7.86;8,142.31,333.87,112.12,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,224.27,301.00,256.31,7.86;8,142.31,311.95,314.21,7.86">The 9th annual MLSP competition: New methods for acoustic classification of multiple simultaneous bird species in a noisy environment</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Briggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,464.45,311.95,16.13,7.86;8,142.31,322.91,184.34,7.86">Machine Learning for Signal Processing (MLSP)</title>
		<imprint>
			<date type="published" when="2013-09">2013. Sept. 2013</date>
			<biblScope unit="page" from="22" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,344.83,342.22,7.86;8,142.31,355.79,338.27,7.86;8,142.31,366.75,18.43,7.86" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="8,418.59,344.83,61.98,7.86;8,142.31,355.79,333.24,7.86">Proc. of Neural Information Processing Scaled for Bioacoustics: from Neurons to Big Data -NIP4B</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Artières</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Tchernichovski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,377.71,342.22,7.86;8,142.31,388.67,175.18,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,232.09,377.71,163.99,7.86">Scikit-learn: Machine Learning in Python</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,403.45,377.71,77.12,7.86;8,142.31,388.67,72.63,7.86">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
