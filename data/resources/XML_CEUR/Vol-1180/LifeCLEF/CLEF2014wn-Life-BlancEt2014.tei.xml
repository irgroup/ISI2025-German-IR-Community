<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,141.16,115.96,333.04,12.62;1,277.86,133.89,59.64,12.62">Fish species recognition from video using SVM classifier</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,205.41,171.56,47.86,8.74"><forename type="first">Katy</forename><surname>Blanc</surname></persName>
						</author>
						<author>
							<persName coords="1,261.17,171.56,66.18,8.74"><forename type="first">Diane</forename><surname>Lingrand</surname></persName>
						</author>
						<author role="corresp">
							<persName coords="1,335.37,171.56,74.58,8.74"><forename type="first">Frédéric</forename><surname>Precioso</surname></persName>
							<email>precioso@i3s.unice.fr</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR</orgName>
								<orgName type="institution">Univ. Nice Sophia Antipolis</orgName>
								<address>
									<addrLine>I3S, 7271</addrLine>
									<postCode>06900</postCode>
									<settlement>Sophia Antipolis</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">UMR 7271</orgName>
								<orgName type="institution">CNRS</orgName>
								<address>
									<addrLine>I3S</addrLine>
									<postCode>06900</postCode>
									<settlement>Sophia Antipolis</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,141.16,115.96,333.04,12.62;1,277.86,133.89,59.64,12.62">Fish species recognition from video using SVM classifier</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">208DF41849BF993DD425ECC266FC49F2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For this first edition of LifeCLEF fish identification, we have built a processing chain based on background extraction, selection and description of keypoints with an adaptive scale and learning of each species by a binary linear SVM classifier. From the foreground segmentation, we have extracted several groups of blobs, representing a fish each. We have submitted three runs for different blob sizes and associations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Fish task of the LifeCLEF2014 <ref type="bibr" coords="1,303.89,362.99,10.52,8.74" target="#b3">[4,</ref><ref type="bibr" coords="1,316.07,362.99,7.75,8.74" target="#b2">3]</ref> is organized in 4 subtasks such as video based fish identification, image based fish identification, image based fish identification and species recognition, and image based fish species recognition. We consider the subtask 3: we have to detect a fish and recognize his species inside a video. The training set is made of 285 videos labeled with 19868 bounding boxes of fishes and their species annotated. Our method is mainly based on motion detection and a set of binary Support Vector Machines trained with OpponentSift descriptors. The next section will describe our method in details. Results are discussed in Section 3, followed by conclusions and perspectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Detection and Identification Method</head><p>Our processing chain starts with a background-foreground segmentation from motion detection (see figure <ref type="figure" coords="1,259.38,524.50,3.87,8.74" target="#fig_0">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Background and motion detection</head><p>For this first step, an adaptive background mixture model <ref type="bibr" coords="1,407.07,572.43,10.52,8.74" target="#b6">[7]</ref> is built. This method consists in assuming that each pixel in the scene is modeled by a mixture of many Gaussian distributions and then each pixel of the background is computed with the distributions with the smallest fitness value. Finally a background subtraction is performed by marking as a foreground pixel, any pixel that is over 2.5 times any standard deviation of any background distribution. We end up with a mask for the motion detection that we first erode and then dilate to define blobs of detected moving objects. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Points of interest and their descriptions</head><p>The XML metadata provided with the training set define the bounding boxes of annotated fishes, so we extract keypoints in these bounding boxes (figure <ref type="figure" coords="2,468.98,333.38,3.87,8.74" target="#fig_1">2</ref>). Specifically, these keypoints are located at the center of the given bounding box and several scales are set for the keypoint descriptor so that the resulting set of keypoints describe the whole fish. The initial scale is set to fit the size of the bounding box, the other scales are sequentially incremented from the initial center. To detect a fish with a part of it, the head or the tail, we also slightly shift the central initial keypoint to the left and the right of the bounding box in a spatial-pyramid-like technique. We adopt this strategy of large keypoint descriptors owing to the low resolution of the videos. The OpponentSift schema <ref type="bibr" coords="3,265.82,118.99,10.52,8.74" target="#b4">[5]</ref> is used to describe these keypoints. The offline part of our processing chain ends here and we will now explain the online part of the system: analyzing a test video. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Background description, filtering and learning</head><p>Since the training distribution must be the same as the test distribution in a supervised learning process, we decide to train a SVM classifier with some Op-ponentSift descriptors from the video background as negative training samples. The SIFT detector did not give us enough keypoints owing to the low resolution. Thus we choose to densely extract keypoints with approximately same scales as the scales of the training keypoints. Specifically, we have extracted keypoints with fixed scales from 30 to 110 pixels of diameter with respect to the size of the video. To ensure the definition of the background, these keypoints were located in the still areas of the motion mask (black areas on figure <ref type="figure" coords="3,364.77,511.54,3.87,8.74" target="#fig_0">1</ref>).</p><p>For most of the training bounding boxes, part of the background is inside. Thus, before training, we filter the keypoints inside the bounding boxes (which should then be considered as positive training samples of the associated fish): For each one of these keypoints we look for its 10 nearest neighbors and remove any keypoint from the bounding box with more negative neighbors (keypoints from the background not in bounding boxes from the same video) than positive ones (keypoints from other bounding boxes on the same video).</p><p>Then, for each species, we train a linear SVM classifier with the species keypoints descriptors as positive samples and the aforementioned filtered background descriptors as negative examples.</p><p>We have hence done the first row of our processing chain during the query video classification. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Species and bounding box</head><p>First of all, we segment the motion mask to differentiate each blob. Then, for each blob, the centered and shifted keypoints are extracted with several scales (fixed scales from 30 to 110) and their scores are computed by each species SVM classifier. This score represents the distance between the current descriptor and the SVM decision boundary with the sign of the selected class. Only positively classified points with a distance larger than 0.5 are considered. For a blob, we sum up the score over each keypoint associated with each species in order to obtain a global score per species. Then, the list of potential species is given by decreasing scores. For our bounding box construction, we frame every keypoint with respect to its scale and associate the species classified with the highest score (first species from the list). Finally, we submitted three runs:</p><p>1. The first one with the blobs computed from the mask segmentation. 2. For the second run, we wanted to detect if there were several fishes in connected blobs. Thus we have computed a lighter dilation (than in the first run) on the initial motion mask and detected the dominant color in each blob. Small blobs with same dominant color were merged into one blob. Then each blob was dilated to obtain the maximum numbers of keypoints on the fishes. You can see on figure <ref type="figure" coords="5,251.60,206.45,4.98,8.74" target="#fig_4">5</ref> that even if the fish is separated in several blobs, these ones are then merged together. Finally the same processing is applied to each group of blobs as it is in run 1.</p><p>Fig. <ref type="figure" coords="5,202.78,357.11,4.13,7.89">6</ref>. Fish segmentation with dilatation and dominant color 3. The third run has computed groups of blobs as in the run 2 except that we have provided as many answers as little connected blobs regardless their dominant color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimentation and results</head><p>At the beginning, we tried to track fishes in order to obtain more descriptors with some standard tracking methods as CamShift and segmentation of optical flow for instance. But the tracking was not effective enough owing to the resolution. We also encountered difficulties with moving seaweed and changing brightness as warned by the organizers since the dataset contains videos recorded from sunrise to sunset. We have built up our processing chain to deal with these difficulties. As scoring functions, the organizers of LifeCLEF have computed both average precision vs recall and precision vs recall for each fish species for the subtask3 (figure <ref type="figure" coords="5,166.21,558.54,4.98,8.74">6</ref> and<ref type="figure" coords="5,193.64,558.54,3.87,8.74" target="#fig_5">7</ref>). Only bounding boxes matching with a bounding box from test set with a PASCAL score above a certain threshold are considered. The organizers have computed a baseline program with the ViBe background modeling approach <ref type="bibr" coords="5,178.25,594.40,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="5,190.43,594.40,7.75,8.74" target="#b1">2]</ref> for fish detection and VLFeat <ref type="bibr" coords="5,336.98,594.40,10.52,8.74" target="#b5">[6]</ref> for fish species recognition to compare against our method.</p><p>Compared to the baseline, we have a better precision and worse recall. It is worth noting that the performance in terms of species recognition (based only on the correctly detected bounding boxes) was comparable to the baseline, but our bounding boxes were often too large. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This competition was a real challenge since the video data set was difficult regarding video resolution, natural phenomena (e.g. murky water, algae on camera lens, ...) and the huge amount of data to be processed in a limited time. Our results are so pretty encouraging.</p><p>To improve our processing chain, a good tracking from the annotated fishes would allow us obtaining more positive descriptors for each species. Moreover, some metadata could be used as the GPS coordinates. Finally, with the tracking, we could study the movement of each species to see if this is specific to species and we could try analyzing the interactions between species.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,213.67,260.07,188.01,7.89;2,148.14,116.84,157.50,118.50"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An original frame and its motion mask</figDesc><graphic coords="2,148.14,116.84,157.50,118.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,213.67,655.03,188.01,7.89;2,197.08,462.65,221.20,167.65"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Keypoints on a learning annotated fish</figDesc><graphic coords="2,197.08,462.65,221.20,167.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,148.11,328.20,319.12,7.89;3,136.88,175.07,341.60,128.40"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Extraction of positive descriptors from training data set with metadata</figDesc><graphic coords="3,136.88,175.07,341.60,128.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,135.41,295.68,344.53,7.89;4,134.77,116.83,356.40,154.11"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Our processing chain from acquiring test video to detection and identification</figDesc><graphic coords="4,134.77,116.83,356.40,154.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="4,134.77,623.90,345.81,7.89;4,134.77,634.88,345.81,7.86;4,134.77,645.84,345.81,7.86;4,134.77,656.80,23.68,7.86;4,134.98,512.04,345.42,87.12"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. From left to right:(a) original frame, (b) the detected and positively classified keypoints with the color corresponding to the species with highest score and (c) the final bounding boxes with the color corresponding to species with the highest sum over scores</figDesc><graphic coords="4,134.98,512.04,345.42,87.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="6,134.77,291.77,345.81,7.89;6,134.77,302.76,56.41,7.86;6,145.14,327.03,325.08,122.30"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Recall on each species and average recall of our three runs and the baseline of the organizers</figDesc><graphic coords="6,145.14,327.03,325.08,122.30" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,138.35,142.59,342.22,7.86;7,146.91,153.55,333.67,7.86;7,146.91,164.51,333.67,7.86;7,146.91,175.47,190.99,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,308.91,142.59,171.66,7.86;7,146.91,153.55,159.00,7.86">ViBe: a powerful random technique to estimate the background in video sequences</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Barnich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Van Droogenbroeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,324.68,153.55,155.90,7.86;7,146.91,164.51,188.52,7.86">International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2009)</title>
		<imprint>
			<date type="published" when="2009-04">April 2009</date>
			<biblScope unit="page" from="945" to="948" />
		</imprint>
	</monogr>
	<note>PDF available on the University site or at the IEEE</note>
</biblStruct>

<biblStruct coords="7,138.35,186.42,342.23,7.86;7,146.91,197.38,333.68,7.86;7,146.91,208.34,67.20,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,311.04,186.42,169.54,7.86;7,146.91,197.38,115.36,7.86">ViBe: A universal background subtraction algorithm for video sequences</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Barnich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Van Droogenbroeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,269.27,197.38,158.40,7.86">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1709" to="1724" />
			<date type="published" when="2011-06">June 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,219.30,342.22,7.86;7,146.91,230.26,168.63,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,358.68,219.30,121.89,7.86;7,146.91,230.26,16.79,7.86">Lifeclef fish identification task 2014</title>
		<author>
			<persName coords=""><forename type="first">Spampinato</forename><surname>Concetto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bob</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bas</forename><surname>Boom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,183.51,230.26,103.70,7.86">CLEF working notes 2014</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,241.22,342.23,7.86;7,146.91,252.18,333.66,7.86;7,146.91,263.14,333.68,7.86;7,146.91,274.10,20.99,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,451.77,252.18,28.80,7.86;7,146.91,263.14,208.89,7.86">Lifeclef 2014: multimedia life species identification challenges</title>
		<author>
			<persName coords=""><forename type="first">Alexis</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hervé</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hervé</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Concetto</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andreas</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Willem-Pier</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bob</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,373.36,263.14,102.96,7.86">Proceedings of CLEF 2014</title>
		<meeting>CLEF 2014</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,285.05,342.23,7.86;7,146.91,296.01,333.68,7.86;7,146.91,306.97,333.67,7.86;7,146.91,317.93,100.00,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,425.55,285.05,55.02,7.86;7,146.91,296.01,193.92,7.86">Evaluation of color descriptors for object and scene recognition</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Theo</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,359.22,296.01,121.37,7.86;7,146.91,306.97,281.88,7.86">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition<address><addrLine>Anchorage, Alaska, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06">June 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,328.89,342.22,7.86;7,146.91,339.85,196.21,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="7,271.23,328.89,209.34,7.86;7,146.91,339.85,67.43,7.86">VLFeat: An open and portable library of computer vision algorithms</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Fulkerson</surname></persName>
		</author>
		<ptr target="http://www.vlfeat.org/" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,350.81,342.23,7.86;7,146.91,361.77,333.66,7.86;7,146.91,372.73,333.67,7.86;7,146.91,383.68,148.10,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,216.94,350.81,263.64,7.86;7,146.91,361.77,30.51,7.86">Improved adaptive gaussian mixture model for background subtraction</title>
		<author>
			<persName coords=""><forename type="first">Zoran</forename><surname>Zivkovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,197.61,361.77,282.97,7.86;7,146.91,372.73,56.93,7.86;7,308.73,372.73,37.86,7.86">Proceedings of the Pattern Recognition, 17th International Conference on (ICPR&apos;04)</title>
		<meeting>the Pattern Recognition, 17th International Conference on (ICPR&apos;04)<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="28" to="31" />
		</imprint>
	</monogr>
	<note>ICPR &apos;04</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
