<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,134.77,116.95,345.82,12.62;1,251.93,134.89,111.49,12.62">Audio-only bird classification using unsupervised feature learning</title>
				<funder ref="#_3G7gMMt #_Pqr3j7x">
					<orgName type="full">EPSRC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,229.42,172.78,52.45,8.74"><forename type="first">Dan</forename><surname>Stowell</surname></persName>
							<email>dan.stowell@qmul.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Digital Music</orgName>
								<orgName type="institution">Queen Mary University of London</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,304.55,172.78,81.40,8.74"><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Digital Music</orgName>
								<orgName type="institution">Queen Mary University of London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,134.77,116.95,345.82,12.62;1,251.93,134.89,111.49,12.62">Audio-only bird classification using unsupervised feature learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E16D0D6F01ED5B921F56E7F45616B7F7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe our method for automatic bird species classification, which uses raw audio without segmentation and without using any auxiliary metadata. It successfully classifies among 501 bird categories, and was by far the highest scoring audio-only bird recognition algorithm submitted to BirdCLEF 2014. Our method uses unsupervised feature learning, a technique which learns regularities in spectro-temporal content without reference to the training labels, which helps a classifier to generalise to further content of the same type. Our strongest submission uses two layers of feature learning to capture regularities at two different time scales.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic species classification of birds from their sounds has many potential applications in conservation, ecology and archival <ref type="bibr" coords="1,354.85,417.56,15.50,8.74" target="#b10">[11,</ref><ref type="bibr" coords="1,372.00,417.56,7.01,8.74" target="#b5">6]</ref>. However, to be useful it must work with high accuracy across large numbers of possible species, on noisy outdoor recordings and at big data scales. The ability to scale to big data is crucial: remote monitoring stations can generate huge volumes of audio recordings, and audio archives contain large volumes of audio, much of it without detailed labelling. Big data scales also imply that methods must work without manual intervention, in particular without manual segmentation of recordings into song syllables, or into vocal/silent sections. The lack of segmentation is a pertinent issue for both remote monitoring and archive collections, since many species of bird may be audible for only a minority of the recorded time, and therefore much of the audio will contain irrelevant information.</p><p>In this paper we describe a method for dramatically improving the performance of a supervised classifier for bird sounds, as submitted to the "BirdCLEF" 2014 evaluation contest. The method achieved the strongest performance among audio-only methods (i.e. methods that did not use additional information such as date or location).</p><p>Our method is the subject of a full-length article which can be read at <ref type="bibr" coords="1,462.34,609.29,14.61,8.74" target="#b14">[15]</ref>. In the following shorter presentation, we describe the method, which works on raw audio with no segmentation. We describe how we evaluated variants of our method and chose which variants to submit, and we consider the run-time of different stages of the workflow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Spectral features and feature learning</head><p>For classification, audio data is often converted to a spectrogram-like representation, i.e. the magnitudes of short-time Fourier transformed (STFT) frames of audio, around 10 ms duration per frame. It is common to transform the frequency axis to a more perceptual scale, such as the Mel scale originally intended to represent the approximately logarithmic sensitivity of human hearing. This also reduces the dimensionality of the spectrum, but even the Mel spectrum has traditionally been considered rather high-dimensional for automatic analysis. A further convention, originating from speech processing, is to transform the Mel spectrum using a cepstral analysis and then to keep the lower coefficients (e.g. the first 13) which typically contain most of the energy. These coefficients, the Mel frequency cepstral coefficients (MFCCs), became widespread in applications of machine learning to audio, including bird vocalisations <ref type="bibr" coords="2,388.16,273.38,14.61,8.74" target="#b12">[13]</ref>.</p><p>MFCCs have some advantages, including that the feature values are approximately decorrelated from each other, and they give a substantially dimensionreduced summary of spectral data. Dimension reduction is advantageous for manual inspection of data, and also for use in systems that cannot cope with high-dimensional data. However, as we will see, modern classification algorithms can cope very well with high-dimensional data, and dimension reduction always reduces the amount of information that can be made available to later processing, risking discarding information that a classifier could have used. Further, there is little reason to suspect that MFCCs should capture information optimal for bird species identification: they were designed to represent human speech, yet humans and birds differ in their use of the spectrum both perceptually and for production. MFCCs aside, one could use raw (Mel-)spectra as input to a classifier, or one could design a new transformation of the spectral data that would tailor the representation to the subject matter. Rather than designing a new representation manually, we consider automatic feature learning.</p><p>The topic of feature learning (or representation learning, dictionary learning) has been considered from many perspectives within the realm of statistical signal processing <ref type="bibr" coords="2,210.39,489.35,10.96,8.74" target="#b0">[1]</ref>[10][4] <ref type="bibr" coords="2,249.90,489.35,9.96,8.74" target="#b4">[5]</ref>. The general aim is for an algorithm to learn some transformation that, when applied to data, improves performance on tasks such as sparse coding, signal compression or classification. This procedure may be performed in a "supervised" manner, meaning it is supplied with data as well as some side information about the downstream task (e.g. class labels), or "unsupervised", operating on a dataset but with no information about the downstream task. A simple example that can be considered to be unsupervised feature learning is principal components analysis (PCA): applied to a dataset, PCA chooses a linear projection which ensures that the dimensions of the transformed data are decorrelated <ref type="bibr" coords="2,208.20,596.95,9.96,8.74" target="#b0">[1]</ref>. It therefore creates a new feature set, without reference to any particular downstream use of the features, simply operating on the basis of qualities inherent in the data.</p><p>Recent work in machine learning has shown that unsupervised feature learning can lead to representations that perform very strongly in classification tasks, despite their ignorance of training data labels that may be available <ref type="bibr" coords="2,435.12,657.11,10.52,8.74" target="#b3">[4,</ref><ref type="bibr" coords="2,447.30,657.11,7.01,8.74" target="#b0">1]</ref>. This Fig. <ref type="figure" coords="3,154.40,327.39,4.13,7.89">1</ref>. Example of spherical k-means applied to a simple two-dimensional dataset. We generated synthetic 2D data points by sampling from three clusters which were each Gaussian-distributed in terms of their angle and log-magnitude (coloured dots), and then applied our online spherical k-means algorithm to find 10 unit vectors (crosses). These unit vectors form an overcomplete basis with which one could represent this toy data, projecting two-dimensional space to ten-dimensional space.</p><p>rather surprising outcome suggests that feature learning methods emphasise patterns in the data that turn out to have semantic relevance, patterns that are not already made explicit in the basic feature processing such as STFT.</p><p>Birdsong often contains rapid temporal modulations, and this information should be useful for identifying species-specific characteristics. From this perspective, a useful aspect of feature learning is that it can be applied not only to single spectral frames, but to short sequences (or "patches") of a few frames. The representation can then reflect not only characteristics of instantaneous frequency patterns in the input data, but characteristics of frequencies and their short-term modulations, such as chirps sweeping upwards or downwards. This bears some analogy with the "delta-MFCC" features sometimes used by taking the first difference in the time series of MFCCs, but is more flexible since it can represent amplitude modulations, frequency modulations, and correlated modulations of both sorts. In our study we tested variants of feature learning with different temporal structures: either considering one frame at a time (which does not capture modulation), multiple frames at a time, or a variant with two layers of feature learning, which captures modulation across two timescales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>As discussed in Section 1.1, the aim of unsupervised feature learning is to find some transformation of a dataset, driven only by the characteristics inherent in that dataset. For this we use a method that has has shown promise in previous studies, and can be run effectively at big data scales: spherical k-means, described by <ref type="bibr" coords="4,148.51,188.97,10.52,8.74" target="#b3">[4]</ref> and first applied to audio by <ref type="bibr" coords="4,289.61,188.97,9.96,8.74" target="#b4">[5]</ref>. Spherical k-means is related to the wellknown k-means clustering algorithm, except that instead of searching for cluster centroids which minimise the Euclidean distance to the data points, we search for unit vectors (directions) to minimise their angular distance from the data points. This is achieved by modifying the iterative update procedure for the k-means algorithm: for an input data point, rather than finding the nearest centroid by Euclidean distance and then moving the centroid towards that data point, the nearest centroid is found by cosine distance,</p><formula xml:id="formula_0" coords="4,210.65,288.93,269.95,22.31">cosine distance = 1 -cos(θ) = 1 - A • B A B ,<label>(1)</label></formula><p>where A and B are vectors to be compared, θ is the angle between them, and • is the Euclidean vector norm. The centroid is renormalised after update so that it is always a unit vector. Fig. <ref type="figure" coords="4,297.18,343.35,4.98,8.74">1</ref> shows an example of spherical k-means applied to synthetic data. Spherical k-means thus finds a set of unit vectors which represent the distribution of directions found in the data: it finds a basis (here an overcomplete basis) so that data points can in general be well approximated as a scalar multiple of one of the basis vectors. This basis can then be used to represent input data in a new feature space which reflects the discovered regularities, in the simplest case by representing every input datum by its dot product with each of the basis vectors <ref type="bibr" coords="4,304.08,427.04,10.52,8.74" target="#b3">[4,</ref><ref type="bibr" coords="4,316.26,427.04,7.01,8.74" target="#b4">5]</ref>:</p><formula xml:id="formula_1" coords="4,251.67,444.85,228.92,30.32">x (n, j) = M i=1 b j (i)x(n, i) ,<label>(2)</label></formula><p>where x represents the input data indexed by time frame n and feature index i (with M the number of input features, e.g. the number of spectral bins), b j is one of the learnt basis vectors (indexed by j ∈ [1, k]), and x is the new feature representation. In our case, the data on which we applied the spherical k-means procedure consisted of Mel spectral frames (M = 40 dimensions), which we first normalised and PCA-whitened as in <ref type="bibr" coords="4,295.84,542.24,9.96,8.74" target="#b4">[5]</ref>. We also tested configurations in which the input data was not one spectral frame but a sequence of them-e.g. a sequence of four spectral frames at a time-allowing the clustering to respond to short-term temporal patterns as well as spectral patterns. We can write this as</p><formula xml:id="formula_2" coords="4,231.09,607.32,245.26,30.55">x (n, j) = ∆ δ=0 M i=1 b j (δ, i)x(n + δ, i) , (<label>3</label></formula><formula xml:id="formula_3" coords="4,476.35,617.74,4.24,8.74">)</formula><p>where ∆ is the number of frames considered at a time, and the b are now indexed by a frame-offset as well as the feature index. Alternatively, this can be thought of as "stacking" frames, e.g. stacking each sequence of four 40-dimensional spectral frames to give a 160-dimensional vector, before applying (2) as before. In all our experiments we used a fixed k = 500, a value which has been found useful in previous studies <ref type="bibr" coords="5,207.75,155.86,9.96,8.74" target="#b4">[5]</ref>.</p><p>The standard implementation of k-means clustering requires an iterative batch process which considers all data points in every step. This is not feasible for high data volumes. Some authors use "minibatch" updates, i.e. subsamples of the dataset. For scalability as well as for the potential to handle real-time streaming data, we instead adapted an online streaming k-means algorithm, "online Hartigan k-means" <ref type="bibr" coords="5,204.57,227.59,15.50,8.74" target="#b11">[12,</ref><ref type="bibr" coords="5,223.52,227.59,54.20,8.74">Appendix B]</ref>. This method takes one data point at a time, and applies a weighted update to a selected centroid dependent on the amount of updates that the centroid has received so far. We adapted the method of <ref type="bibr" coords="5,465.10,251.50,15.50,8.74">[12,</ref> Appendix B] for the case of spherical k-means. K-means is a local optimisation algorithm rather than global, and may be sensitive to the order of presentation of data. Therefore in order to minimise the effect of order of presentation for the experiments conducted here, we did not perform the learning in true single-pass streaming mode. Instead, we performed learning in two passes: a first streamed pass in which data points were randomly subsampled (using reservoir sampling) and then shuffled before applying PCA whitening and starting the k-means procedure, and then a second streamed pass in which k-means was further trained by exposing it to all data points. Our Python code implementation of online streaming spherical k-means is available on request.</p><p>As a further extension we also tested a two-layer version of our featurelearning method, intended to reflect detail across multiple temporal scales. In this variant, we applied spherical k-means feature learning to a dataset, and then projected the dataset into that learnt space. We then downsampled this projected data by a factor of 8 on the temporal scale (by max-pooling, i.e. taking the max across each series of 8 frames), and applied spherical k-means a second time. The downsampling operation means that the second layer has the potential to learn regularities that emerge across a slightly longer temporal scale. The two-layer process overall has analogies to deep learning techniques, most often considered in the context of artificial neural networks <ref type="bibr" coords="5,318.15,490.60,9.96,8.74" target="#b0">[1]</ref>, and to the progressive abstraction believed to occur towards the higher stages of auditory neural pathways.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Classification workflow</head><p>Our full classification workflow started by converting each audio file to a standard sample-rate of 44.1 kHz. We then calculated Mel spectrograms for each file, using a frame size of 1024 frames with Hamming windowing and no overlap. We filtered out spectral energy below 500 Hz, a common choice to reduce the amount of environmental noise present, and then normalised the root-mean-square (RMS) energy in each spectrogram.</p><p>For each spectrogram we then optionally applied the noise-reduction procedure that we had found to be useful in our NIPS4B contest submission <ref type="bibr" coords="5,453.61,633.20,14.61,8.74" target="#b13">[14]</ref>, a simple and common median-based thresholding. The Mel spectrograms, either noise-reduced or otherwise, could be used directly as features. We also tested their reduction to MFCCs (including delta features, making 26-dimensional data), and their projection onto learned features, using the spherical k-means method described above. For the latter option, we tested projections based on single frame as well as on sequences of 2, 3, 4 and 8 frames, to explore the benefit of modelling short-term temporal variation. We also tested the two-layer version based on the repeated application to 4-frame sequences across two timescales.</p><p>The feature representations thus derived were all time series. In order to reduce them to summary features for use in the classifier, we used the simple approach of summarising each feature dimension independently by its mean and standard deviation. These are widespread but are not designed to reflect any temporal structure in the features; however, note that our multi-frame learnt features intrinsically capture some fine-scale temporal information.</p><p>To perform classification on our temporally-pooled feature data, then, we used a random forest classifier <ref type="bibr" coords="6,267.83,585.38,9.96,8.74" target="#b1">[2]</ref>. Random forests and other tree-ensemble classifiers perform very strongly in a wide range of empirical evaluations <ref type="bibr" coords="6,447.07,597.34,9.96,8.74" target="#b2">[3]</ref>, and were used by many of the strongest-performing entries to the SABIOD evaluation contests <ref type="bibr" coords="6,199.01,621.25,10.52,8.74" target="#b7">[8,</ref><ref type="bibr" coords="6,211.19,621.25,7.01,8.74" target="#b6">7]</ref>. For this experiment we used the implementation from the Python scikit-learn project. Note that scikit-learn v0.14 was found to have a specific issue preventing training on large data, so we used a pre-release v0.15 after verifying that it led to the same results with our smaller datasets. We did not manually tune any parameters of the classifier. Fig. <ref type="figure" coords="7,380.26,507.28,4.98,8.74" target="#fig_0">2</ref> summarises the main stages of the workflow described.</p><p>Prior to the contest, we did not have access to the held-out testing data, so for evaluation we split the training dataset into two equal partitions and performed two-fold crossvalidation. We also tested model averaging: namely, we tested a meta-classifier which simply averaged over the outputs from up to 16 different configurations of our main system. strongly outperformed the other approaches. The difference between the learnt representations was small in terms of performance, although for this data it is notable that the two-layer variant (rightmost column) consistently outperformed the single-layer variants. Even though the BirdCLEF challenge is a single-label classification challenge, we found that training the random forest as a multilabel classifier gave slightly better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>Model averaging yielded improved performance in general (Figure <ref type="figure" coords="8,453.04,521.95,3.87,8.74">4</ref>), although it was not clear from our own tests whether model averaging or a single classifier would achieve the highest point performance.</p><p>We measured the total time taken for each step in our workflow, to determine the approximate computational load for the steps (Fig. <ref type="figure" coords="8,391.54,573.43,3.87,8.74" target="#fig_3">5</ref>). The timings are approximate-in particular because our code was modularised to save/load state on disk between each process, which impacted particularly on the "classify" step which loaded large random forest settings from disk before processing. Singlelayer feature learning was efficient, taking a similar amount of time as did the initial feature extraction. Double-layer feature learning took more than double this, because of the two layers as well as performing max-pooling downsampling. . Times taken for each step in the process. Note that these are heuristic "wallclock" times measured on processes across two compute servers. Each measurement is averaged across the two folds and across two settings (noise reduction on/off) across the runs using the multilabel classifier and no decision-pooling.</p><p>the higher dimensionality. However, once the system was trained, the time taken to classify new data was the same across all configurations.</p><p>We submitted decisions from our system to the LifeCLEF 2014 bird identification challenge <ref type="bibr" coords="9,222.31,513.65,9.96,8.74" target="#b8">[9]</ref>. In that evaluation, our system attained the strongest audio-only classification results, with a MAP peaking at 42.9% (Table <ref type="table" coords="9,441.65,525.61,3.87,8.74">1</ref>, Figure <ref type="figure" coords="9,134.77,537.56,3.87,8.74" target="#fig_4">6</ref>), ten percentage points stronger than other audio-only classifiers that were submitted. (Only one system outperformed ours, peaking at 51.1% in a variant of the challenge which provided additional metadata as well as audio.) We submitted the outputs from individual models, as well as model-averaging runs using the simple mean of outputs from multiple models. Notably, the strongest classification both in our own tests and the official evaluation was attained not by model averaging, but by a single model based on two-layer feature learning. Also notable is that our official scores, which were trained and tested on larger data subsets, were substantially higher than our crossvalidated scores, corroborating our observation that the method works particularly well at high data volumes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System variant submitted</head><p>Cross-val MAP (%) Official MAP (%) melspec-kfl3-ms, noise red., binary relevance 30.56 36.9 Average from 12 single-layer models 32.73 38.9 melspec-kfl4pl8kfl4-ms, noise red., binary relevance 35.31 42.9 Average from 16 single-and double-layer models 35.07 41.4</p><p>Table <ref type="table" coords="10,164.68,167.61,4.13,7.89">1</ref>. Summary of MAP scores attained by our system in the public LifeCLEF 2014 Bird Identification Task <ref type="bibr" coords="10,256.79,178.59,9.22,7.86" target="#b8">[9]</ref>. The first column lists scores attained locally in our two-fold split. The second column lists scores evaluated officially, using a classifier(s) trained across the entire training set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>Current interest in automatic classification of bird sounds is motivated by the practical scientific need to label large volumes of data coming from sources such as remote monitoring stations and sound archives. Unsupervised feature learning is a simple and effective method to boost classification performance by learning spectro-temporal regularities in the data. It does not require training labels or any other side-information, it can be used within any classification workflow, and once trained it imposes negligible extra computational effort on the classifier. The principal practical issue with unsupervised feature learning is that it requires large data volumes to be effective. However, this exhibits a synergy with the large data volumes that are increasingly becoming standard.</p><p>In our experiments, learnt features strongly outperformed MFCC and Mel spectral features. In the BirdCLEF 2014 contest, our system was by far the strongest audio-only submission, outperforming even some systems which made use of auxiliary metadata.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,134.77,361.43,345.81,7.89;6,134.77,372.42,96.14,7.86"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Summary of the classification workflow, here showing the case where single-layer feature learning is used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,134.77,430.52,345.81,7.89;7,134.77,441.50,345.81,7.86;7,134.77,452.46,345.81,7.86;7,134.77,463.42,345.82,7.86;7,134.77,474.38,273.76,7.86"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. MAP statistics, summarised for each feature-type tested. Each column in the boxplot summarises the crossvalidated scores attained over many combinations of the other configuration settings tested (for the full multi-class classifier only). The ranges indicated by the boxes therefore do not represent random variation due to training data subset, but systematic variation due to classifier configuration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,134.77,633.20,345.84,8.74;7,134.77,645.16,345.84,8.74;7,134.77,657.11,345.84,8.74"><head>Figure 3 Fig. 4 .</head><label>34</label><figDesc>Figure 3 illustrates the cross-validated MAP scores we obtained from six nonlearnt feature representations (based on simple MFCC and Mel spectra) and from six learnt feature representations. The learnt representations consistently and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,134.77,389.81,345.81,7.89;9,134.77,400.79,345.81,7.86;9,134.77,411.75,345.81,7.86;9,134.77,422.71,254.75,7.86"><head>Fig. 5</head><label>5</label><figDesc>Fig.5. Times taken for each step in the process. Note that these are heuristic "wallclock" times measured on processes across two compute servers. Each measurement is averaged across the two folds and across two settings (noise reduction on/off) across the runs using the multilabel classifier and no decision-pooling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="10,149.68,464.89,312.91,7.89;10,136.49,231.06,342.38,209.09"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Official plot of evaluation scores. Ours are the four labelled "QMUL".</figDesc><graphic coords="10,136.49,231.06,342.38,209.09" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to thank the people and projects which made available the data used for this research-the Xeno Canto website and its many volunteer contributors-as well as the SABIOD research project for instigating the contest, and the CLEF contest hosts.</p><p>This work was supported by <rs type="funder">EPSRC</rs> <rs type="grantName">Leadership Fellowship</rs> <rs type="grantNumber">EP/G007144/1</rs> and <rs type="funder">EPSRC</rs> <rs type="grantName">Early Career Fellowship</rs> <rs type="grantNumber">EP/L020505/1</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_3G7gMMt">
					<idno type="grant-number">EP/G007144/1</idno>
					<orgName type="grant-name">Leadership Fellowship</orgName>
				</org>
				<org type="funding" xml:id="_Pqr3j7x">
					<idno type="grant-number">EP/L020505/1</idno>
					<orgName type="grant-name">Early Career Fellowship</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="11,142.96,292.56,337.62,7.86;11,151.52,303.51,329.06,7.86;11,151.52,314.47,92.15,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,307.05,292.56,173.53,7.86;11,151.52,303.51,47.38,7.86">Representation learning: A review and new perspectives</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,207.36,303.51,273.22,7.86">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">17981828</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,324.82,280.07,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,205.30,324.82,61.96,7.86">Random forests</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,273.90,324.82,72.33,7.86">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,335.16,337.62,7.86;11,151.52,346.12,329.06,7.86;11,151.52,357.08,142.08,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,292.85,335.16,187.73,7.86;11,151.52,346.12,57.07,7.86">An empirical comparison of supervised learning algorithms</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,231.24,346.12,249.34,7.86;11,151.52,357.08,30.97,7.86">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="161" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,367.43,337.62,7.86;11,151.52,378.39,242.11,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,243.28,367.43,187.73,7.86">Learning feature representations with k-means</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,453.68,367.43,26.90,7.86;11,151.52,378.39,118.47,7.86">Neural Networks: Tricks of the Trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="561" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,388.73,337.62,7.86;11,151.52,399.69,329.05,7.86;11,151.52,410.65,84.34,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,271.80,388.73,208.77,7.86;11,151.52,399.69,11.14,7.86">Multiscale approaches to music audio feature learning</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,181.55,399.69,299.02,7.86;11,151.52,410.65,55.67,7.86">Proceedings of the International Conference on Music Information Retrieval (ISMIR 2013)</title>
		<meeting>the International Conference on Music Information Retrieval (ISMIR 2013)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,420.99,337.62,7.86;11,151.52,431.95,329.06,7.86;11,151.52,442.91,82.93,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,330.77,420.99,149.81,7.86;11,151.52,431.95,181.06,7.86">A practical comparison of manual and autonomous methods for acoustic monitoring</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Digby</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Towsey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">D</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">D</forename><surname>Teal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,339.96,431.95,140.62,7.86">Methods in Ecology and Evolution</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="675" to="683" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,453.26,337.62,7.86;11,151.52,464.22,329.06,7.86;11,151.52,475.18,109.68,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,197.46,453.26,201.38,7.86">The ninth annual MLSP competition: First place</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Fodor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,421.29,453.26,59.29,7.86;11,151.52,464.22,329.06,7.86;11,151.52,475.18,20.48,7.86">Proceedings of the International Conference on Machine Learning for Signal Processing (MLSP 2013)</title>
		<meeting>the International Conference on Machine Learning for Signal Processing (MLSP 2013)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,485.52,337.62,7.86;11,151.52,496.48,329.06,7.86;11,151.52,507.44,269.64,8.11" xml:id="b7">
	<monogr>
		<ptr target="http://sabiod.org/NIPS4B2013_book.pdf" />
		<title level="m" coord="11,180.89,496.48,299.69,7.86;11,151.52,507.44,34.69,7.86">Neural Information Processing Scaled for Bioacoustics, from Neurons to Big Data</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Glotin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Artières</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">O</forename><surname>Tchernichovski</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Halkias</surname></persName>
		</editor>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,517.78,337.62,7.86;11,151.52,528.74,195.81,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,364.15,517.78,116.42,7.86;11,151.52,528.74,36.56,7.86">LifeCLEF bird identification task 2014</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rauber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,209.42,528.74,109.24,7.86">CLEF Working Notes 2014</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,539.09,337.96,7.86;11,151.52,550.05,329.06,7.86;11,151.52,561.01,47.09,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,266.74,539.09,213.83,7.86;11,151.52,550.05,55.27,7.86">Fast dictionary learning for sparse representations of speech signals</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,213.93,550.05,217.70,7.86">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1025" to="1031" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,571.35,337.96,7.86;11,151.52,582.31,199.35,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,193.96,571.35,282.57,7.86">The emerging significance of bioacoustics in animal species conservation</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Laiolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,151.52,582.31,94.91,7.86">Biological Conservation</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1635" to="1645" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,592.66,337.96,7.86;11,151.52,603.61,329.07,8.11;11,151.52,616.96,178.87,7.47" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mcfee</surname></persName>
		</author>
		<ptr target="http://cseweb.ucsd.edu/~bmcfee/papers/bmcfee_dissertation.pdf" />
		<title level="m" coord="11,197.54,592.66,252.95,7.86">More like this: Machine learning approaches to music similarity</title>
		<meeting><address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
		<respStmt>
			<orgName>University of California</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct coords="11,142.61,624.92,337.96,7.86;11,151.52,635.88,329.05,7.86;11,151.52,646.84,329.07,8.11;11,151.52,658.44,289.62,7.47" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="11,270.95,624.92,209.62,7.86;11,151.52,635.88,164.17,7.86">Birdsong and C4DM: A survey of UK birdsong and machine recognition for music researchers</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Stowell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<idno>DM-TR-09-12</idno>
		<ptr target="http://c4dm.eecs.qmul.ac.uk/papers/2010/Stowell2010-C4DM-TR-09-12-birdsong.pdf" />
		<imprint>
			<date type="published" when="2010-08">Aug 2010</date>
		</imprint>
		<respStmt>
			<orgName>Centre for Digital Music, Queen Mary University of London</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. C4</note>
</biblStruct>

<biblStruct coords="12,142.61,120.67,337.96,7.86;12,151.52,131.63,329.05,7.86;12,151.52,142.59,232.51,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,266.20,120.67,214.37,7.86;12,151.52,131.63,92.88,7.86">Feature design for multilabel bird song classification in noise (nips4b challenge)</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Stowell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,263.54,131.63,217.04,7.86;12,151.52,142.59,203.85,7.86">Proceedings of NIPS4b: Neural Information Processing Scaled for Bioacoustics, from Neurons to Big Data</title>
		<meeting>NIPS4b: Neural Information Processing Scaled for Bioacoustics, from Neurons to Big Data</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,153.55,337.96,7.86;12,151.52,164.51,329.07,8.11;12,151.52,176.11,117.68,7.47" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Stowell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1405.6524" />
		<title level="m" coord="12,274.13,153.55,206.45,7.86;12,151.52,164.51,223.44,7.86">Automatic large-scale classification of bird sounds is strongly improved by unsupervised feature learning</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">Pre-print</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
