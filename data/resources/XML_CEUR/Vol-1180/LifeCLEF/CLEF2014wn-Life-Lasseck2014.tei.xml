<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,136.94,152.75,321.35,12.54;1,137.18,174.54,320.87,10.72">Large-scale identification of birds in audio recordings Notes on the winning solution of the LifeCLEF 2014 Bird Task</title>
				<funder ref="#_VaugybV">
					<orgName type="full">DFG</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,268.01,212.12,59.04,9.05"><forename type="first">Mario</forename><surname>Lasseck</surname></persName>
							<email>mario.lasseck@mfn-berlin.de</email>
							<affiliation key="aff0">
								<orgName type="department">Animal Sound Archive Museum für Naturkunde</orgName>
								<address>
									<settlement>Berlin</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,136.94,152.75,321.35,12.54;1,137.18,174.54,320.87,10.72">Large-scale identification of birds in audio recordings Notes on the winning solution of the LifeCLEF 2014 Bird Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">56AD4DFF4AA4B9940CBCC852E762FE54</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bird Identification</term>
					<term>Information Retrieval</term>
					<term>Citizen Sciences</term>
					<term>Image Segmentation</term>
					<term>Median Clipping</term>
					<term>Template Matching</term>
					<term>Decision Trees</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Bird Identification Task of the LifeCLEF 2014 lab is to automatically identify 501 different species in over 4000 audio recordings collected by amateur and expert ornithologists through a citizen sciences initiative. It is one of the biggest bird classification challenges so far considering the quality, quantity and variability of the recordings and the very large number of different species to be classified. The solution presented here achieves a Mean Average Precision of 51.1% on the test set and 53.9% on the training set with an Area Under the Curve of 91.5% during cross-validation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction and Task Overview</head><p>The LifeCLEF 2014 Bird Identification challenge asks participants to automatically identify the vocalizing species in 4339 audio recordings with undetermined content.</p><p>For training, 9688 audio recordings paired with metadata including dominant and background species are provided. A recording may contain only one or up to 11 simultaneously vocalizing birds. What makes this challenge unique but also quite difficult is the very large amount of data, the high variability of the recordings, both in quality and content and of course the large number of different species to be classified. The all in all 14,027 audio files, if added together 33.3 GB of data with over 4.5 days of acoustic material, are provided by Xeno-canto (http://www.xeno-canto.org/). The files were recorded between 1979 and 2013 in over 2000 different locations centered on Brazil by almost 250 amateur and expert ornithologists, using different combinations of microphones and portable recorders. The duration of the recordings varies from half a second to several minutes. Also the quality of the audio files is quite diverse and challenging. One has to deal with all kinds of background noise and in some cases artifacts due to lossy mp3 data compression. An overview and further details about the LifeCLEF Bird Identification Task is given in <ref type="bibr" coords="1,136.08,661.48,10.69,9.05" target="#b0">[1]</ref>. The task is among others part of the CLEF 2014. A general overview of all tasks can be found in <ref type="bibr" coords="1,211.47,673.48,11.02,9.05" target="#b1">[2,</ref><ref type="bibr" coords="1,222.49,673.48,7.35,9.05" target="#b2">3,</ref><ref type="bibr" coords="1,229.84,673.48,7.35,9.05" target="#b3">4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Extraction</head><p>The features used for classification are taken from three different sources briefly described in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Metadata</head><p>The first source for feature extraction is the provided metadata. Each audio file is paired with additional contextual information about the date, time, location and author of the recording. This information is used to extract 8 features per file:</p><formula xml:id="formula_0" coords="2,124.70,283.31,69.81,94.49"> Year  Month  Time  Latitude  Longitude  Elevation  Locality Index  Author Index</formula><p>To use the provided metadata a few steps had to be taken for preparation. From the recording date only the year and month were extracted and considered as relevant features. The recording time was converted in minutes. Since only numeric values can be used as features, for locality and author a look up table was created and the corresponding index was used. All missing or none numeric values were replaced by the mean value of its category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">openSMILE</head><p>The openSMILE feature extraction tool <ref type="bibr" coords="2,293.81,498.85,11.60,9.05" target="#b4">[5]</ref> was used to extract a large number of features per audio recording. The framework was configured with the emo_large.conf configuration file written by Florian Eyben. It was originally designed for emotion detection in speech signals but was also recently applied in the field of audio scene analysis <ref type="bibr" coords="2,160.58,546.85,10.68,9.05" target="#b5">[6]</ref>. The here used configuration file first calculates 57 so called low-level descriptors (LLD) per frame, adds delta (velocity) and delta-delta (acceleration) coefficients to each LLD and finally applies 39 statistical functionals after moving average smoothing the feature trajectories.</p><p>The 57 LLDs consist of: To describe an entire audio recording, statistics are calculated from all LLD, velocity and acceleration trajectories by 39 functionals including e.g. means, extremes, moments, percentiles and linear as well as quadratic regression. This sums up to 6669 (57 x 3 x 39) features per recording. Further details regarding openSMILE and the extracted features can be found in the openSMILE 1.0.1 manual and the emo_large.conf configuration file (http://opensmile.sourceforge.net/).</p><formula xml:id="formula_1" coords="3,124.70,170.24,4.58,9.05"></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Segment-Probabilities</head><p>The idea of using the matching probabilities of segments as features or more precisely the maxima of the normalized cross-correlation <ref type="bibr" coords="3,317.45,539.89,11.72,9.05" target="#b6">[7]</ref> between segments, also referred to as region of interests (ROIs) or templates, and spectrogram images was previously used by Nick Kriedler in The Marinexplore and Cornell University Whale Detection Challenge, Fodor Gabor in the MLSP 2013 Bird Classification Challenge [8] and Ilyas Potamitis in the NIPS 2013 Bird Song Classification Challenge <ref type="bibr" coords="3,402.43,587.92,10.69,9.05" target="#b7">[9]</ref>.</p><p>For the current competition an adaptation of this method was used which was already very successfully applied also in the NIPS 2013 Challenge <ref type="bibr" coords="3,370.03,611.92,15.43,9.05" target="#b8">[10]</ref>. It differs mainly in the way how segments are extracted and which subsets of segments and their probabilities are used during classification. It turned out that proper preprocessing and segmentation of the spectrogram images is a key element to improve classification performance. The number of segments should be rather small but still representative, capturing typical elements and combinations of sounds of the species to be identified.</p><p>The following sections give a brief overview of the feature extraction steps regarding Segment-Probabilities. Some additional details can be found in <ref type="bibr" coords="4,379.28,162.08,15.51,9.05" target="#b8">[10]</ref>.</p><p>Preprocessing and Segmentation. As mentioned above the way of preprocessing and segmentation is crucial to gather a good repertoire of segments especially when dealing with unknown content and noisy recordings. The following steps were performed for each audio file in the training set:</p><p> resample to 22050 Hz  get spectrogram via STFT (512 samples, hanning window, 75% overlap)  normalize spectrogram to 1.0  remove 4 lowest and 24 highest spectrogram rows  get binary image via Median Clipping per frequency band and time frame by setting each pixel to 1, if it is above 3 times the median of its corresponding row AND 3 times the median of its corresponding column, otherwise to 0  apply closing, dilation and median filter for further noise reduction  label all connected pixels exceeding a certain spatial extension as a segment  define its size and position by a rectangle with a small area added to each direction Selection of typical Segments per Species. In opposite to the Metadata and openSMILE feature sets that are species respectively class independent, Segment-Probabilities form individual feature sets for each species. In order to get a small but representative set of features per species, only segments from files without background species and very good quality (metadata: Quality = 1) were selected. For some species this condition was too strict, leading to none or too few segments. The following queries were applied successively for every target species until there was at least one file that met the conditions and the number of retrieved segments was greater than 40:</p><p>Select all segments of files WHERE: Species = target species AND:</p><p>1. BackgroundSpecies = {} AND Quality = 1 2. BackgroundSpecies = {} AND (Quality = 1 OR Quality = 2) 3. BackgroundSpecies = {} AND (Quality = 1 OR Quality = 2 OR Quality = 3) 4. Quality = 1 OR Quality = 2</p><p>The number of segments retrieved this way sums up to 492,753 for all training files with an average of approximately 984 segments per species.</p><p>Template Matching. After selecting a set of segments for each species, template matching was performed to get an individual feature set per species. The highest matching probability was determined using normalized cross-correlation after applying a gaussian blur to segment and target image. Due to the large number of audio files and segments used as templates, the method described in <ref type="bibr" coords="6,379.27,240.08,16.72,9.05" target="#b8">[10]</ref> was way too time consuming and had to be modified. In order to speed up the process the following changes were applied:</p><p> segments and target spectrogram images were calculated via STFT using only 50% overlap (instead of 75%)  search range for segments within the target images along the frequency axes was set to ± 3 pixel (instead of 4 pixel)  segments and target spectrogram images were converted to 8 bit unsigned integer before the template matching procedure (instead of 32 bit floating point)</p><p>Even with these modifications, the process of template matching (sliding almost half a million templates over 14,027 target images) took very long and kept four computers with regular hardware quite busy for several days.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Feature Selection</head><p>To cope with the large number of features and to improve and speed up the classification process a reduction of features was inevitable. It was performed in two phases, before and during classification. The openSMILE features were reduced from 6669 to 1277 features per file before the actual classification step. This was done by recursive feature elimination with the scikit-learn <ref type="bibr" coords="6,172.22,514.69,16.76,9.05" target="#b9">[11]</ref> RFECV selector <ref type="bibr" coords="6,260.09,514.69,16.64,9.05" target="#b10">[12]</ref> and a support vector machine with linear kernel and 2-fold cross-validation. For this preselection only a small subset from the training data consisting of 50 species and good quality files was used. During classification, furthermore the k highest scoring features were individually selected per species using univariate feature selection. This was done separately for each fold during classifier training with cross-validation. Different values for k were tested, ranging from 150 to 400 features per class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Training and Classification</head><p>Since it was optional to use the information about background species, single-and multi-label approaches were tested. In both cases the classification problem was split up into 501 independent classification problems using one classifier for each species following the one-vs.-all resp. the binary relevance method. For the single-label approach only dominant species were considered as targets. In case of the multi-label approach background species (BS), if assigned, were also considered for each training file but were set to lower probabilities compared to the dominant species. The classification was done with the scikit-learn library (ExtraTreesRegressor) by training ensembles of randomized decision trees <ref type="bibr" coords="7,295.73,273.11,16.76,9.05" target="#b11">[13]</ref>  With following variations of tree parameters:</p><p> number of estimators o 300, 400, 500</p><formula xml:id="formula_2" coords="7,124.70,605.68,82.38,53.33"> max_features o 4, 5, 6, 7  min_sample_split o 1, 2, 3, 4, 5</formula><p>During cross-validation using stratified folds the probability of each species in all test files was predicted and averaged. Additionally each species was predicted in the held out training files for validation. This way it was possible to choose a variation and/or parameter set separately per species and to increase the MAP score on the test files by optimizing the MAP score on the training files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>In Table <ref type="table" coords="8,164.02,255.11,4.98,9.05" target="#tab_2">1</ref> the results of the four submitted runs are summarized using evaluation statistics based on the mean of the Area Under the Curve (AUC) calculated per species and the Mean Average Precision (MAP) on the public training and the private test set. All four runs outperformed the runs of the other participating teams. For the first and the best performing third run a mix of parameter sets individually selected per species was used. As mentioned above the selection was based on how a particular set of trainings parameter was able to increase the overall MAP on the held out training files during cross-validation. A higher mean AUC score might be a hint of a generally good selection of training parameters but it is still possible that for some classes (species) a different selection works better. To give an example, in Fig. <ref type="figure" coords="8,124.70,527.65,4.98,9.05">3</ref> AUC scores are visualized per species using one of the three different feature sets exclusively during training. On average the use of Segment-Probabilities outperforms the other feature sets but for some species the openSMILE and in rare cases even the Metadata feature set is a better choice.</p><p>For the winning third run a list of the parameters and feature sets used for each species, together with their individually achieved AUC scores can be downloaded from http://www.animalsoundarchive.org/RefSys/LifeCLEF2014. Here one can also find additional figures visualizing the preprocessing, segmentation and the most important segments used for classification. To use matching probabilities of segments as features was once again a good choice. The drawback of this method is that the template matching procedure to calculate the feature sets takes quite a long time especially if the number of species and audio files is as large as in the current challenge. To use image-pyramids could help to speed up the process and would be worth to investigate in the near future. The features derived from metadata and the ones calculated with openSMILE did not perform as well but they could increase the overall classification performance by improving the results for individual species. Considering the use of the openSMILE tool there is still a lot of room for improvement. The configuration file could be altered to better capture the characteristics of bird sounds.</p><p>Furthermore a preselection of features on a per species basis to get individually designed feature sets for each species class, like done for Segment-Probabilities, could be advantageous. Also worth considering is windowing the audio files, classifying the fixed length sections and averaging the results via majority voting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,147.38,571.87,300.49,8.18;4,140.07,429.66,315.14,133.65"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>Fig. 1. Example of a spectrogram image (log) with marked segments (MediaId: 86)</figDesc><graphic coords="4,140.07,429.66,315.14,133.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,131.54,311.93,331.91,8.18;9,201.65,322.97,192.01,8.18;9,131.90,147.40,331.33,149.65"><head>Fig. 3 . 4 Parameter variations of Run 4 : 3  7 Fig. 4 .Fig. 5 .</head><label>3443745</label><figDesc>Fig. 3. AUC scores per species for individual feature sets calculated on left out training files during cross-validation (without background species)</figDesc><graphic coords="9,131.90,147.40,331.33,149.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="5,176.63,147.40,242.04,321.40"><head></head><label></label><figDesc></figDesc><graphic coords="5,176.63,147.40,242.04,321.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,159.50,326.93,276.28,89.27"><head>Table 1 .</head><label>1</label><figDesc>Performance of submitted runs (without / with background species)</figDesc><table coords="8,166.22,344.63,265.02,71.57"><row><cell></cell><cell cols="2">Public Training Set</cell><cell>Private Test Set</cell></row><row><cell>Run</cell><cell>Mean AUC [%]</cell><cell>MAP [%]</cell><cell>MAP [%]</cell></row><row><cell>1</cell><cell>91.4 / 85.0</cell><cell>53.7 / 48.6</cell><cell>50.9 / 45.1</cell></row><row><cell>2</cell><cell>91.1 / 84.9</cell><cell>49.4 / 44.6</cell><cell>49.2 / 43.7</cell></row><row><cell>3</cell><cell>91.5 / 85.1</cell><cell>53.9 / 48.7</cell><cell>51.1 / 45.3</cell></row><row><cell>4</cell><cell>91.4 / 85.3</cell><cell>50.1 / 45.3</cell><cell>50.4 / 44.9</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="7,136.10,675.10,334.73,8.18;7,136.10,686.14,314.76,8.18"><p>By the time of the submission deadline Segment-Probabilities were extracted for 485 species. The remaining 16 species used Metadata + openSMILE features for classification.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. I would like to thank <rs type="person">Hervé Glotin</rs>, <rs type="person">Hervé Goëau</rs>, <rs type="person">Andreas Raube</rs> and <rs type="person">Willem-Pier Vellinga</rs> for organizing this competition, the <rs type="institution">Xeno-Canto</rs> foundation for nature sounds for providing the audio recordings and the French projects Pl@ntNet (<rs type="institution">INRIA</rs>, <rs type="institution">CIRAD</rs>, <rs type="institution">Tela Botanica)</rs> and <rs type="institution">SABIOD Mastodons</rs> for supporting this task. I also want to thank <rs type="person">Dr. Karl-Heinz Frommolt</rs> for supporting my work and providing me with the access to the resources of the Animal Sound Archive [14]. The research was supported by the <rs type="funder">DFG</rs> (grant no. <rs type="grantNumber">FR 1028/4-1</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_VaugybV">
					<idno type="grant-number">FR 1028/4-1</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="11,127.03,341.93,343.44,8.18;11,136.10,352.97,130.55,8.18" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,343.15,341.93,127.31,8.18;11,136.10,352.97,16.30,8.18">LifeCLEF Bird Identification Task 2014</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rauber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,171.02,352.97,75.35,8.18">CLEF working notes</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,127.03,364.01,343.68,8.18;11,136.10,374.93,177.24,8.18" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,292.47,364.01,178.24,8.18;11,136.10,374.93,60.88,8.18">LifeCLEF 2014: multimedia life species identification challenges</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,215.09,374.93,77.90,8.18">Proceedings of CLEF</title>
		<meeting>CLEF</meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,127.03,385.97,343.55,8.18;11,136.10,397.01,334.64,8.18;11,136.10,407.93,98.73,8.18" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,346.75,385.97,123.82,8.18;11,136.10,397.01,81.13,8.18">ImageCLEF 2014: Overview and analysis of the results</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Martinez-Gomez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,237.77,397.01,67.44,8.18">CLEF proceedings</title>
		<title level="s" coord="11,443.35,397.01,27.39,8.18;11,136.10,407.93,98.73,8.18">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,127.03,418.99,343.30,8.18;11,136.10,430.03,334.76,8.18;11,136.10,440.95,126.35,8.18" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,389.94,418.99,80.39,8.18;11,136.10,430.03,38.95,8.18">CLEF 2014 Labs and Workshops</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Halvey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kraaij</forename><forename type="middle">W</forename></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-1180/" />
	</analytic>
	<monogr>
		<title level="s" coord="11,182.02,430.03,215.88,8.18">Notebook Papers. CEUR Workshop Proceedings (CEUR-WS</title>
		<idno type="ISSN">1613- 0073</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,127.03,451.99,343.56,8.18;11,136.10,463.03,334.38,8.18;11,136.10,473.95,311.76,8.18" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,299.81,451.99,170.77,8.18;11,136.10,463.03,138.03,8.18">openSMILE -The Munich Versatile and Fast Open-Source Audio Feature Extractor</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wöllmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<idno type="DOI">10.1145/1873951.1874246</idno>
	</analytic>
	<monogr>
		<title level="m" coord="11,293.81,463.03,109.47,8.18">Proc. ACM Multimedia (MM)</title>
		<meeting>ACM Multimedia (MM)<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1459" to="1462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,127.03,484.99,343.52,8.18;11,136.10,496.03,334.37,8.18;11,136.10,506.95,201.07,8.18" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,292.97,484.99,177.57,8.18;11,136.10,496.03,128.78,8.18">Large-Scale Audio Feature Extraction and SVM for Acoustic Scenes Classification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">T</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,286.96,496.03,183.51,8.18;11,136.10,506.95,78.67,8.18">Applications of Signal Processing to Audio and Acoustics (WASPAA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,127.03,517.99,302.42,8.18;11,123.62,529.03,346.93,8.18;11,136.10,539.95,334.29,8.18;11,136.10,550.99,245.63,8.18" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,202.01,517.99,227.43,8.18;11,123.62,529.03,3.41,8.18">Fast Normalized Cross-Correlation, Industrial Light and Magic 8</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="DOI">Identifier:10.1109/MLSP.2013.6661932PublicationYear:2013</idno>
	</analytic>
	<monogr>
		<title level="m" coord="11,199.38,529.03,271.17,8.18;11,136.10,539.95,93.99,8.18;11,257.18,539.95,117.57,8.18">The Ninth Annual MLSP Competition: First place. Machine Learning for Signal Processing (MLSP)</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Fodor</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1995">1995. 2013. 2013</date>
		</imprint>
	</monogr>
	<note>IEEE International Workshop on. Page(s): 1-2</note>
</biblStruct>

<biblStruct coords="11,127.03,562.03,343.63,8.18;11,136.10,572.95,243.21,8.18" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,210.79,562.03,259.86,8.18;11,136.10,572.95,16.19,8.18">Automatic Classification of Taxon-Rich Community Recorded in the Wild</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Potamitis</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0096936</idno>
	</analytic>
	<monogr>
		<title level="j" coord="11,158.51,572.95,40.86,8.18">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">96936</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,126.64,584.02,344.05,8.18;11,136.10,595.06,334.29,8.18;11,136.10,605.98,312.10,8.18" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,211.97,584.02,258.72,8.18;11,136.10,595.06,96.14,8.18">Bird Song Classification in Field Recordings: Winning Solution for NIPS4B 2013 Competition</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,331.78,595.06,138.62,8.18;11,136.10,605.98,111.68,8.18">Proc. of int. symp. Neural Information Scaled for Bioacoustics, sabiod</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Glotin</surname></persName>
		</editor>
		<meeting>of int. symp. Neural Information Scaled for Bioacoustics, sabiod<address><addrLine>, Nevada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-12">2013. dec. 2013</date>
			<biblScope unit="page" from="176" to="181" />
		</imprint>
	</monogr>
	<note>.org/nips4b, joint to NIPS</note>
</biblStruct>

<biblStruct coords="11,126.64,617.02,344.22,8.18;11,136.10,628.06,18.05,8.18" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,236.69,617.02,149.95,8.18">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,394.15,617.02,22.92,8.18">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,126.64,638.98,343.95,8.18;11,136.10,650.02,161.24,8.18" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,224.57,638.98,246.02,8.18;11,136.10,650.02,21.68,8.18">Gene selection for cancer classification using support vector machines</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,163.70,650.02,64.55,8.18">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="389" to="422" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,126.64,661.06,308.49,8.18;11,119.06,671.74,7.58,8.18" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,217.97,661.06,100.33,8.18">Extremely randomized trees</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Geurts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,323.79,661.06,64.65,8.18">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="42" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,126.64,671.74,275.65,8.18" xml:id="b12">
	<analytic>
		<title/>
		<ptr target="http://www.animalsoundarchive.org" />
	</analytic>
	<monogr>
		<title level="j" coord="11,136.10,671.74,81.69,8.18">Animal Sound Archive</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
