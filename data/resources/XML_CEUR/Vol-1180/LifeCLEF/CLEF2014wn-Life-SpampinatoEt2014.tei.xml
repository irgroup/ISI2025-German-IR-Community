<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,158.94,116.95,297.47,12.62">Overview of the LifeCLEF 2014 Fish Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,139.25,154.62,94.65,8.74"><forename type="first">Concetto</forename><surname>Spampinato</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical</orgName>
								<orgName type="department" key="dep2">Electronics and Computer Engineering</orgName>
								<orgName type="institution">University of Catania</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,244.45,154.62,67.95,8.74"><forename type="first">Simone</forename><surname>Palazzo</surname></persName>
							<email>simone.palazzo@dieei.unict.it</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical</orgName>
								<orgName type="department" key="dep2">Electronics and Computer Engineering</orgName>
								<orgName type="institution">University of Catania</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,322.95,154.62,44.90,8.74"><forename type="first">Bas</forename><surname>Boom</surname></persName>
							<email>bboom@inf.ed.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,397.78,154.62,73.86,8.74"><forename type="first">Robert</forename><forename type="middle">B</forename><surname>Fisher</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,158.94,116.95,297.47,12.62">Overview of the LifeCLEF 2014 Fish Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">215DA50AF2F19FDB6E991EEC956A13B1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Underwater video analysis</term>
					<term>Object classification</term>
					<term>Fine-grained visual categorisation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the LifeCLEF 2014 fish task, which aimed at benchmarking automatic fish detection and recognition methods by processing underwater visual data. The task consisted of videobased subtasks for fish detection and fish species recognition in videos and one image-based task for fish species classification in still images. Our underwater visual datasets consisted of about 2,000 videos taken from the Fish4Knowledge video repository and more than 200,000 annotations automatically obtained and manually validated. About fifty teams registered to the fish task, but only two teams submitted runs: the I3S team for subtask 3 and LSIS/DYNI team for subtask 4. The results achieved by both teams are satisfactory.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Underwater video and imaging systems are used increasingly in a range of monitoring or exploratory applications, in particular for biological (e.g. benthic community structure, habitat classification), fisheries (e.g. stock assessment, species richness), geological (e.g. seabed type, mineral deposits) and physical surveys (e.g. pipelines, cables, oil industry infrastructure). Their usage has benefitted from the increasing miniaturisation and cost-effectiveness of submersible ROVs (remotely operated vehicles) and advances in underwater digital cameras. These technologies have revolutionised our ability to capture high-resolution images in challenging aquatic environments and are also greatly improving our ability to effectively manage natural resources, increasing our competitiveness and reducing operational risks in industries that operate in both marine and freshwater systems. Despite these advances in data collection technologies, the analysis of video data usually requires very time-consuming and expensive input by human observers. This is particularly true for ecological and fishery video data, which often requires laborious visual analysis. This analytical "bottleneck" greatly restricts the use of these otherwise powerful video technologies and demands effective methods for automatic content analysis to enable proactive provision of analytical information.</p><p>The development of automatic video analysis tools is, however, particularly challenging because of the complexities of underwater video recordings in terms of the variability of scenarios and factors that may degrade the video quality such as water clarity and/or depth. A recent advance in this direction was developed as part of the Fish4Knowledge (F4K)<ref type="foot" coords="2,297.36,166.24,3.97,6.12" target="#foot_0">3</ref> project (funded by The European Union 7th Framework Programme), where computer vision and machine learning techniques were developed to extract information about fish density and richness from videos taken by underwater cameras installed at coral reefs in Taiwan <ref type="bibr" coords="2,466.14,203.68,9.96,8.74" target="#b0">[1]</ref>.</p><p>However, the underwater is a rather complex environment because of several factors that make the study of it particularly complex and challenging. In particular, dynamic or multi-modal backgrounds, abrupt lighting changes (due also to water caustics), and radical and instant water turbidity changes affect the ability to perform visual tasks also for humans. To complicate even more the situation, the underwater environment shows two almost exclusive characteristics with respect to other domains: three degrees of freedom and erratic and extremely fast movements of objects (i.e. fish). This makes fish less predictable than people or vehicles, as fish may move in all three directions changing suddenly their size and their shape in the video. As a consequence of this, although in the F4K project reliable approaches for video-based fish detection and species identification were devised, the problem of automatic analysis of underwater visual data remains still open. In this paper we describe the "Fish Task" organised as part of the LifeCLEF 2014, where video and image based approaches for fish detection and recognition were tested on underwater video/image datasets achieving very good results.</p><p>The remainder of the paper is as follows: Sect. 2 provides an overview of the task as well as the underwater video and image datasets used, Sect. 3 presents the participants to the fish task and their approaches whose results, compared to our baselines, are given in Sect. 4. Sect. 5 concludes this report.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Fish Task Description</head><p>The LifeCLEF 2014 Fish task aimed at benchmarking automatic fish detection and recognition methods by processing underwater visual data. It basically consisted of three video-based subtasks and one image-based task. The video-based subtasks were: Subtask 1 -detecting moving objects in videos by either background modeling or object detection methods; Subtask 2 -detecting fish instances in video frames, thus discriminating fish instances from non-fish ones; Subtask 3 -detecting fish instances in video frames and recognising their species;</p><p>The image-based subtask, instead, had the goal: Subtask 4 -to identify fish species using only still images containing only one fish instance.</p><p>The participants had to submit at most three runs for a subtask. The run file must have had the same format as the ground truth xml file (see below), i.e. it must contain the frame where the fish was detected together with the bounding box (for all the subtasks 1, 2, and 3), contours (only task 1) and species name (for subtasks 3 and 4) of the fish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dataset</head><p>The underwater video dataset was derived from the Fish4Knowledge video repository, which contains about 700,000 10-minute video clips that were taken in the past five years to monitor Taiwan coral reefs. The Taiwan area is particularly interesting for studying the marine ecosystem, as it holds one of the largest fish biodiversities of the world with more than 3,000 different fish species whose taxonomy is available at http://fishdb.sinica.edu.tw. The dataset contains videos recorded from sunrise to sunset showing several phenomena, e.g. murky water, algae on camera lens, etc., which makes the fish identification task more complex. Each video has a resolution of either 320x240 or 640x480 with 5 to 8 fps. As the LifeCLEF 2014 fish task included four subtasks, we employed different datasets for each subtask. In particular, for subtask 1 we used eight videos (four for training and four for testing) fully labeled (each single fish instance was annotated) using the tool in <ref type="bibr" coords="3,235.62,392.34,10.52,8.74" target="#b1">[2]</ref> resulting in 21,106 annotations corresponding to 9,852 different fish for the training dataset and in 14,829 annotations corresponding to 899 fish for the test set. For the other three subtasks, we used the (partially overlapping) following datasets:</p><p>-The D20M dataset, which contains about 20 million underwater images randomly selected from the whole F4K dataset. -The D35K dataset, which is a subset of D20M containing about 35000 fish images belonging to 10 fish species. Each image of this dataset was manually annotated with the corresponding species. -The D1M dataset, which contains about 1 million automatically-annotated images by using the method in <ref type="bibr" coords="3,283.19,518.44,10.52,8.74" target="#b2">[3]</ref> and further manually checked. This dataset is meant to be the subset of the images in D20M which belong to the classes annotated in D35K.</p><p>The D20M dataset represents a randomly-selected fraction of the data collected within the Fish4Knowledge project, amounting to more than a billion fish images. The D35K dataset is a subset of the D20M one, containing only (but not all) fish images belonging to the 10 most common species. Image annotation was carried out manually and validated by expert marine biologists. Figure <ref type="figure" coords="3,475.62,609.29,4.98,8.74" target="#fig_0">1</ref> shows sample pictures of the chosen 10 species.</p><p>The D1M dataset is also a subset of D20M, obtained by the automatic annotation technique described in <ref type="bibr" coords="3,268.58,657.11,10.52,8.74" target="#b2">[3]</ref> and further manually checked: briefly, images from D35K are used as queries to a similarity-based search in D20M ; after a check for false positives, the resulting images are then assigned to the same species label as the query image. We did not use directly the D35K dataset fish species classification, because it contained several near duplicate images.</p><p>The datasets used in the LifeCLEF 2014 fish task (subtasks 2, 3 and 4) were generated from the D1M dataset. In particular for subtask 2, which was meant to identify only some fish instances (unlike the subtask 1 which asked for identifying all fish in a video) in video frames, we used 112,078 annotations taken from 957 videos for the training set and 15,245 annotations from 89 videos for the test set.</p><p>For subtasks 3 and 4, we, instead, used 24,441 (belonging to 285 videos) annotated fish (and their species) in the training set and 6,956 annotations (belonging to 116 videos) in the test set. Details about the species distribution in the training and test datasets are shown in Table <ref type="table" coords="4,364.74,492.51,3.87,8.74" target="#tab_0">1</ref>.</p><p>It is important to note that some species were more common than others; although we tried to make the dataset as uniformly distributed across species as possible, for some of them (most evidently, Lutjanus fulvus) it was quite difficult to find a large number of adequate images, which resulted in a lower presence in the dataset.</p><p>The datasets (both the training and the test ones) were provided as zipped folders containing:</p><p>-A ground truth folder where the ground truth for all subtasks were given as XML files (Fig. <ref type="figure" coords="4,222.05,607.56,3.87,8.74" target="#fig_1">2</ref>): -A video folder where all the videos of the entire dataset were contained -A image folder which contains the images corresponding to the bounding boxes given in the XML files. These images were provided only for subtasks 2, 3, 4 in the training phase and only for subtask 4 in the test phase. 3 Participants</p><p>About 50 teams registered to the fish task, but only two teams submitted runs for the fish task: one, the I3S team, for subtask 3 and one, the LSIS/DYNI team for subtask 4.</p><p>The strategy employed by the I3S team for fish identification and recognition (subtask 3) consisted of, first, applying a background modeling approach based on Mixture of Gaussian for moving object segmentation. SVM learning using keyframes of species as positive entries and background of current video as negative entries was used for fish species classification.</p><p>The LSIS/DYNI team submitted three runs for subtask 4. Each run followed the strategy proposed in <ref type="bibr" coords="5,247.98,458.52,10.52,8.74" target="#b3">[4]</ref> which, basically, consisted of extracting low level features, patch encoding, pooling with spatial pyramid for local analysis and a linear large-scale supervised classication by averaging posterior probabilities estimated through linear regression of linear SVM's outputs. No image specific pre-processing regarding illumination correction or background subtraction was performed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Performance evaluation was carried out on the released test sets by computing: average precision and recall, and precision and recall for each fish species for subtask 3; average recall and recall for each fish species for subtask 4. The baselines for subtasks 3 and 4 were, respectively:</p><p>-Subtask 3: The ViBe <ref type="bibr" coords="6,253.06,198.35,10.52,8.74" target="#b4">[5]</ref> background modeling approach for fish detection combined to VLFeat BoW <ref type="bibr" coords="6,270.01,210.31,10.52,8.74" target="#b5">[6]</ref> for fish species recognition -Subtask 4: VLFeat BoW for fish species recognition The results achieved by the I3S team for subtask 3 are reported in Fig. <ref type="figure" coords="6,475.62,489.74,4.98,8.74" target="#fig_2">3</ref> and compared to our baseline. When computing the performance in terms of fish detection, we relaxed the constraint on the PASCAL score as the I3S team often detected correctly a fish but the bounding box' size was much bigger than the one provided in our dataset (see Fig. <ref type="figure" coords="6,315.79,537.56,3.87,8.74" target="#fig_3">4</ref>).</p><p>While the average recall obtained by the I3S team was lower than the baseline's recall, the precision was improved (see Fig. <ref type="figure" coords="6,357.39,561.47,3.87,8.74">5</ref>), thus implying that their fish species classification approach was reliable more than the fish detection approach. Furthermore, we allowed participants to provide a ranked list (top three species) of fish species for each fish instance in the test set and we treated a recognition as a true positive if the correct species was in the top three species. Considering only the most probable class provided in the results submitted by the I3S team makes the performance drop considerably. The reason behind this may be found in the size of the bounding boxes extracted by the I3S team. In fact bigger bounding boxes may contain also background objects and/or other fish instances, thus affecting the performance of the fish species classification approach. The results obtained by the LSIS/DYNI team for subtask 4 are shown in Fig. <ref type="figure" coords="7,134.77,483.21,3.87,8.74">6</ref>. In this case, the performance were computed by taking into account only the most probable fish species (i.e. the first class in the provided ranked list).</p><p>Please note the image-based recognition task (subtask 4) was easier than subtask 3 since it does need any fish identification module (which is the most complex part in video-based fish identification) and we had only ten fish species with very distinctive features. However, LSIS/DYNI team did a great job outperforming our baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Concluding remarks</head><p>In this report, we described the LifeCLEF 2014 fish task, which aimed at benchmarking machine learning and computer vision methods for fish detection and recognition in underwater "real-life" video footage. Although the fifty teams registered for the fish task, only two teams submitted runs: the I3S team for subtask 3 and LSIS/DYNI team for subtask 4. The main challenge of our underwater Fig. <ref type="figure" coords="8,154.40,306.47,4.13,7.89">5</ref>. Average precision and precision for fish species achieved by the I3S team on subtask 3 compared to our baseline Fig. <ref type="figure" coords="8,154.40,611.27,4.13,7.89">6</ref>. Average recall and recall for fish species achieved by the LSIS/DYNI team on subtask 4 compared to our baseline dataset was to identify fish correctly (as moving objects) by processing the entire video sequences. This appears evident by looking the results achieved by the I3S team (Fig. <ref type="figure" coords="9,184.46,143.90,4.98,8.74" target="#fig_2">3</ref> and<ref type="figure" coords="9,213.67,143.90,3.87,8.74">5</ref>): for instance, the I3S team was not able to detect any of the Lutjanus fulvus instances, as it is a rather tiny fish which often tends to hide behind rocks. The subtask 4 was easier (as shown by the results achieved by our baseline) as the fish were already segmented from the background and the considered fish species have very distinctive features that make their identification in the feature space almost trivial. Nevertheless, the results achieved by the LSIS/DYNI team are excellent, outperforming our baseline for almost all species.</p><p>We have just completed the labelling of other 25 species and we are working on removing the near duplicates from the dataset to make the fish identification and recognition task more challenging.</p><p>We would like to thank to all teams who participated to the task and the ImageCLEF and LifeCLEF organisation who made the first edition of fish task possible.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,134.77,259.93,345.81,7.89;4,134.77,270.92,345.82,7.86;4,134.77,281.88,345.83,7.86;4,134.77,292.84,345.83,7.86;4,134.77,303.79,148.41,7.86;4,136.94,182.92,62.25,62.24"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The 10 fish species analysed in this work. Please note that these images are taken directly from the D35K dataset. From left to right and top to bottom: Acanthurus nigrofuscus, Amphiprion clarkii, Chaetodon lunulatus, Chromis margaritifer, Dascyllus reticulatus, Hemigymnus fasciatus, Lutjanus fulvus, Myripristis kuntee, Neoniphon sammara, Plectrogly-Phidodon dickii.</figDesc><graphic coords="4,136.94,182.92,62.25,62.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,218.33,632.94,178.70,7.89;5,134.77,550.52,345.82,56.69"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Example of a ground truth XML file</figDesc><graphic coords="5,134.77,550.52,345.82,56.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,134.77,445.30,345.83,7.89;6,134.77,456.28,109.15,7.86;6,134.77,260.60,345.83,158.96"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Average recall and recall for fish species achieved by the I3S team on subtask 3 compared to our baseline</figDesc><graphic coords="6,134.77,260.60,345.83,158.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,134.77,426.37,345.83,7.89;7,134.77,437.36,173.22,7.86;7,148.28,294.89,155.62,116.71"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Matching between the bounding boxes computed by the I3S team (in red) and the ones (in green) provided in our dataset</figDesc><graphic coords="7,148.28,294.89,155.62,116.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="8,134.77,400.90,345.83,184.64"><head></head><label></label><figDesc></figDesc><graphic coords="8,134.77,400.90,345.83,184.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,134.77,116.91,345.81,162.40"><head>Table 1 .</head><label>1</label><figDesc>Distribution of fish species in the training and test sets for subtasks 3 and 4.</figDesc><table coords="5,201.80,140.87,208.67,138.44"><row><cell>Species</cell><cell cols="2">Training Set Test Set</cell></row><row><cell>Acanthurus nigrofoscus</cell><cell>2,511</cell><cell>725</cell></row><row><cell>Amphiprion clarkii</cell><cell>2,985</cell><cell>878</cell></row><row><cell>Chaetodon lunulatus</cell><cell>2,494</cell><cell>917</cell></row><row><cell>Chromis margaritifer</cell><cell>3,282</cell><cell>371</cell></row><row><cell>Dascyllus reticulatus</cell><cell>3,196</cell><cell>681</cell></row><row><cell>Hemygimnus fasciatus</cell><cell>2,224</cell><cell>852</cell></row><row><cell>Lutjanus fulvus</cell><cell>720</cell><cell>146</cell></row><row><cell>Myripristis berndti</cell><cell>2,554</cell><cell>840</cell></row><row><cell>Neoniphon sammara</cell><cell>2,019</cell><cell>969</cell></row><row><cell>Plectrogly-Phidodon dickii</cell><cell>2,456</cell><cell>577</cell></row><row><cell>Total</cell><cell>24,441</cell><cell>6,956</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="2,144.73,657.80,94.26,7.86"><p>www.fish4knowledge.eu</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,138.35,354.79,342.23,7.86;9,146.91,365.75,333.66,7.86;9,146.91,376.71,333.66,7.86;9,146.91,387.64,292.34,7.89" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,273.49,365.75,207.08,7.86;9,146.91,376.71,283.63,7.86">A research tool for long-term and continuous analysis of fish assemblage in coral-reefs using underwater camera footage</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">J</forename><surname>Boom</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Beyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">M</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">P</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Fisher</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ecoinf.2013.10.006</idno>
		<ptr target="http://dx.doi.org/10.1016/j.ecoinf.2013.10.006" />
	</analytic>
	<monogr>
		<title level="j" coord="9,440.01,376.71,40.57,7.86;9,146.91,387.67,45.63,7.86">Ecological Informatics</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,398.63,342.22,7.86;9,146.91,409.59,333.67,7.86;9,146.91,420.52,139.93,7.89" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,435.16,398.63,45.41,7.86;9,146.91,409.59,235.72,7.86">An innovative web-based collaborative platform for video annotation</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Kavasidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salvo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Giordano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,391.38,409.59,89.20,7.86;9,146.91,420.55,50.48,7.86">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="413" to="432" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,431.51,342.23,7.86;9,146.91,442.46,333.66,7.86;9,146.91,453.42,115.26,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,323.28,431.51,157.30,7.86;9,146.91,442.46,176.85,7.86">Nonparametric label propagation using mutual local similarity in nearest neighbors</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Giordano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,333.80,442.46,146.78,7.86;9,146.91,453.42,86.59,7.86">To appear on Computer Vision and Image Understanding</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,464.38,342.22,7.86;9,146.91,475.34,333.66,7.86;9,146.91,486.30,333.66,7.86;9,146.91,497.26,18.43,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,294.89,464.38,185.69,7.86;9,146.91,475.34,294.47,7.86">Sparse coding for histograms of local binary patterns applied for image categorization: Toward a bag-of-scenes analysis</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Halkias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,462.14,475.34,18.43,7.86;9,146.91,486.30,252.53,7.86">2012 21st International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2012-11">Nov 2012</date>
			<biblScope unit="page" from="2817" to="2820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,508.22,342.23,7.86;9,146.91,519.15,333.67,7.89;9,146.91,530.14,66.55,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,308.03,508.22,172.54,7.86;9,146.91,519.18,115.37,7.86">Vibe: A universal background subtraction algorithm for video sequences</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Barnich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Van Droogenbroeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,269.29,519.18,161.60,7.86">Image Processing, IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1709" to="1724" />
			<date type="published" when="2011-06">June 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,541.09,342.23,7.86;9,146.91,552.05,307.59,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,264.37,541.09,216.21,7.86;9,146.91,552.05,67.43,7.86">VLFeat -an open and portable library of computer vision algorithms</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Fulkerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,236.62,552.05,184.77,7.86">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
