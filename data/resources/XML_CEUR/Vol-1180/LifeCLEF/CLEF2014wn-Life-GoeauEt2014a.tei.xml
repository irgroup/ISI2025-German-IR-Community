<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,167.73,115.96,279.89,12.62">LifeCLEF Bird Identification Task 2014</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,151.87,154.18,56.55,8.74"><forename type="first">Hervé</forename><surname>Goëau</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inria ZENITH team</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,218.98,154.18,56.56,8.74"><forename type="first">Hervé</forename><surname>Glotin</surname></persName>
							<email>glotin@univ-tln.fr</email>
							<affiliation key="aff1">
								<orgName type="laboratory">CNRS LSIS</orgName>
								<orgName type="institution" key="instit1">Aix Marseille Univ</orgName>
								<orgName type="institution" key="instit2">ENSAM</orgName>
								<orgName type="institution" key="instit3">Univ. Toulon</orgName>
								<orgName type="institution" key="instit4">Institut Univ. de France</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,286.09,154.18,90.66,8.74"><forename type="first">Willem-Pier</forename><surname>Vellinga</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Xeno-canto Foundation</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,387.31,154.18,68.94,8.74"><forename type="first">Robert</forename><surname>Planqué</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Xeno-canto Foundation</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,227.73,166.13,71.10,8.74"><forename type="first">Andreas</forename><surname>Rauber</surname></persName>
							<email>rauber@ifs.tuwien.ac.at</email>
							<affiliation key="aff3">
								<orgName type="institution">Vienna University of Technology</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,328.76,166.13,48.07,8.74"><forename type="first">Alexis</forename><surname>Joly</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inria ZENITH team</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">LIRMM</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,167.73,115.96,279.89,12.62">LifeCLEF Bird Identification Task 2014</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DA15102B7827F4FB4B7812E68D7A9708</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>LifeCLEF</term>
					<term>bird</term>
					<term>song</term>
					<term>call</term>
					<term>species</term>
					<term>retrieval</term>
					<term>audio</term>
					<term>collection</term>
					<term>identification</term>
					<term>fine-grained classification</term>
					<term>evaluation</term>
					<term>benchmark</term>
					<term>bioacoustics</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The LifeCLEF bird identification task provides a testbed for a system-oriented evaluation of 501 bird species identification. The main originality of this data is that it was specifically built through a citizen science initiative conducted by Xeno-Canto, an international social network of amateur and expert ornithologists. This makes the task closer to the conditions of a real-world application than previous, similar initiatives. This overview presents the resources and the assessments of the task, summarizes the retrieval approaches employed by the participating groups, and provides an analysis of the main evaluation results. With a total of ten groups from seven countries and with a total of twenty-nine runs submitted, involving distinct and original methods, this first year task confirms the interest of the audio retrieval community for biodiversity and ornithology, and highlights further challenging studies in bird identification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Accurate knowledge of the identity, the geographic distribution and the evolution of bird species is essential for a sustainable development of humanity as well as for biodiversity conservation. Unfortunately, such basic information is often only partially available for professional stakeholders, teachers, scientists and citizens. In fact, it is often incomplete for ecosystems that possess the highest diversity, such as tropical regions. A noticeable cause and consequence of this sparse knowledge is that identifying birds is usually impossible for the general public, and often a difficult task for professionals like park rangers, ecology consultants, and of course, the ornithologists themselves. This "taxonomic gap" <ref type="bibr" coords="1,134.77,644.16,15.50,8.74" target="#b23">[25]</ref> was actually identified as one of the main ecological challenges to be solved during United Nations Conference in Rio de Janeiro, Brazil, in 1992.</p><p>The use of multimedia identification tools is considered to be one of the most promising solutions to help bridging this taxonomic gap <ref type="bibr" coords="2,384.70,130.95,14.61,8.74" target="#b12">[14]</ref>, <ref type="bibr" coords="2,406.60,130.95,9.96,8.74" target="#b8">[9]</ref>, <ref type="bibr" coords="2,423.52,130.95,9.96,8.74" target="#b4">[5]</ref>, <ref type="bibr" coords="2,440.43,130.95,14.61,8.74" target="#b20">[22]</ref>, <ref type="bibr" coords="2,462.34,130.95,14.61,8.74" target="#b19">[21]</ref>. With the recent advances in digital devices, network bandwidth and information storage capacities, the collection of multimedia data has indeed become an easy task. In parallel, the emergence of "citizen science" and social networking tools has fostered the creation of large and structured communities of nature observers (e.g. eBird <ref type="foot" coords="2,212.23,189.15,3.97,6.12" target="#foot_0">6</ref> , Xeno-canto<ref type="foot" coords="2,271.45,189.15,3.97,6.12" target="#foot_1">7</ref> , etc.) that have started to produce outstanding collections of multimedia records. Unfortunately, the performance of the state-ofthe-art multimedia analysis techniques on such data is still not well understood and it is far from reaching the real world's requirements in terms of identification tools. Most existing studies or available tools typically identify a few tens of species with moderate accuracy whereas they should be scaled-up to take one, two or three orders of magnitude more, in terms of number of species.</p><p>The LifeCLEF Bird task proposes to evaluate one of these challenges <ref type="bibr" coords="2,465.10,275.26,15.50,8.74" target="#b11">[12]</ref> based on big and real-world data and defined in collaboration with biologists and environmental stakeholders so as to reflect realistic usage scenarios.</p><p>Using audio records rather than bird pictures is justified by current practices <ref type="bibr" coords="2,134.77,323.92,9.96,8.74" target="#b4">[5]</ref>, <ref type="bibr" coords="2,152.54,323.92,14.61,8.74" target="#b20">[22]</ref>, <ref type="bibr" coords="2,175.31,323.92,14.61,8.74" target="#b19">[21]</ref>, <ref type="bibr" coords="2,198.07,323.92,9.96,8.74" target="#b3">[4]</ref>. Birds are actually not easy to photograph; audio calls and songs have proven to be easier to collect and sufficiently species specific.</p><p>Only three notable previous worldwide initiatives on bird species identification based on their songs or calls have taken place, all three in 2013. The first one was the ICML4B bird challenge joint to the International Conference on Machine Learning in Atlanta, June 2013 <ref type="bibr" coords="2,319.16,384.54,9.96,8.74" target="#b1">[2]</ref>. It was initiated by the SABIOD MASTODONS CNRS group<ref type="foot" coords="2,260.11,394.92,3.97,6.12" target="#foot_2">8</ref> , the University of Toulon and the National Natural History Museum of Paris <ref type="bibr" coords="2,270.66,408.45,14.61,8.74" target="#b9">[10]</ref>. It included 35 species, and 76 participants submitted their 400 runs on the Kaggle interface. The second challenge was conducted by F. Brigs at MLSP 2013 workshop, with 15 species, and 79 participants in August 2013. The third challenge, and biggest in 2013, was organised by University of Toulon, SABIOD and Biotope <ref type="bibr" coords="2,351.55,456.27,9.96,8.74" target="#b2">[3]</ref>, with 80 species from the Provence, France. More than thirty teams participated, reaching 92% of average AUC. Descriptions of the best systems of ICML4B and NIPS4B bird identification challenges are given in the on-line books <ref type="bibr" coords="2,349.39,492.14,10.96,8.74" target="#b1">[2,</ref><ref type="bibr" coords="2,360.35,492.14,7.31,8.74" target="#b0">1]</ref> including, in some cases, references to useful scripts. In collaboration with the organizers of these previous challenges, BirdCLEF 2014 goes one step further by (i) significantly increasing the species number by almost an order of magnitude (ii) working on real-world data collected by hundreds of recordists (iii) moving to a more usage-driven and system-oriented benchmark by allowing the use of meta-data and defining information retrieval oriented metrics. Overall, the task is expected to be much more difficult than previous benchmarks because of the higher confusion risk between the classes, the higher background noise and the higher diversity in the acquisition conditions (devices, recordists uses, contexts diversity, etc.). It will therefore probably produce sub-stantially lower scores and offer a better progression margin towards building real-world generalist identification tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset</head><p>The training and test data of the bird task is composed by audio recordings hosted on Xeno-canto (XC). Xeno-canto is a web-based community of bird sound recordists worldwide with about 1800 active contributors that have already collected more than 175,000 recordings of about 9040 species. 501 species from Brazil are used in the BirdCLEF dataset. They represent the species of that country with the highest number of recordings on XC, totalling 14,027 recordings recorded by hundreds of users. The dataset has between 15 and 91 recordings per species, recorded by between 10 and 42 recordists.</p><p>To avoid any bias in the evaluation related to the audio devices used, each audio file has been normalized to a constant bandwidth of 44.1 kHz and coded over 16 bits in .wav mono format (the right channel was selected by default). The conversion from the original Xeno-canto data set was done using ffmpeg, sox and matlab scripts. An optimized 16 Mel Filter Cepstrum Coefficients for bird identification (according to an extended benchmark <ref type="bibr" coords="3,370.23,339.74,10.79,8.74" target="#b6">[7]</ref>) have been computed with their first and second temporal derivatives on the whole set. They were used in the best systems run in ICML4B and NIPS4B challenges <ref type="bibr" coords="3,416.64,363.65,9.96,8.74" target="#b1">[2]</ref>, <ref type="bibr" coords="3,432.85,363.65,9.96,8.74" target="#b0">[1]</ref>, <ref type="bibr" coords="3,446.13,363.65,9.96,8.74" target="#b2">[3]</ref>, <ref type="bibr" coords="3,462.34,363.65,14.61,8.74" target="#b9">[10]</ref>.</p><p>Audio records are associated with various meta-data including the species of the most active singing bird, the species of the other birds audible in the background, the type of sound (call, song, alarm, flight, etc.), the date and location of the observations (from which rich statistics on species distribution can be derived), common names and collaborative quality ratings. All of them were produced collaboratively by the Xeno-canto community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task Description</head><p>Participants were asked to determine the species of the most active singing birds in each query file. The background noise can be used as any other meta-data, but it is forbidden to correlate the test set of the challenge with the original annotated Xeno-canto data base (or with any external content as many of them are circulating on the web). More precisely, the whole BirdCLEF dataset has been split in two parts, one for training (and/or indexing) and one for testing. The test set was built by randomly choosing 1/3 of the observations of each species whereas the remaining observations were kept in the reference training set. Recordings of the same species done by the same person the same day are considered as being part of the same observation and cannot be split across the test and training set. The xml files containing the meta-data of the query recordings were purged so as to erase the foreground and background species names (the ground truth), the vernacular names (common names of the birds) and the collaborative quality ratings (that would not be available at query stage in a real-world mobile application). Meta-data of the recordings in the training set are kept unaltered.</p><p>The groups participating to the task were asked to produce up to 4 runs containing a ranked list of the most probable species for each record of the test set. Each species had to be associated with a normalized score in the range [0, 1] reflecting the likelihood that this species was singing in the sample. For each submitted run, participants had to say if the run was performed fully automatically or with a human assistance in the processing of the queries, and if they used a method based on only audio analysis or with the use of the metadata. The metric used to compare the runs was the Mean Average Precision averaged across all queries. Since the audio records contain a main species and often some background species belonging to the set of 501 species in the training, we decided to use two metrics, one focusing on all species (MAP1) and a second one focusing only on the main species (MAP2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Participants and methods</head><p>87 research groups worldwide registered for the task and downloaded the data (from a total of 127 groups that registered for at least one of the three LifeCLEF tasks). 42 of the 87 registered groups were exclusively registered to the bird task and not to the other LifeCLEF tasks. This shows the high attractiveness of the task in both the multimedia community (presumably interested in several tasks) and in the audio and bioacoustics community (presumably registered only to the bird songs task). Finally, 10 of the 87 registrants, coming from 9 distinct countries, crossed the finish by submitting runs (with a total of 29 runs). These 10 were mainly academics, specialized in bioacoustics, audio processing or multimedia information retrieval. We list them hereafter in alphabetical order and give a brief overview of the techniques they used in their runs. We would like to point out that the LifeCLEF benchmark is a system-oriented evaluation and not a deep or fine evaluation of the underlying algorithms. Readers interested in the scientific and technical details of the implemented methods should refer to the LifeCLEF 2014 working notes or to the research papers of each participant (referenced below):</p><p>BiRdSPec, Brazil/Spain, 4 runs: The 4 runs submitted by this group were based on audio features extracted by the Marsyas framework<ref type="foot" coords="4,397.16,561.71,3.97,6.12" target="#foot_3">9</ref> (Time ZeroCrossings features, Spectral Centroid, Flux and Rolloff, and Mel-Frequency Cepstral Coefficients). The runs then differ in two major things: (i) Flat vs. Hierarchical multi-class Support Vector Machine (i.e. using a multi-class Support Vector Machines at each node of the taxonomy as discussed in a research paper of the authors <ref type="bibr" coords="4,171.29,623.06,15.50,8.74" target="#b16">[18]</ref>) (ii) classification of full records vs. classification of automatically detected segments (and majority voting on the resulting local predictions). The detail of the runs is the following: BirdSPec Run 1 : flat classifier, no segmentation BirdSPec Run 2 : flat classifier, segmentation BirdSPec Run 3 : hierarchical classifier, no segmentation BirdSPec Run 4 : hierarchical classifier, segmentation Their results (see section 5) show that (i) the segments oriented classification approach brings slight improvements (ii) using the hierarchical classifier does not improve the performances over the flat one (at least using our flat evaluation measure). Note that in every submitted run, only one species was proposed for each query involving lower performances that they should expected with several species propositions.</p><p>Golem, Mexico, 3 runs <ref type="bibr" coords="5,255.64,265.76,16.80,8.77" target="#b13">[15]</ref>: The audio-only classification method used by this group consists of four stages: (i) pre-processing of the audio signal based on down-sampling and bandpass filtering (between 500hz and 4500hz) (ii) segmentation in syllables (iii) candidate species generation based on HOG features <ref type="bibr" coords="5,134.77,313.61,10.52,8.74" target="#b5">[6]</ref> extracted from the syllables and Support Vector Machine (iv) final identification using a Sparse Representation-based classication of HOG features <ref type="bibr" coords="5,470.09,325.56,10.52,8.74" target="#b5">[6]</ref> or LBP features <ref type="bibr" coords="5,209.27,337.52,14.61,8.74" target="#b22">[24]</ref>. Runs Golem Run 1 and Golem Run 2 differ only in the number of candidate species kept at the third stage (100 vs. 50). Golem Run 3 uses LBP features rather than HOG features for the last step. Best performances were achieved by Golem Run 1.</p><p>HTL, Singapore, 3 runs <ref type="bibr" coords="5,262.44,401.15,16.80,8.77" target="#b15">[17]</ref>: This group experimented several ensembles of classifiers on spectral audio features (filtered MFCC features &amp; spectrumsummarizing features) and metadata features (using 8 fields: Latitude, Longitude, Elevation, Year, Month, Month + Day, Time, Author). The 3 runs mainly differ in the used ensemble of classifiers and the used features: HLT Run 1 : &amp; LDA on audio features locally pooled within 0.5 seconds windows, Random Forest on Metadata (matlab implementation) HLT Run 2 : &amp; LDA, Logistic Regression, SVM, Adaboost and Knn classifier on Metadata and audio features globally pooled with a max pooling strategy, Random Forest on Metadata only (sklearn implementation) HLT Run 3 : &amp; combination of HLT Run 1 and HLT Run 2 Interestingly, in further experiments reported in their working note <ref type="bibr" coords="5,428.55,532.68,14.61,8.74" target="#b15">[17]</ref>, the authors show that using only the metadata features can perform as well as using only the audio features they experimented.</p><p>Inria Zenith, France, 3 runs <ref type="bibr" coords="5,281.34,584.36,16.80,8.77" target="#b10">[11]</ref>: This group experimented a fine-grained instance-based classification scheme based on the dense indexing of individual 26-dimensional MFCC features and the pruning of the non-discriminant ones. To make such strategy scalable to the 30M of MFCC features extracted from the tens of thousands audio recordings of the training set, they used highdimensional hashing techniques coupled with an efficient approximate nearest neighbors search algorithm with controlled quality. Further improvements were obtained by (i) using a sliding classifier with max pooling (ii) weighting the query features according to their semantic coherence (iii) making use of the metadata to post-filter incoherent species (geo-location, altitude and time-ofday). Runs INRIA Zenith Run 1 and INRIA Zenith Run 2 differ in whether the post-filtering based on metadata is used or not. <ref type="bibr" coords="6,308.25,214.21,10.52,8.74" target="#b7">[8]</ref> to extract 57-dimensional low level audio features per frame (35 spectral features, 13 ceptral features, 6 energy features, 3 voicing related features) and then describe an entire audio recording by calculating statistics from the low level features trajectories (as well as their velocity and accelaration trajectories) through 39 functionals including e.g. means, extremes, moments, percentiles and linear as well as quadratic regression. This sums up to 6669-dimensional global features (57 x 3 x 39) per recording that were reduced to 1277-dimensional features through an unsupervised dimension reduction technique. A second type of audio features, namely segment-probabilities, was then extracted. This method consists in using the matching probabilities of segments as features (or more precisely the maxima of the normalized crosscorrelation between segments and spectrogrm images using a template matching approach). The details of the different steps including the audio signal preprocessing, the segmentation process and the template matching can be found in <ref type="bibr" coords="6,134.77,381.58,14.61,8.74">[13]</ref>. Besides, they also extracted 8 features from the metadata (Year, Month, Time, Latitude, Longitude, Elevation, Locality Index, Author Index). The final classification was done by first selecting the most discriminant features per species (from 100 to 300 features per class) and using the scikit-learn library (ExtraTreesRegressor) for training ensembles of randomized decision trees with probabilistic outputs. Details of the different parameters settings used in each run are detailed in <ref type="bibr" coords="6,223.21,453.31,14.61,8.74">[13]</ref>. On average the use of Segment-Probabilities outperforms the other feature sets but for some species the openSMILE and in rare cases even the Metadata feature set was a better choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MNB TSA, Germany, 4 runs [13]: This participant first used the open-SMILE audio features extraction tool</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QMUL, UK, 4 runs [19]:</head><p>This group focused on unsupervised feature learning in order to learn regularities in spectro-temporal content without reference to the training labels and further help the classifier to generalise to further content of the same type. MFCC features and several temporal variants are first extracted from the audio signal after a median-based thresholding pre-processing. Extracted low level features were then reduced through PCA whitening and clustered via spherical k-means (and a two-layer variant of it) to build the vocabulary. During classification, MFCC features are pooled by projecting them on the vocabulary with different temporal pooling strategies. Final supervised classification is achieved thanks to a random forest classifier. This method is the subject of a full-length article which can be read at <ref type="bibr" coords="6,361.19,632.21,14.61,8.74" target="#b18">[20]</ref>. Details of the different parameters settings used in each run are detailed in the working note <ref type="bibr" coords="6,440.93,644.16,14.61,8.74" target="#b17">[19]</ref>.</p><p>Randall, France, 1 run: This run Randall Run 1 is below the ones of the random classifier, which can be explained because of errors in the use of the labels and also by the fact that only one species was proposed for each query, thus this participant did not submit a working note. SCS, UK, 3 runs <ref type="bibr" coords="7,222.03,179.21,16.80,8.77" target="#b14">[16]</ref>: By participating in the LifeCLEF 2014 Bird Task this participant was hoping to demonstrate that spectrogram correlation as implemented in the Ishmael v2.3 library<ref type="foot" coords="7,288.99,201.58,7.94,6.12" target="#foot_4">10</ref> can be very useful for the automatic detection of certain bird calls. Using this method, each test audio record required approximately 12 hours to be processed. The submitted run was consequently restricted to only 14 of the 4339 test audio records, explaining the close to zero evaluation score. This demonstrates the limitation of the approach in the context of large-scale classification.</p><p>Utrecht Univ., The Netherlands, 1 run <ref type="bibr" coords="7,327.28,287.28,17.82,8.77" target="#b21">[23]</ref> This participant is the only one who experimented with a deep neural network within the task (for the last steps of the method, i.e. feature learning and classification). Their whole framework first includes a decimating and dynamic filtering of the audio signal followed by an energy-based segment detection. Detected segments are then clustered into higher temporal structures through a simple gap-wise merging of smaller sections. MFCC features and several extended variants were then extracted from the consolidated segments before being trained individually by the deep neural network. At query time, an activation-weighted voting strategy was finally used to pool the predictions of the different segments into a final strong classifier.</p><p>Yellow Jackets, USA, 1 run As this participant did not submit a working note, we don't have any meaningful information about the submitted run Yellow Jackets Run 1. We only know that it achieved very low performances, close to the random classifier. Note that only one species was proposed for each query explaining also these low performances.</p><p>Table <ref type="table" coords="7,178.04,503.44,4.98,8.74">1</ref> attempts to summarize the methods used at different stages (feature, classification, subset selection,...) in order to highlight the main choices of participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Figure <ref type="figure" coords="7,166.70,587.84,4.98,8.74" target="#fig_2">1</ref> and table <ref type="table" coords="7,220.78,587.84,4.98,8.74">1</ref> show the scores obtained by all the runs for the two distinct measured Mean Average Precision (MAP) evaluation measures: MAP1 when considering only the foreground species of each test recording and MAP2 when considering additionally the species listed in the Background species field of the metadata. Note that different colors have been used to easily differentiate Table <ref type="table" coords="8,165.37,115.91,4.13,7.89">1</ref>. Approaches used by participants. Several sp. in last column indicates if participants gave several ranked species propositions for each query, or if they gave only one species (retrieval vs. pure classification approach).   The first main outcome is that the two best performing methods were already among the best performing methods in previous bird identification challenges [2,10,1,3] although LifeCLEF dataset is much bigger and more complex. This clearly demonstrates the generic nature and the stability of the underlying methods. The best performing runs of the MNB TSA group notably confirmed that using matching probabilities of segments as features was once again a good choice. In their working note [13], Lassek et al. actually show that the use of such Segment-Probabilities clearly outperforms the other feature sets they used (0.49 mAP compared to 0.30 for the OpenSmile features <ref type="bibr" coords="9,355.59,644.16,10.52,8.74" target="#b7">[8]</ref> and 0.12 for the metadata features). The approach however remains very time consuming as several days on 4 computers were required to process the whole LifeCLEF dataset. Then, the best performing (purely) audio-based runs of QMUL confirmed that unsupervised feature learning is a simple and effective method to boost classification performance by learning spectro-temporal regularities in the data. They actually show in their working note <ref type="bibr" coords="10,299.54,452.88,15.50,8.74" target="#b17">[19]</ref> that their pooling method based on spherical k-means actually produces much more effective features than the raw initial low level features (MFCC based). The principal practical issue with such unsupervised feature learning is that it requires large data volumes to be effective. However, this exhibits a synergy with the large data volumes used within LifeCLEF. This might also explain the rather good performances obtained by the runs of Inria ZENITH group who used hash-based indexing techniques of MFCC features and approximate nearest neigbours classifiers. The underlying hash-based partition and embedding method actually works as an unsupervised feature learning method. As could be expected, the MAP1 evaluation measure (with the background species) scores are generally lower than the MAP2 scores (without the background species). Only the HTL group did not observe this, and demonstrated the ability of their method to perform a multi-label classification. A last interesting remark we derived so far from the results comes from the runs submitted by the BirdSPec group. As their two first runs were based on using flat SVM classifiers whereas the 3rd and 4th runs were based on using a hierarchical multi-class SVM classifier it is possible to assess the contribution of using the taxonomy hierarchy within the classification process. Unfortunately, their results show that this rather tends to slightly degrade the results, at least when using a flat classification evaluation measure as the one we are using. On the other side, we cannot conclude on whether the mistakes done by the flat classifier are further from the correct species compared to the hierarchical one. This would require using a hierarchical evaluation measure (such as the Tree Induced Error) and might be considered in next campaigns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper presented the overview and the results of the first LifeCLEF bird identification task. With a number of 87 registrants, it did show a high interest of the multimedia and the bio-accoustic communities in applying their technologies to real-world environmental data such as the ones collected by Xeno-canto. The main outcome of this evaluation is a snapshot of the performances of state-ofthe-art techniques that will hopefully serves a guideline for developers interested in building end-user applications. One important conclusion of the campaign is that the two best performing methods were already among the best performing methods in previous bird identification challenges although LifeCLEF dataset is much bigger and more complex. This clearly demonstrates the generic nature of the underlying methods as well as their stability. On the other side, the size of the data was a problem for many registered groups who were not able to produce results within the allocated time and finally abandoned. Even the best performing method of the task (used in the best run) was ran on only 96.8% of the test data and had to be completed by an alternative faster solution for the remaining recordings to be identified. For the next years, we believe is it important to continue working on such large scales and even try to scale up the challenge to thousand species. Maintaining the pressure on the training set size is actually the only way to guaranty that the evaluated technologies could be soon integrated in real-world applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="8,191.21,650.22,6.14,21.52;8,191.21,577.54,6.14,54.83;8,191.21,475.16,6.14,33.38;8,191.21,332.56,6.14,52.15;8,191.21,223.05,6.14,37.83;8,191.21,160.49,6.14,28.43;8,199.20,179.11,6.12,9.81;8,220.10,637.11,6.12,34.63;8,220.10,586.61,6.12,45.77;8,220.10,568.89,6.12,15.03;8,220.10,559.12,6.12,7.08;8,208.83,490.51,6.12,18.03;8,208.83,440.07,6.12,48.33;8,208.83,408.40,6.12,29.56;8,208.83,387.50,6.12,18.79;8,216.80,496.09,6.12,12.45;8,216.80,458.04,6.12,32.52;8,216.80,436.54,6.12,15.98;8,216.80,418.24,6.12,12.77;8,216.80,387.50,6.12,25.22;8,224.77,495.77,6.12,12.77;8,224.77,441.58,6.12,51.06;8,224.77,409.20,6.12,29.24;8,224.77,387.50,6.12,18.57;8,232.74,484.24,6.12,24.30;8,212.13,362.29,6.12,22.42;8,212.13,342.15,6.12,17.45;8,212.13,309.96,6.12,29.50;8,212.13,292.25,6.12,15.03;8,212.13,285.59,6.12,3.97;8,212.13,276.79,6.12,6.12;8,212.13,267.03,6.12,7.08;8,220.10,346.82,6.12,37.89;8,220.10,302.37,6.12,41.63;8,220.10,282.10,6.12,17.44;8,220.10,263.67,6.12,15.61;8,228.07,368.13,6.12,16.58;8,228.07,350.42,6.12,15.03;8,228.07,343.76,6.12,3.97;8,228.07,334.96,6.12,6.12;8,228.07,325.19,6.12,7.08;8,220.10,222.99,6.12,37.89;8,220.10,191.72,6.12,23.08;8,228.07,249.18,6.12,11.70;8,220.10,182.70,6.12,6.23;8,238.71,649.26,6.12,22.48;8,238.71,578.23,6.12,54.14;8,238.71,543.56,6.12,31.85;8,238.71,511.33,6.12,29.42;8,246.68,586.61,6.12,45.77;8,246.68,577.26,6.12,6.66;8,246.68,545.40,6.12,29.18;8,238.71,488.12,6.12,20.42;8,238.71,469.59,6.12,15.85;8,238.71,362.13,6.12,22.58;8,238.71,283.37,6.12,76.50;8,238.71,263.66,6.12,17.45;8,238.71,254.65,6.12,6.23;8,233.82,182.38,6.12,6.55;8,269.58,655.24,6.12,16.49;8,269.58,587.46,6.12,44.91;8,269.58,539.01,6.12,45.77;8,265.59,482.82,6.12,25.72;8,265.59,424.82,6.12,48.41;8,265.59,387.50,6.12,27.74;8,273.56,487.78,6.12,20.76;8,261.61,351.73,6.12,32.98;8,261.61,310.74,6.12,37.06;8,261.61,279.61,6.12,27.19;8,261.61,263.67,6.12,12.01;8,269.58,354.89,6.12,29.82;8,269.58,329.23,6.12,19.70;8,269.58,286.77,6.12,36.51;8,269.58,263.66,6.12,17.15;8,277.55,355.64,6.12,29.07"><head></head><label></label><figDesc>segmentation MFCC, time-averaged spectrograms Ensemble Classifiers: Logistic Regression, SVM, AdaBoost, Knn, Random</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,295.62,655.12,6.12,16.62;8,303.59,649.20,6.12,22.54;8,291.83,603.99,6.12,28.38;8,291.83,563.84,6.12,36.69;8,291.83,541.59,6.12,18.79;8,291.83,531.26,6.12,6.87;8,291.83,511.32,6.12,16.47;8,299.80,613.87,6.12,18.51;8,299.80,581.85,6.12,29.33;8,299.80,550.29,6.12,28.88;8,296.26,485.08,6.12,23.47;8,292.31,334.17,6.12,50.54;8,292.31,283.57,6.12,46.36;8,292.31,263.66,6.12,15.67;8,300.28,363.20,6.12,21.51;8,290.73,254.33,6.12,6.55;8,295.62,236.62,6.12,15.03;8,295.62,226.85,6.12,7.08;8,290.73,182.38,6.12,6.55;8,326.48,653.12,14.09,18.62;8,326.48,580.92,6.12,51.46;8,326.48,559.12,6.12,17.33;8,326.48,521.91,6.12,32.74;8,326.48,511.33,6.12,6.12;8,334.45,586.61,6.12,45.77;8,334.45,562.24,6.12,16.10;8,334.45,511.33,6.12,42.66;8,342.42,608.93,6.12,23.44;8,311.22,433.47,6.12,75.08;8,319.19,500.60,6.12,7.94;8,319.19,470.00,6.12,27.31;8,319.19,434.04,6.12,32.68;8,319.19,401.41,6.12,29.35;8,319.19,387.49,6.12,10.63;8,327.16,492.12,6.12,16.43;8,327.16,435.43,6.12,50.99;8,327.16,399.31,6.12,30.42;8,327.16,387.49,6.12,6.12;8,335.13,481.71,6.12,26.84;8,335.13,473.34,6.12,6.12;8,335.13,429.86,6.12,41.22;8,335.13,421.39,6.12,6.23;8,335.13,411.20,6.12,7.94;8,335.13,387.50,6.12,21.45;8,343.10,493.40,6.12,15.14;8,343.10,452.78,6.12,37.94;8,327.16,344.57,6.12,40.14;8,327.16,314.36,6.12,27.52;8,327.16,295.21,6.12,16.47;8,326.48,214.43,6.12,46.45;8,326.48,191.72,6.12,18.89;8,334.45,249.93,6.12,10.95;8,334.45,224.27,6.12,22.97;8,321.59,182.38,6.12,6.55;8,353.36,647.65,6.12,24.09;8,353.36,585.10,6.12,47.27;8,353.36,539.44,6.12,42.98;8,350.06,463.37,6.12,45.17;8,350.06,433.65,6.12,24.15;8,350.06,400.16,6.12,27.91;8,350.06,387.50,6.12,7.08;8,358.03,496.20,6.12,12.35;8,358.03,478.06,6.12,15.46;8,358.03,455.75,6.12,19.62;8,353.36,355.64,6.12,29.07;8,353.36,333.59,6.12,19.37;8,353.36,254.65,6.12,6.23;8,348.47,182.38,6.12,6.55;8,364.00,644.70,6.12,27.04;8,364.00,496.78,6.12,11.76;8,364.00,474.13,6.12,19.97;8,364.00,464.79,6.12,6.66;8,364.00,451.05,6.12,11.06;8,364.00,420.15,6.12,28.21;8,364.00,182.70,6.12,6.23;8,372.37,657.25,6.12,14.49;8,372.37,465.88,6.12,42.66;8,372.37,345.21,6.12,39.51;8,372.37,327.06,6.12,15.46;8,372.37,254.65,6.12,6.23;8,367.48,182.38,6.12,6.55;8,391.24,645.46,6.12,26.28;8,399.21,652.76,6.12,18.98;8,391.24,585.36,6.12,47.01;8,391.24,576.56,6.12,6.12;8,391.24,524.67,6.12,49.20;8,379.93,490.08,6.12,18.46;8,379.93,461.85,6.12,23.47;8,379.93,445.81,6.12,11.27;8,379.93,410.72,6.12,30.32;8,379.93,387.50,6.12,18.46;8,387.90,495.77,6.12,12.77;8,387.90,463.06,6.12,28.55;8,387.90,452.45,6.12,6.44;8,387.90,437.23,6.12,11.06;8,387.90,406.44,6.12,26.63;8,387.90,395.63,6.12,6.66;8,387.90,387.50,6.12,3.97;8,395.87,478.23,6.12,30.31;8,395.87,453.11,6.12,20.71;8,395.87,420.14,6.12,28.55;8,395.87,402.96,6.12,12.77;8,395.87,387.49,6.12,11.06;8,403.84,490.08,6.12,18.46;8,403.84,480.96,6.12,6.44;8,403.84,460.56,6.12,17.71;8,403.84,428.48,6.12,29.39;8,391.24,367.23,6.12,17.48;8,391.24,341.38,6.12,23.16;8,391.24,306.48,6.12,32.22;8,391.24,254.65,6.12,6.23;8,386.35,182.38,6.12,6.55;8,409.81,646.59,14.09,25.15;8,409.81,182.70,6.12,6.23;9,134.77,118.99,345.84,8.74"><head></head><label></label><figDesc>spectral, cepstral, energy, voicing-related features) + velocity + acceleration × 39 statismean MFCC per segment, mean and variance of the MFCCs in a segment, mean, variance and the mean of three sections. use of the metadata from the purely audio-based methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="10,134.77,341.81,345.81,7.89;10,134.77,352.79,345.81,7.86;10,134.77,363.75,226.95,7.86;10,134.77,115.83,345.84,211.20"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Official scores of the LifeCLEF Bird Identification Task. MAP1 is the Mean Average Precision averaged across all queries taking int account the Background species (while MAP2 is considering only the foreground species.</figDesc><graphic coords="10,134.77,115.83,345.84,211.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="9,136.16,160.89,343.39,356.48"><head>Table 2 .</head><label>2</label><figDesc>Raw results of the LifeCLEF 2014 Bird Identification Task</figDesc><table coords="9,136.16,179.94,343.39,337.43"><row><cell>Run name</cell><cell>Type</cell><cell>MAP 1</cell><cell>MAP 2</cell></row><row><cell></cell><cell></cell><cell cols="2">(with Bg. Sp.) (without Bg Sp.)</cell></row><row><cell>MNB TSA Run 3</cell><cell>AUDIO &amp; METADATA</cell><cell>0,453</cell><cell>0,511</cell></row><row><cell>MNB TSA Run 1</cell><cell>AUDIO &amp; METADATA</cell><cell>0,451</cell><cell>0,509</cell></row><row><cell>MNB TSA Run 4</cell><cell>AUDIO &amp; METADATA</cell><cell>0,449</cell><cell>0,504</cell></row><row><cell>MNB TSA Run 2</cell><cell>&amp; METADATA</cell><cell>0,437</cell><cell>0,492</cell></row><row><cell>QMUL Run 3</cell><cell>AUDIO</cell><cell>0,355</cell><cell>0,429</cell></row><row><cell>QMUL Run 4</cell><cell>AUDIO</cell><cell>0,345</cell><cell>0,414</cell></row><row><cell>QMUL Run 2</cell><cell>AUDIO</cell><cell>0,325</cell><cell>0,389</cell></row><row><cell>QMUL Run 1</cell><cell>AUDIO</cell><cell>0,308</cell><cell>0,369</cell></row><row><cell>INRIA Zenith Run 2</cell><cell>AUDIO &amp; METADATA</cell><cell>0,317</cell><cell>0,365</cell></row><row><cell>INRIA Zenith Run 1</cell><cell>AUDIO</cell><cell>0,281</cell><cell>0,328</cell></row><row><cell>HLT Run 3</cell><cell>AUDIO &amp; METADATA</cell><cell>0,289</cell><cell>0,272</cell></row><row><cell>HLT Run 2</cell><cell>AUDIO &amp; METADATA</cell><cell>0,284</cell><cell>0,267</cell></row><row><cell>HLT Run 1</cell><cell>AUDIO &amp; METADATA</cell><cell>0,166</cell><cell>0,159</cell></row><row><cell>BirdSPec Run 2</cell><cell>AUDIO</cell><cell>0,119</cell><cell>0,144</cell></row><row><cell>Utrecht Univ. Run 1</cell><cell>AUDIO</cell><cell>0,123</cell><cell>0,14</cell></row><row><cell>Golem Run 1</cell><cell>AUDIO</cell><cell>0,105</cell><cell>0,129</cell></row><row><cell>Golem Run 2</cell><cell>AUDIO</cell><cell>0,104</cell><cell>0,128</cell></row><row><cell>BirdSPec Run 1</cell><cell>AUDIO</cell><cell>0,08</cell><cell>0,092</cell></row><row><cell>BirdSPec Run 4</cell><cell>AUDIO</cell><cell>0,074</cell><cell>0,089</cell></row><row><cell>Golem Run 3</cell><cell>AUDIO</cell><cell>0,074</cell><cell>0,089</cell></row><row><cell>BirdSPec Run 3</cell><cell>AUDIO</cell><cell>0,062</cell><cell>0,075</cell></row><row><cell>Yellow Jackets Run 1</cell><cell>AUDIO</cell><cell>0,003</cell><cell>0,003</cell></row><row><cell>Randall Run 1</cell><cell>AUDIO</cell><cell>0,002</cell><cell>0,002</cell></row><row><cell>SCS Run 1</cell><cell>AUDIO</cell><cell>0</cell><cell>0</cell></row><row><cell>SCS Run 2</cell><cell>AUDIO</cell><cell>0</cell><cell>0</cell></row><row><cell>SCS Run 3</cell><cell>AUDIO</cell><cell>0</cell><cell>0</cell></row><row><cell>Perfect Main &amp; Bg. Species</cell><cell>AUDIO</cell><cell>1</cell><cell>0,868</cell></row><row><cell>Perfect Main Species</cell><cell>AUDIO</cell><cell>0,784</cell><cell>1</cell></row><row><cell>Random Main Species</cell><cell>AUDIO</cell><cell>0,003</cell><cell>0,003</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_0" coords="2,144.73,634.88,69.41,7.86"><p>http://ebird.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_1" coords="2,144.73,645.84,114.96,7.86"><p>http://www.xeno-canto.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_2" coords="2,144.73,657.44,118.17,7.47"><p>http://sabiod.univ-tln.fr</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_3" coords="4,144.73,656.80,83.57,7.86"><p>http://marsyas.info/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_4" coords="7,144.73,656.80,167.10,7.86"><p>http://www.bioacoustics.us/ishmael.html</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,142.96,525.87,337.62,7.86;11,151.52,536.83,329.06,8.11;11,151.52,548.43,14.12,7.47" xml:id="b0">
	<monogr>
		<ptr target="http://sabiod.univ-tln.fr/NIPS4B2013_book.pdf" />
		<title level="m" coord="11,151.52,525.87,329.06,7.86;11,151.52,536.83,96.58,7.86">Proc. of Neural Information Processing Scaled for Bioacoustics: from Neurons to Big Data, joint to NIPS</title>
		<meeting>of Neural Information essing Scaled for Bioacoustics: from Neurons to Big Data, joint to NIPS</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,558.55,337.62,7.86;11,151.52,569.51,243.54,8.11" xml:id="b1">
	<monogr>
		<ptr target="http://sabiod.univ-tln.fr/ICML4B2013_book.pdf" />
		<title level="m" coord="11,151.52,558.55,329.05,7.86">Proc. of the first workshop on Machine Learning for Bioacoustics, joint to ICML</title>
		<meeting>of the first workshop on Machine Learning for Bioacoustics, joint to ICML</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,580.28,337.62,7.86;11,151.52,591.24,329.05,7.86;11,151.52,602.20,329.06,8.11;11,151.52,613.80,37.66,7.47" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,278.17,580.28,161.82,7.86">Overview of the nips4b bird classification</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Dufour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<ptr target="http://sabiod.univ-tln.fr/NIPS4B2013_book.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="11,459.19,580.28,21.39,7.86;11,151.52,591.24,329.05,7.86;11,151.52,602.20,75.73,7.86">Proc. of Neural Information Processing Scaled for Bioacoustics: from Neurons to Big Data, joint to NIPS</title>
		<meeting>of Neural Information essing Scaled for Bioacoustics: from Neurons to Big Data, joint to NIPS</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="12" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,623.92,337.62,7.86;11,151.52,634.88,329.05,7.86;11,151.52,645.84,329.05,7.86;11,151.52,656.80,135.69,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,265.30,634.88,215.28,7.86;11,151.52,645.84,184.94,7.86">Acoustic classification of multiple simultaneous bird species: A multi-instance multi-label approach</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Briggs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><forename type="middle">Z</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Raich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Hadley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Hadley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Betts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,343.94,645.84,136.64,7.86;11,151.52,656.80,66.07,7.86">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="page">4640</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,119.67,337.62,7.86;12,151.52,130.63,329.06,7.86;12,151.52,141.59,329.05,7.86;12,151.52,152.55,43.91,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,333.76,119.67,146.82,7.86;12,151.52,130.63,141.76,7.86">Sensor network for the monitoring of ecosystem: Bird species recognition</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Roe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,315.15,130.63,165.43,7.86;12,151.52,141.59,46.01,7.86;12,231.50,141.59,191.13,7.86">Intelligent Sensors, Sensor Networks and Information</title>
		<imprint>
			<date type="published" when="2007-12">2007. Dec 2007</date>
			<biblScope unit="page" from="293" to="298" />
		</imprint>
	</monogr>
	<note>ISSNIP 2007. 3rd International Conference on</note>
</biblStruct>

<biblStruct coords="12,142.96,163.52,337.62,7.86;12,151.52,174.48,329.06,7.86;12,151.52,185.44,226.95,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,242.26,163.52,219.46,7.86">Histograms of oriented gradients for human detection</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,151.52,174.48,173.53,7.86;12,358.29,174.48,47.44,7.86">Computer Vision and Pattern Recognition</title>
		<title level="s" coord="12,413.95,174.48,66.63,7.86;12,151.52,185.44,76.31,7.86">IEEE Computer Society Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
	<note>CVPR 2005</note>
</biblStruct>

<biblStruct coords="12,142.96,196.40,337.62,7.86;12,151.52,207.36,329.06,7.86;12,151.52,218.32,260.21,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,359.83,196.40,120.75,7.86;12,151.52,207.36,262.19,7.86">Clusterized mel filter cepstral coefficients and support vector machines for bird song idenfication</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Dufour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Artieres</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Giraudet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,433.68,207.36,46.89,7.86;12,151.52,218.32,174.52,7.86">Soundscape Semiotics -Localization and Categorization</title>
		<imprint>
			<publisher>Glotin</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,229.29,337.62,7.86;12,151.52,240.25,329.06,7.86;12,151.52,251.21,178.16,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,311.23,229.29,169.35,7.86;12,151.52,240.25,139.31,7.86">Opensmile: the munich versatile and fast open-source audio feature extractor</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wöllmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,310.19,240.25,170.39,7.86;12,151.52,251.21,57.24,7.86">Proceedings of the international conference on Multimedia</title>
		<meeting>the international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1459" to="1462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,262.17,337.62,7.86;12,151.52,273.13,329.06,7.86;12,151.52,284.09,329.06,8.11;12,151.52,295.69,136.51,7.47" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,273.36,262.17,207.21,7.86;12,151.52,273.13,220.94,7.86">Automated species identification: why not? Philosophical Transactions of the Royal Society of London</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">J</forename><surname>Gaston</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>O'neill</surname></persName>
		</author>
		<ptr target="http://rstb.royalsocietypublishing.org/content/359/1444/655.abstract" />
	</analytic>
	<monogr>
		<title level="j" coord="12,381.43,273.13,99.15,7.86;12,151.52,284.09,21.05,7.86">Series B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">359</biblScope>
			<biblScope unit="issue">1444</biblScope>
			<biblScope unit="page" from="655" to="667" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,306.02,337.96,7.86;12,151.52,316.98,329.06,7.86;12,151.52,327.93,285.53,8.11" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,239.68,306.02,223.23,7.86">Overview of the 1st int&apos;l challenge on bird classification</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sueur</surname></persName>
		</author>
		<ptr target="http://sabiod.univ-tln.fr/ICML4B2013_book.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="12,151.52,316.98,323.71,7.86">Proc. of the first workshop on Machine Learning for Bioacoustics, joint to ICML</title>
		<meeting>of the first workshop on Machine Learning for Bioacoustics, joint to ICML</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="17" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,338.90,337.96,7.86;12,151.52,349.86,329.06,7.86;12,151.52,360.82,70.42,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,292.12,338.90,188.45,7.86;12,151.52,349.86,185.00,7.86">Instance-based bird species identification with undiscriminant features pruning -lifeclef2014</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Champ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Buisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,359.04,349.86,121.54,7.86;12,151.52,360.82,41.76,7.86">Working notes of CLEF 2014 conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,371.79,337.96,7.86;12,151.52,382.75,329.06,7.86;12,151.52,393.70,40.24,7.86;12,134.77,404.67,345.81,7.86;12,151.52,415.63,154.31,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,276.15,382.75,204.43,7.86;12,151.52,393.70,40.24,7.86;12,134.77,404.67,7.85,7.86;12,205.97,404.67,217.69,7.86">Lifeclef 2014: multimedia life species identification challenges 13</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,446.51,404.67,34.06,7.86;12,151.52,415.63,125.64,7.86">Working notes of CLEF 2014 conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>Large-scale identification of birds in audio recordings</note>
</biblStruct>

<biblStruct coords="12,142.61,426.60,337.96,7.86;12,151.52,437.56,329.06,7.86;12,151.52,448.51,216.82,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,407.31,426.60,73.27,7.86;12,151.52,437.56,215.90,7.86">Contour matching for a fish recognition and migration-monitoring system</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Schoenberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Shiozawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,387.78,437.56,45.22,7.86">Optics East</title>
		<imprint>
			<publisher>International Society for Optics and Photonics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,459.48,337.96,7.86;12,151.52,470.44,329.05,7.86;12,151.52,481.40,70.42,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,416.58,459.48,63.99,7.86;12,151.52,470.44,190.62,7.86">Svm candidates and sparse representation for bird identification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Silvan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">V</forename><surname>Villarreal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Fuentes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Meza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,362.85,470.44,117.73,7.86;12,151.52,481.40,41.76,7.86">Working notes of CLEF 2014 conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,492.37,337.96,7.86;12,151.52,503.33,91.92,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,209.96,492.37,152.71,7.86">Overview of the lifeclef 2014 bird task</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Northcott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,383.85,492.37,96.73,7.86;12,151.52,503.33,63.25,7.86">Working notes of CLEF 2014 conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,514.29,337.96,7.86;12,151.52,525.25,248.70,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,342.26,514.29,138.31,7.86;12,151.52,525.25,36.77,7.86">Bird classification using ensemble classifiers</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>William Dennis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Huy Dat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,208.78,525.25,162.77,7.86">Working notes of CLEF 2014 conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,536.22,337.96,7.86;12,151.52,547.18,294.24,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="12,238.80,536.22,241.78,7.86;12,151.52,547.18,59.04,7.86">A survey of hierarchical classification across different application domains</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Silla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,218.12,547.18,158.01,7.86">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="31" to="72" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,558.14,337.96,7.86;12,151.52,569.10,262.90,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="12,268.09,558.14,212.48,7.86;12,151.52,569.10,50.44,7.86">Audio-only bird classification using unsupervised feature learning</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Stowell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,222.99,569.10,162.77,7.86">Working notes of CLEF 2014 conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,580.07,337.96,7.86;12,151.52,591.03,329.06,7.86;12,151.52,601.99,93.19,7.86" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="12,289.77,580.07,190.80,7.86;12,151.52,591.03,259.87,7.86">Automatic large-scale classification of bird sounds is strongly improved by unsupervised feature learning</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Stowell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.6524</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,142.61,612.96,337.96,7.86;12,151.52,623.91,207.33,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="12,394.41,612.96,86.17,7.86;12,151.52,623.91,60.09,7.86">A toolbox for animal call recognition</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Towsey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Planitz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nantes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wimmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Roe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,218.61,623.91,49.63,7.86">Bioacoustics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="125" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,634.88,337.96,7.86;12,151.52,645.84,329.05,7.86;12,151.52,656.80,216.88,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="12,371.29,634.88,109.29,7.86;12,151.52,645.84,272.72,7.86">Automated species recognition of antbirds in a mexican rainforest using hidden markov models</title>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">M</forename><surname>Trifa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Kirschel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">E</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">E</forename><surname>Vallejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,431.48,645.84,49.10,7.86;12,151.52,656.80,147.27,7.86">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page">2424</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,119.67,337.96,7.86;13,151.52,130.63,313.37,7.86" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="13,339.65,119.67,140.92,7.86;13,151.52,130.63,100.92,7.86">A deep neural network approach to the lifeclef 2014 bird task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Vincent Koops</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Van Balen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wiering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,273.44,130.63,162.77,7.86">Working notes of CLEF 2014 conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,141.59,337.96,7.86;13,151.52,152.55,114.16,7.86" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="13,234.13,141.59,178.56,7.86">Texture classification using texture spectrum</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">C</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,419.91,141.59,60.67,7.86;13,151.52,152.55,23.55,7.86">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="905" to="910" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,163.51,337.96,7.86;13,151.52,174.47,329.06,8.11;13,151.52,186.07,42.36,7.47" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="13,327.04,163.51,153.54,7.86">Taxonomy: Impediment or expedient?</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">D</forename><surname>Wheeler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">H</forename><surname>Raven</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">O</forename><surname>Wilson</surname></persName>
		</author>
		<ptr target="http://www.sciencemag.org/content/303/5656/285.short" />
	</analytic>
	<monogr>
		<title level="j" coord="13,151.52,174.47,29.19,7.86">Science</title>
		<imprint>
			<biblScope unit="volume">303</biblScope>
			<biblScope unit="issue">5656</biblScope>
			<biblScope unit="page">285</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
