<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,167.28,116.95,280.79,12.62;1,154.32,134.89,306.71,12.62;1,270.28,152.82,74.79,12.62">Fine-Grained Plant Classification Using Convolutional Neural Networks for Feature Extraction</title>
				<funder ref="#_6Pu4QSf">
					<orgName type="full">Department of Agriculture Fisheries and Forestry</orgName>
					<orgName type="abbreviated">DAFF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,166.00,191.16,71.10,8.74"><forename type="first">Niko</forename><surname>Sünderhauf</surname></persName>
							<email>niko.suenderhauf@qut.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Agricultural Robotics Program</orgName>
								<orgName type="institution">Queensland University of Technology</orgName>
								<address>
									<addrLine>2 George Street</addrLine>
									<postCode>QLD, 4001</postCode>
									<settlement>Brisbane</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,245.18,191.16,57.96,8.74"><forename type="first">Chris</forename><surname>Mccool</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Agricultural Robotics Program</orgName>
								<orgName type="institution">Queensland University of Technology</orgName>
								<address>
									<addrLine>2 George Street</addrLine>
									<postCode>QLD, 4001</postCode>
									<settlement>Brisbane</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,311.68,191.16,52.09,8.74"><forename type="first">Ben</forename><surname>Upcroft</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Agricultural Robotics Program</orgName>
								<orgName type="institution">Queensland University of Technology</orgName>
								<address>
									<addrLine>2 George Street</addrLine>
									<postCode>QLD, 4001</postCode>
									<settlement>Brisbane</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,391.00,191.16,58.36,8.74"><forename type="first">Tristan</forename><surname>Perez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Agricultural Robotics Program</orgName>
								<orgName type="institution">Queensland University of Technology</orgName>
								<address>
									<addrLine>2 George Street</addrLine>
									<postCode>QLD, 4001</postCode>
									<settlement>Brisbane</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,167.28,116.95,280.79,12.62;1,154.32,134.89,306.71,12.62;1,270.28,152.82,74.79,12.62">Fine-Grained Plant Classification Using Convolutional Neural Networks for Feature Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F8E3B8C8448E5A272EA63BCDCE8F6DB1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>convolutional neural network</term>
					<term>extremely random forest</term>
					<term>plant classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an overview of the QUT plant classification system submitted to LifeCLEF 2014. This system uses generic features extracted from a convolutional neural network previously used to perform general object classification. We examine the effectiveness of these features to perform plant classification when used in combination with an extremely randomised forest. Using this system, with minimal tuning, we obtained relatively good results with a score of 0.249 on the test set of LifeCLEF 2014.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Future food security presents a serious challenge. To sustain the projected world population of over 9 billion in 2050, the current worldwide production of food will have to almost double. This is a significant challenge given that the land allocation dedicated to agriculture has already peaked in most countries. In addition, agriculture requires a significant amount of energy and water, and it is a large contributor to green-house-gas emissions. To overcome these problems requires alternative approaches such as the use of new crop varieties with increased yield and robustness to climate change, the adoption of policies leading to sustainable practices, and the development of new technologies to increase the efficiency of farms.</p><p>Robot technology in farms will play an important role in the near future in several applications. For both infield and crop management, robots could contribute to the efficient use of energy, herbicides, pesticides, water, and fertiliser by measuring plant growth and detecting weeds/pests. In harvesting of horticultural produce, robots could contribute to increased yield and quality by automating the picking and grading processes. To do this requires sophisticated algorithms that could identify plants in their various states of growth.</p><p>The Plant Identification Task <ref type="bibr" coords="2,282.98,119.99,10.52,8.74" target="#b4">[5]</ref> of the LifeCLEF challenge <ref type="bibr" coords="2,416.05,119.99,10.52,8.74" target="#b6">[7]</ref> is an established benchmark for fine-grained plant classification. The task asks the participating teams to correctly identify the images taken of 500 different herbs, trees, and fern species from France. The provided training dataset consists of over 47,000 individual images that are organized into observations. Each of these observations can contain multiple images from the same plant, but from different parts of it. These different parts are referred to as content categories and include: Branch, Stem, Leaf, LeafScan, Fruit, Flower and Entire.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our Approach</head><p>We investigate the potential for generic features obtained from a well trained convolutional neural network (CNN) to perform the task of plant classification. This is a fine-grained image classification problem which has previously been addressed by deriving hand-crafted features. By contrast, our proposed system uses features from a CNN that was initially trained for general object classification <ref type="bibr" coords="2,155.59,325.19,15.50,8.74" target="#b11">[11]</ref> using millions of images from the ImageNet dataset. To classify these generic pre-trained features we make use of an extremely randomised forest. We briefly describe our features and classifier below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Convolutional Neural Networks as Generic Feature Detectors</head><p>Convolutional neural networks (CNNs) were proposed in 1989 by LeCun et al. <ref type="bibr" coords="2,470.09,404.87,10.52,8.74" target="#b9">[9]</ref> to recognize hand-written digits. CNNs learn a sparser connection between regions of an image and the NNs by imposing spatial dependencies; this can reduce complexity. However, the broad applicability of CNNs has only recently shown promise most likely due to the availability of large datasets, growth in computational power, availability of GPUs and efficient algorithms such as rectified linear units <ref type="bibr" coords="2,187.44,476.60,10.51,8.74" target="#b0">[1,</ref><ref type="bibr" coords="2,199.24,476.60,7.75,8.74" target="#b5">6]</ref> which have been used to train these large networks.</p><p>Several research groups have explored the potential of these large CNNs to outperform more classical approaches to object recognition or detection that are based on hand-crafted features <ref type="bibr" coords="2,270.45,513.06,12.40,8.74" target="#b1">[2,</ref><ref type="bibr" coords="2,282.85,513.06,8.26,8.74" target="#b3">4,</ref><ref type="bibr" coords="2,291.12,513.06,8.26,8.74" target="#b7">8,</ref><ref type="bibr" coords="2,299.38,513.06,12.40,8.74" target="#b10">10,</ref><ref type="bibr" coords="2,311.78,513.06,12.40,8.74" target="#b11">11]</ref>. The CNN systems applied in these large-scale object recognition or detection tasks consist of a feature extractor (the actual CNN) followed by a classifier or regressor. Razavian et al. <ref type="bibr" coords="2,410.82,536.97,15.50,8.74" target="#b10">[10]</ref> showed that combining CNN features with a simple classifier such as a linear SVM is highly competitive or even superior to classical approaches for a variety of recognition and detection tasks. We therefore follow the approach of Razavian et al. <ref type="bibr" coords="2,465.11,572.83,15.50,8.74" target="#b10">[10]</ref> and apply a pre-trained CNN as a generic feature extractor to the images of the LifeCLEF Plant Task.</p><p>In our approach, we use the pre-trained Overfeat <ref type="bibr" coords="2,376.18,609.29,15.50,8.74" target="#b11">[11]</ref> features which is a CNN system. For feature extraction, we examine the effectiveness of using either the first or second fully connected layers -referred to as Layer 17 and Layer 19 respectively. To use the Overfeat features, we downsample the images so that the smallest dimension is 231 pixels wide or high respectively, this is because the Overfeat network operates on a region of size 231 × 231. The extracted features are vectors of length 3, 072 or 4, 096 for Layer 17 and Layer 19 respectively. Overfeat will extract several feature vectors per image in a sliding window fashion unless the images are square (231 × 231). These feature vectors are then fed into an ensemble of extremely randomized trees that performs the actual classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Extremely Randomized Trees Classifier</head><p>An extremely randomized tree classifier (extratree) is a tree-based ensemble method for supervised classification. It is conceptually similar to random forest classifiers, but takes the idea of randomness one step further. In contrast to decision trees that are designed offline and derived from expert knowledge, an extremely randomized tree classifier learns the layout of its ensemble of trees from training data. The classifier output is a probability distribution over all classes, in our case this would over all 500 possible species in LifeCLEF. We refer the reader to the literature <ref type="bibr" coords="3,235.14,310.83,10.52,8.74" target="#b2">[3]</ref> for a further discussion of extremely randomized trees.</p><p>We train a separate classifier for each of the content categories in the Life-CLEF dataset. Thus for each category, Branch, Leaf or LeafScan, we have a separate extremely randomized tree. To handle multiple samples from each image the classification results of the extratrees are combined to provide just one prediction per observation. This is achieved by averaging the output (probability distribution) for all features from an image and results in one probability distribution for each image.</p><p>PlantCLEF introduces a further difficulty in that an observation can have multiple images or even multiple content categories available. To combine the information from multiple images, or even multiple content categories, we treat the result of each image as a likelihood score and sum them into a single distribution of prediction scores over the 500 different classes for each observation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head><p>To derive the hyper parameters for our system, we performed cross validation. The training dataset was split into two tests: the system is trained using 95% of the training observations and then evaluated on the remaining 5%. We note that our choice for forming the cross-validation dataset is overly simplified as we uniformly sample across observations and do not uniformly sample over classes (species) and observations; this means that we do not enforce an equal ratio between classes and observations of the training and the evaluation sets. Finally, we only used observations with a user-provided quality score of 3 or above in the training and validation stage.</p><p>The parameters of the ensemble classifier are tuned by sampling the parameter space. The resultant values are listed in Table <ref type="table" coords="3,351.46,627.19,3.87,8.74" target="#tab_0">1</ref>. Our system is implemented in Python and uses the extratrees implementation of scikit-learn </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>A total of ten teams participated in the challenge and we ranked 4th in the team ranking. In Figure <ref type="figure" coords="4,220.51,273.32,3.87,8.74" target="#fig_0">1</ref>, we present the results using the LifeCLEF score metric where it can be seen that our approach scored 0.249; to facilitate interpretation of this plot we associated the runs of the top 4 teams with a different color. This is slightly worse than BME TMIT systems and considerably better than the FINKI systems. To understand better the performance of our system, we examined the recognition accuracy for each content category. In Figure <ref type="figure" coords="4,357.86,657.11,3.87,8.74" target="#fig_1">2</ref>, we present the recognition accuracy for our system for each content category and also for using different layers of the neural network (Layers 17 and 19). We can see that our system is most accurate for the LeafScan category and achieves a rank-1 identification rate of more than 50%. The second best category is Flower with a rank-1 identification rate of almost 25%. After this, our rank-1 identification rate for the categories falls below 20% and performs worst on the Branch category. We believe that the considerable performance difference on the LeafScan category is due to the fact that these images contain a single leaf image which is usually well centered and has a homogeneous background; this greatly simplifies the feature extraction stage and consequently classification. By contrast, other categories such as Fruit, Branch and Leaf, have highly varying backgrounds and the object of interest is rarely well centered or at a consistent scale. Finally, we note that despite the considerable performance difference between LeafScan and the other categories, our overall system performance is acceptable. We believe this is because most of the images in the dataset appear to be LeafScan images. Comparing the performance of the two different extracted feature sets, we can see in Figure <ref type="figure" coords="5,192.65,585.38,4.98,8.74" target="#fig_1">2</ref> that Layer 19 provides the best overall score. Layer 17 is the first fully connected layer and Layer 19 is the second fully connected layer. Examining the performance on a per category basis, we can see that Layer 17 and 19 have similar performance except for Flower and Fruit where Layer 19 outperforms Layer 17. By contrast, Layer 17 provides slightly better performance for the Stem category. We want to point out that Razavian et al. <ref type="bibr" coords="5,384.11,645.16,15.50,8.74" target="#b10">[10]</ref> used the first fully connected layer (Layer 17) in their work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>The initial results of our plant classification system using CNNs provides competitive performance. We have found that using Layer 19, rather than Layer 17, provides superior performance for the task of plant classification. Nevertheless, some issues are worth considering further.</p><p>One major issue is that we need to consider how to perform localisation of the object of interest within the image. We believe this is an important factor that degrades performance. One possible way approach this problem is to search the image for the most salient parts in the image. Also, we should examine the properties of the CNN features, and how they could be re-trained to deal with the specific task of plant classification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,134.77,588.73,345.81,7.89;4,134.77,599.71,345.81,7.86;4,134.77,610.67,345.82,7.86;4,134.77,352.79,345.84,211.20"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The results of the LifeCLEF Plant Task 2014. Image adapted from the organizers' website. Our run (QUT) is marked in red, the runs committed by the top 3 teams are color coded in blue (IBM), green (PlantNet) and yellow (BME TMIT) respectively.</figDesc><graphic coords="4,134.77,352.79,345.84,211.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,134.77,503.30,345.81,7.89;5,134.77,514.29,345.81,7.86;5,134.77,525.25,345.81,7.86;5,134.77,536.21,48.97,7.86;5,203.93,322.25,207.48,156.32"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Recognition accuracies (rank-1 identification rate) for the different content categories present in the dataset. The plot compares the performances of features extracted from the first and second fully connected layer presented in blue and red respectively.</figDesc><graphic coords="5,203.93,322.25,207.48,156.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,414.94,637.57,7.24,10.31"><head>Table 1 :</head><label>1</label><figDesc>1 . A table outlining the final system parameters used for the extratrees that were used.</figDesc><table coords="4,237.72,150.05,136.84,56.70"><row><cell>Parameter</cell><cell>Value</cell></row><row><cell>size of ensemble</cell><cell>65 trees</cell></row><row><cell>used feature elements</cell><cell>64</cell></row><row><cell>tree depth</cell><cell>15</cell></row><row><cell cols="2">information gain measure entropy</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,144.73,658.44,113.46,7.47"><p>http://scikit-learn.org/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work has been supported by the <rs type="funder">Department of Agriculture Fisheries and Forestry (DAFF)</rs> of the Queensland government through the <rs type="programName">Agricultural Robotics Program</rs> <rs type="institution">at QUT</rs>. We would also like to thank <rs type="person">David Hall</rs> and <rs type="person">Dr. Feras Dayoub</rs> for the fruitful conversations.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_6Pu4QSf">
					<orgName type="program" subtype="full">Agricultural Robotics Program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,142.96,387.52,337.62,7.86;6,151.52,398.48,329.06,7.86;6,151.52,409.44,329.07,7.86;6,151.52,420.40,95.86,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,388.82,387.52,91.76,7.86;6,151.52,398.48,236.31,7.86">Improving deep neural networks for lvcsr using rectified linear units and dropout</title>
		<author>
			<persName coords=""><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,410.54,398.48,70.05,7.86;6,151.52,409.44,299.48,7.86">Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="8609" to="8613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,430.83,337.62,7.86;6,151.52,441.79,329.05,7.86;6,151.52,452.75,204.66,7.86" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="6,229.94,441.79,250.64,7.86;6,151.52,452.75,43.20,7.86">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName coords=""><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.1531</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="6,142.96,463.18,337.62,7.86;6,151.52,474.14,143.76,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,364.04,463.18,112.94,7.86">Extremely randomized trees</title>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><surname>Geurts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Damien</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Louis</forename><surname>Wehenkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,151.52,474.14,68.24,7.86">Machine learning</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="42" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,484.57,337.62,7.86;6,151.52,495.53,329.06,7.86;6,151.52,506.49,92.39,7.86" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="6,430.17,484.57,50.41,7.86;6,151.52,495.53,265.40,7.86">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName coords=""><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.2524</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="6,142.96,516.92,337.62,7.86;6,151.52,527.87,329.07,7.86;6,151.52,538.83,104.13,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,292.81,527.87,145.11,7.86">Lifeclef plant identification task 2014</title>
		<author>
			<persName coords=""><forename type="first">Hervé</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexis</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jean-François</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Barthélémy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nozha</forename><surname>Boujemaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,455.98,527.87,24.61,7.86;6,151.52,538.83,75.79,7.86">CLEF working notes 2014</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,549.26,337.62,7.86;6,151.52,560.22,329.05,7.86;6,151.52,571.18,227.53,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="6,242.86,560.22,237.71,7.86;6,151.52,571.18,66.08,7.86">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruslan</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="6,142.96,581.61,337.62,7.86;6,151.52,592.57,329.05,7.86;6,151.52,603.53,329.07,7.86;6,151.52,614.49,45.44,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,451.77,592.57,28.80,7.86;6,151.52,603.53,218.30,7.86">Lifeclef 2014: multimedia life species identification challenges</title>
		<author>
			<persName coords=""><forename type="first">Alexis</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hervé</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hervé</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Concetto</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andreas</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Willem-Pier</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bob</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,393.88,603.53,86.70,7.86;6,151.52,614.49,17.10,7.86">Proceedings of CLEF 2014</title>
		<meeting>CLEF 2014</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,624.92,337.62,7.86;6,151.52,635.88,329.06,7.86;6,151.52,646.84,329.06,7.86;6,151.52,657.80,84.21,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,389.28,624.92,91.30,7.86;6,151.52,635.88,160.57,7.86">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,273.67,646.84,206.91,7.86">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,238.80,657.80,120.76,7.86" xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Curran Associates</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Inc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,120.67,337.62,7.86;7,151.52,131.63,329.06,7.86;7,151.52,142.59,301.94,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,367.34,131.63,113.24,7.86;7,151.52,142.59,131.53,7.86">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName coords=""><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bernhard</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donnie</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wayne</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,291.07,142.59,77.19,7.86">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,153.55,337.96,7.86;7,151.52,164.51,329.07,7.86;7,151.52,175.47,92.39,7.86" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Ali</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hossein</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefan</forename><surname>Carlsson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1403.6382</idno>
		<title level="m" coord="7,151.52,164.51,261.93,7.86">Cnn features off-the-shelf: an astounding baseline for recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,142.61,186.42,337.96,7.86;7,151.52,197.38,329.05,7.86;7,151.52,208.34,252.32,7.86" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="7,212.50,197.38,268.08,7.86;7,151.52,208.34,90.54,7.86">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
