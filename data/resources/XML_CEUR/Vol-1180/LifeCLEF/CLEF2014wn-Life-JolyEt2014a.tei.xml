<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,164.42,115.96,286.52,12.62">LifeCLEF Plant Identification Task 2014</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,143.70,153.63,56.55,8.74"><forename type="first">Hervé</forename><surname>Goëau</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inria ZENITH team</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,210.81,153.63,48.08,8.74"><forename type="first">Alexis</forename><surname>Joly</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inria ZENITH team</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">LIRMM</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,275.77,153.63,60.95,8.74"><forename type="first">Pierre</forename><surname>Bonnet</surname></persName>
							<email>pierre.bonnet@cirad.fr</email>
							<affiliation key="aff2">
								<orgName type="laboratory">CIRAD</orgName>
								<orgName type="institution">UMR AMAP</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,347.27,153.63,58.68,8.74"><forename type="first">Souheil</forename><surname>Selmi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inria ZENITH team</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,416.50,153.63,55.16,8.74;1,192.27,165.58,30.17,8.74"><forename type="first">Jean-Franois</forename><surname>Molino</surname></persName>
							<email>wjean-francois.molino@ird.fr</email>
							<affiliation key="aff3">
								<orgName type="laboratory">IRD</orgName>
								<orgName type="institution">UMR AMAP</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,232.99,165.58,81.67,8.74"><forename type="first">Daniel</forename><surname>Barthélémy</surname></persName>
							<email>daniel.barthelemy@cirad.fr</email>
							<affiliation key="aff4">
								<orgName type="department">BIOS Direction and INRA</orgName>
								<orgName type="laboratory">CIRAD</orgName>
								<orgName type="institution">UMR AMAP</orgName>
								<address>
									<postCode>F-34398</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,344.59,165.58,74.03,8.74"><forename type="first">Nozha</forename><surname>Boujemaa</surname></persName>
							<email>nozha.boujemaa@inria.fr</email>
							<affiliation key="aff5">
								<orgName type="department">Direction of Saclay Center</orgName>
								<orgName type="institution">INRIA</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,164.42,115.96,286.52,12.62">LifeCLEF Plant Identification Task 2014</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7E255D171D1F58D24E66FE7721A3A344</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>LifeCLEF</term>
					<term>plant</term>
					<term>leaves</term>
					<term>leaf</term>
					<term>flower</term>
					<term>fruit</term>
					<term>bark</term>
					<term>stem</term>
					<term>branch</term>
					<term>species</term>
					<term>retrieval</term>
					<term>images</term>
					<term>collection</term>
					<term>identification</term>
					<term>fine-grained classification</term>
					<term>evaluation</term>
					<term>benchmark</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The LifeCLEFs plant identification task provides a testbed for a system-oriented evaluation of plant identification about 500 species trees and herbaceous plants. Seven types of image content are considered: scan and scan-like pictures of leaf, and 6 kinds of detailed views with unconstrained conditions, directly photographed on the plant: flower, fruit, stem &amp; bark, branch, leaf and entire view. The main originality of this data is that it was specifically built through a citizen sciences initiative conducted by Tela Botanica, a French social network of amateur and expert botanists. This makes the task closer to the conditions of a realworld application. This overview presents more precisely the resources and assessments of task, summarizes the retrieval approaches employed by the participating groups, and provides an analysis of the main evaluation results. With a total of ten groups from six countries and with a total of twenty seven submitted runs, involving distinct and original methods, this fourth year task confirms Image &amp; Multimedia Retrieval community interest for biodiversity and botany, and highlights further challenging studies in plant identification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Content-based image retrieval approaches are nowadays considered to be one of the most promising solution to help bridge the botanical taxonomic gap, as discussed in <ref type="bibr" coords="1,187.61,584.39,15.50,8.74" target="#b9">[10]</ref> or <ref type="bibr" coords="1,216.51,584.39,15.50,8.74" target="#b21">[22]</ref> for instance. We therefore see an increasing interest in this trans-disciplinary challenge in the multimedia community (e.g. in <ref type="bibr" coords="1,423.98,596.34,14.61,8.74" target="#b11">[12]</ref>, <ref type="bibr" coords="1,445.65,596.34,9.96,8.74" target="#b4">[5]</ref>, <ref type="bibr" coords="1,462.34,596.34,14.61,8.74" target="#b19">[20]</ref>, <ref type="bibr" coords="1,134.77,608.30,14.61,8.74" target="#b22">[23]</ref>, <ref type="bibr" coords="1,156.67,608.30,14.61,8.74" target="#b13">[14]</ref>, <ref type="bibr" coords="1,178.57,608.30,10.30,8.74" target="#b1">[2]</ref>). Beyond the raw identification performances achievable by stateof-the-art computer vision algorithms, the visual search approach offers much more efficient and interactive ways of browsing large floras than standard field guides or online web catalogs. Smartphone applications relying on such imagebased identification services are particularly promising for setting-up massive ecological monitoring systems, involving hundreds of thousands of contributors at a very low cost.</p><p>Noticeable progress in this way was achieved by several project and apps like LeafSnap<ref type="foot" coords="2,175.03,153.48,3.97,6.12" target="#foot_0">7</ref>  <ref type="bibr" coords="2,181.82,155.05,14.61,8.74" target="#b21">[22]</ref>, PlantNet<ref type="foot" coords="2,241.85,153.48,3.97,6.12" target="#foot_1">8</ref> ,<ref type="foot" coords="2,249.09,153.48,3.97,6.12" target="#foot_2">9</ref>  <ref type="bibr" coords="2,255.88,155.05,14.61,8.74" target="#b15">[16]</ref>. But as promising as these applications are, their performances are however still far from the requirements of a real-world socialbased ecological surveillance scenario. Allowing the mass of citizens to produce accurate plant observations requires to equip them with much more accurate identification tools. Measuring and boosting the performances of content-based identification tools is therefore crucial. This was precisely the goal of the Im-ageCLEF <ref type="foot" coords="2,175.86,225.21,7.94,6.12" target="#foot_3">10</ref> plant identification task organized since 2011 in the context of the worldwide evaluation forum CLEF <ref type="foot" coords="2,285.78,237.17,7.94,6.12" target="#foot_4">11</ref> (see <ref type="bibr" coords="2,314.20,238.74,14.61,8.74" target="#b9">[10]</ref>, <ref type="bibr" coords="2,335.79,238.74,15.50,8.74" target="#b10">[11]</ref> and <ref type="bibr" coords="2,373.98,238.74,15.50,8.74" target="#b16">[17]</ref> for more details).</p><p>Contrary to previous evaluations reported in the literature, the key objective was since the first campaign to build a realistic task closer to real-world conditions (different users, cameras, areas, periods of the year, individual plants, etc.). This was initially achieved through a citizen science initiative initiated 4 years ago in the context of the Pl@ntNet project in order to boost the image production of Tela Botanica social network. The evaluation data was enriched each year with the new contributions and progressively diversified with other input feeds (annotation and cleaning of older data, contributions made through Pl@ntNet mobile applications). The plant task of LifeCLEF 2014 is directly in the continuity of this effort. Main novelties compared to the last years are the following:</p><p>an explicit multi-image query scenario, the supply of user ratings on image quality in the meta-data, a new type of view called "Branch" additionally to the 6 previous ones, and basically more species: 500 which is an important step towards covering the entire flora of a given region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset</head><p>More precisely, PlantCLEF 2014 dataset is composed of 60,962 pictures belonging to 19,504 observations of 500 species of trees, herbs and ferns living in a European region centered around France. This data was collected by 1608 distinct contributors. Each picture belongs to one and only one of the 7 types of view reported in the meta-data (entire plant, fruit, leaf, flower, stem, branch, leaf scan) and is associated with a single plant observation identifier allowing to link it with the other pictures of the same individual plant (observed the same day by the same person). It is noticeable that most image-based identification methods and evaluation data proposed in the past were so far based on leaf images (e.g. in <ref type="bibr" coords="3,204.08,118.99,14.61,8.74" target="#b21">[22]</ref>, <ref type="bibr" coords="3,226.62,118.99,9.96,8.74" target="#b2">[3]</ref>, <ref type="bibr" coords="3,244.17,118.99,10.52,8.74" target="#b4">[5]</ref> or in the more recent methods evaluated in <ref type="bibr" coords="3,458.47,118.99,14.76,8.74" target="#b10">[11]</ref>). Only few of them were focused on flower's images as in <ref type="bibr" coords="3,374.33,130.95,15.50,8.74" target="#b24">[25]</ref> or <ref type="bibr" coords="3,404.58,130.95,9.96,8.74" target="#b0">[1]</ref>. Leaves are far from being the only discriminant visual key between species but, due to their shape and size, they have the advantage to be easily observed, captured and described. More diverse parts of the plants however have to be considered for accurate identification. An originality of PlantCLEF dataset is that its social nature makes it closer to the conditions of a real-world identification scenario: (i) images of the same species are coming from distinct plants living in distinct areas (ii) pictures are taken by different users that might not used the same protocol to acquire the images (iii) pictures are taken at different periods in the year. Each image of the dataset is associated with contextual meta-data (author, date, locality name, plant id) and social data (user ratings on image quality, collaboratively validated taxon names, vernacular names) provided in a structured xml file. The gps geolocalization and the device settings are available only for some of the images. Table <ref type="table" coords="3,162.30,298.32,4.98,8.74" target="#tab_0">1</ref> gives some examples of pictures with decreasing averaged users ratings for the different types of views. Note that the users of the specialized social network creating these ratings (Tela Botanica) are explicitly asked to rate the images according to their plant identification ability and their accordance to the pre-defined acquisition protocol for each view type. This is not an aesthetic or general interest judgement as in most social image sharing sites.</p><p>To sum up each image is associated with the following meta-data:</p><p>-ObservationId: the plant observation ID from which several pictures can be associated -FileName -MediaId: id of the image -View Content: Branch or Entire or Flower or Fruit or Leaf or LeafScan or Stem -ClassId: the class number ID that must be used as ground-truth. It is a numerical taxonomical number used by Tela Botanica -Species the species names (containing 3 parts: the Genus name, the Species name, the author(s) who discovered or revised the name of the species) -Genus: the name of the Genus, one level above the Species in the taxonomical hierarchy used by Tela Botanica -Family: the name of the Family, two levels above the Species in the taxonomical hierarchy used by Tela Botanica -Date: (if available) the date when the plant was observed, -Vote: the (round up) average of the user ratings on image quality -Location: (if available) locality name, most of the time a town -Latitude &amp; Longitude: (if available) the GPS coordinates of the observation in the EXIF metadata, or, if no GPS information were found in the EXIF, the GPS coordinates of the locality where the plant was observed (only for the towns of metropolitan France) -Author: name of the author of the picture, -YearInCLEF: ImageCLEF2011, ImageCLEF2012, ImageCLEF2013, Plant-CLEF2014 when the image was integrated in the benchmark </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task Description</head><p>The task was evaluated as a plant species retrieval task based on multi-image plant observations queries. The goal was to retrieve the correct plant species among the top results of a ranked list of species returned by the evaluated system. Contrary to previous plant identification benchmarks, queries are not defined as single images but as plant observations, meaning a set of one to several images depicting the same individual plant, observed by the same person, the same day, with the same device. Each image of a query observation is associated with a single view type (entire plant, branch, leaf, fruit, flower, stem or leaf scan) and with contextual meta-data (data, location, author). Each participating group was allowed to submit up to 4 runs built from different methods. Any human assistance in the processing of the test queries has therefore to be signaled in the submitted runs meta-data.</p><p>In practice, the whole PlantCLEF dataset was split in two parts, one for training (and/or indexing) and one for testing. All observations with pictures used in the previous plant identification tasks were directly integrated in the training dataset. Then for the new observations and pictures, in order to guarantee that most of the time each species contained more images in the training dataset than in the test dataset, we used a constrained random rule for putting with priority observations with more distinct organs and views in the training dataset. The test set was built by choosing 1/2 of the observations of each species with this constrained random rule, whereas the remaining observations were kept in the reference training set. Thus, 1/3 of the pictures are in the test dataset (see Table <ref type="table" coords="5,134.77,262.46,4.98,8.74" target="#tab_0">1</ref> for more detailed stats). The xml files containing the meta-data of the query images were purged so as to erase the taxon names (the ground truth) and the image quality ratings (that would not be available at query stage in a real-world mobile application). Meta-data of the observations in the training set are kept unaltered. The metric used to evaluate the submitted runs is a score related to the rank of the correct species in the returned list. Each query observation is attributed with a score between 0 and 1 reflecting equal to the inverse of the rank of the correct species (equal to 1 if the correct species is the top-1 decreasing quickly while the rank of the correct species increases). An average score is then computed across all plant observation queries. A simple mean on all plant observation test would however introduce some bias. Indeed, we remind that the PlantCLEF dataset was built in a collaborative manner. So that few contributors might have provided much more observations and pictures than many other contributors who provided few. Since we want to evaluate the ability of a system to provide the correct answers to all users, we rather measure the mean of the average classification rate per author. Finally, our primary metric was defined as the following average classification score S:</p><formula xml:id="formula_0" coords="6,259.47,136.59,221.13,30.31">S = 1 U U u=1 1 P u Pu p=1 s u,p<label>(1)</label></formula><p>where U : number of users (who have at least one image in the test data), P u : number of individual plants observed by the u-th user, s u,p : the score between 1 and 0 equals to the inverse of the rank of the correct species (for the p-th plant observed by the u-th user).</p><p>A secondary metric was used to evaluate complementary (but not mandatory) runs providing species prediction at the image level. Each test image is attributed with a score between 0 and 1: of 1 if the 1st returned species is correct and decrease quickly while the rank of the correct species increases. An average score is then be computed on all test images. Following the same motivations expressed above, a simple mean on all test images would however introduce some bias. Some authors sometimes provided many pictures of the same individual plant (to enrich training data with less efforts). Since we want to evaluate the ability of a system to provide the correct answer based on a single plant observation, we also have to average the classification rate on each individual plant. Finally, our secondary metric is defined as the following average classification score S:</p><formula xml:id="formula_1" coords="6,234.63,364.55,245.97,31.28">S = 1 U U u=1 1 P u Pu p=1 1 N u,p Nu,p n=1 s u,p,n<label>(2)</label></formula><p>where U is the number of users, P u the number of individual plants observed by the u-th user, N u,p the number of pictures of the p-th plant observation of the u-th user, s u,p,n is the score between 1 and 0 equals to the inverse of the rank of the correct species.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Participants and methods</head><p>74 research groups worldwide registered to the plant task (31 of them being exclusively registered to the plant task). Among this large raw audience, 10 research groups did cross the finish line by submitting runs (from 1 to 4 depending on the teams). 6 teams submitted 14 complementary runs on images. Participants were mainly academics, specialized in computer vision, machine learning and multimedia information retrieval. We list below the participants and give a brief overview of the techniques they used in their runs. We remind here that LifeCLEF benchmark is a system-oriented evaluation and not a deep or fine evaluation of the underlying algorithms. Readers interested by the scientific and technical details of any of these methods should refer to the LifeCLEF 2014 working notes of each participant (referenced below): BME TMIT (3 runs), <ref type="bibr" coords="6,253.54,632.18,16.80,8.77" target="#b28">[29]</ref>, Hungary. These participants used Gaussian Mixture Model (GMM) based Fisher vector (FV) representation from a dense-SIFT features extraction. Principal Component Analysis (PCA) is first used on the SIFT vectors in order to reduce the dimension from 128 to 80. Then a Fisher Vector representation based on a concise codebook of 256 visual words is used for embedding the PCA-SIFT descriptors in a single high level representation for each image. The chosen classifier was the C-Support Vector Classification (C-SVC) algorithm with Radial Basis Function kernel. The two hyperparameters (C from C-SVC and from RBF kernel) were optimized by a grid search with two-dimensional grid. The algorithm was trained with the training image set, and then validated on the validation set, while the hyperparameters were different in each iteration. In order to obtain a final species list for all test images from a same observation, since the C-SVC classifier calculates continuous reliability value for each class at each image, they used a combined classifier using a weighted average of reliability values. FINKI (3 runs) <ref type="bibr" coords="7,213.19,278.08,11.46,8.77" target="#b6">[7]</ref>, Macedonia. For the LeafScan category these participants used the multiscale triangular shape descriptor <ref type="bibr" coords="7,343.18,290.07,14.61,8.74" target="#b23">[24]</ref>. For the other pictures, opponent SIFT were extracted around 20 000 points of interest obtained using Harris-Laplace detector. In addition, a rhomboid-shaped mask was applied to the input image to minimize the effect of the cluttered background and to reduce the number of points as in <ref type="bibr" coords="7,251.66,337.89,9.96,8.74" target="#b3">[4]</ref>. Then an approximate k-means (AKM) algorithm is used for clustering these descriptors and for producing a large number of visual words (approximately 200K). Then, they used for each test image a Bagof-visual-Word representation and used a classical TF-IDF measure in order to compute a training image list. Finally, in order to combine the result lists from several test images belonging to a same plant observation, two fusion operators as experimented: Min rank (run 2), a "Probability fusion" (run 3). The combination of the two approaches (run 1) gave their best results. Then a 1-nn rule is used at the end for producing a list of ranked species. <ref type="bibr" coords="7,198.34,461.11,16.80,8.77" target="#b14">[15]</ref>, France. These participants used a Bag-of-Word framework starting from SIFT and Opponent Color SIFT features extracted from 1000 points localised with the SIFT detector. Several visual dictionaries were constructed with a K-means clustering algorithms: K=4000 words for the LeafScan category, K=2000 for the Leaf, and K=500 for the other types of views. Then, 3500 (7 image categories x 500 species) SVM binary classifiers were trained in order to give for each test image a list of species with a decreasing normalized score of confidence. Pictures from a same test plant observation are gathered according to two rules: sum (Run 1) and max (Run 2) of confidence normalized scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I3S (2 runs)</head><p>IBM AU (4 runs) <ref type="bibr" coords="7,230.43,596.31,11.46,8.77" target="#b5">[6]</ref>, Australia. These participants tested and combined distinct approaches. Run1 uses a deep Convolutional Neural Network. Their CNN has around 60 million parameters and is composed of 5 convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final softmax layer. They followed the pipeline in <ref type="bibr" coords="7,407.58,644.16,14.61,8.74" target="#b20">[21]</ref>, but they restricted the node number of the fully connection network to 2048 as this number is far more enough to model the plant images. Run 2 used a Gaussian Mixture Model based Fisher Kernel approach. First, they extracted dense SIFT and Color Moments (CMs) in images and each local feature was reduced to a 64-dim after using PCA. Then for each type of features, a GMM model with 512 components is estimated for producing two Fisher Vector representations by image. Then, they averaged the output from linear trained SVMs classifiers, one for each type of features. In Run 3 &amp; 4 they combined the CNN and the FV approaches with an empirical rule. Run 4 is like the run 3 but with a segmentation preprocessing step on images in order to find better regions on interest to analyse (only on Flower, Fruit, Leaf, LeafScan and Stem).</p><p>IV-Processing (1 run) <ref type="bibr" coords="8,254.61,254.17,11.46,8.77" target="#b7">[8]</ref>, Tunisia. These participants embeded several types of features (the outputs of the harris detector, a haar wavelet decomposition, a RGB color histogram) into a single binary code after a Principal Component Analysis step. Then the hamming distance is used in order to compare images from the training dataset with image query. <ref type="bibr" coords="8,233.14,329.60,16.80,8.77" target="#b18">[19]</ref>, Tunisia. This team tested visual and textual approaches. For the visual part they used a combination of standard global descriptors: Color Layout Descriptor (CLD), Edge Orientation Histogram (EOH) and a Scalable Color Descriptor (SCD). Then, they attempted to use the contextual content in the associated XML documents for each image with textual and structural representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MIRACL (3 runs)</head><p>PlantNet (4 runs) <ref type="bibr" coords="8,228.77,416.98,16.80,8.77" target="#b12">[13]</ref>, France. These participants used for all categories a large scale matching approach, and some shape descriptors in the specific case of LeafScan (Directional Fragment Histogram and standard shape parameters). A geometrically constrained multi-scale &amp; multi-orientation Harris-Laplace detector is used in order detect around 100-150 points mostly located at the center of the pictures. Then, they extracted numerous local features: SURF, Fourier2D, rotation invariant Local Binary Patterns, Edge Orientation Histogram, weighted RGB, weighted-LUV and HSV histograms. After preliminary evaluations, each type of view had its own subsets of types of local features. These local features are hashed, indexed and searched in separate index with the Random Maximum Margin Hashing approach (one for each type of view and for each type of feature). Then, a hierarchical late fusion scheme is applied in order to combine the image response lists of the different modalities: first from the different types of local features, then from the multiple-images from a same category, and finally from all the categories in order to obtain a final list related to one plant observation. Different fusion algorithms are experimented in order to combine the information at each level: a weighted probabilities approach (run 1), and the BordaMNZ count (run 2,3,4) and IprMNZ count (run 4 only for LeafScan and Fruit) inspired from the voting theory. The final species list is produced thanks to an adaptive k-nn rule (k being related to plant observations, not images).</p><p>QUT (1 run) <ref type="bibr" coords="9,202.64,118.96,16.80,8.77" target="#b27">[28]</ref>, Australia. These participants used (Overfeat) a Convolutional Neural Network pre-trained with the generic ImageNet dataset previously used to perform general object classification. They used two layers in this network in order to obtain two sets of visual features: the Layer 17 gave a set of 3072-dim vectors, and Layer 19 gave a set of 4096-dim vectors. Then, a extremely Randomized Trees Classifier is used in order to output a probability distribution over the 500 species, one for each feature. The probability distributions are then averaged in order to compute a single probability distribution for a test image. Finally, probability distributions from several pictures from a same test observation are added in order to obtain the final list of ranked species. Note that this kind of approach was not really allowed since the Overfeat features are pre-trained with some external resources.</p><p>Sabanki-Okan (2 runs) <ref type="bibr" coords="9,254.37,285.91,16.80,8.77" target="#b29">[30]</ref>, Turkey. These participants used distinct approaches, depending to the category of picture. For the LeafScan category, an automatic segmentation was performed using edge preserving morphological simplification by means of area attribute filters, followed by an adaptive threshold. Then, a variety of shape and texture features are extracted (the same than the ones used during the previous 2012-13 campaigns): Circular Covariance Hist. (CCH), Rotation Invariant Point Triplets (RIT), Orientation Hist. (OH), Color Auto-correlogram, etc. For the Flower, Fruit and Entire categories, they used a Bag of Visual Word approach: they extracted some dense-SIFT features and used a K-Means in order to obtain the visual dictionary of 1200 words and produce BoW representations for each image. For the Stem category, they used the same global descriptors on texture and color used for the LeafScan category (CCH, OH, RIT) with an additional Morphological Covariance descriptor. Finally in each system they used SVM classifiers for predicting a list of ranked species. In the specific case of Branch and Leaf categories, they used a Convolutional Neural Network approach. The CNN employed contains 8 layers where the output of the last fully connected softmax layer produces a distribution over the species. SZTE (4 runs) <ref type="bibr" coords="9,214.08,512.63,16.80,8.77" target="#b25">[26]</ref>, Hungary. The SZTE focused their work on the Leaf-Scan category: after a first Otsu segmentation, they extracted a Vein density description, various shape parameters (area/perimeter, perimeter/diameter, diameter/perpendicular-diameter) and the cumulative histogram representation of Multiscale Triangular shape descriptors successfully evaluated in last year plant identification task <ref type="bibr" coords="9,243.53,572.43,14.61,8.74" target="#b17">[18]</ref>, <ref type="bibr" coords="9,265.65,572.43,14.61,8.74" target="#b23">[24]</ref>. Then a Random Forest classifier is used for predicting species. For the other categories, they used a Color-Gradient Histogram CGH on pictures. The test images are then compared with the training images and k-nn classifier gave a ranked species list. Finally an heuristic rule is proposed for combining pictures from a same test plant observation by allowing a priority to the LeafScan images.</p><p>Table <ref type="table" coords="10,176.48,118.99,4.98,8.74" target="#tab_1">2</ref> attempts to summarize the methods used at different stages (feature, classification,...) in order to highlight the main choices of participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Main task</head><p>The following graphic 2 and table <ref type="table" coords="10,285.89,209.71,4.98,8.74" target="#tab_2">3</ref> show the scores obtained on the main task on plant observation queries. The best results are indisputably obtained by the three last runs of the IBM AU team. This results confirms that the Fisher Vector encoding is currently the state-of-art as a generic approach in most of the problems in computer vision. Convolutional Neural Networks, which is an another well-know recent state-ofart technique for object recognition, have not performed as well here in this problem of plant identification as we can see in the first run of IBM AU or the Sabanki-Okan runs. The main reason, as discussed in the working note of IBM AU team <ref type="bibr" coords="10,203.41,608.30,9.96,8.74" target="#b5">[6]</ref>, is that deep models usually require much training data to learn their millions of parameters and avoid overfitting (e.g. up to 1000 images per class within ImagNet). To solve this issue, deep neural networks are usually pre-trained on generalist classification tasks before being fine-tuned on the targeted task. But as using external training data was not authorized in PlantCLEF Another outcome is that the second best performing method from PlantNet was already among the best performing methods in previous plant identification challenges <ref type="bibr" coords="13,182.83,346.90,10.52,8.74" target="#b3">[4]</ref> although LifeCLEF dataset is much bigger and somehow more complex because of the social dimension of the data. This demonstrates the genericity and stability of the underlying matching method and features.</p><p>This year few teams attempted to explore the metadata. The date was exploited in the Sabanki-Okan runs, only on flowers or fruits, but we don't have a point of comparison in order to see if the use of this information was useful or not. Miracl team attempted to combine the whole textual and structural informations contained in the xml files, but it has been showed to degrade the performances of their pure visual approach. Note that for the first year, after three years of unsuccessful attempts during the previous ImageCLEF Plant Identification Tasks, none of the teams explored the locality and GPS information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Complementary results on images</head><p>6 teams submitted 14 complementary runs on images. The following graphic 3 and table 4 below present the scores obtained on the complementary run files focusing on images. Thanks to the participants who produced these not mandatory run files. In order to evaluate the benefit of the combination of the test images from the same observation, the graphic compares the pairs of run files on images and on observations assumed to have been produced with the same method (it is not the case for the BME TMIT team). Basically, for each method, we can observe a substantial improvement by combining the different views from a same plant observation. It is a good news, since this is the current practice of botanists, who most of the time can't identify a species with only one picture on only one organ. However, we can say that the improvement are not so much high: we guess that there is a room of improvement here, basically with   With the emergence of more and more plant identification apps <ref type="bibr" coords="16,413.64,447.36,15.50,8.74" target="#b21">[22]</ref> [9], <ref type="bibr" coords="16,448.69,447.36,9.96,8.74" target="#b0">[1]</ref>, <ref type="bibr" coords="16,465.10,447.36,15.50,8.74" target="#b26">[27]</ref> and the ecological urgency to build real-world and effective identification tools, we believe that the results and working notes produced during the task will be of high interest for the computer vision and machine learning community. A possible evolution for a new plant identification task in 2015 is to extend the task to all French flora which is estimated to around 5000 species.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,134.77,408.84,345.81,7.89;4,134.77,419.83,93.05,7.86;4,134.96,115.84,342.35,278.23"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Examples of PlantCLEF pictures with decreasing averaged users ratings for the different types of views</figDesc><graphic coords="4,134.96,115.84,342.35,278.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="10,164.82,490.42,285.70,7.89;10,134.77,253.49,345.84,211.20"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Official results of the LifeCLEF 2014 Plant Identification Task.</figDesc><graphic coords="10,134.77,253.49,345.84,211.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="14,134.77,621.96,345.81,7.89;14,134.77,632.95,151.43,7.86;14,134.77,385.03,345.84,211.20"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Comparison of the methods: before and after combining the prediction for each image from a same plant observation.</figDesc><graphic coords="14,134.77,385.03,345.84,211.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="16,194.27,352.77,226.82,7.89;16,134.77,115.83,345.84,211.20"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Results detailed for each type of image category.</figDesc><graphic coords="16,134.77,115.83,345.84,211.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,159.15,355.63,297.06,115.58"><head>Table 1 :</head><label>1</label><figDesc>Detailed content of the LifeCLEF 2014 Plant Task dataset.</figDesc><table coords="5,161.19,372.85,286.84,98.35"><row><cell></cell><cell cols="5">Total Branch Entire Flower Fruit Leaf LeafScan Stem</cell></row><row><cell>Observations</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Train</cell><cell cols="5">11341 1173 3969 5328 1608 5196 2386 1823</cell></row><row><cell>Test</cell><cell>8163</cell><cell>574</cell><cell>2488 3529 744 1522</cell><cell>294</cell><cell>567</cell></row><row><cell>All</cell><cell cols="5">19504 1747 6457 8857 2352 6718 2680 1823</cell></row><row><cell>Images</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Train</cell><cell cols="5">47816 1987 6356 13165 3753 7754 11335 3466</cell></row><row><cell>Test</cell><cell cols="2">13146 731</cell><cell>2983 4559 1184 2058</cell><cell>696</cell><cell>935</cell></row><row><cell>All</cell><cell cols="5">60962 2718 9339 17724 4937 9812 12031 4401</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="11,134.77,121.76,345.84,535.49"><head>Table 2 :</head><label>2</label><figDesc>Approaches used by participants. Column "Obs." indicates if participants avoid to split images from a same plant Observation during evaluation on training dataset.</figDesc><table coords="11,152.14,157.91,313.22,499.34"><row><cell>Obs. Meta-</cell><cell>data</cell><cell cols="2">× ×</cell><cell></cell><cell></cell><cell></cell><cell>×</cell><cell></cell><cell></cell><cell cols="2">×</cell><cell></cell><cell cols="3">√ ×</cell><cell></cell><cell>×</cell><cell cols="2">√</cell><cell></cell><cell></cell><cell cols="2">√ ×</cell><cell></cell><cell>×</cell><cell>date</cell><cell>√ on flower</cell><cell>&amp;</cell><cell>fruit</cell><cell>×</cell></row><row><cell>Image Combination</cell><cell></cell><cell>weighted average of reli-</cell><cell cols="2">ability values</cell><cell>late hierarchical fusion</cell><cell>schema: combination of</cell><cell>list images results (Min</cell><cell>Rank and/or Probability</cell><cell>fusion).</cell><cell>weighted-sum or max</cell><cell cols="2">score of confidence</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">late hierarchical fusion</cell><cell>schema: BordaMNZ,</cell><cell>IrpMNZ rules, weighted</cell><cell>probabilities combina-</cell><cell>tion</cell><cell>image level: average of</cell><cell>probability distributions</cell><cell>from the prediction on</cell><cell>each feature,</cell><cell>observation level: likeli-</cell><cell>hood sum</cell><cell>heuristic</cell></row><row><cell>Classification/</cell><cell>Similarity search</cell><cell>C-SVC with RBF</cell><cell cols="2">kernel</cell><cell></cell><cell></cell><cell>1-nn rule</cell><cell></cell><cell></cell><cell cols="2">binary SVMs</cell><cell></cell><cell>Logistic Regression</cell><cell cols="2">on CNN</cell><cell>SVM on FVs</cell><cell></cell><cell></cell><cell>euclidian distance</cell><cell></cell><cell></cell><cell></cell><cell>adaptive k-nn rule</cell><cell></cell><cell>Extremely Ran-</cell><cell>domized Trees</cell><cell>LeafScan, Fruit,</cell><cell>Flower, Entire,</cell><cell>Stem: SVMs</cell><cell>Leaf,Branch: out-</cell><cell>put of CNN</cell><cell>LeafScan: Random</cell><cell>Forest</cell><cell>Photos: knn</cell></row><row><cell>Features representa-</cell><cell>tion</cell><cell>GMM based FV</cell><cell cols="2">(256x80)</cell><cell></cell><cell></cell><cell>BoW (200k Words)</cell><cell></cell><cell></cell><cell cols="2">BoWs</cell><cell></cell><cell cols="2">GMM based FV (run</cell><cell cols="2">2,3,4)</cell><cell>hamming distance</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RMMH</cell><cell></cell><cell>Fruit,Flower,Entire:</cell><cell>BoW Leaf,Branch: CNN</cell></row><row><cell>Preprocessing Features</cell><cell></cell><cell>dense-SIFT + PCA (reducing</cell><cell cols="2">128 to 80 dim.)</cell><cell cols="5">Otsu segmenta-LeafScan: Multiple triangular representations tion for LeafS-Photos: Harris-like corner de-can tection + opponent SIFT</cell><cell cols="2">× SIFT and Opponent-Color</cell><cell>SIFT</cell><cell>segmentation of CNN (runs 1,3,4)</cell><cell cols="2">region of inter-dense-SIFT and CMs (runs</cell><cell>est (run 4) 2,3,4)</cell><cell cols="2">× Harris corner detection, Haar wavelet transform, RGB hist.</cell><cell>CLD, EOH, SCD</cell><cell>LeafScan: DFH and shape</cell><cell cols="5">Otsu-like seg-parameters All: Harris-like corner de-mentation for tection + SURF, ri-LBP, LeafScan w-RGB, w-Luv, HSV,</cell><cell>Fourier2D, EOH, Hough</cell><cell>Overfeat pre-trained CNN:</cell><cell>downscale to Layer 17-¿a set of 3072-dim vectors, Layer 19-¿a set of 231 pixels 4096-dim vectors</cell><cell>Scan: CCH, RIT, OH, CAC,</cell><cell>etc</cell><cell>Flower, Fruit, Entire: dense-</cell><cell>Auto segmenta-SIFT</cell><cell>tion for LeafS-Branch, Leaf : CNN</cell><cell>can Stem: CCH, RIT, OH, CAC,</cell><cell>Otsu segmenta-MC. LeafScan: Vein density, shape</cell><cell>tion for LeafS-parameters</cell><cell>can Photos: CGH</cell></row><row><cell>Team</cell><cell></cell><cell cols="2">BME</cell><cell>TMIT</cell><cell></cell><cell></cell><cell>FINKI</cell><cell></cell><cell></cell><cell cols="2">I3S</cell><cell></cell><cell></cell><cell cols="2">IBM AU</cell><cell></cell><cell>IV-</cell><cell>Processing</cell><cell>MIRACL</cell><cell></cell><cell></cell><cell></cell><cell>PlantNet</cell><cell></cell><cell>QUT</cell><cell>Sabanki-</cell><cell>Okan</cell><cell>SZTE</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="12,134.77,209.83,345.84,357.27"><head>Table 3 :</head><label>3</label><figDesc>Results of the LifeCLEF 2014 Plant Identification Task. Column "Keywords" attempts to give the main idea of the method used in each run. Fisher Vector (FV) and Bags Of Visual Word (BoW) representations involve SVM classifiers. approach could not be evaluated by the participants. Allowing such approaches in next campaigns might be possible but is a tricky problem as we need to guaranty that none of the images of test set could be found somewhere on the web. Despite the supremacy of IBM fisher vectors runs, it is surprising to see that the performances of BME TMIT runs, which are based on a very close training model, reached much lower performances. It demonstrates that different implementations and parameters tuning can bring very different performances (e.g. 512x60 fisher vectors dimensions for IBM AU vs. 258x80 for BME TMIT, IBM AU used additionnal Color Moments descriptors while BME used only SIFT).Morever, like it was demonstrated during previous ImageCLEF Plant Identification Task campaigns, teams who split the training data according to the observation id during their preliminary evaluations on validation sets, seem have to take benefit of it, avoiding certainly overfitting problems like for the IBM AU, PlantNet teams for instance. BME TMIT not mentioned that and may be were in this case, explaining also the difference of performances with the IBM AU runs.</figDesc><table coords="12,173.71,262.95,261.79,304.15"><row><cell>Run name</cell><cell>Key-words</cell><cell cols="2">Metadata Score</cell></row><row><cell>IBM AU Run 4</cell><cell>FV+CNN+Segm.</cell><cell>×</cell><cell>0,471</cell></row><row><cell>IBM AU Run 3</cell><cell>FV+CNN</cell><cell>×</cell><cell>0,459</cell></row><row><cell>IBM AU Run 2</cell><cell>FV</cell><cell>×</cell><cell>0,454</cell></row><row><cell>PlantNet Run 2</cell><cell>Matching RMMH</cell><cell>×</cell><cell>0,289</cell></row><row><cell>PlantNet Run 3</cell><cell>Matching RMMH</cell><cell>×</cell><cell>0,289</cell></row><row><cell>PlantNet Run 4</cell><cell>RMMH</cell><cell>×</cell><cell>0,282</cell></row><row><cell>PlantNet Run 1</cell><cell>Matching RMMH</cell><cell>×</cell><cell>0,278</cell></row><row><cell>IBM AU Run 1</cell><cell>CNN</cell><cell>×</cell><cell>0,271</cell></row><row><cell>BME TMIT Run 1</cell><cell>FV</cell><cell>×</cell><cell>0,255</cell></row><row><cell>BME TMIT Run 2</cell><cell>FV</cell><cell>×</cell><cell>0,255</cell></row><row><cell>BME TMIT Run 3</cell><cell>FV</cell><cell>×</cell><cell>0,255</cell></row><row><cell>QUT Run 1</cell><cell>CNN feat.</cell><cell></cell><cell>0,249</cell></row><row><cell>FINKI Run 1</cell><cell>BoW</cell><cell>×</cell><cell>0,205</cell></row><row><cell>FINKI Run 3</cell><cell>BoW</cell><cell>×</cell><cell>0,204</cell></row><row><cell>FINKI Run 2</cell><cell>BoW</cell><cell>×</cell><cell>0,166</cell></row><row><cell>Sabanci-Okan Run 1</cell><cell>BoW, CNN</cell><cell>date</cell><cell>0,127</cell></row><row><cell>Sabanci-Okan Run 2</cell><cell>BoW, CNN</cell><cell>date</cell><cell>0,127</cell></row><row><cell>I3S Run 1</cell><cell>BoW</cell><cell>×</cell><cell>0,091</cell></row><row><cell>I3S Run 2</cell><cell>BoW</cell><cell>×</cell><cell>0,089</cell></row><row><cell>SZTE Run 1</cell><cell>Global mixed</cell><cell>×</cell><cell>0,088</cell></row><row><cell>SZTE Run 3</cell><cell>Global mixed</cell><cell>×</cell><cell>0,086</cell></row><row><cell>SZTE Run 2</cell><cell>Global mixed</cell><cell>×</cell><cell>0,085</cell></row><row><cell>SZTE Run 4 Miracl Run 1 Miracl Run 2 Miracl Run 3</cell><cell>Global mixed Global Global, Textual, Structural Global, Textual, Structural</cell><cell>× √ √ √</cell><cell>0,085 0,063 0,051 0,047</cell></row><row><cell>IV Processing Run 1</cell><cell>Local/Global mixed</cell><cell>×</cell><cell>0,043</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="14,134.77,138.06,345.84,192.83"><head>Table 4 :</head><label>4</label><figDesc>Comparison between results on identification by images with results on identification by observation</figDesc><table coords="14,200.71,169.21,207.80,161.69"><row><cell>Run name</cell><cell cols="2">Score Image Score Observation</cell></row><row><cell>IBM AU Run 4</cell><cell>0,456</cell><cell>0,471</cell></row><row><cell>IBM AU Run 3</cell><cell>0,446</cell><cell>0,459</cell></row><row><cell>IBM AU Run 2</cell><cell>0,438</cell><cell>0,454</cell></row><row><cell>PlantNet Run 2</cell><cell>0,28</cell><cell>0,289</cell></row><row><cell>PlantNet Run 4</cell><cell>0,276</cell><cell>0,282</cell></row><row><cell>PlantNet Run 1</cell><cell>0,271</cell><cell>0,278</cell></row><row><cell>IBM AU Run 1</cell><cell>0,263</cell><cell>0,271</cell></row><row><cell>FINKI Run 1</cell><cell>0,205</cell><cell>0,205</cell></row><row><cell>FINKI Run 3</cell><cell>0,204</cell><cell>0,204</cell></row><row><cell>FINKI Run 2</cell><cell>0,166</cell><cell>0,166</cell></row><row><cell>Sabanci Okan Run1</cell><cell>0,123</cell><cell>0,127</cell></row><row><cell>Sabanci Okan Run2</cell><cell>0,123</cell><cell>0,127</cell></row><row><cell>BME TMIT Run 1</cell><cell>0,086</cell><cell>0,255</cell></row><row><cell>I3S Run 1</cell><cell>0,043</cell><cell>0,091</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_0" coords="2,144.73,613.61,94.14,7.47"><p>http://leafsnap.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_1" coords="2,144.73,624.57,301.26,7.47"><p>https://play.google.com/store/apps/details?id=org.plantnet&amp;hl=en</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_2" coords="2,144.73,635.53,174.66,7.47"><p>http://identify.plantnet-project.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_3" coords="2,144.73,646.48,117.68,7.47"><p>http://www.imageclef.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_4" coords="2,144.73,657.44,141.71,7.47"><p>http://www.clef-initiative.eu/</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>more images and may be with new methods of fusions dealing with this specific problem of multi-image and multi-organ problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Complementary results on images detailed by organs</head><p>The following table 5 and graphic 4 below show the detailed scores obtained for each type of organs. Remember that we use a specific metric weighted by authors and plants, and not by sub-categories, explaining why the score on images not detailed is not the mean of the 7 scores of these sub-categories. Like during 0,292 0,333 0,585 0,339 0,318 0,64 0,269 IBM AU Run 3 0,298 0,34 0,57 0,326 0,304 0,614 0,267 IBM AU Run 2 0,294 0,335 0,555 0,317 0,3 0,612 0,267 PlantNet Run 2 0,112 0,181 0,376 0,22 0,164 0,453 0,156 PlantNet Run 4 0,112 0,167 0,366 0,197 0,165 0,541 0,152 PlantNet Run 1 0,112 0,168 0,366 0,197 0,165 0,449 0,133 IBM AU Run 1 0,103 0,193 0,389 0,161 0,103 0,278 0,138 FINKI Run 1 0,088 0,117 0,255 0,177 0,16 0,4 0,157 FINKI Run 3 0,088 0,117 0,255 0,177 0,162 0,36 0,159 FINKI Run 2 0,108 0,099 0,187 0,16 0,14 0,399 0,18 Sabanci Okan Run1 0,007 0,077 0,149 0,118 0,066 0,449 0,089 Sabanci Okan Run2 0,007 0,077 0,149 0,118 0,066 0,449 0,089 BME TMIT Run 1 0,052 0,06 0,115 0,07 0,019 0,119 0,072 I3S Run 1 0,041 0,023 0,04 0,04 0,035 0,089 0,086 the previous Plant Identification Task, the LeafScan and the Flower categories obtained the best results, while it is not easy to rank the other organ by difficulty (maybe Fruit, Entire, Leaf, Stem, Branch). Results obtained on the Branch category by the three last runs of IBM AU outperformed completely the other approaches, while more shape dedicated approaches reduce the difference with this generic approach on LeafScan (PlantNet, Sabanki-Okan &amp; Finki). Interestingly, the pure CNN approach in IBM AU Run 1 obtained rather good results on Flower, the organ where there is a lot of data (as much as in the LeafScan category in terms of number of observations), confirming the potential of the CNN approach with more data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper presented the overview and the results of LifeCLEF 2014 plant identification testbed following the three previous campains within ImageCLEF. The number of participants was around 10 groups showing an interest in applying</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="16,142.96,568.04,337.62,7.86;16,151.52,579.00,329.05,7.86;16,151.52,589.96,329.06,8.11;16,151.52,601.56,164.75,7.47" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="16,376.98,568.04,103.59,7.86;16,151.52,579.00,199.04,7.86">Development and deployment of a large-scale flower recognition mobile app</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Shpecht</surname></persName>
		</author>
		<ptr target="http://www.nec-labs.com/research/information/infoAM_website/pdfs/MobileFlora.pdf" />
		<imprint>
			<date type="published" when="2012-12">December 2012</date>
		</imprint>
	</monogr>
	<note type="report_type">NEC Labs America Technical Report</note>
</biblStruct>

<biblStruct coords="16,142.96,612.42,239.90,7.86;16,410.29,612.42,70.28,7.86;16,151.52,623.38,307.28,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="16,261.81,612.42,121.05,7.86;16,410.29,612.42,66.36,7.86">Morphological features for leaf plant recognition</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Aptoula</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Yanikoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,165.60,623.38,150.70,7.86">Proc. IEEE Int. Conf. Image Process</title>
		<meeting>IEEE Int. Conf. Image ess<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.96,634.88,337.62,7.86;16,151.52,645.84,329.06,7.86;16,151.52,656.80,162.29,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="16,324.58,634.88,156.00,7.86;16,151.52,645.84,96.12,7.86">Plant leaf identification based on volumetric fractal dimension</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Backes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">M</forename><surname>Bruno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,254.89,645.84,225.69,7.86;16,151.52,656.80,62.47,7.86">International Journal of Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1145" to="1160" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.96,119.67,337.62,7.86;17,151.52,130.63,329.06,7.86;17,151.52,141.59,329.05,7.86;17,151.52,152.55,25.60,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="17,240.01,130.63,240.57,7.86;17,151.52,141.59,15.40,7.86">Inria&apos;s participation at imageclef 2013 plant identification task</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Bakic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mouine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ouertani-Litayem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Verroust-Blondet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,187.78,141.59,188.24,7.86">CLEF (Online Working Notes/Labs/Workshop</title>
		<meeting><address><addrLine>Valencia, Espagne</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.96,162.79,337.62,7.86;17,151.52,173.75,329.06,7.86;17,151.52,184.71,127.47,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="17,363.58,162.79,117.00,7.86;17,151.52,173.75,168.19,7.86">A parametric active polygon for leaf segmentation and shape estimation</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cerutti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Tougne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vacavant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Coquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,339.49,173.75,141.08,7.86;17,151.52,184.71,42.85,7.86">International Symposium on Visual Computing</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="202" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.96,194.94,337.62,7.86;17,151.52,205.90,329.06,7.86;17,151.52,216.86,25.60,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="17,355.54,194.94,125.04,7.86;17,151.52,205.90,137.06,7.86">Ibm research australia at lifeclef2014: Plant identification task</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Abedini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Garnavi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,311.93,205.90,168.65,7.86">Working notes of CLEF 2014 conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.96,227.10,337.62,7.86;17,151.52,238.06,329.06,7.86;17,151.52,249.02,70.42,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="17,383.10,227.10,97.47,7.86;17,151.52,238.06,191.70,7.86">Maestra at lifeclef 2014 plant task: Plant identification using visual data</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Dimitrovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Madjarov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Lameski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kocev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,363.86,238.06,116.72,7.86;17,151.52,249.02,41.76,7.86">Working notes of CLEF 2014 conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.96,259.25,337.62,7.86;17,151.52,270.21,315.13,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="17,357.16,259.25,123.42,7.86;17,151.52,270.21,82.48,7.86">A visual search of multimedia documents in lifeclef</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Fakhfakh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Akrout</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Mahdi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,275.22,270.21,162.77,7.86">Working notes of CLEF 2014 conference</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.96,280.45,337.62,7.86;17,151.52,291.41,329.05,7.86;17,151.52,302.37,329.06,7.86;17,151.52,313.33,127.12,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="17,416.70,291.41,63.87,7.86;17,151.52,302.37,147.23,7.86">Plantnet mobile 2014: Android port and new features</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Affouard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Bakic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Barbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dufour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Selmi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Vignau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Barthélémy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,319.54,302.37,161.04,7.86;17,151.52,313.33,98.45,7.86">Proceedings of International Conference on Multimedia Retrieval</title>
		<meeting>International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.61,323.57,337.96,7.86;17,151.52,334.52,329.05,7.86;17,151.52,345.48,176.36,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="17,302.55,334.52,178.03,7.86;17,151.52,345.48,43.05,7.86">The ImageCLEF 2011 plant images classification task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Barthélémy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Birnbaum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mouysset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,215.57,345.48,83.64,7.86">CLEF working notes</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.61,355.72,337.96,7.86;17,151.52,366.68,329.06,7.86;17,151.52,377.64,25.60,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="17,205.46,366.68,172.66,7.86">The imageclef 2012 plant identification task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Barthelemy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Molino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,398.03,366.68,82.55,7.86">CLEF working notes</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.61,387.88,337.96,7.86;17,151.52,398.83,329.06,7.86;17,151.52,409.79,292.97,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="17,333.17,398.83,147.41,7.86;17,151.52,409.79,114.28,7.86">Visual-based plant species identification from crowdsourced data</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Selmi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mouysset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Joyeux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Birnbaum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bathelemy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,287.04,409.79,128.79,7.86">ACM conference on Multimedia</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.61,420.03,337.96,7.86;17,151.52,430.99,329.05,7.86;17,151.52,441.95,25.60,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="17,372.82,420.03,107.75,7.86;17,151.52,430.99,145.06,7.86">Pl@ntnet&apos;s participation at lifeclef 2014 plant identification task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Bakic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">B</forename><surname>Anne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,317.65,430.99,162.93,7.86">Working notes of CLEF 2014 conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.61,452.19,337.96,7.86;17,151.52,463.15,329.06,7.86;17,151.52,474.10,194.24,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="17,351.52,452.19,129.05,7.86;17,151.52,463.15,124.03,7.86">Shape oriented feature selection for tomato plant identification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hazra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Hazra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,283.30,463.15,197.28,7.86;17,151.52,474.10,102.08,7.86">International Journal of Computer Applications Technology and Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">449</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.61,484.34,337.96,7.86;17,151.52,495.30,329.06,7.86;17,151.52,506.26,130.19,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="17,320.02,484.34,160.55,7.86;17,151.52,495.30,249.79,7.86">Plant species recognition using bag-ofword with svm classifier in the context of the lifeclef challenge</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Issolah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lingrand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Precioso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,422.39,495.30,58.19,7.86;17,151.52,506.26,101.53,7.86">Working notes of CLEF 2014 conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.61,516.50,337.96,7.86;17,151.52,527.46,329.05,7.86;17,151.52,538.42,329.07,8.11;17,151.52,550.02,287.14,7.47" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="17,412.86,527.46,67.72,7.86;17,151.52,538.42,169.27,7.86">Interactive plant identification based on social image data</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Bakic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Barbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Selmi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carré</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mouysset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Barthélémy</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S157495411300071X" />
	</analytic>
	<monogr>
		<title level="j" coord="17,329.34,538.42,90.65,7.86">Ecological Informatics</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.61,559.61,337.96,7.86;17,151.52,570.57,329.06,7.86;17,151.52,581.53,329.07,8.11;17,151.52,593.13,127.59,7.47" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="17,167.77,570.57,180.79,7.86">The imageclef plant identification task 2013</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Bakic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Barthélémy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
		<ptr target="http://hal.inria.fr/hal-00908934" />
	</analytic>
	<monogr>
		<title level="m" coord="17,372.21,570.57,108.36,7.86;17,151.52,581.53,161.22,7.86">International workshop on Multimedia analysis for ecological data</title>
		<meeting><address><addrLine>Barcelone, Espagne</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10">Oct 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.61,602.73,337.96,7.86;17,151.52,613.68,329.05,7.86;17,151.52,624.64,199.02,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="17,192.24,613.68,154.52,7.86">The imageclef plant identification task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Bakic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Barthélémy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,387.48,613.68,93.10,7.86;17,151.52,624.64,170.35,7.86">International workshop on Multimedia analysis for ecological data</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,142.61,634.88,337.96,7.86;17,151.52,645.84,329.05,7.86;17,151.52,656.80,46.08,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="17,362.39,634.88,118.18,7.86;17,151.52,645.84,162.39,7.86">Miracl at lifeclef 2014: Multiorgan observation for plant identification</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Karamti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Fakhfakh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Gargouri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,333.99,645.84,146.58,7.86;17,151.52,656.80,17.41,7.86">Working notes of CLEF 2014 conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.61,119.67,337.96,7.86;18,151.52,130.63,261.88,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="18,304.34,119.67,176.23,7.86;18,151.52,130.63,62.35,7.86">Plant image retrieval using color, shape and texture features</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kebapci</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Yanikoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Unal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="18,220.74,130.63,92.84,7.86">The Computer Journal</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1475" to="1490" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.61,141.59,337.96,7.86;18,151.52,152.55,329.06,7.86;18,151.52,163.51,25.60,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="18,328.08,141.59,152.49,7.86;18,151.52,152.55,103.36,7.86">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,274.47,152.55,206.11,7.86">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.61,174.47,337.96,7.86;18,151.52,185.43,329.05,7.86;18,151.52,196.39,325.59,7.86" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="18,215.91,185.43,264.66,7.86;18,151.52,196.39,51.37,7.86">Leafsnap: A computer vision system for automatic plant species identification</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">J</forename><surname>Kress</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">C</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">V B</forename><surname>Soares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,223.70,196.39,169.56,7.86">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="502" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.61,207.35,337.96,7.86;18,151.52,218.30,329.06,7.86;18,151.52,229.26,185.99,7.86" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="18,343.83,207.35,136.75,7.86;18,151.52,218.30,184.86,7.86">Advanced shape context for plant species identification using leaf image retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mouine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Verroust-Blondet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,356.61,218.30,123.97,7.86;18,151.52,229.26,97.13,7.86">ACM International Conference on Multimedia Retrieval</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.61,240.22,337.96,7.86;18,151.52,251.18,329.06,7.86;18,151.52,262.14,229.82,7.86" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="18,347.77,240.22,132.81,7.86;18,151.52,251.18,220.25,7.86">A shape-based approach for leaf classification using multiscale triangular representation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mouine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Verroust-Blondet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,392.99,251.18,87.59,7.86;18,151.52,262.14,201.15,7.86">ICMR &apos;13 -3rd ACM International Conference on Multimedia Retrieval</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.61,273.10,337.96,7.86;18,151.52,284.06,329.05,7.86;18,151.52,295.02,124.91,7.86" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="18,279.21,273.10,201.37,7.86;18,151.52,284.06,53.96,7.86">Automated flower classification over a large number of classes</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,228.32,284.06,252.25,7.86;18,151.52,295.02,40.95,7.86">Indian Conference on Computer Vision, Graphics and Image Processing</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.61,303.71,337.97,10.13;18,151.52,316.94,329.05,7.86;18,151.52,327.89,70.42,7.86" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="18,395.54,305.98,85.04,7.86;18,151.52,316.94,187.51,7.86">Wlab of university of szeged at lifeclef 2014 plant identification task</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Paczolay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bánhalmi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Nyúl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Bilicki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Á</forename><surname>Sárosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,360.90,316.94,119.67,7.86;18,151.52,327.89,41.76,7.86">Working notes of CLEF 2014 conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.61,338.85,337.97,8.11" xml:id="b26">
	<analytic>
		<title/>
		<author>
			<persName coords=""><surname>Reves</surname></persName>
		</author>
		<ptr target="https://itunes.apple.com/fr/app/folia/id547650203" />
	</analytic>
	<monogr>
		<title level="j" coord="18,179.49,338.85,19.59,7.86">Folia</title>
		<imprint>
			<date type="published" when="2012-11">Nov 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.61,349.81,337.96,7.86;18,151.52,360.77,329.06,7.86;18,151.52,371.73,130.19,7.86" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="18,366.00,349.81,114.58,7.86;18,151.52,360.77,250.15,7.86">Fine-grained plant classification using convolutional neural networks for feature extraction</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Sunderhauf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Mccool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Upcroft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Tristan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,422.51,360.77,58.06,7.86;18,151.52,371.73,101.53,7.86">Working notes of CLEF 2014 conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.61,382.69,337.96,7.86;18,151.52,393.65,329.06,7.86;18,151.52,404.61,25.60,7.86" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="18,290.51,382.69,190.07,7.86;18,151.52,393.65,146.51,7.86">Viewpoints combined classification method in image-based plant identification task</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Szúcs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dávid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lovas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,318.68,393.65,161.90,7.86">Working notes of CLEF 2014 conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.61,415.57,337.96,7.86;18,151.52,426.52,329.06,7.86;18,151.52,437.48,70.42,7.86" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="18,396.28,415.57,84.30,7.86;18,151.52,426.52,188.27,7.86">Sabanci-okan system at lifeclef 2014 plant identification competition</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Yanikoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tolga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tirkaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fuencaglartes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,361.63,426.52,118.95,7.86;18,151.52,437.48,41.76,7.86">Working notes of CLEF 2014 conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
