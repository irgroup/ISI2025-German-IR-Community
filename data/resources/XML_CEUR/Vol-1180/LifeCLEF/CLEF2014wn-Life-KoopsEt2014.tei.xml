<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,160.74,115.96,293.86,12.62;1,218.92,133.89,177.50,12.62">A Deep Neural Network Approach to the LifeCLEF 2014 Bird task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,178.41,171.07,100.49,9.96"><forename type="first">Hendrik</forename><forename type="middle">Vincent</forename><surname>Koops</surname></persName>
							<email>h.v.koops@uu.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information and Computing Sciences</orgName>
								<orgName type="institution">Utrecht University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,287.26,171.07,60.45,9.96"><forename type="first">Jan</forename><surname>Van Balen</surname></persName>
							<email>j.m.h.vanbalen@uu.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information and Computing Sciences</orgName>
								<orgName type="institution">Utrecht University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,374.99,171.07,61.96,9.96"><forename type="first">Frans</forename><surname>Wiering</surname></persName>
							<email>f.wiering@uu.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information and Computing Sciences</orgName>
								<orgName type="institution">Utrecht University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,160.74,115.96,293.86,12.62;1,218.92,133.89,177.50,12.62">A Deep Neural Network Approach to the LifeCLEF 2014 Bird task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E41D55D2D47CF70CEF72A969D8674D0C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep Learning</term>
					<term>Neural Networks</term>
					<term>Feature Learning</term>
					<term>Birdsong Recognition</term>
					<term>Bioacoustics</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the methods that are used in our submission to the LifeCLEF 2014 Bird task. A segmentation algorithm is created that is capable of segmenting the audio files of the Bird task dataset. These segments are used to select relevant Mel-Frequency Cepstral Coefficients (MFCC) frames from the MFCC dataset. Three datasets are created, 48: containing only the mean MFCC per segment, 96: containing the mean and variance of the MFCCs in a segment, and 240: containing the mean, variance and the mean of three sections. These dataset are shuffled and split in a test and train set to train Deep Neural Networks with several topologies, which are capable to classify the segments of the datasets. It was found that the best network was capable of correctly classifying 73% of the segments. The results of a run from our system placed us 6th in the list of 10 participating teams. In a follow-up research it is found that shuffling the data before splitting introduces overfitting, which can be reduced by not shuffling the datasets prior to splitting, and using dropout networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The LifeCLEF 2014 Bird task is a bird identification task based on different types of audio records over 501 species from South America centred on Brazil. This paper will describe the methods used to create the "Utrecht Univ." run that was submitted to the task.</p><p>Features are predominantly handcrafted in audio information retrieval research. Handcrafting features relies on a significant amount of domain-and engineering knowledge to translate insights into algorithmic methods. Building features on heuristics makes the process of good feature extraction a difficult problem, as it hinges on the assumption that the feature designer can know what a good representation of a signal must be to solve a problem. Feature design is thus constrained by what a designer can conceive and comprehend. Furthermore, manual optimisation of handcrafted features is a slow process, costly in effort.</p><p>A research area that tries to solve some of the aforementioned problems in feature design is called deep learning, which uses deep architectures to learn feature hierarchies. The features that are higher up in deep hierarchies are formed by the composition of features on lower levels. These multi-level representations allow a deep architecture to learn the complex functions that map the input (such as digital audio) to output (e.g. classes), without the need of dependence on manual handcrafted features.</p><p>In our submission, we used deep neural networks (neural networks with several hidden layers) to facilitate hierarchical feature learning and classification of birdsongs. Our system consists roughly out of tree composed steps: The following sections will describe these steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Automatic Segmentation of Noisy Bird Sounds</head><p>A dataset containing audio data normalised to 44.1 kHz, .wav format (16 bits) is provided for the Bird task. In addition to audio recordings, a dataset containing Mel-Frequency Cepstral Coefficients (MFCC) is provided. This MFCC dataset contains comma separated value files corresponding to the audio files, containing 48 coefficients per line, computed on windows of 11.6 ms, each 3.9 ms. The 48 coefficients are the first 16 MFCCs and their speed an acceleration.</p><p>To facilitate the learning of features on only the relevant birdsong parts of the audio files, a segmentation algorithm is created that is based on the assumption that the loudest parts of an audio recording are the most relevant. The algorithm consists of three major sections:</p><p>1. Decimating and dynamic filtering of the input signal 2. Detection of segments 3. Clustering of segments into higher temporal structures</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Decimating and Dynamic Filtering</head><p>The elements of the BirdCLEF2014 dataset are normalised to a bit rate of 16 bits per second and a sample rate of 44100 Hz. This input signal is downsampled by a factor 4, resulting in a signal with a sample rate of 11025 Hz. Decimation of a signal is common practice in speech recognition, as it reduces the amount of information by removing the top part of the spectrum that we know cannot hold the most important information. The spectrum energy in song birds is typically concentrated on a very narrow area in the range of 1 to 6 kHz <ref type="bibr" coords="2,401.86,619.58,9.96,9.96" target="#b1">[2]</ref>. By decimating the input signal by a factor 4, we reduce the highest possible frequency to 5512.5 Hz. Although some bird song overtone information could exist beyond 5 kHz, this is never the loudest frequency.</p><p>After decimation, the signal is passed through a high pass filter F 1 with a passband frequency of 1kHz, to filter unwanted low frequency noise such as continuous noise from recording equipment, low frequency animal sounds and mechanical sounds. This filter is a 10 th order high pass filter with a passband frequency of 1kHz and a stop band attenuation of 80 dB. From this resulting signal, the fundamental frequency is calculated by finding the maximum value of the spectrogram. This value, f 0 , is then used in a adaptive filter.</p><p>The signal is filtered by another high pass filter that adapts its passband frequency to 0.6 * f 0 . An adaptive filter such as this can account for loud sounds that occur below the bird sound in the spectrogram. This filter is also a 10 th order high pass filter, with a passband frequency of 0.6 * f 0 Hz and a stop band attenuation of 80 dB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Detection of Segments</head><p>This part of the algorithm uses an energy-based algorithm somewhat similar to <ref type="bibr" coords="3,147.54,309.18,10.52,9.96" target="#b2">[3]</ref> to detect the loudest elements in the spectrogram of the decimated and filtered input signal. In the spectrogram |S(f n , t n )| of a signal, the maximum value of f n with t n is found. From this position t n , the frequency parameter</p><formula xml:id="formula_0" coords="3,134.77,345.04,345.83,22.67">ω n (0) = f n and amplitude value a n (0) = 20 log 10 |S(f n , t n )| [dB] is stored. From this position |S(f n , t n )|,</formula><p>a left and right wise trace from the maximum peak of S(f, t) for t &gt; t 0 and for t &lt; t 0 until a n (t -t 0 ) &lt; a n (0) -T dB is performed. From a visual inspection on a subset of the BirdCLEF2014 dataset, the stopping criteria T of 17 dB for marking the boundary of a section was found to create the best positions. The obtained frequency and amplitude trajectories corresponding to the n th annotated part are stored in functions ω n (τ ) and a n (τ ), where τ = t 0t s , . . . , t 0 +t e . By removing the annotated part from starting position t s until end position t e S(f, [t s , t s + 1, . . . , t e ]) from the spectrogram, a reanalysis of the same area is prevented. These steps are repeated N times until no maximum value in the spectrogram above 17 dB is found, resulting in N annotated segments with corresponding start and end time values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Clustering of Segments into Higher Temporal Structures</head><p>A problem with the aforementioned segmentation algorithm, especially in recordings with little background noise, is that a lot of small areas of only a few milliseconds in length are detected. Bird songs are better described at a higher temporal level, which is richer in information. The smaller sections are therefore merged into larger chunks, representing larger parts of the bird song.</p><p>After experimenting with several clustering techniques, it was found that simple gap-wise merging of smaller sections was the most successful in creating meaningful larger sections. Larger chunks are created by analysing the silences between sections and merging subsequent sections with silences smaller than a m milliseconds into one annotated section. Because the segmentation algorithm in the previous step does not annotate each found segment in rank of loudness, the segments need to be sorted first. From this sorted list of n segments S n , each gap between S i and S i+1 is analysed. If the difference between the beginning of S i+1 and the end of S i is smaller than m milliseconds, then let S i = S i +S i+1 and each subsequent section S i+2 is demoted one position in the sorted list of segments, i.e. S i+2 = S i+1 . From an evaluation experiment in which handcrafted segments were compared to automatically generated segments, m = 800 milliseconds was found to create good segments.</p><p>The start and end boundaries of the segments of each file are stored in comma separated value (csv) files, with each line representing one segment. These annotations will be used to select the part of a file from the BirdCLEF2014 data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Extracting Relevant Mel-Frequency Cepstral Coefficients (MFCC) Features</head><p>The start and end boundaries of the segments created by our algorithm are used to select MFCC features from the MFCC dataset that was included in the BirdCLEF2014 training set. Using the calculated segments, three datasets are created:</p><p>-48: only the mean of the MFCCs of a segment is calculated -96: the mean and variance of the MFCCs of a segment are calculated -240: the mean, variance, speed, acceleration and each mean of the section divided in three equal parts are calculated </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Feature Learning and Classification With Deep Neural Networks</head><p>Artificial Neural Networks (ANNs) are a class of machine learning techniques that take inspiration from the physical structure and workings of the animal brain. Recent developments in neural network research unveiled ways to train neural networks with more than one hidden layer, which developed a research area now known as deep learning <ref type="bibr" coords="5,282.27,142.24,9.96,9.96" target="#b4">[5]</ref>.</p><p>Deep neural networks with several topologies are used in feature learning and classification experiments. All networks are implemented in Python, a widely used general-purpose, high-level programming language, using the numerical computation library Theano <ref type="bibr" coords="5,261.82,190.06,9.96,9.96" target="#b3">[4]</ref>. Training and testing errors of these topologies can be found in Table <ref type="table" coords="5,233.57,202.01,3.87,9.96" target="#tab_3">2</ref>. The neural networks take as input the elements from the 48, 96, and 240 datasets, and are trained to classify the segments as one of the 501 species. Figure <ref type="figure" coords="5,152.55,440.26,4.98,9.96" target="#fig_0">1</ref> shows an abstract version of a three-hidden layer Deep Artificial Neural Network used in experiments. The implementations in this experiment use minibatch optimisation, in which the updates of the parameters of the network are based on the summed gradients measured on a rotating subset of the complete training set. To achieve mini-batch optimisation, a selection mechanism is implemented that randomly selects a subset of size n from the data set, in which n is a size that is a parameter to the main call to the network. During early testing and implementation, it was found that a batch size of 250 examples returned favourable results. In all of the experiments in this research, a batch size of 250 is used, which means that at every iteration of training phase, 250 randomly selected training examples are passed though the network, from which a gradient and updates are computed.</p><p>The input layers of the network correspond with the dataset that is used: networks with input layer size 48 are trained and tested with the 48-dataset, the same holds for the 96 and 240 input layer sizes. For the hidden layers, two approaches are used in experiments: one in which the hidden layer size is smaller than the input layer, and one in which the hidden layer is larger than the input layer. For the networks with input layer size 48, experiments are carried out with hidden layer sizes of 48 and 40. For the networks with input layer size 96, networks with hidden layer sizes 64 and 84 are used. For networks with layer size 240, experiments are carried out with hidden layer sizes 128 and 350. All of these networks contain two hidden layers of equal size, except for networks with input layer size 240, two experiments are carried out with three hidden layers. The classification layer is always of size 501 in every network, simply because all the audio files in the BirdCLEF2014 data set belong to one of 501 species.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Voting</head><p>The input to the networks are segments, and the networks therefore perform segment based classification. The goal of the LifeCLEF 2014 Bird task is to classify files, which means a mechanism is needed to use the segment classification for file classification. Several voting schemes are implemented to accomplish this, of which we will discuss the one with the best performance: activation-weighted voting.</p><p>In activation-weighted voting, the activations of the classification layer of the network is used to create a weighted vector of classes from which the mode is chosen. Let F be segmented into five segments {s 1 , s 2 , s 3 , s 4 , s 5 }, and assume there are three classes C 1 , C 2 and C 3 . Now let [0.1, 0.1, 0.8] be the activations for the classes for s 1 . Create a new vector w 1 and add C 1 one time, C 2 one time and C 3 eight times. Repeat for all segments in the file, and concatenate every w n for n = 1 . . . 5 in w. This results in a weighted vector w from which the mode can be taken, which will be the class of F . The result of this voting mechanism on the output of several neural networks can be found in the third column of Table <ref type="table" coords="6,162.16,425.20,3.87,9.96" target="#tab_3">2</ref>.</p><p>The advantage of this approach is the usage of the complete layer activations as probabilities, thereby taking advantage of the network's classification uncertainty. A drawback of this scheme is that it can become slow for a large number of species and a large dataset. Fortunately, the Python package Numpy has great optimisation strategies to deal with this kind of vector processing that minimise computation time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Submission</head><p>Table <ref type="table" coords="6,162.30,558.72,4.98,9.96" target="#tab_3">2</ref> shows training and testing results of experiments with various network topologies using shuffled 48, 96 and 240 Mel-Frequency Cepstral Coefficients (MFCC) datasets. It is found that on average, networks with an input layer of size 240 perform the best on the created MFCC datasets.</p><p>The first two rows of Table <ref type="table" coords="6,271.52,607.08,4.98,9.96" target="#tab_3">2</ref> show that of the networks with an input layer of size 48, the network with a hidden layer size of 40 outperforms the network with a hidden layer of size 48 without voting on the test set.</p><p>A big difference can be found in the testing accuracy of the networks with input size 96. The network with hidden layer size 64 outperforms the network with hidden layer size 84, with and without voting. For both the 48 and 96 networks it is found that increasing the hidden layer size decreases the classification accuracy. This is not the case in the networks with input layer size 240, of which four networks are created with different topologies.</p><p>In the 240-networks with two hidden layers, the network with hidden layers of size 350 outperforms the network with hidden layers of size 128 with a dramatic increase of 62.5%. The bad performance of the network with two hidden layers of size 128 could be attributed to the relative large difference between input layer and hidden layer size. Then again, in the network with three hidden layers of size 128, a classification accuracy is obtained that is close to the accuracy of the network with three hidden layers of size 350. The reason for this might be that by adding more hidden layers, the network is capable of learning a higher representation of the input data. The networks with three hidden layers again show an increase in accuracy when the hidden layer size is increased.</p><p>Overall, it shows that when using voting, the classification accuracy increases on average with 0.02 percent points. If the the big drop in accuracy due to voting in the 48-48-48-501 network is excluded, the average accuracy increases with 4.1 percent points using voting.</p><p>The best performing neural network (240-350-350-501) was able to classify the segments with 27 % error (i.e. 73 % accuracy). This was chosen to calculate the submitted run. The network was retrained using the entire segmented training set, and its classification results are transformed into the BirdCLEF comma separated value file format. This run is submitted to the BirdCLEF submission system, as 1400099635524 clef 240350350v under the team name "Utrecht Univ. Run 1".</p><p>Based on the Mean Average Precision (MAP), our run ranks 15th out of 29 submitted total runs. By comparing the best runs from each group, we are reported to be 6th in the list of 10 participating teams. The MAP of our run with background species is 0.123 and the MAP without background species is 0.14.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Discussion</head><p>After receiving the results, and observing the large difference between results and the results calculated by the LifeCLEF 2014 Bird task committee, we re-evaluated our system. It is believed that overfitting of the neural network occurs when the obtained segments from the elements from the BirdCLEF dataset are shuffled prior to splitting the dataset in a train and test set. Shuffling before dividing the dataset dramatically raises the probability of segments of one file being present in both the test and train set, which could result in the neural networks learning a wrong underlying relationship (such as microphone characteristics, auditory location characteristics, recorder noise or a particular individual bird) in the training set. This in turn results in a poorer performance in the test set.</p><p>Therefore, after receiving the results from all the teams, the datasets are re-created and split without shuffling. In addition to the new datasets, new networks are created that incorporate dropout <ref type="bibr" coords="8,345.21,298.92,9.96,9.96" target="#b0">[1]</ref>, to prevent overfitting. The obtained results so far are closer to the results obtained by other teams in the LifeCLEF 2014 Bird task, with test set classification accuracies ranging between 10% and 20%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Future Work</head><p>At the time of writing this paper, it is not clear where overfitting of the neural networks precisely has occurred. It would be interesting to compare the metadata that was included in the BirdCLEF2014 datasets with classification results by the neural networks. This might show what unwanted relationships are learned by the neural networks, which in turn could help with preventing overfitting in the future.</p><p>It would also be interesting to see how the performance of other deep neural network architectures, such as stacked autoencoders or convolutional neural networks compare to our implementation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,134.77,371.32,345.82,7.89;5,134.77,382.30,61.77,7.86"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Abstract depiction of a three-hidden layer Deep Artificial Neural Network used in experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,138.97,247.05,335.59,33.61"><head></head><label></label><figDesc>1. Automatic segmentation of noisy bird sounds 2. Extracting relevant Mel-Frequency Cepstral Coefficients (MFCC) features 3. Feature learning and classification with Deep Neural Networks</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,134.77,401.54,345.84,45.83"><head>Table 1</head><label>1</label><figDesc>depicts an overview features contained in the 48, 96, and 240 dataset. Each dataset contains 46799 segments, which equals to around 4.83 segments per file. The datasets are shuffled and divided into a 75% training and 25% test set, and used as input for several deep neural network architectures.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,134.77,468.64,345.81,94.91"><head>Table 1 .</head><label>1</label><figDesc>Contents of different network sizes with Mel-Frequency Cepstral Coefficients (MFCC) input.</figDesc><table coords="4,222.51,500.50,170.33,63.06"><row><cell>48 96 240</cell></row><row><cell>Mean (48)</cell></row><row><cell>Variance (48)</cell></row><row><cell>Speed (48)</cell></row><row><cell>Acceleration (48)</cell></row><row><cell>Means of three sections (48 × 3)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,134.77,115.91,345.82,160.17"><head>Table 2 .</head><label>2</label><figDesc>Training and testing results of experiments various network topologies using shuffled 48, 96 and 240 Mel-Frequency Cepstral Coefficients datasets. The first number in the network topology describes the input layer size, the last number describes the output layer size. The numbers in between describe the amount and size of the hidden layers.</figDesc><table coords="7,170.09,180.15,275.17,95.93"><row><cell cols="4">Network topology Training error Test error Test error with voting</cell></row><row><cell>48-40-40-501</cell><cell>63.8 %</cell><cell cols="2">66.16 % 76.91 %</cell></row><row><cell>48-48-48-501</cell><cell>64 %</cell><cell>74.9 %</cell><cell>82 %</cell></row><row><cell>96-64-64-501</cell><cell>42.6 %</cell><cell cols="2">41.56 % 40.0 %</cell></row><row><cell>96-84-84-501</cell><cell>25.0 %</cell><cell cols="2">56.04 % 55 %</cell></row><row><cell>240-128-128-501</cell><cell>0 %</cell><cell>93.6 %</cell><cell>93.7 %</cell></row><row><cell>240-350-350-501</cell><cell>0 %</cell><cell>31.1 %</cell><cell>27 %</cell></row><row><cell cols="2">240-128-128-128-501 0 %</cell><cell>48.5 %</cell><cell>48.1 %</cell></row><row><cell cols="2">240-350-350-350-501 0 %</cell><cell>38.2 %</cell><cell>38.5 %</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,138.35,546.79,342.23,7.86;8,146.91,557.75,333.66,7.86;8,146.91,568.71,189.18,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,211.96,557.75,268.62,7.86;8,146.91,568.71,35.09,7.86">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruslan</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>abs/1207.0580</idno>
	</analytic>
	<monogr>
		<title level="j" coord="8,189.99,568.71,21.82,7.86">CoRR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,579.88,342.23,7.86;8,146.91,590.83,333.67,7.86;8,146.91,601.79,333.67,7.86;8,146.91,612.75,48.76,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,204.61,579.88,275.97,7.86;8,146.91,590.83,68.97,7.86">Automatic identification of bird species based on sinusoidal modeling of syllables</title>
		<author>
			<persName coords=""><forename type="first">Aki</forename><surname>Harma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,242.85,590.83,168.72,7.86;8,173.37,601.79,207.91,7.86">ICASSP&apos;03). 2003 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003">2003. 2003</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">545</biblScope>
		</imprint>
	</monogr>
	<note>Acoustics, Speech, and Signal Processing</note>
</biblStruct>

<biblStruct coords="8,138.35,623.92,342.23,7.86;8,146.91,634.88,333.67,7.86;8,146.91,645.84,333.67,7.86;8,146.91,656.80,48.76,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,305.94,623.92,174.64,7.86;8,146.91,634.88,63.42,7.86">Bird song recognition based on syllable pair histograms</title>
		<author>
			<persName coords=""><forename type="first">Panu</forename><surname>Somervuo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aki</forename><surname>Harma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,239.96,634.88,170.64,7.86;8,173.37,645.84,196.74,7.86">ICASSP&apos;04). IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">825</biblScope>
		</imprint>
	</monogr>
	<note>Acoustics, Speech, and Signal Processing</note>
</biblStruct>

<biblStruct coords="9,138.35,119.67,288.03,7.86;9,463.25,119.67,17.33,7.86;9,146.91,130.63,333.66,7.86;9,146.91,141.59,333.67,7.86;9,146.91,152.55,333.67,7.86;9,146.91,163.51,18.43,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,168.06,141.59,215.49,7.86">Theano: a CPU and GPU math expression compiler</title>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Olivier</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Frédéric</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guillaume</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joseph</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoshua</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,406.92,141.59,73.67,7.86;9,146.91,152.55,214.61,7.86">Proceedings of the Python for Scientific Computing Conference (SciPy)</title>
		<meeting>the Python for Scientific Computing Conference (SciPy)</meeting>
		<imprint>
			<date type="published" when="2010-06">June 2010</date>
		</imprint>
	</monogr>
	<note>Oral Presentation</note>
</biblStruct>

<biblStruct coords="9,138.35,174.47,342.23,7.86;9,146.91,185.43,307.86,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,378.80,174.47,101.78,7.86;9,146.91,185.43,75.38,7.86">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yee-Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,230.19,185.43,77.19,7.86">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,196.39,342.23,7.86;9,146.91,207.35,300.40,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,146.91,207.35,139.19,7.86">LifeCLEF Bird Identification Task</title>
		<author>
			<persName coords=""><forename type="first">Hervé</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hervé</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Willem-Pier And</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andreas</forename><surname>Rauber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,314.26,207.35,103.70,7.86">CLEF working notes 2014</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
