<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,144.59,115.96,326.17,12.62;1,151.82,133.89,311.72,12.62;1,291.54,151.82,32.28,12.62">Instance-based bird species identification with undiscriminant features pruning -LifeCLEF 2014</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,197.31,189.49,48.07,8.74"><forename type="first">Alexis</forename><surname>Joly</surname></persName>
							<email>alexis.joly@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">LIRMM</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,255.93,189.49,61.03,8.74"><forename type="first">Julien</forename><surname>Champ</surname></persName>
							<email>julien.champ@inra.fr</email>
							<affiliation key="aff1">
								<orgName type="laboratory" key="lab1">Inra</orgName>
								<orgName type="laboratory" key="lab2">AMAP</orgName>
								<orgName type="institution">LIRMM</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,346.89,189.49,66.69,8.74"><forename type="first">Olivier</forename><surname>Buisson</surname></persName>
							<email>olivier.buisson@inra.fr</email>
							<affiliation key="aff2">
								<orgName type="institution">Institut National de l&apos;Audiovisuel (INA)</orgName>
								<address>
									<settlement>Bry-sur-Marne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,144.59,115.96,326.17,12.62;1,151.82,133.89,311.72,12.62;1,291.54,151.82,32.28,12.62">Instance-based bird species identification with undiscriminant features pruning -LifeCLEF 2014</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6ECACB2654704982E26A581E95D9DF23</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper reports the participation of Inria to the audiobased bird species identification challenge of LifeCLEF 2014 campaign. Inspired by recent works on fine-grained image classification, we introduce an instance-based classification scheme based on the dense indexing of MFCC features and the pruning of the non-discriminant ones. To make such strategy scalable to the 30M of MFCC features extracted from the tens of thousands audio recordings of the training set, we used highdimensional hashing techniques coupled with an efficient approximate nearest neighbors search algorithm with controlled quality. Further improvements are obtained by (i) using a sliding classifier with max pooling (ii) weighting the query features according to their semantic coherence (iii) making use of the metadata to filter incoherent species. Results show the effectiveness of the proposed technique which ranked 3rd among the 10 participating groups.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Building accurate knowledge of the identity, the geographic distribution and the evolution of living species is essential for a sustainable development of humanity as well as for biodiversity conservation. In this context, using multimedia identification tools is considered as one of the most promising solution to help bridging the taxonomic, i.e. the difficulty for common people to name observed living organisms and then produce or access to useful knowledge. The LifeCLEF <ref type="bibr" coords="1,134.77,572.43,10.52,8.74" target="#b5">[6]</ref> lab proposes to evaluate this challenge in the continuity of the image-based plant identification task was run within ImageCLEF the years before but with a broader scope (considering birds and fish in addition to plants and audio and video contents in addition to images). This paper particularly reports the participation of Inria ZENITH research group to the audio-based bird identification task. Inspired by some recent works on fine-grained image classification <ref type="bibr" coords="1,457.91,632.21,10.52,8.74" target="#b6">[8,</ref><ref type="bibr" coords="1,470.09,632.21,7.01,8.74" target="#b4">5]</ref>, we introduce an instance-based classification scheme based on the dense indexing of MFCC features and the pruning of the non-discriminant ones. Section 2 describes the preliminary audio processings and features extraction steps. Section 3 then presents our raw instance-based classification scheme making use of high-dimensional hashing techniques coupled with an efficient approximate nearest neighbors search algorithm with controlled quality. Section 4 then introduces the additional semantic filtering and weighting rules that are used to improve the raw scheme. Section 6 finally reports our official results within LifeCLEF 2014 as well as few additional experiments conducted to evaluate the contribution of the different steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Pre-processing and features extraction</head><p>The dataset used for this challenge is composed of 14,027 audio recordings belonging to 501 bird species from Brazil area. As various recording devices are used, and because it is difficult to capture these sounds as birds are often far away from the recording devices, many recordings contains a lot of noise. To overcome this problem, we used SoX, the "Swiss Army knife of sound processing programs"<ref type="foot" coords="2,197.29,318.10,3.97,6.12" target="#foot_0">4</ref> . As a first step, we used the noisered specialised filter, to filter out noise from the audio, and then we reduce the length of large (i.e. &gt; 0.1s) silent passages from audio files to 0.1s. In order to obtain audio files with ideally no more noise but still enough signal, we tried removing as much noise as possible (using the noisered amount parameter) while guaranteeing that the resulting audio file was at least 20% the size of the initial audio record. After this pre-processing step, we used an open source software framework, marsyas <ref type="foot" coords="2,460.62,389.83,3.97,6.12" target="#foot_1">5</ref> , to extract MFCC features with parameters based on the provided audio features in the Birdclef task : MFCC are computed on windows of 11.6 ms, each 3.9 ms, and we additionally derive their speed resulting in 26-dimensional feature vectors (13+13) for each frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Raw instance-based classification scheme</head><p>We consider a training set S of |S| audio records weakly annotated with one or several labels among |C| classes (corresponding to one or several bird species recognizable in the record). Each picture is described by a set of MFCC features forming a reference dataset of N features x i . Our instance-based classification scheme can be summarized as follows: MFCC features are extracted from the query record I Q and searched independently in the reference set using an efficient k-NN search scheme. A sliding vote procedure followed by a max pooling step is then used to determine the best matching interval of each retrieved record and rank them accordingly. A strong classifier is finally derived from the ranked list of retrieved records via a simple top-K classifier (K=30 in all our experiments). The details of these different steps are provided below:</p><p>Hash-based k-NN search. The approximate k-Nearest Neighbors (k-NN) of each MFCC feature x Q j belonging to a query record I Q are computed efficiently thanks to the hash-based multi-probe search method introduced in <ref type="bibr" coords="3,467.32,143.99,9.96,8.74" target="#b2">[3]</ref>. Its principle is to train an adaptive search model at indexing time through kernel density estimates computed on exact k-NN samples. This model is used at search time to select the most probable buckets to be visited so as to retrieve on average a fraction α of the real K-NN's (α was set to 0.80 in all our experiments). The advantage of this method compared to other state-of-the-art methods (such as PQ-code [7] or Soft-Assignment <ref type="bibr" coords="3,277.70,215.72,10.79,8.74" target="#b7">[9]</ref>) is that it allows controlling accurately the quality of the retrieved k-NN while being applicable to any quantization function and metrics. In our case, the original scheme is transposed to the use of a more effective hash function and to the application of the Hamming Embedding principle <ref type="bibr" coords="3,441.58,263.54,9.96,8.74" target="#b0">[1]</ref>. More precisely, we used RMMH <ref type="bibr" coords="3,251.61,275.50,9.96,8.74" target="#b3">[4]</ref>, a recent data-dependent hash function family, in order to embed the original feature vectors in compact binary hash codes of 128 bits (the parameter M of RMMH was fixed to M = 32). The m = log 2 (N ) first bits of the hash codes are used as keys of a hash map containing the N 128bits hash codes divided into the 2 m buckets. Still at indexing time, the search model is estimated based on the exact k-NN's in the 128-bits Hamming space rather than in the original space. It has actually be shown in <ref type="bibr" coords="3,410.95,347.23,10.52,8.74" target="#b3">[4]</ref> that RMMH somehow works as an unsupervised metric learning technique and can provide better matches in the embedded space than in the original one. At search time, the query feature x Q j is also embedded via RMMH and the resulting 128-bits hash code h Q j is passed to the probabilistic multi-probe algorithm (see <ref type="bibr" coords="3,454.04,397.77,10.52,8.74" target="#b2">[3]</ref> for more details). Once the most probable buckets have been selected, the k nearest neighbors are computed as the k nearest hash codes according to the Hamming distance between the query hash code h Q j and the hash codes belonging to the selected buckets. The value of k was trained by cross-validation and finally fixed to k = 200 in all our runs.</p><p>Sliding vote and max pooling. A voting scheme is used to merge the raw independent matches produced by the k-NN search of each MFCC feature and return a ranked list of records. To increase the robusteness of the matching and reduce the influence of outliers and background features, the score of each record is computed by first voting within a sliding window (centered around each frame) and then keeping the max score over the whole record. The size of the sliding window was trained by cross-validation and then fixed to s = 1000 frames in all our runs (resulting in a sliding window of 3.9 seconds).</p><p>Top-K classifier. The final instance-based classifier simply consists in summing the scores of the records of each class C j within the top-K retrieved records. For a given test record I Q , the classifier then returns a ranked list of species sorted by their score S Q (C j ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Semantic pruning and weighting</head><p>As we are in the context of weakly annotated audio records with multiple classes (primary and secondary species) and highly cluttered contexts, we suggest improving the raw matching scheme by semantically pruning the non-discriminant training features (offline) and weighting the query features according to the semantic coherence of their k nearest neighbours (online).</p><p>Offline training features pruning. To select the most discriminant features of the training set, we first compute the full k-nearest neighbors graph of the training set by searching each individual MFCC feature x i of each training audio record with the hash-based k-NN search technique described in section 3. We then affect a discrimination score f (x i ) to each feature computed as the percentage of k-nearest having the same weak label than the feature itself. We finally removed from the index all the features having a discrimination score equal to zero. Note that we did experiment several other strategies such as keeping all features in the index and using the discrimination scores f (x i ) as a weights within the vote of the search phase. This did not improve the results over the brute-force pruning of non-discriminant features we finally used (we experimented different regularization function and different values of k). This even degraded the performances over the raw instance-based classification scheme due to over-fitting. Too much weights are actually concentrated on very few features so that the contribution of the vast majority of useful features with moderate scores remains negligeable in the vote. Our pruning strategy allows keeping the contribution of these features by removing only the very bad ones with a null discrimination score. It is important to note that the fraction of such bad features is still large (about XX% of the 30M MFCC features). Removing them consistently reduce the noise of the dataset as well as the size of the index and the search complexity. Besides, we also explored alternative discrimination scoring strategy considering the reverse k-nn's rather the direct k-nn's for the computation of f (x i ) (in a more Bayesian way). This lead to slightly worse results. Using the product of both the direct and the reverse discrimination scores lead to similar conclusions.</p><p>Online query features weighting. Similarly to the audio records of the training set, test records might also contain a lot of unuseful features corresponding to background noise or intervals of time where two much audible species overlap. We therefore also compute a discrimination score (referred as f Q ) for each MFCC feature x Q j belonging to a query record I Q . A weak label l Q j is first estimated for each x Q j as the most represented label within the k-nearest neighbors of x Q j (still computed by the hash-based k-nn search method described in section 3). Similarly to f (), f Q () is then computed as the percentage of k-nearest neighbors having the same weak label than the feature itself (i.e. the percentage of k-nearest neighbors whose label is equal to l Q j ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Metadata</head><p>To further improve the identification performances, we explored the use of metadata as a way to re-rank the returned species. The basic principle is to reduce the score S Q (C j ) of the species whose metadata values distribution in the training set does not fit the value of the metadata of the query record. More particularly, we focused on three types of metadata that did bring some in our cross-validation tests: geo-location, altitude and time-of-day. Revised scores S Q (C j ) are computed from the original ones according to:</p><formula xml:id="formula_0" coords="5,200.79,237.45,279.80,22.79">S Q (C j ) = S Q (C j ) × e - δ 2 alt 2σ 2 alt × e - δ 2 geo 2σ 2 geo × e - δ 2 hour 2σ 2 hour (1)</formula><p>where the δ y 's are equal to the difference between the metadata value y q of the query (e.g. the altitude of the query observation) and the closest metadata value of all training records belonging to class C j (e.g. the closest observation in terms of altitude). The radial basis function allows preserving the score of the species having at least one observation close to the query. On the other side, the score of the species that do not have any hit close to the query tends to zero when the distance to the closest observation grows. Note that we could have sum the RBF weights on all training records rather than considering only the closest one. This would have lead to a classical kernel density estimation and the weight affected to each species whould have been equivalent to its likelihood according to the considered metadata. But it is important to note that the distribution of the observations in the training set is biased in many ways. The density of the observations is typically more correlated to the observer's displacements than to the density of plants. Our model rather rely on the proved presence of a species at a given place (or time of day) independently of the density of the observations of that species at that place (or time of day). The values of the sigma y 's normalization factors were set proportionally to the standard deviation of the distance to the nearest neighbor of any observation in the dataset.</p><p>6 Experiments and results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Dataset and task</head><p>The LifeCLEF 2014 bird dataset <ref type="bibr" coords="5,282.35,553.21,10.52,8.74" target="#b1">[2]</ref> is built from the Xeno-canto collaborative database <ref type="foot" coords="5,175.34,563.60,3.97,6.12" target="#foot_2">6</ref> involving at the time of writing more than 140k audio records covering 8700 bird species observed all around the world thanks to the active work of more than 1400 contributors. The subset of Xeno-canto data used for the 2014 edition of the task contains 14,027 audio recordings belonging to 501 bird species in Brazil area, i.e. the ones having the more recordings in Xeno-canto database.</p><p>The dataset contains minimally 15 recordings per species (maximum 91) and minimally 10 different recordists, maximally 42, per species.Audio records are associated to various metadata such as the type of sound (call, song, alarm, flight, etc.), the date and localization of the observations (from which rich statistics on species distribution can be derived), some textual comments of the authors, multilingual common names and collaborative quality ratings (more details can be found in <ref type="bibr" coords="6,185.38,166.81,10.30,8.74" target="#b1">[2]</ref>). The task will be evaluated as a bird species retrieval task. A part of the collection will be delivered as a training set available a couple of months before the remaining data is delivered. The goal will be to retrieve the singing species among the top-k returned for each of the undetermined observation of the test set. Participants will be allowed to use any of the provided metadata complementary to the audio content. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Preliminary cross-validation experiments</head><p>To tune the parameters of our instance-based retrieval scheme, we randomly split the training set in 3/4 for training and 1/4 for testing. We then measured the Mean Average Precision of our method at the audio records level, i.e. before applying the final top-K classifier at the species level. Some of the intermediate results are displayed in Table <ref type="table" coords="6,271.41,446.82,4.98,8.74" target="#tab_0">1</ref> to illustrate the contribution of the different steps of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Submitted runs and official results</head><p>We submitted two runs to the official competition:</p><p>INRIA Zenith Run 1: Our instance-based classification scheme with semantic pruning &amp; weighting but WITHOUT the metadata oriented reranking INRIA Zenith Run 2: Our full instance-based classification scheme with both the semantic pruning &amp; weighting AND the metadata oriented reranking</p><p>The official results are reported in Figure <ref type="figure" coords="6,331.84,608.30,4.98,8.74" target="#fig_0">1</ref> and Table <ref type="table" coords="6,386.28,608.30,3.87,8.74" target="#tab_1">2</ref>. We globally ranked as the third team over the ten participants and as the second one if we consider only the purely audio-based runs. Our best run achieved a mAP of 0, 365 when considering only the primary species of each test recording (vs. 0.317 when considering the background species as well). Compared to the mAP of our second run equals to 0.328 (0.281 with the background species), it shows that the contribution of our reranking scheme making use of the metatata is about 3.6 points of mAP. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="8,134.77,394.01,345.83,7.89;8,134.77,404.97,181.43,7.89;8,134.77,172.26,338.91,206.98"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Official results of LifeCLEF 2014 Bird Task -Our runs are referred as INRIA Zenith Run 1 and INRIA Zenith Run 2</figDesc><graphic coords="8,134.77,172.26,338.91,206.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,136.16,258.71,337.41,71.15"><head>Table 1 .</head><label>1</label><figDesc>Cross-validation retrieval tests</figDesc><table coords="6,136.16,277.74,36.44,7.89"><row><cell>Method</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,136.16,226.84,339.01,323.21"><head>Table 2 .</head><label>2</label><figDesc>Official results of LifeCLEF 2014 Bird Task</figDesc><table coords="7,136.16,245.90,339.01,304.15"><row><cell>Run name</cell><cell>Type</cell><cell>mAP 1</cell><cell>mAP 2</cell></row><row><cell></cell><cell></cell><cell cols="2">with bckg species without bkg species</cell></row><row><cell>MNB TSA Run 3</cell><cell>AUDIO &amp; META</cell><cell>0,453</cell><cell>0,511</cell></row><row><cell>MNB TSA Run 1</cell><cell>AUDIO &amp; META</cell><cell>0,451</cell><cell>0,509</cell></row><row><cell>MNB TSA Run 4</cell><cell>AUDIO&amp;META</cell><cell>0,449</cell><cell>0,504</cell></row><row><cell>MNB TSA Run 2</cell><cell>AUDIO &amp; META</cell><cell>0,437</cell><cell>0,492</cell></row><row><cell>QMUL Run 3</cell><cell>AUDIO</cell><cell>0,355</cell><cell>0,429</cell></row><row><cell>QMUL Run 4</cell><cell>AUDIO</cell><cell>0,345</cell><cell>0,414</cell></row><row><cell>QMUL Run 2</cell><cell>AUDIO</cell><cell>0,325</cell><cell>0,389</cell></row><row><cell>QMUL Run 1</cell><cell>AUDIO</cell><cell>0,08</cell><cell>0,369</cell></row><row><cell cols="2">INRIA Zenith Run 2 AUDIO &amp; META</cell><cell>0,317</cell><cell>0,365</cell></row><row><cell>INRIA Zenith Run 1</cell><cell>AUDIO</cell><cell>0,281</cell><cell>0,328</cell></row><row><cell>HLT Run 3</cell><cell>AUDIO &amp; META</cell><cell>0,289</cell><cell>0,272</cell></row><row><cell>HLT Run 2</cell><cell>AUDIO &amp; META</cell><cell>0,284</cell><cell>0,267</cell></row><row><cell>HLT Run 1</cell><cell>AUDIO &amp; META</cell><cell>0,166</cell><cell>0,159</cell></row><row><cell>BirdSPec Run 2</cell><cell>AUDIO</cell><cell>0,119</cell><cell>0,144</cell></row><row><cell>Utrecht Univ. Run 1</cell><cell>AUDIO</cell><cell>0,123</cell><cell>0,14</cell></row><row><cell>Golem Run 1</cell><cell>AUDIO</cell><cell>0,105</cell><cell>0,129</cell></row><row><cell>Golem Run 2</cell><cell>AUDIO</cell><cell>0,104</cell><cell>0,128</cell></row><row><cell>BirdSPec Run 1</cell><cell>AUDIO</cell><cell>0,08</cell><cell>0,092</cell></row><row><cell>BirdSPec Run 4</cell><cell>AUDIO</cell><cell>0,074</cell><cell>0,089</cell></row><row><cell>Golem Run 3</cell><cell>AUDIO</cell><cell>0,074</cell><cell>0,089</cell></row><row><cell>BirdSPec Run 3</cell><cell>AUDIO</cell><cell>0,062</cell><cell>0,075</cell></row><row><cell>Yellow Jackets Run 1</cell><cell>AUDIO</cell><cell>0,003</cell><cell>0,003</cell></row><row><cell>Randall Run 1</cell><cell>AUDIO</cell><cell>0,002</cell><cell>0,002</cell></row><row><cell>SCS Run 1</cell><cell>AUDIO</cell><cell>0</cell><cell>0</cell></row><row><cell>SCS Run 2</cell><cell>AUDIO</cell><cell>0</cell><cell>0</cell></row><row><cell>SCS Run 3</cell><cell>AUDIO</cell><cell>0</cell><cell>0</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="2,144.73,645.84,109.20,7.86"><p>http://sox.sourceforge.net/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="2,144.73,656.80,83.57,7.86"><p>http://marsyas.info/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2" coords="5,144.73,656.80,114.96,7.86"><p>http://www.xeno-canto.org/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,138.35,481.77,342.23,7.86;8,146.91,492.73,333.67,7.86;8,146.91,503.69,241.56,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,243.80,481.77,236.78,7.86;8,146.91,492.73,110.49,7.86">Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Andoni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,278.06,492.73,133.12,7.86;8,441.54,492.73,39.04,7.86;8,146.91,503.69,125.09,7.86">FOCS&apos;06. 47th Annual IEEE Symposium</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page" from="459" to="468" />
		</imprint>
	</monogr>
	<note>Foundations of Computer Science</note>
</biblStruct>

<biblStruct coords="8,138.35,514.59,342.23,7.86;8,146.91,525.54,18.43,7.86" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="8,356.07,514.59,124.50,7.86">Lifeclef bird identification task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rauber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,536.44,342.22,7.86;8,146.91,547.40,333.66,7.86;8,146.91,558.36,333.67,7.86;8,146.91,569.32,22.01,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,239.99,536.44,205.30,7.86">A posteriori multi-probe locality sensitive hashing</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Buisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,146.91,558.36,229.48,7.86">ACM International Conference on Multimedia (MM&apos;08)</title>
		<editor>
			<persName><forename type="first">A</forename><surname>El-Saddik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vuong</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Griwodz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Bimbo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Candan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Jaimes</surname></persName>
		</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">10 2008</date>
			<biblScope unit="page" from="209" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,580.21,342.23,7.86;8,146.91,591.17,161.34,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,238.06,580.21,142.07,7.86">Random maximum margin hashing</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Buisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,402.55,580.21,52.56,7.86">CVPR. IEEE</title>
		<meeting><address><addrLine>United States</addrLine></address></meeting>
		<imprint>
			<publisher>Colorado springs</publisher>
			<date type="published" when="2011-06">Jun 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,602.07,164.18,7.86;8,320.35,602.07,160.23,7.86;8,146.91,613.03,333.67,7.86;8,146.91,623.99,168.12,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,296.31,613.03,184.27,7.86;8,146.91,623.99,43.00,7.86">Interactive plant identification based on social image data</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Bakić</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Barbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Selmi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carré</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mouysset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Molino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,197.09,623.99,89.27,7.86">Ecological Informatics</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,634.88,342.22,7.86;8,146.91,645.84,333.67,7.86;8,146.91,656.80,40.24,7.86;9,134.77,119.67,345.81,7.86;9,146.91,130.63,333.67,7.86;9,146.91,141.59,25.60,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,273.85,645.84,206.73,7.86;8,146.91,656.80,40.24,7.86;9,134.77,119.67,3.58,7.86;9,283.79,119.67,192.87,7.86">Lifeclef 2014: multimedia life species identification challenges 7</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,146.91,130.63,270.38,7.86">Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="117" to="128" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>IEEE Transactions on</note>
</biblStruct>

<biblStruct coords="9,138.35,152.55,342.22,7.86;9,146.91,163.51,333.66,7.86;9,146.91,174.47,274.05,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,343.92,152.55,136.65,7.86;9,146.91,163.51,53.56,7.86">Instance classification with prototype selection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krapac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Furon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<ptr target="http://hal.inria.fr/hal-00942275" />
	</analytic>
	<monogr>
		<title level="m" coord="9,220.48,163.51,256.21,7.86">ICMR -ACM International Conference on Multimedia Retrieval</title>
		<meeting><address><addrLine>Glasgow, Royaume-Uni</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-02">Feb 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,185.43,342.23,7.86;9,146.91,196.39,333.66,7.86;9,146.91,207.35,311.78,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,393.98,185.43,86.59,7.86;9,146.91,196.39,265.49,7.86">Lost in quantization: Improving particular object retrieval in large scale image databases</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,432.50,196.39,48.07,7.86;9,146.91,207.35,283.11,7.86">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
