<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,189.86,115.90,238.01,12.90;1,183.61,133.83,248.13,12.90">Overview of CLEF NEWSREEL 2014: News Recommendation Evaluation Labs</title>
				<funder ref="#_GeActet">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_F9bPneJ">
					<orgName type="full">German Federal Ministry for Economic Affairs and Energy</orgName>
				</funder>
				<funder ref="#_RaZWeaW">
					<orgName type="full">European Union</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,150.15,171.95,61.16,8.64"><forename type="first">Benjamin</forename><surname>Kille</surname></persName>
							<email>kille@dai-labor.de</email>
							<affiliation key="aff0">
								<orgName type="department">DAI-Labor</orgName>
								<orgName type="institution">Technische Universität Berlin</orgName>
								<address>
									<addrLine>Ernst-Reuter-Platz 7</addrLine>
									<postCode>D-10587</postCode>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,220.76,171.95,53.16,8.64"><forename type="first">Torben</forename><surname>Brodt</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">plista GmbH</orgName>
								<address>
									<addrLine>Torstr. 33-35</addrLine>
									<postCode>D-10119</postCode>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,283.37,171.95,55.37,8.64"><forename type="first">Tobias</forename><surname>Heintz</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">plista GmbH</orgName>
								<address>
									<addrLine>Torstr. 33-35</addrLine>
									<postCode>D-10119</postCode>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,348.19,171.95,74.37,8.64"><forename type="first">Frank</forename><surname>Hopfgartner</surname></persName>
							<email>hopfgartner@dai-labor.de</email>
							<affiliation key="aff0">
								<orgName type="department">DAI-Labor</orgName>
								<orgName type="institution">Technische Universität Berlin</orgName>
								<address>
									<addrLine>Ernst-Reuter-Platz 7</addrLine>
									<postCode>D-10587</postCode>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,432.01,171.95,33.20,8.64;1,242.71,183.91,51.47,8.64"><forename type="first">Andreas</forename><surname>Lommatzsch</surname></persName>
							<email>lommatzsch@dai-labor.de</email>
							<affiliation key="aff0">
								<orgName type="department">DAI-Labor</orgName>
								<orgName type="institution">Technische Universität Berlin</orgName>
								<address>
									<addrLine>Ernst-Reuter-Platz 7</addrLine>
									<postCode>D-10587</postCode>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,320.51,183.91,47.87,8.64"><forename type="first">Jonas</forename><surname>Seiler</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">plista GmbH</orgName>
								<address>
									<addrLine>Torstr. 33-35</addrLine>
									<postCode>D-10119</postCode>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,189.86,115.90,238.01,12.90;1,183.61,133.83,248.13,12.90">Overview of CLEF NEWSREEL 2014: News Recommendation Evaluation Labs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1784F0DD9C3A2ED46B87DC9842600E58</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>recommender systems</term>
					<term>news</term>
					<term>on-line evaluation</term>
					<term>living lab</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper summarises objectives, organisation, and results of the first news recommendation evaluation lab (NEWSREEL 2014). NEWSREEL targeted the evaluation of news recommendation algorithms in the form of a campaignstyle evaluation lab. Participants had the chance to apply two types of evaluation schemes. On the one hand, participants could apply their algorithms onto a data set. We refer to this setting as off-line evaluation. On the other hand, participants could deploy their algorithms on a server to interactively receive recommendation requests. We refer to this setting as on-line evaluation. This setting ought to reveal the actual performance of recommendation methods. The competition strived to illustrate differences between evaluation with historical data and actual users. The on-line evaluation does reflect all requirements which active recommender systems face in practise. These requirements include real-time responses and large-scale data volumes. We present the competition's results and discuss commonalities regarding participants' approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The spectrum of available news continuously grows as news publishers keep producing news items. At the same time, we observe publishers shifting from pre-dominantely print media towards on-line news outlets. These on-line news portals confront users with the choice between numerous news items inducing an information overload. Readers struggle to detect relevant news items in the continuous flow of information. Therefore, operators of news portals have established systems to support them <ref type="bibr" coords="1,413.97,572.68,10.79,8.64" target="#b1">[2]</ref>. The support includes personalisation, navigation, context-awareness, and news aggregation.</p><p>CLEF NEWSREEL focuses on support through (personalised) content selection in form of news recommendations. We assume that users benefit as news portals adapt to current trends, news' relevancy, and individual tastes. News recommendation partially includes enhanced navigation as well as context-awareness. Recommended news items serves as a mean to quickly navigate to relevant contents. Thus, users avoid returning to the home page to continue consuming news. In addition, news recommender systems may take advantage of contextual factors. These factors include time, locality, along with trends.</p><p>Within NEWSREEL participants ought to find recommendation algorithms suggesting news items for a variety of news portals. These news portals cover several domains including general news, sports, and information technology. All news portals provide pre-dominantely German news articles. Consequently, approximately 4 out of 5 visitors' browsers carry location identifiers pointing to Germany, Austria, or Switzerland, respectively. The goal of the lab was to let participants determine which of these factors play an important role when recommending news items. The remainder of this paper is organised as follows. Section 2 describes the two tasks and their evaluation methodology. Section 3 summarises the results of the lab and discusses difficulties reported by participants. Section 4 concludes the paper and gives an outlook on how we attempt to continue evaluating news recommendation algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Lab Setup</head><p>CLEF NEWSREEL consisted of two tasks. For Task 1 we provided a data set containing recorded interactions with news portals. We refer to Task 1 as off-line evaluation. In addition, participants could deploy their recommendation algorithms in a living lab for Task 2. We refer to Task 2 as on-line evaluation and to the living lab platform as the Open Recommendation Platform (ORP) <ref type="foot" coords="2,267.39,363.67,3.49,6.05" target="#foot_0">3</ref> . ORP is operated by plista <ref type="foot" coords="2,376.19,363.67,3.49,6.05" target="#foot_1">4</ref> , a company that provides content distribution as well as targeted advertising services for a variety of websites. We dedicate a section to each task describing the goal and evaluation methodology. The reader is refered to <ref type="bibr" coords="2,212.22,401.20,16.60,8.64" target="#b9">[10]</ref> for a detailed overview of the setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task 1: Off-line Evaluation</head><p>Task 1 mirrors the paradigm of formerly held recommendation challenges such as the Netflix Prize challenge (cf. <ref type="bibr" coords="2,245.05,461.92,10.65,8.64" target="#b0">[1]</ref>). As part of the challenge, Netflix released a collection of movie ratings. Participants had to predict ratings for unknown (user, item)-pairs in a hold-out evaluation set. Analoguously, we split a collections of interaction with news items in training and test partitions. The initial data set has been described in <ref type="bibr" coords="2,462.87,497.79,15.58,8.64" target="#b10">[11]</ref>. Netflix could split their data randomly. This is due to the underlying assumption that movie preferences remain constant over time. In other words, users will continue to (dis-)like movies they once (dis-)liked. In contrast, we refrain to assume that users will enjoy reading news articles they once read. Conversely, we suppose that news' relevancy decreases relatively quickly. Thus, we relinquished to randomly select interactions for evaluation. Instead, we randomly selected time frames which we completely removed from the data set. We avoided a moving time-window approach, as this would have meant to release the entire data collection. We considered 3 parameters for the randomised sampling:</p><p>-Portal specificity -Interval width -Interval frequency Portal specificity refers to the choice between using identical time intervals for all news portals and having portal-specific intervals. The former alternative lets us treat all portals in the same way. On the other hand, the latter alternative provides a setting where participants may utilise information from other sources -i.e., other news portals -which better reflects the situation actual news recommenders reflect. For instance, articles targeting a certain event may have been published on some news portals already. The complementary portals could use interactions with these articles to boost their own articles. We decided to sample portal-specific time slots for evaluation. Selecting a suited interval width represents a non-trivial task. Choosing the width too small will result in an insufficient amount of evaluation data. Conversely, setting the width too large will entail a rather high number of articles as well as users missing in the training data. Additionally, the amount of interactions varies over the day and the week. For instance, we observe considerably fewer interaction in the night than in day times. We decided not to keep the interval width fixed, since we expected that this would remedy coincidental bad choices. Thus, we varied the interval width in the set {30, 60, 120, 180, 240} minutes. We observed that recommendation algorithms will struggle to provide adequate suggestion based on training data that lacks the most recent 4 hours <ref type="bibr" coords="3,360.43,343.80,15.24,8.64" target="#b14">[15]</ref>. This is mainly due to the rapidly evolving character of news. Moreover, we observe that news portals continue to provide new items which attract a majority of readers. The intial raw data covers a time span of about 1 month. We faced the decision on how many time slots to remove for evaluation. We had to avoid removing data as extensively as leaving insufficient data for training. Conversely, we strived to obtain expressive results. We decided to sample approximately 15 time slots per news portal. Thus, we expected to extract evaluation data about every second day. We noticed some time slots overlapped by chance. We decided to merge both time slots and refrained from resampling. Algorithm 1 outlines the sampling procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Sampling Procedure</head><p>Input: set of p news portals P , set of w interval widths W , number of samples s T ← ∅ set to contain the sample result function SAMPLE(P, W, s)</p><formula xml:id="formula_0" coords="3,157.18,542.46,111.78,30.04">for i ← 1top do for j ← 1tos do T ← T ∪ random(t, w)</formula><p>randomly choose a time point t and interval width w end for end for return T end function</p><p>Having created data for training and testing, we yet have to determine an evaluation metric. Literature on recommender systems' evaluation provides a rich set of metrics.</p><p>Metrics relating to rating prediction accuracy and item ranking are among the most popular choices. Hereby, root mean squared error (RMSE) and mean absolute error (MAE) are frequently used for the former evaluation setting (cf. <ref type="bibr" coords="4,405.79,143.22,11.01,8.64" target="#b7">[8,</ref><ref type="bibr" coords="4,418.73,143.22,7.19,8.64" target="#b8">9]</ref>). Supporters of ranking-oriented evaluation favour metrics such as precision/recall <ref type="bibr" coords="4,418.55,155.18,10.79,8.64" target="#b4">[5]</ref>, normalised discounted cumulative gain <ref type="bibr" coords="4,244.38,167.13,14.98,8.64" target="#b15">[16]</ref>, or mean reciprocal rank <ref type="bibr" coords="4,361.28,167.13,14.98,8.64" target="#b13">[14]</ref>. Rating prediction as well as ranking-based evaluation require preferences with graded relevancy as input. However, users do not tend to rate news articles. Thus, we cannot apply rating prediction metrics. Also, we cannot apply ranking metrics as we lack data about the pair-wise preference towards news items. Our data carry the signal of users interacting with news items. Thus, we ended up to define the evaluation metric based upon the ability to correctly predict whether an interaction will occur.</p><p>Let the pair (u, i) denote user u reading news item i included in the evaluation data. We challenged participants to select the 10 items each previously observed user would interact with in each evaluation time slot. The choice of exactly 10 items to suggest may appear arbitrary. We observe a majority of users interacting with only few items. Thus, most of the suggestions are likely not correct. On the other hand, limiting the number of suggestions to very few items entails drawbacks as well. Imagine a user who actually reads five articles in a time slot contained in the evaluation data. Having participants suggesting 3 items, a recommender predicting all 5 interactions correctly will appear to perform on level with a recommender only predicting 3 interactions correctly. Thus, requesting many suggestions will provide more sensitivity. This sensitivity allows us to better differentiate the individual recommendation algorithms' performances. On the other hand, the included news portals do not provide more than 6 recommendations at a time. Hence, requesting substantially more recommendation will induce a setting which insufficiently reflects the actual use case. Thus, we opted for 10 suggestions which represents a reasonable trade-off between sensitivity and reflecting the actual scenario. Note that <ref type="bibr" coords="4,213.51,430.31,11.83,8.64" target="#b5">[6]</ref> found that 10 preferences typically suffice to provide adequate recommendation. Finally, we define the evaluation metric according to Equation 1:</p><formula xml:id="formula_1" coords="4,254.58,462.07,226.01,26.77">h = u∈U 10 j=1 I(u, i j ) 10|U |<label>(1)</label></formula><p>where h refers to the hitrate. I represents the indicator function returning 1 if the predicted interaction occurred and 0 otherwise. The denominator normalises the number of hits by the maximal number possible. Thus, the hitrate falls into the interval [0, 1]. Since most users will not exhibit 10 interactions in the evaluation time slot, we expect the hitrate to be closer to 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Task 2: On-line Evaluation</head><p>Task 2 follows an alternative paradigm compared to Task 1. Task 1 assures comparability of results. This is mainly due to the fact that all participants apply their algorithm onto identical data. Contrarily, Task 2 provides a setting where participants have to handle similar yet not identical data. The plista GmbH has established a living lab where researchers and practitioners can deploy their recommendation algorithms to interact with actual users. This approach allows us to observe the actual performance of recommendation methods. This means that our findings will reflect actual benefits for real users. Further, we are able to observe variations throughout time and conduct studies on large scale as we record more and more data. Conversely, evaluation on recorded data expresses how a method would have performed. The approach does entail some disadvantages as well. Participants had to deal with technical requirements including response times, scalability, and availability. Deployed systems faced numerous requests which they had to reply to in at most 100ms. This response time restriction represents a particular challenge for participants located far from Germany where the ORP servers are located. Network latencies might further reduce the available response time. We offered virtual machines to participants who either had no servers at their disposal or suffered from high network latency. As a result, these requirements allowed us to verify how well certain recommendation algorithms adapt to real-world settings.</p><p>We asked participants to deploy their recommendation algorithm to a server. Subsequently, they connected the server to ORP which forwarded recommendation requests. Widgets on the individual news portals' website displayed the suggested news items to users. ORP tracks success in terms of clicks. This opens up several ways to evaluate participants' performances. One option is to consider the number of clicks. Considering the relative number of clicks by requests represents another option. Industry refers to this metric as click-through-rate. Given a comparable number of requests, both quantities coincide. In situations with varying number of requests, evaluation becomes tricky. Considering the total number of clicks may bias the evaluation in favour of the participant with more clicks. Conversely, considering the relative number of clicks per requests may favour teams with few requests. We want to evaluate the performance of a recommendation algorithm. ORP provides all participants with the chance to obtain similar number of requests. We decided to consider the absolute number of clicks as decisive criteria. Nevertheless, we additionally present the relative number of clicks per requests in our evaluation.</p><p>Baselines support comparing the relative performance of algorithms. We deployed a baseline which is detailed in <ref type="bibr" coords="5,247.75,454.06,14.96,8.64" target="#b12">[13]</ref>. The baseline combines two important factors for news recommendation: popularity and recency. We consider a fixed number of interactions that most recently occurred. Our baseline recommends news items included in this list that users had not previously seen. Consequently, we obtain a computationally efficient method that inherently considers popularity and recency.</p><p>We realised the participants' need to tune their algorithms. For this reason, we explicitly defined 3 evaluation periods during which performances would be logged. Participants could improve their algorithms before as well as in between the periods. We set the 3 periods to 7-23 February, 1-14 April, and 27-31 May.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>In this section, we detail results of CLEF NEWSREEL 2014. We start by giving some statistics about the participation in general. Then, we discuss the results for both tasks. Note that we unfortunately did not receive any submissions for Task 1. We provide some considerations about reasons for this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Participation</head><p>51 participants registered for Task 1. 52 participants registered for Task 2. Thereof, no participant submitted a solution for Task 1. We observed 13 active participants for Task 2. Note that participants had the chance to contribute several solutions for Task 2. 4 participants submitted a working notes paper to the CLEF proceedings <ref type="bibr" coords="6,418.58,266.96,10.58,8.64" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation of Task 1</head><p>We have not received any submissions for Task 1. Thus, we cannot report any results on how well the future interactions could be predicted. We can think of several reasons which may have prevented participants from submitting results. First, the data set exhibits a large volume of more than 60GB. Thus, we required participants to process such volumes. Participants' available computational resources may not have allow to iteratively optimise their recommendation algorithms for this amount of data. Second, we imagine that participants might have preferred Task 2 over Task 1. This preferences may be due to the interactive character as well as the rather unique chance to evaluate algorithms with actual users' feedback. We admit that there are rather plenty of data set driven competition. For instance, the online platform www.kaggle.com offers a variety of data sets. Finally, the restriction to German news articles might have prevented participants who attempted to evaluate content-based approach but do not speak German.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation of Task 2</head><p>Throughout the pre-defined evaluation periods, we observed 13 active participants on the ORP. Unfortunately, the component recording the performance failed twice. Thus, we did not receive data for the times between 7-12 February and 27-31 May. None of the teams were active in all periods. This illustrates the technical requirements which participants faced. ORP automatically disables the communication with participants in case their servers do not respond in time. ORP tries to re-establish the communication. We noticed that the re-establishing has not succeeded in all occasions. We allowed participants to simultaneously deploy several algorithms. Some participants used this more extensively than other did. Table <ref type="table" coords="6,218.67,596.66,4.93,8.64">1</ref> shows the results for the evaluation periods 7-23 February, 1-14 April, and 27-31 May. We list the number of clicks, requests and their ratio for each algorithm which was active during the period. We note that the number of requests does vary between algorithms. Algorithm AL gathered the most clicks in periods 2 and 3 as well as the second most clicks in the first period. Note that the baseline constantly appears under the five best performing algorithms. This indicates that popularity and recency represent two important factors when recommending news. Additionally, the baseline provides low computational complexity such that it is able to reply to a large fraction of requests. Table <ref type="table" coords="7,238.35,143.22,4.88,8.64">2</ref> aggregates the results per participant. The aggregated results confirm our impressions from the algorithm-level.</p><p>In addition to the overall figure, we investigate whether particular algorithms perform exceptionally well in specific contexts. Context refers either to specific news portals or daytimes. News portals offer varying contents. For instance, www.sport1.de is dedicated to sports-related news while www.gulli.com provides news on information technology. Thus, we look at the performance of individual algorithm with respect to specific publishers. Likewise, we investigate algorithms' performances throughout the day. We suppose that different types of users consume news at varying hours of the day. For instance, users reading news early in the morning may have other interests than users reading late in the evening. This matches with the findings of <ref type="bibr" coords="7,425.79,262.77,15.42,8.64" target="#b17">[18]</ref>. Figure <ref type="figure" coords="7,476.32,262.77,5.03,8.64" target="#fig_0">1</ref> shows a heatmap relating algorithms with the publisher and hour of day. We note that few algorithms perform on comparable levels for all publishers and throughout the day. This indicates that combining several recommendation algorithms in an ensemble yields potential to obtain better performance.</p><p>We do not know details to all recommendation algorithms. The participants who submitted their ideas in form of working notes used different ideas. Most systems carried a fall-back solution in terms of most-popular and/or most-recent strategies. Additionally, participants contributed more sophisticated algorithms. These algorithms included association rules <ref type="bibr" coords="7,244.78,370.37,15.58,8.64" target="#b11">[12]</ref>, content-based recommenders <ref type="bibr" coords="7,390.63,370.37,10.79,8.64" target="#b3">[4]</ref>, and ensembles of different recommendation strategies <ref type="bibr" coords="7,283.56,382.33,10.79,8.64" target="#b6">[7]</ref>. Reportedly, more sophisticated methods had trouble dealing with the high volume of requests. In particular the peaking hours during lunch break were reportedly hard to handle. Our baseline method combines the notions of most-popular and most-recent recommendation. The evaluation shows that the baseline is hard to beat. This may be due to the technical restrictions rather than the recommendation quality. More sophisticated method which just miss the response time limit may provide better recommendations. Table <ref type="table" coords="10,155.91,297.47,3.42,7.77">2</ref>: Aggregated results for the participants by clicks, requests, and their ratio. Column 2 refers to publications detailing the algorithms if available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Team</head><p>Reference Clicks Requests CTR labor <ref type="bibr" coords="10,295.79,342.10,14.94,7.77" target="#b12">[13]</ref> 26,165 1,365,100 0.02 abc <ref type="bibr" coords="10,295.79,353.06,14.94,7.77" target="#b12">[13]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>CLEF NEWSREEL attempted to let participants evaluate their recommendation algorithms. Participants could evaluate their algorithm in two varying fashions. Task 1 offered a rich data set recorded through a one month period on 12 news portals. We removed time slots for evaluation purposes. Participants ought to predict which articles users would read during these held-out times. We received no contribution for this task. Task 2 enabled participants to evaluate their recommendation algorithms by interacting with actual users. Participants could deploy their algorithms on a server which subsequently received recommendation requests. This setting closely mirrors circumstances under which actual recommender systems operate. Participants struggled with the high volume of requests and the narrow response time limits. We observed that most-popular and most-recent approaches are hard to beat due to their low complexity.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="8,154.02,650.09,307.30,7.77;8,158.10,119.55,299.15,518.76"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Algorithms' click-through-rate grouped by time of day as well as by publisher.</figDesc><graphic coords="8,158.10,119.55,299.15,518.76" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="2,144.73,646.75,112.97,6.31"><p>http://orp.plista.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="2,144.73,657.93,91.45,6.31"><p>http://plista.com</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>The work leading to these results has received funding (or partial funding) from the <rs type="programName">Central Innovation Programme for SMEs</rs> of the <rs type="funder">German Federal Ministry for Economic Affairs and Energy</rs>, as well as from the <rs type="funder">European Union</rs>'s <rs type="programName">Seventh Framework Programme</rs> (<rs type="grantNumber">FP7/2007-2013</rs>) under grant agreement number <rs type="grantNumber">610594</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_F9bPneJ">
					<orgName type="program" subtype="full">Central Innovation Programme for SMEs</orgName>
				</org>
				<org type="funding" xml:id="_RaZWeaW">
					<idno type="grant-number">FP7/2007-2013</idno>
					<orgName type="program" subtype="full">Seventh Framework Programme</orgName>
				</org>
				<org type="funding" xml:id="_GeActet">
					<idno type="grant-number">610594</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Table <ref type="table" coords="9,155.95,127.60,3.42,7.77">1</ref>: Results of Task 2 grouped by the evaluation periods. We list the number of clicks, number of requests, and their ratio for all participating algorithms. Notice that the highest numbers of clicks per period are highlighted in bold font. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="11,142.61,424.53,280.03,7.93" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,249.06,424.68,58.15,7.77">The netflix prize</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lanning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,323.52,424.53,30.96,7.72">KDDCup</title>
		<imprint>
			<biblScope unit="page" from="3" to="6" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,435.89,337.97,7.77;11,150.53,446.70,290.59,7.93" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,265.49,435.89,83.81,7.77">Adaptive News Access</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Billsus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Pazzani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,215.95,446.70,62.82,7.72">The Adaptive Web</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Brusilovsky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Kobsa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Nejdl</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="550" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,458.06,338.19,7.77;11,150.95,468.87,330.75,7.93;11,150.95,479.98,20.17,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,335.28,458.06,106.03,7.77">Clef 2014 labs and workshops</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Halvey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kraajl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,188.48,468.87,114.16,7.72">CLEF 2014 Labs and Workshops</title>
		<title level="s" coord="11,308.80,468.87,169.14,7.93">Notebook Papers. CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>notebook papers</note>
</biblStruct>

<biblStruct coords="11,142.61,491.04,337.98,7.93;11,150.73,502.00,184.91,7.93" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,340.88,491.19,81.57,7.77">Uned @ clef-newsreel</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Castellanos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcia-Serrano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cigarran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,458.27,491.04,22.32,7.72;11,150.73,502.00,92.68,7.72">CLEF 2014 Labs and Workshops</title>
		<title level="s" coord="11,249.78,502.00,59.61,7.72">Notebook Papers</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,513.36,339.45,7.77;11,150.95,524.17,329.63,7.93;11,150.73,535.12,102.60,7.93" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,203.12,513.36,278.94,7.77;11,150.95,524.32,102.78,7.77">Performance of recommender algorithms on top-n recommendation tasks categories and subject descriptors</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cremonesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,269.55,524.17,211.03,7.72;11,150.73,535.12,26.80,7.72">Proceedings of the 2010 ACM Conference on Recommender Systems</title>
		<meeting>the 2010 ACM Conference on Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="39" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,546.49,339.54,7.77;11,150.95,557.29,250.91,7.93" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,296.67,546.49,182.27,7.77">User effort vs . accuracy in rating-based elicitation</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cremonesi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Milano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Turrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,160.66,557.29,165.40,7.72">6th ACM Conferene on Recommender Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,568.66,337.97,7.77;11,150.95,579.46,242.46,7.93" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,294.23,568.66,186.35,7.77;11,150.95,579.62,16.35,7.77">An analysis of recommender algorithms for online news</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Doychev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lawor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Rafter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,184.34,579.46,116.84,7.72">CLEF 2014 Labs and Workshops</title>
		<title level="s" coord="11,307.55,579.46,59.62,7.72">Notebook Papers</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,590.68,337.98,7.93;11,150.95,601.63,191.66,7.93" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,215.83,590.83,231.07,7.77">A survey of accuracy evaluation metrics of recommendation tasks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gunawardana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,453.42,590.68,27.17,7.72;11,150.95,601.63,109.05,7.72">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="2935" to="2962" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,613.00,337.97,7.77;11,150.95,623.80,258.05,7.93" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,365.44,613.00,115.14,7.77;11,150.95,623.96,78.34,7.77">Evaluating collaborative filtering recommender systems</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Herlocker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">G</forename><surname>Terveen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">T</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,236.36,623.80,102.05,7.72">ACM Trans. Inf. Syst. (TOIS)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="53" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,635.17,339.83,7.77;11,150.95,645.97,329.63,7.93;11,150.66,656.93,245.95,7.93" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,438.50,635.17,43.57,7.77;11,150.95,646.13,144.07,7.77">Benchmarking news recommendations in a living lab</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hopfgartner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kille</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lommatzsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Plumbaum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brodt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Heintz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,310.24,645.97,170.35,7.72;11,150.66,656.93,121.29,7.72">CLEF&apos;14: Proceedings of the Fifth International Conference of the CLEF Initiative</title>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="12,142.24,119.81,338.35,7.93;12,150.80,130.77,270.15,7.93" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,332.57,119.96,62.90,7.77">The plista dataset</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kille</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hopfgartner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brodt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Heinzt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,412.35,119.81,68.24,7.72;12,150.80,130.77,243.84,7.72">Proceedings of the International News Recommender Systems Workshop and Challenge</title>
		<meeting>the International News Recommender Systems Workshop and Challenge</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,141.73,338.35,7.93;12,150.21,152.69,129.36,7.93" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,239.16,141.88,148.08,7.77">Inbeat: Recommender system as a service</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kuchar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kliegr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,403.56,141.73,77.03,7.72;12,150.21,152.69,37.14,7.72">CLEF 2014 Labs and Workshops</title>
		<title level="s" coord="12,193.71,152.69,59.61,7.72">Notebook Papers</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,163.80,338.34,7.77;12,150.41,174.60,232.85,7.93" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,220.24,163.80,241.34,7.77">Real-time news recommendation using context-aware ensembles</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lommatzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,150.41,174.60,122.38,7.72">Advances in Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="51" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,185.72,338.34,7.77;12,150.95,196.52,329.64,7.93;12,150.28,207.63,56.04,7.77" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,421.53,185.72,59.05,7.77;12,150.95,196.67,256.19,7.77">Climf : Learning to maximize reciprocal rank with collaborative less-is-more filtering</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Baltrunas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanjalic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,428.19,196.52,24.16,7.72">RecSys</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="139" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,218.59,339.83,7.77;12,150.95,229.55,329.77,7.77;12,150.95,240.36,330.75,7.93;12,150.95,251.47,20.17,7.77" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,310.45,229.55,170.28,7.77;12,150.95,240.51,26.27,7.77">Workshop and challenge on news recommender systems</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tavakolifard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Gulla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">C</forename><surname>Almeroth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hopfgartner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kille</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Plumbaum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lommatzsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Brodt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bucko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Heintz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,192.72,240.36,228.57,7.72">Proceedings of the 7th ACM conference on Recommender systems</title>
		<meeting>the 7th ACM conference on Recommender systems</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="481" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,262.43,338.49,7.77;12,150.95,273.23,330.76,7.93;12,150.95,284.35,57.27,7.77" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,239.85,262.43,240.88,7.77;12,150.95,273.39,27.34,7.77">Rank and relevance in novelty and diversity metrics for recommender systems</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Castells</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,185.91,273.23,292.18,7.72">Proceedings of the fifth ACM conference on Recommender systems -RecSys &apos;11</title>
		<meeting>the fifth ACM conference on Recommender systems -RecSys &apos;11</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page">109</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,295.30,339.83,7.77;12,150.95,306.11,278.81,7.93" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="12,266.33,295.30,215.73,7.77;12,150.95,306.26,53.10,7.77">Optimizing and evaluating stream-based news recommendation algorithms</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Werner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lommatzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,220.69,306.11,116.84,7.72">CLEF 2014 Labs and Workshops</title>
		<title level="s" coord="12,343.90,306.11,59.62,7.72">Notebook Papers</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,317.22,338.34,7.77;12,150.95,328.03,329.63,7.93;12,150.58,338.99,200.15,7.93" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="12,339.59,317.22,140.98,7.77;12,150.95,328.18,182.49,7.77">When to recommend what? a study on the role of contextual factors in ip-based tv services</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Marx</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Sivrikaya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hopfgartner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,349.76,328.03,130.83,7.72;12,150.58,338.99,95.59,7.72">MindTheGap&apos;14: Proceedings of the MindTheGap&apos;14 Workshop</title>
		<imprint>
			<publisher>CEUR</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="12" to="18" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
