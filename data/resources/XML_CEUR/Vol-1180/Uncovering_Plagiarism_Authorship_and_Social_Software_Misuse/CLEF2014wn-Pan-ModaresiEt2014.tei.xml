<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,141.78,115.90,331.79,12.90;1,245.72,133.83,123.92,12.90;1,223.43,153.68,168.49,10.75">A Language Independent Author Verifier Using Fuzzy C-Means Clustering Notebook for PAN at CLEF 2014</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,225.12,190.08,75.95,8.64"><forename type="first">Pashutan</forename><surname>Modaresi</surname></persName>
							<email>modaresi@cs.uni-duesseldorf.de</email>
							<affiliation key="aff0">
								<orgName type="institution">pressrelations GmbH</orgName>
								<address>
									<settlement>Düsseldorf</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computer Science</orgName>
								<orgName type="institution">Heinrich-Heine-University of Düsseldorf</orgName>
								<address>
									<settlement>Düsseldorf</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,331.24,190.08,54.53,8.64"><forename type="first">Philipp</forename><surname>Gross</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">pressrelations GmbH</orgName>
								<address>
									<settlement>Düsseldorf</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,141.78,115.90,331.79,12.90;1,245.72,133.83,123.92,12.90;1,223.43,153.68,168.49,10.75">A Language Independent Author Verifier Using Fuzzy C-Means Clustering Notebook for PAN at CLEF 2014</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">80C7294A694CD9301958FCC84E8AE7E3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work we describe our approach to solve the author verification problem introduced in the PAN 2014 Author Identification task. The author verification task presents participants with a set of problems where each problem consists of a set of documents written by the same author and a questioned document with an unknown author. The task is then to decide whether the questioned document has the same author as the other documents or not. Inspired by a psychological personality model, our approach uses basic lexical feature extraction and fuzzy clustering. Using the created fuzzy clusters, the membership values of documents to the clusters can be computed. The distribution of the cluster membership values will be used finally to solve the verification problem.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Given a set of documents with known authors, authorship attribution is the task of identifying the author of an unseen document. Having a small number of candidate authors, this task can be easily solved using the state-of-the-art approaches <ref type="bibr" coords="1,401.14,465.15,12.63,8.64" target="#b0">[1]</ref>. A realistic and common scenario for authorship attribution is the author verification problem. Given a set of documents written by a single author, the task here is to determine whether a questioned document is written by the same author or not.</p><p>The PAN 2014 Author Identification task focuses on the author verification problem. To be more specific, in this task a multi-lingual corpus is provided which consists of several problems. Each problem contains a maximum of 5 documents written by a single author and a questioned document by an unknown author. The task in then to determine whether the questioned document is written by the same author or not.</p><p>The fact that an author may consciously or unconsciously vary his or her writing style, makes the task of author verification a hard problem <ref type="bibr" coords="1,359.62,584.71,12.90,8.64" target="#b6">[7]</ref>. In this paper we introduce a novel approach for solving the task of author verification. For this we extract language independent features from our training corpus and use a fuzzy clustering algorithm to construct our models. Finally using the membership distribution of documents over the clusters, we do solve the verification task.</p><p>In Section 2 we define the problem of author verification formally and introduce some notations. Section 3 addresses the process of feature extraction and normalization.</p><p>The process of clustering and model construction is discussed in Section 4. Section 5 covers the process of verification and scoring. An overview of the evaluation results can be seen in Section 6. Finally in Section 7 the work will be concluded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Statement</head><p>In this section we formally define the problem of author verification in the context of the PAN 2014 Author Identification task.</p><p>Let P = {D, d u } be a problem consisting of a set of documents D = {d 1 , . . . , d n } with 1 ≤ n ≤ 5 written by a single author, and a questioned document d u with an unknown author. The task in author verification is to determine whether the questioned document d u is written by the same author or not. We denote the author of a document d i by A(d i ). In other words an author verifier ϕ is a binary classification function of the following form:</p><formula xml:id="formula_0" coords="2,212.48,320.62,268.11,23.30">ϕ(d u , D) = 1, if A(d u ) = A(d i ) ∀d i ∈ D 0, if otherwise<label>(1)</label></formula><p>In the PAN 2014 Author Identification task, problems are from 4 different languages, namely Dutch, English, Greek and Spanish. The author verification algorithm has to be able to deal with documents from the specified languages. The performance of the author verifier will be evaluated according to the area under the ROC curve (AUC) of its probability scores and also based on the c@1 measure <ref type="bibr" coords="2,366.96,407.95,12.90,8.64" target="#b7">[8]</ref>. The evaluation process will be discussed in more details in Section 6.</p><p>In the following section, we start the description of our algorithm by discussing the feature extraction and normalization step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Feature Extraction and Normalization</head><p>Feature extraction is considered as one of the important steps in author verification <ref type="bibr" coords="2,465.72,511.69,11.15,8.58" target="#b8">[9]</ref>. Different kinds of stylometric features like lexical, syntactic or semantic features have been used for solving the author verification task. In order to design an efficient author verification algorithm, which can deal with huge amounts of documents, we only consider a limited number of lexical features and construct our learning algorithm in a way that would result in an acceptable performance even with a small number of features. Lexical features have the advantage over the syntactic or semantic features, that this kind of features can be computed very efficiently and without the use of any external knowledge or training.</p><p>We represent documents as vectors in R 4 . Each component of these 4-dimensional vectors can be computed using the feature extraction functions. Independent of the document language we use the following functions to compute the feature vector components of documents:</p><p>Average Sentence Length (f sl ) : Using a sentence detector, sentence boundaries of the document will be detected (In our case we use a regular expression based sentence detector for optimizing the performance). For each sentence s in the document, its length l(s) will be computed. We denote the set of all sentences inside a document with S. Finally the average sentence length of the document can be computed as follows:</p><formula xml:id="formula_1" coords="3,266.19,187.56,214.40,22.97">f sl (d) = s∈S l(s) |S|<label>(2)</label></formula><p>Punctuation Marks Usage (f pm ) : Using a predefined set of punctuation marks T = { ( ) , : ; ! ? } the frequency of the elements of the set T inside the document will be computed and finally normalized by the length of the document. With f (t, d) we denote the frequency of the punctuation mark t in document d.</p><formula xml:id="formula_2" coords="3,258.23,281.18,222.36,22.97">f pm (d) = t∈T f (t, d) |d|<label>(3)</label></formula><p>Space After Comma (f sac ) : Our experimental results show that whether a space is used after a comma or not, can be a good discriminating feature in the author verification task. Let α denote the number of times a comma is followed by a space and β be the number of times a comma is not followed by a space. In his way f sac can be defined as follows:</p><formula xml:id="formula_3" coords="3,272.49,374.25,208.10,22.31">f sac (d) = α -β |d|<label>(4)</label></formula><p>Analogue to f sac we define f sbc which is the Space Before Comma feature. Through this feature, authors that use a space before comma can be discriminated from the ones who do not use a space before comma. As the extracted features may exhibit significant differences in their range and distribution, out learning algorithm could be more sensitive to features that are in a wider range (e.g. Average Sentence Length). In order to avoid this behavior we use feature normalization through which we can modify the mean and variance of the features using a transformation function. The transformation function that we use in this work is the min-max function. Given a feature f , the min-max transformation function which is defined as follows:</p><formula xml:id="formula_4" coords="3,255.33,520.90,225.26,22.34">f = f -min(f ) max(f ) -min(f )<label>(5)</label></formula><p>In the above formula f denotes the feature vector and f is the transformed feature vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Fuzzy Clustering and Model Construction</head><p>In this section we illustrate the main idea behind our learning algorithm. We believe that different personality dimensions have a close relationship with the writing style of authors. In psychology, the Big Five Personality Traits are 5 dimensions of personality that are used to describe the personality of humans <ref type="bibr" coords="3,338.76,656.44,13.53,8.64" target="#b2">[3]</ref>. Openness, Conscientiousness, Extraversion, Agreeableness and Neuroticism are the personality dimensions which are described as the factors of the Big Five model. Based on these dimensions, each persons personality can be described using a combination of the above dimensions. Inspired by the Big Five model, we construct c clusters, where each cluster represents a personality dimension. An author's personality can then be determined by computing his or her membership to these clusters. Finally two authors that have the same (or similar) membership distribution over the clusters would be considered as the same.</p><p>For this we collect all the documents in our training set from which we know that they are written by the same author and extract their features (See Section 3). This will result in a matrix Z = [z tr 1 , z tr 2 , . . . , z tr N ] ∈ R 4×N where N is the number of collected documents and z tr i denotes the transpose of the vector z i . As already mentioned the personality of an author can be determined using his or her membership values to the available clusters. Due to this consideration, we use the Fuzzy C-Means <ref type="bibr" coords="4,412.31,262.60,14.44,8.58" target="#b1">[2]</ref> clustering algorithm to construct fuzzy clusters. For constructing c clusters, we assign initial cluster membership values for each document in the collection (The collection of these values constructs the partition matrix U = [µ ik ] ∈ R c×N ). The partition matrix will be updated after each iteration of the algorithm until no significant changes are observable. After initializing the partition matrix randomly, the Fuzzy C-Means algorithm can be summarized as follows:</p><p>Repeat for l = 1, 2, . . .</p><formula xml:id="formula_5" coords="4,151.09,380.61,325.63,50.45">Step 1: Compute the cluster centers with m ∈ [1, ∞) v (l) i = N k=1 (µ (l-1) ik ) m z k N k=1 (µ (l-1) ik ) m , 1 ≤ i ≤ c (<label>6</label></formula><formula xml:id="formula_6" coords="4,476.72,410.91,3.87,8.64">)</formula><p>Step 2: Compute the distances</p><formula xml:id="formula_7" coords="4,160.67,460.87,319.92,17.89">D 2 ik = z k -v (l) i 2 = (z k -v (l) i ) T (z k -v (l) i ), 1 ≤ i ≤ c, 1 ≤ k ≤ N<label>(7)</label></formula><p>Step 2: Update the partition matrix:</p><formula xml:id="formula_8" coords="4,164.91,516.69,315.68,66.63">for 1 ≤ k ≤ N if D ik &gt; 0 for all i = 1, 2, . . . , c µ (l) ik = 1 c j=1 (D ik /D jk ) 2/(m-1)<label>(8)</label></formula><p>otherwise</p><formula xml:id="formula_9" coords="4,139.75,611.82,340.85,53.18">µ (l) ik = 0 if D ik &gt; 0, and µ (l) ik ∈ [0, 1] with c i=1 µ (l) ik = 1 (9) Until U (l) -U (l-1) &lt; ϕ(d u , D) = 1, if S μ,µu ≥ 0.5 0, if otherwise (12)</formula><p>Using the above function definition, for each problem P it can be decided if the documents inside P belong to the same author or not. A value of 1 means that the documents inside P have the same author and a value of 0 means that the questioned document has a different author than the documents with a known author.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation Results</head><p>In order to evaluate our approach we used the training set provided by the PAN 2014 Author Identification task. The training set consists of documents belonging to 4 different languages, namely Dutch, English, Greek and Spanish. Dutch documents are divided into essays and reviews, and English documents into essays and novels. Greek and Spanish documents belong only to the genre Articles. In total we constructed 6 models, where each model corresponds to a specific language and a specific genre.</p><p>For constructing the clusters of language L and genre G, we randomly selected 20% of the available training data to create the clusters. The experiments have been repeated 1000 times and average c@1 measure of the iterations has been computed. The c@1 measure of a single iteration can be computed as follows <ref type="bibr" coords="6,358.95,375.76,12.01,8.64" target="#b7">[8]</ref>:</p><formula xml:id="formula_10" coords="6,253.79,395.38,226.80,22.31">c@1 = ( 1 n )(n c + (n u n c n ))<label>(13)</label></formula><p>where, n = number of problems, n c = number of correct answers and n u = number of unanswered problems. The results are summarized in Table <ref type="table" coords="6,384.65,435.53,3.74,8.64" target="#tab_0">1</ref>. In Table <ref type="table" coords="6,186.23,572.61,4.98,8.64" target="#tab_0">1</ref> the number of created clusters and the parameter m are also specified. These parameters are the ones that returned the best results during our experiments. As we can see the algorithm returns the best results for the English novels with an c@1 value of 0.852. The worst results are also for the English documents but the ones in the genre essays. Even though the c@1 values for all languages and genres are greater than 0.66.</p><p>Beside the above approach, we evaluate the performance of our algorithm according to the area under the ROC curve (AUC) <ref type="bibr" coords="6,294.80,656.44,14.53,8.64" target="#b3">[4]</ref> of its returned probability scores. Table <ref type="table" coords="6,475.62,656.44,4.98,8.64" target="#tab_0">1</ref> summarizes the results. As it can be seen in the table, the AUC values are consistent and comparable with c@1 values. The reason for this is that the verification algorithm outputs very high probability scores for the positive cases, and very low probability scores for the negative cases.</p><p>For ranking the performance of participants in the competition a test corpus has been provided. We have evaluated our algorithm using Tira <ref type="bibr" coords="7,376.71,189.01,12.04,8.58" target="#b4">[5]</ref> which is a service for running experiments in computer science. Table <ref type="table" coords="7,333.38,201.14,4.98,8.64" target="#tab_1">2</ref> represents the performance results and also the run-time of our algorithm on the test corpus.</p><p>From the performance results based on the test corpus it can be seen that our algorithm performs very well for English Novels, and Essays reaching a final score of 0.508 and 0.349 respectively. But for the other languages the results are not as satisfactory as expected. This difference between the results indicates that for languages other than English, a deeper feature engineering is needed. The run-time of our algorithm on different data sets also shows that the introduced algorithm can be efficiently used for large collections of author verification problems. This is due to the small number of features that we extract from documents. This has from one side the advantage that the author verification problems can be solved very efficiently, but from the other side, it will result in a lower performance for specific languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work we have described our approach to solve the author verification problem introduced in the PAN 2014 Author Identification task. Using the fuzzy c-means clustering algorithm, we partitioned the provided training set (Section 4) into several clusters. Given an author verification problem, we used the membership values of the documents inside the problem to verify whether two documents have the same author or not.</p><p>In order to design an efficient algorithm we only considered a limited number of features for each language. This resulted in very low run-times for our algorithm. Accordingly we acquired the 1 st place among the participants regarding the run-time of algorithms.</p><p>Our introduced approach also revealed sound results for the English language achieving the 1 st place for English Novels and the 5 th place for English Essays among the 13 participating teams. For other languages we did not get the expected satisfactory results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,189.13,456.35,229.78,99.15"><head>Table 1 :</head><label>1</label><figDesc>Evaluation results for the training set</figDesc><table coords="6,189.13,476.90,229.78,78.60"><row><cell cols="2">Language Genre</cell><cell cols="3">#Clusters m c@1 AUC c@1 • AUC</cell></row><row><cell>Dutch</cell><cell>Essays</cell><cell>4</cell><cell>4 0.731 0.752</cell><cell>0.549</cell></row><row><cell>Dutch</cell><cell>Reviews</cell><cell>3</cell><cell>4 0.680 0.763</cell><cell>0.518</cell></row><row><cell cols="2">English Essays</cell><cell>3</cell><cell>3 0.664 0.651</cell><cell>0.432</cell></row><row><cell cols="2">English Novels</cell><cell>4</cell><cell>3 0.852 0.852</cell><cell>0.725</cell></row><row><cell>Greek</cell><cell>Articles</cell><cell>4</cell><cell>3 0.671 0.697</cell><cell>0.467</cell></row><row><cell cols="2">Spanish Articles</cell><cell>3</cell><cell>5 0.684 0.712</cell><cell>0.487</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,177.62,293.79,255.63,97.09"><head>Table 2 :</head><label>2</label><figDesc>Evaluation results for the test set</figDesc><table coords="7,177.62,312.28,255.63,78.60"><row><cell cols="2">Language Genre</cell><cell cols="3">c@1 AUC c@1 • AUC Runtime (in seconds)</cell></row><row><cell>Dutch</cell><cell cols="2">Essays 0.635 0.594</cell><cell>0.377</cell><cell>4</cell></row><row><cell>Dutch</cell><cell cols="2">Reviews 0.500 0.493</cell><cell>0.246</cell><cell>6</cell></row><row><cell cols="3">English Essays 0.580 0.602</cell><cell>0.349</cell><cell>6</cell></row><row><cell cols="3">English Novels 0.715 0.711</cell><cell>0.508</cell><cell>7</cell></row><row><cell>Greek</cell><cell cols="2">Articles 0.540 0.543</cell><cell>0.293</cell><cell>4</cell></row><row><cell cols="3">Spanish Articles 0.650 0.640</cell><cell>0.416</cell><cell>7</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We use the cluster information produced by the cluster algorithm, to verify whether two documents are written by the same author or not. The process of author verification will be discussed in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Verification and Scoring</head><p>In order to find an answer to an author verification problem P , we compute the cluster membership values for documents with known authors and documents with unknown authors. Then using the membership values we will decide if the documents have the same author or not.</p><p>Given a problem P = {D = {d 1 , . . . , d n }, d u } and c cluster prototypes (centroids) V = {v 1 , . . , v c } we compute the membership values of the documents with known authors to the constructed clusters. In this way, for each document d i , 1 ≤ i ≤ n a cluster membership vector µ i = {µ i1 , . . . , µ ic } will be computed where the j-th element in the vectors represents the membership value of the document d i to the cluster j.</p><p>In the same way we compute the cluster membership values of the document with unknown author d u . This would result in the membership vector µ u . At this step the cluster membership values for all documents in the problem P are known. Notice that the documents d 1 , . . . , d n are assumed to be written by the same author. Theoretically we would expect that the cluster membership vectors of these documents look very similar to each other. Experimental results show that this is usually not the case, which relies on the fact the authors write in different psychological states.</p><p>In order to solve the above problem, for the documents with known authors, we compute a mean cluster membership vector. Through this vector a more stable estimation of membership to available personality dimensions can be made. The mean cluster membership vector of a set of documents d 1 , . . . , d n with known authors can be computed as follows:</p><p>Now using the cosine similarity between the average cluster membership vector of documents with known authors and the questioned document, the similarity between these two vectors can be computed. The cosine similarity between these two vectors is defined as follows <ref type="bibr" coords="5,205.39,543.70,12.01,8.64" target="#b5">[6]</ref>:</p><p>Through the cosine similarity measure we compute the angle between the vectors. A cosine values of 0 means that the vectors are orthogonal to each other and a cosine value of 1 means that the vectors are identical. Through the cosine similarity measure we assigned a score to each problem. Additionally we need a transformation function which can return binary values for author verification problem. In Section 2 we defined the function ϕ(d u , D). Here we modify this definition and redefine the function:</p><p>The reason for this lies in the small amount of training set that we use for constructing our fuzzy clusters. We also use the same set of features for all available languages which is probably the main reason for insufficient results for languages other than English.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="8,138.13,198.67,322.01,7.77;8,146.47,209.63,23.90,7.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,196.57,198.67,152.71,7.77">Scalability issues in authorship attribution</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Argamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,352.84,198.67,58.74,7.77">kim luyckx. LLC</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="95" to="97" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.13,220.58,337.49,7.77;8,146.47,231.54,145.17,7.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,265.18,220.58,165.13,7.77">FCM: The fuzzy c-means clustering algorithm</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bezdek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ehrlich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Full</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,436.26,220.58,39.35,7.77;8,146.47,231.54,54.03,7.77">Computers &amp; Geosciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="191" to="203" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.13,242.50,334.95,7.77;8,146.47,253.46,135.22,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,200.51,242.50,210.47,7.77">Personality Structure: Emergence of the Five-Factor Model</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Digman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,417.50,242.50,55.58,7.77;8,146.47,253.46,51.55,7.77">Annual Review of Psychology</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="417" to="440" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.13,264.42,314.42,7.77;8,146.47,275.38,108.82,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,190.86,264.42,222.53,7.77">Roc graphs: Notes and practical considerations for researchers</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Fawcett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,419.18,264.42,33.37,7.77">ReCALL</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2004">2004</date>
			<publisher>HPL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.13,286.34,331.25,7.77;8,146.47,297.30,313.35,7.77;8,146.47,308.26,309.62,7.77;8,146.47,319.21,304.52,7.77;8,146.47,330.17,159.46,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,159.43,297.30,197.32,7.77">Recent trends in digital text forensics and its evaluation</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Busse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,284.31,308.26,171.78,7.77;8,146.47,319.21,116.89,7.77">Information Access Evaluation. Multilinguality, Multimodality, and Visualization</title>
		<title level="s" coord="8,269.13,319.21,126.47,7.77">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Mãijller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Paredes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Rosso</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Stein</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">8138</biblScope>
			<biblScope unit="page" from="282" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.13,341.13,326.25,7.77;8,146.47,352.09,100.36,7.77" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<title level="m" coord="8,176.35,341.13,140.15,7.77">Data Mining: Concepts and Techniques</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.13,363.05,321.56,7.77;8,146.47,374.01,317.26,7.77;8,146.47,384.97,166.86,7.77" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,229.22,363.05,214.24,7.77">Authorship verification as a one-class classification problem</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,146.47,374.01,282.04,7.77;8,146.47,384.97,35.30,7.77">Proceedings of the Twenty-first International Conference on Machine Learning</title>
		<meeting>the Twenty-first International Conference on Machine Learning<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">62</biblScope>
		</imprint>
	</monogr>
	<note>ICML &apos;04</note>
</biblStruct>

<biblStruct coords="8,138.13,395.93,325.84,7.77;8,146.47,406.89,323.25,7.77;8,146.47,417.84,298.23,7.77;8,146.47,428.80,151.51,7.77" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,232.39,395.93,146.56,7.77">A simple measure to assess non-response</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rodrigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,397.24,395.93,66.73,7.77;8,146.47,406.89,323.25,7.77;8,146.47,417.84,52.91,7.77;8,298.84,417.84,29.49,7.77">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1415" to="1424" />
		</imprint>
	</monogr>
	<note>HLT &apos;11</note>
</biblStruct>

<biblStruct coords="8,138.13,439.76,325.42,7.77;8,146.47,450.72,131.84,7.77" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,202.75,439.76,182.00,7.77">A survey of modern authorship attribution methods</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,391.08,439.76,72.47,7.77;8,146.47,450.72,31.00,7.77">J. Am. Soc. Inf. Sci. Technol</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="538" to="556" />
			<date type="published" when="2009-03">Mar 2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
