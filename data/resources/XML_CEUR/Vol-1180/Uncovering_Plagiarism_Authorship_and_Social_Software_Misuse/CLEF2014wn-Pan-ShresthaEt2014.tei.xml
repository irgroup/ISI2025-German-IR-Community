<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,160.29,115.90,294.78,12.90;1,275.40,133.83,64.56,12.90;1,223.43,153.68,168.49,10.75">Machine Translation Evaluation Metric for Text Alignment Notebook for PAN at CLEF 2014</title>
				<funder ref="#_qUSyAVE">
					<orgName type="full">The Office of Naval Research</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,200.32,190.08,61.77,8.64"><forename type="first">Prasha</forename><surname>Shrestha</surname></persName>
							<email>prasha@cis.uab.edu</email>
						</author>
						<author>
							<persName coords="1,268.67,190.08,59.67,8.64"><forename type="first">Suraj</forename><surname>Maharjan</surname></persName>
						</author>
						<author>
							<persName coords="1,352.23,190.08,62.82,8.64"><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
							<email>solorio@cis.uab.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Birmingham</orgName>
								<orgName type="institution">University of Alabama</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer and Information Sciences</orgName>
								<orgName type="institution">Campbell Hall</orgName>
								<address>
									<addrLine>1300 University Boulevard</addrLine>
									<postCode>35294-1170</postCode>
									<settlement>Birmingham</settlement>
									<region>Alabama</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,160.29,115.90,294.78,12.90;1,275.40,133.83,64.56,12.90;1,223.43,153.68,168.49,10.75">Machine Translation Evaluation Metric for Text Alignment Notebook for PAN at CLEF 2014</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E9CA4368927F89F52E11BB7614FAF0CD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As plagiarisers become cleverer, plagiarism detection becomes harder. Plagiarisers will find new ways to obfuscate the plagiarized passages so that humans and automatic plagiarism detectors are not able to point them out. So, a plagiarism detection system needs to be robust enough to detect plagiarism, no matter what obfuscation techniques have been applied. Our system attempts to do the same by combining two different methods, one stricter to catch mildly obfuscated passages and one more lenient to catch difficult ones. We use a machine translation evaluation metric and n-gram matching to detect overlaps between the source and suspicious documents. On the PAN'14 corpus, which contains data with various types of obfuscation to mimic human plagiarisers, we obtained plagdet scores of 0.84404 and 0.86806 on the two datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The first step in plagiarism detection is source retrieval, which finds the source documents from which a given document may have been plagiarised from. But we cannot stop here because at this point we have no solid proof of plagiarism. In order to get this proof, we need to find corresponding plagiarised passages in both the source and suspicious documents. This process, called text alignment is the focus of this paper. Text alignment is important because without it, we will not be able to provide reasoning for why we believe that the document has been plagiarised.</p><p>Text alignment may seem like simple text matching, but it is hardly so because a plagiariser rarely copies the text verbatim. Instead it is more likely to have been changed significantly in order to make it seem like an original piece of text. The task is now to find the same as well as similar pieces of text between the two documents. Also, there might be different levels of obfuscation applied to the text. A single system must be able to catch all these passages, from those that are perfectly similar to each other to those that only have a slight resemblance to one another. If the system is too strict, it might only catch those passages that are very similar and if it is too lenient it might catch a lot of false positives. This raises the difficulty level for the task.</p><p>In our system, we have tried to find a balance between a strict and a lenient system. We used two different methods, one stricter and one more lenient and then combined the results obtained from them. Our strict system uses TER-p <ref type="bibr" coords="2,391.37,119.31,11.62,8.64" target="#b3">[4]</ref> and to capture the matches between two sentences. TER-p is a machine translation evaluation metric that finds the word edit distance from a hypothesis translation to a reference translation <ref type="bibr" coords="2,466.49,143.22,10.58,8.64" target="#b3">[4]</ref>. In TER-p along with matches, insertions, deletions, substitutions and shifts, it also considers stem and synonym matches as well as phrasal substitutions as edits. The edit costs in TER-p have been tuned to correlate with human judgments of how close an instance of machine translation is to corresponding reference translations. Since, it measures the edit distance, this metric can be used in tasks that need to measure similarity in text. Some of the creators of the metric themselves used this measure for paraphrase detection <ref type="bibr" coords="2,153.35,226.91,10.58,8.64" target="#b0">[1]</ref>. In that work, they had also tried to use other machine translation metrics for the same task, but TER-p produced the best results. Our TER-p method will produce a detection only when the similarity between two sentences is high. So, it will not consider similarity between fragments of sentences or between phrases. Our more lenient system uses n-grams, more specifically bigrams, to generate matches. Bigrams are very lenient and will more likely catch most of the plagiarised passages. But as in any lenient system, it is also inclined to catch a lot of coincidental matches. But when we combine the detections from these two methods, we merge those that are close to each other. So, we will catch sentence fragments or phrases around the passage caught by the TER-p method. Also, if the bigram detections are indeed from a plagiarised passage, there will be a high density of bigram detections in that area of the text, which will form a large passage. In the end, we will only keep large enough passages and throw out the rest. So, the coincidental matches are likely to be removed and we will end up with only the correct detections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Our system follows the same basic steps of any plagiarism detection system: seeding, match merging and postprocessing. The two methods that we have used to generate matches, produce different seeds: the method that uses TER-p scores finds matches between sentences and the other method finds matches between n-grams. So, the natural first step was to generate sentences and n-grams from the text. The next step is to find matches between sentence pairs and n-gram pairs. Given a hypothesis and a reference file, TER-p gives the number of edits required to transform the sentences in the hypothesis file to the sentences in the reference file. It also provides scores per sentence, which we have used. TER-p score ranges from 0 to 1, with 0 being completely similar and 1 being most dissimilar. We considered the source document as the reference file and the suspicious document as the hypothesis file. We obtained TERp scores for each sentence in the source compared with each sentence in the suspicious document. We then considered two sentences to be plagiarised if their TER-p score is above a certain value, which in our final system is set at 0.9. For n-grams, we performed exact matching. We used bigrams and thus there were a lot of false matches. To remove false positives we merged the detections from bigrams separately before combining it with results from the TER-p system and took only 1 matching bigram in source for each bigram in the suspicious document.</p><p>After we obtained matches from both methods, we merged them to obtain passages. First we merged all the matches that were close to each other by a certain threshold in both the source and the suspicious documents. We had tried to set this threshold according to the length of the matching passages to be merged. The longer the passages, the larger would be the threshold. But we ended up using a fixed threshold of 80 characters because it produced better results. We repeated this merging process until the passages could not be merged anymore.</p><p>We also performed a postprocessing step after we found the matching passages. We kept our postprocessing step very simple by just removing the passages that were shorter than 200 characters. Although this reduced the recall by a small amount, we were able to obtain better granularity. The values for all the parameters and thresholds were chosen empirically as those that produce a good balance between precision and recall, while keeping the granularity as close to 1 as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results and Discussion</head><p>Our results on the PAN'14 text alignment corpus for the training and test datasets are shown in Tables <ref type="table" coords="3,202.18,330.22,4.98,8.64" target="#tab_0">1</ref> and<ref type="table" coords="3,226.97,330.22,4.98,8.64" target="#tab_1">2</ref> respectively. The evaluation metrics used are macro-averaged precision and recall, granularity and plagdet <ref type="bibr" coords="3,316.82,342.17,10.58,8.64" target="#b1">[2]</ref>. The plagiarised passages in the data have different levels of obfuscation. Our method performs the best when there is no obfuscation, which is what we expected. But even with random and translation obfuscations, we are very close to the plagdet we obtain for no obfuscation. We also have a good balance between precision and recall. When we participated last year, we had used larger values of n <ref type="bibr" coords="3,443.46,620.57,11.62,8.64" target="#b2">[3]</ref> while this year we used bigrams. When we use just the bigrams and leave out results from the part of the system that uses TER-p, we get plagdet of 0.92880, 0.74728, 0.67112, 0.09335 for the different levels of obfuscation in order on the training data. Our last year's system had obtained plagdet scores of 0.89040, 0.67649, 0.62194, 0.12174 for respective levels of obfuscation on the same data. Even with just bigrams we get better results than last year's system. This shows that smaller n is better when trying to find matches between two texts. This might be because smaller n-gram matches can always be merged to get larger n-grams but the reverse is not possible.</p><p>The part of the dataset where our method fails to catch plagiarised passages is summary obfuscation. Here, the plagiarised passages have been summarized and then inserted into the suspicious document. We have not used any summarizer in our system, which might be why we are not able to catch these. Since several sentences are summarized into one, the TER-p method is unable to find matching Also, the bigram method catches some of the matching bigrams but since they are too far apart in the source document, they are not merged. They remain as short passages and are subsequently removed in the postprocessing step.</p><p>The postprocessing step did help to lower our granularity. Without this step, granularity was high and this brought down our plagdet significantly, although we had good values for precision and recall. So, this step helped lower the granularity considerably and in turn increase the plagdet score. But for summary obfuscation even after this postprocessing step, granularity is close to 2. This is because the detected passages, even when they are long enough to not be removed by the postprocessing step are the too sparse and too far between.</p><p>All in all, we were able to get good results as well as balanced precision and recall for all the obfuscation levels except for summary obfuscation. For the obfuscations on which we did perform well, we have similar plagdet scores that show that our system is equally capable of detecting these varieties of obfuscations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Future Work</head><p>Our idea of using one strict and one lenient method did work in catching most types of obfuscations. We also improved on our results from last year although we used a significantly different system. We were able to use TER-p, which is a machine translation metric for a task that it was not designed for but was definitely suitable for. This also opens the doors to explore other similar metrics that can be used for similarity detection, which we plan to add to our system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,192.31,383.31,230.74,73.33"><head>Table 1 .</head><label>1</label><figDesc>Evaluation results for the training corpus</figDesc><table coords="3,192.31,404.62,230.74,52.01"><row><cell>Plagiarism Type</cell><cell>Plagdet Precision Recall Granularity</cell></row><row><cell>No Obfuscation</cell><cell>0.88009 0.80287 0.97374 1.00000</cell></row><row><cell cols="2">Random Obfuscation 0.86150 0.89992 0.83358 1.00642</cell></row><row><cell cols="2">Translation Obfuscation 0.85427 0.85322 0.86086 1.00447</cell></row><row><cell cols="2">Summary Obfuscation 0.15853 0.98171 0.08975 1.05263</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,195.11,503.58,225.13,51.41"><head>Table 2 .</head><label>2</label><figDesc>Evaluation results for the test corpus</figDesc><table coords="3,195.11,524.90,225.13,30.09"><row><cell>Dataset</cell><cell>Plagdet Precision Recall Granularity</cell></row><row><cell cols="2">PAN'14 Test Corpus 2 0.84404 0.85906 0.83782 1.00701</cell></row><row><cell cols="2">PAN'14 Test Corpus 3 0.86806 0.84418 0.89839 1.00381</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>We want to thank the organizers of the PAN competition for providing us with the opportunity to participate in this competition. This research was partially funded by <rs type="funder">The Office of Naval Research</rs> under grant <rs type="grantNumber">N00014-12-1-0217</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_qUSyAVE">
					<idno type="grant-number">N00014-12-1-0217</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="4,138.13,646.13,325.52,7.77;4,146.47,657.09,319.22,7.77;5,146.47,119.96,327.10,7.77;5,146.47,130.92,228.13,7.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,298.93,646.13,164.71,7.77;4,146.47,657.09,87.33,7.77">Re-examining machine translation metrics for paraphrase identification</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Madnani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chodorow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,251.55,657.09,214.13,7.77;5,146.47,119.96,323.26,7.77">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="182" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.13,141.88,306.42,7.77;5,146.47,152.84,333.44,7.77;5,146.47,163.80,307.59,7.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,340.49,141.88,104.06,7.77;5,146.47,152.84,71.70,7.77">An evaluation framework for plagiarism detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,236.12,152.84,243.78,7.77;5,146.47,163.80,69.21,7.77">Proceedings of the 23rd International Conference on Computational Linguistics: Posters</title>
		<meeting>the 23rd International Conference on Computational Linguistics: Posters</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="997" to="1005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.13,174.76,327.97,7.77;5,146.47,185.72,266.87,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,235.22,174.76,230.87,7.77;5,146.47,185.72,36.45,7.77">Using a variety of n-grams for the detection of different kinds of plagiarism</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Solorio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,201.01,185.72,118.66,7.77">Notebook for PAN at CLEF 2013</title>
		<meeting><address><addrLine>Valencia, Espana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.13,196.67,321.95,7.77;5,146.47,207.63,313.20,7.77;5,146.47,218.59,23.90,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,333.16,196.67,126.92,7.77;5,146.47,207.63,168.69,7.77">Ter-plus: paraphrase, semantic, and alignment enhancements to translation edit rate</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Madnani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,320.54,207.63,74.14,7.77">Machine Translation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="117" to="127" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
