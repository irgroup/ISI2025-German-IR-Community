<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,139.21,115.90,336.94,12.90;1,237.86,133.83,139.63,12.90;1,223.43,153.68,168.49,10.75">A Slightly-modified GI-based Author-verifier with Lots of Features (ASGALF) Notebook for PAN at CLEF 2014</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,236.01,190.08,71.13,8.64"><forename type="first">Mahmoud</forename><surname>Khonji</surname></persName>
							<email>mahmoud.khonji@ku.ac.ae</email>
							<affiliation key="aff0">
								<orgName type="institution">Khalifa University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,326.50,190.08,52.85,8.64"><forename type="first">Youssef</forename><surname>Iraqi</surname></persName>
							<email>youssef.iraqi@ku.ac.ae</email>
							<affiliation key="aff0">
								<orgName type="institution">Khalifa University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,139.21,115.90,336.94,12.90;1,237.86,133.83,139.63,12.90;1,223.43,153.68,168.49,10.75">A Slightly-modified GI-based Author-verifier with Lots of Features (ASGALF) Notebook for PAN at CLEF 2014</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E4BA01C7F0C64F633EBF1ECF1DBE1811</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the performance evaluation of an authorship verification technique that is based on a modified version of General Impostors (GI) <ref type="bibr" coords="1,181.73,278.62,9.52,7.77" target="#b1">[2]</ref>. The novelties of this implementation are: 1. a modified way of combining the min-max similarity measure and, 2. a relatively large set of diverse features that spans letter-level, word-level, function word-level, word shape-level, and word tag-level features. The technique ranked high in overall in the author identification task of PAN 2014.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper presents the implementation and evaluation of our classifier for the author identification task in PAN at CLEF 2014.</p><p>PAN organizers provided us with a collection of problems P = {P i : ∀ i ∈ I} where I = {1, 2, . . . , n} is the index set of P. Each problem P i ⊂ F , for all i ∈ I, where F = {f j : ∀ j ∈ J} is a set of Unicode text files and J = {1, 2, . . . m} is the index set of F . Additionally, each P i ∈ P has the following properties:</p><p>-All f j ∈ P i are written in only one of the following language-genre combinations:</p><p>Dutch essays, Dutch reviews, English essays, English novels, Greek articles and Spanish articles. -Only one text file f u ∈ P i , for some u ∈ J, is written by an author whose identity is unknown, and the rest of the files P i -{f u } are written by authors whose identities are known. -The cardinality |P i -{f u }| is allowed to vary from 1 to 5 inclusive as i varies, but (obviously) fixed for a given i.</p><p>Thus the goal of the authorship identification problem is to find a mapping function h : P → Y that solves the optimization problem <ref type="bibr" coords="1,349.91,562.69,10.58,8.64" target="#b0">(1)</ref>, where Y = [0, 1] with the semantics of <ref type="bibr" coords="1,187.34,574.65,10.58,8.64" target="#b1">(2)</ref>.</p><formula xml:id="formula_0" coords="1,178.84,598.17,301.75,16.75">arg max h∈H AUC({h(P i ) : ∀ P i ∈ P}) × C@1({h(P i ) : ∀ P i ∈ P})<label>(1)</label></formula><p>, where H is the set of all possible mapping functions, AUC is the area under the Receiver Operating Characteristic (ROC) curve, and C@1 is essentially a modified accuracy measure that rewards models h ∈ H that know when not to decide as presented in <ref type="bibr" coords="1,134.77,656.44,10.58,8.64" target="#b2">(3)</ref>.</p><formula xml:id="formula_1" coords="2,142.50,137.02,338.09,81.66">h(P i ) =      y ∈ [0, 0.5) if h predicts authors of f u and P i -{f u } are different y = 0.5 if h does not predict y ∈ (0.5, 1] if h predicts authors of f u and P i -{f u } are same (2) C@1 = N c NcNu |P| |P|<label>(3)</label></formula><p>, where |P| is the total number of problems, N c is the total number of problems that the model h has solved correctly, and N u is the total number of problems that the model h has decided to not solve. Additionally, and in order to enhance the quality of the found models h ∈ H, the organizers of PAN have also provided us with another problems collection L that has an identical structure to the collection P. L is intended to be used as a training set to find better h ∈ H models and thus PAN has also provided its ground truth information as a form of function G : L → {same authors, different authors}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method description</head><p>Our classifier, namely A Slightly-modified GI-based Author-verifier with Lots of Features (ASGALF), is (as the name implies) based on a modified version of the General Impostors (GI) framework <ref type="bibr" coords="2,245.85,391.01,10.58,8.64" target="#b1">[2]</ref>. It differs compared to other GI implementations in a couple of ways. First, we discuss why we have chosen to use GI as the starting point for ASGALF, then we discuss the differences between the GI implementation <ref type="bibr" coords="2,451.64,414.92,11.62,8.64" target="#b2">[3]</ref> and ASGALF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Reasons for adopting the GI framework</head><p>Based on our preliminary tests, previous PAN results, and our intuitive reasoning, it is very clear to us that the set of impostors (or distractors) does contain some information that helps in answering the question of how similar two vectors are, and considering such information in the decision making process is indeed helpful to solve (1) in a better way than otherwise.</p><p>For example, consider two feature vectors v 1 = k(f 1 ) and v 2 = k(f 2 ), where k : P i → R d is a function that extracts the features from any input text file f j ∈ P i , for any j ∈ J, and returns a features vector v j ∈ R d that corresponds to f j .</p><p>Simply knowing the value of q(v 1 , v 2 ) is not enough in reality as far as we know, where q : R d × R d → Y is a function that returns the similarity of its input vectors.</p><p>Our justification to this is that the output of all q functions that are known to-date are somewhat relevant to the topic/context of the evaluated vectors. For example, in some topic/context authors in general tend to be similar, while in some other topic/context authors tend to be dissimilar. E.g. in some context q(v 1 , v 2 ) = y indicates that v 1 and v 2 represent documents authored by the same individual only if y is greater than (say) 0.9. This can be the case if the topic/context is a restrictive one that causes authors to be very similar to each other (e.g. reports). On the other hand, some other topic/context may allow high variability among authors, thus vectors v 1 and v 2 can be assumed to represent documents that are authored by the same individual even if q &lt; 0.9, such as q = 0.4 can possibly indicate that the authors are the same.</p><p>Thus fetching a set of impostor text files M = {m w : ∀ w ∈ W }, where W = {1, 2, . . . , n} is the index set of M , is -in our view -a solid step towards a better optimization of the problem <ref type="bibr" coords="3,248.78,203.00,10.58,8.64" target="#b0">(1)</ref>.</p><p>We believe that measuring the distance against impostor vectors {x w = k(m w ) : ∀w ∈ W }, allows the model to see how close v 1 and v 2 are to each other relative to other impostor vectors in the same topic.</p><p>Additionally, the GI framework is one that is based upon ensembling randomized models. Although it might not be very obvious at the first glance, we believe that GI essentially creates a set of randomized models in every run (by choosing different features and impostors subsets in every run). Relying on ensembles of models is another strength of the GI framework that is appreciated by the Machine Learning community as well as other competitions such as the Netflix Prize<ref type="foot" coords="3,345.19,308.93,3.49,6.05" target="#foot_0">1</ref> where most of the top techniques were composed of some form of ensembles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Differences between ASGALF and [3]</head><p>-Instead of using the original Impostors score measure (4), we adopted a modified one as presented in (5). The advantages of ( <ref type="formula" coords="3,329.83,382.62,3.87,8.64" target="#formula_3">5</ref>) is that it allows us to measure how similar input vectors are as opposed to whether they are similar enough. On the other hand, a possible disadvantage could be over fitting the collection of training problems L.</p><formula xml:id="formula_2" coords="3,183.70,446.48,296.89,26.67">score ← score + 1 if min-max(v 1 , v 2 ) 2 &gt; min-max(v 1 , x 1 ) × min-max(v 2 , x 2 )<label>(4)</label></formula><p>, where x 1 is the most similar impostor to v 1 and x 2 is the most similar impostor to v 2 .</p><p>score</p><formula xml:id="formula_3" coords="3,222.74,515.10,257.85,24.80">← score + min-max(v 1 , v 2 ) 2 min-max(v 1 , x 1 ) × min-max(v 2 , x 2 )<label>(5)</label></formula><p>-Using a large set of diverse features. Essentially, we have extracted letter-level, word-level, function word-level, word shape-level, part-of-speech tag-level features as follows:</p><p>• n-grams with various combinations of n values and gram types -n values are {1, 2, .., 10}, gram types are {letters, words, function words, word shapes 2 , POS tags 3 , POS-words 4 } and the resultant features were based on the combinatorics of all n values and gram types. This resulted in a large number of features, most of which were too infrequent to be reliable, thus we have only considered features that occurred for at least 5 times in any single document. However, the number of features remained large in general even after removing the infrequent features. The total number of extracted features after removing the infrequent ones varied depending on language-genre combinations as shown in Table <ref type="table" coords="4,231.51,203.00,3.74,8.64" target="#tab_0">1</ref>. • Body richness -the total number of unique words in a given text file f j , for any j ∈ J, normalized by total number of words in the same file f j .</p><p>Other details of ASGALF that are not mentioned in this notebook are the same as suggested in <ref type="bibr" coords="4,185.73,401.46,10.58,8.64" target="#b2">[3]</ref>. For example, similar to <ref type="bibr" coords="4,294.85,401.46,11.62,8.64" target="#b2">[3]</ref> we fetch the set of impostors of a given document from a search engine by submitting search queries of 5 randomly chosen words from the subject document, download the 10-highest ranked HTML pages, strip them from any HTML markup, and use their first 1500 words. In ASGALF, we fetched the first 1500 words of 10 impostors for each document in the training set, and then grouped them on per language-genre basis. The output of this process is a set of impostors for each language-genre combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Parameter tuning</head><p>The parameters were tuned based on our preliminary tests against problems in L as follows:</p><p>-Score correction offsets are: -0.4585, -0.62950, -0.43850, -0.24850, -0.478, and -0.56600 for English essays, English novels, Dutch essays, Dutch reviews, Spanish 3 Part of speech tags such as NN, NNS, NNP, VB, VBD, VBG, etc. For example, if the word "school" existed in a text as a noun, then it would be represented as the gram "NN". A comprehensive list of such tags can be found in https://www.ling.upenn.edu/courses/ Fall_2003/ling001/penn_treebank_pos.html. 4 Combinations of words and their respective POS tags. For example, if the word "saw" was a noun then it would be represented as the gram "saw-NN", and if the word was a verb then it would be represented as the gram "saw-VBD".</p><p>articles, and Greek articles respectively. This allowed the final score to be centered around 0.5 in order to satisfy the semantics in (2). -Total number of Impostor rounds: 50, which also matches the optimal value for the Spanish set in <ref type="bibr" coords="5,209.55,155.89,10.58,8.64" target="#b0">[1]</ref>. -Total number of impostor documents: 8,614, 1,257, 2,728, 2,073, 5,347 and 3,104 for English essays, English novels, Dutch essays, Dutch reviews, Spanish articles and Greek articles respectively. -Total number of randomly chosen impostors per round: 20.</p><p>-Total number of randomly chosen features per round: 40% of the total features, which also matches the optimal value for the English and Spanish sets in <ref type="bibr" coords="5,443.96,229.76,10.58,8.64" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation results</head><p>Evaluation results are presented in Table <ref type="table" coords="5,298.63,292.69,3.74,8.64" target="#tab_1">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Notable limitations</head><p>We believe that the score of the model can improve if the following limitations are addressed:</p><p>-Our classifier is implemented such that it always attempts to predict an answer. I.e. it never outputs a score y = 0.5, thus limiting its ability in taking advantage of the metric C@1. -The parameters were tuned in a preliminary testing phase against the training datasets.</p><p>The outcome was that all language-genre combinations had similar optimal parameter values. However, a more rigorous optimization process could have revealed better language-genre-specific parameter values. -At the core of Impostors as described in <ref type="bibr" coords="6,322.25,202.93,11.62,8.64" target="#b2">[3]</ref> is the min-max similarity measure, which is also the measure that we have used as an implementation of the q function. It is possible that a more sophisticated model could have found a better use of our diverse set of features. -No feature subset selection was performed other than removing features that did not occur frequently enough according to a simplistic criteria as described in sections. Using more sound feature subset selection methods (e.g. IG, Wraper, etc) can possibly reduce CPU time requirements as well as enhance the accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>This paper describes an authorship verification classifier that is based on the General Impostors (GI) framework with the exception of using a modified method of combining the scores, as well as a diverse set of features. The features were of various types, namely: letter-level, word-level, function word-level, word shape-level, and partof-speech tag-level. The evaluation on the testing set shown high classification accuracy in general. This also confirms that impostor/distractor-based methodologies are indeed a step forward. Although the selection of the impostors/distractors set is a known limitation, it seems that it is practically not a major issue and that such set of impostors can be obtained with relative ease (e.g. such as by using search engines as in <ref type="bibr" coords="6,376.75,449.69,10.46,8.64" target="#b2">[3]</ref>).</p><p>On the downside, our technique generally had a slow runtime. However, we believe that this issue can be minimized in a number of ways, such as:</p><p>-Our code made excessive use of automatic execution of external commands via the shell. Some of such external commands were themselves very slow in general, such as the software used to extract the part-of-speech tags. If such external dependency is replaced by some faster one (or completely removed), the speed of the feature extraction phase can improve dramatically. -We believe that techniques that are based on the GI framework can be easily distributed should multiple cores or machines be available. This is due to the fact that all GI rounds are independent from each other, thus allowing the runs to be distributed across multiple cores or even different machines, ultimately leading in a much reduced runtime.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,177.54,232.08,260.27,94.85"><head>Table 1 .</head><label>1</label><figDesc>Total number of extracted features on per language-genre basis.</figDesc><table coords="4,238.20,253.00,136.71,73.93"><row><cell cols="2">Language Genre Features extracted</cell></row><row><cell>Dutch Essays</cell><cell>19,883</cell></row><row><cell>Dutch Reviews</cell><cell>3,312</cell></row><row><cell>English Essays</cell><cell>70,738</cell></row><row><cell>English Novels</cell><cell>43,931</cell></row><row><cell>Greek Articles</cell><cell>72,944</cell></row><row><cell>Spanish Articles</cell><cell>39,099</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,134.77,325.43,345.82,239.30"><head>Table 2 .</head><label>2</label><figDesc>PAN14 evaluation results. Note that Testing 1 is the early-bird evaluation dataset, and Testing 2 is the one that is used by PAN's rankings</figDesc><table coords="5,181.94,357.03,249.23,207.71"><row><cell cols="2">Language Genre Eval. set AUC</cell><cell>C@1</cell><cell cols="2">AUC × C@1 Runtime</cell></row><row><cell>Dutch</cell><cell cols="3">Essays Training 0.94076 0.875 0.82316</cell><cell>00:51:38</cell></row><row><cell></cell><cell>Testing 1</cell><cell></cell><cell></cell><cell>00:30:22</cell></row><row><cell></cell><cell cols="3">Testing 2 0.91254 0.84375 0.76996</cell><cell>00:58:21</cell></row><row><cell>Dutch</cell><cell cols="2">Reviews Training 0.7258 0.67</cell><cell>0.48629</cell><cell>00:12:40</cell></row><row><cell></cell><cell>Testing 1</cell><cell></cell><cell></cell><cell>00:06:05</cell></row><row><cell></cell><cell cols="2">Testing 2 0.7364 0.65</cell><cell>0.47866</cell><cell>00:12:24</cell></row><row><cell cols="4">English Essays Training 0.5285 0.545 0.28803</cell><cell>09:29:28</cell></row><row><cell></cell><cell>Testing 1</cell><cell></cell><cell></cell><cell>04:46:23</cell></row><row><cell></cell><cell cols="3">Testing 2 0.59875 0.5829 0.34901</cell><cell>09:10:01</cell></row><row><cell cols="3">English Novels Training 0.9716 0.87</cell><cell>0.84529</cell><cell>01:02:03</cell></row><row><cell></cell><cell>Testing 1</cell><cell></cell><cell></cell><cell>01:09:46</cell></row><row><cell></cell><cell>Testing 2 0.75</cell><cell>0.61</cell><cell>0.4575</cell><cell>02:06:16</cell></row><row><cell>Greek</cell><cell cols="3">Articles Training 0.5596 0.5959 0.33347</cell><cell>03:43:16</cell></row><row><cell></cell><cell>Testing 1</cell><cell></cell><cell></cell><cell>01:47:47</cell></row><row><cell></cell><cell cols="2">Testing 2 0.8892 0.81</cell><cell>0.72025</cell><cell>03:41:48</cell></row><row><cell cols="3">Spanish Articles Training 0.8992 0.87</cell><cell>0.7823</cell><cell>04:54:45</cell></row><row><cell></cell><cell>Testing 1</cell><cell></cell><cell></cell><cell>02:41:57</cell></row><row><cell></cell><cell cols="3">Testing 2 0.898 0.7777 0.69837</cell><cell>04:50:49</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,144.73,613.90,225.94,6.31"><p>http://en.wikipedia.org/wiki/Netflix_Prize</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,144.73,624.21,335.85,7.77;3,144.73,635.17,335.85,7.77;3,144.73,646.13,335.85,7.77;3,144.73,657.09,161.08,7.77"><p>The word shapes are based on three properties: characters case (e.g. lower/upper case), characters type (whether it is a letter or a number), and words length. For example, the word "School" is represented as the gram "Cccccc", "2014" is represented as the gram "NNNN", "x86" is represented as the gram "cNN", etc.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>Thanks to <rs type="person">Shachar Seidman</rs> for answering our questions about General Impostors (GI).</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="7,138.13,142.88,314.00,7.77;7,146.47,153.83,192.12,7.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,236.88,142.88,199.01,7.77">Overfiew of the Author Identification Task at PAN 2013</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Joula</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,146.47,153.83,165.98,7.77">Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.13,164.79,339.49,7.77;7,146.47,175.75,88.55,7.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,239.55,164.79,174.97,7.77">Automatically Identifying Pseudepigraphic Texts</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Seidman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,432.53,164.79,26.36,7.77">EMNLP</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1449" to="1454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.13,186.71,342.04,7.77;7,146.47,197.67,113.70,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,194.29,186.71,191.05,7.77">Authorship Verification Using the Impostors Methods</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Seidman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,403.99,186.71,76.18,7.77;7,146.47,197.67,87.55,7.77">Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
