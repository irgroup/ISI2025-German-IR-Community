<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,155.71,115.90,303.93,12.90;1,243.73,133.83,127.89,12.90">Overview of the 6th International Competition on Plagiarism Detection</title>
				<funder ref="#_hS465q8">
					<orgName type="full">FP7 Marie Curie</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,173.25,171.88,60.37,8.64"><forename type="first">Martin</forename><surname>Potthast</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Web Technology &amp; Information Systems</orgName>
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,244.42,171.88,61.11,8.64"><forename type="first">Matthias</forename><surname>Hagen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Web Technology &amp; Information Systems</orgName>
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,317.23,171.88,45.52,8.64"><forename type="first">Anna</forename><surname>Beyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Web Technology &amp; Information Systems</orgName>
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,373.99,171.88,59.27,8.64"><forename type="first">Matthias</forename><surname>Busse</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Web Technology &amp; Information Systems</orgName>
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,203.26,183.83,67.92,8.64"><forename type="first">Martin</forename><surname>Tippmann</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Web Technology &amp; Information Systems</orgName>
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,282.92,183.83,47.41,8.64"><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Natural Language Engineering Lab</orgName>
								<orgName type="institution">Universitat Politècnica de València</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,358.65,183.83,48.99,8.64"><forename type="first">Benno</forename><surname>Stein</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Web Technology &amp; Information Systems</orgName>
								<orgName type="institution">Bauhaus-Universität Weimar</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,155.71,115.90,303.93,12.90;1,243.73,133.83,127.89,12.90">Overview of the 6th International Competition on Plagiarism Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A54ADC282D31E04907B4C8BC22859DAA</idno>
					<note type="submission">Submitted</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Web Evaluation corpus Corpus TIRA experimentation platform Virtual machines plagiarism detector ChatNoir Cluster</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper overviews 17 plagiarism detectors that have been evaluated within the sixth international competition on plagiarism detection at PAN 2014. We report on their performances for the two tasks source retrieval and text alignment of external plagiarism detection. For the third year in a row, we invite software submissions instead of run submissions for this task, which allows for cross-year evaluations. Moreover, we introduce new performance measures for text alignment to shed light on new aspects of detection performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="29" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="30" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="31" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="32" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Algorithms for the retrieval and extraction of text reuse from large document collections are central to applications such as plagiarism detection, copyright protection, and information flow analysis. They have to be able to deal with all kinds of text reuse ranging from verbatim copies and quotations to paraphrases and translations to summaries <ref type="bibr" coords="1,461.51,436.27,15.27,8.64" target="#b24">[21]</ref>. Particularly the latter kinds of text reuse still present a formidable challenge to both engineering and evaluation of retrieval and extraction algorithms. Until recently, one of the primary obstacles to the development of new algorithms has been a lack of evaluation resources. To rectify this lack, we have build a variety of high-quality, large-scale evaluation resources <ref type="bibr" coords="1,201.87,496.04,15.77,8.64" target="#b32">[29,</ref><ref type="bibr" coords="1,219.93,496.04,11.83,8.64" target="#b30">27]</ref>, which have been employed within our annual shared tasks on plagiarism detection since 2009, whereas this paper reports on the results of our shared task's sixth edition. <ref type="foot" coords="1,211.98,518.28,3.49,6.05" target="#foot_0">1</ref>Since the plagiarism detection task has been running for six years in a row, we observe a multi-year life cycle within this shared task. It can be divided into three phases, namely an innovation phase, a consolidation phase, and a production phase. In the innovation phase, new evaluation resources are being developed and introduced for the first time, such as new corpora, new performance measures, and new technologies. The introduction of new evaluation resources typically stirs up a lot of dust and is prone to errors and inconsistencies that may spoil evaluation results to some extent. This cannot be avoided, since only the use of new evaluation resources by many different parties . Generic retrieval process to detect plagiarism <ref type="bibr" coords="2,399.13,252.32,13.74,7.77" target="#b39">[36]</ref>.</p><p>will reveal their shortcomings. Therefore, the evaluation resources are released only sparingly so they last for the remainder of a cycle. In the consolidation phase, based on the feedback and results obtained from the first phase, the new evaluation resources are developed to maturity by making adjustments and fixing errors. In the production phase, the task is repeated with little changes to allow participants to build upon and to optimize against what has been accomplished, and, to make the most of the prior investment in developing the new evaluation resources. Meanwhile, new ideas are being developed to introduce further innovation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Terminology and Related Work</head><p>Terminology. Figure <ref type="figure" coords="2,219.19,411.24,4.98,8.64" target="#fig_0">1</ref> shows a generic retrieval process to detect plagiarism in a given suspicious document d plg , when also given a (very large) document collection D of potential source documents. This process is also referred to as external plagiarism detection since plagiarism in d plg is detected by searching for text passages in D that are highly similar to text passages in d plg . <ref type="foot" coords="2,291.12,457.39,3.49,6.05" target="#foot_1">2</ref> The process is divided into three basic steps, which are typically implemented in most plagiarism detectors. First, source retrieval, which identifies a small set of candidate source documents D src ⊆ D that are likely sources for plagiarism regarding d plg . Second, text alignment, where each candidate source document d src ∈ D src is compared to d plg , extracting all passages of text that are highly similar. Third, knowledge-based post-processing, where the extracted passage pairs are cleaned, filtered, and possibly visualized for later inspection.</p><p>Shared Tasks on Plagiarism Detection. We have organized shared tasks on plagiarism detection annually since 2009. In the innovation phase of our shared task at PAN 2009 <ref type="bibr" coords="2,179.88,572.64,15.27,8.64" target="#b33">[30]</ref>, we developed the first standardized evaluation framework for plagiarism detection <ref type="bibr" coords="2,195.41,584.59,15.27,8.64" target="#b32">[29]</ref>. This framework was consolidated in the second and third task at PAN 2010 and 2011 <ref type="bibr" coords="2,220.06,596.55,15.77,8.64" target="#b25">[22,</ref><ref type="bibr" coords="2,238.84,596.55,11.83,8.64" target="#b26">23]</ref>, and it has since entered the production phase while being adopted by the community. Our initial goal with this framework was to evaluate the process of plagiarism detection depicted in Figure <ref type="figure" coords="3,355.00,119.31,4.98,8.64" target="#fig_0">1</ref> as a whole. We expected that participants would implement source retrieval algorithms as well as text alignment algorithms and use them as modules in their plagiarism detectors. However, the results of the innovation phase proved otherwise, since participants implemented only text alignment algorithms, whereas they resorted to exhaustively comparing all pairs of documents within our evaluation corpora, even when the corpora were tens of thousands of documents large. Therefore, upon entering the production phase after the third edition of our shared task, and, because of continued interest from the research community, we rechristened the shared task to text alignment and continued to offer it in the three following years at PAN 2012 to 2014.</p><p>To establish source retrieval as a shared task of its own, we introduced it at PAN 2012 next to the text alignment task <ref type="bibr" coords="3,305.50,250.82,15.27,8.64" target="#b27">[24]</ref>, thus entering a new task life cycle for this task. We developed a new, large-scale evaluation corpus of essay-length plagiarism cases that have been written manually, and whose sources have been retrieved manually from the ClueWeb corpus <ref type="bibr" coords="3,242.58,286.69,15.27,8.64" target="#b30">[27]</ref>. Given our above observation from the text alignment task, the ClueWeb was deemed too large to be exhaustively compared to a given suspicious document in a reasonable time. Furthermore, we developed a new search engine for the ClueWeb called ChatNoir <ref type="bibr" coords="3,272.89,322.55,15.27,8.64" target="#b29">[26]</ref>, which serves participants who do not wish to develop their own ClueWeb search engine as a means of participation. We then offered source retrieval as an individual task based on the new evaluation resources <ref type="bibr" coords="3,445.84,346.46,15.77,8.64" target="#b27">[24,</ref><ref type="bibr" coords="3,464.83,346.46,11.83,8.64" target="#b28">25]</ref>, whereas this year marks the third time we do so, and the transition of the source retrieval task into the production phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Contributions</head><p>Both source retrieval and text alignment are now in the production phase of their life cycles. Therefore, we refrain from changing the existing evaluation resources too much, whereas we continue to maintain them. Therefore, our contributions this year consist of (1) a survey of submitted approaches, which reveals new trends among participants at solving the respective tasks, and (2) an analysis of the participants' retrieval performances on a per-obfuscation basis, using new performance measures, and in direct comparison to participants from previous years.</p><p>In this connection, our goal with both shared tasks is to further automate them. Hence, we continue to develop the TIRA evaluation platform <ref type="bibr" coords="3,380.02,515.83,15.77,8.64" target="#b13">[10,</ref><ref type="bibr" coords="3,398.22,515.83,11.83,8.64" target="#b14">11]</ref>, which gives rise to software submissions with minimal organizational overhead. Last year, we focused on TIRA's infrastructure, which employs virtualization technology to allow participants to use their preferred development environment, and to secure the execution of untrusted software while making the release the test corpora unnecessary <ref type="bibr" coords="3,386.61,563.65,10.58,8.64" target="#b12">[9]</ref>. This year, we introduce a fully-fledged web service as a user interface that enables participants to remote control their evaluations on the test corpora under our supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Source Retrieval</head><p>In source retrieval, given a suspicious document and a web search engine, the task is to retrieve all source documents from which text has been reused whilst minimizing retrieval costs. The cost-effectiveness of plagiarism detectors in this task is important since using existing search engines is perhaps the only feasible way for researchers as well as small and medium-sized businesses to implement plagiarism detection against the web, whereas search companies charge considerable fees for automatic usage.</p><p>In what follows, we briefly describe the building blocks of our evaluation setup, provide brief details about the evaluation corpus, and discuss the performance measures (see last year's task overview for more details on these three points <ref type="bibr" coords="4,397.05,501.40,14.94,8.64" target="#b28">[25]</ref>). We also survey the submitted softwares, and finally, report on their achieved results in this year's setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Evaluation Setup</head><p>For the evaluation of source retrieval from the web, we consider the real-world scenario of an author who uses a web search engine to retrieve documents in order to reuse text from them in a document. A plagiarism detector typically uses a search engine, too, to find reused sources of a given document. Over the past years, we assembled the necessary building blocks to allow for a meaningful evaluation of source retrieval algorithms; Figure <ref type="figure" coords="4,211.69,620.57,4.98,8.64" target="#fig_1">2</ref> shows how they are connected. The setup was described in much more detail in last year's task overview <ref type="bibr" coords="4,292.77,632.53,15.27,8.64" target="#b28">[25]</ref>.</p><p>Two main components are the TIRA experimentation platform and the ClueWeb09 with two associated search engines. TIRA <ref type="bibr" coords="4,307.50,656.44,16.60,8.64" target="#b13">[10]</ref> itself consists of a number of building blocks; one of them, depicted in Figure <ref type="figure" coords="5,292.28,119.31,4.98,8.64" target="#fig_1">2</ref> bottom left, facilitates both platform independent software development and software submissions at the same time by its capability to create and remote control virtual machines on which our lab's participants deploy their plagiarism detectors.</p><p>The ClueWeb corpus 2009 (ClueWeb09) <ref type="foot" coords="5,314.79,165.46,3.49,6.05" target="#foot_2">3</ref> is one of the most widely adopted web crawls which regularly used for large-scale web search-related evaluations. It consists of about one billion web pages, half of which are English ones. Although an updated version of the corpus has been released, <ref type="foot" coords="5,289.64,201.33,3.49,6.05" target="#foot_3">4</ref> our evaluation is still based on the 2009 version since our corpus of suspicious documents was built on top of ClueWeb09. Indri <ref type="foot" coords="5,459.06,213.28,3.49,6.05" target="#foot_4">5</ref> and ChatNoir <ref type="bibr" coords="5,175.26,226.91,16.60,8.64" target="#b29">[26]</ref> are currently the only publicly available search engines that index the ClueWeb09 corpus; their retrieval models are based on language modeling and BM25F, respectively. For developer convenience, we also provide a proxy server which unifies the APIs of the search engines. At the same time, the proxy server logs all accesses to the search engines for later performance analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Evaluation Corpus</head><p>The evaluation corpus employed for source retrieval is based on the Webis text reuse corpus 2012 (Webis-TRC-2012) <ref type="bibr" coords="5,268.20,336.50,15.77,8.64" target="#b31">[28,</ref><ref type="bibr" coords="5,287.21,336.50,11.83,8.64" target="#b30">27]</ref>. The corpus consists of 297 documents that have been written by 27 writers who worked with our setup as shown in the first row of Figure <ref type="figure" coords="5,174.49,360.41,3.88,8.64" target="#fig_1">2</ref>: given a topic, a writer used ChatNoir to search for source material on that topic while preparing a document of 5700 words length on average, reusing text from the found sources.</p><p>In the last years, we sampled 98 documents from the Webis-TRC-2012 as training and test documents. This year, these documents were provided for training, and another 99 documents were sampled as test documents. The remainder of the corpus will be used within future labs on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Performance Measures</head><p>Given a suspicious document d plg that contains passages of text that have been reused from a set of source documents D src , we measure the retrieval performance of a source retrieval algorithm in terms of precision and recall of the retrieved documents D ret taking into account the effect of near-duplicate web documents as follows (cf. last year's task overview <ref type="bibr" coords="5,192.32,529.77,16.60,8.64" target="#b28">[25]</ref> for more details).</p><p>For any d ret ∈ D ret , we employ a near-duplicate detector to judge whether it is a true positive detection; i.e., whether there is a d src ∈ D src of d plg that is a nearduplicate of d ret . We say that d ret is a true positive detection for a given pair of d src and d plg iff (1) d ret = d src (equality), or (2) the Jaccard similarity of the word n-grams in d ret and d src is above 0.8 for n = 3, above 0.5 for n = 5, and above 0 for n = 8 (similarity), or (3) the passages in d plg known to be reused from d src are contained in d ret (containment). Here, containment is measured as asymmetrical set overlap of the passages' set of word n-grams regarding that of d ret , so that the overlap is above 0.8 for n = 3, above 0.5 for n = 5, and above 0 for n = 8. This three-way approach of determining true positive detections inherently entails inaccuracies. While there is no straightforward way to solve this problem, this error source affects all detectors, still allowing for relative comparisons.</p><p>Let d dup denote a near-duplicate of a given d src that would be considered a true positive detection according to the above conditions. Note that every d src may have more than one such near-duplicate and every d dup may be a near-duplicate of more than one source document. Based on these sets, we define precision and recall of D ret regarding D src and d plg as follows:</p><formula xml:id="formula_0" coords="6,207.93,477.08,199.49,23.23">prec = |D ret ∩ D src | |D ret | , r ec = |D ret ∩ D src | |D src | .</formula><p>Rationale for this definition is that retrieving more than one near-duplicate of a source document does not decrease precision, but it does not increase recall, either, since no additional information is obtained. A further graphical explanation of how we take nearduplicates into account for precision and recall is given in Figure <ref type="figure" coords="6,400.39,542.33,3.74,8.64" target="#fig_2">3</ref>. Note that D ret as defined above does not actually contain all duplicates of the retrieved documents, but only those that are already part of D src . Finally, to measure the cost-effectiveness of a source retrieval algorithm in retrieving D ret , we count the numbers of queries and downloads made and compute the workload in terms of queries and downloads until the first true positive detection is made.</p><p>The Source Oracle To allow for participation in the source retrieval task without the need of having a text alignment component at hand, we provide a source oracle that automatically enriches a downloaded document with information about whether or not it is considered a true positive source for the given suspicious document. Note that the oracle employs the aforementioned conditions to determine whether a document is a true positive detection. However, the oracle does not, yet, tell for which part of a suspicious document a downloaded document is a true positive detection. Hence, applying a custom text alignment strategy can still be beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Survey of Retrieval Approaches</head><p>Six of the 16 participants submitted softwares for the source retrieval task, all of whom also submitted a notebook describing their approach. An analysis of these descriptions reveals the same building blocks that were commonly used in last years' source retrieval algorithms: (1) chunking, (2) keyphrase extraction, (3) query formulation, (4) search control, and (5) download filtering. Some participants only slightly changed their approach from the previous year; in what follows, we describe the employed ideas in detail.</p><p>Chunking Given a suspicious document, it is divided into (possibly overlapping) passages of text. Each chunk of text is then processed individually. Rationale for chunking the suspicious document is to evenly distribute "attention" over a suspicious document so that algorithms employed in subsequent steps are less susceptible to unexpected characteristics of the suspicious document.</p><p>The chunking strategies employed by the participants are no chunking (i.e., the whole document as one chunk) <ref type="bibr" coords="7,287.77,367.13,15.27,8.64" target="#b40">[37]</ref>, 50-line chunks <ref type="bibr" coords="7,376.73,367.13,10.58,8.64" target="#b8">[5]</ref>, headings as separate chunks <ref type="bibr" coords="7,165.71,379.09,15.27,8.64" target="#b40">[37]</ref>, headings to split documents into chunks <ref type="bibr" coords="7,351.11,379.09,15.27,8.64" target="#b40">[37]</ref>, 100-word chunks based on heading detection <ref type="bibr" coords="7,207.87,391.04,15.27,8.64" target="#b34">[31]</ref>, 200-word chunks <ref type="bibr" coords="7,301.40,391.04,15.27,8.64" target="#b34">[31]</ref>, 5-sentence chunks <ref type="bibr" coords="7,398.88,391.04,15.77,8.64" target="#b44">[41,</ref><ref type="bibr" coords="7,417.18,391.04,11.83,8.64" target="#b19">16]</ref>, and combinations thereof.</p><p>Note that chunks typically are stated as non-overlapping. The potentially interesting question of whether overlapping chunks might help was not really tackled by any approach. However, typical plagiarism cases have no fixed length and overlapping chunks might reduce the risk of, for instance, having more than one source in one chunk of 50 lines. Furthermore, relying on the given document structure (e.g., chunking by lines or paragraphs) bears the risk of failing for some unseen documents that are not as well-formatted as the ones in our evaluation corpus. Maybe mixed chunking strategies as seen in Suchomel and Brandejs <ref type="bibr" coords="7,290.46,498.64,15.93,8.64" target="#b40">[37]</ref>' approach is an interesting future direction. Notably, their document level queries seem to also guarantee an early recall (cf. Section 2.5).</p><p>Keyphrase Extraction Given a chunk, keyphrases are extracted from it in order to formulate queries with them. Rationale for keyphrase extraction is to select only those phrases (or words) which maximize the chance of retrieving source documents matching the suspicious document. Keyphrase extraction may also serve as a means to limit the amount of queries formulated, thus reducing the overall costs of using a search engine. This step is perhaps the most important one of a source retrieval algorithm since the decisions made here directly affect the overall performance: the fewer keywords are extracted, the better the choice must be or recall is irrevocably lost. Some participants use single keywords while others extract whole phrases. Most of the participants preprocessed the suspicious document by removing stop words before the actual keyphrase extraction. Phrasal search was provided by the Indri search engine. All participants did use Indri when submitting phrasal queries; some of which also combine phrases with non-phrasal ChatNoir queries, the search engine that the original essay authors had used. In particular, Elizalde <ref type="bibr" coords="8,335.84,155.18,11.62,8.64" target="#b8">[5]</ref> applies three different keyphrase extraction strategies very similar to her last year's approach: (1) one query per 50lines chunk containing the top 10 words scored by tf •idf values, (2) first 8-gram with three words from 1 per chunk, (3) 15 phrases based on head noun clusters <ref type="bibr" coords="8,432.37,191.04,10.58,8.64" target="#b6">[3]</ref>. Prakash and Saha <ref type="bibr" coords="8,174.98,203.00,16.60,8.64" target="#b34">[31]</ref> use the top 5 document-level tf -ranked terms and five paragraph-level tf -ranked terms and the nouns from sentence subgroups to form queries. Kong et al. <ref type="bibr" coords="8,134.77,226.91,16.60,8.64" target="#b19">[16]</ref> choose the ten best phrases per chunk according to an own keyphrase extraction based on BM25 and tf • idf weighting. Williams et al. <ref type="bibr" coords="8,362.00,238.86,16.60,8.64" target="#b44">[41]</ref> use their very simplistic keyphrase extraction strategy from last year: only nouns, adjectives, and verbs form the keyphrases. Zubarev and Sochenkov <ref type="bibr" coords="8,303.38,262.77,16.60,8.64" target="#b45">[42]</ref> follow a similar strategy; for 83 highweighting sentences (weighting according to overlap with other sentences) they form queries by ignoring articles, pronouns, prepositions, and repeated words. One problem with sentence weighting might be that it does not distribute the selected sentences over the entire document such that for specific parts no keywords might used.</p><p>Suchomel and Brandejs <ref type="bibr" coords="8,245.86,322.55,16.60,8.64" target="#b40">[37]</ref> apply three different strategies very similar to their last year's approach. First, from the whole document, they use the top 6 words ranked by tf•idf values. These top 6 keywords are then also combined with their most frequent two or three term collocations. Second, they use the longest sentence from each paragraph. Third, they detect headers in the text and use 6-term phrases from these headers.</p><p>Altogether, the participants' approaches to keyphrase extraction can still basically be divided into four different categories. (1) Rather simplistic strategies that identify keyphrases by chunking the whole document into some longer n-grams. This probably conforms with the folklore human strategy of identifying some suspicious n-gram in a suspicious document and submitting this n-gram to a search engine. Using all longer n-grams probably also "hits" parts of the n-grams a human would have chosen. Thus, it is interesting to analyze the final performance of approaches that use this kind of keyphrases (cf. Section 2.5). ( <ref type="formula" coords="8,290.15,466.01,3.87,8.64">2</ref>) Another very common strategy is to use the tf •idf -ranked top scoring words or phrases relying on some background collection for document frequencies. (3) Notably, established keyphrase extraction schemes developed from the respective research community are only used in one approach. (4) Some participants do not rely on one strategy alone but combine the other three approaches for keyphrase extraction. This way, just as with chunking, the risk of algorithm error is further diminished and it becomes possible to exploit potentially different sources of information that complement each other.</p><p>Query Formulation Interestingly, most of the participants hardly combine keyphrases into one query apart from merging, for instance, the top k tf • idf -ranked terms, then the next k terms, etc. This way, most participants implicitly formulate nonoverlapping queries (i.e., they do not explicitly use the same keyword in more than one query) except for some of the participants who basically use all the longer n-grams in the suspicious document or who do not mind same keywords in different queries that appear rather "by accident" than by intention. This non-overlap-approach is in line with many query-by-document strategies but in contrast to previous source retrieval strategies that were shown to better identify highly related documents using overlapping queries <ref type="bibr" coords="9,187.27,131.27,15.27,8.64" target="#b17">[14]</ref>. Also note that hardly any of the participants made use of advanced search operators offered by Indri or ChatNoir, such as the facet to search for web pages of at least 300 words of text, and the facet to filter search results by readability.</p><p>Search Control Given sets of keywords or keyphrases extracted from chunks, queries are formulated which are tailored to the API of the search engine used. Rationale for this is to adhere to restrictions imposed by the search engine and to exploit search features that go beyond basic keyword search (e.g., Indri's phrasal search). The maximum number of search terms enforced by ChatNoir is 10 keywords per query while Indri allows for longer queries.</p><p>Given a set of queries, the search controller schedules their submission to the search engine and directs the download of search results. Rationale for this is to dynamically adjust the search based on the results of each query, which may include dropping queries, reformulating existing ones, or formulating new ones based on the relevance feedback obtained from search results. Some participants do not describe a search control. The ones who do basically schedule queries and drop some of the previously generated queries. Prakash and Saha <ref type="bibr" coords="9,260.81,317.32,16.60,8.64" target="#b34">[31]</ref> drop a query when more than 60% of its terms are contained in another query. Suchomel and Brandejs <ref type="bibr" coords="9,349.19,329.28,16.60,8.64" target="#b40">[37]</ref> schedule queries dependent on the keyphrase extractor which extracted the words: the order of precedence corresponds to the order in which they have been explained above. Whenever later queries were formulated for portions of the suspicious document that were already mapped to a source, these queries are not submitted and discarded from the list of open queries. Also Zubarev and Sochenkov <ref type="bibr" coords="9,234.62,389.05,16.60,8.64" target="#b45">[42]</ref> remove queries for sentences that already are mapped to a potential retrieved source.</p><p>Note that still (just as last year) none of the teams did try to reformulate existing queries or formulating new ones based on the available number of search results, the search snippets, or the downloaded documents, which leaves significant room for improvement. Another interesting aspect might be the scheduling of the queries themselves. The experimental results (cf. Section 2.5) seem to suggest that some documentlevel queries in the first submission positions guarantee an early recall (e.g., Suchomel and Brandejs <ref type="bibr" coords="9,189.54,484.69,14.94,8.64" target="#b40">[37]</ref>). Simply scheduling queries in the order of chunks in the documents instead, might run into problems with early recall as maybe there is not that much reused text at the beginning of a document. This might also be an interesting point for future research. Download Filtering Given a set of search engine results, a download filter removes all documents that are probably not worthwhile being compared in detail with the suspicious document. Rationale for this is to further reduce the set of candidates and to save invocations of the subsequent detailed comparison step.</p><p>In particular, Elizalde <ref type="bibr" coords="9,238.97,587.06,11.62,8.64" target="#b8">[5]</ref> focuses on the top 10 results of a query and downloads a result document when at least 90% of the 4-grams in a 500-character snippet are contained in the suspicious document. Prakash and Saha <ref type="bibr" coords="9,350.46,610.97,16.60,8.64" target="#b34">[31]</ref> download a document from the top 10 when at least one 5-gram in a 500-character snippet is contained in the suspicious document; they explicitly avoid double downloads of the same URL. Zubarev and Sochenkov <ref type="bibr" coords="9,180.99,646.83,16.60,8.64" target="#b45">[42]</ref> base their strategy on the top 7 results and compute similarities of snip- pet sentences to the suspicious document's sentences. A download was scheduled when the similarity is high. Suchomel and Brandejs <ref type="bibr" coords="10,327.09,342.80,16.60,8.64" target="#b40">[37]</ref> obtain snippets for each individual query term and download documents (no information on the number of results per query is given) when more than 20% of the word 2-grams in the concatenated snippets also appear in the suspicious document. Williams et al. <ref type="bibr" coords="10,358.72,378.67,16.60,8.64" target="#b44">[41]</ref> try to train a classifier for download scheduling using a lot of search engine and snippet features. However, comparing their approach from last year with a more simplistic download filtering and this year's classifier idea, not much improvement was achieved (basically the same number of downloads and hardly any improvements in overall or early recall). Kong et al. <ref type="bibr" coords="10,464.00,426.49,16.60,8.64" target="#b19">[16]</ref> simply download the top 3 results per query. Interestingly, the participants heavily rely on the retrieval models of the search engines by focusing on the at most top 10 results per query. It is probably not much more expensive to get more results per query to be able to select from a wider range of results. Interestingly, Elizalde <ref type="bibr" coords="10,266.10,486.26,11.62,8.64" target="#b8">[5]</ref> requests 30 results per query but then immediately focuses on the top 10 documents without any further consideration of the lower ranks. Other participants restrict themselves to only a few documents per query while comparing against maybe a hundred results might not be much more costly than selecting from only 3 results. Considering more results per query might be an interesting option for future research based on the User-over-Ranking hypothesis <ref type="bibr" coords="10,386.90,546.04,15.77,8.64" target="#b38">[35,</ref><ref type="bibr" coords="10,405.16,546.04,11.83,8.64" target="#b16">13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Evaluation Results</head><p>Table <ref type="table" coords="10,160.04,595.85,4.98,8.64" target="#tab_0">1</ref> shows the performances of the six plagiarism detectors that took part in this year's source retrieval subtask as well as those of the last year's participants whose approaches were re-evaluated on this year's test corpus using the TIRA experimentation platform. Since there is currently no single formula to organize retrieval performance and cost-effectiveness into an absolute order, the detectors are ordered alphabetically, whereas the best performance value for each metric is highlighted. As can be seen, there is no single detector that performs best on all accounts. Rather, different detectors have different characteristics.</p><p>Arguably, highest possible recall at a reasonable workload (queries and downloads) is the goal of source retrieval. One might discuss for instance, whether recall should be given more weight in the F-measure. Still, when sorting the participants by recall, it appears that only two of the top 8 participants are not from 2014. This indicates some progress in the "right" direction for this task. However, the 2013-approach of Kong et al. <ref type="bibr" coords="11,157.85,454.85,16.60,8.64" target="#b20">[17]</ref> still achieves the best recall-at the cost of poor precision. To further shed some light on the recall of the different approaches, Figure <ref type="figure" coords="11,376.40,466.81,4.98,8.64" target="#fig_4">4</ref> shows the recall against the number of downloaded documents. It can be seen that recall is typically gained over the whole process of downloading documents and not with the very first downloads (the plateau effect at the upper right end of each plot is due to the averaging). Unsurprisingly, some of the low-workload approaches achieve higher recall levels with fewer downloads while approaches with more downloads typically achieve their better final recall levels only at a much higher number of downloads. Thus, focusing on download filtering strategies at document-level (when all queries are submitted) might improve their early recall at the download-level-but this would probably harm recall when measured against the submitted queries.</p><p>Interestingly, the ensemble of all submitted approaches would achieve an average recall of 0.85 retrieving all sources for 48 topics. Only for 14 topics the recall is below 0.6 (which is the best individual average recall).</p><p>Not just focusing on recall, a per-participant analysis also reveals some interesting observations when comparing the approaches from different years. For instance, Suchomel and Brandejs <ref type="bibr" coords="11,224.99,646.13,16.60,8.64" target="#b40">[37]</ref> almost doubled their recall without increased query load. Also Elizalde <ref type="bibr" coords="12,192.01,119.31,11.62,8.64" target="#b8">[5]</ref> managed to save a lot of downloads and improve precision over last year without harming recall. By saving a lot of downloads, Kong et al. <ref type="bibr" coords="12,422.90,131.27,16.60,8.64" target="#b19">[16]</ref> sacrificed their good recall from last year. A little disappointing is the almost negligible effect of Williams et al. <ref type="bibr" coords="12,209.91,155.18,16.60,8.64" target="#b44">[41]</ref> efforts to improve over last year: the better scheduling of the downloads yields no real changes in their overall performance; still, a lot of queries are invested for only a tenth of actual downloads. Prakash and Saha <ref type="bibr" coords="12,410.87,179.09,16.60,8.64" target="#b34">[31]</ref> and Zubarev and Sochenkov <ref type="bibr" coords="12,200.13,191.04,16.60,8.64" target="#b45">[42]</ref> achieve very good results with their first participation in source retrieval, contributing a lot to the better average recall of this year's approaches over the last year.</p><p>The detectors of Williams et al. <ref type="bibr" coords="12,282.38,226.91,15.77,8.64" target="#b43">[40,</ref><ref type="bibr" coords="12,301.47,226.91,13.28,8.64" target="#b44">41]</ref> achieves the best trade-off between precision and recall and therefore the best F 1 value. This detector is followed closely by that of Zubarev and Sochenkov <ref type="bibr" coords="12,267.37,250.82,15.27,8.64" target="#b45">[42]</ref>, which also achieves almost the same balanced precision and recall with a much smaller workload. It is not easy to decide which of the participating detectors solves the task best, since each of them may have their justification in practice. For example, the detector of Haggag and El-Beltagy downloads only about five documents on average per suspicious document and minimizes the time to first detection; however, it also has no detection at all for 12 documents. Despite the excellent precision/recall trade-off of Williams et al.'s detectors, it incurs the secondhighest costs in terms of queries on average, much more than some other participants that achieve better or only slightly worse recall. Kong et al.'s detector has the highest download costs, but one may argue that downloads are much cheaper than queries, and that in the source retrieval task recall is more important than precision.</p><p>Altogether, the current strategies might be a little too focused on saving downloads (and queries) compared to for instance increased recall. Also runtime should probably not be the key metric to optimize (e.g., using threads instead of sequential processing does not decrease the workload on the search engines). A reasonable assumption probably is that recall is most important to the end user of a source retrieval system. Investing a couple of queries and a couple (maybe even hundreds) of downloads to achieve a recall above 0.8 might be a very important research direction. In the end, whatever source the source retrieval step misses, cannot be found by a later text alignment step. This probably is a key argument for a recall-oriented source retrieval strategy that also takes into account basic considerations on total workload of query submission and downloads. It would be interesting to see efforts in that direction of significantly improved recall at a moderate cost increase in future approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Text Alignment</head><p>In text alignment, given a pair of documents, the task is to identify all contiguous passages of reused text between them. The challenge with this task is to identify passages of text that have been obfuscated, sometimes to the extent that, apart from stop words, little lexical similarity remains between an original passage and its plagiarized counterpart. To provide a challenging evaluation corpus, we resort to our previous corpus construction efforts. The performance of a plagiarism detector is measured based on the traditionally employed measures plagdet, precision, recall, and granularity, whereas we also introduce new measures that shed light on different performance aspects of plagiarism detection. Finally, we conduct a cross-year evaluation and compare the performance of this year's detectors with those of last year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Evaluation Corpus</head><p>As an evaluation corpus for this year, we reuse that of last year <ref type="bibr" coords="13,401.96,181.08,15.27,8.64" target="#b28">[25]</ref>. The corpus is based on the Webis-TRC-13 <ref type="bibr" coords="13,252.41,193.04,15.27,8.64" target="#b30">[27]</ref>. Instead of employing the documents of that corpus directly, pairs of documents that comprise reused passages have been constructed automatically, similarly to previous years <ref type="bibr" coords="13,297.84,216.95,15.27,8.64" target="#b32">[29]</ref>. The corpus comprises plagiarism cases whose reused portions of text have been subject to four obfuscation strategies, namely verbatim copies, random obfuscation, cyclic translation obfuscation, and summary obfuscation. The latter strategy has been found to be the most challenging kind of obfuscation in terms of being detected to date.</p><p>While the best performing approach was determined based on last year's test corpus, we have compiled a variant of the above corpus using the same methods as before which comprises only unobfuscated and randomly obfuscated plagiarism. This supplemental corpus serves as a baseline corpus.</p><p>Discussion Reusing a previously constructed evaluation corpus another time has been a compromise on our part to free up time for working on the front end of the TIRA experimentation platform. This strategy, however, bears the risk of approaches being overfitted to a given corpus and participants cheating. While reusing a previously released corpus would not be a problem if participants used only the training data to fine-tune their approach, it cannot be entirely ruled out that the publicly available test data has been used as well. Therefore, the evaluation results for text alignment of this year must be taken with a grain of salt.</p><p>In an attempt to alleviate this problem, we constructed a supplemental corpus using the same corpus construction process that was used for the reused corpus, however, the supplemental corpus comprises only basic obfuscation strategies that can be detected a lot easier than, for example, summary obfuscation. Therefore, this corpus was not chosen to be the reference for this year's ranking among participants.</p><p>In general, we are thinking of new ways to organize text alignment as a shared task. Throughout the years, we have created a new evaluation corpus every year, except for this year, adding new kinds of obfuscation strategies each time. However, we feel that this level of output cannot be sustained much longer; should there still be considerable interest in continuing this task, we will involve participants in the corpus construction efforts by submitting a corpus of their own design, while cross-evaluating the submitted corpora using all submitted approaches. We have invited data submissions on a voluntary basis before, however, without much success. Perhaps, by making data submissions a mandatory part of this shared task, there will be more participant engagement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Performance Measures Revisited</head><p>To assess the performance of the submitted text alignment softwares, we employ the performance measures used in previous evaluations <ref type="bibr" coords="13,345.18,644.09,15.49,8.64" target="#b32">[29]</ref>: precision, recall, and granularity, which are combined into the plagdet score. While these measures are not beyond criticism, they have served as a reliable means to rank plagiarism detectors; these measures are generally perceived as very strict.</p><p>This year, we revisit performance measurement of plagiarism detectors by shedding light on more abstract levels of detection performance. Until now, plagiarism detection performance has been measured at the character level under the model of a user who expects her plagiarism detector to retrieve and extract contiguous plagiarized passages of text from a given pair of documents. For example, many current plagiarism detectors extract only overlapping substrings of a given plagiarism case which makes reviewing their detections cumbersome, whereas extracting contiguous passages has turned out to be much more convenient. Therefore, the current measures have been specifically developed to capture the "completeness" of detection of a given plagiarism case: they measure the precision and recall of detecting a plagiarism case at character level and combine that with the granularity measure, which counts the number of times a given plagiarism case has been detected.</p><p>However, there is more than one relevant user model, and performance can be measured at different levels of abstraction, which may even lead to contrary results as to which particular plagiarism detection approach is best suited for a particular user. In what follows, we review the previous performance measures, and add two new levels of abstraction. They incorporate different assumptions about the detection characteristics preferred by users. Besides the character level, we propose to measure detection performance at the case level, which fixes the minimum precision and recall at which a plagiarism case has to be detected, and, at the document level, which disregards whether all plagiarism cases present in a document are detected as long as a significant portion of one of them is detected. The case level measures assume a user is interested in covering all plagiarism cases which are present in a given collection, whereas the document level measures assume a user wants to determine if a document is suspicious and worth further analysis.</p><p>Character level performance measures Let S denote the set of plagiarism cases in the corpus, and let R denote the set of detections reported by a plagiarism detector for the suspicious documents. A plagiarism case s = s plg , d plg , s src , d src , s ∈ S, is represented as a set s of references to the characters of d plg and d src , specifying the passages s plg and s src . Likewise, a plagiarism detection r ∈ R is represented as r. We say that r detects s iff s ∩ r = ∅ and s plg overlaps with r plg and s src overlaps with r src . Based on this notation, precision and recall of R under S can be measured as follows:</p><formula xml:id="formula_1" coords="14,145.38,535.21,324.59,27.47">prec(S, R) = 1 |R| r∈R | s∈S (s r)| |r| , r ec(S, R) = 1 |S| s∈S | r∈R (s r)| |s| ,</formula><p>where s r = s ∩ r if r detects s, ∅ otherwise. Observe that neither precision nor recall account for the fact that plagiarism detectors sometimes report overlapping or multiple detections for a single plagiarism case. This is undesirable, and to address this deficit also a detector's granularity is quantified as follows:</p><formula xml:id="formula_2" coords="14,245.48,640.46,124.39,27.42">gran(S, R) = 1 |S R | s∈S R |R s |,</formula><p>where S R ⊆ S are cases detected by detections in R, and R s ⊆ R are detections of s; i.e., S R = {s | s ∈ S and ∃r ∈ R : r detects s} and R s = {r | r ∈ R and r detects s}. Note further that the above three measures alone do not allow for a unique ranking among detection approaches. Therefore, the measures are combined into a single overall score as follows:</p><formula xml:id="formula_3" coords="15,224.30,174.72,162.79,24.17">plagdet(S, R) = F 1 log 2 (1 + gran(S, R))</formula><p>,</p><p>where F 1 is the equally weighted harmonic mean of precision and recall.</p><p>Case level performance measures Let S and R be defined as above. Further, let S = {s | s ∈ S and r ec char (s, R) &gt; τ 1 and ∃r ∈ R: r detects s and prec char (S, r) &gt; τ 2 } denote the subset of all plagiarism cases S which have been detected with more than a threshold τ 1 in terms of character recall r ec char and more than a threshold τ 2 in terms of character precision prec char . Likewise, let R = {r | r ∈ R and prec char (S, r) &gt; τ 2 and ∃s ∈ S: r detects s and r ec char (s, R) &gt; τ 1 } denote the subset of all detections R which contribute to detecting plagiarism cases with more than a threshold τ 1 in terms of character recall r ec char and more than a threshold τ 2 in terms of character precision prec char . Here, character recall and precision derive from the character level performance measures defined above:</p><formula xml:id="formula_4" coords="15,165.51,381.21,284.35,22.98">prec char (S, r) = | s∈S (s r)| |r| , r ec char (s, R) = | r∈R (s r)| |s| .</formula><p>Based on this notation, we compute case level precision and recall as follows:</p><formula xml:id="formula_5" coords="15,205.46,432.79,204.44,22.31">prec case (S, R) = |R | |R| , r ec case (S, R) = |S | |S| .</formula><p>The thresholds τ 1 and τ 2 can be used to adjust the minimal detection accuracy with regard to passage boundaries. Threshold τ 1 adjusts how accurate a plagiarism case has to be detected, whereas threshold τ 2 adjusts how accurate a plagiarism detection has to be. Beyond the minimal detection accuracy imposed by these thresholds, however, a higher detection accuracy does not contribute to case level precision and recall. If τ 1 → 1 and τ 2 → 1, the minimal required detection accuracy approaches perfection, whereas if τ 1 → 0 and τ 2 → 0, it is sufficient to report an entire document as plagiarized to achieve perfect case level precision and recall. In between these extremes, it is an open question which threshold settings are valid with regard to capturing the minimally required detection quality beyond which most users of a plagiarism detection system will not perceive improvements, anymore. Hence, we choose τ 1 = τ 2 = 0.5 as a reasonable trade off, for the time being: for case level precision, a plagiarism detection r counts a true positive detection if it contributes to detecting at least τ 1 = 0.5 ∼ 50% of a plagiarism case s, and, if at least τ 2 = 0.5 ∼ 50% of r contributes to detecting plagiarism cases. Likewise, for case level recall, a plagiarism case s counts as detected if at least 50% of s are detected, and, if a plagiarism detection r contributes to detecting s while at least 50% of r contributes to detecting plagiarism cases in general. Likewise, D pairs|R denotes the subset of D pairs for which plagiarism was detected when requiring a minimal detection accuracy as per R defined above. Based on this notation, we compute document level precision and recall as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document level performance measures</head><formula xml:id="formula_6" coords="16,136.41,298.98,342.54,24.22">prec doc (S, R) = |D pairs|S ∩ D pairs|R | |D pairs|R | , r ec doc (S, R) = |D pairs|S ∩ D pairs|R | |D pairs|S | .</formula><p>Again, the thresholds τ 1 and τ 2 allow for adjusting the minimal required detection accuracy for R , but for document level recall, it is sufficient that at least one plagiarism case is detected beyond that accuracy in order for the corresponding document pair (d plg , d src ) to be counted as true positive detection. If none of the plagiarism cases present in (d plg , d src ) is detected beyond the minimal detection accuracy, it is counted as false negative, whereas if detections are made for a pair of documents in which no plagiarism case is present, it is counted as false positive.</p><p>Discussion Compared to the character level measures, the case level measures relax the fine-grained measurement of plagiarism detection quality to allow for judging a detection algorithm by its capability of "spotting" plagiarism cases reasonably well with respect to the minimum detection accuracy fixed by the thresholds τ 1 and τ 2 . For example, a user who is interested in maximizing case level performance may put emphasis on the coverage of all plagiarism cases rather than the precise extraction of each individual plagiarized pair of passages. The document level measures further relax the requirements to allow for judging a detection algorithm by its capability "to raise a flag" for a given pair of documents, disregarding whether it finds all plagiarism cases contained. For example, a user who is interested in maximizing these measures puts emphasis on being made suspicious which might lead to further, more detailed investigations. In this regard the three levels of performance measurement complement each other. To rank plagiarism detection with regard to their case level performance and their document level performance, we currently use the F α -Measure. While the best setting of α is also still unclear, we resort to α = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Survey of Text Alignment Approaches</head><p>Eleven of the 16 participants submitted softwares that implement text alignment, and for ten of them also a notebook describing their approach has been submitted. An analysis of these notebooks reveals that a number of building blocks are commonly used to build text alignment algorithms: (1) seeding, (2) extension, and (3) filtering. Text alignment is closely related to gene sequence alignment in bioinformatics, of which the terminology is borrowed: most of this year's approaches to text alignment implement the so-called seed and extend-paradigm which is frequently applied in gene sequence alignment. However, also a new trend can be observed, namely methods to predict the obfuscation type at hand and the dynamic choice or adjustment of text alignment approaches based on the prediction. In what follows, we survey the approaches in detail.</p><p>Seeding Given a suspicious document and a source document, matches (so-called "seeds") between the two documents are identified using some seed heuristic. Seed heuristics either identify exact matches or create matches by changing the underlying texts in a domain-specific or linguistically motivated way. Rationale for this is to pinpoint substrings that altogether make up for the perceived similarity between a suspicious and a source document. By coming up with as many reasonable seeds as possible, the subsequent step of extending them into aligned passages of text becomes a lot easier.</p><p>A number of seed heuristics have been applied by this year's participants:</p><p>-Alvi et al. <ref type="bibr" coords="17,198.45,323.87,11.62,8.64" target="#b5">[2]</ref> use character 20-grams and the Rabin-Karp algorithm for string matching between suspicious and source document. -Glinos <ref type="bibr" coords="17,181.84,347.53,11.62,8.64" target="#b11">[8]</ref> use word 1-grams and exact matching. In a supplementary approach, they use only the top 30 most frequent words longer than 5 characters. -Sanchez-Perez et al. <ref type="bibr" coords="17,238.90,371.20,16.60,8.64" target="#b35">[32]</ref> use sentences, whereas short sentences of less than 4 words are joined with their respective succeeding sentence, and they apply a sentence similarity measure where each sentence is represented as tf • idf -weighted vector which are compared using cosine similarity and the Dice coefficient. Sentences match if their similarities under both similarity measures exceeds a threshold of 0.33. -Gross and Modaresi <ref type="bibr" coords="17,233.77,442.69,16.60,8.64" target="#b15">[12]</ref> use skip word 2-grams with skips ranging from 1 to 4 and exact matching, whereas seeds appearing more than four times are discarded. -Torrejón and Ramos <ref type="bibr" coords="17,237.41,466.36,16.60,8.64" target="#b42">[39]</ref> reuse their previous approach which is based on sorted 3-grams and sorted 1-skip 3-grams and exact matching. -Abnar et al. <ref type="bibr" coords="17,202.18,490.03,11.62,8.64" target="#b4">[1]</ref> use word 2-grams to word 5-grams. Matching is based on a similarity measure that computes pairwise word similarities between a pair of n-grams, incorporating knowledge about how likely a given word is exchanged by another (e.g., because it is a synonym). This approach differs from others, where, for example, synonyms are normalized and exact matching is applied. -Palkovskii and Belov <ref type="bibr" coords="17,242.43,549.56,16.60,8.64" target="#b23">[20]</ref> use word n-grams, stop word n-grams, named entity n-grams, frequent word n-grams, stemmed n-grams, sorted n-grams, and skip-ngrams and exact matching, however, it is not clear which n is used. -Gillam and Notley [7] apply a custom fingerprinting approach and compute similarity-sensitive hash values over portions of the input documents, whereas algorithm details and parameter settings remain obscure. -Kong et al. <ref type="bibr" coords="17,196.37,620.81,16.60,8.64" target="#b19">[16]</ref> reuse their previous year's approach using sentences as seeds which match if they exceed a similarity threshold. -Shrestha et al. <ref type="bibr" coords="17,214.63,644.48,16.60,8.64" target="#b37">[34]</ref> also use sentences which are matched if their TER-p score exceeds a threshold, but they also use word 2-grams and exact matching.</p><p>Before computing seeds, many participants choose to collapse whitespace, reduce cases, remove non-alphanumeric characters, remove stop words, and stem the remaining words, if applicable to their respective seed heuristics.</p><p>Extension Given seed matches identified between a suspicious document and a source document, they are extended into aligned text passages between the two documents of maximal length, which are then reported as plagiarism detections. Rationale for merging seed matches is to determine whether a document contains plagiarized passages at all rather than just seeds matching by chance, and to identify a plagiarized passage as a whole rather than only its fragments.</p><p>The extension algorithms applied this year have become more diverse, including rule-based approaches, dynamic programming, and clustering-based approaches. In previous years, rule-based approaches have been used by almost everyone; they merge seeds into aligned passages if they are adjacent in both suspicious and source document and the size of the gap between them is below some threshold. The exact rules depend on the seeds used, and instead of using just one rule, many participants develop sets of constraints that have to be fulfilled by aligned passages in order to be reported as plagiarism detections. The complexity of the rule sets and their interdependencies has outgrown a casual description, whereas this year's rules are comparably simple. For example, Alvi et al. <ref type="bibr" coords="18,234.54,341.23,11.62,8.64" target="#b5">[2]</ref> merge seeds to aligned passages if they are less than 200 chars apart, and Gillam and Notley [7] continue to develop their previous year's approach which used to merge seeds that are less than 900 chars apart. However, given the success of alternative, more dynamic extension algorithms, crafting rule sets by hand appears not to be a competitive approach, anymore.</p><p>One of the classical approaches to extension is dynamic programming, and two participants make use of corresponding bioinformatics algorithms: Glinos <ref type="bibr" coords="18,436.11,412.96,11.62,8.64" target="#b11">[8]</ref> employ a variant of the Smith-Waterman algorithm, which they tailored to the application for pairs of texts instead of pairs of gene sequences, making improvements in terms of runtime and multiple detections. Oberreuter and Eiselt <ref type="bibr" coords="18,355.71,448.83,16.60,8.64" target="#b22">[19]</ref> employ an algorithm from the BLAST family of local gene sequence alignment algorithms. These algorithms can handle noise, which is why Glinos <ref type="bibr" coords="18,274.09,472.74,11.62,8.64" target="#b11">[8]</ref> is at liberty to use word 1-grams as seeds, which may help to uncover text similarities that are lost with longer seeds. However, these algorithms may have difficulties to handle heavy re-ordering of phrases and words.</p><p>Many participants apply some kind of clustering algorithm or at least algorithms which relate to clustering algorithms. In their secondary approach, Glinos <ref type="bibr" coords="18,442.79,520.56,11.62,8.64" target="#b11">[8]</ref> try to identify clusters of frequent topic-related words in order to better pinpoint summaries and highly obfuscated plagiarism, whereas they do not employ one of the standard clustering algorithms but a handcrafted set of rules. Similarly, Sanchez-Perez et al. <ref type="bibr" coords="18,464.00,556.42,16.60,8.64" target="#b35">[32]</ref> apply an approach that relates to divisive clustering. They first merge all subsequent seeds using a broad gap threshold into what they call fragments, and then divide the merged fragments until all divided fragments exceed a similarity threshold, in which case the resulting fragments are output as aligned pairs of passaged. By contrast, Gross and Modaresi <ref type="bibr" coords="18,192.11,616.20,16.60,8.64" target="#b15">[12]</ref> apply agglomerative single-linkage clustering, where pairs of seeds are merged based on a distance measure, where the merging order follows that of least distance, and the stopping criterion is defined by a distance threshold that may not be exceeded. Abnar et al. <ref type="bibr" coords="18,240.62,652.07,11.62,8.64" target="#b4">[1]</ref> apply the density-based clustering algorithm DBSCAN, and Palkovskii and Belov <ref type="bibr" coords="19,244.20,119.31,16.60,8.64" target="#b23">[20]</ref> apply what they call "angled ellipse-based graphical clustering," whereas, for the latter, it remains unclear which clustering algorithm is used, exactly, since no reference nor a description is given.</p><p>Filtering Given a set of aligned passages, a passage filter removes all aligned passages that do not meet certain criteria. Rationale for this is mainly to deal with overlapping passages and to discard extremely short passages, whereas a real-world plagiarism detection might attempt to discern reused text that has been properly acknowledged from reused text for which an acknowledgement is missing. Participants, however, use filtering mainly to optimize against the evaluation corpus in order to maximize the performances measured, which is of course impractical, but in the nature of things in a competition.</p><p>Alvi et al. <ref type="bibr" coords="19,193.76,257.54,11.62,8.64" target="#b5">[2]</ref> discard alignments of less than 200 chars length in the source document and less than 100 chars length in the suspicious document; Glinos <ref type="bibr" coords="19,436.90,269.50,11.62,8.64" target="#b11">[8]</ref> discard alignments that contain less than 40 words on both sides; Sanchez-Perez et al. <ref type="bibr" coords="19,464.00,281.45,16.60,8.64" target="#b35">[32]</ref> attempt to disambiguate and merge overlapping alignments and discard alignments of less than 150 chars length; Gross and Modaresi <ref type="bibr" coords="19,325.98,305.37,16.60,8.64" target="#b15">[12]</ref> discard alignments of less then 15 words length; Abnar et al. <ref type="bibr" coords="19,243.91,317.32,11.62,8.64" target="#b4">[1]</ref> and Shrestha et al. <ref type="bibr" coords="19,336.55,317.32,16.60,8.64" target="#b37">[34]</ref> discard short alignments based on an unclear threshold; and other participants either do not give details, or they do not filter alignments.</p><p>Remarks and New Trends Given the fact that eleven teams participated in text alignment this year, five of whom for the first time, we conclude that there is still a lot of interest in this shared task, and that there is also a lot to be accomplished still. One of the new trends that many participants have picked up simultaneously is that of tailoring their approaches to specific kinds of obfuscation and then to dynamically select the appropriate approach either based on a prediction which kind of obfuscation is at hand in a given pair of to be analyzed documents, or based on an a posteriori decision rule when applying more than one variant at the same time. This development is encouraging as it opens new avenues of research around text alignment, let alone the opportunity to improve significantly over one-fits-all approaches:</p><p>-Glinos <ref type="bibr" coords="19,180.70,487.43,11.62,8.64" target="#b11">[8]</ref> distinguishes word order-preserving plagiarism from all other kinds and apply their dynamic programming approach and their clustering-based approach at the same time. It is not entirely clear what the decision rule is, or which approach takes precedence over another. The clustering approach, however, turns out to aim at summaries in particular, so that it may be that the dynamic programming algorithm's output takes precedence. -Sanchez-Perez et al. <ref type="bibr" coords="19,236.58,559.16,16.60,8.64" target="#b35">[32]</ref> distinguish summaries from all other kinds and employ two parameter settings for their approach at the same time, one conservative, the other progressive. Afterwards, the decision which of the two outputs obtained is returned is based on the length imbalance between suspicious passage and source passage. If the source passage is more than 3 times longer than the suspicious passage, the progressive settings take precedence. -Torrejón and Ramos <ref type="bibr" coords="19,235.25,630.89,16.60,8.64" target="#b42">[39]</ref> attempted to tune their approach to the evaluation corpus based on hundreds of runs under different parameter settings. They discuss three parameter sets that are supposedly applicable in different situations, but they do not go further to combine the three settings in a dynamic manner. -Palkovskii and Belov <ref type="bibr" coords="20,239.50,314.65,16.60,8.64" target="#b23">[20]</ref> attempt to predict which of four kinds of obfuscation is at hand, namely no obfuscation, random obfuscation, summaries, and "undefined." Based on the prediction, a choice is made among four corresponding parameter sets.</p><p>Only little hints are given about the features used for prediction, and it remains entirely unclear how the prediction pipeline works, exactly, what classifiers are used, and how well the prediction performs. -Kong et al. <ref type="bibr" coords="20,198.44,386.39,16.60,8.64" target="#b19">[16]</ref> attempt to predict whether obfuscated or unobfuscated plagiarism is at hand. This distinction corresponds to that of Glinos <ref type="bibr" coords="20,374.21,398.34,11.62,8.64" target="#b11">[8]</ref> mentioned above. They employ logistic regression to train a classifier based on lexical similarity features using the training data set of the evaluation corpus. However, it remains unclear whether the entire document pairs are compared using the similarity measures and how the prediction is incorporated into their text alignment approach. Moreover, no analysis of prediction performance is conducted.</p><p>While this development is encouraging, it must be noted that the attempts made are still in their infancy and not yet analyzed well enough to form a conclusion whether they yield useful overall performance improvements or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Evaluation Results</head><p>In this section, we report on the evaluation of this year's submissions on the aforementioned evaluation corpora. Moreover, we conduct a cross-year evaluation of all softwares submitted since 2012 on the current evaluation corpus. We further differentiate performance with regard to obfuscation strategies to provide insights into how the softwares deal with different strengths of obfuscation. Finally, we compute the performances of all software using the newly proposed performance measures and contrast them with the traditionally applied measures.</p><p>Overall Results of 2014 Table <ref type="table" coords="20,270.43,642.18,4.98,8.64" target="#tab_1">2</ref> shows the overall performances of the eleven plagiarism detectors that implement text alignment and were submitted this year on the 2013 test data. The overall best performing approach is that of Sanchez-Perez et al. <ref type="bibr" coords="21,134.77,302.70,15.27,8.64" target="#b35">[32]</ref>, followed by that of Oberreuter and Eiselt <ref type="bibr" coords="21,321.24,302.70,16.60,8.64" target="#b22">[19]</ref> and Palkovskii and Belov <ref type="bibr" coords="21,443.68,302.70,15.27,8.64" target="#b23">[20]</ref>. The former two detectors have balanced precision and recall, while the latter does not. None of the detectors achieve perfect granularity, yet the scores obtained are very reasonable. While the best performing approach this year comes from a first-time participant, the performances of all five newcomers range from very good to poor. One detector's performance does not exceed the baseline. In terms of precision and granularity, the lower-ranked detectors have some shortcomings, whereas performance decreases more or less steadily toward the lower ranks. In terms of runtime, three detectors took more than one hour to finish, one of which took almost three days. The best performing detector of Torrejón and Ramos <ref type="bibr" coords="21,242.95,410.30,16.60,8.64" target="#b42">[39]</ref> finishes the 5185 document pairs in less than a minute, whereas the authors claim even faster runtimes on multi-core machines. Table <ref type="table" coords="21,175.22,434.21,4.98,8.64" target="#tab_2">3</ref> shows the overall performances of the elven plagiarism detectors on the supplemental test data, which comprises only unobfuscated, and randomly obfuscated plagiarism. These obfuscation strategies have been found to be easier to detect, which explains the generally higher performances. Interestingly, Sanchez-Perez et al. <ref type="bibr" coords="21,447.35,470.07,16.60,8.64" target="#b35">[32]</ref> and Palkovskii and Belov <ref type="bibr" coords="21,221.31,482.03,16.60,8.64" target="#b23">[20]</ref> switch places, which may be an artifact of the former's focus on detecting summary plagiarism and the latter's focus on detecting verbatim plagiarism. The performance differences, however, are not very big, and the global ranking does not change a lot compared to Table <ref type="table" coords="21,301.73,517.89,3.74,8.64" target="#tab_1">2</ref>. Note that the results of Table <ref type="table" coords="21,432.80,517.89,4.98,8.64" target="#tab_1">2</ref> determine the best performing approach of 2014, since the 2013 test data pose a much bigger challenge.</p><p>Cross-Year Evaluation between 2012 and 2014 Tables <ref type="table" coords="21,380.93,560.48,26.33,8.64" target="#tab_6">4 to 7</ref> show the performances of all 29 plagiarism detectors submitted since 2012 that implement text alignment on the 2013 test data. The overall performance of the detectors with regard to the plagdet score can be found in Table <ref type="table" coords="21,284.41,596.35,3.74,8.64" target="#tab_3">4</ref>. As can be seen, many of the approaches submitted 2014 significantly improve over the best performing detectors of previous years. Sanchez-Perez et al. <ref type="bibr" coords="21,218.19,620.26,16.60,8.64" target="#b35">[32]</ref> takes the lead across all years on the 2013 test data. The best performing detector from previous years from Kong et al. <ref type="bibr" coords="21,362.67,632.21,16.60,8.64" target="#b21">[18]</ref> is ranked sixth so that the 2014 participants seem to have raised the bar for future participants significantly. Again, these results must be taken with a grain of salt, since the 2013 test has been available to participants beforehand: although no participants mention in their notebook paper that they also used this corpus to train their approach, it may be the case that part of the good performance of the 2014 participants can be attributed to the fact that they had a priori access to the test data.</p><p>Regarding the obfuscation strategies, the detectors' performances correlate with their overall performance. On unobfuscated plagiarism (column "None" in the tables), Palkovskii and Belov <ref type="bibr" coords="26,222.32,191.04,16.60,8.64" target="#b23">[20]</ref> and Glinos <ref type="bibr" coords="26,287.73,191.04,11.62,8.64" target="#b11">[8]</ref> perform best, beating the performance of the first-ranked Sanchez-Perez et al. <ref type="bibr" coords="26,265.45,203.00,16.60,8.64" target="#b35">[32]</ref> by far. On random obfuscation and cyclic translation, however, the latter maintains his lead. On summary obfuscation, the approach of Glinos <ref type="bibr" coords="26,163.49,226.91,11.62,8.64" target="#b11">[8]</ref> and that of Suchomel et al. <ref type="bibr" coords="26,284.53,226.91,16.60,8.64" target="#b41">[38]</ref> perform best, again, outperforming Sanchez-Perez et al. <ref type="bibr" coords="26,180.03,238.86,16.60,8.64" target="#b35">[32]</ref> by far. This hints that the clustering approach of Glinos <ref type="bibr" coords="26,417.34,238.86,11.62,8.64" target="#b11">[8]</ref> works rather well, whereas the combination with the Smith-Waterman algorithm provides for a competitive trade off.</p><p>Table <ref type="table" coords="26,175.58,274.73,4.98,8.64" target="#tab_4">5</ref> shows the detectors' performances with regard to precision. In general, achieving a high precision appears to be less of a problem compared to achieving a high recall. This is underpinned by the fact that our basic baseline approach outperforms almost all detectors in precision. However, the detectors that perform best in precision typically have deficiencies in terms of recall, but not the other way around: the aforementioned overall best performing detectors achieve mid-range precision. The only exception to the rule is the detector of Glinos <ref type="bibr" coords="26,335.06,346.46,11.62,8.64" target="#b11">[8]</ref> which, for the first time, achieves both best overall precision and a competitive overall ranking with regard to plagdet performance.</p><p>Table <ref type="table" coords="26,175.25,382.33,4.98,8.64" target="#tab_5">6</ref> shows the detectors' performances with regard to recall. Four 2014 participants now outperform the formerly best performing pair of detectors submitted by Kong et al. <ref type="bibr" coords="26,180.73,406.24,15.77,8.64" target="#b21">[18,</ref><ref type="bibr" coords="26,198.95,406.24,11.83,8.64" target="#b20">17]</ref>. Some participants achieve 0.99 recall on unobfuscated plagiarism, whereas the recall on such plagiarism is generally high, even among the low-ranked detectors. Recall performances on random obfuscation and cyclic obfuscation correlate with those on the entire corpus. The best performing detector on summary obfuscation is still that of Suchomel et al. <ref type="bibr" coords="26,271.96,454.06,15.27,8.64" target="#b41">[38]</ref>, whereas even Glinos <ref type="bibr" coords="26,378.73,454.06,11.62,8.64" target="#b11">[8]</ref> performs significantly worse in terms of recall.</p><p>Table <ref type="table" coords="26,174.32,477.97,4.98,8.64" target="#tab_6">7</ref> shows the detectors' performances with regard to granularity. The top fifth of the table entries have unanimously perfect granularity, so that these approaches are ranked alphabetically. Despite their perfect granularity scores, these detectors do not perform well with regard to other measures which may hint that these detectors emphasize granularity performance too much at the expense of recall, precision, and therefore plagdet. With the exception of summary obfuscation and therefore the performance on the entire corpus, the top half of the table shows near-perfect scores. Only summary obfuscation still poses a slight challenge, whereas it appears that granularity is still mostly under control. We repeat our concern, however, that participants often resort to postretrieval filtering in order to optimize granularity only for the sake of achieving a good ranking, while some admit that they would not do this in practice.</p><p>New Performance Measures Table <ref type="table" coords="26,288.14,616.20,4.98,8.64">8</ref> contrasts the character level performance measures, which are traditionally applied to measure the performance of a plagiarism detector, to the new measures introduced above, which measure performance at case level and at document level. The table shows the performances of all detectors that have been Table <ref type="table" coords="27,158.53,161.76,3.36,8.06">8</ref>. Cross-year evaluation of text alignment software submissions from 2012 to 2014 with respect to performance measures at character level, case level, and document level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Software Submission</head><p>Character Level Case Level Document Level</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Team</head><p>Year plagdet prec r ec gran prec r ec F1 prec r ec F1</p><p>evaluated for text alignment since 2012. When comparing the plagdet performances with the F 1 performances of the case level measures and the document level measures, they are highly correlated. This is in the nature of things, since it is unlikely that a plagiarism detector that performs poor at character level performs excellent at case level or at document level. However, the rankings still differ. For example, at case level, the detectors of Shrestha et al. <ref type="bibr" coords="28,233.11,179.09,16.60,8.64" target="#b37">[34]</ref> and Gross and Modaresi <ref type="bibr" coords="28,351.66,179.09,16.60,8.64" target="#b15">[12]</ref> are ranked second and third to that of Sanchez-Perez et al. <ref type="bibr" coords="28,257.65,191.04,15.27,8.64" target="#b35">[32]</ref>, whereas the detector of Oberreuter and Eiselt <ref type="bibr" coords="28,464.00,191.04,16.60,8.64" target="#b22">[19]</ref> looses some ranks. Most of the other detectors maintain their rank relative to the other detectors. It can be followed that the detectors whose ranks are better than before do a sensible job of "spotting" at least 50% of each plagiarism case, whereas, at character level, they are outperformed by other detectors. At document level, the detector of Glinos <ref type="bibr" coords="28,316.01,250.82,11.62,8.64" target="#b11">[8]</ref> catches up with that of Sanchez-Perez et al. <ref type="bibr" coords="28,156.73,262.77,15.27,8.64" target="#b35">[32]</ref>, whereas those of Shrestha et al. <ref type="bibr" coords="28,306.80,262.77,16.60,8.64" target="#b37">[34]</ref> and Gross and Modaresi <ref type="bibr" coords="28,425.76,262.77,16.60,8.64" target="#b15">[12]</ref> also gain a few ranks. Interestingly, also detectors that are ranked lower at character level, such as those of Kong et al. <ref type="bibr" coords="28,229.89,286.69,15.77,8.64" target="#b20">[17,</ref><ref type="bibr" coords="28,248.80,286.69,13.28,8.64" target="#b19">16]</ref> and Saremi and Yaghmaee <ref type="bibr" coords="28,376.38,286.69,16.60,8.64" target="#b36">[33]</ref> perform significantly better, which hints that these detectors, along with the other top-ranked ones, are useful for raising suspicions about a given pair of documents.</p><p>Nevertheless, the detector of Sanchez-Perez et al. <ref type="bibr" coords="28,347.44,322.55,16.60,8.64" target="#b35">[32]</ref> dominates all other detectors at all three levels.</p><p>Assessing a new performance measures is a difficult task, since at the beginning it remains unclear whether the intuitions that guided their definition are captured well, and whether they actually reveal performance aspects which other measures do not capture well enough. In our case, all of the current plagiarism detectors have been optimized against the character level performance measures, so that it cannot, yet, be told whether it is possible to build a plagiarism detection which outperforms all others, say, at document level, but not so at the other levels. Only time will tell. Moreover, the hyper-parameters τ 1 and τ 2 as well as the weight α used in the F α -Measure are not yet fixed and subject to ongoing research. Therefore, it would be premature to give the new sets of performance measures precedence over the existing performance measures, which have been already adopted by the community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Outlook</head><p>Altogether, the sixth international competition on plagiarism detection at PAN 2014 has been a success: despite being organized for the sixth time in a row, we see steady interest from from the community to further study this task, as is evidenced by the fact that many participants have returned to make a new submissions. Moreover, the two tasks source retrieval and text alignment are picked up by new participants each year, which makes for a steady stream of new input and inspiration at solving these tasks. In total, 16 teams submitted plagiarism detectors, 6 for source retrieval and 11 for text alignment.</p><p>Since both source retrieval and text alignment are in the production phase of their shared task life cycles-they are well-defined and all evaluation resources are set up and provide for a challenging testbed-we have refrained from introducing too many changes to the two tasks. Nevertheless, we continuously work to make the maintenance of the evaluation resources for both tasks easier. This pertains particularly to the resources required for source retrieval which include a fully-fledged search engine for ClueWeb corpus.</p><p>Moreover, we continue to pursue our goal of automating evaluations within shared tasks by developing the TIRA experimentation platform <ref type="bibr" coords="29,358.66,167.13,15.27,8.64" target="#b14">[11]</ref>. TIRA facilitates software submissions, where participants submit their plagiarism detection software to be evaluated at our site <ref type="bibr" coords="29,194.60,191.04,10.58,8.64" target="#b12">[9]</ref>. As of this year, the newly introduced web front end for TIRA allows participants to conduct self-service evaluations on the test data of both our shared tasks under our supervision and guidance, whereas the test data remains hidden from direct access from participants. <ref type="foot" coords="29,233.72,225.24,3.49,6.05" target="#foot_5">6</ref> This has allowed us to put participants back in charge of executing their software while the software itself remains in a running state within virtual machines managed by TIRA. Based on this technology, we conduct cross-year evaluations of all plagiarism detectors that have been submitted to our tasks since 2012.</p><p>This year, we place emphasis on analyzing the detection performances of the plagiarism detectors by developing new means of visualizing their performance as well as new performance measures that shed light on different performance aspects of plagiarism detection than the traditionally applied measures. This is ongoing research, and our goal is to provide a more in-depth analysis of each plagiarism detector that enters our evaluations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,199.04,251.97,217.26,8.12"><head>Figure 1</head><label>1</label><figDesc>Figure1. Generic retrieval process to detect plagiarism<ref type="bibr" coords="2,399.13,252.32,13.74,7.77" target="#b39">[36]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,134.77,372.20,345.82,8.12;4,134.77,383.51,345.81,7.77;4,134.77,394.47,345.81,7.77;4,134.77,405.43,255.08,7.77"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Overview of the building blocks used in the evaluation of the source retrieval subtask. The components are organized by the two activities corpus construction and evaluation runs (top two rows). Both activities are based on a static evaluation infrastructure (bottom row) consisting of an experimentation platform, web search engines, and a web corpus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,134.77,234.19,345.81,8.12;6,134.77,245.50,345.81,7.77;6,134.77,256.46,90.15,7.77"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Effect of near-duplicates on computing precision (left) and recall (right) of retrieved source documents. Without taking near-duplicates into account, a lot of potentially correct sources might be missed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,246.99,376.25,56.07,8.96;6,303.06,380.75,9.81,6.12;6,316.59,376.57,164.01,8.64;6,134.77,388.21,213.63,9.65;6,348.39,392.71,9.77,6.12;6,361.41,388.21,119.18,9.65;6,134.77,400.16,249.91,9.65;6,134.77,420.88,8.25,8.74;6,143.01,425.38,9.81,6.12;6,156.11,420.88,323.27,9.65;6,134.81,435.82,8.25,8.74;6,143.06,440.32,9.77,6.12;6,156.11,435.82,323.01,9.65"><head></head><label></label><figDesc>Further, let D src denote the set of all near-duplicates of a given set of source documents D src of d plg and let D ret denote the subset of D src that have at least one corresponding true positive detection in D ret : D src = {d dup | d dup ∈ D and ∃d src ∈ D src : d dup is a true positive detection of d src }, D ret = {d src | d src ∈ D src and ∃d ret ∈ D ret : d ret is a true positive detection of d src }.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="11,144.19,345.39,326.97,8.12"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Recall at a specific number of downloads per participant averaged over all topics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="16,309.61,118.99,170.98,8.96;16,134.77,130.95,345.83,9.65;16,134.77,142.90,345.83,9.65;16,134.77,155.18,191.18,8.64;16,141.69,176.78,331.98,9.96;16,134.77,198.69,345.83,9.65;16,140.67,220.61,334.01,9.96;16,134.77,242.53,345.83,9.65"><head></head><label></label><figDesc>Let S, R, and R be defined as above. Further, let D plg be the set of suspicious documents and D src be the set potential source documents. Then D pairs = D plg × D src denotes the set of possible pairs of documents that a plagiarism detector may analyze, whereas D pairs|S = {(d plg , d src ) | (d plg , d src ) ∈ D pairs and ∃s ∈ S : d plg ∈ s and d src ∈ s} denotes the subset of D pairs whose document pairs contain the plagiarism cases S, and D pairs|R = {(d plg , d src ) | (d plg , d src ) ∈ D pairs and ∃r ∈ R : d plg ∈ r and d src ∈ r} denotes the corresponding subset of D pairs for which plagiarism was detected in R.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="10,134.77,115.83,345.86,193.36"><head>Table 1 .</head><label>1</label><figDesc>Source retrieval results with respect to retrieval performance and cost-effectiveness.</figDesc><table coords="10,134.77,133.64,345.86,175.55"><row><cell cols="3">Software Submission Downloaded</cell><cell>Total</cell><cell></cell><cell cols="3">Workload to No Runtime</cell></row><row><cell>Team</cell><cell>Year</cell><cell>Sources</cell><cell cols="2">Workload</cell><cell cols="3">1st Detection Detect.</cell></row><row><cell cols="2">(alphabetical order)</cell><cell cols="5">F1 Prec. Rec. Queries Dwlds Queries Dwlds</cell><cell></cell></row><row><cell>Elizalde</cell><cell>2013</cell><cell>0.16 0.12 0.37</cell><cell>41.6</cell><cell>83.9</cell><cell>18.0</cell><cell>18.2</cell><cell>4 11:18:50</cell></row><row><cell>Elizalde</cell><cell>2014</cell><cell>0.34 0.40 0.39</cell><cell>54.5</cell><cell>33.2</cell><cell>16.4</cell><cell>3.9</cell><cell>7 04:02:00</cell></row><row><cell>Foltynek</cell><cell>2013</cell><cell>0.11 0.08 0.26</cell><cell>166.8</cell><cell>72.7</cell><cell>180.4</cell><cell cols="2">4.3 32 152:26:23</cell></row><row><cell>Gillam</cell><cell>2013</cell><cell>0.06 0.04 0.15</cell><cell>15.7</cell><cell>86.8</cell><cell>16.1</cell><cell cols="2">28.6 34 02:24:59</cell></row><row><cell>Haggag</cell><cell>2013</cell><cell>0.38 0.67 0.31</cell><cell>41.7</cell><cell>5.2</cell><cell>13.9</cell><cell cols="2">1.4 12 46:09:21</cell></row><row><cell>Kong</cell><cell>2013</cell><cell>0.01 0.01 0.59</cell><cell cols="2">47.9 5185.3</cell><cell cols="2">2.5 210.2</cell><cell>0 106:13:46</cell></row><row><cell>Kong</cell><cell>2014</cell><cell>0.12 0.08 0.48</cell><cell cols="2">83.5 207.1</cell><cell>85.7</cell><cell>24.9</cell><cell>6 24:03:31</cell></row><row><cell>Lee</cell><cell>2013</cell><cell>0.40 0.58 0.37</cell><cell>48.4</cell><cell>10.9</cell><cell>6.5</cell><cell>2.0</cell><cell>9 09:17:10</cell></row><row><cell>Prakash</cell><cell>2014</cell><cell>0.39 0.38 0.51</cell><cell>60.0</cell><cell>38.8</cell><cell>8.1</cell><cell>3.8</cell><cell>7 19:47:45</cell></row><row><cell>Suchomel</cell><cell>2013</cell><cell>0.05 0.04 0.23</cell><cell cols="2">17.8 283.0</cell><cell>3.4</cell><cell cols="2">64.9 18 75:12:56</cell></row><row><cell>Suchomel</cell><cell>2014</cell><cell>0.11 0.08 0.40</cell><cell cols="2">19.5 237.3</cell><cell>3.1</cell><cell>38.6</cell><cell>2 45:42:06</cell></row><row><cell>Williams</cell><cell>2013</cell><cell>0.47 0.60 0.47</cell><cell>117.1</cell><cell>12.4</cell><cell>23.3</cell><cell>2.2</cell><cell>7 76:58:22</cell></row><row><cell>Williams</cell><cell>2014</cell><cell>0.47 0.57 0.48</cell><cell>117.1</cell><cell>14.4</cell><cell>18.8</cell><cell>2.3</cell><cell>4 39:44:11</cell></row><row><cell>Zubarev</cell><cell>2014</cell><cell>0.45 0.54 0.45</cell><cell>37.0</cell><cell>18.6</cell><cell>5.4</cell><cell>2.3</cell><cell>3 40:42:18</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="20,156.19,115.83,302.97,153.26"><head>Table 2 .</head><label>2</label><figDesc>Text alignment performances of the 2014 participants on the 2013 test data.</figDesc><table coords="20,174.15,133.64,267.03,135.45"><row><cell>Team</cell><cell cols="3">PlagDet Recall Precision Granularity Runtime</cell></row><row><cell cols="2">Sanchez-Perez 0.87818 0.87904 0.88168</cell><cell>1.00344</cell><cell>00:25:35</cell></row><row><cell>Oberreuter</cell><cell>0.86933 0.85779 0.88595</cell><cell>1.00369</cell><cell>00:05:31</cell></row><row><cell>Palkovskii</cell><cell>0.86806 0.82637 0.92227</cell><cell>1.00580</cell><cell>01:10:04</cell></row><row><cell>Glinos</cell><cell>0.85930 0.79331 0.96253</cell><cell>1.01695</cell><cell>00:23:13</cell></row><row><cell>Shrestha</cell><cell>0.84404 0.83782 0.85906</cell><cell>1.00701</cell><cell>69:51:15</cell></row><row><cell>R. Torrejón</cell><cell>0.82952 0.76903 0.90427</cell><cell>1.00278</cell><cell>00:00:42</cell></row><row><cell>Gross</cell><cell>0.82642 0.76622 0.93272</cell><cell>1.02514</cell><cell>00:03:00</cell></row><row><cell>Kong</cell><cell>0.82161 0.80746 0.84006</cell><cell>1.00309</cell><cell>00:05:26</cell></row><row><cell>Abnar</cell><cell>0.67220 0.61163 0.77330</cell><cell>1.02245</cell><cell>01:27:00</cell></row><row><cell>Alvi</cell><cell>0.65954 0.55068 0.93375</cell><cell>1.07111</cell><cell>00:04:57</cell></row><row><cell>Baseline</cell><cell>0.42191 0.34223 0.92939</cell><cell>1.27473</cell><cell>00:30:30</cell></row><row><cell>Gillam</cell><cell>0.28302 0.16840 0.88630</cell><cell>1.00000</cell><cell>00:00:55</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="21,141.25,115.83,332.85,153.26"><head>Table 3 .</head><label>3</label><figDesc>Text alignment performances of the 2014 participants on the supplemental test data.</figDesc><table coords="21,174.15,133.64,267.03,135.45"><row><cell>Team</cell><cell cols="3">PlagDet Recall Precision Granularity Runtime</cell></row><row><cell>Palkovskii</cell><cell>0.90779 0.88916 0.92757</cell><cell>1.00027</cell><cell>00:57:15</cell></row><row><cell>Oberreuter</cell><cell>0.89268 0.91539 0.87171</cell><cell>1.00051</cell><cell>00:05:37</cell></row><row><cell cols="2">Sanchez-Perez 0.89197 0.91984 0.86606</cell><cell>1.00026</cell><cell>00:22:10</cell></row><row><cell>Glinos</cell><cell>0.88770 0.84511 0.96007</cell><cell>1.01761</cell><cell>00:19:32</cell></row><row><cell>Shrestha</cell><cell>0.86806 0.89839 0.84418</cell><cell>1.00381</cell><cell>74:52:47</cell></row><row><cell>Gross</cell><cell>0.85500 0.81819 0.92522</cell><cell>1.02187</cell><cell>00:02:49</cell></row><row><cell>R. Torrejón</cell><cell>0.84870 0.80267 0.90032</cell><cell>1.00000</cell><cell>00:00:31</cell></row><row><cell>Kong</cell><cell>0.83514 0.84156 0.82882</cell><cell>1.00000</cell><cell>00:05:18</cell></row><row><cell>Alvi</cell><cell>0.73416 0.67283 0.90081</cell><cell>1.06943</cell><cell>00:04:17</cell></row><row><cell>Abnar</cell><cell>0.66377 0.84779 0.54833</cell><cell>1.00455</cell><cell>20:14:51</cell></row><row><cell>Baseline</cell><cell>0.64740 0.52838 0.90024</cell><cell>1.04005</cell><cell>00:15:15</cell></row><row><cell>Gillam</cell><cell>0.44076 0.29661 0.85744</cell><cell>1.00000</cell><cell>00:00:56</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="22,134.77,181.45,346.06,410.88"><head>Table 4 .</head><label>4</label><figDesc>Cross-year evaluation of text alignment software submissions from 2012 to 2014 with respect to plagdet. The darker a cell, the better the performance compared to the entire column.</figDesc><table coords="22,134.77,216.54,346.06,375.78"><row><cell cols="2">Software Submission</cell><cell cols="4">Obfuscation Strategies of the 2013 Evaluation Corpus</cell><cell>Entire Corpus</cell></row><row><cell>Team</cell><cell>Year</cell><cell>None</cell><cell>Random</cell><cell>Cyclic translation</cell><cell>Summary</cell><cell></cell></row><row><cell>Sanchez-Perez</cell><cell>2014</cell><cell>0.90032</cell><cell>0.88417</cell><cell>0.88659</cell><cell>0.56070</cell><cell>0.87818</cell></row><row><cell>Oberreuter</cell><cell>2014</cell><cell>0.91976</cell><cell>0.86775</cell><cell>0.88118</cell><cell>0.36804</cell><cell>0.86933</cell></row><row><cell>Palkovskii</cell><cell>2014</cell><cell>0.96004</cell><cell>0.86495</cell><cell>0.85750</cell><cell>0.27645</cell><cell>0.86806</cell></row><row><cell>Glinos</cell><cell>2014</cell><cell>0.96236</cell><cell>0.80623</cell><cell>0.84722</cell><cell>0.62359</cell><cell>0.85930</cell></row><row><cell>Shrestha</cell><cell>2014</cell><cell>0.89174</cell><cell>0.86556</cell><cell>0.84384</cell><cell>0.15550</cell><cell>0.84404</cell></row><row><cell>Kong</cell><cell>2012</cell><cell>0.87249</cell><cell>0.83242</cell><cell>0.85212</cell><cell>0.43635</cell><cell>0.83679</cell></row><row><cell>R. Torrejón</cell><cell>2014</cell><cell>0.93184</cell><cell>0.75378</cell><cell>0.85899</cell><cell>0.35298</cell><cell>0.82952</cell></row><row><cell>Oberreuter</cell><cell>2012</cell><cell>0.94170</cell><cell>0.74955</cell><cell>0.84618</cell><cell>0.13208</cell><cell>0.82678</cell></row><row><cell>Gross</cell><cell>2014</cell><cell>0.89950</cell><cell>0.80293</cell><cell>0.83825</cell><cell>0.31869</cell><cell>0.82642</cell></row><row><cell>R. Torrejón</cell><cell>2013</cell><cell>0.92586</cell><cell>0.74711</cell><cell>0.85113</cell><cell>0.34131</cell><cell>0.82220</cell></row><row><cell>Kong</cell><cell>2014</cell><cell>0.83777</cell><cell>0.82300</cell><cell>0.85162</cell><cell>0.43135</cell><cell>0.82161</cell></row><row><cell>Kong</cell><cell>2013</cell><cell>0.82740</cell><cell>0.82281</cell><cell>0.85181</cell><cell>0.43399</cell><cell>0.81896</cell></row><row><cell>Palkovskii</cell><cell>2012</cell><cell>0.88161</cell><cell>0.79692</cell><cell>0.74032</cell><cell>0.27507</cell><cell>0.79155</cell></row><row><cell>R. Torrejón</cell><cell>2012</cell><cell>0.88222</cell><cell>0.70151</cell><cell>0.80112</cell><cell>0.44184</cell><cell>0.78767</cell></row><row><cell>Suchomel</cell><cell>2013</cell><cell>0.81761</cell><cell>0.75276</cell><cell>0.67544</cell><cell>0.61011</cell><cell>0.74482</cell></row><row><cell>Suchomel</cell><cell>2012</cell><cell>0.89848</cell><cell>0.65213</cell><cell>0.63088</cell><cell>0.50087</cell><cell>0.73224</cell></row><row><cell>Saremi</cell><cell>2013</cell><cell>0.84963</cell><cell>0.65668</cell><cell>0.70903</cell><cell>0.11116</cell><cell>0.69913</cell></row><row><cell>Shrestha</cell><cell>2013</cell><cell>0.89369</cell><cell>0.66714</cell><cell>0.62719</cell><cell>0.11860</cell><cell>0.69551</cell></row><row><cell>Abnar</cell><cell>2014</cell><cell>0.85124</cell><cell>0.49058</cell><cell>0.67370</cell><cell>0.17148</cell><cell>0.67220</cell></row><row><cell>Alvi</cell><cell>2014</cell><cell>0.92693</cell><cell>0.50247</cell><cell>0.54506</cell><cell>0.09032</cell><cell>0.65954</cell></row><row><cell>Kueppers</cell><cell>2012</cell><cell>0.81977</cell><cell>0.51602</cell><cell>0.56932</cell><cell>0.13848</cell><cell>0.62772</cell></row><row><cell>Palkovskii</cell><cell>2013</cell><cell>0.82431</cell><cell>0.49959</cell><cell>0.60694</cell><cell>0.09943</cell><cell>0.61523</cell></row><row><cell>Nourian</cell><cell>2013</cell><cell>0.90136</cell><cell>0.35076</cell><cell>0.43864</cell><cell>0.11535</cell><cell>0.57716</cell></row><row><cell>Sánchez-Vega</cell><cell>2012</cell><cell>0.52179</cell><cell>0.45598</cell><cell>0.44323</cell><cell>0.28807</cell><cell>0.45923</cell></row><row><cell>Baseline</cell><cell></cell><cell>0.93404</cell><cell>0.07123</cell><cell>0.10630</cell><cell>0.04462</cell><cell>0.42191</cell></row><row><cell>Gillam</cell><cell>2012</cell><cell>0.87655</cell><cell>0.04723</cell><cell>0.01225</cell><cell>0.00218</cell><cell>0.41373</cell></row><row><cell>Gillam</cell><cell>2013</cell><cell>0.85884</cell><cell>0.04191</cell><cell>0.01224</cell><cell>0.00218</cell><cell>0.40059</cell></row><row><cell>Gillam</cell><cell>2014</cell><cell>0.66329</cell><cell>0.05500</cell><cell>0.00403</cell><cell>0.00000</cell><cell>0.28302</cell></row><row><cell>Jayapal</cell><cell>2013</cell><cell>0.38780</cell><cell>0.18148</cell><cell>0.18181</cell><cell>0.05940</cell><cell>0.27081</cell></row><row><cell>Jayapal</cell><cell>2012</cell><cell>0.34758</cell><cell>0.12049</cell><cell>0.10504</cell><cell>0.04541</cell><cell>0.20169</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="23,134.77,181.32,346.06,411.13"><head>Table 5 .</head><label>5</label><figDesc>Cross-year evaluation of text alignment software submissions from 2012 to 2014 with respect to precision. The darker a cell, the better the performance compared to the entire column.</figDesc><table coords="23,134.77,216.42,346.06,376.03"><row><cell cols="2">Software Submission</cell><cell cols="4">Obfuscation Strategies of the 2013 Evaluation Corpus</cell><cell>Entire Corpus</cell></row><row><cell>Team</cell><cell>Year</cell><cell>None</cell><cell>Random</cell><cell>Cyclic translation</cell><cell>Summary</cell><cell></cell></row><row><cell>Glinos</cell><cell>2014</cell><cell>0.96445</cell><cell>0.96951</cell><cell>0.96165</cell><cell>0.96451</cell><cell>0.96253</cell></row><row><cell>Nourian</cell><cell>2013</cell><cell>0.92921</cell><cell>0.96274</cell><cell>0.95856</cell><cell>0.99972</cell><cell>0.94707</cell></row><row><cell>Jayapal</cell><cell>2012</cell><cell>0.98542</cell><cell>0.95984</cell><cell>0.89590</cell><cell>0.83259</cell><cell>0.94507</cell></row><row><cell>Alvi</cell><cell>2014</cell><cell>0.91875</cell><cell>0.94785</cell><cell>0.95984</cell><cell>0.88036</cell><cell>0.93375</cell></row><row><cell>Gross</cell><cell>2014</cell><cell>0.91761</cell><cell>0.96000</cell><cell>0.92105</cell><cell>0.94876</cell><cell>0.93272</cell></row><row><cell>Baseline</cell><cell></cell><cell>0.88741</cell><cell>0.98101</cell><cell>0.97825</cell><cell>0.91147</cell><cell>0.92939</cell></row><row><cell>Palkovskii</cell><cell>2014</cell><cell>0.95584</cell><cell>0.91453</cell><cell>0.89941</cell><cell>0.91315</cell><cell>0.92227</cell></row><row><cell>R. Torrejón</cell><cell>2014</cell><cell>0.89901</cell><cell>0.93843</cell><cell>0.90088</cell><cell>0.89793</cell><cell>0.90427</cell></row><row><cell>R. Torrejón</cell><cell>2013</cell><cell>0.90060</cell><cell>0.90996</cell><cell>0.89514</cell><cell>0.90750</cell><cell>0.89484</cell></row><row><cell>Oberreuter</cell><cell>2012</cell><cell>0.89037</cell><cell>0.87921</cell><cell>0.90328</cell><cell>0.98983</cell><cell>0.89443</cell></row><row><cell>Gillam</cell><cell>2014</cell><cell>0.88097</cell><cell>0.95157</cell><cell>1.00000</cell><cell>0.00000</cell><cell>0.88630</cell></row><row><cell>Oberreuter</cell><cell>2014</cell><cell>0.85231</cell><cell>0.90608</cell><cell>0.89977</cell><cell>0.93581</cell><cell>0.88595</cell></row><row><cell>Gillam</cell><cell>2012</cell><cell>0.88128</cell><cell>0.95572</cell><cell>0.97273</cell><cell>0.99591</cell><cell>0.88532</cell></row><row><cell>Gillam</cell><cell>2013</cell><cell>0.88088</cell><cell>0.95968</cell><cell>0.97273</cell><cell>0.99591</cell><cell>0.88487</cell></row><row><cell>Sanchez-Perez</cell><cell>2014</cell><cell>0.83369</cell><cell>0.91015</cell><cell>0.88465</cell><cell>0.99910</cell><cell>0.88168</cell></row><row><cell>Jayapal</cell><cell>2013</cell><cell>0.91989</cell><cell>0.92314</cell><cell>0.85653</cell><cell>0.68832</cell><cell>0.87901</cell></row><row><cell>Shrestha</cell><cell>2013</cell><cell>0.80933</cell><cell>0.92335</cell><cell>0.88008</cell><cell>0.90455</cell><cell>0.87461</cell></row><row><cell></cell><cell>2012</cell><cell></cell><cell>0.89889</cell><cell>0.89985</cell><cell>0.86239</cell><cell>0.86923</cell></row><row><cell>Saremi</cell><cell>2013</cell><cell>0.82676</cell><cell>0.91810</cell><cell>0.84819</cell><cell>0.94600</cell><cell>0.86509</cell></row><row><cell>Shrestha</cell><cell>2014</cell><cell>0.82202</cell><cell>0.91098</cell><cell>0.84604</cell><cell>0.93862</cell><cell>0.85906</cell></row><row><cell>Kong</cell><cell>2012</cell><cell>0.80786</cell><cell>0.89367</cell><cell>0.85423</cell><cell>0.96399</cell><cell>0.85297</cell></row><row><cell>Suchomel</cell><cell>2012</cell><cell>0.81678</cell><cell>0.87581</cell><cell>0.85151</cell><cell>0.87478</cell><cell>0.84437</cell></row><row><cell>Kong</cell><cell>2014</cell><cell>0.78726</cell><cell>0.87003</cell><cell>0.85822</cell><cell>0.96381</cell><cell>0.84006</cell></row><row><cell>Kong</cell><cell>2013</cell><cell>0.76077</cell><cell>0.86224</cell><cell>0.85744</cell><cell>0.96384</cell><cell>0.82859</cell></row><row><cell>R. Torrejón</cell><cell>2012</cell><cell>0.81313</cell><cell>0.83881</cell><cell>0.81159</cell><cell>0.92666</cell><cell>0.82540</cell></row><row><cell>Palkovskii</cell><cell>2012</cell><cell>0.79219</cell><cell>0.84844</cell><cell>0.83218</cell><cell>0.94736</cell><cell>0.82371</cell></row><row><cell>Palkovskii</cell><cell>2013</cell><cell>0.79971</cell><cell>0.93137</cell><cell>0.82207</cell><cell>0.67604</cell><cell>0.81699</cell></row><row><cell>Abnar</cell><cell>2014</cell><cell>0.74910</cell><cell>0.82988</cell><cell>0.76575</cell><cell>0.92946</cell><cell>0.77330</cell></row><row><cell>Suchomel</cell><cell>2013</cell><cell>0.69323</cell><cell>0.82973</cell><cell>0.68494</cell><cell>0.67088</cell><cell>0.72514</cell></row><row><cell>Sánchez-Vega</cell><cell>2012</cell><cell>0.40340</cell><cell>0.49524</cell><cell>0.37300</cell><cell>0.45184</cell><cell>0.39857</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="24,134.77,181.36,346.06,411.09"><head>Table 6 .</head><label>6</label><figDesc>Cross-year evaluation of text alignment software submissions from 2012 to 2014 with respect to recall. The darker a cell, the better the performance compared to the entire column.</figDesc><table coords="24,134.77,216.46,346.06,375.99"><row><cell cols="2">Software Submission</cell><cell cols="4">Obfuscation Strategies of the 2013 Evaluation Corpus</cell><cell>Entire Corpus</cell></row><row><cell>Team</cell><cell>Year</cell><cell>None</cell><cell>Random</cell><cell>Cyclic translation</cell><cell>Summary</cell><cell></cell></row><row><cell>Sanchez-Perez</cell><cell>2014</cell><cell>0.97853</cell><cell>0.86067</cell><cell>0.88959</cell><cell>0.41274</cell><cell>0.87904</cell></row><row><cell>Oberreuter</cell><cell>2014</cell><cell>0.99881</cell><cell>0.83254</cell><cell>0.86335</cell><cell>0.24455</cell><cell>0.85779</cell></row><row><cell>Shrestha</cell><cell>2014</cell><cell>0.97438</cell><cell>0.83161</cell><cell>0.85318</cell><cell>0.08875</cell><cell>0.83782</cell></row><row><cell>Palkovskii</cell><cell>2014</cell><cell>0.96428</cell><cell>0.82244</cell><cell>0.82031</cell><cell>0.17672</cell><cell>0.82637</cell></row><row><cell>Kong</cell><cell>2012</cell><cell>0.94836</cell><cell>0.77903</cell><cell>0.85003</cell><cell>0.29892</cell><cell>0.82449</cell></row><row><cell>Kong</cell><cell>2013</cell><cell>0.90682</cell><cell>0.78682</cell><cell>0.84626</cell><cell>0.30017</cell><cell>0.81344</cell></row><row><cell>Kong</cell><cell>2014</cell><cell>0.89521</cell><cell>0.78079</cell><cell>0.84512</cell><cell>0.29636</cell><cell>0.80746</cell></row><row><cell>Glinos</cell><cell>2014</cell><cell>0.96028</cell><cell>0.72478</cell><cell>0.76248</cell><cell>0.48605</cell><cell>0.79331</cell></row><row><cell>Saremi</cell><cell>2013</cell><cell>0.95416</cell><cell>0.68877</cell><cell>0.80473</cell><cell>0.10209</cell><cell>0.77123</cell></row><row><cell>R. Torrejón</cell><cell>2014</cell><cell>0.96715</cell><cell>0.62985</cell><cell>0.82082</cell><cell>0.23149</cell><cell>0.76903</cell></row><row><cell>Oberreuter</cell><cell>2012</cell><cell>0.99932</cell><cell>0.65322</cell><cell>0.79587</cell><cell>0.07076</cell><cell>0.76864</cell></row><row><cell>Gross</cell><cell>2014</cell><cell>0.90724</cell><cell>0.71884</cell><cell>0.78410</cell><cell>0.20577</cell><cell>0.76622</cell></row><row><cell>Suchomel</cell><cell>2013</cell><cell>0.99637</cell><cell>0.68886</cell><cell>0.66621</cell><cell>0.56296</cell><cell>0.76593</cell></row><row><cell>R. Torrejón</cell><cell>2013</cell><cell>0.95256</cell><cell>0.63370</cell><cell>0.81124</cell><cell>0.21593</cell><cell>0.76190</cell></row><row><cell>Palkovskii</cell><cell>2012</cell><cell>0.99379</cell><cell>0.75130</cell><cell>0.66672</cell><cell>0.16089</cell><cell>0.76181</cell></row><row><cell>R. Torrejón</cell><cell>2012</cell><cell>0.96414</cell><cell>0.60283</cell><cell>0.79092</cell><cell>0.29007</cell><cell>0.75324</cell></row><row><cell>Shrestha</cell><cell>2013</cell><cell>0.99902</cell><cell>0.71461</cell><cell>0.63618</cell><cell>0.09897</cell><cell>0.73814</cell></row><row><cell>Suchomel</cell><cell>2012</cell><cell>0.99835</cell><cell>0.51946</cell><cell>0.50106</cell><cell>0.35305</cell><cell>0.64667</cell></row><row><cell>Abnar</cell><cell>2014</cell><cell>0.99110</cell><cell>0.35360</cell><cell>0.60498</cell><cell>0.12200</cell><cell>0.61163</cell></row><row><cell>Sánchez-Vega</cell><cell>2012</cell><cell>0.74452</cell><cell>0.43502</cell><cell>0.58133</cell><cell>0.22161</cell><cell>0.56225</cell></row><row><cell>Alvi</cell><cell>2014</cell><cell>0.98701</cell><cell>0.36603</cell><cell>0.41988</cell><cell>0.05685</cell><cell>0.55068</cell></row><row><cell>Palkovskii</cell><cell>2013</cell><cell>0.85048</cell><cell>0.36420</cell><cell>0.49667</cell><cell>0.08082</cell><cell>0.53561</cell></row><row><cell>Kueppers</cell><cell>2012</cell><cell>0.83854</cell><cell>0.36865</cell><cell>0.42427</cell><cell>0.09265</cell><cell>0.51074</cell></row><row><cell>Nourian</cell><cell>2013</cell><cell>0.87626</cell><cell>0.23609</cell><cell>0.28568</cell><cell>0.07622</cell><cell>0.43381</cell></row><row><cell>Jayapal</cell><cell>2013</cell><cell>0.86040</cell><cell>0.18182</cell><cell>0.19411</cell><cell>0.07236</cell><cell>0.38187</cell></row><row><cell>Baseline</cell><cell></cell><cell>0.99960</cell><cell>0.04181</cell><cell>0.08804</cell><cell>0.03649</cell><cell>0.34223</cell></row><row><cell>Gillam</cell><cell>2012</cell><cell>0.87187</cell><cell>0.02422</cell><cell>0.00616</cell><cell>0.00109</cell><cell>0.26994</cell></row><row><cell>Gillam</cell><cell>2013</cell><cell>0.83788</cell><cell>0.02142</cell><cell>0.00616</cell><cell>0.00109</cell><cell>0.25890</cell></row><row><cell>Jayapal</cell><cell>2012</cell><cell>0.51885</cell><cell>0.11148</cell><cell>0.09195</cell><cell>0.04574</cell><cell>0.22287</cell></row><row><cell>Gillam</cell><cell>2014</cell><cell>0.53187</cell><cell>0.02832</cell><cell>0.00202</cell><cell>0.00000</cell><cell>0.16840</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="25,134.77,181.86,346.06,410.04"><head>Table 7 .</head><label>7</label><figDesc>Cross-year evaluation of text alignment software submissions from 2012 to 2014 with respect to granularity. The darker a cell, the better the performance compared to the entire column.</figDesc><table coords="25,134.77,216.96,346.06,374.94"><row><cell cols="2">Software Submission</cell><cell cols="4">Obfuscation Strategies of the 2013 Evaluation Corpus</cell><cell>Entire Corpus</cell></row><row><cell>Team</cell><cell>Year</cell><cell>None</cell><cell>Random</cell><cell>Cyclic translation</cell><cell>Summary</cell><cell></cell></row><row><cell>Gillam</cell><cell>2012</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00000</cell></row><row><cell>Gillam</cell><cell>2013</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00000</cell></row><row><cell>Gillam</cell><cell>2014</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00000</cell></row><row><cell>Oberreuter</cell><cell>2012</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00000</cell></row><row><cell>Palkovskii</cell><cell>2012</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00000</cell></row><row><cell>R. Torrejón</cell><cell>2012</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00000</cell></row><row><cell>Suchomel</cell><cell>2013</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00476</cell><cell>1.00028</cell></row><row><cell>Suchomel</cell><cell>2012</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00610</cell><cell>1.00032</cell></row><row><cell>R. Torrejón</cell><cell>2013</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.03086</cell><cell>1.00141</cell></row><row><cell>R. Torrejón</cell><cell>2014</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.06024</cell><cell>1.00278</cell></row><row><cell>Kong</cell><cell>2012</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.06452</cell><cell>1.00282</cell></row><row><cell>Kong</cell><cell>2014</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.07190</cell><cell>1.00309</cell></row><row><cell>Kong</cell><cell>2013</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.07742</cell><cell>1.00336</cell></row><row><cell>Sanchez-Perez</cell><cell>2014</cell><cell>1.00000</cell><cell>1.00086</cell><cell>1.00081</cell><cell>1.05882</cell><cell>1.00344</cell></row><row><cell>Oberreuter</cell><cell>2014</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.00000</cell><cell>1.07568</cell><cell>1.00369</cell></row><row><cell>Palkovskii</cell><cell>2014</cell><cell>1.00000</cell><cell>1.00176</cell><cell>1.00088</cell><cell>1.10112</cell><cell>1.00580</cell></row><row><cell>Shrestha</cell><cell>2014</cell><cell>1.00000</cell><cell>1.00630</cell><cell>1.00948</cell><cell>1.06034</cell><cell>1.00701</cell></row><row><cell>Glinos</cell><cell>2014</cell><cell>1.00000</cell><cell>1.04037</cell><cell>1.00547</cell><cell>1.05128</cell><cell>1.01695</cell></row><row><cell>Sánchez-Vega</cell><cell>2012</cell><cell>1.00394</cell><cell>1.02200</cell><cell>1.03533</cell><cell>1.04523</cell><cell>1.02196</cell></row><row><cell>Abnar</cell><cell>2014</cell><cell>1.00332</cell><cell>1.01509</cell><cell>1.00462</cell><cell>1.39130</cell><cell>1.02245</cell></row><row><cell>Gross</cell><cell>2014</cell><cell>1.01998</cell><cell>1.03336</cell><cell>1.01466</cell><cell>1.08667</cell><cell>1.02514</cell></row><row><cell>Kueppers</cell><cell>2012</cell><cell>1.02687</cell><cell>1.01847</cell><cell>1.01794</cell><cell>1.31061</cell><cell>1.03497</cell></row><row><cell>Nourian</cell><cell>2013</cell><cell>1.00092</cell><cell>1.11558</cell><cell>1.00485</cell><cell>1.34234</cell><cell>1.04343</cell></row><row><cell>Alvi</cell><cell>2014</cell><cell>1.03731</cell><cell>1.07203</cell><cell>1.10207</cell><cell>1.26966</cell><cell>1.07111</cell></row><row><cell>Palkovskii</cell><cell>2013</cell><cell>1.00000</cell><cell>1.06785</cell><cell>1.02825</cell><cell>1.73596</cell><cell>1.07295</cell></row><row><cell>Shrestha</cell><cell>2013</cell><cell>1.00083</cell><cell>1.30962</cell><cell>1.26184</cell><cell>1.83696</cell><cell>1.22084</cell></row><row><cell>Saremi</cell><cell>2013</cell><cell>1.06007</cell><cell>1.29511</cell><cell>1.24204</cell><cell>2.15556</cell><cell>1.24450</cell></row><row><cell>Baseline</cell><cell></cell><cell>1.00912</cell><cell>1.18239</cell><cell>1.86726</cell><cell>1.97436</cell><cell>1.27473</cell></row><row><cell>Jayapal</cell><cell>2012</cell><cell>2.87916</cell><cell>2.15530</cell><cell>2.00578</cell><cell>2.75743</cell><cell>2.45403</cell></row><row><cell>Jayapal</cell><cell>2013</cell><cell>3.90017</cell><cell>2.19096</cell><cell>2.34218</cell><cell>3.60987</cell><cell>2.90698</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,635.17,335.85,7.77;1,144.73,646.13,335.85,7.77;1,144.73,657.09,162.73,7.77"><p>Some of the concepts found in this paper have been described earlier, so that, because of the inherently incremental nature of shared tasks, and in order for this paper to be self-contained, we reuse text from previous overview papers.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,144.73,627.98,335.86,7.94;2,144.73,639.10,335.85,7.77;2,144.73,650.06,206.95,7.77"><p>Another approach to detect plagiarism is called intrinsic plagiarism detection, where detectors are given only a suspicious document and are supposed to identify text passages in them which deviate in their style from the remainder of the document.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="5,144.73,633.09,122.38,7.77"><p>http://lemurproject.org/clueweb09</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="5,144.73,644.24,122.38,7.77"><p>http://lemurproject.org/clueweb12</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="5,144.73,655.39,195.22,7.77"><p>http://lemurproject.org/clueweb09/index.php#Services</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="29,144.73,657.09,42.25,7.77"><p>www.tira.io</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank the participating teams of this task for their devoted work. This paper was partially supported by the <rs type="programName">WIQ-EI IRSES project</rs> (Grant No. <rs type="grantNumber">269180</rs>) within the <rs type="funder">FP7 Marie Curie</rs> action.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_hS465q8">
					<idno type="grant-number">269180</idno>
					<orgName type="program" subtype="full">WIQ-EI IRSES project</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="27,134.77,221.24,341.55,8.53;27,134.77,234.29,38.83,7.77;27,192.68,234.29,283.64,8.53;27,134.77,247.34,37.50,7.77;27,192.68,247.34,283.64,8.53;27,134.77,260.39,23.91,7.77;27,192.68,260.39,283.64,8.53;27,134.77,273.44,19.61,7.77;27,192.68,273.44,283.64,8.53;27,134.77,286.49,30.88,7.77;27,192.68,286.49,283.64,8.53;27,134.77,299.55,20.92,7.77;27,192.68,299.55,283.64,8.53;27,134.77,312.60,38.83,7.77;27,192.68,312.60,283.64,8.53" xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Sanchez-Perez</surname></persName>
		</author>
		<idno>.00 0.90 0.91 0.90 0.92 0.91 0.91 Oberreuter 2014 0.87 0.89 0.86 1.00 0.84 0.89 0.87 0.89 0.89 0.89 Palkovskii 2014 0.87 0.92 0.83 1.01 0.90 0.85 0.87 0.90 0.84 0.87 Glinos 2014 0.86 0.96 0.79 1.02 0.90 0.83 0.87 0.93 0.88 0.91 Kong 2012 0.84 0.85 0.82 1.00 0.86 0.85 0.85 0.89 0.85 0.87 Shrestha 2014 0.84 0.86 0.84 1.01 0.91 0.85 0.88 0.94 0.85 0.89 Gross 2014 0.83 0.93 0.77 1.03 0.90 0.86 0.88 0.93 0.85 0.89 Oberreuter 2012 0.83 0.89 0.77 1.00 0.81 0.79 0.80 0.83 0.80 0.81</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">0</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,134.77,325.65,41.12,7.77;27,192.68,325.65,283.64,8.53" xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Torrejón</surname></persName>
		</author>
		<idno>.00 0.84 0.83 0.83 0.89 0.84 0.86</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">0</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,134.77,338.70,41.12,7.77;27,192.68,338.70,283.64,8.53;27,134.77,351.75,19.61,7.77;27,192.68,351.75,283.64,8.53;27,134.77,364.80,19.61,7.77;27,192.68,364.80,283.64,8.53;27,134.77,377.85,37.50,7.77;27,192.68,377.85,283.64,8.53" xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Torrejón</surname></persName>
		</author>
		<idno>.00 0.83 0.83 0.83 0.87 0.84 0.85 Kong 2013 0.82 0.83 0.81 1.00 0.85 0.86 0.85 0.89 0.86 0.87 Kong 2014 0.82 0.84 0.81 1.00 0.86 0.85 0.85 0.89 0.85 0.87 Palkovskii 2012 0.79 0.82 0.76 1.00 0.80 0.80 0.80 0.82 0.80 0.81</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">0</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,134.77,390.90,41.12,7.77;27,192.68,390.90,283.64,8.53;27,134.77,403.90,35.86,7.77;27,192.68,403.90,283.64,8.53;27,134.77,416.90,35.86,7.77;27,192.68,416.90,283.64,8.53;27,134.77,429.95,25.40,7.77;27,192.68,429.95,283.64,8.53;27,134.77,442.94,30.88,7.77;27,192.68,442.94,283.64,8.53;27,134.77,455.94,22.41,7.77;27,192.68,455.94,283.64,8.53;27,134.77,468.99,15.94,7.77;27,192.68,468.99,283.64,8.53;27,134.77,482.04,34.22,7.77;27,192.68,482.04,283.64,8.53;27,134.77,495.09,37.50,7.77;27,192.68,495.09,283.64,8.53;27,134.77,508.14,29.38,7.77;27,192.68,508.14,283.64,8.53;27,134.77,521.14,341.55,8.53;27,134.77,538.62,30.88,7.77;27,222.99,539.32,253.33,7.83;27,134.77,551.67,24.91,7.77;27,192.68,551.67,283.64,8.53;27,134.77,564.72,24.91,7.77;27,192.68,564.72,283.64,8.53;27,134.77,577.77,24.91,7.77;27,192.68,577.77,283.64,8.53;27,134.77,590.83,26.89,7.77;27,192.68,590.83,283.64,8.53;27,134.77,603.88,26.89,7.77;27,192.68,603.88,283.64,8.53;29,134.77,432.26,66.92,10.75" xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Torrejón</surname></persName>
		</author>
		<idno>.00 0.65 0.79 0.72 0.65 0.78 0.71 Suchomel 2013 0.74 0.73 0.77 1.00 0.66 0.83 0.73 0.67 0.82 0.74 Suchomel 2012 0.73 0.84 0.65 1.00 0.76 0.70 0.73 0.77 0.69 0.73 Saremi 2013 0.70 0.87 0.77 1.24 0.59 0.80 0.68 0.82 0.82 0.82 Shrestha 2013 0.70 0.87 0.74 1.22 0.57 0.76 0.65 0.77 0.78 0.77 Abnar 2014 0.67 0.77 0.61 1.02 0.76 0.63 0.69 0.88 0.65 0.75 Alvi 2014 0.66 0.93 0.55 1.07 0.77 0.59 0.67 0.88 0.63 0.73 Kueppers 2012 0.63 0.87 0.51 1.03 0.76 0.59 0.67 0.83 0.64 0.72 Palkovskii 2013 0.62 0.82 0.54 1.07 0.62 0.56 0.59 0.76 0.59 0.66 Nourian 2013 0.58 0.95 0.43 1.04 0.84 0.44 0.58 0.88 0.45 0.59 Sánchez-Vega 2012 0.46 0.40 0.56 1.02 0.35 0.63 0.45 0.64 0.67 0.66 Baseline 0.42 0.93 0.34 1.27 0.42 0.31 0.36 0.55 0.32 0.41 Gillam 2012 0.41 0.89 0.27 1.00 0.92 0.28 0.43 0.93 0.30 0.46 Gillam 2013 0.40 0.88 0.26 1.00 0.92 0.27 0.42 0.93 0.29 0.44 Gillam 2014 0.28 0.89 0.17 1.00 0.91 0.18 0.31 0.92 0.19 0.31 Jayapal 2013 0.27 0.88 0.38 2.91 0.04</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="page" from="79" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="29,142.61,455.56,330.01,7.77;29,150.95,466.52,192.42,7.77" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="29,334.94,455.56,137.68,7.77;29,150.95,466.52,147.61,7.77">Expanded N-Grams for Semantic Text Alignment-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Shakery</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="29,142.61,476.97,308.68,7.77;29,150.95,487.93,188.93,7.77" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="29,279.90,476.97,171.39,7.77;29,150.95,487.93,144.11,7.77">Hashing and Merging Heuristics for Text Reuse Detection-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alvi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Stevenson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="29,142.61,498.39,326.00,7.77;29,150.95,509.34,329.22,7.77;29,150.95,520.30,319.74,7.77;29,150.95,531.26,328.03,7.77;29,150.95,542.22,83.92,7.77" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="29,251.34,498.39,213.30,7.77">Using Noun Phrase Heads to Extract Document Keyphrases</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Cornacchia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="29,237.12,509.34,243.06,7.77;29,150.95,520.30,246.49,7.77">Advances in Artificial Intelligence, 13th Biennial Conference of the Canadian Society for Computational Studies of Intelligence, AI 2000</title>
		<title level="s" coord="29,248.82,531.26,174.78,7.77">Proceedings. Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Hamilton</surname></persName>
		</editor>
		<meeting><address><addrLine>Montréal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000">May 14-17, 2000. 2000</date>
			<biblScope unit="volume">1822</biblScope>
			<biblScope unit="page" from="40" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="29,142.61,552.67,326.93,7.77;29,150.95,563.63,312.90,7.77;29,150.95,574.59,131.07,7.77;29,150.95,585.55,198.20,7.77" xml:id="b7">
	<analytic>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
	</analytic>
	<monogr>
		<title level="m" coord="29,351.72,552.67,117.82,7.77;29,150.95,563.63,124.45,7.77">CLEF 2014 Evaluation Labs and Workshop -Working Notes Papers</title>
		<title level="s" coord="29,401.06,563.63,62.79,7.77;29,150.95,574.59,68.08,7.77">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Halvey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</editor>
		<meeting><address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-09">September. 2014</date>
			<biblScope unit="page" from="15" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="29,142.61,596.00,337.43,7.77;29,150.95,606.96,107.50,7.77" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="29,197.11,596.00,282.93,7.77;29,150.95,606.96,62.68,7.77">Using Noun Phrases and tf-idf for Plagiarized Document Retrieval-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Elizalde</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="29,142.61,617.41,312.64,7.77;29,150.95,628.37,234.60,7.77;29,150.95,639.33,198.20,7.77" xml:id="b9">
	<monogr>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
		<title level="m" coord="29,291.83,617.41,163.42,7.77;29,150.95,628.37,78.85,7.77">CLEF 2013 Evaluation Labs and Workshop -Working Notes Papers</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Navigli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Tufis</surname></persName>
		</editor>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-09">September. 2013</date>
			<biblScope unit="page" from="23" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,142.61,119.96,333.58,7.77;30,150.95,130.92,226.04,7.77" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="30,232.67,119.96,243.52,7.77;30,150.95,130.92,184.71,7.77">Evaluating Robustness for &apos;IPCRESS&apos;: Surrey&apos;s Text Alignment for Plagiarism Detection-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gillam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Notley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,142.61,141.16,331.00,7.77;30,150.95,152.12,46.06,7.77" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="30,192.79,141.16,280.81,7.77">A Hybrid Architecture for Plagiarism Detection-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Glinos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,142.61,162.35,331.25,7.77;30,150.95,173.31,317.97,7.77;30,150.95,184.27,329.63,7.77;30,150.95,195.23,307.08,7.77;30,150.95,206.19,272.69,7.77" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="30,163.91,173.31,207.77,7.77">Recent Trends in Digital Text Forensics and its Evaluation</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Busse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="30,288.21,184.27,192.37,7.77;30,150.95,195.23,307.08,7.77;30,150.95,206.19,37.79,7.77">Information Access Evaluation meets Multilinguality, Multimodality, and Visualization. 4th International Conference of the CLEF Initiative (CLEF 13)</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Paredes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Rosso</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Stein</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013-09">Sep 2013</date>
			<biblScope unit="page" from="282" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,142.24,216.43,301.79,7.77;30,150.95,227.39,319.86,7.77;30,150.95,238.35,327.47,7.77;30,150.95,249.30,246.98,7.77" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="30,274.60,216.43,169.43,7.77;30,150.95,227.39,182.42,7.77">Ousting Ivory Tower Research: Towards a Web Framework for Providing Experiments as a Service</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Burrows</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="30,245.83,238.35,232.60,7.77;30,150.95,249.30,119.02,7.77">International ACM Conference on Research and Development in Information Retrieval (SIGIR 12)</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Hersh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Callan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Maarek</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012-08">Aug 2012</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1125" to="1126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,142.24,259.54,301.08,7.77;30,150.95,270.50,310.61,7.77;30,150.95,281.46,329.01,7.77;30,150.95,292.42,256.75,7.77" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="30,316.18,259.54,127.13,7.77;30,150.95,270.50,177.38,7.77">TIRA: Configuring, Executing, and Disseminating Information Retrieval Experiments</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Burrows</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hoppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="30,229.64,281.46,250.32,7.77;30,150.95,292.42,44.62,7.77">9th International Workshop on Text-based Information Retrieval (TIR 12) at DEXA</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Tjoa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Liddle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Schewe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</editor>
		<meeting><address><addrLine>Los Alamitos, California</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012-09">Sep 2012</date>
			<biblScope unit="page" from="151" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,142.24,302.66,284.86,7.77;30,150.95,313.61,178.47,7.77" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="30,236.38,302.66,190.72,7.77;30,150.95,313.61,130.17,7.77">Plagiarism Alignment Detection by Merging Context Seeds-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Modaresi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,142.24,323.85,330.21,7.77;30,150.95,334.81,324.43,7.77;30,150.95,345.77,316.37,7.77;30,150.95,356.73,199.97,7.77" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="30,229.40,323.85,239.17,7.77">Applying the User-over-Ranking Hypothesis to Query Formulation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="30,163.16,334.81,312.23,7.77;30,150.95,345.77,128.73,7.77">Advances in Information Retrieval Theory. 3rd International Conference on the Theory of Information Retrieval (ICTIR 11)</title>
		<title level="s" coord="30,285.47,345.77,126.47,7.77">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">6931</biblScope>
			<biblScope unit="page" from="225" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,142.24,366.97,332.74,7.77;30,150.95,377.93,322.32,7.77;30,150.95,388.88,293.82,7.77;30,150.95,399.84,104.44,7.77" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="30,229.40,366.97,241.87,7.77">Candidate Document Retrieval for Web-Scale Text Reuse Detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="30,163.16,377.93,310.12,7.77;30,150.95,388.88,10.64,7.77">18th International Symposium on String Processing and Information Retrieval (SPIRE 11)</title>
		<title level="s" coord="30,167.39,388.88,126.47,7.77">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">7024</biblScope>
			<biblScope unit="page" from="356" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,142.24,410.08,303.62,7.77;30,150.95,421.04,326.90,7.77" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="30,251.18,410.08,194.68,7.77;30,150.95,421.04,278.60,7.77">Plagiarism Candidate Retrieval Using Selective Query Formulation and Discriminative Query Scoring-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Haggag</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>El-Beltagy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,142.24,431.28,334.86,7.77;30,150.95,442.24,291.13,7.77;30,150.95,453.20,233.01,7.77" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="30,382.17,431.28,94.92,7.77;30,150.95,442.24,291.13,7.77;30,150.95,453.20,184.71,7.77">Source Retrieval Based on Learning to Rank and Text Alignment Based on Plagiarism Type Recognition for Plagiarism Detection-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,142.24,463.43,321.97,7.77;30,150.95,474.39,283.32,7.77" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="30,313.78,463.43,150.42,7.77;30,150.95,474.39,235.02,7.77">Approaches for Source Retrieval and Text Alignment of Plagiarism Detection-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,142.24,484.63,338.34,7.77;30,150.95,495.59,322.96,7.77;30,150.95,506.55,318.66,7.77;30,150.95,517.51,297.64,7.77;30,150.95,528.46,198.20,7.77" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="30,347.00,484.63,133.58,7.77;30,150.95,495.59,322.96,7.77;30,150.95,506.55,16.14,7.77">Approaches for Candidate Document Retrieval and Detailed Comparison of Plagiarism Detection-Notebook for PAN at CLEF 2012</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
	</analytic>
	<monogr>
		<title level="m" coord="30,366.98,506.55,102.63,7.77;30,150.95,517.51,139.64,7.77">CLEF 2012 Evaluation Labs and Workshop -Working Notes Papers</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Karlgren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Womser-Hacker</surname></persName>
		</editor>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-09">17-20 September. Sep 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,142.24,538.70,328.13,7.77;30,150.95,549.66,227.64,7.77;30,150.95,560.62,290.99,7.77" xml:id="b22">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Oberreuter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Eiselt</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
		<title level="m" coord="30,245.46,538.70,224.91,7.77;30,150.95,549.66,33.39,7.77">Submission to the 6th International Competition on Plagiarism Detection</title>
		<meeting><address><addrLine>Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,142.24,570.86,316.60,7.77;30,150.95,581.82,228.84,7.77" xml:id="b23">
	<monogr>
		<title level="m" type="main" coord="30,243.61,570.86,215.22,7.77;30,150.95,581.82,180.54,7.77">Developing High-Resolution Universal Multi-Type N-Gram Plagiarism Detector-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Palkovskii</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Belov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="30,142.24,592.05,269.33,7.77;30,150.95,603.01,148.82,7.77;30,150.95,613.97,282.15,7.77" xml:id="b24">
	<monogr>
		<title level="m" type="main" coord="30,199.28,592.05,158.87,7.77">Technologies for Reusing Text from the Web</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<ptr target="http://nbn-resolving.de/urn/resolver.pl?urn:nbn:de:gbv:wim2-20120217-15663" />
		<imprint>
			<date type="published" when="2011-12">Dec 2011</date>
		</imprint>
		<respStmt>
			<orgName>Bauhaus-Universität Weimar</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Dissertation</note>
</biblStruct>

<biblStruct coords="30,142.24,624.21,314.43,7.77;30,150.95,635.17,323.87,7.77;30,150.95,646.13,284.55,7.77;30,150.95,657.09,198.20,7.77" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="30,383.08,624.21,73.59,7.77;30,150.95,635.17,180.33,7.77">Overview of the 2nd International Competition on Plagiarism Detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Eiselt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
	</analytic>
	<monogr>
		<title level="m" coord="30,183.32,646.13,208.11,7.77">Working Notes Papers of the CLEF 2010 Evaluation Labs</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Harman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Pianta</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2010-09">Sep 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,142.24,119.96,312.94,7.77;31,150.95,130.92,313.64,7.77;31,150.95,141.88,274.59,7.77;31,150.95,152.84,198.20,7.77" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="31,383.08,119.96,72.10,7.77;31,150.95,130.92,180.33,7.77">Overview of the 3rd International Competition on Plagiarism Detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Eiselt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
	</analytic>
	<monogr>
		<title level="m" coord="31,173.36,141.88,208.11,7.77">Working Notes Papers of the CLEF 2011 Evaluation Labs</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Petras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2011-09">Sep 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,142.24,163.80,329.93,7.77;31,150.95,174.76,310.38,7.77;31,150.95,185.72,282.40,7.77;31,150.95,196.67,322.04,7.77;31,150.95,207.63,223.60,7.77" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="31,389.74,174.76,71.60,7.77;31,150.95,185.72,180.33,7.77">Overview of the 4th International Competition on Plagiarism Detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Graßegger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Oberländer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tippmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
	</analytic>
	<monogr>
		<title level="m" coord="31,246.21,196.67,208.11,7.77">Working Notes Papers of the CLEF 2012 Evaluation Labs</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Karlgren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Womser-Hacker</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2012-09">Sep 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,142.24,218.59,324.54,7.77;31,150.95,229.55,306.95,7.77;31,150.95,240.51,326.83,7.77;31,150.95,251.47,261.95,7.77" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="31,186.83,229.55,254.18,7.77">Overview of the 5th International Competition on Plagiarism Detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tippmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
	</analytic>
	<monogr>
		<title level="m" coord="31,289.34,240.51,188.44,7.77;31,150.95,251.47,17.43,7.77">Working Notes Papers of the CLEF 2013 Evaluation Labs</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Navigli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Tufis</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2013-09">Sep 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,142.24,262.43,328.70,7.77;31,150.95,273.39,321.25,7.77;31,150.95,284.35,329.63,7.77;31,150.95,295.30,229.30,7.77" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="31,150.95,273.39,195.47,7.77">ChatNoir: A Search Engine for the ClueWeb09 Corpus</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Graßegger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tippmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Welsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="31,257.57,284.35,223.01,7.77;31,150.95,295.30,128.23,7.77">International ACM Conference on Research and Development in Information Retrieval (SIGIR 12)</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Hersh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Callan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Maarek</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012-08">Aug 2012</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">1004</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,142.24,306.26,305.26,7.77;31,150.95,317.22,316.02,7.77;31,150.95,328.18,306.22,7.77;31,150.95,339.14,267.91,7.77" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="31,321.97,306.26,125.53,7.77;31,150.95,317.22,132.35,7.77">Crowdsourcing Interaction Logs to Understand Text Reuse from the Web</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Völske</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P13-1119" />
	</analytic>
	<monogr>
		<title level="m" coord="31,400.24,317.22,66.73,7.77;31,150.95,328.18,289.22,7.77">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 13)</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Fung</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Poesio</surname></persName>
		</editor>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics (ACL 13)</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2013-08">Aug 2013</date>
			<biblScope unit="page" from="1212" to="1221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,142.24,350.10,321.68,7.77;31,150.95,361.06,318.51,7.77;31,150.95,372.02,287.93,7.77;31,150.95,382.98,210.27,7.77;31,150.95,393.93,252.84,7.77" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="31,321.97,350.10,141.95,7.77;31,150.95,361.06,22.23,7.77">Exploratory Search Missions for TREC Topics</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Völske</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="http://www.cs.nott.ac.uk/mlw/euroHCIR2013/proceedings/paper3.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="31,457.51,361.06,11.95,7.77;31,150.95,372.02,287.93,7.77">3rd European Workshop on Human-Computer Interaction and Information Retrieval</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Wilson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Russell-Rose</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Larsen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Hansen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Norling</surname></persName>
		</editor>
		<meeting><address><addrLine>EuroHCIR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08">2013. Aug 2013</date>
			<biblScope unit="page" from="11" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,142.24,404.89,310.52,7.77;31,150.95,415.85,329.21,7.77;31,150.95,426.81,317.54,7.77;31,150.95,437.77,183.73,7.77" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="31,344.97,404.89,107.78,7.77;31,150.95,415.85,73.99,7.77">An Evaluation Framework for Plagiarism Detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="31,379.58,415.85,100.59,7.77;31,150.95,426.81,147.38,7.77">International Conference on Computational Linguistics (COLING 10)</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</editor>
		<meeting><address><addrLine>Stroudsburg, Pennsylvania</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010-08">Aug 2010</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="997" to="1005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,142.24,448.73,311.45,7.77;31,150.95,459.69,326.01,7.77;31,150.95,470.65,326.17,7.77;31,150.95,481.61,271.11,7.77;31,150.95,492.56,94.88,7.77" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="31,383.08,448.73,70.61,7.77;31,150.95,459.69,180.33,7.77">Overview of the 1st International Competition on Plagiarism Detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Eiselt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-502" />
	</analytic>
	<monogr>
		<title level="m" coord="31,256.47,470.65,220.66,7.77;31,150.95,481.61,135.49,7.77">SEPLN 09 Workshop on Uncovering Plagiarism, Authorship, and Social Software Misuse (PAN 09)</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Stein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Rosso</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Koppel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Agirre</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2009-09">Sep 2009</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,142.24,503.52,314.39,7.77;31,150.95,514.48,257.71,7.77" xml:id="b34">
	<monogr>
		<title level="m" type="main" coord="31,230.89,503.52,225.74,7.77;31,150.95,514.48,209.41,7.77">Experiments on Document Chunking and Query Formation for Plagiarism Source Retrieval-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Saha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,142.24,525.44,332.41,7.77;31,150.95,536.40,282.72,7.77" xml:id="b35">
	<monogr>
		<title level="m" type="main" coord="31,316.89,525.44,157.76,7.77;31,150.95,536.40,234.42,7.77">A Winning Approach to Text Alignment for Text Reuse Detection at PAN 2014-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanchez-Perez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sidorov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,142.24,547.36,331.37,7.77;31,150.95,558.32,227.64,7.77;31,150.95,569.28,311.32,7.77" xml:id="b36">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Saremi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Yaghmaee</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
		<title level="m" coord="31,248.70,547.36,224.91,7.77;31,150.95,558.32,33.39,7.77">Submission to the 5th International Competition on Plagiarism Detection</title>
		<meeting><address><addrLine>Iran</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
		<respStmt>
			<orgName>From Semnan University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="31,142.24,580.24,319.78,7.77;31,150.95,591.19,195.91,7.77" xml:id="b37">
	<monogr>
		<title level="m" type="main" coord="31,290.25,580.24,171.76,7.77;31,150.95,591.19,147.61,7.77">Machine Translation Evaluation Metric for Text Alignment-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Maharjan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Solorio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,142.24,602.15,317.63,7.77;31,150.95,613.11,326.98,7.77;31,150.95,624.07,324.31,7.77;31,150.95,635.03,20.92,7.77" xml:id="b38">
	<analytic>
		<title level="a" type="main" coord="31,229.40,602.15,166.82,7.77">Introducing the User-over-Ranking Hypothesis</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="31,415.53,602.15,44.35,7.77;31,150.95,613.11,270.65,7.77">Advances in Information Retrieval. 33rd European Conference on IR Resarch (ECIR 11)</title>
		<title level="s" coord="31,427.39,613.11,50.54,7.77;31,150.95,624.07,73.68,7.77">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011-04">Apr 2011</date>
			<biblScope unit="volume">6611</biblScope>
			<biblScope unit="page" from="503" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="31,142.24,645.99,296.36,7.77;31,150.95,656.95,300.52,7.77;32,150.95,119.96,312.33,7.77;32,150.95,130.92,194.17,7.77" xml:id="b39">
	<analytic>
		<title level="a" type="main" coord="31,308.22,645.99,130.37,7.77;31,150.95,656.95,38.77,7.77">Strategies for Retrieving Plagiarized Documents</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Meyer Zu Eißen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="31,435.53,656.95,15.94,7.77;32,150.95,119.96,312.33,7.77;32,150.95,130.92,39.28,7.77">30th International ACM Conference on Research and Development in Information Retrieval (SIGIR 07)</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Clarke</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Fuhr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Kando</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>De Vries</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007-07">Jul 2007</date>
			<biblScope unit="page" from="825" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="32,142.24,141.88,305.06,7.77;32,150.95,152.84,181.95,7.77" xml:id="b40">
	<monogr>
		<title level="m" type="main" coord="32,272.74,141.88,174.55,7.77;32,150.95,152.84,133.65,7.77">Heterogeneous Queries for Synoptic and Phrasal Search-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Šimon</forename><surname>Suchomel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brandejs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="32,142.24,163.80,336.59,7.77;32,150.95,174.76,247.43,7.77" xml:id="b41">
	<monogr>
		<title level="m" type="main" coord="32,321.29,163.80,157.53,7.77;32,150.95,174.76,199.13,7.77">Diverse Queries and Feature Type Selection for Plagiarism Discovery-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Šimon</forename><surname>Suchomel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kasprzak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brandejs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="32,142.24,185.72,312.81,7.77;32,150.95,196.67,185.44,7.77" xml:id="b42">
	<monogr>
		<title level="m" type="main" coord="32,276.50,185.72,178.55,7.77;32,150.95,196.67,137.14,7.77">CoReMo 2.3 Plagiarism Detector Text Alignment Module-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A R</forename><surname>Torrejón</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M M</forename><surname>Ramos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="32,142.24,207.63,333.55,7.77;32,150.95,218.59,217.11,7.77" xml:id="b43">
	<monogr>
		<title level="m" type="main" coord="32,340.56,207.63,135.23,7.77;32,150.95,218.59,168.81,7.77">Unsupervised Ranking for Plagiarism Source Retrieval-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Giles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="32,142.24,229.55,293.51,7.77;32,150.95,240.51,189.97,7.77" xml:id="b44">
	<monogr>
		<title level="m" type="main" coord="32,282.84,229.55,152.91,7.77;32,150.95,240.51,141.67,7.77">Supervised Ranking for Plagiarism Source Retrieval-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Giles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="32,142.24,251.47,316.63,7.77;32,150.95,262.43,186.49,7.77" xml:id="b45">
	<monogr>
		<title level="m" type="main" coord="32,251.18,251.47,207.69,7.77;32,150.95,262.43,141.67,7.77">Using Sentence Similarity Measure for Plagiarism Source Retrieval-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zubarev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sochenkov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
