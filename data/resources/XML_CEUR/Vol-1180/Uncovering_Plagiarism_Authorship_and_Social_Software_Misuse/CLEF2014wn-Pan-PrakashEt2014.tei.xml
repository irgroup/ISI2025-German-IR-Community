<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,155.78,148.76,298.50,15.96;1,154.58,166.16,286.11,15.96;1,264.89,183.56,65.87,15.96;1,211.37,204.04,172.60,12.00">Experiments on Document Chunking and Query Formation for Plagiarism Source Retrieval Notebook for PAN at CLEF 2014</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,237.17,242.12,52.78,9.05"><forename type="first">Amit</forename><surname>Prakash</surname></persName>
							<email>aprakash@bitmesra.ac.in</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Birla Institute of Technology</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,296.75,242.12,72.63,9.05"><forename type="first">Sujan</forename><surname>Kumar Saha</surname></persName>
							<email>sujan.kr.saha@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Birla Institute of Technology</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,155.78,148.76,298.50,15.96;1,154.58,166.16,286.11,15.96;1,264.89,183.56,65.87,15.96;1,211.37,204.04,172.60,12.00">Experiments on Document Chunking and Query Formation for Plagiarism Source Retrieval Notebook for PAN at CLEF 2014</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C664719CB275CE1859A6AB5B90718DAF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the details of the system we prepare as a participant of the PAN 2014 task on 'Source Retrieval: Uncovering Plagiarism, Authorship, and Social Software Misuse'. Our work is focused on intelligent chunking of suspicious documents and a hybrid approach of query formation. A method based on term frequency and word co-occurrence is proposed to extract query terms from a non-overlapping chunk of topically related sentences. The queries are then submitted to the ChatNoir search API to retrieve documents that are likely to be the sources of plagiarism. Finally a snippet matching and duplicate download restriction based filtering technique reduces the number of downloads. The evaluation results of the PAN14 Source Retrieval task show that the performance of our system is highly promising. The f-measure accuracy of the system is .3871 with a recall of .5083 which is the highest among all the participants.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The World Wide Web has become the most popular source of information. Exponential increase in the amount of information available on the web and improved access to this via the Internet has tremendous potential and a lot to offer in terms of services. Internet now is a virtual treasure trove of information about every subject known to man. However one of the major disadvantages of this ease of access of vast amount of information lead to a serious problem called plagiarism <ref type="bibr" coords="1,391.99,566.53,10.69,9.05" target="#b0">[1]</ref>.</p><p>Plagiarism is defined as the act of using the ideas or work of another person or persons as if they were one's own, without giving credit to the source <ref type="bibr" coords="1,417.75,589.57,10.72,9.05" target="#b1">[2]</ref>. Here the word "work" can be defined as variety of things which include ideas, words, opinion, etc. Anything that is seen as an unethical and unattributed use of another's original creation can be defined as plagiarism <ref type="bibr" coords="1,289.97,624.04,10.69,9.05" target="#b2">[3]</ref>. However this definition is not always consistent, different industries follow their own standard to define plagiarism. Our work is concerned with the cases of natural language text plagiarism whose potential source is World Wide Web.</p><p>Reports suggest that the Internet has led to a dramatic increase in plagiarism over the past decade due to the easy availability of resources on the internet that allow plagiarists to find materials from which to copy and turn in as their own. Plagiarism is a serious problem in all levels of academia. Numerous studies show that the trend to copy existing information from Internet is increasing day-by-day among students <ref type="bibr" coords="2,459.07,172.52,11.75,9.05;2,124.82,184.04,10.69,9.05">[2] [4]</ref>. The survey of Pew Internet &amp; American Life Project (2011) <ref type="bibr" coords="2,389.35,184.04,11.69,9.05" target="#b4">[5]</ref> reported that, 55 percent of college presidents accepted that there was a noticeable increase in the numbers of plagiarized works in their colleges. Of that 55 percent, 89 percent believe that computers and the Internet have played a major role in this trend.</p><p>Due to absence of controlled evaluation environment to compare results of the algorithms, plagiarism detection is still a challenging task. So far various conferences and shared tasks have been organized to deal with plagiarism problem. PAN <ref type="bibr" coords="2,432.91,253.04,11.69,9.05" target="#b6">[7]</ref> is one of them, which has been organizing an international competition on plagiarism detection since 2009. It provides a real world scenario and standardized evaluation framework for researchers to develop and evaluate their systems. We participated in source retrieval sub-task of PAN 2014 competition where the goal is to retrieve documents (candidate documents) which serve as possible source of plagiarism for a given plagiarized document (suspicious document) from a web like scenario. For evaluation of such systems five evaluation measures have been considered by the PAN organizers: 1) number of queries submitted, 2) number of web pages downloaded, 3) precision and recall of web pages downloaded regarding the actual sources, 4) number of queries until the first actual source is found, v) number of downloads until the first actual source is downloaded.</p><p>The system we develop is consists of four core modules namely, chunking, query generation, downloading and filtering. During the design of the individual modules we mainly focused on maintaining a high recall of the system. Additionally, we targeted to keep the number of queries and number of downloads as low as possible so that the system achieves moderate performance with respect to all the evaluation metrics. The detail of the system is discussed in this paper.</p><p>Our approach is mainly focused on intelligent chunking of documents and a hybrid approach of keyword extraction from them, using two well known term extraction strategies: term frequency and word co-occurrence. First, we split the suspicious documents in variable length chunks. From these chunks a subgroup of topically related sentences formed based on co-occurrences of top frequent words. We have extracted nouns from these subgroups to form queries of maximum 10 words. We optionally submit four queries per chunk to ChatNoir <ref type="bibr" coords="2,342.07,529.09,11.72,9.05" target="#b7">[8]</ref> search engine and download maximum 10 documents per query. To further reduce the retrieved documents set we have applied a download filter based on 5-gram similarity check with 500 character snippet. Evaluation using TIRA <ref type="bibr" coords="2,262.13,563.53,11.72,9.05" target="#b6">[7]</ref> experimentation platform shows that using an average work load our system retrieves more than 50% of plagiarism sources with an accuracy of 38.24%. The following sections give the detail of methods used in the development of our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The research on plagiarism detection started with the detection of plagiarism in large piece of software codes <ref type="bibr" coords="2,235.12,674.80,10.66,9.05" target="#b0">[1]</ref>. As with the improvement of plagiarism cases in academics the researcher's interest shifted towards the plagiarism involving natural language texts. The research on natural language text plagiarism detection began in mid-1990 and has made a significant progress till date. The early researches were carried out on relatively small corpora consist of hundreds to few thousands of documents. However, now researchers consider the whole web as a possible source of plagiarism and generally use a search engine to retrieve the sources of plagiarized text. This leads to the development of online plagiarism detection services like plagiarism.org [9], turnitin.com [10] etc.</p><p>Compare to program code, detection of plagiarism in natural language text is a more challenging task due to the absence of formal syntax and ambiguity at various levels <ref type="bibr" coords="3,150.74,264.59,10.69,9.05" target="#b0">[1]</ref>. Natural language text can be plagiarized in number of ways. Beside simple copy and paste one can rearrange words, obfuscate or paraphrase the reused sentences. A lot of work has been done on simple copy paste detection, but still other problems have not received much attention.</p><p>The task of plagiarism detection has been divided into two main categories external plagiarism detection and intrinsic plagiarism detection <ref type="bibr" coords="3,359.11,322.07,10.69,9.05" target="#b5">[6]</ref>. In external plagiarism detection the contents of suspicious document is checked against a collection of external documents that have been used as source of plagiarism. On the other hand, in intrinsic plagiarism detection the plagiarized text is identified by investigating the changes in writing style within the same document. Since 2012, PAN separated the external and intrinsic plagiarism tasks. Intrinsic plagiarism detection migrated under author attribution task and external plagiarism detection task further divided into two subtasks source retrieval and detailed comparison.</p><p>A brief discussion of approaches used for source retrieval task can be found in the overview papers of previous PAN tracks <ref type="bibr" coords="3,309.99,425.51,10.66,9.05" target="#b5">[6]</ref>. Most approaches starts with the separation of large document text into smaller chunks. After that a keyword extraction method is applied on chunks to extract terms in order to formulate queries. Queries are formed in various ways so that it can retrieve the similar documents with maximum probability. This followed by a search controller which dynamically adjust the search based on the results of previously submitted queries. The final step of this process is download filtering. A download filter further reduces the document set returned by search engine by removing all the documents that are not worthwhile being compared in detail with suspicious document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Our approach involves four main steps: 1) Document Chunking, 2) Term Extraction, 3) Query Formation and Search Control, 4) Document Downloading and Filtering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Document Chunking</head><p>A close analysis of suspicious documents show the text is categorized among various titles. Our document chunking strategy is based on the idea that the paragraphs under same title are topically related. We have considered text separated by two newline characters as a paragraph and a paragraph of length less than nine words as title. In order to form a chunk, first we partition the document text into paragraphs. In next step we merge these paragraphs to form non-overlapping chunks of variable lengths. We have used following two strategies for conditional merging of paragraphs. 1) Starting from the first paragraph, whenever we encounter a title or a paragraph of more than 100 words we form a new chunk. However, we avoid this when such paragraphs just proceeds after a title. 2) We merge the proceeding paragraphs in existing chunk till we don't encounter a paragraph necessary for creating a new chunk discussed above. While merging paragraphs we continuously check for the size of chunk. In case the size exceeds 200 words we stop merging paragraphs in existing chunk and create a new chunk form next paragraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Keyword Extraction</head><p>Our keyword extraction approach is based on two well known term extraction strategies term frequency and word co-occurrence. This section describes these approaches in detail.</p><p>The term frequency reflects the importance of each word of the document by counting their number of occurrences. The top frequent words (after removing the stop-words) can be used to define the center of attraction in a particular piece of text. These are the words around which the whole text is written. Based on this hypothesis we have extracted top 5 frequent words of a document and named it document level tf and the most frequent word of each chunk and referred it as chunk level tf. Before extracting frequent words we preprocess the documents by removing the stop-words and the words of length less than three characters.</p><p>We have used co-occurrence to extract sentences from chunk in order to form subgroups. For each chunk we form two subgroups based on the co-occurrence of frequent words extracted earlier. The first subgroup has been formed using the word co-occurrences of document level tf only. Whenever two or more words of document level tf co-occur in a sentence we include that sentence in subgroup. In case the subgroup contains less than 5 sentences we include the sentences that contain any word of document level tf.</p><p>We form second subgroup based on the co-occurrences of chunk level tf word with document level tf words. Whenever the most frequent word of chunk co-occurs with any document level tf words in a sentence we include that sentence in subgroup. In case the subgroup contains less than 5 sentences we include the sentences containing the word of chunk level tf only.</p><p>We POS tag these subgroups using Maximum Entropy Part-of-Speech Tagger <ref type="bibr" coords="4,453.91,605.89,16.79,9.05" target="#b8">[11]</ref> and extract all the nouns as keywords. The reason behind taking only the nouns is to minimize the number of keywords and the hypothesis that nouns are sufficient enough define a piece of text uniquely in most of the cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Query Formation and search control</head><p>Forming query for ChatNoir search engine is a challenging task due to the fact that ChatNoir allows maximum 10 words per query to retrieve the sources. We form maximum four queries from each chunk and conditionally submit them to ChatNoir in order to minimize the workload. Before submitting each query we ensure that the 60% of current query terms differs from any previously submitted query otherwise we drop the current query.</p><p>If the subgroups return at least one noun we form a query from them. We have formed first two queries using this strategy taking the first 10 nouns extracted from subgroups. In case these queries contain less than 6 words, we append the nouns returned by tagging the chunk itself to make the query of 10 unique words. We form the third query from the nouns returned by tagging the chunk itself and submit it only if first query couldn't be formed or returns no result. The fourth query constitute the top 10 frequent words of a chunk and we submit it only if second query couldn't be constructed or dropped.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Document Downloading and Filtering</head><p>`Number of downloads' is considered as one of the metrics for evaluation of the system. Therefore we aim to keep the 'number of downloads' as low as possible. To achieve this we have adopted a two-stage approach. In the first stage we use a snippet based pre-checking of the retrieved documents. Initially we have retrieved 10 candidate documents for each query. For each of these documents we generate a 500 character snippet using ChatNoir's snippet generator facility. Then we check whether the snippet is containing any 5-gram from the suspicious document. If not, then we reject the document. Otherwise we log and download the corresponding document using ChatNoir API for detailed comparison. As a second stage, we restrict the system from duplicate download. We observe that many of the queries share common terms; that may lead to same download from two different queries. Once a document is downloaded by one query, it is not anticipated to be downloaded again by another query. To restrict this we maintain a list of downloaded documents which is checked before downloading the documents. A document is downloaded only if the corresponding entry is not there in the list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluations and Performance</head><p>We implemented our approach in Java programming language with the help of OpenNLP <ref type="bibr" coords="5,169.10,629.92,16.76,9.05" target="#b9">[12]</ref> natural language processing library. During system development we performed all the experiments on training corpus <ref type="bibr" coords="5,333.31,641.44,16.72,9.05" target="#b10">[13]</ref> only. The developed system then deployed on virtual machine for evaluation on test corpus <ref type="bibr" coords="5,399.67,652.96,16.72,9.05" target="#b10">[13]</ref> using TIRA experimentation platform. The test data was not revealed to participants in order to avoid the result optimization based on data set. Table <ref type="table" coords="6,161.64,300.47,3.76,9.05" target="#tab_0">1</ref>, shows the performance of systems participated in source retrieval sub-task of PAN 2014. Our approach achieved precision and recall of 0.5083 and 0.3824 respectively. The recall is highest among all the participants and we got fourth position in terms of precision. However, we achieved third position in terms of fmeasure which is considered as the tradeoff between precision and recall.</p><p>We submitted an average 59.95 queries to download 38.76 sources per suspicious document. We formed four queries per chunk but their conditional submission to search engine further reduced the total number of queries submitted. As we retrieves 10 documents per query but an average 38.76 downloads per document show that our download filter performs quite well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper we have presented an approach to retrieve possible sources of reused text for a given plagiarized document. We have introduced an intelligent way of document chunking and a combination of two well known keyword extraction strategies to extract query terms. During the development of our system we experimented on the various parameters used, such as the title size, the size of paragraph need to create a new chunk, the size limit of chunk and the POS tags to be extracted after tagging the subgroups.</p><p>Our system performance is evaluated on PAN 14 test data set and compared with the systems of other participants. Results show that our system's performance is best in terms of finding the reused sources using an average workload. The plagiarism detection method we proposed does minimal computations and performs the task at a speed suitable enough for practical applications.</p><p>However, there are certain possibilities to improve the performance of our system. Our method succeeded in terms of recall, but we need to further reduce the total workload. For this purpose a deeper investigation into query formation and download filtering is required. A better performance can be achieved by using the advanced functionalities offered by ChatNoir search engine. These include the batch query service and addition parameters returned in search results. Furthermore our plan is to extend our approach to deal with cross-language plagiarism cases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,127.58,149.88,340.38,131.55"><head>Table 1 . PAN 2014 Source retrieval results</head><label>1</label><figDesc></figDesc><table coords="6,127.58,164.64,340.38,116.79"><row><cell></cell><cell></cell><cell>Downloads</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Queries</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Until First</cell><cell>F-</cell><cell>No</cell><cell></cell><cell></cell><cell>Until First</cell><cell></cell><cell></cell></row><row><cell>Users</cell><cell>Downloads</cell><cell>Detection</cell><cell>Measure</cell><cell>Detection</cell><cell>Precision</cell><cell>Queries</cell><cell>Detection</cell><cell>Recall</cell><cell>Runtime</cell></row><row><cell>elizalde14</cell><cell>33.2</cell><cell>3.9</cell><cell>0.3432</cell><cell>7</cell><cell>0.4002</cell><cell>54.5</cell><cell>16.4</cell><cell>0.3860</cell><cell>04:02:00</cell></row><row><cell>kong14</cell><cell>207.1</cell><cell>24.9</cell><cell>0.1197</cell><cell>6</cell><cell>0.0756</cell><cell>83.5</cell><cell>85.7</cell><cell>0.4820</cell><cell>24:03:31</cell></row><row><cell>prakash14</cell><cell>38.76</cell><cell>3.76</cell><cell>0.3871</cell><cell>7</cell><cell>0.3824</cell><cell>59.95</cell><cell>8.08</cell><cell>0.5083</cell><cell>19:47:45</cell></row><row><cell>suchomel14</cell><cell>237.3</cell><cell>38.6</cell><cell>0.1062</cell><cell>2</cell><cell>0.0775</cell><cell>19.5</cell><cell>3.1</cell><cell>0.3984</cell><cell>45:42:06</cell></row><row><cell>williams14</cell><cell>14.41</cell><cell>2.33</cell><cell>0.4726</cell><cell>4</cell><cell>0.5716</cell><cell>117.13</cell><cell>18.82</cell><cell>0.4762</cell><cell>39:44:11</cell></row><row><cell>zubarev14</cell><cell>18.61</cell><cell>2.25</cell><cell>0.4483</cell><cell>3</cell><cell>0.5378</cell><cell>37.03</cell><cell>5.39</cell><cell>0.4475</cell><cell>40:42:18</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,128.58,177.32,342.16,9.05;7,142.82,188.84,216.13,9.05" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="7,190.58,177.32,230.92,9.05">Old and new challenges in automatic plagiarism detection</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003-02">Feb 2003</date>
		</imprint>
		<respStmt>
			<orgName>Plagiarism Advisory Service, University of Sheffield</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="7,128.58,200.36,341.88,9.05;7,142.82,211.76,195.27,9.05" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,287.23,200.36,89.04,9.05">Plagiarism -a survey</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Maurer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Kappe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,385.11,200.36,85.36,9.05;7,142.82,211.76,73.35,9.05">Journal of Universal Computer Science</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1050" to="1084" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,128.58,223.28,342.00,9.05;7,142.82,234.80,119.42,9.05" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="7,191.42,223.28,279.17,9.05;7,142.82,234.80,68.51,9.05">The Difference Between Copyright Infringement and Plagiarism, Plagiarism Today</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-10">Oct 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,128.58,246.32,342.17,9.05;7,142.82,257.84,327.95,9.05;7,142.82,269.39,26.67,9.05" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,211.31,246.32,259.44,9.05;7,142.82,257.84,86.61,9.05">Cheating among college and university students: A North American perspective</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">L</forename><surname>Mccabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,236.93,257.84,187.62,9.05">International Journal for Educational Integrity</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,128.58,280.79,341.95,9.05;7,142.82,292.31,182.51,9.05" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="7,312.89,280.79,157.65,9.05;7,142.82,292.31,38.64,9.05">The Digital Revolution and Higher Education</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lenhart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Moore</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011-08">Aug 2011</date>
			<publisher>Pew Research Center</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,128.58,303.83,341.77,9.05;7,142.82,315.35,327.60,9.05;7,142.82,326.87,327.60,9.05;7,142.82,338.39,327.93,9.05;7,142.82,349.79,23.33,9.05" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,264.91,315.35,205.51,9.05;7,142.82,326.87,83.37,9.05">Overview of the 5th International Competition on Plagiarism Detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tippmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,434.99,326.87,35.43,9.05;7,142.82,338.39,195.67,9.05">Working Notes Papers of the CLEF 2013 Evaluation Labs</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Navigli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Tufis</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2013-09">Sep 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,128.58,361.31,341.50,9.05;7,142.82,372.83,327.65,9.05;7,142.82,384.35,327.73,9.05;7,142.82,395.87,322.88,9.05;7,465.82,393.72,5.04,5.89;7,142.82,407.27,327.88,9.05;7,142.82,418.79,106.72,9.05" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,214.21,372.83,240.40,9.05">Recent Trends in Digital Text Forensics and its Evaluation</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Busse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,422.90,384.35,47.65,9.05;7,142.82,395.87,322.88,9.05;7,465.82,393.72,5.04,5.89;7,142.82,407.27,237.30,9.05">Information Access Evaluation meets Multilinguality, Multimodality, and Visualization. 4 th International Conference of the CLEF Initiative (CLEF 13)</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Paredes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Rosso</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Stein</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013-09">Sep 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,128.58,430.31,341.89,9.05;7,142.82,441.85,327.54,9.05;7,142.82,453.37,327.60,9.05;7,142.82,464.89,285.20,9.05;7,124.82,476.29,100.06,9.05;7,124.82,487.81,91.34,9.05" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,200.50,441.85,247.75,9.05">ChatNoir: A Search Engine for the ClueWeb09 Corpus</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Graßegger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tippmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Welsch</surname></persName>
		</author>
		<ptr target="www.plagiarism.org10.www.turnitin.com" />
	</analytic>
	<monogr>
		<title level="m" coord="7,142.82,453.37,327.60,9.05;7,142.82,464.89,199.66,9.05">Proceedings of the 35th International ACM Conference on Research and Development in Information Retrieval (SIGIR 12)</title>
		<meeting>the 35th International ACM Conference on Research and Development in Information Retrieval (SIGIR 12)</meeting>
		<imprint>
			<date type="published" when="2012-08">Aug 2012</date>
			<biblScope unit="page">1004</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,133.20,499.33,337.13,9.05;7,142.82,510.85,327.45,9.05;7,142.82,522.37,327.90,9.05;7,142.82,533.89,205.24,9.05" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,288.77,499.33,181.56,9.05;7,142.82,510.85,169.92,9.05">Enriching the Knowledge Sources Used in a Maximum Entropy Part-of-Speech Tagger</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,331.88,510.85,138.39,9.05;7,142.82,522.37,327.90,9.05;7,142.82,533.89,145.06,9.05">Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-2000)</title>
		<meeting>the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-2000)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="63" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,133.20,545.29,337.12,9.05;7,142.82,556.81,327.47,9.05;7,142.82,568.33,184.56,9.05" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,401.01,556.81,64.95,9.05">Apache opennlp</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kottmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Margulies</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ingersoll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Drost</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kosin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Goetz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Morton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Autayeu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Galitsky</surname></persName>
		</author>
		<ptr target="www.opennlp.apache.org" />
	</analytic>
	<monogr>
		<title level="j" coord="7,142.82,568.33,27.09,9.05">Online</title>
		<imprint>
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,133.20,579.85,337.27,9.05;7,142.82,591.37,194.37,9.05;7,337.51,589.22,5.40,5.89;7,349.75,591.37,120.91,9.05;7,142.82,602.89,327.67,9.05;7,142.82,614.29,98.40,9.05" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,411.03,579.85,59.43,9.05;7,142.82,591.37,158.16,9.05">An Evaluation Framework for Plagiarism Detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,327.18,591.37,10.01,9.05;7,337.51,589.22,5.40,5.89;7,349.75,591.37,120.91,9.05;7,142.82,602.89,327.67,9.05;7,142.82,614.29,42.85,9.05">23 rd International Conference on Computational Linguistics (COLING 10), Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010-08">Aug 2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
