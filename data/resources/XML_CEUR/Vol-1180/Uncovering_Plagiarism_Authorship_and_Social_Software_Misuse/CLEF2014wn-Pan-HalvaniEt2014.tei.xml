<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,192.54,116.90,230.27,12.90;1,210.33,134.83,194.70,12.90;1,223.43,154.68,168.49,10.75">VEBAV -A Simple, Scalable and Fast Authorship Verification Scheme Notebook for PAN at CLEF 2014</title>
				<funder ref="#_Az9uJ7d">
					<orgName type="full">German state government of Hesse</orgName>
				</funder>
				<funder>
					<orgName type="full">CASED Center for Advanced Security Research Darmstadt, Germany</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,232.19,191.07,53.70,8.64"><forename type="first">Oren</forename><surname>Halvani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fraunhofer Institute for Secure Information Technology SIT</orgName>
								<address>
									<addrLine>Rheinstrasse 75</addrLine>
									<postCode>64295</postCode>
									<settlement>Darmstadt</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,309.84,191.07,73.33,8.64"><forename type="first">Martin</forename><surname>Steinebach</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fraunhofer Institute for Secure Information Technology SIT</orgName>
								<address>
									<addrLine>Rheinstrasse 75</addrLine>
									<postCode>64295</postCode>
									<settlement>Darmstadt</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,192.54,116.90,230.27,12.90;1,210.33,134.83,194.70,12.90;1,223.43,154.68,168.49,10.75">VEBAV -A Simple, Scalable and Fast Authorship Verification Scheme Notebook for PAN at CLEF 2014</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">513449D3DB7CEC4DE1E5FF13333DDD7A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Authorship verification, one-class-classification</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present VEBAV -a simple, scalable and fast authorship verification scheme for the Author Identification (AI) task within the PAN-2014 competition. VEBAV (VEctor-Based Authorship Verifier), which is a modification of our existing PAN-2013 approach, is an intrinsic one-class-verification method, based on a simple distance function. VEBAV provides a number of benefits as for instance the independence of linguistic resources and tools like ontologies, thesauruses, language models, dictionaries, spellcheckers, etc. Another benefit is the low runtime of the method, due to the fact that deep linguistic processing techniques like POS-tagging, chunking or parsing are not taken into account. A further benefit of VEBAV is the ability to handle more as only one language. More concretely, it can be applied on documents written in Indo-European languages such as Dutch, English, Greek or Spanish. Regarding its configuration VEBAV can be extended or modified easily by replacing its underlying components. These include, for instance the distance function (required for classification), the acceptance criterion, the underlying features including their parameters and many more. In our experiments we achieved regarding a 20%-split of the PAN 2014 AI-training-corpus an overall accuracy score of 65,83% (in detail: 80% for Dutch-Essays, 55% for Dutch-Reviews, 55% for English-Essays, 80% English-Novels, 70% for Greek-Articles and 55% for Spanish-Articles).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Authorship Verification (AV ) is a sub-discipline of Authorship Analysis <ref type="bibr" coords="1,430.88,543.75,10.79,8.64" target="#b3">[4,</ref><ref type="bibr" coords="1,444.74,543.75,32.26,8.64">Page: 3]</ref>, which itself is an integral part of digital text forensics. It can be applied in many forensic scenarios as for instance checking the authenticity of written contracts, threats, insults, testaments, etc. where the goal of AV remains always the same: Verify if two documents D A and D A ? are written by the same author A, or not. An alternative reformulation of the goal is to verify the authorship of D A ? , given a set of sample documents of A. From a Machine Learning perspective AV clearly forms an one-class-classification problem <ref type="bibr" coords="1,170.36,627.43,10.58,8.64" target="#b2">[3]</ref>, due to the fact that A is the only target class to be distinguished among all other possible classes (authors), where their number can be theoretically infinite.</p><p>In order to perform AV at least four components are mandatorily required:</p><p>-The document D A ? , which should be verified regarding its alleged authorship.</p><p>-A training set D A = {D 1A , D 2A , ...}, where each D iA represents a sample document of A. -A set of features F = {f 1 , f 2 , ...}, where each f j (style marker) should help to model the writing style of D A ? and each D iA ∈ D A . -At least one classification method, which accepts or rejects the given authorship based on F and a predefined or dynamically determined threshold θ.</p><p>The aim of this paper is to provide a simple, scalable and fast AV scheme, which offers a number of benefits as for instance promising detection rates, easy implementation, low runtime, independence of language or linguistic resources as well as easy modifiability and expandability. Our proposed AV scheme, denoted by VEBAV, is based on our earlier approach regarding the PAN 2013 AI-task, which itself formed a modification of the Nearest Neighbor (NN) one-class classification technique, described by Tax in <ref type="bibr" coords="2,469.81,308.34,10.79,8.64" target="#b4">[5,</ref><ref type="bibr" coords="2,134.77,320.29,36.06,8.64">Page: 69]</ref>. In a nutshell, VEBAV takes as an input a set of sample documents of a known author (D A ) and one document of an unknown author (D A ? ). All documents in D A are first concatenated to a big document which is then splitted again into (near) eqal-sized chunks (such that D A includes now only these chunks). Afterwards, feature vectors are constructed from each D iA ∈ D A and from D A ? according to preselected feature sets. Next, a representative is selected among the training feature vectors (based on two options), which is important to determine the decision regarding the alleged authorship. In the next step distances are calculated between the representative and the remaining training feature vectors, as well as between the representative and the test feature vector. Depending on all calculated distances a twofold decision regarding the alleged authorship is generated, which includes a ternary decision δ ∈ {Yes, No, Unanswered} and a probability score ρ that describes the soundness of δ.</p><p>The rest of this paper is structured as follows. In the following section we explain how we extract features from the corresponding linguistic layers and which features have been used in our approach. Afterwards, we describe in section 3 our verification scheme. In section 4 we present the corpus that was used to evaluate our scheme, while in section 5 we show our evaluation results. Finally, we draw our conclusions and the challenges we were faced in section 6 and provide some ideas for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Features</head><p>Features (style markers) are the core of AV , since they are able to approximate writing styles and thus, can help to judge automatically if two texts have a very similar style. If this holds, it is an indicator that both texts could be written by the same author. In the next subsections we explain which sources exactly quantifiable features can be retrieved from, which tools are required for this extraction process and also what kind of feature sets we used in our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Linguistic layers</head><p>From a linguistic point of view, features are extracted from so-called linguistic layers that can be understood as abstract units within a text. In summary, the most important linguistic layers are the following:</p><p>-Phoneme layer: This layer includes phoneme-based features as for example vowels, consonants or also the more sophisticated supra-segmental or prosodic features. Such features can typically be won out of texts by using (pronouncing) dictionaries (e.g. IPA).</p><p>-Character layer: This layer includes character-based features as for instance prefixes, suffixes or letter n-grams, which typically are extracted from texts via regular expressions.</p><p>-Lexical layer: This layer includes token-based features as for instance function words or POS-Tags (Part-Of-Speech Tags). These features can be extracted from texts via tokenizers (which often are based on simple regular expressions).</p><p>-Syntactic layer: This layer includes syntax-based features as for instance constituents (e.g. nominal phrases) or collocations. Such features can be extracted by sophisticated regular expressions or by natural language processing tools (e.g. POS-Tagger). However, the latter one is normally bounded to a specific language model and thus, cannot scale to multiple languages. Besides this the runtime of natural language processing tools is much more higher as the runtime caused by pattern matching via regular expressions.</p><p>-Semantic Layer: This layer includes semantic-based features, e.g. semantic relations (hyponymous, synonymys, meronyms, etc.). Such features require deep linguistic processing, which often rely on external knowledge resources (e.g. Word-Net) or complex tools (e.g. parsers, named entity eecognizers, etc.).</p><p>In VEBAV we only make use of the Character, Lexical and Syntactic linguistic layers, due to the effectiveness of their underlying features and the low runtime, caused by the feature extraction process via regular expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Feature sets</head><p>In this paper we use the term feature set, denoted by F , which represents features belonging to one or more linguistic layers. In Table <ref type="table" coords="3,329.46,599.12,4.98,8.64" target="#tab_0">1</ref> we list 14 feature sets that have been used in our experiments. Here, one should pay attention that F 12 , F 13 and F 14 form mixtures of existing feature sets. The idea behind it was to see if such mixtures can outperform <ref type="foot" coords="3,179.59,633.31,3.49,6.05" target="#foot_0">1</ref> single feature sets when it comes to classification. A mix of three feature sets</p><formula xml:id="formula_0" coords="4,140.16,250.90,213.51,15.76">F1 ∪ F3 ∪ F6 F13 Mix2</formula><p>A mix of three feature sets</p><formula xml:id="formula_1" coords="4,140.16,260.21,213.51,15.76">F1 ∪ F2 ∪ F5 F14 Mix3</formula><p>A mix of four feature sets F3 ∪ F4 ∪ F5 ∪ F8</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Parameters</head><p>As can be seen in Table <ref type="table" coords="4,235.58,320.42,3.74,8.64" target="#tab_0">1</ref>, six feature sets can be parameterized by n (n-gram sizes) and/or k (length of prefixes/suffixes). It should be emphasized that adjusting these two parameters can influence the results massively. Hence, we generalized both settings by using constant values (n = k = 3), learned from earlier experiments, which turned out to be reliable also in the current experiments among the diverse PAN subcorpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Verification Scheme</head><p>In this section we give a detailed description of our AV scheme VEBAV. For overview reasons we divided the entire workflow of the algorithm into six subsections, where we first explain what kind of preprocessing we perform on the data. The other five subsections focus on the algorithm itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preprocessing</head><p>In contrast to our approach in PAN-2013 we decided in our current approach neither to apply normalization nor noise reduction techniques. Instead, we treat each text as it is, which turned out to be not only less burdensome but also promising. Our only preprocessing mechanism is restricted to concatenate all D iA ∈ D A to a single document D A which, depending on the length, is splitted again into (near) equal-sized chunks D 1A , D 2A , ..., D A . Note that in our experiments is statically set to five chunks if, and only if, the length of D A is above 15,000 characters, otherwise is statically set to three chunks. The reason for this decision is that we observed quite better results in contrast by using only one fixed for both, short or long texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Vocabulary Generation</head><p>In order the form a basis for the construction of feature vectors, we need to build a global feature vocabulary, denoted by V. But beforehand, we first need to select at least one (appropriate) feature set F , to know which sort of features should be taken into account. Here, appropriate refers to features that...</p><p>-... are able to model high similarity between D A ? and D A (for the case A ? = A).</p><p>-... are able to discriminate well between the writing style of A and all other possible authors (for the case A ? = A). -... appear in all generated vocabularies. Afterwards, we construct from D A ? and each chunk D iA ∈ D A the corresponding feature vocabularies V D A ? and V D1 A , V D2 A , ..., V D A . As a last step we apply an intersection among all constructed vocabularies to build the global vocabulary V:</p><formula xml:id="formula_2" coords="5,226.52,251.14,254.08,12.01">V = V D A ? ∩ V D1 A ∩ V D2 A ∩ ... ∩ V D A<label>(1)</label></formula><p>Note that in VEBAV we consider case-sensitive features, such that word ∈ V and Word ∈ V are both valid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Constructing feature vectors</head><p>Once V is generated, the next step is to construct the feature vectors F 1A , F 2A , ..., F A from each D iA ∈ D A and F A? from D A ? . The construction process regarding these vectors is very straightforward. We look up for each f i ∈ V how often it occurs in some document D and denote its absolute frequency by α i . Next, we normalize α i by the length of D, to obtain its relative frequency, which represents a number x i ∈ [0 ; 1]. Hence, the feature vector representation regarding D is formally defined by:</p><formula xml:id="formula_3" coords="5,235.21,413.87,144.93,9.65">F = (x 1 , x 2 , ..., x n ) , with n = |V| .</formula><p>(2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Representative Selection</head><p>After constructing all feature vectors, a representative (training-) feature vector F rep must be selected. This step is essential for the later determination of the decision regarding the alleged authorship. In general, VEBAV offers two options to select F rep :</p><p>1. Selecting F rep dynamically by using a similarity function (e.g. cosine similarity) between all training feature vectors. Here, F rep is selected according to the feature vector, who is mostly dissimilar from the others. In other terms F rep can be understand as an outlier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Selecting F rep manually (static).</head><p>Note that in our experiments we chose the first option.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Distances Calculations</head><p>In this step we calculate the distances, needed for the decision determination regarding the alleged authorship. Concretely, we calculate the distances d 1 , d 2 , ..., d between F rep and each F 1A , F 2A , ..., F A as well as the the distance d ? between F rep and F A? .</p><p>The calculation of these distances requires a predefined distance function dist(X, Y ) with X = F rep and Y ∈ {F 1A , F 2A , ..., F A }. We implemented a broad range of distance functions in VEBAV, where the majority have been taken from <ref type="bibr" coords="6,411.45,144.22,10.58,8.64" target="#b0">[1]</ref>. However, for complexity reasons we used only the Minkowski distance function in our experiments, which is defined formally as:</p><formula xml:id="formula_4" coords="6,198.63,186.58,281.96,30.32">dist(X, Y ) = n i=1 |x i -y i | λ 1 λ , with λ ∈ R + \ {0} ,<label>(3)</label></formula><p>As a last step we calculate the average regarding the distances between F rep and each F 1A , F 2A , ..., F A as follows:</p><formula xml:id="formula_5" coords="6,249.28,258.05,231.31,16.78">d ∅ = 1 (d 1 + d 2 + . . . + d )<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Decision Determination</head><p>The goal of this step is to construct a twofold decision regarding the alleged authorship, which consists of a ternary decision δ ∈ {Yes, No, Unanswered} and a probability score ρ. In order to calculate these terms both values are required d ? and d ∅ . For the latter one we use the following adjusted form:</p><formula xml:id="formula_6" coords="6,298.09,352.59,76.75,10.04">d = d ∅ + (ω • τ ),</formula><p>where ω denotes a weight and τ a tolerance parameter, calculated from the a standard deviation of d 1 , d 2 , ..., d . The idea behind ω and τ is to cope with the presence of noisy writing styles, that might be the result of mixing different sample documents of A together in his training set D A . Note that in our experiments we chose a neutral weight (τ = 1), since we could not investigate an optimal value for it (even adjustig it by 0.1 can cause a massive drop regarding the classification results). With these we first define the probability score as:</p><formula xml:id="formula_7" coords="6,283.46,443.47,197.13,26.23">ρ = 1 1 + d ? d (5)</formula><p>Next, we define the ternary decision as follows:</p><formula xml:id="formula_8" coords="6,205.72,498.59,274.87,41.38">δ =      Yes, d ? &lt; d No, d ? &gt; d Unanswered, (d ? = d ) ∨ (|d ? -d | &lt; ε) (6)</formula><p>The concrete semantic behind δ is:</p><p>-Yes: VEBAV accepts the alleged author as the true author (A ? = A).</p><p>-No: VEBAV rejects the alleged author as the true author (A ? = A).</p><p>-Unanswered: VEBAV is unable to generate a prediction because d ? and d are equal/near-equal or due to another unexpected result. In any case ρ is set to 0.5. Note that depending on how ε was chosen (regarding the case that d ? and d are near-equal) the number of Unanswered decisions, in the context of a corpus evaluation, can vary considerably. In our experiments we chose ε = 0, 001 as this small value restricts the number of Unanswered decisions, where ρ is near 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Used Corpora</head><p>Regarding our experiments we used the official PAN-2014 Author Identification training corpus, denoted by C PAN-14 , which has been released <ref type="foot" coords="7,366.15,152.91,3.49,6.05" target="#foot_1">2</ref>  For our experiments we used a 80% portion of C PAN-14 for training/parameter learning (denoted by C PAN-Train ), while the remaining 20% portion was used for testing (denoted by C PAN-Test ). Regarding C PAN-Test we used the same structure as C PAN-14 such that  <ref type="table" coords="7,404.47,419.86,3.74,8.64" target="#tab_2">3</ref>. </p><formula xml:id="formula_9" coords="7,134.77,395.63,191.71,9.64">C PAN-Test = {C DE , C DR , C EE , C EN , C GR , C SP } holds,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>In this section we carry out our evaluation regarding the C PAN-14 = C PAN-Train ∪ C PAN-Test corpus. First we explain which performance measures were used and secondly, how the most important parameters were learned from C PAN-Train . Finally, we evaluate our approach on C PAN-Test .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Performance measures</head><p>In order to evaluate our approach, we used several performance measures, sharing the following variables:</p><formula xml:id="formula_10" coords="8,198.51,215.23,282.08,39.54">m = Number of problems in C (7) m c = Number of correct answers per C (8) m u = Number of unanswered problems answers per C (9)</formula><p>The first performance measure is Accuracy = m c m , whereas for the second performance measure we first need to define two terms:</p><formula xml:id="formula_11" coords="8,191.69,302.07,288.91,30.32">AUC = 1 m m i=1 ρ i , c@1 = 1 m m c + m u • m c m<label>(10)</label></formula><p>Here, ρ i denotes the probability score regarding its corresponding problem p i , which was defined in section 4. With these two measures we define the second performance measure: AUC • c@1. Note that for parameter learning from the C PAN-Train corpus we only used the Accuracy measure, (in our opinion) this measure is better interpretable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experiment I: Finding an optimal λ for the Minkowski distance function</head><p>The intention behind this experiment was to find an optimal λ parameter, used by the Minkowski distance function. Since λ has a strong influence on the classification result it must be well-chosen, in order to generalize across the range of all involved corpora and all feature sets. To achieve this generalization we merged all training subcorpora, such that C PAN-Train = C DE ∪ C DR ∪ C EE ∪ C EN ∪ C GR ∪ C SP holds. Afterwards, we applied VEBAV on C PAN-Train , where as an input we used all mentioned feature sets in Table <ref type="table" coords="8,459.24,488.32,4.98,8.64" target="#tab_0">1</ref> and the following 14 predefined λ values {0.2, 0.4, 0.6, 0.8, 1, 2, ..., 10}. We constructed from the results a table, where the rows represent the feature sets F 1 , F 2 , . . . , F 14 , while the columns represent the 14 λ values. Next, we derived a row from this table that includes the medians regarding all columns. This row is illustrated in Figure <ref type="figure" coords="8,442.88,536.14,3.74,8.64" target="#fig_1">1</ref>. As can be seen in Figure <ref type="figure" coords="8,206.58,548.09,3.74,8.64" target="#fig_1">1</ref>, an optimal range for λ is [0.2; 1], where 0.6 seems to be the most promising one in terms of robustness, among all involved feature sets. As a consequence of this analysis, we decided to use λ = 0.6 for the further experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiment II: Determinig the classification stregth of all feature sets</head><p>In this experiment we wanted to compare the classification stregth of all involved feature sets. For this we applied VEBAV on F 1 , F 2 , . . . , F 14 regarding the six subcorpora in C PAN-Train , where this time we used the fixed setting λ = 0.6. From the resulting table (rows = feature sets, columns = subcorpora) we calculated for each row (containing the classification results regarding all six subcorpora) the median, which gave us a new column, illustrated in Figure <ref type="figure" coords="9,256.02,302.52,3.74,8.64" target="#fig_2">2</ref>. Here, it can be seen that the majority of all feature sets are more or less equally strong, excepting F 10 and F 11 , which seem to be useless for VEBAV (at least for C PAN-Train ). Furthermore, it can be observed that using mixed feature sets lead to slightly better classification results, compared with the majority of the non-mixed feature sets.  In order to get a better picture of how VEBAV performs without considering the medians among the six subcorpora, we show in Figure <ref type="figure" coords="9,355.67,561.79,4.98,8.64" target="#fig_3">3</ref> a comparison of the top three performing feature sets for each individual subcorpus. One interesting observation that can be concluded from Figure <ref type="figure" coords="9,254.53,585.70,4.98,8.64" target="#fig_3">3</ref> is that the feature set F 1 (characters) is almost as strong as the mixed feature sets, which involve more sophisticated features (such as character n-grams or tokens). This shows that by using only characters as features, it is possible to verify authorships with a classification result, which is even better as a random guess (50%). Another observation, which is worth to be mentioned, is the fact that the greek corpus seems to contain problems that are more difficult to judge, compared to the problems in the other subcorpora, where the classification results are obviously better. We believe that this is not a language-based issue, but more an effort of the organizers to make the task more challenging, as this was also the case in PAN-2013 <ref type="bibr" coords="10,430.01,302.83,10.58,8.64" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Experiment III: Single feature sets vs. feature set combinations</head><p>In this experiment we were curious to know, if using combinations of feature sets by applying majority-voting can outperform classifications based only on single feature sets. As a setup for this experiment, we picked out the six most promising feature sets {F 1 , F 2 , F 5 , F 12 , F 13 , F 14 } and used them to construct a power set P, which includes 2 6 = 64 feature set combinations. Next, we removed those subsets F combi ∈ (P \ ∅) comprising of an even number of feature set combinations, to enable a fair (non-random based) majority-voting and also to speed the classification process up a little by avoiding unnecessary runs. This led to 2 5 = 32 suitable combinations F comb1 , F comb2 , ..., F comb32 , where we applied each F comb as an input for VEBAV regarding C PAN-Train . Next, we stored all combinations and their corresponding classification results in a list (sorted in descending order) and selected the top five combinations, listed in Table <ref type="table" coords="10,413.10,474.16,3.74,8.64" target="#tab_4">4</ref>. Unfortunately, it can be observed from the comparison between Table <ref type="table" coords="10,363.30,486.11,4.98,8.64" target="#tab_4">4</ref> and Figure <ref type="figure" coords="10,418.96,486.11,4.98,8.64" target="#fig_2">2</ref> that applying majority-voting on feature set combinations gives only negligible improvements (≈ 1-2%) for the most cases. When focussing on F 12 in Figure <ref type="figure" coords="10,367.79,510.02,4.98,8.64" target="#fig_2">2</ref> it can be even seen that its median accuracy of 62.50% outperfroms any sort of feature set combination. One possible reason for the unsatisfactory results could be the fact that many single feature sets made identical ternary decisions (Yes, No, Unanswered), such that applying majorityvoting is only effectively in few cases. We observed this phenomenon several times by stepping through the code, using the debugger. Hence, we do not consider the usage of feature set combinations for further evaluations in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Experiment IV: Obtaining corpus dependent parameters</head><p>Due to the fact that the classification scores regarding the Experiments I-III were relatively low, we decided in this experiment to learn individual parameters from each corpus C ∈ {C DE , C DR , C EE , C EN , C GR , C SP } and to thereby improve the classification results. For this we first applied VEBAV on each C to obtain individual λ scores. Since there where six corpora, we constructed six tables, where the rows denote F 1 , F 2 , ..., F 14 and the coloumns the 14 λ values. Then, we picked those six tuples (F i , λ j ), which led to the maximum accuracy score in each table. These tuples are listed in Table <ref type="table" coords="11,453.45,269.18,3.74,8.64" target="#tab_5">5</ref>. One can see here that it definitely make sense to use corpus-dependent (or more precisely language/genre-dependent) parameters, instead of using a global setting. However, the price for better results may be expensive in terms of overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Evaluation results for the test set</head><p>In this section we evaluate VEBAV on the test set C PAN-Test , where we used all relevant information, learned from the prior experiments. For a better overview we divide the evaluations into three subsections, where we first show how VEBAV performed with generalized parameters then how it performed with individual parameters and finally how it performed regarding the runtime.</p><p>Evaluation results regarding generalized parameters. For the first evaluation we set as an input for VEBAV the generalized parameters λ = 0.6 and F 12 that were learned from Experiments I-II. The results are given in Table <ref type="table" coords="11,351.44,609.61,3.74,8.64" target="#tab_6">6</ref>. As can be observed from this table, using generalized parameters seems not to be the best choice, as achieving optimal results was possible for only one subcorpus, while for two subcorpora the results where even lower than a random guess. Moreover, it can be seen that the AUC • c@1 scores are very low, which are not only caused by the low accuracies themselves, but Evaluation results regarding individual parameters. In the second evaluation we used the individual parameters, learned in Experiment IV. The results are given in Table <ref type="table" coords="12,134.77,305.27,3.74,8.64" target="#tab_7">7</ref>. By looking on the results in this table, we can conclude once again that using an indi- The runtime (measured in milliseconds) for each feature set and each subcorpus is given in Table <ref type="table" coords="12,193.25,585.70,3.74,8.64" target="#tab_8">8</ref>. As can be seen in the Median column in Table <ref type="table" coords="12,388.65,585.70,3.74,8.64" target="#tab_8">8</ref>, the fastest classification was performed with the feature set F 3 (punctuation marks). The reason for this is that F 3 leads to very few feature-lookups (≈ 20 per document), such that IO-accesses are negligible. In contrast to this, F 1 leads to the highest runtime (excepting F 12 and F 13 since they are mixtures of existing sets), due to the fact that it requires an iteration over each character in a text. Another interesting observation is that token-based features (F 8-11 ) also require very low runtime. This is explained by the fact that there are much less tokens in texts than characters and thus, the number of lookups is also very limited. The overall runtime for C PAN-14 , without considering the subcorpora, is given in Figure <ref type="figure" coords="13,163.94,410.33,3.74,8.64" target="#fig_4">4</ref>. An interesting observation that can be made by comparing Figure <ref type="figure" coords="13,444.95,410.33,4.98,8.64" target="#fig_4">4</ref>  ure 2 is that F 5 can be seen as an optimal candidate when a trade-off between a high classification result and low runtime must be taken into account. Here, F 5 achieves a classification result of 59, 96% by requiring only 2,423 milliseconds. F 8 behaves in a similar way (55, 56% accuracy by requiring only 1,830 milliseconds).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion &amp; future work</head><p>In this paper we presented a simple, scalable and fast authorship verification scheme for the Author Identification task within the PAN-2014 competition. Our method provides a number of benefits as for example that it is able to handle (even very short) texts written in several languages, across different kinds of genre. Besides this, the method is independent of linguistic resources such as ontologies, thesauruses, language models, etc. A further benefit is the low runtime of the method, since there is no need for deep linguistic processing like POS-tagging, chunking or parsing. Another benefit is that the involved components within the method can be replaced easily as for example the distance function (required for classification), the acceptance-threshold or the feature sets including their parameters. Moreover, the classification itself can be modified easily, e.g. by using an ensemble of several distance functions. Unfortunately, besides benefits our approach has several pitfalls too. One of the biggest challenges, for example, is the inscrutability of the methods parameter-space, due to the fact that the number of possible configuration settings is near infinite. Such settings include for instance the λ parameter for the involved distance function, the values for the n and k parameters, the weight (ω) and tolerance (τ ) parameters that influence the classification quality but also other options such as (the number of chunks) or the used feature normalization strategy. Due to the complexity of our scheme we could only perform a small number of experiments to obtain at least an optimal and the most promising feature sets. This, however, was done only for the official PAN corpus but not for other corpora. Thus, it had not been proved if the learned parameters are able to perform satisfactorily on other corpora. Another challenge that remains unsolved is how to optimize the probability score ρ, determined in the decision determination step, as this value has also a strong influence on the resulting AUC • c@1 scores, which in our case are very low. Hence, further tests are essential for the applicability of our scheme.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,328.73,395.95,151.87,8.64;7,134.77,407.59,345.83,9.64;7,134.77,419.86,267.22,8.64"><head></head><label></label><figDesc>where the number of problems in each C ∈ C PAN-Test equals 20% of the problems in each C ∈ C PAN-Train . The concrete statistic including the distribution of true/false authorships is given in Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,163.89,253.57,287.57,8.12"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Comparison of different λ values for the Minkowski distance function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,150.85,509.58,313.66,8.12"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Comparison of the 14 feature sets across all training corpora (using λ = 0.6).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="10,134.77,245.25,345.81,8.12;10,134.77,256.56,70.97,7.77"><head>F14Figure 3 .</head><label>3</label><figDesc>Figure 3. Comparison of the top three performing feature sets (without applying medians among the six subcorpora).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="13,134.77,569.72,345.82,8.12;13,134.77,581.03,45.06,7.77"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Runtime needed by VEBAV for the classification of CPAN-14 (without considering the subcorpora).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,140.16,116.82,336.87,140.52"><head>Table 1 .</head><label>1</label><figDesc>All feature sets used in our approach.</figDesc><table coords="4,140.16,137.45,336.87,119.89"><row><cell>Fi Feature set</cell><cell>Description</cell><cell>Examples</cell><cell></cell></row><row><cell>F1 Characters</cell><cell>All kind of characters</cell><cell cols="3">{a,b,1,8,#,&lt;,%,!,. . .}</cell></row><row><cell>F2 Letters</cell><cell>All kind of letters</cell><cell cols="3">{a,b,α, β,ä,ß,ó,á,ñ,. . .}</cell></row><row><cell cols="5">F3 Punctuation marks Symbols that structure sentences {.,:,;-,",(,),. . .}</cell></row><row><cell>F4 Word k Prefixes</cell><cell cols="2">The k starting letters of words example</cell><cell cols="2">{e,ex,exa,exam,. . .}</cell></row><row><cell>F5 Word k Suffixes</cell><cell>The k ending letters of words</cell><cell>example</cell><cell cols="2">{e,le,ple,mple,. . .}</cell></row><row><cell cols="3">F6 Character n-grams Overlapping character-fragments ex-ample</cell><cell cols="2">{ex-,x-a,-am,. . .}</cell></row><row><cell>F7 Letter n-grams</cell><cell>Overlapping letter-fragments</cell><cell>ex-ample</cell><cell cols="2">{exa,xam,amp,. . .}</cell></row><row><cell>F8 Tokens</cell><cell cols="3">Segmented character-based units A [sample] text!</cell><cell>{A,[sample],text!}</cell></row><row><cell>F9 Words</cell><cell>Segmented letter-based units</cell><cell cols="2">A [sample] text!</cell><cell>{A,sample,text}</cell></row><row><cell>F10 Token n-grams</cell><cell>Overlapping token-fragments</cell><cell cols="2">Wind and rain!</cell><cell>{Wind and, and rain!}</cell></row><row><cell>F11 Word n-grams</cell><cell>Overlapping word-fragments</cell><cell cols="2">Wind and rain!</cell><cell>{Wind and, and rain}</cell></row><row><cell>F12 Mix1</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,134.77,154.58,345.84,178.37"><head>Table 2 .</head><label>2</label><figDesc>by the PAN organizers on April 22, 2014. C PAN-14 consists of 695 problems (in total 2,382 documents), equally distributed regarding true/false authorships. A problem p i forms a tuple (D Ai , D Ai ? ), where D Ai denotes the training set of A i and D Ai ? the questioned document, which is (or not) written by A i . Each problem belongs to one of four languages (Greek, Spanish, Greek and Spanish and to one of four genres (Essays, Reviews, Novels and Articles). For simplification reasons, C PAN-14 is divided into six subcorpora and thus, can be formulated asC PAN-14 = {C DE , C DR , C EE , C EN , C GR , C SP }.This makes it easier to treat each subcorpus independently (e.g. in terms of parameterizations). The full name of each C ∈ C PAN-14 is given in Table2. Full names of all subcorpora within the CPAN-14 corpus.</figDesc><table /><note coords="7,186.03,313.73,235.56,7.86;7,186.03,325.08,241.06,7.86"><p><p>CDE: Dutch-Essays</p>CEE: English-Essays CGR: Greek-Articles CDR: Dutch-Reviews CEN: English-Novels CSP: Spanish-Articles</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,221.15,450.38,173.05,98.36"><head>Table 3 .</head><label>3</label><figDesc>Statistics of CPAN-Test. C |C| Distribution of true/false authorships CDE 20 9 true cases / 11 false cases CDR 20 11 true cases / 9 false cases CEE 40 2 true cases / 38 false cases CEN 20 10 true cases / 10 false cases CGR 20 11 true cases / 9 false cases CSP 20 10 true cases / 10 false cases</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="11,208.52,116.82,196.08,87.68"><head>Table 4 .</head><label>4</label><figDesc>Top five performing feature set combinations.</figDesc><table coords="11,244.74,137.79,125.87,66.71"><row><cell>Fcomb</cell><cell>Accuracy</cell></row><row><cell>{F1, F2, F5, F12, F14}</cell><cell>61, 8%</cell></row><row><cell>{F3, F12, F13}</cell><cell>61, 62%</cell></row><row><cell cols="2">{F1, F2, F5, F13, F14} 61, 62%</cell></row><row><cell cols="2">{F2, F5, F12, F13, F14} 61, 26%</cell></row><row><cell>{F1, F3, F12}</cell><cell>61, 26%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="11,181.43,301.21,250.24,99.03"><head>Table 5 .</head><label>5</label><figDesc>Most promising tuple for each individual training subcorpus.</figDesc><table coords="11,261.62,322.18,92.12,78.06"><row><cell cols="2">C (F i , λj) Accuracy</cell></row><row><cell cols="2">CDE (F12, 0.6) 73, 68%</cell></row><row><cell cols="2">CDR (F12, 0.8) 60, 76%</cell></row><row><cell>CEE (F12, 1)</cell><cell>63, 75%</cell></row><row><cell>CEN (F3, 0.6)</cell><cell>75%</cell></row><row><cell>CGR (F3, 0.2)</cell><cell>65%</cell></row><row><cell>CSP (F7, 1)</cell><cell>71, 25%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="12,134.77,116.82,345.84,145.51"><head>Table 6 .</head><label>6</label><figDesc>Results regarding CPAN-Test, using generalized parameters.</figDesc><table coords="12,134.77,137.79,345.84,124.54"><row><cell cols="2">C Accuracy AUC•c@1</cell></row><row><cell>CDE</cell><cell>80% 0, 40248</cell></row><row><cell>CDR</cell><cell>45% 0, 2445525</cell></row><row><cell>CEE</cell><cell>65% 0, 3147625</cell></row><row><cell>CEN</cell><cell>45% 0, 2309625</cell></row><row><cell>CGR</cell><cell>50% 0, 262025</cell></row><row><cell>CSP</cell><cell>55% 0, 2602875</cell></row><row><cell cols="2">also by an inappropriate calculation of ρ, performed through linear scaling. However,</cell></row><row><cell cols="2">further investigations with other scaling methods must show if this hypothesis is valid.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="12,134.77,335.47,345.84,234.96"><head>Table 7 .</head><label>7</label><figDesc>Results regarding CPAN-Test, using individual parameters.Evaluation results regarding runtime. In the third and last evaluation we applied the entire PAN corpus C PAN-14 = C PAN-Train ∪ C PAN-Test on VEBAV, where we considered only the runtime needed for the clasification, rather than the clasification results. Regarding this evaluation we used a laptop with the following configuration: Intel R Core</figDesc><table coords="12,134.77,356.44,345.84,213.99"><row><cell cols="3">C (F i , λj) Accuracy AUC•c@1</cell></row><row><cell>CDE (F12, 0.6)</cell><cell>80%</cell><cell>0, 40248</cell></row><row><cell>CDR (F12, 0.8)</cell><cell>55%</cell><cell>0, 30569</cell></row><row><cell>CEE (F12, 1)</cell><cell cols="2">55% 0, 25801125</cell></row><row><cell>CEN (F3, 0.6)</cell><cell>80%</cell><cell>0, 4146</cell></row><row><cell>CGR (F3, 0.2)</cell><cell cols="2">70% 0, 362425</cell></row><row><cell>CSP (F7, 1)</cell><cell cols="2">55% 0, 28072275</cell></row><row><cell cols="4">vidual parameter setting over a global setting is much more promising. Thus, individual</cell></row><row><cell cols="4">parameter settings should be (among other things) the subject for further investigations,</cell></row><row><cell cols="3">when applying VEBAV on other corpora beyond those of PAN.</cell></row><row><cell></cell><cell></cell><cell>TM</cell><cell>i5-</cell></row><row><cell cols="3">3360M processor (2.80 GHz), 16GB RAM, 256GB SSD hard drive.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="13,134.77,168.29,345.83,190.28"><head>Table 8 .</head><label>8</label><figDesc>Runtime needed by VEBAV for the classification of CPAN-14 (considering each subcorpus).</figDesc><table coords="13,208.39,198.05,200.55,160.52"><row><cell></cell><cell>CDE CDR</cell><cell>CEE</cell><cell>CEN</cell><cell>CGR</cell><cell cols="2">CSP Median</cell></row><row><cell>F1</cell><cell cols="6">1, 015 148 5, 361 6, 358 7, 279 7, 395 5, 860</cell></row><row><cell>F2</cell><cell cols="6">775 113 4, 083 5, 891 5, 761 5, 819 4, 922</cell></row><row><cell>F3</cell><cell>44 7</cell><cell>235</cell><cell>359</cell><cell>150</cell><cell>307</cell><cell>193</cell></row><row><cell>F4</cell><cell>55 6</cell><cell>358</cell><cell>623</cell><cell>445</cell><cell>502</cell><cell>402</cell></row><row><cell>F5</cell><cell>98 6</cell><cell>396</cell><cell>675</cell><cell>470</cell><cell>569</cell><cell>433</cell></row><row><cell>F6</cell><cell cols="6">356 17 2, 083 2, 963 3, 019 4, 457 2, 523</cell></row><row><cell>F7</cell><cell cols="6">532 29 3, 349 4, 612 4, 512 6, 942 3, 931</cell></row><row><cell>F8</cell><cell>67 8</cell><cell>402</cell><cell>428</cell><cell>371</cell><cell>530</cell><cell>387</cell></row><row><cell>F9</cell><cell>70 8</cell><cell>449</cell><cell>478</cell><cell>438</cell><cell>582</cell><cell>444</cell></row><row><cell>F10</cell><cell>53 10</cell><cell>253</cell><cell>283</cell><cell>253</cell><cell>328</cell><cell>253</cell></row><row><cell>F11</cell><cell>95 18</cell><cell>458</cell><cell>521</cell><cell>481</cell><cell>583</cell><cell>470</cell></row><row><cell>F12</cell><cell cols="6">1, 529 178 8, 505 11, 554 12, 383 14, 821 10, 030</cell></row><row><cell>F13</cell><cell cols="6">2, 035 287 11, 082 13, 476 16, 369 15, 817 12, 279</cell></row><row><cell>F14</cell><cell cols="6">269 26 1, 698 2, 255 1, 856 2, 280 1, 777</cell></row><row><cell>Median</cell><cell cols="5">184 18 1, 078 1, 465 1, 169 1, 432</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,144.73,658.08,295.36,7.77"><p>This was in fact the case, as can be observed in the later experiments (section 5.3).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="7,144.73,647.12,335.86,7.77;7,144.73,658.08,23.16,7.77"><p>The corpus can be downloaded from: http://pan.webis.de (accessed on June 26, 2014).</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This work was supported by the <rs type="funder">CASED Center for Advanced Security Research Darmstadt, Germany</rs> funded by the <rs type="funder">German state government of Hesse</rs> under the <rs type="programName">LOEWE program</rs> (www.CASED.de).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Az9uJ7d">
					<orgName type="program" subtype="full">LOEWE program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="14,138.13,523.57,318.75,7.77;14,146.47,534.53,320.01,7.77;14,146.47,545.49,110.56,7.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="14,186.07,523.57,270.81,7.77;14,146.47,534.53,60.20,7.77">Comprehensive survey on distance/similarity measures between probability density functions</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">H</forename><surname>Cha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,212.47,534.53,254.01,7.77;14,146.47,545.49,31.37,7.77">International Journal of Mathematical Models and Methods in Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="300" to="307" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,138.13,555.46,321.97,7.77;14,146.47,566.42,331.46,7.77;14,146.47,577.38,73.07,7.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="14,236.88,555.46,201.00,7.77">Overview of the Author Identification Task at PAN 2013</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Juola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,282.61,566.42,195.32,7.77;14,146.47,577.38,46.93,7.77">CLEF 2013 Evaluation Labs and Workshop -Working Notes Papers</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Navigli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Tufis</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,138.13,587.35,329.18,7.77;14,146.47,598.31,334.11,7.77;14,146.47,609.27,142.20,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="14,229.22,587.35,221.80,7.77">Authorship Verification as a One-Class Classification Problem</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,146.47,598.31,274.58,7.77;14,458.17,598.31,22.42,7.77;14,146.47,609.27,10.64,7.77">Proceedings of the twenty-first international conference on Machine learning</title>
		<meeting>the twenty-first international conference on Machine learning<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">62</biblScope>
		</imprint>
	</monogr>
	<note>ICML &apos;04</note>
</biblStruct>

<biblStruct coords="14,138.13,619.24,333.89,7.77;14,146.47,630.19,131.84,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="14,202.75,619.24,190.35,7.77">A Survey of Modern Authorship Attribution Methods</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,399.55,619.24,72.47,7.77;14,146.47,630.19,31.00,7.77">J. Am. Soc. Inf. Sci. Technol</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="538" to="556" />
			<date type="published" when="2009-03">Mar 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,138.13,640.16,278.56,7.77;14,146.47,651.12,257.57,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="14,193.56,640.16,87.25,7.77">One-Class Classification</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M J</forename><surname>Tax</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,286.46,640.16,130.22,7.77;14,146.47,651.12,65.22,7.77">Concept Learning In the Absence of Counter-Examples</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>Delft University of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
