<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,158.60,146.10,289.55,18.75;1,265.70,163.50,64.16,18.75;1,211.40,184.70,172.58,14.06">Proof of Concept Framework for Author Profiling Notebook for PAN at CLEF 2014</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,259.00,223.09,88.77,11.07"><forename type="first">Christopher</forename><forename type="middle">Ian</forename><surname>Baker</surname></persName>
							<email>pan@ctac.me.uk</email>
						</author>
						<title level="a" type="main" coord="1,158.60,146.10,289.55,18.75;1,265.70,163.50,64.16,18.75;1,211.40,184.70,172.58,14.06">Proof of Concept Framework for Author Profiling Notebook for PAN at CLEF 2014</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">056448B13BC4E7D5FE5C8B313BA7FF40</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A proof-of-concept basic prediction framework able to read the PAN author profiling data and be adaptable to accept multiple classification and training functions. The framework was used to investigate system resource usage and to experiment with data sub-setting techniques to enable efficient creation of the base model and convergence of the training functions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The PAN <ref type="bibr" coords="1,165.75,396.89,11.62,11.07" target="#b0">[1]</ref> author profiling task <ref type="bibr" coords="1,264.81,396.89,11.63,11.07" target="#b1">[2]</ref> is designed to evaluate algorithms that can read text from blogs, twitter, social media and on-line reviews and classify the author's gender and age bracket. The task provided a training corpus in two languages, English and Spanish, comprising a total of 7 categories together with a defined execution and output specification that would be used in an automated testing environment (TIRA). The objective was to achieve the highest success rate in predicting age group and gender on an undisclosed test corpus, "corpus2".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>The project broke down into 4 main sub-tasks:</p><p>1. Load text from supplied XML whilst dealing gracefully with bad data rejection; 2. Design and creation of data structures for internal representation of the training corpus; 3. Optimization of use of machine resources using data sub-setting; 4. Iterative refinement of the prediction model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Development of the Framework</head><p>The algorithm was developed using the Perl programming language with functions, particularly for reading XML, from the Comprehensive Perl Archive Network (CPAN) <ref type="bibr" coords="2,160.68,193.49,10.63,11.07" target="#b2">[3]</ref>.</p><p>A separate dictionary was created for each of the seven input media types. Each XML data frame had the text extracted and sanitised (extracted from HTML, case converted, invalid character removal, multiple white space suppression etc.). This clean text was then tokenised using 4 token extraction functions: single words, word pairs, word triples and meta tokens based on other text features. The meta token function attempts to extract information based on language features such as:</p><p> The ratio of white space to non-white space, indicative of average word length;  The ratio of punctuation to text;  The ratio of numeric data in the text;  The ratio of capital letters to lower case.</p><p>Each of these categories had a scaling factor assigned by manual observation to ensure the data was collected and spread across a small number of token buckets, approximately 10 each in this instance. During text load, each time a token was found it was added to the dictionary and a running count of the gender and age group of the text was kept with the token in the dictionary. Each time the total dictionary memory size passed 500MB the dictionary was pruned to the 20,000 most frequent tokens in each of the 4 token function categories. This typically reduced the memory requirement to under 100MB. Once the full dictionary load was complete each dictionary was pruned to the 3,000 most frequent tokens in each of the 4 token function categories.</p><p>The base counts for each token token were then converted to a frequency for each of the 7 input type categories being measured (2 gender, 5 age groups) based on the frequency of that token in the corpus relative to the total number of texts of that type. E.g. If we load a total of m texts with the "male" gender attribute, and the word "w" has a gender/male hit count of h, then the male frequency for w is h/m. The frequencies for each word/gender and word/age group were then converted to model weighting factors by normalizing so that they sum to 1.0. E.g. for word "w": weight(male) = frequency(male) / (frequency(male)+frequency(female)) Some early tests were conducted with a simple "majority wins" weighting rather than a proportional weighting but initial testing suggested this was less effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scoring a Text</head><p>A text was scored by loading, tokenising, and accumulating these proportional weights for each token matched in the dictionaries. Further, a multiplicative second weighting factor was generated by counting the frequency of hits in each of the 7 dictionaries and using that as a bias to create a dynamic dictionary selector function. The highest accumulated score for gender and for age group were the selected prediction for that text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scoring a Corpus Document</head><p>Each test corpus document could have one or many texts. The document had its attributes predicted by running each text in the document through this weighting algorithm and then picking the most frequently predicted gender and age group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Refinement</head><p>It was recognized that the prediction function is fundamentally frequency based and the more hits a token has the higher the contribution to accumulation scoring. Given that there may be tokens which, while of relatively low frequency, are strong selectors for gender or age group, the code was designed so that an iterative post processing phase could hunt for these "key deciders" and adjust their weights so they would become dominant in the prediction process. The refinement process follows the same basic process:</p><p> Load the existing model;  Score the test corpus as though it were new data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Iterate as follows:</head><p> Change some of the weighting factors using the selected refinement plug-in;  Re-score the test corpus with the new weighting factors;  Keep the highest scoring set of weighting factors, new or previous and repeat.</p><p>Three different refiners were tested:  Random variation (within a defined scope) of weighting factors;  Negative reinforcement -adjust each of the found token weights towards truth for every mis-identified text;  Positive and negative reinforcement -adjust each of the found token weights towards truth for every text both correctly and incorrectly predicted.</p><p>For useful improvement it was required that the refiner ran hundreds to thousands of iterations. To speed up the process the corpus was sub-setted. Each text in the corpus was sequentially numbered and then the refiner used a "modulus selector" to break the corpus into 1% interleaved slices and refine on a subset percentage "p" of the test corpus. Assuming a text with sequence number "s", the text could be processed if:</p><p>s mod 100 == p Selects a particular slice of the corpus s mod 100 &lt; p Selects multiple percent slices Shortly before final submission, a bug was identified in the text scorer that meant the model that had been refined had numeric integrity issues and there wasn't time to retrain so the version of the model submitted to TIRA for evaluation was the base statistical model with no iterative refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimisation</head><p>Machine resource limitations were a key factor in designing the algorithm. Corpus tokenisation was a significant processing load and too expensive to run from scratch for each iteration of the refiner. The whole tokenised training corpus could be stored internally at a memory cost of around 10GB, but as this was a proof of concept designed to investigate resource issues a token cache was used that could store tokenised percentage "p" corpus slices and be flushed any time it's memory usage got too high. The refiner was modified to train on the same "p" slice for n iterations (typically n=3 to 10) before moving onto a new "p" slice getting re-use from the token cache and speeding processing up significantly. Similarly, the model was split into 7 dictionaries for data management reasons. As each text was tokenised and stored in the dictionary we found dictionary processing started to slow down noticeably above 500MB dictionary memory load (circa 1 million total tokens) and as 1 GB was approached load speed slowed to a crawl.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>The results from TIRA of the previously unseen "corpus2" tests are shown below. There were 5 age groups, so random chance was 0.2000 for age group prediction. There were 2 gender categories, so random chance was 0.5000 for gender. The age classifier had some reasonable success, though far from the results of the best, but the gender predictor was little better than tossing a coin. This was not a surprise as this had been observed in testing with corpus1. Prior experimentation showed that iterative refinement had improved both categories but it was necessary to submit what was available at the deadline and there was never any expectation this test-bed would be class competitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Future Work</head><p>This project was developed from scratch without reference to existing research material or any particular subject matter experience. This was deliberate for the purposes of focusing on the proof-of-concept work and thinking through the problem from scratch. Having reached this point, I would consider the following for future work:</p><p> Run the refiner and see how much better a refined model would have done over the base statistics model submitted;  Experiment further with plug-in classifier and refiner functions;  Research existing literature to discover best practice and current state-of-the-art before deciding whether to adapt this existing framework or start from scratch for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,124.90,147.89,345.39,206.47"><head>Table 1 .</head><label>1</label><figDesc>Results from TIRA assessment on unseen data corpus2.Based on the published results, this algorithm was not as successful as hoped, but as a proof of concept and test-bed it served its purpose well.</figDesc><table coords="5,139.00,174.19,316.71,142.67"><row><cell>Test Corpus:</cell><cell>Age Group</cell><cell>Gender</cell></row><row><cell>pan14-author-profiling-test-</cell><cell></cell><cell></cell></row><row><cell>corpus2-english-blogs-2014-05-15</cell><cell>0.2949</cell><cell>0.5000</cell></row><row><cell>corpus2-english-reviews-2014-05-15</cell><cell>0.2594</cell><cell>0.5292</cell></row><row><cell>corpus2-english-socialmedia-2014-05-15</cell><cell>0.2494</cell><cell>0.5012</cell></row><row><cell>corpus2-english-twitter-2014-05-15</cell><cell>0.3377</cell><cell>0.5065</cell></row><row><cell>corpus2-spanish-blogs-2014-05-15</cell><cell>0.4464</cell><cell>0.5000</cell></row><row><cell>corpus2-spanish-socialmedia-2014-05-15</cell><cell>0.3445</cell><cell>0.5000</cell></row><row><cell>corpus2-spanish-twitter-2014-05-15</cell><cell>0.4889</cell><cell>0.5000</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,128.65,175.69,336.57,11.07;6,141.90,187.19,321.83,11.07;6,141.90,198.79,321.27,11.07;6,141.90,210.29,328.39,11.07;6,141.90,221.89,328.29,11.07;6,141.90,233.39,324.21,11.07;6,141.90,244.99,215.71,11.07" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,141.90,198.79,266.15,11.07">Recent Trends in Digital Text Forensics and its Evaluation</title>
		<author>
			<persName coords=""><forename type="first">Pan: Tim</forename><surname>Tira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anna</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthias</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Francisco</forename><surname>Busse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paolo</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Efstathios</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,174.16,221.89,296.03,11.07;6,141.90,233.39,320.26,11.07">Information Access Evaluation meets Multilinguality, Multimodality, and Visualization. 4th International Conference of the CLEF Initiative (CLEF 13)</title>
		<editor>
			<persName><forename type="first">Pamela</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roberto</forename><surname>Paredes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013-09">September 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,128.65,256.49,334.01,11.07;6,141.90,268.09,320.60,11.07;6,141.90,279.59,325.58,11.07;6,141.90,291.19,319.78,11.07;6,141.90,302.69,176.32,11.07" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,352.16,268.09,110.34,11.07;6,141.90,279.59,109.53,11.07">Overview of the Author Profiling Task at PAN 2013</title>
		<author>
			<persName coords=""><forename type="first">Francisco</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Moshe</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Efstathios</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Giacomo</forename><surname>Inches</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,179.85,291.19,277.47,11.07">Working Notes Papers of the CLEF 2013 Evaluation Labs</title>
		<editor>
			<persName><forename type="first">Pamela</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Dan</forename><surname>Tufis</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2013-09">September 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,128.65,314.29,257.07,11.07" xml:id="b2">
	<monogr>
		<ptr target="http://www.cpan.org/" />
		<title level="m" coord="6,141.90,314.29,151.24,11.07">Comprehensive Perl Archive Network</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
