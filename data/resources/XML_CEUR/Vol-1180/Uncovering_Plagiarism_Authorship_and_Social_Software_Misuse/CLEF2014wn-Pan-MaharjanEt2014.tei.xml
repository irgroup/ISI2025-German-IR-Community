<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,140.90,115.90,333.56,12.90;1,223.43,136.31,168.49,10.75">A Simple Approach to Author Profiling in MapReduce Notebook for PAN at CLEF 2014</title>
				<funder ref="#_JWrv4x7">
					<orgName type="full">The Office of Naval Research</orgName>
				</funder>
				<funder ref="#_CXaZxHg">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,200.32,173.26,59.67,8.64"><forename type="first">Suraj</forename><surname>Maharjan</surname></persName>
						</author>
						<author>
							<persName coords="1,267.00,173.26,61.77,8.64"><forename type="first">Prasha</forename><surname>Shrestha</surname></persName>
							<email>prasha@cis.uab.edu</email>
						</author>
						<author>
							<persName coords="1,352.23,173.26,62.82,8.64"><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
							<email>solorio@cis.uab.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Birmingham</orgName>
								<orgName type="institution">University of Alabama</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer and Information Sciences</orgName>
								<orgName type="institution">Campbell Hall</orgName>
								<address>
									<addrLine>1300 University Boulevard</addrLine>
									<postCode>35294-1170</postCode>
									<settlement>Birmingham</settlement>
									<region>Alabama</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,140.90,115.90,333.56,12.90;1,223.43,136.31,168.49,10.75">A Simple Approach to Author Profiling in MapReduce Notebook for PAN at CLEF 2014</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5EF079A4B5073A8ECBC71189E3EC58DF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Author profiling, being an important problem in forensics, security, marketing, and literary research, needs to be accurate. With massive amounts of online text readily available on which we might need to perform author profiling, building a fast system is as important as building an accurate system, but this can be challenging. However, the use of distributive computing techniques like MapReduce can significantly lower processing time by distributing tasks across multiple machines. Our system uses MapReduce programming paradigm for most parts of the training process, which makes our system fast. Our system uses word n-grams including stopwords, punctuations and emoticons as features and TF-IDF (term frequency inverse document frequency) as the weighing scheme. These are fed to the logistic regression classifier that predicts the age and gender of the authors. We were able to obtain a competitive accuracy in most categories and even obtained winning accuracy for two of the categories each in both test corpus 1 and test corpus 2.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The process of identifying age-group, gender, native language, personality and other aspects that constitute the profile of an author, by analyzing his/her writings is called author profiling. Since most of the text is now online and written behind a curtain of anonymity, author profiling has become a very important problem. In fields like forensic, security, literary research and marketing, finding out an author's demographic and personality information has proven to be very helpful. In literary research, author profiling helps to resolve disputed authorship for unknown documents. In forensics, author profiling can be used to shortlist potential suspects, given a piece of writing from them. In marketing, ad campaigns can be directed towards the demographic of users that review certain products the most.</p><p>The PAN'14 <ref type="bibr" coords="1,203.26,608.62,11.62,8.64" target="#b0">[1]</ref> author profiling task requires us to predict the author's age-group and gender. The PAN'14 corpus contains data collected from authors' writings in English or Spanish. The data has been divided into different categories according to the source of the data. For Spanish there are three categories: blogs, social media and twitter. Whereas for English, there is one more category along with those three, namely reviews. The task is to create a system that can predict an author's age and gender for all these categories when we are given their writings.</p><p>Prior work has tackled the task of author profiling by employing a range of lexical, stylistic, syntactic and readability measures. Schwartz et al. <ref type="bibr" coords="2,370.45,155.23,16.60,8.64" target="#b9">[10]</ref> used n-grams and LDA topic features to profile gender and five personality traits on Facebook user's data and obtained 91.9% accuracy. Burger et al. <ref type="bibr" coords="2,290.44,179.14,11.62,8.64" target="#b1">[2]</ref> used word and character n-grams along with Twitter user profile (full name, screen name and description) as features to determine the gender of the users. They experimented with Naive Bayes, LIBSVM and balanced Winnow2 classification algorithms and found that Winnow was better in both accuracy and speed. Likewise, Estival et al. <ref type="bibr" coords="2,271.88,226.96,10.58,8.64" target="#b2">[3]</ref>, in addition to age and gender prediction, tried to predict the first language and country of an author. They experimented with different classification algorithms like SVM using SMO, Random forest and rule based learners and concluded that SMO performed the best for both age and gender.</p><p>In this paper, we have experimented with word and character n-grams as features. We also analysed the use of five different classification algorithms viz Naive Bayes, Cosine Similarity, Weighted Cosine Similarity, LIBLINEAR <ref type="bibr" coords="2,367.13,298.74,11.62,8.64" target="#b3">[4]</ref> Logistic Regression with L2 regularization, and LIBLINEAR SVM with linear kernel. Since feature computation as well as classification can be very slow, we have implemented most our system in MapReduce. All of the feature computation has been implemented in MapReduce, which make our system very fast. We also implemented Naive Bayes, and weighted as well as non-weighted Cosine Similarity in MapReduce. Our code is available for use and extension for people who want to make use of the computing prowess of a distributed system without having to go into much detail about MapReduce.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Most of our training takes place in Hadoop and uses MapReduce. For our training process, we started by randomly dividing the available data into training and cross validation dataset in a ratio of 70:30. The training data was then preprocessed to filter out all the HTML and XML tags. The plain text files thus obtained were then combined into sequence files because MapReduce jobs run faster when we use a small number of large files rather than when using a large number of small files. We then used MapReduce jobs to compute feature vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Features Vector Creation</head><p>Since most of the training data was from some sort of social media, we used Ark-Tweet-NLP tokenizer <ref type="bibr" coords="2,222.55,572.70,11.62,8.64" target="#b4">[5]</ref> to tokenize the document because it is well adapted for online text. We retained all the stopwords, punctuations and emoticons as these are important features for the author profiling task. Since any MapReduce job requires key and value, the tokenizer job uses the filename with all the class information as the key and the file content as its value. After tokenizing with Ark-Tweet-NLP, this job generates the necessary n-grams.</p><p>After obtaining the n-grams, we compute the inverse document frequency (IDF) count for each token. Based on these counts, we filter out the tokens that have not even been used by at least two authors. We have an idf MapReduce job to compute the idf. The mapper computes the partial count of each individual ngram and passes them to the reducer, which sums these partial counts to produce the final idf counts. The filter job takes the idf counts and a threshold and filters out all the tokens whose idf score is less than the threshold. Also, this job creates a dictionary file that contains all the unique tokens mapped to integer ids. After the tokens are filtered out, our TF-IDF vector creation job takes in the idf counts and dictionary file to compute the tf-idf scores for the remaining tokens. This map only job outputs a tf-idf vector for each document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Training</head><p>For training, we tried five different classification algorithms. For Naive Bayes, cosine similarity, and weighted cosine similarity, we created another MapReduce job that computes all the statistics needed for classification. The weighted cosine similarity algorithm multiplies the cosine similarity score by prior probability score for each class. The mapper emits class label as key and tf-idf vector as value. Instead of building separate models for age groups and gender, we considered the problem as a 10-class problem by combining the age and gender classes. The class labels were extracted from the document names and were mapped to unique integer ids. The mapper also emits a special key -1 and a vector that contains partial counts of number of documents with that class label. We used VectorSumReducer provided by Mahout as reducer, which groups vectors by their class id and sums them. For both logistic regression and SVM, we first transformed the tf-idf score vectors into a format as expected by LIBLINEAR. Then we trained on these feature vectors by using the LIBLINEAR command utility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Testing</head><p>The PAN'14 shared task organizers provided us with a Virtual Machine(VM) with 4GB of memory. We could have deployed Hadoop in pseudo distributive mode and ran the MapReduce version of our application for testing. However, it would not have given us any advantage, as the process would run on just that machine, which did not even have multiple cores. Running the tests on Hadoop would thus only add overhead and would make our system slower. So, for testing, we created a normal java application (not MapReduce), that would read the trained models and predict class labels for the test documents. For testing, we need to create test vectors similar to trained vectors. Hence, we applied the same steps as we did for the training data. After the test document was preprocessed to removed HTML tags and tokenized using Ark-Tweet-NLP tokenizer, we generated word or char n-grams by using Lucene<ref type="foot" coords="3,346.86,559.61,3.49,6.05" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>We setup a local Hadoop 2 cluster with a master node and 7 slave nodes, each node having 16 cores and 12GB memory. We are running Hadoop version 1.0.4 and Mahout version 0.7. Since the data is large, MapReduce is ideal for feature extraction from this data. We were able to finish training in a short amount of time even though the data is large. We were also able to train five different models because we did not need to spend a lot of time for feature extraction. In order to find the best model, we tried different classification algorithms and also compared the use of word vs character n-grams. We also performed experiments to find out if building separate models for different categories: blogs, social media, twitter and reviews produce better results than building a single, combined model for all categories. The test was performed in cross validation dataset, which obtained by randomly separating 30% of the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Separate Word N-gram Models for Each Category</head><p>Table <ref type="table" coords="4,159.27,275.69,4.98,8.64" target="#tab_0">1</ref> shows the result for word ngram experiments (unigrams, bigrams and trigrams combined) for cross validation set. Here, we built separate models for the different categories and different languages. So, we ended up with 14 models total. The results show that different classifiers perform better for different categories. Also, if we are to choose a classifier, either Naive Bayes or logistic regression seems to be more promising across different categories. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Separate Character N-gram Models for each Category</head><p>Table <ref type="table" coords="4,161.35,560.80,4.98,8.64" target="#tab_1">2</ref> shows the results for character ngram experiments (bigrams and trigrams combined) for the cross validation set. We again built separate models for different categories and different languages. The results show that word ngrams are better features than character ngrams. In all experiments, we found that the word ngram method beats the character ngram method. One possible reason for this might be that we just considered character bigrams and trigrams. If we had considered higher character n-grams, we might have achieved similar results for both experiments. In addition, different people may have used different spelling for same words when using social media. This might have caused character n-gram not be able to capture the style of authors. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Single Word N-gram Model for all Categories</head><p>Since we already figured out that word n-grams are better at predicting the profile of an author, we decided to use them as features rather than character n-grams. The next decision we needed to make was either to build a separate model for each category or to build a single, combined model for all the categories. Since we had already run experiments for separate models, we built a single combined model next. But we still built two separate models for English and Spanish. Table <ref type="table" coords="5,367.94,336.50,4.98,8.64" target="#tab_2">3</ref> tabulates the accuracy obtained by running combined model and separate model for the cross validation set. It is clear from the table that having a single model is better than using separate ones. This might be because since all the data has been obtained from some sort of social media, authors might use similar writing styles across all of them. So, having a model trained on all categories or genres performs better than when separate models are trained on each category. We also found logistic regression to be the best classification algorithms for this task. So, our final model uses word n-grams trained on data from all categories as features and logistic regression as the classification algorithm. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Test Results</head><p>Table <ref type="table" coords="5,159.03,632.53,4.98,8.64" target="#tab_3">4</ref> shows accuracy along with runtimes when our final system was run on test corpus 1 and 2 for both languages. Despite using simple word n-gram features, our system achieved competitive results both in terms of accuracy and speed. We ranked first in spanish-socialmedia and spanish-twitter for test corpus 1 and in english-socialmedia and spanish-twitter for test corpus 2. We were ranked in the top three for most of the other categories as well. This might even indicate that some problems are better suited by simple solutions. For english-socialmedia, which has largest number of test documents, we ranked second in runtime for both test corpora. Our system took 26 minutes to complete the classification. Whereas, some of the other participants took from around 30 to 69 hours to complete the testing. Although even for other test corpora, our runtime performances are quite fast, we could not obtain equally high ranking because we have a huge model that takes a lot of time to load. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">PAN'13 Result</head><p>Since PAN'13 <ref type="bibr" coords="6,195.42,465.15,11.62,8.64" target="#b7">[8]</ref> had nearly double the amount of data than PAN'14, we also tested our system on PAN'13 data to see how we do in terms of both accuracy and speed. Table <ref type="table" coords="6,160.53,489.06,4.98,8.64" target="#tab_4">5</ref> shows the accuracy obtained when we ran our system was on PAN'13 test dataset. Here, we used word n-grams (unigrams, bigrams and trigrams combined) as features, TF-IDF as weighing scheme and Naive Bayes for classification. We had nearly 3 million features for English and Spanish languages. We ran everything from training to testing on our local Hadoop cluster. For English, we obtained better accuracy than that obtained by the contestants of the PAN'13 competition. For Spanish, our accuracy was only lower than that of two of the contestants. For Spanish dataset, when we trained a logistic regression model with L2 regularization with the same features and obtained an accuracy of 44.28% which was higher than that those in the competition. But this was not done with MapReduce. Also we were able to train and test nearly 2.4 GB of data in just 72.12 minutes, which when compared with PAN'13 participants' test runtimes, is a lot less. The total testing time is 2.86 minutes which is faster than the fastest PAN'13 system which took 10.26 minutes to run. These accuracy and runtimes show that our system performs better in both fronts when compared to the systems in the PAN'13 author profiling task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion and Conclusion</head><p>In the end, we were able to produce a system that performs the author profiling task with good accuracy. We also observed that analyzing word usage seems to be promising for this task. But character n-grams were not as good of features. This might be because people tend to use their own version of spelling for words especially when the writing is informal as in this dataset. Also, stopwords, punctuation and emoticons proved to be predictive features as well. The fact that we produced good results even with such simple features might indicate that some problems are better suited for simple solutions. Due to our model's load time, we were not able to obtain very high rankings for runtime. But this will not be of concern in a practical system because we need to load the model only once. Also, all of the systems in the competition are likely to be supervised classification systems and all of them must have trained a model, which takes a lot more time than testing. But, there is no mention of the training runtimes so we cannot make a comparison. But, since we used MapReduce for feature extraction, our training time was significantly shortened. So, our system is very likely to have less training runtime.</p><p>Because of MapReduce, we were able to play with different threshold parameter settings in a reasonable amount of time and thus we can say that MapReduce is ideal for the task of feature extraction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,136.16,368.96,360.06,104.26"><head>Table 1 .</head><label>1</label><figDesc>Accuracy for word n-grams for cross validation dataset.</figDesc><table coords="4,136.16,398.95,360.06,74.28"><row><cell></cell><cell></cell><cell cols="2">English (%)</cell><cell>Spanish (%)</cell></row><row><cell>Classification Algorithm</cell><cell cols="4">Blog Reviews Social Media Twitter Blog Social Media Twitter</cell></row><row><cell>Naive Bayes</cell><cell>27.50</cell><cell>21.55</cell><cell>20.62 28.89 55.00</cell><cell>20.48 34.78</cell></row><row><cell>Cosine Similarity</cell><cell>20.00</cell><cell>23.64</cell><cell>19.72 27.78 35.00</cell><cell>26.33 36.96</cell></row><row><cell>Weighted Cosine Similarity</cell><cell>30.00</cell><cell>23.16</cell><cell>19.97 26.67 40.00</cell><cell>22.07 32.61</cell></row><row><cell>Logistic Regression</cell><cell>27.50</cell><cell>23.08</cell><cell>20.62 33.33 35.00</cell><cell>25.80 32.61</cell></row><row><cell>SVM</cell><cell>25.00</cell><cell>22.28</cell><cell>19.80 32.22 30.00</cell><cell>26.33 34.78</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,136.16,115.83,360.06,104.26"><head>Table 2 .</head><label>2</label><figDesc>Accuracy for character n-grams for cross validation dataset.</figDesc><table coords="5,136.16,145.81,360.06,74.28"><row><cell></cell><cell></cell><cell cols="2">English (%)</cell><cell>Spanish (%)</cell></row><row><cell>Classification Algorithm</cell><cell cols="4">Blog Reviews Social Media Twitter Blog Social Media Twitter</cell></row><row><cell>Naive Bayes</cell><cell>25.00</cell><cell>18.99</cell><cell>18.33 24.44 40.00</cell><cell>19.68 23.91</cell></row><row><cell>Cosine Similarity</cell><cell>20.00</cell><cell>21.63</cell><cell>17.90 30.00 50.00</cell><cell>21.81 26.09</cell></row><row><cell>Weighted Cosine Similarity</cell><cell>20.00</cell><cell>21.15</cell><cell>16.78 23.33 40.00</cell><cell>19.68 28.26</cell></row><row><cell>Logistic Regression</cell><cell>22.50</cell><cell>21.71</cell><cell>16.78 25.56 35.00</cell><cell>23.67 17.39</cell></row><row><cell>SVM</cell><cell>20.00</cell><cell>20.83</cell><cell>15.92 24.44 35.00</cell><cell>23.14 17.39</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,171.69,463.17,271.99,95.24"><head>Table 3 .</head><label>3</label><figDesc>Accuracy for separate and combined models.</figDesc><table coords="5,171.69,484.14,271.99,74.27"><row><cell></cell><cell cols="2">English (%)</cell><cell cols="2">Spanish (%)</cell></row><row><cell>Classification Algorithm</cell><cell cols="4">Separate Combined Separate Combined</cell></row><row><cell>Naive Bayes</cell><cell>21.21</cell><cell>20.13</cell><cell>23.53</cell><cell>21.04</cell></row><row><cell>Cosine Similarity</cell><cell>19.89</cell><cell>17.34</cell><cell>27.83</cell><cell>27.6</cell></row><row><cell>Weighted Cosine Similarity</cell><cell>21.32</cell><cell>18.18</cell><cell>23.98</cell><cell>24.89</cell></row><row><cell>Logistic Regression</cell><cell>21.83</cell><cell>21.92</cell><cell>26.92</cell><cell>28.96</cell></row><row><cell>SVM</cell><cell>20.99</cell><cell>20.48</cell><cell>27.37</cell><cell>28.05</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,149.80,250.23,315.76,117.56"><head>Table 4 .</head><label>4</label><figDesc>Accuracy on test corpus.</figDesc><table coords="6,149.80,271.20,315.76,96.59"><row><cell></cell><cell></cell><cell>Test 1</cell><cell>Test 2</cell></row><row><cell cols="2">Language Category</cell><cell>Both Age Gender Runtime Both Age Gender Runtime</cell></row><row><cell></cell><cell>Blog</cell><cell>16.67 25.00 54.17 00:01:50 23.08 38.46 57.69 00:01:56</cell></row><row><cell>English</cell><cell cols="2">Reviews Social Media 20.09 36.27 53.32 00:07:18 20.62 36.52 53.82 00:26:31 20.12 28.05 62.80 00:01:46 22.23 33.31 66.87 00:02:13</cell></row><row><cell></cell><cell>Twitter</cell><cell>40.00 43.33 73.33 00:02:01 30.52 44.16 66.88 00:02:31</cell></row><row><cell></cell><cell>Blog</cell><cell>28.57 42.86 57.14 00:00:35 25.00 46.43 42.86 00:00:39</cell></row><row><cell>Spanish</cell><cell cols="2">Social Media 30.33 40.16 68.03 00:01:13 28.45 42.76 64.49 00:03:26</cell></row><row><cell></cell><cell>Twitter</cell><cell>61.54 69.23 88.46 00:00:43 43.33 61.11 65.56 00:01:10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,199.15,115.83,217.06,104.77"><head>Table 5 .</head><label>5</label><figDesc>PAN'13 Results.</figDesc><table coords="7,352.04,134.96,53.53,8.06"><row><cell>Accuracy (%)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,144.73,645.94,88.49,7.77"><p>http://lucene.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,144.73,657.09,91.48,7.77"><p>http://hadoop.apache.org/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We want to thank PAN'14 organizers and committee members for organizing the author profiling task. This research was partially funded by <rs type="funder">The Office of Naval Research</rs> under grant <rs type="grantNumber">N00014-12-1-0217</rs> and <rs type="funder">National Science Foundation</rs> under grant <rs type="grantNumber">1350360</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_JWrv4x7">
					<idno type="grant-number">N00014-12-1-0217</idno>
				</org>
				<org type="funding" xml:id="_CXaZxHg">
					<idno type="grant-number">1350360</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,142.61,592.00,333.62,7.77;7,150.95,602.96,305.82,7.77;7,150.95,613.92,147.43,7.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,399.28,592.00,76.96,7.77;7,150.95,602.96,38.13,7.77">CLEF 2014 Labs and Workshops</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Halvey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-1180/" />
	</analytic>
	<monogr>
		<title level="s" coord="7,195.56,602.96,212.72,7.77">Notebook Papers. CEUR Workshop Proceedings (CEUR-WS</title>
		<idno type="ISSN">1613-0073</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,624.21,320.84,7.77;7,150.95,635.17,325.46,7.77;7,150.95,646.13,311.90,7.77;7,150.95,657.09,234.76,7.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,332.21,624.21,115.90,7.77">Discriminating gender on twitter</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zarrella</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2145432.2145568" />
	</analytic>
	<monogr>
		<title level="m" coord="7,150.95,635.17,308.27,7.77;7,195.78,646.13,43.27,7.77">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1301" to="1309" />
		</imprint>
	</monogr>
	<note>EMNLP &apos;11</note>
</biblStruct>

<biblStruct coords="8,142.61,119.96,318.96,7.77;8,150.95,130.92,303.42,7.77;8,150.95,141.88,170.86,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,390.34,119.96,71.23,7.77;8,150.95,130.92,50.13,7.77">Author profiling for english emails</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Estival</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gaustad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">B</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Hutchinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,219.19,130.92,235.18,7.77;8,150.95,141.88,94.13,7.77">Proceedings of the 10th Conference of the Pacific Association for Computational Linguistics</title>
		<meeting>the 10th Conference of the Pacific Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.61,152.84,320.95,7.77;8,150.95,163.80,311.09,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,366.20,152.84,97.36,7.77;8,150.95,163.80,88.45,7.77">LIBLINEAR: A library for large linear classification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><forename type="middle">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,244.92,163.80,139.43,7.77">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.61,174.76,326.89,7.77;8,150.95,185.72,314.56,7.77;8,150.95,196.67,321.59,7.77;8,150.95,207.63,321.08,7.77;8,150.95,218.59,313.81,7.77;8,150.95,229.55,214.59,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,298.56,185.72,166.96,7.77;8,150.95,196.67,90.15,7.77">Part-of-speech tagging for twitter: Annotation, features, and experiments</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2002736.2002747" />
	</analytic>
	<monogr>
		<title level="m" coord="8,259.38,196.67,213.17,7.77;8,150.95,207.63,276.91,7.77;8,191.30,218.59,29.49,7.77">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="42" to="47" />
		</imprint>
	</monogr>
	<note>HLT &apos;11</note>
</biblStruct>

<biblStruct coords="8,142.61,240.51,305.52,7.77;8,150.95,251.47,324.94,7.77;8,150.95,262.43,309.44,7.77;8,150.95,273.39,23.90,7.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,221.01,251.47,201.73,7.77">INAOE&apos;s participation at PAN&apos;13 : Author profiling task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>Lopez-Monroy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes-Y-Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">J</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Villasenor-Pineda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Villatoro-Tello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,440.53,251.47,35.36,7.77;8,150.95,262.43,204.08,7.77">Notebook Papers of CLEF 2013 LABs and Workshops, CLEF-2013</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-09">September (2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.61,284.35,319.44,7.77;8,150.95,295.30,311.55,7.77;8,150.95,306.26,309.44,7.77;8,150.95,317.22,23.90,7.77" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,150.95,295.30,258.34,7.77">Ensemble-based classification for author profiling using various features</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Meina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Brodzinska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Celmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Czoków</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Patera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pezacki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wilk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,427.14,295.30,35.36,7.77;8,150.95,306.26,204.08,7.77">Notebook Papers of CLEF 2013 LABs and Workshops, CLEF-2013</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-09">September (2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.61,328.18,312.02,7.77;8,150.95,339.14,255.63,7.77" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,371.58,328.18,83.05,7.77;8,150.95,339.14,71.48,7.77">Overview of the author profiling task at pan</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Inches</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="8,247.09,339.14,95.25,7.77">Notebook Papers of CLEF</title>
		<imprint>
			<biblScope unit="page" from="23" to="26" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.61,350.10,314.38,7.77;8,150.95,361.06,328.62,7.77;8,150.95,372.02,124.75,7.77" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,327.00,350.10,130.00,7.77;8,150.95,361.06,64.37,7.77">Author profiling: Predicting age and gender from blogs</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Santosh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,233.38,361.06,241.69,7.77">Notebook Papers of CLEF 2013 LABs and Workshops, CLEF-2013</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-09">September (2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.24,382.98,326.42,7.77;8,150.95,393.93,305.70,7.77;8,150.95,404.89,310.53,7.77;8,150.95,415.85,106.08,7.77" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,414.65,393.93,42.01,7.77;8,150.95,404.89,284.45,7.77">Personality, gender, and age in the language of social media: The open-vocabulary approach</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">A</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Eichstaedt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">L</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Dziurzynski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Ramones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kosinski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Stillwell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E P</forename><surname>Seligman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">H</forename><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,441.55,404.89,19.93,7.77;8,150.95,415.85,18.43,7.77">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">73791</biblScope>
			<date type="published" when="2013-09">09 2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
