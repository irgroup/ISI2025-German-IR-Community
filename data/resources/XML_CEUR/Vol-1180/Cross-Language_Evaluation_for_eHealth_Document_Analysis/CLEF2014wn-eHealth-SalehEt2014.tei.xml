<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,179.91,117.75,255.54,12.62;1,235.33,135.68,144.69,12.62">CUNI at the ShARe/CLEF eHealth Evaluation Lab 2014</title>
				<funder ref="#_qrEcCCx">
					<orgName type="full">EU</orgName>
				</funder>
				<funder ref="#_zhR3s72">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_KQrAS4M">
					<orgName type="full">Czech Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,243.20,173.64,50.92,8.74"><forename type="first">Shadi</forename><surname>Saleh</surname></persName>
							<email>saleh@ufal.mff.cuni.cz</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Formal and Applied Linguistics</orgName>
								<orgName type="institution">Charles University in</orgName>
								<address>
									<settlement>Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,316.82,173.64,55.35,8.74"><forename type="first">Pavel</forename><surname>Pecina</surname></persName>
							<email>pecina@ufal.mff.cuni.cz</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Formal and Applied Linguistics</orgName>
								<orgName type="institution">Charles University in</orgName>
								<address>
									<settlement>Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,179.91,117.75,255.54,12.62;1,235.33,135.68,144.69,12.62">CUNI at the ShARe/CLEF eHealth Evaluation Lab 2014</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0C66057F6389C2CE7D061D911CF7BC1B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>multilingual information retrieval</term>
					<term>data cleaning</term>
					<term>machine translation</term>
					<term>spelling correction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This report describes the participation of the team of Charles University in Prague at the ShARe/CLEF eHealth Evaluation Lab in 2014. We took part in Task 3 (User-Centered Health Information Retrieval) and its both subtasks (monolingual and multilingual retrieval). Our system was based on the Terrier platform and its implementation of the Hiemstra retrieval model. We experimented with several methods for data cleaning and automatic spelling correction of query terms. For data cleaning, the most effective method was to employ simple HTML markup removal. The more advanced cleaning methods which remove boilerplate decreased retrieval performance. Automatic correction of spelling errors performed on the English queries for the monolingual task proved to be efficient and leaded to our best P@10 score equal to 0.5360. In the multilingual retrieval task, we employed the Khresmoi medical translation system developed at the Charles University in Prague and translated the source queries from Czech, German, and French to English and employed the same retrieval system as for the monolingual task. The cross-lingual retrieval performance measured by P@10 relative to the scores obtained in the monolingual task ranged between 80% and 90% depending on the source language of the queries.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The digital medical content available on-line has grown rapidly in recent years. This increase has a potential to improve user experience with Web medical information retrieval (IR) systems which are more and more often used to consult users' health related issues. Recently, Fox <ref type="bibr" coords="1,313.76,597.05,10.52,8.74" target="#b1">[2]</ref> reported that about 80% of Internet users in the U.S. look for health information on-line and this number is expected to grow in future.</p><p>In this report, we describe our participation at the the ShARe/CLEF eHealth Evaluation Lab 2014, Task 3 <ref type="bibr" coords="1,266.36,645.16,10.52,8.74" target="#b2">[3]</ref> which focus on developing methods and data resources for evaluation of IR from the perspective of patients.</p><p>Our system is built on Terrier <ref type="bibr" coords="2,291.44,119.99,10.52,8.74" target="#b6">[7]</ref> and employs its implementation of the Hiemstra retrieval model. The main contribution of our participation is the examination of several methods for cleaning the document collection (provided as raw documents with HTML markup) and automatic correction of spelling errors in query terms and handling unknown words.</p><p>In the remainder of the paper, we review the task specification, describe the test collection and queries, our experiments, their results and conclude with the main findings of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task description</head><p>The goal of Task 3 in ShARe/CLEF eHealth Evaluation Lab 2014 is to design an IR system which returns a ranked list of medical documents (English web pages) from the provided test collection as a response to patients' queries. The task is split into two tasks:</p><p>-Task 3a is a standard TREC-style text IR task<ref type="foot" coords="2,354.06,319.10,3.97,6.12" target="#foot_0">1</ref> . The participants had to develop monolingual retrieval techniques for a set of English queries and return the top 1,000 relevant documents from the collection for each query. They could submit up to seven ranked runs: Run 1 as a baseline, Runs 2-4 for experiments exploiting discharge summaries provided for each query, and Runs 5-7 for experiments not using the discharge summaries (see Section 3.2). -Task 3b extends Task 3a by providing the queries in Czech, German, and French. The participants were asked to use these queries to retrieve relevant documents from the same collection as in Task 3a. They were allowed to submit up to seven ranked runs for each language using same restrictions as in Task 3a. No restrictions were put on techniques for translating the queries to English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Document Collection</head><p>The document collection for Task 3 consists of automatically crawled pages from various medical web sites, including pages certified by the Health On the Net<ref type="foot" coords="2,476.12,541.69,3.97,6.12" target="#foot_1">2</ref> and other well-known medical web sites and databases. The collection was provided by the Khresmoi project<ref type="foot" coords="2,271.13,565.60,3.97,6.12" target="#foot_2">3</ref> and covers a broad set of medical topics. For details, see <ref type="bibr" coords="2,185.25,579.13,9.96,8.74" target="#b4">[5]</ref>.</p><p>The collection contains a total of 1,104,298 web pages. We excluded a small portion of the pages because of no or un-readable content (382 pages contained a Flash-related error message, and 658 pages were unreadable binary files).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Queries</head><p>Two sets of queries were provided for Task 3 <ref type="bibr" coords="3,326.25,360.87,9.96,8.74" target="#b2">[3]</ref>. The training set of 5 queries and their matching relevance assessments and a test set of 50 queries for the main evaluation. All queries were provided in English (for Task 3a) and in Czech, German, and French (for Task 3b).</p><p>The English queries were constructed by medical professionals from the main disorder diagnosed in discharge summaries of real patients (i.e. documents containing a summary of important information from their entire hospitalization) provided for Task 2. Then, the queries were translated to Czech, German, and French by medical professionals and reviewed. Each query description consists of the following fields:</p><p>-title: text of the query, -description: longer description of what the query means, -narrative: expected content of the relevant documents, -profile: main information on the patient (age, gender, condition), -discharge summary: ID of the matching discharge summary.</p><p>An example of a query is given in Figure <ref type="figure" coords="3,322.32,556.52,4.98,8.74" target="#fig_0">1</ref> and some basic statistics associated with the query sets are shown in Table <ref type="table" coords="3,307.67,568.48,3.88,8.74" target="#tab_1">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">System description</head><p>Our system consists of three main components: a document processing component, a query processing component, and a search engine (see Figure <ref type="figure" coords="4,441.59,320.32,3.87,8.74">2</ref>). First, the collection is processed and data to be indexed is extracted from each document in the collection. Second, the search engine is employed to index the data. Third, each query is processed (eventually translated), enters the search engine which retrieves the top 1,000 ranked documents based on a retrieval model and its parameters.</p><p>The main evaluation metrics for Task 3 is precision at top 10 ranked documents (P@10), however, we also present results of other well known metrics implemented in the standard trec eval tool: <ref type="foot" coords="4,326.19,414.39,3.97,6.12" target="#foot_3">4</ref> such as precision at top 5 ranked documents (P@5), Normalized Discount Cumulative Gain at top 5 and 10 ranked documents (NDCG@5, NDCG@10), Mean Average Precision (MAP), precision after R documents have been retrieved where R is the number of known relevant documents (Rprec), binary preference (bpref), and the number of relevant documents (rel ret). In the remainder of this section, we describe our retrieval system in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Retrieval model</head><p>We employ Terrier 3.5 <ref type="bibr" coords="4,237.48,534.58,10.52,8.74" target="#b6">[7]</ref> as the search engine for indexing and retrieval. The retrieval model is the standard Hiemstra language model <ref type="bibr" coords="4,387.02,546.53,10.52,8.74" target="#b3">[4]</ref> as implemented in Terrier, where given a query Q and its terms Q = (t 1 , t 2 , . . . , t n ), each document D in a collection C is scored using the following formula:</p><formula xml:id="formula_0" coords="4,188.65,587.55,238.05,30.32">P (D, Q) = P (D) • n i=1 ((1 -λ i ) P (t i |C) + λ i P (t i |D)) ,</formula><p>where P (D) is the prior probability of D to be relevant estimated as by summing up frequencies of query terms in the document D over their frequencies in the collection C. P (t i |C) and P (t i |D) are probabilities of t i in the collection C and document D, respectively. They are estimated by maximum likelihood estimation using frequencies of the term t i in the collection C and document D, respectively. λ i is a linear interpolation coefficient reflecting the overall importance of the term t i . In our system, we used the same value λ for all the terms and tune it on the training query set by grid search to maximize MAP (see Figure <ref type="figure" coords="5,414.28,404.72,3.87,8.74" target="#fig_1">3</ref>). The highest MAP value was achieved with λ=0.087 which is used in all our experiments. After releasing the relevance assessments of the test queries, we measured the effect of λ on the test set performance and the results are shown in Figure <ref type="figure" coords="5,455.88,440.59,4.98,8.74" target="#fig_1">3</ref> too. The figure also contains test and training curves for P@10, the official measures for Task 3 in 2014, which was announced together with the evaluation results. We also perform Pseudo Relevance Feedback (PRF) implemented in Terrier as query expansion which modifies (expands) a given query by adding the most informative terms from top retrieved documents and performs the retrieval again with the expanded query. We use Terrier's implementation for Bo1 (Bose-Einstein 1) from the Divergence From Randomness framework <ref type="bibr" coords="5,406.57,524.28,9.96,8.74" target="#b0">[1]</ref>. We expanded each query by taking ten highest scored terms from three top ranked documents. These values achieved the best results measured on the training set, although they were lower than the results without PRF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Document processing</head><p>The documents in the collection are provided as raw web pages including all the HTML markup and eventually also CSS style definitions and Javascript code which should be removed before indexing. We employed three data cleaning methods and evaluate their effect on the retrieval quality measured on the training queries. First, we simply removed all markup, style definitions, and script code by the a Perl module HTML-Strip<ref type="foot" coords="6,270.02,350.79,3.97,6.12" target="#foot_4">5</ref> (but keep meta keywords and meta description tags). This reduces the total size of the collection from 41,628 MB to 6,821 MB, which is about 16% of the original size and and average document length is 911 tokens (words and punctuation marks).</p><p>Although the size reduction is very substantial, the resulting documents still contained a lot of noise (such as web page menus, navigation bars, various headers and footers), which is likely not to be relevant to the main content of the page. Such noise is often called boilerplate. We used two methods to remove it: Boilerpipe <ref type="bibr" coords="6,182.27,448.55,10.52,8.74" target="#b5">[6]</ref> reduced the total number of tokens in the collection by additional 58% (the average document length is 383 tokens) and JusText <ref type="bibr" coords="6,412.77,460.50,10.52,8.74" target="#b8">[9]</ref> by 55% (the average document length is 409 tokens).</p><p>More details from the data cleaning phase are provided in Table <ref type="table" coords="6,436.61,484.96,3.87,8.74" target="#tab_2">2</ref>. Table <ref type="table" coords="6,475.62,484.96,4.98,8.74" target="#tab_3">3</ref> then reports the IR results obtained by the Hiemstra model using the training set and the collection processed by the three methods compared with the case where no cleaning was performed at all. Surprisingly, the most effective method is the simple HTML-Strip tool. The two other methods are probably too aggressive and remove some relevant material important for IR. In all the following experiments, the collection is cleaned by HTML-Strip.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Query processing</head><p>For Task 3a, the queries entering the search engine of our system are constructed from the title and narrative description fields. For Task 3b, we translated the queries to English by the Khresmoi translator described in <ref type="bibr" coords="6,400.95,635.82,10.52,8.74" target="#b7">[8]</ref> which is tuned specifically to translate user queries from the medical domain <ref type="bibr" coords="7,405.80,220.96,14.61,8.74" target="#b9">[10]</ref>. For comparison, we also provide results obtained by translating the queries using on-line translators Google Translate<ref type="foot" coords="7,258.26,243.30,3.97,6.12" target="#foot_5">6</ref> and Bing Translator<ref type="foot" coords="7,353.11,243.30,3.97,6.12" target="#foot_6">7</ref> . In the baseline experiment, we take the title terms as they are. As an additional query processing step, we attempt to handle words which are unknown. There are three types for unknown words in the queries. The first (and frequent) type is made of words with spelling errors. Such errors could be automatically and corrected. The second type of unknown words is made of words, which are correct in the source language, but they are out-of-vocabulary (OOV) for the translation system and thus remain untranslated. Such words could be modified/replaced by known words (e.g. morphological variant) before translation or translated ex-post by a dictionary look-up or another translation system. The third type is made of query terms which are correct (and correctly translated) but they do not appear in the test collection and are not indexed. In such a case, there is no straightforward and easy solution how to deal with them (possibly they could be replaced by a synonym or another related words).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spelling correction</head><p>The queries for Task 3 were written by medical professionals, but this does not guarantee that they do not contain spelling errors, see e.g. the query in Figure <ref type="figure" coords="7,242.68,464.21,3.87,8.74" target="#fig_0">1</ref>, where the word tretament contains a spelling error. To detect and correct the spelling errors we employed an on-line English medical dictionary MedlinePlus<ref type="foot" coords="7,255.94,486.54,3.97,6.12" target="#foot_7">8</ref> . This dictionary provides a definition for correct medical words and for those which are not correct, it offers a possible correction.</p><p>We automated the process and for each term in the title and narrative of the English queries, we check whether the word exists or not. If the response is "404 not found", we parse the page to get the closest word.</p><p>Two steps translation After translation, our spell checking script reported some unknown words which left untranslated by the Khresmoi system. We passed all such words to Google Translate and obtained their translation which replaced the untranslated forms. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Task 3b: multilingual IR</head><p>In the second part of Task 3, we apply the previous runs on translated queries using the same setup. But we handle OOV problem in RUN5 not by spelling correction as we do in task 3a. We found 7 untranslated words from Czech test queries, 5 words French, and 12 from German, which were post-translated. The best P@10 is achieved by Czech IR using RUN5 as shown in Table <ref type="table" coords="9,472.86,322.07,3.87,8.74">5</ref>. Solving the OOV issue in Czech queries enhances the results by 5.4%, 1.2% in French IR and 3.6% in German IR, while PRF in all multilingual runs does not help. It might have happened because of complex morphological forms for medical terms. Also the usage of narrative tags does not improve the results.</p><p>Unofficially, we also show results for queries have been translated using Google Translate (See RUN1 G ) and Bing Translator (See RUN1 B ). Google Translate does better than Bing Translator on Czech and German, while Bing Translator performs better than Google Translate when the source language is French. However, both these services outperform the Khresmoi translator on this test set.</p><p>Table <ref type="table" coords="9,176.43,457.28,4.98,8.74" target="#tab_5">8</ref> compares our results in Task 3a and Task 3b. The best relative scores are obtained by translation from Czech, which gives 91.04% of the best monolingual results (RUN5). For translation from French, the relative performance is similar (90.29%) but for German, it is only 79.85%. Here the problem is probably in German compound words, which are difficult to translate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Conclusion and future work</head><p>We have described our participation in ShARe/CLEF 2014 eHealth Evaluation Lab Task 3 in its two subtask. Our system was based on the Terrier platform and its implementation of the Hiemstra retrieval model. We experimented with several methods for data cleaning in the test collection and domain-specific language processing (e.g., correction of spelling errors) and found that the optimal cleaning method is a simple removal of HTML markup. In future, we would like to examine query expansion techniques based on the UMLS <ref type="bibr" coords="9,400.46,645.16,15.50,8.74" target="#b10">[11]</ref> thesaurus and extend our work on spelling correction to languages other than English.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,164.87,307.00,285.61,7.89"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An example of a test query. Note the spelling error in the title.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,134.77,291.38,345.81,7.89;5,134.77,302.36,345.81,7.86;5,134.77,313.32,25.21,7.86"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Tuning the lambda parameter of the Hiemstra model on the training data (by MAP) and the scores obtained on the test data. The plot also contains the results for P@10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,134.77,600.03,345.81,61.94"><head>Table 1 .</head><label>1</label><figDesc>Statistics of the query sets: number of queries, average number of tokens in the titles, descriptions, narratives, and profiles, and total number of relevant documents.</figDesc><table coords="3,152.89,631.79,309.55,30.18"><row><cell>query set</cell><cell>size</cell><cell>title</cell><cell>description</cell><cell>narrative</cell><cell>profile</cell><cell>relevant docs</cell></row><row><cell>training</cell><cell>5</cell><cell>3.6</cell><cell>10.6</cell><cell>12.8</cell><cell>52.4</cell><cell>134</cell></row><row><cell>test</cell><cell>50</cell><cell>4.3</cell><cell>8.9</cell><cell>12.4</cell><cell>33.4</cell><cell>3,209</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,134.77,117.41,345.81,84.35"><head>Table 2 .</head><label>2</label><figDesc>Collection size (in MB and millions of tokens) and average document length (in tokens) after applying the different cleaning methods.</figDesc><table coords="6,135.01,149.66,345.42,52.10"><row><cell>method</cell><cell cols="5">total size (MB) % total length (mil. tokens) % avg length (tokens)</cell></row><row><cell>none</cell><cell>41,628</cell><cell>100.00</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HTML-Strip</cell><cell>6,821</cell><cell>16.38</cell><cell>1,006</cell><cell>100.00</cell><cell>911</cell></row><row><cell>Boilerpipe</cell><cell>3,248</cell><cell>7.80</cell><cell>423</cell><cell>42.11</cell><cell>383</cell></row><row><cell>JusText</cell><cell>2,853</cell><cell>6.85</cell><cell>452</cell><cell>44.93</cell><cell>409</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,134.77,216.73,345.82,83.86"><head>Table 3 .</head><label>3</label><figDesc>The effect of the different data cleaning methods on the retrieval results using the training queries, and the results before cleaning.</figDesc><table coords="6,137.50,248.49,340.36,52.10"><row><cell>run ID</cell><cell cols="4">P@5 P@10 NDCG@5 NDCG@10 MAP Rprec bpref rel ret</cell></row><row><cell>none</cell><cell>0.5200 0,6200 0.4758</cell><cell>0.5643</cell><cell cols="2">0.5019 0.5006 0.6542 127</cell></row><row><cell cols="2">HTML-Strip 0.7200 0.6800 0.7030</cell><cell cols="3">0.6844 0.5757 0.4833 0.6597 130</cell></row><row><cell>Boilerpipe</cell><cell>0.6400 0.5200 0.6767</cell><cell>0.5793</cell><cell cols="2">0.4601 0.4665 0.5800 121</cell></row><row><cell>JusText</cell><cell>0.5600 0.5000 0.5785</cell><cell>0.5315</cell><cell>0.3061 0.2983 0.5087</cell><cell>99</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,142.10,116.91,331.16,72.90"><head>Table 4 .</head><label>4</label><figDesc>Task 3a: Monolingual IR performance on test queries.</figDesc><table coords="7,142.10,137.71,331.16,52.10"><row><cell>run ID P@5</cell><cell cols="4">P@10 NDCG@5 NDCG@10 MAP Rprec bpref rel ret</cell></row><row><cell cols="2">RUN1 0.5240 0.5060</cell><cell>0.5353</cell><cell>0.5189</cell><cell>0.3064 0.3538 0.4586 2562</cell></row><row><cell cols="3">RUN5 0.5320 0.5360 0.5449</cell><cell cols="2">0.5408 0.3134 0.3599 0.4697 2556</cell></row><row><cell cols="2">RUN6 0.5080 0.5320</cell><cell>0.5310</cell><cell>0.5395</cell><cell>0.2100 0.2317 0.3984 1832</cell></row><row><cell cols="2">RUN7 0.5120 0.4660</cell><cell>0.5333</cell><cell>0.4878</cell><cell>0.1845 0.2141 0.3934 1676</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,134.77,116.91,345.81,94.81"><head>Table 8 .</head><label>8</label><figDesc>Comparison of the P@10 scores achieved in the cross-lingual runs with the scores from the corresponding monolingual runs.</figDesc><table coords="9,149.58,148.67,316.19,63.06"><row><cell></cell><cell>English</cell><cell>Czech</cell><cell></cell><cell cols="2">German</cell><cell>French</cell><cell></cell></row><row><cell>run ID</cell><cell>P@10</cell><cell>P@10</cell><cell>%</cell><cell>P@10</cell><cell>%</cell><cell>P@10</cell><cell>%</cell></row><row><cell>RUN1</cell><cell>0.5060</cell><cell cols="2">0.4340 85.77</cell><cell cols="2">0.3920 77.47</cell><cell cols="2">0.4720 93.28</cell></row><row><cell>RUN5</cell><cell>0.5360</cell><cell cols="2">0.4880 91.04</cell><cell cols="2">0.4280 79.85</cell><cell cols="2">0.4840 90.29</cell></row><row><cell>RUN6</cell><cell>0.5320</cell><cell cols="2">0.4560 85.71</cell><cell cols="2">0.3820 71.80</cell><cell cols="2">0.4560 85.71</cell></row><row><cell>RUN7</cell><cell>0.4660</cell><cell cols="2">0.3020 64.80</cell><cell cols="2">0.3200 68.66</cell><cell cols="2">0.3240 69.52</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.73,636.52,98.85,7.47"><p>http://trec.nist.gov/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,144.73,647.48,84.73,7.47"><p>http://www.hon.ch/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,144.73,658.44,89.44,7.47"><p>http://khresmoi.eu/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,144.73,658.44,145.92,7.47"><p>http://trec.nist.gov/trec_eval/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="6,144.73,658.44,221.73,7.47"><p>http://search.cpan.org/dist/HTML-Strip/Strip.pm</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="7,144.73,636.52,131.80,7.47"><p>http://translate.google.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="7,144.73,647.48,145.92,7.47"><p>http://www.bing.com/translator/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="7,144.73,658.44,164.75,7.47"><p>http://www.nlm.nih.gov/medlineplus/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was funded by the <rs type="funder">Czech Science Foundation</rs> (grant n. <rs type="grantNumber">P103/12/G084</rs>) and the <rs type="funder">EU</rs> <rs type="projectName">FP7</rs> project <rs type="projectName">Khresmoi</rs> (contract no. <rs type="grantNumber">257528</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_KQrAS4M">
					<idno type="grant-number">P103/12/G084</idno>
				</org>
				<org type="funded-project" xml:id="_qrEcCCx">
					<orgName type="project" subtype="full">FP7</orgName>
				</org>
				<org type="funded-project" xml:id="_zhR3s72">
					<idno type="grant-number">257528</idno>
					<orgName type="project" subtype="full">Khresmoi</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>5 Experiments and results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Task 3a: monolingual IR</head><p>We submitted 4 runs (RUN1, RUN5, RUN6 and RUN7). We did not submit RUN2-4 because we did not use the discharge summaries in our experiments. In all the submitted runs, we apply the Hiemstra retrieval mode with the tuned parameter value on the test collection processed by the HTML-Strip. The submitted runs employs the techniques discussed in the previous section as follows:</p><p>RUN1 exploits queries constructed from the titles only without any processing. RUN5 extends RUN1 by conducting spelling correction on query titles. RUN6 extends RUN5 by applying PRF (query expansion). RUN7 extends RUN6 by adding queries from both of titles and narrative tags.</p><p>The results of our runs submitted to Task 3a are summarized in Table <ref type="table" coords="8,472.85,609.29,3.87,8.74">4</ref>. The only improvement was achieved by RUN5 implementing spelling correction of English. We found 11 misspelled words in English test queries which affected 7 queries in total. Neither query expansion using PRF in RUN5 nor adding addition query terms from the narrative fields bring any improvement.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="10,142.96,211.33,337.62,7.86;10,151.52,222.29,224.90,7.86" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="10,198.11,211.33,282.47,7.86;10,151.52,222.29,45.26,7.86">Probability models for information retrieval based on divergence from randomness</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>University of Glasgow</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct coords="10,142.96,233.25,337.62,7.86;10,151.52,244.21,159.87,7.86" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="10,186.00,233.25,290.78,7.86">Health Topics: 80% of internet users look for health information online</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Fox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
		<respStmt>
			<orgName>Tech. rep. ; Pew Research Center</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,255.17,337.62,7.86;10,151.52,266.13,329.05,7.86;10,151.52,277.08,267.38,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,243.82,266.13,236.76,7.86;10,151.52,277.08,110.16,7.86">Share/clef ehealth evaluation lab 2014, task 3: User-centred health information retrieval</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Palotti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pecina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mueller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,282.39,277.08,107.84,7.86">Proceedings of CLEF 2014</title>
		<meeting>CLEF 2014</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,288.04,337.62,7.86;10,151.52,299.00,329.06,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,253.63,288.04,223.11,7.86">Twenty-one at TREC-7: ad-hoc and cross-language track</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,166.18,299.00,254.25,7.86">Proceedings of the seventh Text Retrieval Conference TREC-7</title>
		<meeting>the seventh Text Retrieval Conference TREC-7</meeting>
		<imprint>
			<biblScope unit="page" from="227" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,320.92,337.62,7.86;10,151.52,331.88,329.05,7.86;10,151.52,342.84,329.06,7.86;10,151.52,353.80,212.20,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,414.99,331.88,65.58,7.86;10,151.52,342.84,135.01,7.86">Overview of the share/clef ehealth evaluation lab</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Schrek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">L</forename><surname>Mowery</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Velupillai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">W</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Palotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,331.60,342.84,110.07,7.86">Proceedings of CLEF 2014</title>
		<title level="s" coord="10,450.22,342.84,30.36,7.86;10,151.52,353.80,136.86,7.86">Lecture Notes in Computer Science (LNCS</title>
		<meeting>CLEF 2014</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,364.76,337.62,7.86;10,151.52,375.71,329.06,7.86;10,151.52,386.67,298.11,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,338.09,364.76,142.49,7.86;10,151.52,375.71,49.23,7.86">Boilerplate detection using shallow text features</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Kohlschütter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Fankhauser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Nejdl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,221.09,375.71,259.49,7.86;10,151.52,386.67,96.83,7.86">Proceedings of the Third ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Third ACM International Conference on Web Search and Data Mining<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="441" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,397.63,337.62,7.86;10,151.52,408.59,329.06,7.86;10,151.52,419.55,329.06,7.86;10,151.52,430.51,25.60,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,450.56,397.63,30.02,7.86;10,151.52,408.59,258.57,7.86">Terrier: A high performance and scalable information retrieval platform</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,432.50,408.59,48.08,7.86;10,151.52,419.55,274.65,7.86">Proceedings of ACM SIGIR&apos;06 Workshop on Open Source Information Retrieval</title>
		<meeting>ACM SIGIR&apos;06 Workshop on Open Source Information Retrieval<address><addrLine>OSIR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,441.47,337.62,7.86;10,151.52,452.43,329.05,7.86;10,151.52,463.39,329.06,7.86;10,151.52,474.34,231.95,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,165.13,463.39,315.45,7.86;10,151.52,474.34,61.43,7.86">Adaptation of machine translation for multilingual information retrieval in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pecina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Dušek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hlaváčová</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Leveling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mareček</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Novák</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tamchyna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Urešová</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,220.64,474.34,134.16,7.86">Artificial Intelligence in Medicine</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,485.30,337.62,7.86;10,151.52,496.26,217.42,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="10,212.65,485.30,263.48,7.86">Removing Boilerplate and Duplicate Content from Web Corpora</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pomikalek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>Masaryk University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct coords="10,142.61,507.22,337.96,7.86;10,151.52,518.18,329.06,7.86;10,151.52,529.14,329.06,7.86;10,151.52,540.10,329.06,7.86;10,151.52,551.06,329.05,7.86;10,151.52,562.02,269.92,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,341.55,507.22,139.03,7.86;10,151.52,518.18,329.06,7.86;10,151.52,529.14,27.65,7.86">Multilingual test sets for machine translation of search queries for cross-lingual information retrieval in the medical domain</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Urešová</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pecina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Dušek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,377.94,540.10,102.63,7.86;10,151.52,551.06,306.78,7.86">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)</title>
		<editor>
			<persName><forename type="first">)</forename><surname>Chair</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">C C</forename><surname>Choukri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Declerck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Loftsson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Maegaard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Mariani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Moreno</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Odijk</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Piperidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename></persName>
		</editor>
		<meeting>the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,572.98,337.96,7.86;10,151.52,583.93,82.47,7.86" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">U</forename><forename type="middle">S</forename></persName>
		</author>
		<title level="m" coord="10,170.96,572.98,215.82,7.86">National Library of Medicine: UMLS reference manual</title>
		<meeting><address><addrLine>Bethesda, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>metathesaurus</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
