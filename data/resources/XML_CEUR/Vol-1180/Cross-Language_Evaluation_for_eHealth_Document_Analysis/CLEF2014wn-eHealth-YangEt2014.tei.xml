<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,171.89,115.96,271.57,12.62;1,254.50,133.89,106.35,12.62">The University of Iowa at CLEF 2014: eHealth Task 3</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,174.64,171.56,45.94,8.74"><forename type="first">Chao</forename><surname>Yang</surname></persName>
							<email>chao-yang@uiowa.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,228.88,171.56,100.07,8.74"><forename type="first">Sanmitra</forename><surname>Bhattacharya</surname></persName>
							<email>sanmitra-bhattacharya@uiowa.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,356.37,171.56,84.36,8.74"><forename type="first">Padmini</forename><surname>Srinivasan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,171.89,115.96,271.57,12.62;1,254.50,133.89,106.35,12.62">The University of Iowa at CLEF 2014: eHealth Task 3</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">16B773A44432CBA04817669725857F57</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Information Retrieval</term>
					<term>Query Expansion</term>
					<term>Pseudo Relevance Feedback</term>
					<term>Markov Random Field</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The task 3 of CLEF eHealth Evaluation lab aims to help laypeople get more accurate information from health related documents. In this task, we did several experiments and tried different technologies to improve the retrieval performance. We tried to clean the original dataset and did sentence level retrieval. We explored different parameter settings for pseudo relevance feedback. Description and Narrative was utilized to expand the query as well. We also modified Markov Random Field (MRF) model to expand the query using medical phrase only. In our training set (2013 test set), using those methods can significantly improve the retrieval performance by 8-15% from baseline. We submitted 4 runs. Results on 2014 test set suggest that the technologies we used except MRF have the potential to improve the performance for the top 5 retrieved results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ShARe/CLEF eHealth Evaluation Lab <ref type="bibr" coords="1,320.53,461.64,13.63,8.74" target="#b2">[3]</ref> is part of CLEF 2014 Conference and Labs of the Evaluation Forum<ref type="foot" coords="1,282.63,472.02,3.97,6.12" target="#foot_0">1</ref> . It aims to help laypeople understand health related documents better. We participated in Task 3: User-centred health information retrieval <ref type="bibr" coords="1,202.74,497.51,11.32,8.74" target="#b1">[2]</ref>. Its goal is to develop a more accurate retrieval strategy for health related documents. Specifically, participants were required to submit a list of relevant health related document ids for each query (topic). In 2014, Task 3 includes a monolingual IR task (Task 3a) and a multilingual IR task (Task 3b). We participated in Task 3a only.</p><p>In particular we asked questions like: 1) Does sentence splitting on documents help improve retrieval performance?</p><p>2) How does one optimize the parameters for pseudo relevance feedback? 3) Is query expansion using descriptions and narratives more effective than using titles only?</p><p>3) Can we include medical phrase detection to make a better Markov random field (MRF) model?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset</head><p>The dataset for Task 3 is provided by Khresmoi project<ref type="foot" coords="2,373.08,145.05,3.97,6.12" target="#foot_1">2</ref> . It has a set of medicalrelated documents in HTML format. The documents are from well-known health and medical sites and databases. The size of dataset is about 41G (uncompressed), it has 1,103,450 documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">HTML to Text</head><p>Since the format of the documents is HTML, it has a lot of HTML tags and other noises which may affect the retrieval performance if we index them directly. We employed Lynx<ref type="foot" coords="2,200.15,262.07,3.97,6.12" target="#foot_2">3</ref> , a command line browser to convert the HTML files to text only. The size of the text only dataset decreased to 8.6G. Then we replaced frequent UTF-8 broken characters <ref type="foot" coords="2,244.35,285.98,3.97,6.12" target="#foot_3">4</ref> . We named this text only dataset "All Text".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Content Cleaning</head><p>The ideal text we extract should be the main article from the webpage. However, there are different sections in a typical webpage. The sections could be the structure information about the website, contact information, headlines, even advertisement. The example in Figure <ref type="figure" coords="2,302.88,380.66,4.98,8.74">1</ref> shows the beginning of one text output from Lynx. Except for the last two lines, all the information is unrelated to the main article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attract NPHS Logo</head><p>Search Clinical Questions Enter search details Search A total of 1713 clinical questions available Quick Guide to ATTRACT What is the evidence for betamethasone cream versus circumcision in phimosis? Associated tags:child health, men's health, circumcision, phimosis, treatment, corticosteroid ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 1. Example Output From Lynx</head><p>However, to remove all those irrelevant information is not trivial. In order to keep the main article only, we tried to use simple rules to remove the headlines, titles. In particular, we removed all the lines which have less and equal than 3 tokens. We also removed all the lines which start with either '*', '+', '-', 'o', '#', and '@'. Those are the headline start symbols from Lynx.</p><p>After the data cleaning mentioned above, the dataset we have is about 5.4G. We name this collection "Text Clean".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Sentence Splitting</head><p>Besides indexing whole documents, we also explored sentence level retrieval. We used GENIA Sentence Splitter (GeniaSS) <ref type="bibr" coords="3,317.59,265.21,10.52,8.74" target="#b5">[6]</ref> to split sentences of each text document from "All Text". This sentence splitter is optimized for the biomedical documents and has good performance. Keeping track of the original text document id we created 3 sentence level datasets: "Sent 1", "Sent 2", and "Sent 3".</p><p>"Sent 1" has only single sentences. (In other words, we treat each sentence as a logical 'document'.) "Sent 2" has pairs of adjacent sentences. "Sent 3" has sequences of 3 adjacent sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Training Topic Set</head><p>We did not use the training topics provided in CLEF eHealth 2014 because there were only 5 topics and the coverage of qrels file is small. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Baseline</head><p>To find out our baseline strategy we created separate indexes from different datasets ("All Text", "Text Clean", "Sent 1", "Sent 2" and "Sent 3") using Indri <ref type="bibr" coords="4,149.92,164.63,9.96,8.74" target="#b6">[7]</ref>. We filtered out stopwords during indexing and in the queries. We ran Indri's Query Likelihood model used title only as query to retrieve documents from different indexes and the one with best performance is our baseline. For instance, the query for the example in Section 2.4 is "#combine(Hypothyreoidism)"</p><p>The evaluation focused on P@5, P@10, NDCG@5, and NDCG@10. These results including MAP are shown in Table <ref type="table" coords="4,322.77,224.40,3.87,8.74" target="#tab_1">1</ref>. We also include the baselines and the best performing runs in 2013. Scores bolded are the best for that measure in the table. Again, Title All Text is the retrieval strategy using title as query and All Text as index which mentioned before. BM25 and BM25 FB (with Pseudo Relevance Feedback) are the official baselines in 2013. The two official baselines only use title as query. (The same strategy with Title All Text.) Mayo2 and Mayo3 are the best 2 runs last year from Zhu et al. at Mayo Clinic <ref type="bibr" coords="4,379.56,485.15,11.62,8.74" target="#b7">[8]</ref>. Our Title All Text is better than BM25 in all the measures, it could have benefited from using Lynx to output text format. It even outperforms Mayo2 and Mayo3 in terms of NDCG@5 and NDCG@10 (but not in P@5, P@10 or MAP). However, using title only to retrieve from Text Clean and Sent 1/2/3 indexes did not improve the performance. Especially for using Sent 1/2/3, the performance for all the measures dropped significantly.</p><p>Therefore, we use Title All Text as the baseline for the later experiments. We drop the Text Clean and the three sentence level datasets since these do not improve retrieval performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Optimize Pseudo Relevance Feedback</head><p>Pseudo Relevance Feedback is a popular and successful method for expanding queries. We can see in Table <ref type="table" coords="4,281.50,656.12,3.87,8.74" target="#tab_1">1</ref>, the official baseline BM25 FB outperforms BM25 in almost all of the measures. We tried to improve on our baseline results with Title All Text by optimizing the parameters of Pseudo Relevance Feedback (Lavrenko's relevance models <ref type="bibr" coords="5,287.45,142.90,10.79,8.74" target="#b3">[4]</ref>) using Indri. There are 3 parameters that need to be set. The first is the weight of original query (Weight). The weight for the expanded query is 1-Weight. The number of documents used for pseudo relevance feedback. The number of terms selected for the feedback query.</p><p>One important notice is that in the later experiments, if a retrieved document which ranked in top 10 is not in the 2013 test qrels (since 2013 test topics are our training topics) provided, we judge it by ourselves and add it to the 2013 test qrels. When judging the documents, we always tried to refer how the documents were labeled in the official qrels (Actually, a lot of documents are almost identical, but only some of them were labeled because of pooling). In the end of our experiments, we added total of 310 documents in the qrels. (80 relevant and 230 non-relevent documents.) It is true adding the qrels might make the later comparison against the 2013 official submitted runs and 2013 baselines unfair. But it would be also impossible to improve our retrieval strategies if we don't label the unjudged top 10 retrieved documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Weight of Original Query</head><p>We experimented with Weight from 0.1 to 0.9. We set the initial value of # terms and # docs to 20 and 5 respectively. Result is shown in Table <ref type="table" coords="5,435.84,375.65,3.87,8.74" target="#tab_2">2</ref>. Weight between 0.6 and 0.9 seem strong across the measures. We favor 0.6 and 0.7 in terms of emphasizing precision at high ranks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Number of Documents</head><p>We explored different values for number of documents from 5 to 50. We tried both 0.6 and 0.7 for Weight, which is the optimal values from the last experiment. Again, the initial value for number of terms is set to 20. Table <ref type="table" coords="6,403.59,253.56,4.98,8.74" target="#tab_3">3</ref> shows the result for Weight=0.6, as it performs better than 0.7 in the experiment.</p><p>The optimal value for number of documents is 10 (both for = 0.6 and 0.7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Number of Terms</head><p>Next we explored values of number of terms from 5 to 50. We set Weight and # Docs to 0.6 and 10 respectively based on the previous experiments. Table <ref type="table" coords="6,475.62,357.29,4.98,8.74" target="#tab_4">4</ref> shows the result. We also show the baseline results (without the benefit of pseudo relevance feedback). Both 40 and 45 are good values for # Terms. We choose 45 for the later experiment since we would like to focus more on top 5 performance (In the later official evaluation, top 10 was used in the primary measures). Finally our parameters for pseudo relevance feedback, Weight, number of Docs, number of Terms are 0.6, 10, and 45 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Expanding the Query Using Description &amp; Narrative</head><p>From the topic example in Section 2.4, we know the title only contains the minimum information for the topic. In order to better describe the information needs of the user, we could expand the query using description or narrative field of the topic. We explored linear combinations of title and description, title and narrative to improve retrieval performance. Specifically we weight the title by WeightT and weight for description or narrative by 1-WeightT. (We also filtered out stopwords for description or narrative fields.)</p><p>The results of linear combination of title and description, title and narrative are shown in Table <ref type="table" coords="7,219.65,250.01,4.98,8.74" target="#tab_5">5</ref> and Table <ref type="table" coords="7,273.98,250.01,4.98,8.74" target="#tab_6">6</ref> respectively. We can see that for both Table <ref type="table" coords="7,475.63,250.01,4.98,8.74" target="#tab_5">5</ref> and Table <ref type="table" coords="7,181.72,261.96,3.87,8.74" target="#tab_6">6</ref>, when the weightT increases, performance also increases. But even the weightT=0.9, it is still not as good as the baseline. Therefore, using description or narrative fields did not significantly improve retrieval performance. These fields may require more sophisticated methods to extract keywords and combine them with the title. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Markov Random Field Model</head><p>Inspired by Zhu et al. <ref type="bibr" coords="7,233.05,553.70,9.96,8.74" target="#b7">[8]</ref>, we explored Markov Random Field (MRF) model <ref type="bibr" coords="7,470.09,553.70,10.52,8.74" target="#b4">[5]</ref> as well. Zhu et al. used the parameters settings described in <ref type="bibr" coords="7,408.73,565.66,9.96,8.74" target="#b4">[5]</ref>. For example if the topic title is "Coronary artery disease", the expanded Indri query using MRF model should be:</p><p>#weight( 0.8 #combine(coronary artery disease) 0.1 #combine( #1(coronary artery) #1(artery disease) ) 0.1 #combine( #uw8(coronary artery) #uw8(artery disease) ) )</p><p>In this section, we describe how we modified the MRF model and explored the parameters. In order to distinguish the original MRF from our modified version, we call the original MRF, MRF Bigram since it expands the query using bigrams in the query. And we call our modified version, MRF MedPhrase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">MRF Bigram</head><p>There are 3 parameters for MRF Bigram model: weight of the title (WeightT) (weights for #1 part and uw8 part are both equal to (1-WeightT)/2 ), Window Type (uw or od: uw/od means unordered/ordered window for the terms), and Window Size (e.g uw8 means unordered window size 8 in Indri). We began with the experiment for the WeightT. The initial value for Window Type &amp; Size are set to uw and 8 respectively. The result is shown in Table <ref type="table" coords="8,389.71,423.27,3.87,8.74" target="#tab_7">7</ref>. MRF Bigram model does improve retrieval performance compared to our baseline (Title All Text). The optimal value for the WeightT is 0.8 or 0.9. We choose 0.8 since we focused on the top 5 performance more (Again, the official evaluation later focuses on the top 10 ).</p><p>Next, we would like to find if changing Window Type &amp; Size would affect the retrieval performance. Results exploring Window Type &amp; Size are shown in Table <ref type="table" coords="8,162.16,631.74,3.87,8.74" target="#tab_8">8</ref>.</p><p>Therefore, WeightT 0.8, uw5 are our optimal parameters for MRF Bigram model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">MRF MedPhrase</head><p>MRF Bigram does improve the retrieval performance, but using bigram does not always make sense. For example, ideally topic "facial cuts and scar tissue" should be interpreted as phrases "facial cuts" and "scar tissue". Bigram "cuts scar" (ignore stopwords) does not make sense. Therefore, we modified the original MRF model and only use medical phrases to expand the query. Using the same example in Section 2.4, MRF MedPhrase model should generate the query like:</p><p>#weight( 0.8 #combine(coronary artery disease) 0.1 #combine( #1(coronary artery disease)) 0.1 #combine( #uw5(coronary artery disease) ) )</p><p>Because coronary artery disease is a medical phrase. Using another topic example: "shortness breath swelling". The query using MRF MedPhrase model should generate the query like:</p><p>#weight( 0.8 #combine(shortness breath swelling) 0.1 #combine( #1(shortness breath) swelling ) 0.1 #combine( #uw5(shortness breath) swelling ) )</p><p>To identify the medical phrases, we use MetaMap <ref type="bibr" coords="9,354.76,461.97,10.52,8.74" target="#b0">[1]</ref> to parse the title of topic. Similar with the MRF Bigram, we found the optimal parameter value for WeightT is 0.8, the Window Type &amp; Size should be set as uw5 as well.</p><p>To make the extraction of medical phrases correct, we need to also enabled spell checking (SC) for MRF models. Table <ref type="table" coords="9,359.13,509.79,4.98,8.74" target="#tab_9">9</ref> shows the comparison for MRF Bigram and MRF MedPhrase. In the comparison, we combined MRF with Pseudo Relevance Feedback (RF) as well. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Expand Medical Abbreviation</head><p>Our best run using MRF MedPhrase with spell checking and pseudo relevance feedback is significantly better than the best runs last year. But there is one more important thing to do. There are several abbreviations in the medical topics, which would be very helpful if we can expand them. However, to expand medial abbreviation is also not trivial. We tried several medical abbreviation lists and found the one from Wikipedia<ref type="foot" coords="10,305.76,201.79,3.97,6.12" target="#foot_4">5</ref> might be the most appropriate one for our task. However, there are still some abbreviations missed. In the 2014 test data, we found "L" could mean "left" which our method cannot expand.</p><p>The result is shown in Table <ref type="table" coords="10,276.94,239.36,8.49,8.74" target="#tab_10">10</ref>. Using medical abbreviation expansion does help achieve higher performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Submitted Runs And Results</head><p>Because the discharge summary is very noisy, we didn't develop retrieval strategies utilizing it. We submitted 4 runs in our final submission. (The baseline is run 1, the experiments without discharge summaries should be Runs 5-7. 5 is the highest priority while 7 is the lowest.) Table <ref type="table" coords="11,359.20,130.95,9.96,8.74" target="#tab_12">12</ref> shows our runs and the technologies used. </p><formula xml:id="formula_0" coords="11,136.56,213.65,359.56,41.94">1 Run 5 X X X X Run 6 X X X Run 7 X X X</formula><p>Run 1 is our baseline, which only uses title to retrieve medical documents. Run 5 is our best run, it uses Markov Random Field (MRF) model which expands queries using only medical phrases, it also utilizes abbreviations expansion, pseudo relevance feedback and spell checking. Run 6 is the same as Run 5, but without pseudo relevance feedback. Run 7 is the same as Run 5, but without MRF model.</p><p>Table <ref type="table" coords="11,177.58,366.79,9.96,8.74" target="#tab_3">13</ref> shows the final performance from the official evaluation. Unfortunately, the runs do not significantly differ from each other. Our Run 7 has better scores for P@5 and NDCG@5 which is our original focus. It shows that pseudo relevance feedback has the ability to achieve high accuracy retrieval especially for the top 5 results. (In the final judgement, run 7 submission was not in the judged pool. Therefore, the real performance for run 7 could be even higher.) But our baseline (Run 1) has better performance for P@10 and NDCG@10 which are the primary official measures. The MRF model we trained using 2013 test data does not improve retrieval performance using 2014 test dataset. The reason could be that we overfitted the model though we attempted to avoid that pitfall.</p><p>Table <ref type="table" coords="11,247.72,512.97,9.04,7.89" target="#tab_3">13</ref>. Performance Of Submitted Runs Runs P@5 P@10 NDCG@5 NDCG@10 MAP Run 1 0.6880 0.6900 0.6705 0.6784 0.3589 Run 5 0.6840 0.6600 0.6579 0.6509 0.3226 Run 6 0.6760 0.6820 0.6380 0.6520 0.3259 Run 7 0.7000 0.6760 0.6777 0.6716 0.3452</p><p>Figure <ref type="figure" coords="11,181.55,620.25,4.98,8.74" target="#fig_0">3</ref> shows our Run 1 (since it has the best top 10 performance in our runs) against the median and best performance (p@10) across all systems submitted to CLEF for each query topic. Topics 8, 13, 15, 28, 34, 44, and 50 are easily handled by Run 1, but topics <ref type="bibr" coords="11,293.59,656.12,7.75,8.74" target="#b6">7,</ref><ref type="bibr" coords="11,304.67,656.12,12.73,8.74">11,</ref><ref type="bibr" coords="11,320.72,656.12,12.73,8.74">22,</ref><ref type="bibr" coords="11,336.76,656.12,12.73,8.74">32,</ref><ref type="bibr" coords="11,352.81,656.12,12.74,8.74">38,</ref><ref type="bibr" coords="11,368.87,656.12,12.73,8.74">40,</ref><ref type="bibr" coords="11,384.92,656.12,9.96,8.74">47</ref> are difficult for it. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>We explored cleaning of the dataset and sentence level retrieval. We showed that retrieval performance did not improve by utilizing the two methods. We also tried linear combinations of title and description/narrative, it seems it is a non trivial task. We did experiments to find out the optimal parameters for pseudo relevance feedback, showed that it can achieve higher performance for top 5 retrieved items. We modified the Markov Random Field model by using the medical phrases to expand the query. This method shows the ability to achieve higher performance on the 2013 queries but fails using the 2014 test dataset. Future work planned includes a more sophisticated method to combine the title and description/narrative/discharge summary, avoiding the overfitting of the MRF model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="12,218.93,631.92,177.49,7.89;12,139.55,138.93,336.25,478.22"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Run 1 Performance For Each Topics</figDesc><graphic coords="12,139.55,138.93,336.25,478.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,152.31,277.27,310.71,131.38"><head>Table 1 .</head><label>1</label><figDesc>Baselines and 2013 Best Runs on 2013 Test Set (Our Training Set)</figDesc><table coords="4,171.55,298.54,269.18,110.11"><row><cell>Run ID</cell><cell cols="4">P@5 P@10 NDCG@5 NDCG@10 MAP</cell></row><row><cell cols="3">Title All Text 0.4840 0.4760 0.4764</cell><cell>0.4811</cell><cell>0.2350</cell></row><row><cell cols="2">Title Text Clean 0.4520 0.4560</cell><cell>0.4583</cell><cell>0.4680</cell><cell>0.2028</cell></row><row><cell>Title Sent 1</cell><cell>0.2040 0.1960</cell><cell>0.2219</cell><cell>0.2114</cell><cell>0.2040</cell></row><row><cell>Title Sent 2</cell><cell>0.2040 0.2000</cell><cell>0.2178</cell><cell>0.2108</cell><cell>0.0964</cell></row><row><cell>Title Sent 3</cell><cell>0.1880 0.1740</cell><cell>0.1921</cell><cell>0.1818</cell><cell>0.0863</cell></row><row><cell>BM25</cell><cell>0.4520 0.4700</cell><cell>0.3979</cell><cell>0.4169</cell><cell>0.3043</cell></row><row><cell>BM25 FB</cell><cell>0.4840 0.4860</cell><cell>0.4205</cell><cell>0.4328</cell><cell>0.2945</cell></row><row><cell>Mayo2</cell><cell cols="2">0.4960 0.5180 0.4391</cell><cell>0.4665</cell><cell>0.3108</cell></row><row><cell>Mayo3</cell><cell>0.5280 0.4880</cell><cell>0.4742</cell><cell>0.4584</cell><cell>0.2900</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,134.77,408.29,345.81,142.34"><head>Table 2 .</head><label>2</label><figDesc>Pseudo</figDesc><table coords="5,134.77,408.32,345.81,142.31"><row><cell cols="5">Relevance Feedback Results Varying Weight (# Docs: 5, # Terms:</cell></row><row><cell>20)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">weight P@5 P@10 NDCG@5 NDCG@10 MAP</cell></row><row><cell>0.1</cell><cell>0.4520 0.3860</cell><cell>0.4516</cell><cell>0.4099</cell><cell>0.1652</cell></row><row><cell>0.2</cell><cell>0.4640 0.4080</cell><cell>0.4644</cell><cell>0.4295</cell><cell>0.1800</cell></row><row><cell>0.3</cell><cell>0.4720 0.4260</cell><cell>0.4671</cell><cell>0.4412</cell><cell>0.1924</cell></row><row><cell>0.4</cell><cell>0.4800 0.4400</cell><cell>0.4679</cell><cell>0.4483</cell><cell>0.2003</cell></row><row><cell cols="2">0.5 0.4880 0.4480</cell><cell>0.4725</cell><cell>0.4540</cell><cell>0.2075</cell></row><row><cell cols="2">0.6 0.4880 0.4520</cell><cell>0.4748</cell><cell>0.4587</cell><cell>0.2161</cell></row><row><cell>0.7</cell><cell cols="2">0.4840 0.4620 0.4710</cell><cell>0.4662</cell><cell>0.2238</cell></row><row><cell>0.8</cell><cell>0.4840 0.4600</cell><cell>0.4749</cell><cell>0.4675</cell><cell>0.2294</cell></row><row><cell>0.9</cell><cell cols="2">0.4840 0.4560 0.4751</cell><cell>0.4642</cell><cell>0.2327</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,134.77,116.41,345.81,108.26"><head>Table 3 .</head><label>3</label><figDesc>Pseudo Relevance Feedback Results Varying Number of Documents (Weight: 0.6, # Terms: 20)</figDesc><table coords="6,187.83,148.64,236.62,76.03"><row><cell cols="5"># Docs P@5 P@10 NDCG@5 NDCG@10 MAP</cell></row><row><cell>5</cell><cell>0.4880 0.4520</cell><cell>0.4748</cell><cell>0.4587</cell><cell>0.2161</cell></row><row><cell>10</cell><cell cols="2">0.5000 0.4660 0.4983</cell><cell>0.4788</cell><cell>0.2216</cell></row><row><cell>20</cell><cell>0.4680 0.4260</cell><cell>0.4563</cell><cell>0.4341</cell><cell>0.2100</cell></row><row><cell>30</cell><cell>0.5000 0.4400</cell><cell>0.4868</cell><cell>0.4532</cell><cell>0.2126</cell></row><row><cell>40</cell><cell>0.4880 0.4380</cell><cell>0.4804</cell><cell>0.4519</cell><cell>0.2158</cell></row><row><cell>50</cell><cell>0.4800 0.4340</cell><cell>0.4739</cell><cell>0.4479</cell><cell>0.2149</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,134.77,414.85,345.81,165.05"><head>Table 4 .</head><label>4</label><figDesc>Experiment of # Terms for Pseudo Relevance Feedback (Weight: 0.6, #</figDesc><table coords="6,134.77,425.84,300.46,154.07"><row><cell>Docs: 10)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell># Terms</cell><cell cols="4">P@5 P@10 NDCG@5 NDCG@10 MAP</cell></row><row><cell>5</cell><cell>0.4840 0.4420</cell><cell>0.4935</cell><cell>0.4650</cell><cell>0.2180</cell></row><row><cell>10</cell><cell>0.4960 0.4420</cell><cell>0.4958</cell><cell>0.4630</cell><cell>0.2232</cell></row><row><cell>15</cell><cell>0.5080 0.4620</cell><cell>0.5015</cell><cell>0.4755</cell><cell>0.2222</cell></row><row><cell>20</cell><cell>0.5000 0.4660</cell><cell>0.4983</cell><cell>0.4788</cell><cell>0.2216</cell></row><row><cell>25</cell><cell>0.5080 0.4600</cell><cell>0.5063</cell><cell>0.4771</cell><cell>0.2232</cell></row><row><cell>30</cell><cell>0.5080 0.4580</cell><cell>0.5040</cell><cell>0.4743</cell><cell>0.2247</cell></row><row><cell>35</cell><cell>0.5080 0.4600</cell><cell>0.5069</cell><cell>0.4776</cell><cell>0.2258</cell></row><row><cell>40</cell><cell>0.5160 0.4720</cell><cell>0.5128</cell><cell>0.4880</cell><cell>0.2290</cell></row><row><cell>45</cell><cell cols="2">0.5240 0.4680 0.5173</cell><cell>0.4851</cell><cell>0.2284</cell></row><row><cell>50</cell><cell>0.5080 0.4580</cell><cell>0.5040</cell><cell>0.4743</cell><cell>0.2247</cell></row><row><cell cols="3">Title All Text 0.4840 0.4760 0.4764</cell><cell>0.4811</cell><cell>0.2350</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,172.44,339.09,270.47,142.24"><head>Table 5 .</head><label>5</label><figDesc>Results with Linear Combinations of Title &amp; Description</figDesc><table coords="7,177.06,359.86,258.17,121.46"><row><cell cols="5"># WeightT P@5 P@10 NDCG@5 NDCG@10 MAP</cell></row><row><cell>0.1</cell><cell>0.1400 0.1480</cell><cell>0.1353</cell><cell>0.1429</cell><cell>0.0699</cell></row><row><cell>0.2</cell><cell>0.1560 0.1620</cell><cell>0.1513</cell><cell>0.1575</cell><cell>0.0731</cell></row><row><cell>0.3</cell><cell>0.1800 0.1780</cell><cell>0.1758</cell><cell>0.1759</cell><cell>0.0782</cell></row><row><cell>0.4</cell><cell>0.2040 0.2040</cell><cell>0.2084</cell><cell>0.2092</cell><cell>0.0887</cell></row><row><cell>0.5</cell><cell>0.2600 0.2360</cell><cell>0.2609</cell><cell>0.2481</cell><cell>0.1093</cell></row><row><cell>0.6</cell><cell>0.2960 0.3120</cell><cell>0.3060</cell><cell>0.3179</cell><cell>0.1428</cell></row><row><cell>0.7</cell><cell>0.3960 0.3800</cell><cell>0.3939</cell><cell>0.3897</cell><cell>0.1844</cell></row><row><cell>0.8</cell><cell>0.4520 0.4320</cell><cell>0.4474</cell><cell>0.4446</cell><cell>0.2132</cell></row><row><cell>0.9</cell><cell cols="2">0.4800 0.4780 0.4690</cell><cell>0.4809</cell><cell>0.2331</cell></row><row><cell cols="2">Title All Text 0.4840 0.4760</cell><cell>0.4764</cell><cell>0.4811</cell><cell>0.2350</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="8,176.36,115.91,262.64,140.49"><head>Table 6 .</head><label>6</label><figDesc>Results with Linear Combinations of Title &amp; Narrative</figDesc><table coords="8,177.06,134.94,258.17,121.46"><row><cell cols="5"># WeightT P@5 P@10 NDCG@5 NDCG@10 MAP</cell></row><row><cell>0.1</cell><cell>0.2720 0.2580</cell><cell>0.2760</cell><cell>0.2694</cell><cell>0.1261</cell></row><row><cell>0.2</cell><cell>0.2800 0.2760</cell><cell>0.2854</cell><cell>0.2855</cell><cell>0.1316</cell></row><row><cell>0.3</cell><cell>0.3160 0.3040</cell><cell>0.3181</cell><cell>0.3138</cell><cell>0.1467</cell></row><row><cell>0.4</cell><cell>0.3360 0.3140</cell><cell>0.3390</cell><cell>0.3274</cell><cell>0.1576</cell></row><row><cell>0.5</cell><cell>0.3600 0.3300</cell><cell>0.3625</cell><cell>0.3457</cell><cell>0.1697</cell></row><row><cell>0.6</cell><cell>0.4000 0.3800</cell><cell>0.4029</cell><cell>0.3929</cell><cell>0.1868</cell></row><row><cell>0.7</cell><cell>0.4240 0.4040</cell><cell>0.4236</cell><cell>0.4153</cell><cell>0.1978</cell></row><row><cell>0.8</cell><cell>0.4360 0.4200</cell><cell>0.4362</cell><cell>0.4310</cell><cell>0.2128</cell></row><row><cell>0.9</cell><cell>0.4520 0.4600</cell><cell>0.4487</cell><cell>0.4619</cell><cell>0.2230</cell></row><row><cell cols="3">Title All Text 0.4840 0.4760 0.4764</cell><cell>0.4811</cell><cell>0.2350</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="8,134.77,456.19,345.82,74.59"><head>Table 7 .</head><label>7</label><figDesc>Results from Varying WeightT for MRF Bigram (Window Type &amp; Size: uw8)</figDesc><table coords="8,177.06,477.46,258.17,53.32"><row><cell cols="5"># Weight P@5 P@10 NDCG@5 NDCG@10 MAP</cell></row><row><cell>0.7</cell><cell>0.4560 0.4300</cell><cell>0.4553</cell><cell>0.4477</cell><cell>0.2189</cell></row><row><cell>0.8</cell><cell cols="2">0.4960 0.4760 0.5051</cell><cell>0.4968</cell><cell>0.2411</cell></row><row><cell>0.9</cell><cell cols="2">0.4760 0.4900 0.4860</cell><cell>0.5028</cell><cell>0.2525</cell></row><row><cell cols="2">Title All Text 0.4840 0.4760</cell><cell>0.4764</cell><cell>0.4811</cell><cell>0.2350</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="9,134.77,116.41,345.83,97.31"><head>Table 8 .</head><label>8</label><figDesc>Results from Varying Window Type &amp; Size for MRF Bigram (WeightT: 0.8)</figDesc><table coords="9,182.69,137.68,246.91,76.03"><row><cell cols="5"># Weight P@5 P@10 NDCG@5 NDCG@10 MAP</cell></row><row><cell>UW5</cell><cell cols="2">0.5000 0.4820 0.5045</cell><cell>0.4988</cell><cell>0.2402</cell></row><row><cell>UW10</cell><cell>0.4840 0.4580</cell><cell>0.4912</cell><cell>0.4801</cell><cell>0.2383</cell></row><row><cell>UW15</cell><cell>0.4960 0.4620</cell><cell>0.4997</cell><cell>0.4851</cell><cell>0.2394</cell></row><row><cell>OD5</cell><cell>0.4920 0.4740</cell><cell>0.4909</cell><cell>0.4873</cell><cell>0.2358</cell></row><row><cell>OD10</cell><cell>0.4840 0.4680</cell><cell>0.4879</cell><cell>0.4842</cell><cell>0.2375</cell></row><row><cell>OD15</cell><cell>0.4840 0.4680</cell><cell>0.4894</cell><cell>0.4859</cell><cell>0.2354</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="9,134.77,565.13,364.05,99.73"><head>Table 9 .</head><label>9</label><figDesc>Comparision for MRF Bigram &amp; MRF MedPhrase</figDesc><table coords="9,134.77,585.90,364.05,78.95"><row><cell>Runs</cell><cell cols="3">P@5 P@10 NDCG@5 NDCG@10 MAP</cell></row><row><cell cols="2">Title All Text MRF Bigram RF SC 0.5320 0.5060 0.5254</cell><cell>0.5168</cell><cell>0.2528</cell></row><row><cell cols="2">Title All Text MRF MedPhrase RF SC 0.5400 0.5060 0.5372</cell><cell>0.5227</cell><cell>0.2651</cell></row><row><cell cols="4">Supporting our intuition, MRF MedPhrase model outperforms MRF Bigram</cell></row><row><cell>for all the measures.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="10,134.77,283.08,388.13,160.95"><head>Table 10 .</head><label>10</label><figDesc>Expanding Medical AbbreviationsSo far, we did several experiments including cleaning the web text, sentence level retrieval, pseudo relevance feedback, linear combination of title and description/narrative, MRF model, spell checking and abbreviation expansion. The comparison between our best strategy and our baseline is shown in Table11. Our best strategy improved about 15% for the measures on top 5 retrieved results. It also improved about 8-9% for the measures on top 10 retrieved results from baseline.</figDesc><table coords="10,136.56,303.86,386.33,30.61"><row><cell>Runs</cell><cell cols="4">P@5 P@10 NDCG@5 NDCG@10 MAP</cell></row><row><cell>Title All Text MRF MedPhrase RF SC</cell><cell>0.5400 0.5060</cell><cell>0.5372</cell><cell>0.5227</cell><cell>0.2651</cell></row><row><cell cols="3">Title All Text MRF MedPhrase RF SC Abbr 0.5520 0.5120 0.5498</cell><cell>0.5257</cell><cell>0.2625</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="10,157.04,465.13,298.20,84.25"><head>Table 11 .</head><label>11</label><figDesc>Comparison Between Best Strategy And Baseline</figDesc><table coords="10,157.04,485.90,298.20,63.48"><row><cell>Runs</cell><cell>P@5</cell><cell cols="4">P@10 NDCG@5 NDCG@10 MAP</cell></row><row><cell>Title All Text (Baseline)</cell><cell>0.4840</cell><cell>0.4760</cell><cell>0.4764</cell><cell>0.4811</cell><cell>0.2350</cell></row><row><cell>Title All Text MRF MedPhrase RF SC Abbr</cell><cell>0.5520 (14.05%↑)</cell><cell>0.5120 (7.56%↑)</cell><cell>0.5498 (15.41%↑)</cell><cell>0.5257 (9.27%↑)</cell><cell>0.2625 (11.7%↑)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="11,136.56,181.49,395.34,40.02"><head>Table 12 .</head><label>12</label><figDesc>Submittion and the technologies used Runs Pseudo Relevance Feedback MRF MedPhrase Spell Checking Abbr. Expansion Run</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,657.44,160.54,7.47"><p>http://clef2014.clef-initiative.eu</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,144.73,635.53,207.61,7.47"><p>http://clefehealth2014.dcu.ie/task-3/dataset</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,144.73,646.48,108.26,7.47"><p>http://lynx.browser.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,144.73,657.44,202.90,7.47"><p>http://www.i18nqa.com/debug/utf8-debug.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="10,144.73,657.44,287.14,7.47"><p>http://en.wikipedia.org/wiki/List_of_medical_abbreviations:_A</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="13,138.35,317.93,342.23,7.86;13,146.91,328.89,333.68,7.86;13,146.91,339.85,82.42,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="13,285.07,317.93,195.51,7.86;13,146.91,328.89,81.60,7.86">An overview of metamap: historical perspective and recent advances</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Aronson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F.-M</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,240.31,328.89,236.22,7.86">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="229" to="236" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,138.35,350.81,342.23,7.86;13,146.91,361.77,333.66,7.86;13,146.91,372.73,234.50,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="13,213.68,361.77,266.89,7.86;13,146.91,372.73,82.00,7.86">Share/clef ehealth evaluation lab 2014, task 3: User-centred health information retrieval</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Palotti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pecina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mueller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,248.08,372.73,105.00,7.86">Proceedings of CLEF 2014</title>
		<meeting>CLEF 2014</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,138.35,383.68,342.23,7.86;13,146.91,394.64,333.66,7.86;13,146.91,405.60,333.67,7.86;13,146.91,416.56,184.24,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,414.48,394.64,66.09,7.86;13,146.91,405.60,148.98,7.86">Overview of the share/clef ehealth evaluation lab 2014</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Schrek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">L</forename><surname>Mowery</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Velupillai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">W</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Palotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,314.38,405.60,103.60,7.86">Proceedings of CLEF 2014</title>
		<title level="s" coord="13,424.83,405.60,55.76,7.86;13,146.91,416.56,110.96,7.86">Lecture Notes in Computer Science (LNCS</title>
		<meeting>CLEF 2014</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,138.35,427.52,342.24,7.86;13,146.91,438.48,333.66,7.86;13,146.91,449.44,211.94,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,271.72,427.52,132.68,7.86">Relevance based language models</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,424.02,427.52,56.57,7.86;13,146.91,438.48,333.66,7.86;13,146.91,449.44,93.93,7.86">Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 24th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,138.35,460.40,342.22,7.86;13,146.91,471.36,333.66,7.86;13,146.91,482.31,282.28,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,265.83,460.40,210.51,7.86">A markov random field model for term dependencies</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,157.82,471.36,322.76,7.86;13,146.91,482.31,164.26,7.86">Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 28th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="472" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,138.35,493.27,342.23,7.86;13,146.91,504.23,333.66,7.86;13,146.91,515.19,333.68,7.86;13,146.91,526.15,40.45,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,146.91,504.23,333.66,7.86;13,146.91,515.19,28.98,7.86">Akane system: protein-protein interaction pairs in biocreative2 challenge, ppi-ips subtask</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Saetre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Yakushiji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Matsubayashi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ohta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,195.00,515.19,234.71,7.86">Proceedings of the Second BioCreative Challenge Workshop</title>
		<meeting>the Second BioCreative Challenge Workshop</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="209" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,138.35,537.11,342.23,7.86;13,146.91,548.07,333.67,7.86;13,146.91,559.03,242.01,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,359.57,537.11,121.01,7.86;13,146.91,548.07,135.34,7.86">Indri: A language model-based search engine for complex queries</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,303.45,548.07,177.14,7.86;13,146.91,559.03,89.37,7.86">Proceedings of the International Conference on Intelligent Analysis</title>
		<meeting>the International Conference on Intelligent Analysis</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,138.35,569.99,342.23,7.86;13,146.91,580.95,333.67,7.86;13,146.91,591.90,120.05,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,361.52,569.99,119.06,7.86;13,146.91,580.95,190.31,7.86">Using discharge summaries to improve information retrieval in clinical domain</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,345.77,580.95,134.82,7.86;13,146.91,591.90,91.78,7.86">Proceedings of the ShARe/-CLEF eHealth Evaluation Lab</title>
		<meeting>the ShARe/-CLEF eHealth Evaluation Lab</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
