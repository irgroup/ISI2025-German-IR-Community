<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,142.46,152.67,310.34,12.64">IRIT at INEX 2014: Tweet Contextualization Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,232.85,192.18,63.81,8.96"><forename type="first">Liana</forename><surname>Ermakova</surname></persName>
							<email>liana.ermakova.87@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Institut de Recherche en Informatique</orgName>
								<orgName type="institution">Toulouse</orgName>
								<address>
									<addrLine>118 Route de Narbonne</addrLine>
									<postCode>31062, Cedex 9</postCode>
									<settlement>Toulouse</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,304.04,192.18,58.03,8.96"><forename type="first">Josiane</forename><surname>Mothe</surname></persName>
							<email>josiane.mothe@irit.fr</email>
							<affiliation key="aff0">
								<orgName type="department">Institut de Recherche en Informatique</orgName>
								<orgName type="institution">Toulouse</orgName>
								<address>
									<addrLine>118 Route de Narbonne</addrLine>
									<postCode>31062, Cedex 9</postCode>
									<settlement>Toulouse</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,142.46,152.67,310.34,12.64">IRIT at INEX 2014: Tweet Contextualization Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8E615FD04F5F69C06627BFFF4751041F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Information retrieval</term>
					<term>tweet contextualization</term>
					<term>summarization</term>
					<term>sentence extraction</term>
					<term>readability</term>
					<term>topic-comment relationship</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The paper presents IRIT's approach used at INEX Tweet Contextualization Track 2014. Systems had to provide a context to a tweet from the perspective of the entity. This year we further modified our approach presented at INEX 2011, 2012 and 2013 underlain by the product of different measures based on smoothing from local context, named entity recognition, part-ofspeech weighting and sentence quality analysis. We introduced two ways to link an entity and a tweet, namely (1) concatenation of the entity and the tweet and (2) usage of the results obtained for the entity as a restriction to filter results retrieved for the tweet. Besides, we examined the influence of topic-comment relationship on contextualization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Millions of tweets are published every day. Twitter is an online social network and microblogging that enables to send and read text messages up to 140 characters <ref type="bibr" coords="1,456.31,477.23,10.71,8.96" target="#b0">[1]</ref>. This limit provokes the fact that often tweets are not self-content and need to be explained, i.e. to be contextualized. In 2014 INEX Tweet Contextualization Track aims to evaluate systems providing context to 240 tweets in English from the perspective of the related entities <ref type="bibr" coords="1,217.73,525.23,10.69,8.96" target="#b1">[2]</ref>. These tweets were collected by the organizers of CLEF RepLab 2013. They have at least 80 characters and do not contain URLs. A tweet has the following annotation types: the category (4 distinct), an entity name from the Wikipedia (64 distinct) and a manual topic label (235 distinct).</p><p>The context has to explain the relationship between a tweet and an entity. It should be a readable summary up to 500 words extracted from a dump of the Wikipedia from November 2012.</p><p>This paper presents IRIT's approach used at INEX Tweet Contextualization Track 2014. Since the task introduced the notion of entities associated to tweets, we include this new feature and propose two ways to link an entity and a tweet:  Making a new query that includes both the entity and the tweet;  Using of the results obtained for the entity as a restriction to filter results retrieved for the tweet.</p><p>Moreover, we analyzed the influence of topic-comment relationship within a sentence in contextualization task.</p><p>As in previous years, we consider tweet contextualization task as multi-document extractive summarization <ref type="bibr" coords="2,228.53,186.18,10.87,8.96" target="#b2">[3,</ref><ref type="bibr" coords="2,241.93,186.18,8.24,8.96" target="#b3">4]</ref> underlain by  the product of scores based on hashtag processing;  TF-IDF cosine similarity measure;  smoothing from local context;  named entity (NE) recognition;  part-of-speech (POS) weighting;  sentence quality measure based on Flesch reading ease test, lexical diversity, meaningful word ratio and punctuation ratio.</p><p>The paper is organized as follows. The Section 2 presents our method by recalling the principles of the 2011-2013 system and describing the modifications we made. The Section 3 discusses the obtained results. The Section 4 concludes the paper and provides some perspectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preprocessing</head><p>Firstly, we performed query preprocessing which differs over the runs:</p><p>1. In order to link an entity and a tweet we combined the fields entity, topic and content into a single search query. 2. The second way is to process fields entity and content as separate queries and then use the results obtained for the entity as a restriction to filter results retrieved for the tweet. Thus, the document retrieved by using the field content as a query are rejected if they do not coincide with top-ranked documents retrieved by using the field entity.</p><p>The queries are encoded by ASCII (characters are normalized). An entity is treated as a single phrase, i.e. a document has to contain all words expressing the entity. Document retrieval was performed by the Terrier platform <ref type="bibr" coords="2,388.99,548.51,10.69,8.96" target="#b4">[5]</ref>, an open-source search engine developed by the School of Computing Science, University of Glasgow. Terrier implements various weighting and retrieval models and allows stemming and blind relevance feedback. We use Porter stemmer <ref type="bibr" coords="2,342.67,584.51,10.69,8.96" target="#b5">[6]</ref>.</p><p>The next step is to parse tweets and retrieved documents by Stanford CoreNLP which integrates such tools as POS tagger <ref type="bibr" coords="2,296.45,608.51,11.69,8.96" target="#b6">[7]</ref> and named entity recognizer <ref type="bibr" coords="2,427.75,608.51,10.69,8.96" target="#b7">[8]</ref>. It uses the Penn Treebank tag set <ref type="bibr" coords="2,230.45,620.51,10.69,8.96" target="#b8">[9]</ref>.</p><p>Then, we merged annotations obtained by parsers and Wikipedia tagging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Searching for Relevant Sentences</head><p>We modified the extraction component developed for INEX 2011-2013. As in previous years, the general idea is to compute similarity between the query and sentences and to retrieve the most similar passages. We model a sentence as a set of vectors:</p><p> Unigram vector represents the lemmas associated with tokens occurred within the sentence. For unigram vectors we compute cosine similarity measure.  A lemma possesses the following features: POS, frequency and IDF. Functional words, such as conjunctions, prepositions and determiners, are not taken into account. POS, frequency and IDF represents vectors of weights for the unigram vector. We used generalized POS (e.g. we merge regular adverbs, superlative and comparative into a single adverb group).  NE vector. NE vectors are treated in the following way:</p><formula xml:id="formula_0" coords="3,216.89,328.48,253.78,19.95">𝑁𝐸 𝐶𝑂𝐸𝐹 = 𝑁𝐸 𝑐𝑜𝑚𝑚𝑜𝑛 +𝑁𝐸 𝑤𝑒𝑖𝑔ℎ𝑡 𝑁𝐸 𝑞𝑢𝑒𝑟𝑦 +1 (1)</formula><p>where 𝑁𝐸 𝑤𝑒𝑖𝑔ℎ𝑡 is floating point parameter given by a user (by default it is equal to 1.0), 𝑁𝐸 𝑐𝑜𝑚𝑚𝑜𝑛 is the number of NE appearing in both query and sentence, 𝑁𝐸 𝑞𝑢𝑒𝑟𝑦 is the number of NE appearing in the query.</p><p>Each sentence has a set of attributes, e.g. which section it belongs to, whether it is a title or header, whether it has personal verbs etc. We assumed that relevant sentences come from relevant documents therefore we multiply sentence score by document relevance or/and by inverted document rank. These characteristics are used for sentence weighting.</p><p>We introduced an algorithm for smoothing from the local context. We assumed that the importance of the context reduces as the distance increases. Thus, the nearest sentences should produce more effect on the target sentence sense than others. For sentences with the distance greater than k this coefficient was zero. The total of all weights should be equal to one. The system allows taking into account k neighboring sentences with the weights depending on their remoteness from the target sentence. Last year we added smoothing from document beginning. Wikipedia abstracts contain the summary of the entire paper; therefore they can be also used for smoothing. However, this parameter did not improve results. Therefore we didn't use it this year.</p><p>As in 2013, we did not apply anaphora resolution. Neither we used redundancy treatment nor sentence reordering since the analysis of previous results showed that their impact is small.</p><p>In 2013 we introduced sentence quality measure based on the product of the Flesch reading ease test <ref type="bibr" coords="3,194.30,613.19,15.46,8.96" target="#b9">[10]</ref>, lexical diversity, meaningful word ratio and punctuation score. We defined lexical diversity as the number of different lemmas used within a sentence divided by the total number of tokens in this sentence. Analogically, meaningful word ration is the number of non-stop words within a sentence divided by the total number of tokens in this sentence. We kept this measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.3</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic-comment relationship in contextualization task</head><p>Linguistics establishes the difference between the clause-level topic and the discourse-level topic. However, within the bound of this paper we are interested in clause-level topic only. The topic (or theme) is the phrase in a clause that the rest of the clause is understood to be about, and the comment (also called rheme or focus) is what is being said about the topic. In simple English clause the topic usually coincides with the subject, however it is not a case of the passive voice. In most languages the common means to mark topic-comment relation are word order and intonation. Moreover, there exist special constructions to introduce the comment. However, the tendency is to use so-called topic fronting, i.e. to place topic at the beginning of a clause.</p><p>We hypothesize that topic-comment relationship identification is useful for contextualization. Quick query analysis provides evidence that an entity is considered as a topic, while tweet content refers rather to comment, i.e. what is said about the entity. Moreover, we assume that providing the context to an entity implies that this context should be about the entity, i.e. the entity is the topic, while the retrieved context presents the comment.</p><p>We used these assumptions for candidate sentence scoring. We double the weight of sentences in which the topic contains the entity under consideration.</p><p>Topic identification is performed under assumption of topic fronting. We simplify this hypothesis by assuming that topic should be place at the sentence beginning. Sentence beginning is viewed as the first half of the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>Summaries were evaluated according to their informativeness and readability. Informativeness was estimated as the overlap of a summary with the pool of relevant passages.</p><p>As in previous years, the lexical overlap between a summary and a pool was estimated in three terms: Unigrams, Bigrams and Skip bigrams representing the proportion of shared unigrams, bigrams and bigrams with gaps of two tokens respectively. Official ranking was based on decreasing score of divergence with the gold standard estimated by skip bigrams.</p><p>The organizers used 2 gold standards:</p><p> pool of relevant sentences per topic;  pool of noun phrases extracted from these sentences together with the corresponding Wikipedia entry.</p><p>The gold standard thorough is a manual run on 1/5 of the 2014 topics. We submitted 3 runs:</p><p>1. The first run (ETC) was performed by the system 2013. As a query three fields entity, topic and content were treated. An entity was treated as a single phrase. 2. The second run (ETC_ENTITY) differed from ETC by double weight for sentences where the entity represented the topic.</p><p>3. Unlike ETC, the third run (ETC_RESTR_NOENT) was based on document set restricted by entities (see the subsection 2.1 Preprocessing).</p><p>Table <ref type="table" coords="5,161.78,182.22,4.98,8.96" target="#tab_0">1</ref> and Table <ref type="table" coords="5,212.69,182.22,4.98,8.96" target="#tab_1">2</ref> provide evaluation results. The evaluation results presented in the Table <ref type="table" coords="5,165.26,194.22,4.98,8.96" target="#tab_0">1</ref> was based on the pool of relevant sentences, while the results obtained on the pool of noun phrases are given in the Table <ref type="table" coords="5,315.29,206.22,3.77,8.96" target="#tab_1">2</ref>.</p><p>ref2013 and ref2012 are the baselines generated using 2013 and 2012 corpus. They are using the same system and index. However, they seem to be artificial. Therefore, we believe that they can be ignored in ranking.</p><p>According to the evaluation performed on the pool of sentences, our runs ETC, ETC_ENTITY and ETC_RESTR_NOENT were classified 3-rd, 4-nd and 6-th; while according to the evaluation based on noun phrases, they got slightly better ranks, namely 2, 3 and 5 respectively.</p><p>Thus, the best results among our runs were obtained by the system that merges fields entity, topic and content into a single query. The run #360 is better than our runs according to sentence evaluation; nevertheless, it showed worse results according to noun phrase evaluation. Our system is targeted on the nouns and especially named entities. This could provoke the differences in ranking with respect to sentences and noun phrases.</p><p>The worst results were showed by the run based on entity restriction. This could be explained by the fact that filtering out the documents that are considered irrelevant to the entity may cause the big loss of relevant documents if they are not top-ranked according to entities. ETC_RESTR_NOENT demonstrated the worst results among our runs even in the case of noun phrases. We believe that this is caused by loss in recall since the importance of noun phrases is not evaluated, but filtering out some documents could have negative effect on noun phrase recall.</p><p>The results of ETC and ETC_ENTITY are very close. However, topic-subject identification slightly decreased the performance of the system. Yet we believe that finer topic-comment identification procedure may ameliorate the results. Readability evaluation was performed by one assessor over a pool of 12 summaries per run. Readability was estimated as mean average scores per summary over soundness, structure (no unresolved anaphora), non-redundancy (diversity) and syntactical correctness.</p><p>The readability results are given in the Table <ref type="table" coords="6,318.77,612.47,3.77,8.96" target="#tab_2">3</ref>. In general we can see that informativeness results are opposite to readability ones. However, our runs kept the same relative order. We received very low score for diversity and structure. This may be related to the fact that we decide not to treat this problem since in previous years their impact was small. Despite we retrieved the entire sentences from the Wikipedia, unexpectedly we received quite low score for syntactical correctness. ETC_ENTITY demonstrated slightly higher results according to all readability measures except diversity. The differences of readability scores between ETC_RESTR_NOENT and ETC are very small since these runs are very similar. The only difference is the documents used as sources of the retrieved sentences. However, all readability scores of ETC_RESTR_NOENT are lower. This can be caused by lower quality of the documents or the influence of the informativeness on the assessor perception of readability. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This year we further modified our approach presented at INEX 2011, 2012 and 2013 underlain by the product of different measures based on smoothing from local context, named entity recognition, part-of-speech weighting and sentence quality analysis. We introduced two ways to link an entity and a tweet, namely (1) concatena-tion of the entity and the tweet and (2) usage of the results obtained for the entity as a restriction to filter results retrieved for the tweet. Besides, we examined the influence of topic-comment relationship on contextualization. Despite these modifications did not improve results, we believe that small changes in implementation may produce positive effect on the system performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,145.94,506.08,303.40,174.30"><head>Table 1 .</head><label>1</label><figDesc>Informativeness evaluation: pool of sentences</figDesc><table coords="5,145.94,528.95,303.40,151.43"><row><cell>Rank</cell><cell>Run</cell><cell cols="3">Unigrams Bigrams Skip bigrams</cell></row><row><cell>1</cell><cell>ref2013</cell><cell>0.705</cell><cell>0.794</cell><cell>0.796</cell></row><row><cell>2</cell><cell>ref2012</cell><cell>0.7528</cell><cell>0.8499</cell><cell>0.8516</cell></row><row><cell>3</cell><cell>361</cell><cell>0.7632</cell><cell>0.8689</cell><cell>0.8702</cell></row><row><cell>4</cell><cell>360</cell><cell>0.782</cell><cell>0.8925</cell><cell>0.8934</cell></row><row><cell>5</cell><cell>ETC</cell><cell>0.8112</cell><cell>0.9066</cell><cell>0.9082</cell></row><row><cell>6</cell><cell>ETC_ENTITY</cell><cell>0.814</cell><cell>0.9098</cell><cell>0.9114</cell></row><row><cell>7</cell><cell>359</cell><cell>0.8022</cell><cell>0.912</cell><cell>0.9127</cell></row><row><cell>8</cell><cell>ETC_RESTR_NOENT</cell><cell>0.8152</cell><cell>0.9137</cell><cell>0.9154</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,145.94,267.23,303.40,279.20"><head>Table 2 .</head><label>2</label><figDesc>Informativeness evaluation: pool of noun phrases</figDesc><table coords="6,145.94,290.13,303.40,256.30"><row><cell>Rank</cell><cell>Run</cell><cell cols="3">Unigrams Bigrams Skip bigrams</cell></row><row><cell>1</cell><cell>ref2013</cell><cell>0.7468</cell><cell>0.8936</cell><cell>0.9237</cell></row><row><cell>2</cell><cell>ref2012</cell><cell>0.7784</cell><cell>0.917</cell><cell>0.9393</cell></row><row><cell>3</cell><cell>361</cell><cell>0.7903</cell><cell>0.9273</cell><cell>0.9461</cell></row><row><cell>4</cell><cell>ETC</cell><cell>0.8088</cell><cell>0.9322</cell><cell>0.9486</cell></row><row><cell>5</cell><cell>ETC_ENTITY</cell><cell>0.809</cell><cell>0.9326</cell><cell>0.9489</cell></row><row><cell>6</cell><cell>360</cell><cell>0.8104</cell><cell>0.9406</cell><cell>0.9553</cell></row><row><cell>7</cell><cell>ETC_RESTR_NOENT</cell><cell>0.8131</cell><cell>0.936</cell><cell>0.9513</cell></row><row><cell>8</cell><cell>359</cell><cell>0.8227</cell><cell>0.9487</cell><cell>0.9613</cell></row><row><cell>9</cell><cell>356</cell><cell>0.8477</cell><cell>0.971</cell><cell>0.9751</cell></row><row><cell>10</cell><cell>357</cell><cell>0.8593</cell><cell>0.9709</cell><cell>0.9752</cell></row><row><cell>11</cell><cell>364</cell><cell>0.8628</cell><cell>0.9744</cell><cell>0.9807</cell></row><row><cell>12</cell><cell>358</cell><cell>0.8816</cell><cell>0.984</cell><cell>0.9864</cell></row><row><cell>13</cell><cell>363</cell><cell>0.884</cell><cell>0.9827</cell><cell>0.987</cell></row><row><cell>14</cell><cell>362</cell><cell>0.8849</cell><cell>0.9833</cell><cell>0.9876</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,125.90,245.63,337.22,301.16"><head>Table 3 . Readability evaluation</head><label>3</label><figDesc></figDesc><table coords="7,125.90,273.84,337.22,272.95"><row><cell>Rank</cell><cell>Run</cell><cell>Readablity</cell><cell>Syntax</cell><cell>Diversity</cell><cell>Structure</cell><cell>Average</cell></row><row><cell>1</cell><cell>358</cell><cell cols="5">94.82% 87.31% 72.17% 93.10% 86.85%</cell></row><row><cell>2</cell><cell>356</cell><cell cols="5">95.24% 85.19% 70.31% 92.40% 85.78%</cell></row><row><cell>3</cell><cell>357</cell><cell cols="5">94.88% 82.53% 71.34% 91.58% 85.08%</cell></row><row><cell>4</cell><cell>364</cell><cell cols="5">88.05% 69.94% 63.91% 86.92% 77.20%</cell></row><row><cell>5</cell><cell>360</cell><cell cols="5">92.60% 70.35% 58.84% 86.33% 77.03%</cell></row><row><cell>6</cell><cell>ref2013</cell><cell cols="5">91.74% 69.82% 60.52% 85.80% 76.97%</cell></row><row><cell>7</cell><cell>ref2012</cell><cell cols="5">91.39% 69.58% 60.67% 85.56% 76.80%</cell></row><row><cell>8</cell><cell>359</cell><cell cols="5">93.03% 70.64% 53.53% 86.34% 75.88%</cell></row><row><cell>9</cell><cell>363</cell><cell cols="5">83.68% 67.92% 61.13% 87.55% 75.07%</cell></row><row><cell>10</cell><cell>362</cell><cell cols="5">83.67% 68.00% 60.81% 87.59% 75.02%</cell></row><row><cell>11</cell><cell>361</cell><cell cols="5">93.23% 70.41% 50.12% 85.97% 74.93%</cell></row><row><cell>12</cell><cell>ETC</cell><cell>90.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,125.90,537.83,338.04,45.56"><head>88% 68.89% 56.59% 80.88% 74.31% 13 ETC_ENTITY 91.23% 69.47% 54.93% 81.56% 74.30% 14 ETC_RESTR_NOENT 90.10% 68.30% 53.83% 80.70% 73.23%</head><label></label><figDesc></figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,128.47,255.18,341.76,8.96;8,143.90,267.18,326.27,8.96;8,143.90,279.21,301.32,8.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,276.30,255.18,193.93,8.96;8,143.90,267.18,104.69,8.96">Tweet, Tweet, Retweet: Conversational Aspects of Retweeting on Twitter</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Golder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Lotan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,256.92,267.18,213.24,8.96;8,143.90,279.21,126.26,8.96">Proceedings of the 2010 43rd Hawaii International Conference on System Sciences</title>
		<meeting>the 2010 43rd Hawaii International Conference on System Sciences</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,128.47,291.21,341.69,8.96;8,143.90,303.21,326.42,8.96;8,143.90,315.21,325.05,8.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,402.90,291.21,67.26,8.96;8,143.90,303.21,176.56,8.96">Overview of the INEX 2014 Tweet Contextualization Track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sanjuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Moriceau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Tannier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bellot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mothe</surname></persName>
		</author>
		<ptr target=".org" />
	</analytic>
	<monogr>
		<title level="m" coord="8,328.63,303.21,136.90,8.96">CLEF 2014 Labs and Workshops</title>
		<title level="s" coord="8,143.90,315.21,238.24,8.96">Notebook Papers. CEUR Workshop Proceedings (CEUR-WS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">7424</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,128.47,327.21,341.70,8.96;8,143.90,339.21,224.44,8.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,254.52,327.21,173.42,8.96">IRIT at INEX: Question Answering Task</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ermakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mothe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,437.01,327.21,33.16,8.96;8,143.90,339.21,136.10,8.96">Focused Retrieval of Content and Structure</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="219" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,128.47,351.21,341.96,8.96;8,143.90,363.21,297.85,8.96;8,143.90,375.21,60.27,8.96" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ermakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mothe</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/documents/71612/58a64b0a-cf0c-4751-a91f-9c8aba4312e1" />
		<title level="m" coord="8,252.04,351.21,214.09,8.96">IRIT at INEX 2013: Tweet Contextualization Track</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,128.47,387.21,341.75,8.96;8,143.90,399.21,326.43,8.96;8,143.90,411.11,326.28,9.05;8,143.90,423.21,170.49,8.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,439.53,387.21,30.69,8.96;8,143.90,399.21,269.74,8.96">Terrier: A High Performance and Scalable Information Retrieval Platform</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,421.50,399.21,48.82,8.96;8,143.90,411.11,326.28,9.05;8,143.90,423.21,21.54,8.96">Proceedings of ACM SIGIR&apos;06 Workshop on Open Source Information Retrieval (OSIR 2006)</title>
		<meeting>ACM SIGIR&apos;06 Workshop on Open Source Information Retrieval (OSIR 2006)<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,128.47,435.21,341.48,8.96;8,143.90,447.21,231.28,8.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,198.17,435.21,130.03,8.96">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,334.64,435.21,131.72,8.96">Readings in information retrieval</title>
		<meeting><address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,128.47,459.21,342.16,8.96;8,143.90,471.23,326.23,8.96;8,143.90,483.23,326.54,8.96;8,143.90,495.23,326.56,8.96;8,143.90,507.23,234.50,8.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,360.94,459.21,109.68,8.96;8,143.90,471.23,170.49,8.96">Feature-rich part-of-speech tagging with a cyclic dependency network</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,322.02,471.23,148.11,8.96;8,143.90,483.23,326.54,8.96;8,143.90,495.23,141.04,8.96">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,128.47,519.23,341.98,8.96;8,143.90,531.23,326.43,8.96;8,143.90,543.23,326.60,8.96;8,143.90,555.23,278.39,8.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,307.36,519.23,163.08,8.96;8,143.90,531.23,203.14,8.96">Incorporating non-local information into information extraction systems by Gibbs sampling</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,354.51,531.23,115.82,8.96;8,143.90,543.23,240.02,8.96">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,128.47,567.23,341.85,8.96;8,143.90,579.23,184.88,8.96" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="8,359.67,567.23,110.64,8.96;8,143.90,579.23,148.57,8.96">Building a large annotated corpus of English: the Penn Treebank</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,133.08,591.23,337.12,8.96;8,143.90,603.13,54.13,9.06" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,189.70,591.23,109.47,8.96">A new readability yardstick</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Flesch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,305.72,591.23,122.33,8.96">Journal of Applied Psychology</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="221" to="233" />
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
