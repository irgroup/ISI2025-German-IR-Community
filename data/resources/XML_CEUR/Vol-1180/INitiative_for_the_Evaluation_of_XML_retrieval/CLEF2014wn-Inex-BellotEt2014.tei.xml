<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,152.27,116.95,310.81,12.62;1,270.87,134.89,73.62,12.62">Overview of INEX Tweet Contextualization 2014 track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,143.89,172.73,60.09,8.74"><forename type="first">Patrice</forename><surname>Bellot</surname></persName>
							<email>patrice.bellot@univ-amu.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">LSIS -Aix-Marseille University (</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,214.52,172.73,87.79,8.74"><forename type="first">Véronique</forename><surname>Moriceau</surname></persName>
							<email>moriceau@limsi.fr</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">LIMSI-CNRS</orgName>
								<orgName type="institution" key="instit2">University Paris-Sud (</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,312.86,172.73,63.02,8.74"><forename type="first">Josiane</forename><surname>Mothe</surname></persName>
							<email>josiane.mothe@irit.fr</email>
							<affiliation key="aff2">
								<orgName type="laboratory" key="lab1">IRIT</orgName>
								<orgName type="laboratory" key="lab2">UMR 5505</orgName>
								<orgName type="institution" key="instit1">Université de Toulouse</orgName>
								<orgName type="institution" key="instit2">Institut Universitaire de Formation des Maitres Midi-Pyrénées (France)</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,386.44,172.73,58.42,8.74"><forename type="first">Eric</forename><surname>Sanjuan</surname></persName>
							<email>eric.sanjuan@univ-avignon.fr</email>
							<affiliation key="aff3">
								<orgName type="laboratory">LIA</orgName>
								<orgName type="institution">Université d&apos;Avignon et des Pays de Vaucluse (France)</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,272.76,184.68,65.37,8.74"><forename type="first">Xavier</forename><surname>Tannier</surname></persName>
							<email>xtannier@limsi.fr</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">LIMSI-CNRS</orgName>
								<orgName type="institution" key="instit2">University Paris-Sud (</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,152.27,116.95,310.81,12.62;1,270.87,134.89,73.62,12.62">Overview of INEX Tweet Contextualization 2014 track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C9E7FC499AAF60243B651CEB13DE14BF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Short text contextualization</term>
					<term>Tweet understanding</term>
					<term>Automatic summarization</term>
					<term>Question answering</term>
					<term>Focus information retrieval</term>
					<term>XML</term>
					<term>Natural language processing</term>
					<term>Wikipedia</term>
					<term>Text readability</term>
					<term>Text informativeness</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>140 characters long messages are rarely self-content. The Tweet Contextualization aims at providing automatically information -a summary that explains the tweet. This requires combining multiple types of processing from information retrieval to multi-document summarization including entity linking. Running since 2010, the task in 2014 was a slight variant of previous ones considering more complex queries from RepLab 2013. Given a tweet and a related entity, systems had to provide some context about the subject of the tweet from the perspective of the entity, in order to help the reader to understand it.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Motivation</head><p>The task in 2014 is a slight variant of previous ones and it is complementary to CLEF RepLab. Previously, given a tweet, systems had to help the user to understand it by providing a short textual summary. This summary had to be readable on a mobile device without having to scroll too much. In addition, the user should not have to query any system and the system should use a resource freely available. More specifically, the guideline specified the summary should be 500 words long and built from sentences extracted from a dump of Wikipedia.</p><p>In 2014 a small variant of the task has been explored, considering more complex queries from RepLab 2013, but using the same corpus. The new use case of the task was the following: given a tweet and a related entity, the system must provide some context about the subject of the tweet from the perspective of the entity, in order to help the reader answering questions of the form "why this tweet concerns the entity? should it be an alert?".</p><p>In the remaining we give details about the 2014 track in English language set up and results.</p><p>We also give preliminary results about the pilot task in Spanish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data collection</head><p>The official document collection for 2014 was the same as in 2013. Between 2011 and 2013 the corpus did change every year but not the use case. In 2014, the same corpus was reused but the use case evolved. Since 2014 TC topics are a selection of tweets from RepLab 2013, it was necessary to use prior WikiPedia dumps. Some participants also used the 2012 corpus raising up the question of the impact of updating the WikiPedia over these tasks.</p><p>Let us recall that the document collection has been built based on yearly dumps of the English WikiPedia since November 2011. We released a set of tools to convert a WikiPedia dump into a plain XML corpus for an easy extraction of plain text answers. The same perl programs released for all participants have been used to remove all notes and bibliographic references that are difficult to handle and keep only non empty Wikipedia pages (pages having at least one section).</p><p>The resulting automatically generated documents from WikiPedia dump, consist of a title (title), an abstract (a) and sections (s). Each section has a subtitle (h). Abstract and sections are made of paragraphs (p) and each paragraph can contain entities (t) that refer to other Wikipedia pages.</p><p>As tweets, 240 topics have been collected from RepLab 2013 corpus. These tweets have been selected in order to make sure that:</p><p>they contained "informative content" (in particular, no purely personal messages), the document collections from Wikipedia had related content, so that a contextualization was possible.</p><p>In order to avoid that fully manual, or not robust enough systems could achieve the task, all tweets were to be treated by participants, but only a random sample of them was to be considered for evaluation.</p><p>These tweets were provided in XML and tabulated format with the following information:</p><p>the category (4 distinct), an entity name from the wikipedia (64 distinct), a manual topic label (235 distinct).</p><p>The entity name was to be used as an entry point into WikiPedia or DBpedia. The context of the generated summaries was expected to be fully related to this entity. On the contrary, the usefulness of topic labels for this automatic task was and remains an open question at this moment because of their variety.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>Like in 2013, the entire evaluation process was carried out by the organizers.</p><p>Tweet contextualization <ref type="bibr" coords="3,258.95,154.14,10.52,8.74" target="#b1">[2]</ref> is evaluated on both informativeness and readability. Informativeness aims at measuring how well the summary explains the tweet or how well the summary helps a user to understand the tweet content. On the other hand, readability aims at measuring how clear and easy to understand the summary is.</p><p>Informativeness. It is based on lexical overlap between a set of relevant passages (RPs) and participant summaries based on LogSim divergence introduced in <ref type="bibr" coords="3,146.74,254.05,9.96,8.74" target="#b1">[2]</ref>. Given an integer λ &gt; 30, and two texts T, S the LogSim divergence can be restated as:</p><formula xml:id="formula_0" coords="3,199.98,282.30,280.61,27.42">LS(Ω S |Ω T ) = ω∈Ω T P (ω|Ω T ). min(Φ T (ω), Φ S (ω)) max(Φ T (ω), Φ S (ω)<label>(1)</label></formula><p>where for any text Z, Ω Z is the set of n-grams in Z and for any n-gram ω ∈ Ω Z :</p><formula xml:id="formula_1" coords="3,246.06,338.46,234.54,9.65">Φ Z (ω) = log(1 + λP (ω|Ω Z ))<label>(2)</label></formula><p>The λ parameter used in LS formula represents the summary allowed maximal length in words (500 in our case).</p><p>Once the pool of RPs (t-rels) is constituted, the process is automatic and can be applied to unofficial runs. The release of these pools is one of the main contributions of Tweet Contextualization tracks at INEX <ref type="bibr" coords="3,379.86,406.06,15.36,8.74" target="#b3">[4,</ref><ref type="bibr" coords="3,396.87,406.06,7.75,8.74" target="#b2">3,</ref><ref type="bibr" coords="3,406.29,406.06,7.01,8.74" target="#b0">1]</ref>.</p><p>In previous editions t-rels were based on a pooling of participant submitted passages. Organizers then selected among them those that were relevant. In 2013, to build a more robust reference, two manual runs by participants were added using different on line research engines to find relevant WikiPedia pages and copying the relevant passages into the reference.</p><p>This year, even though there were only five participants, the variety of submitted passages was too high compared to the number of runs. One reason was that this year topics included more facets and converting them into queries for a Research Engine was less straightforward. As a consequence, it was not possible to rely on a pooling from participant runs because it would have been too sparse and incomplete. It was finally decided to rely on a thorough manual run by the organizers based on the reference system that was made available to all participants at http://qa.termwatch.es A manual query in Indri language was set up for every topic over five. These queries have been refined until they provide only a set of relevant passages using the reference system on the 2013 corpus. From this RPs we extracted two trels, one merging all passages for each tweets, another by considering the Noun Phrases (NPs) only from the passages to reduce the risk of introducing document identifiers in the passages.</p><p>The average length of queries to build the reference is 8 tokens with a minimum of 2 and a maximum of 14. Therefore, efficient queries are much shorter than tweets. The average number of relevant tokens in the t-rels based on passages is 620, and on the t-rels based on NPs is only 300.</p><p>Readability. By contrast, readability is evaluated manually and cannot be reproduced on unofficial runs. In this evaluation the assessor indicates where he misses the point of the answers because of highly incoherent grammatical structures, unsolved anaphora, or redundant passages. Since 2012, three metrics have been used: Relaxed metric, counting passages where the T box has not been checked; Syntax, counting passages where the S box was not checked either, and the Structure (or Strict) metric counting passages where no box was checked at all. As in previous editions, participant runs have been ranked according to the average, normalized number of words in valid passages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>In 2014, 4 combined teams from six countries (Canada, France, Germany, India, Russia, Tunesia) submitted 12 runs to the Tweet Contextualization track in the framework of CLEF INEX lab 2014<ref type="foot" coords="4,292.16,328.34,3.97,6.12" target="#foot_0">5</ref> . The total number of submitted passages was 54, 932 with an average length of 32 tokens. The total number of tokens was 1, 764, 373 with an average of 7, 352 per tweet.</p><p>We also generated two reference runs based one the organizer's system made available to participants using 2013 and 2012 corpus respectively.</p><p>To read the scores, the lower they are the better since these are divergences. Informativeness results based on passage t-rels are presented in Table <ref type="table" coords="4,452.39,401.64,3.87,8.74" target="#tab_0">1</ref>, and those on NPs t-rels in Table <ref type="table" coords="4,268.41,413.60,3.87,8.74" target="#tab_2">3</ref>. Statistical significance of differences between scores in Table <ref type="table" coords="4,203.90,425.55,4.98,8.74" target="#tab_0">1</ref> are indicated in Table <ref type="table" coords="4,313.02,425.55,3.87,8.74" target="#tab_1">2</ref>. Readability results are presented in Table <ref type="table" coords="4,162.16,437.51,3.87,8.74" target="#tab_3">4</ref>.</p><p>Both informativeness rankings in Table <ref type="table" coords="4,319.59,449.46,4.98,8.74" target="#tab_0">1</ref> and in Table <ref type="table" coords="4,382.41,449.46,4.98,8.74" target="#tab_2">3</ref> are highly correlated, however discrepancies between the two rankings show that differences between top ranked runs rely on tokens outside NPs, mainly verbs since functional words are removed in the evaluation.</p><p>Table <ref type="table" coords="4,177.12,497.28,4.98,8.74" target="#tab_3">4</ref> reveals that readability of reference runs is low, meanwhile they are made of longer passages than average to ensure local syntax correctness.</p><p>Since reference runs are using the same system and index as the manual run used to build the t-rels, they tend to minimize the informativeness divergence with the reference. However, average divergence remains high pointing out that selecting the right passages in the restricted context of an entity, was more difficult than previous more generic tasks. Considering readability, the fact that reference runs are low ranked confirms that finding the right compromise between readability and informativeness remains the main difficulty of this task.</p><p>This year, the best participating system for informativeness used association rules. Since contextualization was restricted to a facet described by an entity, it could be that association rules helped to focus on this aspect.  The best participating system for readability used an advanced summarization systems that introduced minor changes in passages to improve readability. Changing the content of the passages was not allowed, however this tend to show that to deal with readability some rewriting is required. Moreover, since this year evaluation did not include a pool of passages from participants, systems that provided modified passages have been disadvantaged in informativeness evaluation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Pilot task in spanish</head><p>A extra set of topics (only tweet texts) has been released in Spanish to try a different language and a slightly different task. Topics in Spanish are opinionated personal tweets about music bands, cars and politics. Like for tweets in English, they were also manually selected from CLEF RepLab 2013 test set among those without external url and with at least 15 words. Contextualization had to help the reader to understand the opinion polarity, allusions and humor. Three runs from two teams have been submitted to this task. Informativeness results based on passage t-rels based on a manual run over a random subset of seven topics are presented in Table <ref type="table" coords="6,289.77,645.16,3.87,8.74" target="#tab_4">5</ref>. Best run uses entity extractions in tweets and complex language model queries based on these entities. Other runs provide plain definitions of terms in tweets. Best run significantly outperforms two other runs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Text contextualization can be viewed as a way to provide more information on the corresponding text in the objective to make it understandable and to relate this text to information that explains it. This year we experimented a less generic task where only information that explains the tweet opinion and/or the relation with a given entity is considered as relevant. Surprisingly, participant runs showed that thorough and updated information about opinions related to entities can be extracted from WikiPedia page textual content.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,134.77,117.41,345.81,193.94"><head>Table 1 .</head><label>1</label><figDesc>Informativeness results bases on passage t-rels (official results are "with 2gap").</figDesc><table coords="5,223.90,149.66,167.55,161.69"><row><cell cols="3">Rank Run unigram bigram with 2-gap</cell></row><row><cell cols="3">1 ref2013 0.7050 0.7940 0.7960</cell></row><row><cell cols="3">2 ref2012 0.7528 0.8499 0.8516</cell></row><row><cell>3</cell><cell>361</cell><cell>0.7632 0.8689 0.8702</cell></row><row><cell>4</cell><cell>360</cell><cell>0.7820 0.8925 0.8934</cell></row><row><cell>5</cell><cell>368</cell><cell>0.8112 0.9066 0.9082</cell></row><row><cell>6</cell><cell>369</cell><cell>0.8140 0.9098 0.9114</cell></row><row><cell>7</cell><cell>359</cell><cell>0.8022 0.9120 0.9127</cell></row><row><cell>8</cell><cell>370</cell><cell>0.8152 0.9137 0.9154</cell></row><row><cell>9</cell><cell>356</cell><cell>0.8415 0.9696 0.9702</cell></row><row><cell>10</cell><cell>357</cell><cell>0.8539 0.9700 0.9712</cell></row><row><cell>11</cell><cell>364</cell><cell>0.8461 0.9697 0.9721</cell></row><row><cell>12</cell><cell>358</cell><cell>0.8731 0.9832 0.9841</cell></row><row><cell>13</cell><cell>362</cell><cell>0.8686 0.9828 0.9847</cell></row><row><cell>14</cell><cell>363</cell><cell>0.8682 0.9825 0.9847</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,134.77,334.60,345.82,70.62"><head>Table 2 .</head><label>2</label><figDesc>Statistical significance for official results in table 1 (t-test, two sided, α = 5%,</figDesc><table coords="5,134.77,345.59,345.82,59.63"><row><cell cols="14">three levels of significance 1, 2, 3 corresponding to a p-value lower than 0.1, 0.05 and</cell></row><row><cell>0.01 respectively.)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ref2013</cell><cell>ref2012</cell><cell>361</cell><cell>360</cell><cell>359</cell><cell>368</cell><cell>369</cell><cell>370</cell><cell>356</cell><cell>357</cell><cell>364</cell><cell>358</cell><cell>363</cell><cell>362</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,134.77,117.41,345.81,182.98"><head>Table 3 .</head><label>3</label><figDesc>Informativeness results bases on NP t-rels (official results are "with 2-gap").</figDesc><table coords="6,223.90,138.70,167.55,161.69"><row><cell cols="3">Rank Run unigram bigram with 2-gap</cell></row><row><cell cols="3">1 ref2013 0.7468 0.8936 0.9237</cell></row><row><cell cols="3">2 ref2012 0.7784 0.9170 0.9393</cell></row><row><cell>3</cell><cell>361</cell><cell>0.7903 0.9273 0.9461</cell></row><row><cell>4</cell><cell>368</cell><cell>0.8088 0.9322 0.9486</cell></row><row><cell>5</cell><cell>369</cell><cell>0.8090 0.9326 0.9489</cell></row><row><cell>6</cell><cell>370</cell><cell>0.8131 0.9360 0.9513</cell></row><row><cell>7</cell><cell>360</cell><cell>0.8104 0.9406 0.9553</cell></row><row><cell>8</cell><cell>359</cell><cell>0.8227 0.9487 0.9613</cell></row><row><cell>9</cell><cell>356</cell><cell>0.8477 0.9710 0.9751</cell></row><row><cell>10</cell><cell>357</cell><cell>0.8593 0.9709 0.9752</cell></row><row><cell>11</cell><cell>364</cell><cell>0.8628 0.9744 0.9807</cell></row><row><cell>12</cell><cell>358</cell><cell>0.8816 0.9840 0.9864</cell></row><row><cell>13</cell><cell>363</cell><cell>0.8840 0.9827 0.9870</cell></row><row><cell>14</cell><cell>362</cell><cell>0.8849 0.9833 0.9876</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,204.92,314.56,205.52,182.49"><head>Table 4 .</head><label>4</label><figDesc>Readability results</figDesc><table coords="6,204.92,335.36,205.52,161.69"><row><cell cols="3">Rank Run Relaxed Strict Syntax Average</cell></row><row><cell>1</cell><cell cols="2">358 0.94822 0.721683 0.722796 0.931005</cell></row><row><cell>2</cell><cell cols="2">356 0.952381 0.650917 0.703141 0.923958</cell></row><row><cell>3</cell><cell cols="2">357 0.948846 0.578212 0.713445 0.91575</cell></row><row><cell>4</cell><cell cols="2">362 0.836699 0.366561 0.608136 0.875917</cell></row><row><cell>5</cell><cell cols="2">363 0.836776 0.363954 0.611289 0.8755</cell></row><row><cell>6</cell><cell cols="2">364 0.880508 0.337197 0.639092 0.869167</cell></row><row><cell>7</cell><cell>359</cell><cell>0.9303 0.258563 0.535264 0.863375</cell></row><row><cell>8</cell><cell cols="2">360 0.925959 0.258658 0.588365 0.863274</cell></row><row><cell>9</cell><cell cols="2">361 0.932281 0.247883 0.501199 0.859749</cell></row><row><cell cols="3">10 ref2013 0.917378 0.259702 0.605203 0.857958</cell></row><row><cell cols="3">11 ref2012 0.913858 0.259584 0.606742 0.855583</cell></row><row><cell>12</cell><cell cols="2">369 0.912318 0.259539 0.549334 0.815625</cell></row><row><cell>13</cell><cell cols="2">368 0.908815 0.248981 0.565912 0.80875</cell></row><row><cell>14</cell><cell cols="2">370 0.901044 0.246893 0.538338 0.806958</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,134.77,162.00,345.81,73.39"><head>Table 5 .</head><label>5</label><figDesc>Informativeness results bases on passage t-rels (official results are "with 2gap").</figDesc><table coords="7,229.86,194.26,155.62,41.14"><row><cell>Rank Run unigram bigram with 2-gap</cell></row><row><cell>1 318 0.7636 0.8639 0.8761</cell></row><row><cell>2 316 0.9816 0.9966 0.9973</cell></row><row><cell>3 315 0.9856 0.9973 0.9979</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_0" coords="4,144.73,646.84,335.85,7.86;4,144.73,657.80,218.40,7.86"><p>Two other teams from Mexico and Spain participated to the pilot task in Spanish submitting three runs not considered in this overview.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,138.35,436.21,342.23,7.86;7,146.91,447.17,333.66,7.86;7,146.91,458.13,333.67,7.86;7,146.91,469.09,333.66,7.86;7,146.91,480.05,246.84,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,370.04,458.13,91.13,7.86">Overview of inex 2013</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bellot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Geva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gurajada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kazai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koolen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Moriceau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mothe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Preminger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sanjuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schenkel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Tannier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Trappett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="7,394.59,469.09,85.99,7.86;7,146.91,480.05,82.07,7.86">CLEF. Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Paredes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Rosso</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Stein</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">8138</biblScope>
			<biblScope unit="page" from="269" to="281" />
			<date type="published" when="2013">2013</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,491.01,89.15,7.86;7,243.83,491.01,91.00,7.86;7,352.56,491.01,128.01,7.86;7,146.91,501.97,333.66,7.86;7,146.91,512.92,333.66,7.86;7,146.91,523.88,25.60,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,352.56,491.01,128.01,7.86;7,146.91,501.97,123.25,7.86">Overview of the inex 2010 question answering track (qa@inex)</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sanjuan</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Bellot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Moriceau</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Tannier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="7,172.69,512.92,171.12,7.86">INEX. Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Geva</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Kamps</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Schenkel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Trotman</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">6932</biblScope>
			<biblScope unit="page" from="269" to="281" />
			<date type="published" when="2010">2010</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,534.84,342.23,7.86;7,146.91,545.80,333.66,7.86;7,146.91,556.76,247.52,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,397.27,534.84,83.31,7.86;7,146.91,545.80,136.81,7.86">Overview of the inex 2012 tweet contextualization track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sanjuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Moriceau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Tannier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bellot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mothe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,172.57,556.76,188.45,7.86">CLEF (Online Working Notes/Labs/Workshop</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Karlgren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Womser-Hacker</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,567.72,342.23,7.86;7,146.91,578.68,333.67,7.86;7,146.91,589.64,333.66,7.86;7,146.91,600.60,192.55,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,413.58,567.72,67.00,7.86;7,146.91,578.68,185.76,7.86">Overview of the inex 2011 question answering track (qa@inex)</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sanjuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Moriceau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Tannier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bellot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mothe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,185.55,589.64,176.20,7.86">Focused Retrieval of Content and Structure</title>
		<title level="s" coord="7,369.15,589.64,111.42,7.86;7,146.91,600.60,27.78,7.86">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Geva</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Kamps</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Schenkel</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">7424</biblScope>
			<biblScope unit="page" from="188" to="206" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
