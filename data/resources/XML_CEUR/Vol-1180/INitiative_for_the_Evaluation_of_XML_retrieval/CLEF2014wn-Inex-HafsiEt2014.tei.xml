<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,161.88,116.06,291.63,12.91">LaHC at INEX 2014: Social Book Search Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,205.08,154.22,55.69,8.97"><forename type="first">Meriem</forename><surname>Hafsi</surname></persName>
							<email>meriem.hafsi@etu.univ-st-etienne.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">Université de Lyon</orgName>
								<address>
									<postCode>F-42023</postCode>
									<settlement>Saint-Étienne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">UMR 5516</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">Laboratoire Hubert Curien</orgName>
								<address>
									<postCode>F-42000</postCode>
									<settlement>Saint-Étienne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,270.12,154.22,54.67,8.97"><forename type="first">Mathias</forename><surname>Géry</surname></persName>
							<email>mathias.gery@univ-st-etienne.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">Université de Lyon</orgName>
								<address>
									<postCode>F-42023</postCode>
									<settlement>Saint-Étienne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">UMR 5516</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">Laboratoire Hubert Curien</orgName>
								<address>
									<postCode>F-42000</postCode>
									<settlement>Saint-Étienne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,334.08,154.22,71.85,8.97"><forename type="first">Michel</forename><surname>Beigbeder</surname></persName>
							<email>michel.beigbeder@emse.fr</email>
							<affiliation key="aff2">
								<orgName type="institution">École Nationale Supérieure des Mines de Saint-Étienne</orgName>
								<address>
									<addrLine>158, cours Fauriel -F</addrLine>
									<postCode>42023</postCode>
									<settlement>Saint-Étienne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,161.88,116.06,291.63,12.91">LaHC at INEX 2014: Social Book Search Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">48CC92C36CAAF307F2853AA2AA85B2AE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Social Information Retrieval</term>
					<term>Recommendation</term>
					<term>Structured Information Retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the article, we describe our participation in the INEX 2014 Social Book Search track. We present the different approaches exploiting user social information such as reviews, tags and ratings. These social informations are assigned by users to the books. We optimize our models using the INEX Social Book Search 2013 collection and we test them on the INEX 2014 Social Book Search track.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this article, we present the different approaches used in our participation to the INEX 2014 Social Book Search (SBS). The idea is to exploit some user generated content such as reviews and ratings to recommend some books. We have also used an approach based on the similarity between users. For the experiments, we have used the both collections INEX SBS 2013 and 2014 <ref type="foot" coords="1,239.88,492.14,3.49,6.28" target="#foot_0">3</ref> . Our goal is to improve the information retrieval process by optimizing our models with the INEX SBS 2013 collection. In the following section, we present the INEX SBS 2014 collection and data. Then, we present the models optimized on the INEX SBS 2013. Finally, we detail our official runs ans the results obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Collection and Data</head><p>The collection contains 2.8 million book descriptions from Amazon, composed of 64 XML fields. Among these fields, we distinguish:</p><p>-Metadata: &lt;book&gt;, &lt;isbn&gt;, &lt;title&gt;, &lt;authorid&gt;, etc. -Social information: &lt;review&gt;, &lt;summary&gt;, &lt;tags&gt;, &lt;rating&gt;, etc.</p><p>LT User-Profiles are provided from LibraryThing (LT) in a text file containing 93,976 anonymous users. These profiles do not contain the personal information: they contain only the personal catalog of the users. Each user catalog is presented as a set of rows where each row represents the review of the user on one book with a rating and eventually some tags.</p><p>There are 680 topics in INEX 2014 SBS Track, where each topic contains five fields: &lt;title&gt;, &lt;query&gt;, &lt;narrative&gt;, the &lt;group&gt; where the topic was posted and a personal catalog &lt;catalog&gt; of the anonymous user who wrote the topic.</p><p>In our experiments, we use both collections, INEX SBS 2013 <ref type="bibr" coords="2,403.58,215.06,11.70,8.97" target="#b0">[1]</ref> and INEX SBS 2014. Both collections use the same set of documents (book descriptions). We use INEX SBS 2013 collection because the relevance judgments were available so it allowed the optimization of our system before the actual submission of our 2014 runs. The difference between the two collections lies in the topics and the users profiles. In 2014, we have only the personal catalog of each user, but in 2013 we have complete users profiles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Retrieval models and their optimization with INEX SBS 2013</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preprocessing</head><p>The preprocessing step eliminates the fields we do not need. Each book description can contain one &lt;reviews&gt; field, that contains one or several reviews in &lt;review&gt; fields. Each &lt;review&gt; field is composed of &lt;summary&gt;, &lt;content&gt; and &lt;tags&gt; fields. In the &lt;tags&gt; field, we find some tags &lt;tag&gt;. After this preprocessing step, the collection contains the fields:</p><p>-&lt;docno&gt;: &lt;isbn&gt; field of the book.</p><p>-&lt;title&gt;: &lt;title&gt; field of the book.</p><p>-&lt;summary&gt;: Concatenation of the &lt;summary&gt; fields.</p><p>-&lt;content&gt;: Concatenation of the &lt;content&gt; fields.</p><p>-&lt;tags&gt;: Concatenation of the &lt;tag&gt; fields. The new field &lt;tags&gt; contains as many copies of the &lt;tag&gt; field content as indicated by the count attribute. For example, &lt;tag count="3"&gt;moon&lt;/tag&gt; will be written: &lt;tags&gt;moon moon moon&lt;/tags&gt;.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Indexing and querying</head><p>We use the Terrier 3.6 Search Engine<ref type="foot" coords="2,283.08,554.54,3.49,6.28" target="#foot_1">4</ref> which can index large XML collections. We use the default stop-words list of Terrier and the Porter Stemmer. Then, we create five book description index as follows:</p><p>-Index-Title: Only the &lt;title&gt; field of each book description, so no social information is indexed. -Index-Summary: &lt;summary&gt; field only.</p><p>-Index-Content: &lt;content&gt; field only.</p><p>-Index-Tags: &lt;tags&gt; field only.</p><p>-Index-All-Fields: The concatenation of all the fields: &lt;title&gt;, &lt;summary&gt;, &lt;content&gt; and &lt;tags&gt;.</p><p>We build four set of queries:</p><p>-Topic-Title: Only the &lt;title&gt; field of each topic.</p><p>-Topic-Query: Only the &lt;query&gt; field.</p><p>-Topic-Title-Query: &lt;title&gt; and &lt;query&gt; fields.</p><p>-Topic-All-Fields: &lt;title&gt;, &lt;query&gt; and &lt;narrative&gt; fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Content-Based Retrieval</head><p>First, we combine each of the four queries set with each of the five documents index, using the BM25 model <ref type="bibr" coords="3,231.84,306.14,10.69,8.97" target="#b1">[2]</ref>. We select one of the combinations and then we optimize the weight of each field in the BM25F model <ref type="bibr" coords="3,320.19,318.14,10.69,8.97" target="#b2">[3]</ref>. To evaluate the runs of the 20 combinations with BM25 and the run with BM25F, we test on 52 topics manually selected among the 386 topics of INEX SBS 2013. The selected topics are those in which the information need is based on the actual book content and not only on its usage. We evaluate our results with nDCG@1000 (shortened to nDCG in this paper) using trec eval<ref type="foot" coords="3,474.24,364.34,3.49,6.28" target="#foot_2">5</ref> . Evaluation results of these experiments are shown in Table <ref type="table" coords="3,371.08,377.90,3.74,8.97" target="#tab_0">1</ref>. The results show that indexing user generated content (&lt;summary&gt;, &lt;content&gt; and &lt;tags&gt;) improved the search results. We notice that a field based BM25 weight function (BM25F) obtains better results (0.2132) than the classical weight function BM25 (0.1504), while they index the same information. We choose to focus our experiments on the topics composed only by the &lt;title&gt; and &lt;query&gt; fields. Thus, the BM25F has been used only with one set of queries, even if the best results are obtained with queries including the &lt;narrative&gt; field. In the sequel, we will consider the index BM25F Index-All-Fields which is the most promising one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Social Re-Ranking</head><p>Our goal is to experiment re-ranking methods in order to improve content-based search results obtained in the previous section. We propose four models using the books ratings (Score AmazonRatings , Score LT RatingsP op and Score LT RatingsRep ) and the similarity between users (Score UsersSimilarity ).</p><p>Amazon Books Rating based approach(Score AmazonRatings ): Some books were commented and rated by Amazon users. There are 14,042,020 ratings in the collection ranging from 0 to 5, 5 indicating the maximum rating. These ratings are distributed as shown in Table <ref type="table" coords="4,197.66,237.38,3.77,8.97" target="#tab_1">2</ref>. We compute a score AmazonRating(d) for each book d using m user ratings of d as presented in equation 1. We define the score Score AmazonRatings (d, q) of a book d for a query q by a linear combination of BM 25F and AmazonRating(d) scores (cf. equation 2).</p><formula xml:id="formula_0" coords="4,203.88,421.25,276.80,25.73">AmazonRating(d) = m i=0 Rating d (i) m + ln (m)<label>(1)</label></formula><formula xml:id="formula_1" coords="4,142.20,450.45,338.48,10.87">Score AmazonRatings (d, q) = α 1 BM 25F (d, q) + (1 -α 1 )AmazonRating(d) (2)</formula><p>where α 1 is a free parameter of our model.</p><p>LibraryThing Books Rating based approaches (Score LT RatingsP op/Rep ): The LibraryThing ratings range from 0 to 10. We introduce two concepts:</p><p>-Popularity: P op(d) is based on the number of times the book has been added to catalog. The more the book is reviewed, the higher is its popularity. -Reputation: Rep(d) is based on the number of times the book received a rating grater than 6. Then, the more the book is highly rated, the higher is its reputation.</p><p>P op(d) and Rep(d) are obtained as follows:</p><formula xml:id="formula_2" coords="4,242.04,601.51,238.64,24.24">P op(d) = 0 if (m = 0) ln(m) if (m ≥ 1)<label>(3)</label></formula><formula xml:id="formula_3" coords="4,247.80,641.23,232.88,24.36">Rep(d) = 0 if (l = 0) ln(l) if (l ≥ 1) (4)</formula><p>where m is the number of ratings and l is the number of ratings higher than 6. Then, we define the scores Score LT RatingsP op (d, q) and Score LT RatingsRep (d, q):</p><formula xml:id="formula_4" coords="5,175.80,149.85,304.88,42.55">Score LT RatingsP op (d, q) = α 2 BM 25F (d, q) + (1 -α 2 )P op(d) (5) Score LT RatingsRep (d, q) = α 3 BM 25F (d, q) + (1 -α 3 )Rep(d)<label>(6)</label></formula><p>Users Similarity based approach (Score U sersSimilarity ): This approach is based on the similarity between users. The idea is that users who read liked the same books in the past are likely to like the same things in the future. So, we will recommend to the user who submit a topic, the books reviewed by similar users. For each book d, we calculate the score Sim(d, q) according to the similarity in the following manner:</p><formula xml:id="formula_5" coords="5,144.12,288.93,336.56,48.73">Sim(d, q) = max ui∈Reviewers(d) U sersSim(u q , u i ) if (Reviewers(d) = ∅) 0 else<label>(7) with:</label></formula><p>u q : The user who submit topic q.</p><p>-Reviewers(d): Users who have reviewed d. -U sersSim(u i , u j ): Similarity between the catalogs of the users u i and u j represented by two binary vectors (a component is set to one if the rating of the book is higher than 6).</p><p>The final score of each book is computed as follows:</p><formula xml:id="formula_6" coords="5,167.64,430.41,313.04,10.87">Score UsersSimilarity (d, q) = α 4 BM 25F (d, q) + (1 -α 4 )Sim(d, q)<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Results on INEX SBS 2013</head><p>The results of three of our social information retrieval models, presented in Table <ref type="table" coords="5,473.17,477.62,3.77,8.97">3</ref>, show that two of the three models improve slightly the results compared to the BM 25F model. The free parameters α 1 ,α 2 and α 3 have been optimized to respectively 0.5, 0.75, and 0.75. Note that our fourth model has not been optimized because INEX SBS 2013 collection does not have a large number of users profiles.</p><p>Table <ref type="table" coords="5,198.61,556.30,3.34,8.07">3</ref>. Optimization results of three social information retrieval models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Index Topic nDCG BM25F Index-All-Fields Topic-Title-Query 0.2132 ScoreAmazonRatings Index-All-Fields Topic-Title-Query 0.2129 ScoreLT RatingsP op Index-All-Fields Topic-Title-Query 0.2175 ScoreLT RatingsRep Index-All-Fields Topic-Title-Query 0.2145</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and results on INEX SBS 201Track</head><p>For our participation to INEX SBS 2014 track, we built six runs by applying the models that we optimize on INEX SBS 2013 collection and the model Score UsersSimilarity . These runs are summarized in Table <ref type="table" coords="6,287.29,163.10,4.98,8.97" target="#tab_2">4</ref> and their results are shown in Table <ref type="table" coords="6,446.53,163.10,3.77,8.97" target="#tab_3">5</ref>. With </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Model Index Topic HAFSI-324 BM25F Index-All-Fields Topic-Title-Query HAFSI-325 ScoreAmazonRatings Index-All-Fields Topic-Title-Query HAFSI-326 BM25F Index-All-Fields Topic-All-Fields HAFSI-328 ScoreLT RatingsP op Index-All-Fields Topic-Title-Query HAFSI-329 ScoreLT RatingsRep Index-All-Fields Topic-Title-Query HAFSI-345 Score U sersSimilarity Index-All-Fields Topic-All-Fields  <ref type="figure" coords="6,398.03,483.38,3.77,8.97" target="#fig_0">1</ref>. Our best run is the one that exploits the &lt;narrative&gt; field of the topic and uses BM25F model. Table <ref type="table" coords="6,134.76,507.38,4.98,8.97" target="#tab_3">5</ref> displays also the results of a post-INEX baseline obtained with the BM25 model queried with the &lt;title&gt; and &lt;query&gt; fields of the topic. The baseline is the sole run which does not use the &lt;review&gt; and &lt;tag&gt; field and their results are much worse. This was also observed with the SBS 2013 collection. There is a slight improvement in nDCG@10 and MRR when taking into account the user generated content. As in the 2013 results, taking into account the &lt;narrative&gt; topic field improves the results because the user information needs are sometimes better exposed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we described our participation to the INEX 2014 SBS track. We tested different approaches using social information: indexing of the book reviews and tags, querying with all topic fields, using scores based on book ratings and similarity between users. These approaches give interesting results, except the approach based on the similarity between users. This is probably due to the fact that we recommend to user the whole list of books appreciated by his similar users. It would have been interesting to filter this list with another kind of social information. Also, this approach should be improved by optimizing its parameters.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,251.28,325.90,112.77,8.07;7,134.76,116.48,345.70,194.54"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. INEX SBS 2014 results.</figDesc><graphic coords="7,134.76,116.48,345.70,194.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,134.76,412.06,345.75,106.71"><head>Table 1 .</head><label>1</label><figDesc>nDCG measures of BM25 and BM25F with different combinations of document and document fields.</figDesc><table coords="3,136.20,442.18,322.28,76.59"><row><cell>Model</cell><cell>Index</cell><cell cols="4">Topic-Title Topic-Query Topic-Title-Query Topic-All-field</cell></row><row><cell>BM25</cell><cell>Index-Title</cell><cell>0.0285</cell><cell>0.0377</cell><cell>0.0416</cell><cell>0.0725</cell></row><row><cell cols="3">BM25 Index-Summary 0.0527</cell><cell>0.0657</cell><cell>0.0692</cell><cell>0.0890</cell></row><row><cell cols="2">BM25 Index-Content</cell><cell>0.1005</cell><cell>0.1061</cell><cell>0.1263</cell><cell>0.1726</cell></row><row><cell>BM25</cell><cell>Index-Tags</cell><cell>0.1116</cell><cell>0.1115</cell><cell>0.1342</cell><cell>0.1628</cell></row><row><cell cols="2">BM25 Index-All-Field</cell><cell>0.1161</cell><cell>0.1296</cell><cell>0.1504</cell><cell>0.1991</cell></row><row><cell cols="2">BM25F Index-All-Field</cell><cell>-</cell><cell>-</cell><cell>0.2132</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,220.08,266.26,175.17,95.19"><head>Table 2 .</head><label>2</label><figDesc>Number of occurrences of book ratings.</figDesc><table coords="4,252.96,287.26,109.46,74.19"><row><cell cols="3">Rating Occurrences %</cell></row><row><cell>0</cell><cell>6</cell><cell>0 %</cell></row><row><cell>1</cell><cell>971,288</cell><cell>6.92 %</cell></row><row><cell>2</cell><cell>804,193</cell><cell>5.73 %</cell></row><row><cell>3</cell><cell cols="2">1,311,752 9.34 %</cell></row><row><cell>4</cell><cell cols="2">2,933,483 20.89 %</cell></row><row><cell>5</cell><cell cols="2">8,021,298 57.12 %</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,193.44,190.78,228.44,8.07"><head>Table 4 .</head><label>4</label><figDesc>Summary of submitted runs to INEX 2014 SBS Track.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,134.76,339.22,345.90,153.13"><head>Table 5 .</head><label>5</label><figDesc>INEX 2014 SBS results.</figDesc><table coords="6,134.76,358.42,345.90,133.93"><row><cell>Run</cell><cell>Model</cell><cell cols="2">Rank(40) nDCG@10 MRR MAP R@1000</cell></row><row><cell>HAFSI-326</cell><cell>BM25F</cell><cell>2</cell><cell>0.1424 0.2753 0.1070 0.4262</cell></row><row><cell cols="2">HAFSI-328 ScoreLT RatingsP op</cell><cell>13</cell><cell>0.1167 0.2255 0.0879 0.3923</cell></row><row><cell cols="2">HAFSI-329 ScoreLT RatingsRep</cell><cell>14</cell><cell>0.1161 0.2174 0.0866 0.3923</cell></row><row><cell cols="2">HAFSI-325 ScoreAmazonRatings</cell><cell>15</cell><cell>0.1153 0.2139 0.0873 0.3923</cell></row><row><cell>HAFSI-324</cell><cell>BM25F</cell><cell>17</cell><cell>0.1124 0.2136 0.0857 0.3923</cell></row><row><cell cols="2">HAFSI-345 Score U sersSimilarity</cell><cell>34</cell><cell>0.0524 0.1125 0.0369 0.3832</cell></row><row><cell>Baseline</cell><cell>-</cell><cell>-</cell><cell>0.0178 0.0410 0.0136 0.1164</cell></row><row><cell cols="4">the INEX 2014 SBS track official measures (nDCG@10), our six runs are ranked as</cell></row><row><cell>shown in</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,172.81,483.38,223.05,8.97"><head>Table 5 .</head><label>5</label><figDesc>Their rank/nDCG curves are presented in figure</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="1,144.72,656.86,214.53,8.07"><p>INEX SBS: https://inex.mmci.uni-saarland.de/tracks/books/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="2,144.72,656.86,88.55,8.07"><p>Terrier: http://terrier.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2" coords="3,144.72,656.86,179.49,8.07"><p>trec eval version 9.0: http://trec.nist.gov/trec eval/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,138.10,474.94,342.29,8.07;7,146.52,485.86,327.81,8.07" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,333.24,474.94,147.15,8.07;7,146.52,485.86,41.86,8.07">Overview of the INEX 2013 social book search track</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koolen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kazai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Preminger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,206.30,485.86,242.02,8.07">CLEF 2013 Evaluation Labs and Workshop, Online Working Notes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.10,496.78,342.59,8.07;7,146.52,507.82,289.54,8.07" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,414.84,496.78,61.18,8.07">Okapi at TREC&apos;4</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Payne</surname></persName>
		</author>
		<idno>TREC-4</idno>
	</analytic>
	<monogr>
		<title level="m" coord="7,158.64,507.82,174.35,8.07">The Fourth Text REtrieval Conference (TREC&apos;4)</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="73" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.10,518.74,342.45,8.07;7,146.52,529.66,334.03,8.07;7,146.52,540.58,101.05,8.07" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,293.03,518.74,184.42,8.07">Simple BM25 extension to multiple weighted fields</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,159.60,529.66,207.71,8.07;7,417.47,529.66,32.83,8.07">Conference on Information and Knowledge Management</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="42" to="49" />
		</imprint>
	</monogr>
	<note>CIKM&apos;04</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
