<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,148.91,116.95,317.54,12.62;1,286.52,134.89,42.33,12.62">Answering Natural Language Questions with Intui3</title>
				<funder ref="#_Vt9Ypr3">
					<orgName type="full">Collaborative Research Center &apos;</orgName>
				</funder>
				<funder>
					<orgName type="full">German Research Foundation (DFG)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,279.51,172.56,56.34,8.74"><forename type="first">Corina</forename><surname>Dima</surname></persName>
							<email>corina.dima@uni-tuebingen.de</email>
							<affiliation key="aff0">
								<orgName type="department">Seminar für Sprachwissenschaft</orgName>
								<orgName type="institution">University of Tübingen</orgName>
								<address>
									<addrLine>Wilhemstr. 19</addrLine>
									<postCode>72074</postCode>
									<settlement>Tübingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,148.91,116.95,317.54,12.62;1,286.52,134.89,42.33,12.62">Answering Natural Language Questions with Intui3</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">46C90BCD1981C981AC73AF9AB7241796</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>information retrieval</term>
					<term>question answering</term>
					<term>linked data</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Intui3 is one of the participating systems at the fourth evaluation campaign on multilingual question answering over linked data, QALD4. The system accepts as input a question formulated in natural language (in English), and uses syntactic and semantic information to construct its interpretation with respect to a given database of RDF triples (in this case DBpedia 3.9). The interpretation is mapped to the corresponding SPARQL query, which is then run against a SPARQL endpoint to retrieve the answers to the initial question. Intui3 competed in the challenge called Task 1: Multilingual question answering over linked data, which offered 200 training questions and 50 test questions in 7 different languages. It obtained an F-measure of 0.24 by providing a correct answer to 10 of the test questions and a partial answer to 4 of them.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Keyword-based search is the dominant search paradigm today. It permits computers to sift through the massive amounts of unstructured information available on the web and provide the users a ranked list of pages where the target information might be found. However, as remarked in the literature <ref type="bibr" coords="1,390.41,477.79,9.96,8.74" target="#b4">[5]</ref>, keyword-oriented search has a major drawback: it does not readily provide an answer, but a list of documents where the answer might be found. The user has to manually inspect each of the provided pages in order to find the actual answer.</p><p>The system described in this paper, Intui3, belongs to an alternative search paradigm, that takes a question formulated in natural language and provides a precise answer to it. The system accepts as input a syntactically correct natural language question and constructs its interpretation using syntactic and semantic cues in the question and a target triple store. The construction of a question interpretation is guided by Frege's Principle of Compositionality, namely the interpretation of a complex expression is determined by the interpretation of its constituent expressions and the rules used to combine them. The system uses an ontology, a predicate index and an entity index to construct the interpretations. In the current version all these components are related to the DBpedia 3.9 knowledge base, but with little effort they can be replaced, thus allowing the system to construct interpretations with respect to other knowledge bases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Previous work in Question Answering on structured data</head><p>Natural language questioning answering systems that serve as an interface to structured databases have a long history, going back to systems like BASE-BALL <ref type="bibr" coords="2,166.27,163.94,10.52,8.74" target="#b7">[8]</ref> and LUNAR <ref type="bibr" coords="2,242.38,163.94,15.50,8.74" target="#b13">[14]</ref> which were able to correctly process and answer natural language questions, but only on a very limited domain. More recent efforts ( <ref type="bibr" coords="2,161.13,187.85,14.76,8.74" target="#b11">[12]</ref>, <ref type="bibr" coords="2,182.53,187.85,15.49,8.74" target="#b15">[16]</ref>) have focused on learning to map a complex question to a logical form using an existing lexicon for connecting words to corresponding predicates in a database of facts. These systems focused on the depth of the analysis rather than on the breadth of the domain. Recently, however, there has been a shift from question answering on small databases to question answering on large knowledge bases ( <ref type="bibr" coords="2,165.13,247.62,14.76,8.74" target="#b12">[13]</ref>, <ref type="bibr" coords="2,187.47,247.62,10.79,8.74" target="#b0">[1]</ref>) or to open-domain question answering ( <ref type="bibr" coords="2,381.84,247.62,10.52,8.74" target="#b5">[6]</ref>). These approaches address the problem of automatically mapping a word sequence to a predicate in the database, as well as the one of constructing and combining possible interpretations. The system Intui3 follows in this trend, as it constructs detailed semantic interpretations for natural language questions by taking into account syntactic and semantic cues in the question and connecting them to the facts available in a specified triple store.</p><p>The current paper continues with Section 2 which gives an overview of the main third party resources used by Intui 3. Section 3 details all the steps required to construct a SPARQL interpretation of a natural language question. Section 4 presents the results of Intui3 on the training and test sets provided by the organizers of the QALD4<ref type="foot" coords="2,224.94,377.58,3.97,6.12" target="#foot_0">1</ref> challenge in Task 1, Multilingual question answering over linked data. A discussion of issues that affect the interpretation capabilities of the system and possible ways to address them follows in Section 5, which also concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Resources</head><p>Intui3 makes use of a series of third party resources to construct the interpretation of natural language questions. This section briefly introduces each of them. They are further referenced in Section 3 when describing the interpretation process.</p><p>Two natural language processing (NLP) suites are used: SENNA <ref type="bibr" coords="2,426.32,518.88,10.52,8.74" target="#b2">[3]</ref> and Stanford CoreNLP <ref type="bibr" coords="2,199.23,530.83,9.96,8.74" target="#b6">[7]</ref>. SENNA is a deep neural networks-based system that outputs a host of NLP predictions: part-of-speech (PoS) tags, chunking, name entity recognition (NER), semantic role labeling and syntactic parsing. SENNA's main interest points are its very high processing speed and its state-of-the-art performance. Intui3 uses SENNA (v3.0) for PoS tagging, chunking and performing NER on the input question.</p><p>The Stanford CoreNLP suite (v.3.2.0) is used in Intui3 for obtaining lemma information for each token. Stanford CoreNLP is a flexible NLP suite that offers multiple annotators such as PoS taggers, lemmatizers, named entity annotators, sentiment and coreference annotators, as well as annotators for constituency and dependency parsing. Multiple annotators can be combined and used to annotate the same input text on various levels.</p><p>The system combines the output from the two NLP suites. Initial tests on the QALD4 training set showed that SENNA (v3.0) provides the correct PoS tag labeling of a question more often than Stanford CoreNLP (v.3.2.0) does. SENNA also readily provides the chunking information, which the system uses as a support for constructing the semantic interpretation of the question. SENNA was thus chosen as the main NLP processing suite of the system. SENNA, however, does not offer lemma information, which was then obtained from the Stanford CoreNLP suite.</p><p>Intui3 uses a locally installed version of the DBpedia Lookup service<ref type="foot" coords="3,440.71,239.63,3.97,6.12" target="#foot_1">2</ref>  <ref type="bibr" coords="3,447.66,241.20,9.96,8.74" target="#b1">[2]</ref>. The DBpedia Lookup service provides an easy method for finding DBpedia URIs for a given sequence of words. It is implemented as a Web service and it is based on a Lucene index that provides a weighted label lookup. It combines string similarity with relevance ranking in order to find the most likely matches for a given word or word sequence. To illustrate the advantage of using the DBpedia Lookup service instead of simple string matching techniques we use Q21 from the QALD4 Task 1 test set, Where was Bach born? A query for the string Bach in the triple store directly would result in a multitude of pages referring either to Bach himself, to his opera or to various other entities that are named after Bach. The first match returned by the DBpedia Lookup service is the URI http://dbpedia.org/resource/Johann_Sebastian_Bach, which is, in the case of Q21, the correct choice.</p><p>We compiled a list of demonyms and their corresponding country by scraping the information about demonyms that can be found on Wikipedia<ref type="foot" coords="3,433.90,407.83,3.97,6.12" target="#foot_2">3</ref> . The list contains about 600 pairs of the form (demonym, country). This information is also available in DBpedia but the current version (DBpedia 3.9) contains only 440 concepts that are marked as demonym of a country.</p><p>The system uses a predicate index obtained by selecting all the predicates in a local DBpedia 3.9 install that was used for obtaining results for the QALD4 challenge. This index contains 49,714 predicates, most of them from the dbpedia. org/property namespace (48, 294).</p><p>Intui3 computes similarities between words in the question and predicates in the triple store using the word similarity measures implemented in the WordNet Similarity for Java<ref type="foot" coords="3,216.01,529.04,3.97,6.12" target="#foot_3">4</ref> (WS4J) library. All the similarity measures in the package were tested using a set of pairs of the form (word, dbpedia predicate). The focus of this test was to choose a similarity measure that: (i) is able to score pairs of words with different PoS tags; (ii) provides a high score when the word and the predicate are related and a low score when they are not. The measure that fulfilled both criteria was the Hirst &amp; St. Onge similarity measure <ref type="bibr" coords="3,420.27,590.39,9.96,8.74" target="#b8">[9]</ref>. It is based on an idea that two lexicalized concepts are semantically close if their WordNet synsets are connected by a path that is not too long and that "does not change direction too often".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Interpreting a Natural Language Question</head><p>This section gives a detailed overview of the steps made by Intui3 to interpret a natural language question and to map it to a syntactically correct SPARQL query. The interpretation process involves the following steps: first, the question is tokenized, PoS tagged, and the named entities are identified. Then the question is split into chunks, and the systems assigns initial interpretations to each chunk. The interpretation of the question is constructed by combining the interpretations assigned to each chunk.</p><p>We will use question with id 12, Q12: How many pages does War and Peace have? from the QALD4 Task1 test set to explain in more detail the interpretation process.</p><p>Each question received by the system is pre-processed using a series of standard NLP tools. The SENNA and Stanford CoreNLP suites are used to process the question. The system stores information related to PoS tagging, NER and chunking from the output produced by SENNA and lemma information from the output of Stanford CoreNLP. Next, the system performs a demonym resolution step by looking up each individual token in the question in a list of demonyms and their associated countries (see Section 2 for more details). If a demonym is found, such as the word German in the phrase German lake, the system retrieves and stores the DBpedia URI identifying the referred country (in this case http://dbpedia.org/resource/Germany). Table <ref type="table" coords="4,356.35,420.04,4.98,8.74" target="#tab_0">1</ref> shows the information obtained in the pre-processing step for Q12. All the information obtained in the pre-processing phase is stored using the tokens as a reference entity. Intui3 uses sentence chunks as a basis for constructing interpretations. That is why establishing appropriate chunk boundaries is a crucial step towards constructing the correct interpretation. To this end the system combines the chunk information it has obtained from the SENNA chunker and from the SENNA NER system. Depending on the returned chunks and their associated PoS information, the system can choose to split the provided chunks, or to combine several provided chunks into a larger chunk.</p><p>An example of splitting an existing chunk is given in the last column of Table <ref type="table" coords="5,162.77,168.34,3.87,8.74" target="#tab_0">1</ref>, where the chunk [NP How many pages] recognized by the chunker is further split into [WHADJP How many] and [NP pages].</p><p>The converse situation of the system combining multiple chunks can be illustrated using Q42 in the QALD4 Task1 test set, What is the official color of the University of Oxford?. The processing continues with the interpretation of the chunks obtained in the pre-processing step. The system analyses each chunk individually and assigns one or more interpretations depending on the chunk's type and on the additional semantic and syntactic information available for that chunk. An overview of the types of interpretations that can be assigned by the system is presented below. In all the descriptions, the terms subject, object and predicate refer to the elements of an RDF triple.</p><p>• functional interpretations define a functor such as count, min or max ; such interpretations are triggered by lexical cues (e.g. how many). • concept interpretations are used to model class membership via the rdf:type <ref type="foot" coords="5,476.12,408.15,3.97,6.12" target="#foot_4">5</ref>property; they are triggered by noun phrases that contain plural nouns like languages and are mapped to RDF triples like (?answer, rdf:type, http://dbpedia.org/ontology/Language). The set of reference classes is defined by the DBpedia ontology <ref type="foot" coords="5,300.03,455.97,3.97,6.12" target="#foot_5">6</ref> . The DBpedia ontology defines a hierarchy of 529 classes that include both top-level concepts such as Person, Organization, Event, Place, but also more refined concepts such as Athlete, Company, SportsEvent and Mountain • subject interpretations are used to map a word sequence to a subject triple as in (http://dbpedia.org/resource/War_and_Peace, ?p, ?o); object interpretations are used to map a word sequence to an object triple as in (?s, ?p, http://dbpedia.org/resource/War_and_Peace); such interpretations are triggered by noun phrases such as War and Peace, the official color, etc.; when a chunk that requires a subject or object interpretation is discovered both subject and object interpretations are generated; this ensures that the system has a chance to investigate all the triples that contain a particular URI in a subject or object position; subject and object interpretations are based on the output returned by the DBpedia Lookup service described in Section 2; if the associated chunk was tagged with the NER label PER, the system filters the results returned by the DBpedia Lookup service to have the rdf:type dbo:Person, foaf:Person or yago:Person. Similarly, the rdf:type of the chunks with the NER label LOC is restricted to dbo:Place. The MISC and ORG NER labels are too coarse to create such restrictions. • empty interpretations are used to signal the fact that the mapped word sequence does not have a meaning on its own; the triggers for empty interpretations are function words such as me, I, do, does, etc. • predicate interpretations are used to map a word or word sequence in the question to a predicate that is available in the triple store; predicate interpretations are triggered either by verbs (e.g. spoken), common nouns (e.g. pages, ingredients) or noun phrases (e.g. programming languages, the official color ); as opposed to subject, object or concept interpretations, the predicate interpretations are not immediately resolved, as their interpretation is highly dependent on the exact question context; instead, the predicate interpretation stores a predicate pattern that is taken into account in the interpretation process; • triple interpretations are used to map a sequence of words to an RDF triple;</p><p>subject, object and concept interpretations are all instances of triple interpretations; • query interpretations are used to map a sequence of words to a SPARQL query; query interpretations are the most complex type of interpretation that can be constructed by the system, and are obtained through composition from the other types of interpretations Each type of interpretation comes with a set of combination rules. These rules define how the current interpretation can be combined with any other type of interpretation and also construct the combined interpretation.</p><p>Table <ref type="table" coords="6,177.56,437.15,4.98,8.74" target="#tab_1">2</ref> presents all the interpretations that have been initially assigned to the chunks in our running example, Q12: How many pages does War and Peace have? The next step involves traversing all the chunks in a right-to-left order and combining the interpretations. I Q12 = [I how many , I pages , I does , I W ar and P eace , I have ]</p><p>(1)</p><p>First, the interpretations of rightmost two chunks (I W ar and P eace and I have ) are combined by considering each possible pair of interpretations of the two chunks in the order they appear in the sentence. The possible number of interpretations is the product of the number of interpretations for each chunk. In our case, |I W ar and P eace | = 10 and |I have | = 1, so there are 10 * 1 = 10 possible combinations. In this particular case, I have contains only the empty interpretation, so the interpretations that result by combining I W ar and P eace and I have are the interpretations in I W ar and P eace .</p><p>For each subsequent step we combine the results of the previous combination with the rightmost interpretation that has not yet been combined. In our case, we combine I does with the result of combining I W ar and P eace and I have , which was I W ar and P eace . I does contains again only the empty interpretation, so the result of this combination is still only I W ar and P eace .</p><p>In the next step we combine I pages with the results of the previous combinations, in our case I W ar and P eace . This time we have |I pages | * |I W ar and P eace | = 20 combined interpretations. The combination of a subject and a predicate interpretation involves querying the triple store for all the triples with the given subject and extracting a set of predicates that co-occurred with the subject. All the extracted predicates in this set are scored with respect to the pattern provided in the predicate interpretation. In our example, the system looks for all the triples with the subject http://dbpedia.org/resource/War_and_Peace, extracts the set of predicates it occurs with (25 different predicates), and scores all of them with respect to the predicate pattern (page or pages).</p><p>The scoring mechanism uses two scorers chained together: first a string similarity scorer, then a WordNet-based similarity scorer which uses the Hirst-St.Onge similarity measure (see Section 2). Chaining allows the system to look first for lexical similarities between the candidate predicates and the provided pattern, and if the lexical similarity is too low, to look for similarity at the semantic level. This allows the system to find both the lexical similarity between the predicate http://dbpedia.org/property/pages and the pattern pages and the semantic similarity between http://dbpedia.org/property/spouse and the pattern husband. The system collects all the candidate predicates that score above a specified threshold and constructs query interpretations by combining the predicate interpretations for I pages with the subject/object interpretations for I W ar and P eace . 13 query interpretations are constructed in this particular case, one of them displayed in Listing 1.1. All the combined interpretations are scored by multiplying the scores of the initial interpretations. The last step made by the system in the particular case of Q12 is combining the interpretations it has obtained until this point with I how many . The functional:count interpretation can only be combined with a query interpretation and requires a numeric answer. To obtain the combined interpretation, the system first runs the query interpretation and retrieves the answers. If there is only one answer that can be cast to a number, then the combined interpretation will be the existing query interpretation. Otherwise, the system constructs the SPARQL representation of the combined interpretation by attaching a count clause to the existing query.</p><p>The system outputs the query interpretation with the highest score as the final interpretation, or OUT OF SCOPE if the final interpretation is an empty interpretation. If the final interpretation is any of the other types of interpretation, then the system cannot construct a valid interpretation for that particular question and thus cannot answer the question.</p><p>In the case of Q12: How many pages does War and Peace have?, the system chooses the query in Listing 1.1, which provides the correct answer, 1225 pages. The results of the system on the training and the test set are summarized in Table <ref type="table" coords="8,162.69,505.28,3.87,8.74" target="#tab_2">3</ref>. Table <ref type="table" coords="8,202.20,505.28,4.98,8.74">4</ref> presents the questions answered correctly or partially correct by Intui3, as well as their individual scores. parser would have no possibility to construct the correct parse and the system would fail to provide any interpretations. The SENNA chunker, on the other hand, provided to be more robust: in some cases the system generated an incorrect PoS tag for a word in the chunk, but the chunk itself was correctly labeled. The heuristic for combining interpretations can be improved by constructing interpretations in both directions and choosing the overall best scoring one. Another avenue worth investigating is to use the output of a dependency parser as a method for choosing the order in which the chunk interpretations should be combined. • In the current system the algorithm for refining chunk boundaries uses handwritten rules; a better solution be to develop a dedicated chunker that can provide the correct chunk boundaries for constructing interpretations. • The test set included many questions that have noun phrases containing superlative adjectives (e.g. the tallest player, the most books, the youngest Darts player, etc.). The current system is not equipped to construct an interpretation for such cases. Such phrases are correctly interpreted only if a dedicated predicate for that phrase exists in the knowledge base (e.g. the predicate largestCity for an entity of type country in Q31). • The system is not currently equipped to handle questions that require a</p><p>Yes/No answer.</p><p>Intui3 is restricted to answering questions in English. Porting the system to work with other languages is possible, although restricted to those languages that have all the NLP tools required by Intui3 (PoS tagger, NER system, chunker, lemmatizer, a wordnet and the corresponding similarity measures). Another necessary resource is an instance of the DBpedia Lookup service customized for the target language.</p><p>We plan to further improve the system by taking into account all the issues presented above. Another goal is to test the adaptability of the system by trying to answer questions with respect to other freely available knowledge bases, using already existing sets of questions and answers like the ones described in <ref type="bibr" coords="10,450.72,473.80,10.52,8.74" target="#b0">[1]</ref> and <ref type="bibr" coords="10,134.77,485.76,14.61,8.74" target="#b14">[15]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,235.39,216.70,245.20,8.74;5,134.77,228.65,98.27,8.74;5,257.34,228.65,223.26,8.74;5,134.77,240.61,345.83,8.74;5,134.77,252.56,345.84,8.74;5,134.77,264.52,345.83,8.74;5,134.77,276.47,46.72,8.74"><head></head><label></label><figDesc>The phrase the University of Oxford is initially chunked as [NP the University] of ] [NP Oxford]. The NER system identifies in the same phrase a named entity of type organization: the [ORG University of Oxford]. Intui3 combines the information from the chunker and the NER system and merges the three initial chunks into a single NP chunk, [NP the University of Oxford].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,147.68,406.12,318.91,7.89;7,133.86,421.08,114.61,6.14;7,132.96,429.05,27.45,6.14;7,135.11,437.04,4.08,6.12;7,157.62,445.01,206.62,6.12;7,157.62,452.98,168.50,6.12;7,159.10,460.95,41.13,6.12;7,135.11,468.92,4.08,6.12"><head>Listing 1 . 1 .</head><label>11</label><figDesc>Final query for Q12: How many pages does War and Peace have? SELECT DISTINCT ? answer WHERE { &lt;h t t p : / / d b p e d i a . o r g / r e s o u r c e / War and Peace&gt; &lt;h t t p : / / d b p e d i a . o r g / p r o p e r t y / pages&gt; ? answer . }</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,134.77,476.30,344.73,104.69"><head>Table 1 .</head><label>1</label><figDesc>Question pre-processing for Q12: How many pages does War and Peace have?</figDesc><table coords="4,185.07,499.55,245.23,81.43"><row><cell cols="7">Token Lemma PoS Demonym NER SENNA-Chunk Intui3-Chunk</cell></row><row><cell>How</cell><cell cols="2">how WRB</cell><cell>-</cell><cell>O</cell><cell>B-NP</cell><cell>B-WHADJP</cell></row><row><cell cols="2">many many</cell><cell>JJ</cell><cell>-</cell><cell>O</cell><cell>I-NP</cell><cell>E-WHADJP</cell></row><row><cell cols="3">pages page NNS</cell><cell>-</cell><cell>O</cell><cell>E-NP</cell><cell>S-NP</cell></row><row><cell>does</cell><cell>do</cell><cell>VBZ</cell><cell>-</cell><cell>O</cell><cell>S-VP</cell><cell>S-VP</cell></row><row><cell>War</cell><cell cols="2">War NNP</cell><cell>-</cell><cell>B-MISC</cell><cell>B-NP</cell><cell>B-NP</cell></row><row><cell>and</cell><cell>and</cell><cell>CC</cell><cell>-</cell><cell>I-MISC</cell><cell>I-NP</cell><cell>I-NP</cell></row><row><cell cols="3">Peace Peace NNP</cell><cell>-</cell><cell>E-MISC</cell><cell>E-NP</cell><cell>E-NP</cell></row><row><cell cols="3">have have VBP</cell><cell>-</cell><cell>O</cell><cell>S-VP</cell><cell>S-VP</cell></row><row><cell>?</cell><cell>?</cell><cell>.</cell><cell>-</cell><cell>O</cell><cell>O</cell><cell>O</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,134.77,164.74,364.05,324.71"><head>Table 2 .</head><label>2</label><figDesc>Interpretations for Q12: How many pages does War and Peace have? The question mark does not have an interpretation as it was not assigned to a chunk.</figDesc><table coords="8,134.77,205.18,364.05,284.27"><row><cell>Phrase</cell><cell cols="2">Phrase Type Interpretation</cell></row><row><cell>How many</cell><cell>WHADJP</cell><cell>Functional:COUNT</cell></row><row><cell>pages</cell><cell>NP</cell><cell>Predicate: pattern=page</cell></row><row><cell></cell><cell></cell><cell>Predicate: pattern=pages</cell></row><row><cell>does</cell><cell>VP</cell><cell>Empty</cell></row><row><cell cols="2">War and Peace NP</cell><cell>Subject [http://dbpedia.org/resource/War_and_Peace: 0.70]</cell></row><row><cell></cell><cell></cell><cell>Subject [http://dbpedia.org/resource/War_and_Peace_(opera): 0.33]</cell></row><row><cell></cell><cell></cell><cell>Subject [http://dbpedia.org/resource/Paris_Peace_Conference,_1919: 0.29]</cell></row><row><cell></cell><cell></cell><cell>Subject [http://dbpedia.org/resource/Peace_and_conflict_studies: 0.23]</cell></row><row><cell></cell><cell></cell><cell>Subject [http://dbpedia.org/resource/Peace_movement: 0.20]</cell></row><row><cell></cell><cell></cell><cell>Object [http://dbpedia.org/resource/War_and_Peace: 0.70]</cell></row><row><cell></cell><cell></cell><cell>Object [http://dbpedia.org/resource/War_and_Peace_(opera): 0.33]</cell></row><row><cell></cell><cell></cell><cell>Object [http://dbpedia.org/resource/Paris_Peace_Conference,_1919: 0.29]</cell></row><row><cell></cell><cell></cell><cell>Object [http://dbpedia.org/resource/Peace_and_conflict_studies: 0.23]</cell></row><row><cell></cell><cell></cell><cell>Object [http://dbpedia.org/resource/Peace_movement: 0.20]</cell></row><row><cell>have</cell><cell>VP</cell><cell>Empty</cell></row><row><cell cols="3">4 System Results in QALD4: Task1</cell></row><row><cell cols="3">Intui3 was evaluated on the training and test sets offered by the organizers of</cell></row><row><cell cols="3">QALD4 in Task 1, Multilingual question answering over linked data. Although</cell></row><row><cell cols="3">the organizers offer questions in seven languages, Intui3 can only interpret ques-</cell></row><row><cell cols="3">tions written in English.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,134.77,550.04,365.28,77.04"><head>Table 3 .</head><label>3</label><figDesc>Intui3 results on the QALD4 Task 1 training and test sets. The reported recall, precision and F-score refer to the results obtained by the system on the whole test/training set.</figDesc><table coords="8,139.02,601.43,361.03,25.64"><row><cell>Dataset</cell><cell cols="7">Size Constructed queries Correct Partially correct Recall Precision F-measure</cell></row><row><cell>QALD4 test</cell><cell>50</cell><cell>33</cell><cell>10</cell><cell>4</cell><cell>0.24</cell><cell>0.23</cell><cell>0.24</cell></row><row><cell cols="2">QALD4 train 200</cell><cell>137</cell><cell>38</cell><cell>5</cell><cell>0.20</cell><cell>0.20</cell><cell>0.20</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.73,657.80,312.77,8.11"><p>QALD4 challenge website: http://www.sc.cit-ec.uni-bielefeld.de/qald/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,144.73,624.92,272.91,8.11"><p>The code was obtained from https://github.com/dbpedia/lookup</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,144.73,635.88,335.86,8.11;3,144.73,647.48,178.87,7.47"><p>List of demonyms scraped from http://en.wikipedia.org/wiki/Adjectivals_ and_demonyms_for_countries_and_nations</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="3,144.73,657.80,304.60,8.11"><p>The code for WS4J was obtained from https://code.google.com/p/ws4j/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="5,144.73,647.48,222.72,7.47"><p>http://www.w3.org/1999/02/22-rdf-syntax-ns#type</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="5,144.73,658.44,150.63,7.47"><p>http://wiki.dbpedia.org/Ontology</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. The research leading to these results has received funding from the <rs type="funder">German Research Foundation (DFG)</rs> as part of the <rs type="funder">Collaborative Research Center '</rs><rs type="projectName">Emergence of Meaning</rs>' (<rs type="grantNumber">SFB 833</rs>). The author would like to thank <rs type="person">Emanuel Dima</rs> for his comments on the initial version of the paper, as well as the anonymous reviewers for their suggestions.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_Vt9Ypr3">
					<idno type="grant-number">SFB 833</idno>
					<orgName type="project" subtype="full">Emergence of Meaning</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Future Work</head><p>There are several types of issues that prevented the system from having a better coverage with respect to the provided datasets. We list them below, together with possible methods for alleviating them:</p><p>• The system relies heavily on the part-of-speech tagging information when it chooses a possible interpretation for a chunk. The prediction of a part-ofspeech tagger can, however, be incorrect, especially in the case of questions.</p><p>For example in the case of Q7, Does the Isar flow into a German lake? the part-of-speech tagger assigned the label NN to the verb flow, thus preventing a correct chunking ([the Isar flow] was analyzed as one NP chunk). Such mistakes might seem unlikely from a system like SENNA that reports an per-word accuracy of 97.29%. But, as noted in <ref type="bibr" coords="9,355.89,490.03,14.61,8.74" target="#b10">[11]</ref>, although PoS tagging is considered a "solved" problem due to systems that report over 97% per-word accuracy, the same systems only achieve 55 to 57% per-sentence accuracy.</p><p>A better performance on PoS tagging questions might be obtained by retraining the PoS tagger using both question and non-question data. Such approaches have been shown to significantly improve parsing performance on question data <ref type="bibr" coords="9,227.86,561.76,14.61,8.74" target="#b9">[10]</ref>. • The purely right-to-left method of combining the interpretations does not always provide the best method of constructing the question's interpretation. For example in the case of Q11, Give me all animals that are extinct, the interpretation should start from the left rather than from the right. The motivation for combining the interpretations using the such heuristics instead of the output of a parser is given by our previous experience in the QALD3 challenge. There our system <ref type="bibr" coords="9,277.24,645.16,10.52,8.74" target="#b3">[4]</ref> used the output of a constituency parser to construct interpretations. However, as the PoS tags were often incorrect, the</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="10,142.96,614.32,337.62,7.86;10,151.52,625.27,230.39,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,337.95,614.32,142.63,7.86;10,151.52,625.27,85.60,7.86">Semantic parsing on freebase from question-answer pairs</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,257.95,625.27,95.29,7.86">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,635.88,337.62,7.86;10,151.52,646.84,329.06,7.86;10,151.52,657.80,303.24,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,195.28,646.84,212.04,7.86">Dbpedia-a crystallization point for the web of data</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hellmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,415.63,646.84,64.95,7.86;10,151.52,657.80,217.23,7.86">Web Semantics: Science, Services and Agents on the World Wide Web</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="154" to="165" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,120.67,337.62,7.86;11,151.52,131.63,329.06,7.86;11,151.52,142.59,123.84,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,462.40,120.67,18.17,7.86;11,151.52,131.63,187.26,7.86">Natural language processing (almost) from scratch</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,345.67,131.63,134.91,7.86;11,151.52,142.59,35.78,7.86">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,153.55,337.62,7.86;11,151.52,164.51,329.06,7.86;11,151.52,175.47,25.60,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,196.30,153.55,280.18,7.86">Intui2: A prototype system for question answering over linked data</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,151.52,164.51,329.06,7.86">Proceedings of the Question Answering over Linked Data lab (QALD-3) at CLEF</title>
		<meeting>the Question Answering over Linked Data lab (QALD-3) at CLEF</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,186.42,289.40,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,201.31,186.42,96.00,7.86">Search needs a shake-up</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,304.60,186.42,27.92,7.86">Nature</title>
		<imprint>
			<biblScope unit="volume">476</biblScope>
			<biblScope unit="issue">7358</biblScope>
			<biblScope unit="page" from="25" to="26" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,197.38,337.62,7.86;11,151.52,208.34,203.95,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,315.02,197.38,165.55,7.86;11,151.52,208.34,57.02,7.86">Paraphrase-driven learning for open question answering</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,229.92,208.34,19.06,7.86">ACL</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1608" to="1618" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,219.30,337.62,7.86;11,151.52,230.26,329.06,7.86;11,151.52,241.22,329.06,7.86;11,151.52,252.18,179.69,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,317.61,219.30,162.97,7.86;11,151.52,230.26,205.60,7.86">Incorporating non-local information into information extraction systems by gibbs sampling</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,381.18,230.26,99.41,7.86;11,151.52,241.22,247.51,7.86">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,263.14,337.62,7.86;11,151.52,274.10,329.06,7.86;11,151.52,285.05,243.22,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,386.59,263.14,93.99,7.86;11,151.52,274.10,70.25,7.86">Baseball: an automatic question-answerer</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">F</forename><surname>Green</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Chomsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Laughery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,242.48,274.10,133.68,7.86">Papers presented at the May 9-11</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1961">1961. 1961</date>
			<biblScope unit="page" from="219" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,296.01,337.62,7.86;11,151.52,306.97,329.06,7.86;11,151.52,317.93,42.49,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,242.89,296.01,237.68,7.86;11,151.52,306.97,122.85,7.86">Lexical chains as representations of context for the detection and correction of malapropisms</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hirst</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>St-Onge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,281.20,306.97,159.83,7.86">WordNet: An electronic lexical database</title>
		<imprint>
			<biblScope unit="volume">305</biblScope>
			<biblScope unit="page" from="305" to="332" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,328.89,337.96,7.86;11,151.52,339.85,329.06,7.86;11,151.52,350.81,329.06,7.86;11,151.52,361.77,329.05,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,311.53,328.89,169.05,7.86;11,151.52,339.85,78.73,7.86">Questionbank: Creating a corpus of parseannotated questions</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Judge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cahill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Van Genabith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,250.19,339.85,230.39,7.86;11,151.52,350.81,329.06,7.86;11,151.52,361.77,75.45,7.86">Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="497" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,372.73,337.96,7.86;11,151.52,383.68,329.06,7.86;11,151.52,394.64,81.96,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,219.89,372.73,260.69,7.86;11,151.52,383.68,44.39,7.86">Part-of-speech tagging from 97% to 100%: is it time for some linguistics?</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,211.31,383.68,229.55,7.86">Computational Linguistics and Intelligent Text Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="171" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,405.60,337.96,7.86;11,151.52,416.56,329.06,7.86;11,151.52,427.52,81.96,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,263.84,405.60,216.73,7.86;11,151.52,416.56,138.32,7.86">Using multiple clause constructors in inductive logic programming for semantic parsing</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,311.85,416.56,126.28,7.86">Machine Learning: ECML 2001</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="466" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,438.48,337.96,7.86;11,151.52,449.44,329.05,7.86;11,151.52,460.40,291.88,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,165.54,449.44,198.21,7.86">Template-based question answering over rdf data</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Unger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bühmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Ngonga Ngomo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cimiano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,385.65,449.44,94.92,7.86;11,151.52,460.40,179.52,7.86">Proceedings of the 21st international conference on World Wide Web</title>
		<meeting>the 21st international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,471.36,337.96,7.86;11,151.52,482.32,329.06,7.86;11,151.52,493.27,150.82,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,210.43,471.36,270.14,7.86;11,151.52,482.32,27.77,7.86">Progress in natural language understanding: an application to lunar geology</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">A</forename><surname>Woods</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,200.79,482.32,111.53,7.86">Proceedings of the June 4-8</title>
		<title level="s" coord="11,343.53,482.32,137.05,7.86;11,151.52,493.27,39.61,7.86">national computer conference and exposition</title>
		<meeting>the June 4-8</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1973">1973. 1973</date>
			<biblScope unit="page" from="441" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,504.23,337.96,7.86;11,151.52,515.19,329.06,7.86;11,151.52,526.15,86.01,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,257.52,504.23,223.06,7.86;11,151.52,515.19,51.66,7.86">Learning to parse database queries using inductive logic programming</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Zelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,223.56,515.19,253.31,7.86">Proceedings of the National Conference on Artificial Intelligence</title>
		<meeting>the National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="1050" to="1055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,537.11,337.96,7.86;11,151.52,548.07,329.06,7.86;11,151.52,559.03,329.06,7.86;11,151.52,569.99,196.68,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="11,274.48,537.11,206.10,7.86;11,151.52,548.07,59.87,7.86">Online learning of relaxed ccg grammars for parsing to logical form</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,248.69,548.07,231.89,7.86;11,151.52,559.03,329.06,7.86;11,151.52,569.99,127.38,7.86">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL-2007</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL-2007</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
