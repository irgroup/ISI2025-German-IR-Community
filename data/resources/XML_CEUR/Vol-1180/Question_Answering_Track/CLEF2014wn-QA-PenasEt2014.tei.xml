<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,144.98,152.67,305.39,12.64">Overview of CLEF QA Entrance Exams Task 2014</title>
				<funder ref="#_C8Guesm #_WEH8jwt #_Fsh8QSs">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,127.82,191.98,74.24,10.80"><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
							<email>anselmo@lsi.uned.es</email>
							<affiliation key="aff0">
								<orgName type="department">NLP&amp;IR group</orgName>
								<orgName type="institution">UNED</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,212.21,191.98,70.79,10.80"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
							<email>yusuke@nii.ac.jp</email>
							<affiliation key="aff1">
								<orgName type="institution">National Institute of Informatics</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,293.09,191.98,75.59,10.80"><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
							<email>alvarory@lsi.uned.es</email>
							<affiliation key="aff0">
								<orgName type="department">NLP&amp;IR group</orgName>
								<orgName type="institution">UNED</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,378.91,191.98,64.31,10.80"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
							<email>hovy@cmu.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,261.17,205.90,68.88,10.80"><forename type="first">Noriko</forename><surname>Kando</surname></persName>
							<email>kando@nii.ac.jp</email>
							<affiliation key="aff1">
								<orgName type="institution">National Institute of Informatics</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,144.98,152.67,305.39,12.64">Overview of CLEF QA Entrance Exams Task 2014</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8A9B01E0A0B6E05774F782EB34E2A2F5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the Entrance Exams task at the CLEF QA Track 2014. Following 2013 edition, the data set has been extracted from actual university entrance examinations including a variety of topics and question types. Systems receive a set of Multiple-Choice Reading Comprehension tests where the task is to select the correct answer among a finite set of candidates, according to the given text. Questions are designed originally for testing human examinees, rather than evaluating computer systems. Therefore, the data set challenges human ability to show their understanding of texts. Thus, questions and answers are lexically distant from their supporting excerpts in text, requiring not only a high degree of textual inference, but also the development of strategies for selecting the correct answer. As a novelty this year, data sets originally in English were manually translated into Russian, French, Spanish and Italian.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>Following 2013 edition, the Entrance Exams task at CLEF QA Track 2014 is focused on solving Reading Comprehension tests of English examinations. Reading Comprehension tests are routinely used to assess the degree to which people comprehend what they read, so we work with the hypothesis that it is reasonable to use these tests to assess the degree to which a machine "comprehends" what it is reading. Despite the difficulty of the challenge, we believe we are building a real benchmark that will serve to measure real progress in the field during the next years.</p><p>With this goal in mind, CLEF and NTCIR started collaboration in 2013 around the idea of testing systems against University Entrance Exams, the same exams humans have to pass to enter University. The data set was prepared and distributed by NTCIR, while other organization efforts, including announcements, collecting and evaluating submissions, etc. were managed by CLEF. The success of this coordination also owes to the standard data format and evaluation methodology followed in past editions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">TASK DESCRIPTION</head><p>Participant systems are asked to read a given document and answer a set of questions. Questions are given in multiple-choice format, with several options from which a single answer must be selected. Systems have to answer questions by referring to "common sense knowledge" that high school students who aim to enter the university are expected to have. Another important difference is that we do not intend to restrict question types. Any type of reading comprehension questions in real entrance exams will be included in the test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DATA</head><p>Japanese University Entrance Exams include questions formulated at various levels of complexity and test a wide range of capabilities. The challenge of "Entrance Exams" aims at evaluating systems under the same conditions that humans are evaluated to enter the University.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sources</head><p>The data set is extracted from standardized English examinations for university admission in Japan. Exams are created by the Japanese National Center for University Admissions Tests. Original examinations include various styles of questions, such as word filling, grammatical error recognition, sentence filling, etc.</p><p>One of such styles is reading comprehension; a test provides a text that describes some daily life situation, and questions about the text are asked. As in the previous edition, we reduced the challenge to these Reading Comprehension exercises contained in the English exams, leaving other types of exercises available for future tasks.</p><p>For each examination, one text is given, and five questions on the given text are asked. Each question has four choices. For this year campaign, we reused as development data 12 examinations from last year campaign. Besides, we provided new 12 documents, 60 questions and 240 candidate answers to be validated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Languages</head><p>As a novelty this year, data sets for development and testing originally in English were manually translated into Russian, French, Spanish and Italian. They are parallel translations of texts, questions and candidate answers.</p><p>In addition to the official data, we collected four more unoffcial translations into French. Despite they preserve original meaning, each translation has its particularities that produce different effects on systems performance: text simplification, lexical variation, different uses of anaphora, overall quality, etc. This data is extremely useful to get insights about systems and their level of inference. Synapse <ref type="bibr" coords="2,402.76,660.02,11.69,8.96" target="#b2">[3]</ref> reports some initial experiments with this unofficial data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION</head><p>Scoring of the output produced by participant systems was performed automatically by comparing the answers of systems against the gold standard collection with annotations made by humans. No manual assessment was performed.</p><p>Each test receives an evaluation score between 0 and 1 using c@1 <ref type="bibr" coords="3,434.83,211.14,10.69,8.96" target="#b0">[1]</ref>. This measure, used in previous CLEF QA Tracks, encourages systems to reduce the number of incorrect answers while maintaining the number of correct ones by leaving some questions unanswered. Systems received evaluation scores from two different perspectives:</p><p>1. At the question-answering level: correct answers are counted individually without grouping them 2. At the reading-test level: figures for each reading test as a whole are given.</p><p>A test is considered to be passed if a c@1 score above 0.5 is reached. Then, the proportion of tests that are passed is given as a global score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>Table <ref type="table" coords="3,150.72,392.85,4.98,8.96" target="#tab_0">1</ref> enumerates the participating groups and their reference paper in CLEF 2014 Working Notes. Only LIMSI-CNRS has participated in the two editions and only one team (Synapse) has participated in a second language different than English (French). Results are summarized in Tables <ref type="table" coords="3,267.41,553.67,4.98,8.96" target="#tab_1">2</ref> and<ref type="table" coords="3,293.92,553.67,4.98,8.96">3</ref> for the QA and for Reading perspectives respectively. According to Table 2, the system with higher score (Synapse for French <ref type="bibr" coords="4,446.12,424.29,11.28,8.96" target="#b2">[3]</ref>) is the unique system that answered more questions correctly than incorrectly. Only few runs made use of the leaving questions unanswered option. In these cases, despite some systems reduced considerably the amount of incorrect answers, none of them could improve their overall c@1 score.</p><p>Table <ref type="table" coords="4,178.80,481.79,4.98,8.96">3</ref> shows results for the reading perspective. First column corresponds to systems run id, second column to the overall c@1 obtained, third column shows the number of tests that the systems have passed if we consider the threshold of 0.5, and the rest of columns correspond to the c@1 value for each particular test. 0.29 3/12 0.5 0.17 0.25 0.5 0.33 0.2 0 0.2 0.2 0 0.6 0.4 DIPF-5 0.29 3/12 0.25 0.5 0.25 0.5 0.67 0.4 0 0.4 0.2 0 0 0.2 DIPF-3 0.29 4/12 0 0.17 0.25 0.5 0.67 0.2 0 0.4 0.6 0 0 0.6 Average 0.27 -0.24 0.26 0.37 0.42 0.23 0.34 0.07 0.29 0.27 0.12 0.18 0.31 LIMSI-4-HR 0.25 2/12 0 0.17 0.62 0 0.56 0.24 0 0.32 0.4 0 0.48 0.28 LIMSI-7 0.25 0/12 0 0 0.31 0.44 0 0.24 0 0.4 0.4 0.24 0.24 0 LIMSI-4 0.25 2/12 0 0.17 0.5 0.5 0.33 0. 0.23 1/12 0 0.33 0 0.17 0 0.8 0 0.2 0.4 0.2 0 0.4 cicnlp-5 0.23 1/12 0 0.17 0.25 0.67 0 0.4 0 0.4 0 0.2 0 0.4 LIMSI-2 0.2 2/12 0 0.5 0 0.67 0 0.28 0 0 0.36 0 0 0 DIPF-2-0.2 0/12 0.25 0.33 0.25 0.33 0.33 0 0 0.2 0.4 0 0 0.2 DIPF-1-0.2 0/12 0.25 0 0.25 0.33 0.33 0.4 0 0.2 0.4 0 0 0.2 LIMSI-1-dude1 0.18 1/12 0 0.17 0.5 0 0.33 0.2 0 0.2 0.2 0 0.2 0.4 LIMSI-3 0.16 1/12 0 0.5 0 0.44 0 0.28 0 0 0 0.24 0 0 LIMSI-5 0.15 0/12 0 0 0.31 0 0 0.24 0 0.2 0.2 0.24 0.24 0 LIMSI-1-dude 0.13 1/12 0.25 0 0 0 0 0.24 0 0.2 0 0.24 0 0.64 LIMSI-6 0.06 0/12 0 0 0 0 0 0 0 0.32 0.24 0 0 0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3. Overall results for all runs, reading perspective</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>The results observed under the reading perspective are very encouraging. The three top systems were able to pass at least half of tests. As observed in Table <ref type="table" coords="5,415.27,403.29,3.76,8.96" target="#tab_2">4</ref>, each test has a different degree of difficulty for the systems. There are three main reasons for that: the way the questions are formulated, the lexical gap between the text and the candidate answers, and the inherit difficulty of some questions for which wrong candidate answers seems to be closer to the supporting text in a light reading. An overall overview of systems' descriptions shows the importance given to answer ranking over validation. In fact, all participant systems relied on ranking methods except the LIMSI-CNRS group, which applied an approach based on validation.</p><p>We found two different approaches regarding the use of documents for finding the correct answer: on one hand, some systems work with the whole document while on the other hand, some systems select a set of promising text snippets using retrieval techniques. We do not see observation about the best performance of one approach over the other. Some participants create hypotheses combining questions and candidates, and trying to match these hypotheses with the document excerpts.</p><p>All participants except <ref type="bibr" coords="6,256.73,149.58,11.69,8.96" target="#b5">[6]</ref> reported the use of coreference analysis in their systems, pointing out the importance of this information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">SUMMARY OF SYSTEMS</head><p>DIPF system <ref type="bibr" coords="6,179.54,217.62,11.69,8.96" target="#b3">[4]</ref> retrieves a set of sentences from the document that are likely to contain a correct answer according to a set of lexical and semantic similarity measures. Each candidate answer is combined with the question to form a hypothesis to be checked against the selected sentences. The final decision about the selected answer relies on a linear combination of two scores for each Text-Hypothesis pair: (1) the confidence score given by a state of the art RTE system and; (2) a combination of lexical and semantic similarity measures.</p><p>Synapse Dèveloppement <ref type="bibr" coords="6,264.05,298.17,11.67,8.96" target="#b1">[2]</ref>[3] builds Clause Description Structures (CDS) for documents, questions and answers, and compares them in order to take the final decision. CDSs represent a rich structure containing semantic information of texts, as well as relations among the elements of the text. The system first removes candidate answers that do not match the expected answer type. Then it uses CDSs to compute the number of common elements and their proximity between documents and candidate answers. This value is used to rank the candidate answers and select the first one.</p><p>CICNLP system <ref type="bibr" coords="6,229.25,378.69,11.69,8.96" target="#b4">[5]</ref> combines questions with candidate answers to build hypotheses. First, the system generates graph representations for the hypotheses and documents based on syntactic analysis. Paths sharing initial and final nodes both in text and hypothesis are converted into linguistic features for vector representation. Finally, the system uses these vectors for computing the cosine similarity, and ranking the candidate answers.</p><p>CSGS system <ref type="bibr" coords="6,219.12,447.57,11.69,8.96" target="#b5">[6]</ref> is based on weighting the alignment of text sentences and question answers at token and chunk level. LIMSI-CNRS system <ref type="bibr" coords="6,250.49,470.63,11.69,8.96" target="#b6">[7]</ref> relies on a validation approach in contrast to ranking methods used by other participants. First, the system uses the question and its expansion to retrieve passages of 3 to 5 sentences. Second, the system creates predicate-argument structures for passages and candidate answers, trying to align them at the word level using semantic relations. Then, the system applies a set of validation and invalidation rules. A candidate answer is validated if it fires all the validation rules and does not fire any invalidation rule. If there is more than one answer after the validation process, the system selects the answer with the highest alignment score. Validation and invalidation rules were made manually over information on subjects, predicates and arguments, as well as predicate truth values given by TruthTeller <ref type="bibr" coords="6,447.07,574.19,10.78,8.96" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>Last year exercise experience suggested the need to develop strategies to reject answers more than strategies to accept answers. One system started to develop this strategy but results aren't yet among the top performers. All systems except Synapse's for French select more incorrect answers than correct ones. This is really a measure of progress in systems development. However, at the reading perspective evaluation, we have already three systems (two teams) able to pass at least half of reading tests. Again, the Entrance Exams task shows that Question Answering is a task far from being solved. However, it provides a real benchmark able to assess real progress in the field along future years.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,124.70,439.70,354.72,97.13"><head>Table 1 .</head><label>1</label><figDesc>Participants and reference papers</figDesc><table coords="3,124.70,456.81,354.72,80.02"><row><cell>SYNAPSE</cell><cell>Synapse Développement, France</cell><cell>Laurent et al. 2014 [2] [3]</cell></row><row><cell>DIPF</cell><cell>Technische Universität Darmstadt, Germany</cell><cell>Dhruva et al. 2014 [4]</cell></row><row><cell>CICNLP</cell><cell>Centro de Investigación en Computación Instituto Politécnico Nacional, Mexico</cell><cell>Gómez-Adorno et al. 2014 [5]</cell></row><row><cell>CSGS</cell><cell>Saarland University, Germany</cell><cell>Ostermann et al. 2014 [6]</cell></row><row><cell>LIMSI-CNRS</cell><cell>ILES -LIMSI, France</cell><cell>Gleize et al. 2014 [7]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,135.86,589.00,344.84,101.01"><head>Table 2 .</head><label>2</label><figDesc>Overall results for all runs, QA perspective</figDesc><table coords="3,135.86,606.16,344.84,83.85"><row><cell></cell><cell></cell><cell></cell><cell cols="3"># of questions ANSWERED</cell><cell># of questions</cell></row><row><cell>RUN NAME</cell><cell>C@1</cell><cell cols="3">RIGHT WRONG TOTAL</cell><cell>Prec.</cell><cell>UNANSWERED</cell></row><row><cell>Synapse-French</cell><cell>0.59</cell><cell>33</cell><cell>23</cell><cell>56</cell><cell>0.59</cell><cell>0</cell></row><row><cell>Synapse-English</cell><cell>0.45</cell><cell>25</cell><cell>31</cell><cell>56</cell><cell>0.45</cell><cell>0</cell></row><row><cell>DIPF-7</cell><cell>0.38</cell><cell>21</cell><cell>35</cell><cell>56</cell><cell>0.38</cell><cell>0</cell></row><row><cell>cicnlp-8</cell><cell>0.38</cell><cell>21</cell><cell>35</cell><cell>56</cell><cell>0.38</cell><cell>0</cell></row><row><cell>cicnlp-7</cell><cell>0.36</cell><cell>20</cell><cell>36</cell><cell>56</cell><cell>0.36</cell><cell>0</cell></row><row><cell>csgs-1</cell><cell>0.36</cell><cell>20</cell><cell>36</cell><cell>56</cell><cell>0.36</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,128.42,473.20,340.40,61.11"><head>Table 4 .</head><label>4</label><figDesc>Number of runs (only for English) that passed each test (out of 28), and maximum c@1 score achieved per test.</figDesc><table coords="5,128.42,501.59,340.40,32.72"><row><cell></cell><cell cols="11">T13 T14 T15 T16 T17 T18 T19 T20 T21 T22 T23 T24</cell></row><row><cell cols="2"># Runs pass 7</cell><cell>7</cell><cell>11 14</cell><cell>4</cell><cell>6</cell><cell>0</cell><cell>3</cell><cell>4</cell><cell>0</cell><cell>2</cell><cell>3</cell></row><row><cell>Max. score</cell><cell cols="11">1 0.67 0.75 0.67 0.67 0.8 0.33 0.8 0.6 0.4 0.6 0.64</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENTS</head><p>The collaboration has been developed in the framework of <rs type="projectName">Todai Robot Project in Japan</rs>, and the <rs type="projectName">CHIST-ERA Readers project in Europe</rs> (<rs type="grantNumber">MINECO PCIN-2013-002-C02-01</rs>). The <rs type="projectName">Todai Robot Project</rs> is a grand challenge headed by NII, and aims to develop an end-to-end AI system that can solve real entrance examinations of universities in Japan integrating heterogeneous AI technologies, such as natural language processing, situation understanding, math formula processing or vision processing.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_C8Guesm">
					<orgName type="project" subtype="full">Todai Robot Project in Japan</orgName>
				</org>
				<org type="funded-project" xml:id="_WEH8jwt">
					<idno type="grant-number">MINECO PCIN-2013-002-C02-01</idno>
					<orgName type="project" subtype="full">CHIST-ERA Readers project in Europe</orgName>
				</org>
				<org type="funded-project" xml:id="_Fsh8QSs">
					<orgName type="project" subtype="full">Todai Robot Project</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,128.11,365.90,342.70,8.10;7,142.70,376.34,328.12,8.10;7,142.70,386.66,280.19,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,280.25,365.90,156.88,8.10">A Simple Measure to Assess Non-response</title>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alvaro</forename><surname>Rodrigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,454.26,365.90,16.55,8.10;7,142.70,376.34,328.12,8.10;7,142.70,386.66,167.00,8.10">Proceedings of 49th Annual Meeting of the Association for Computational Linguistics -Human Language Technologies (ACL-HLT 2011)</title>
		<meeting>49th Annual Meeting of the Association for Computational Linguistics -Human Language Technologies (ACL-HLT 2011)<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,128.11,396.98,342.64,8.10;7,142.70,407.30,327.96,8.10;7,142.70,417.74,18.05,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,418.01,396.98,52.73,8.10;7,142.70,407.30,180.57,8.10">English run of Synapse Développement at Entrance Exams 2014</title>
		<author>
			<persName coords=""><forename type="first">Dominique</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Baptiste</forename><surname>Chardon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sophie</forename><surname>Negre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Seguela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,330.29,407.30,98.47,8.10">CLEF 2014 Working Notes</title>
		<meeting><address><addrLine>Sheffield</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,128.11,428.06,342.54,8.10;7,142.70,438.38,327.96,8.10;7,142.70,448.70,18.05,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,420.05,428.06,50.59,8.10;7,142.70,438.38,161.18,8.10">French run of Synapse Développement at Entrance Exams</title>
		<author>
			<persName coords=""><forename type="first">Dominique</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Baptiste</forename><surname>Chardon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sophie</forename><surname>Negre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Seguela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,330.29,438.38,98.47,8.10">CLEF 2014 Working Notes</title>
		<meeting><address><addrLine>Sheffield</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,128.11,459.14,342.59,8.10;7,142.70,469.48,327.92,8.10;7,142.70,479.80,113.79,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,349.42,459.14,121.28,8.10;7,142.70,469.48,275.32,8.10">Solving Open-Domain Multiple Choice Questions with Textual Entailment and Text Similarity Measures</title>
		<author>
			<persName coords=""><forename type="first">Neil</forename><surname>Dhruva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oliver</forename><surname>Ferschke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,426.55,469.48,44.07,8.10;7,142.70,479.80,52.23,8.10">CLEF 2014 Working Notes</title>
		<meeting><address><addrLine>Sheffield</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,128.11,490.12,342.54,8.10;7,142.70,500.56,327.92,8.10;7,142.70,510.88,113.79,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,448.27,490.12,22.37,8.10;7,142.70,500.56,278.54,8.10">Graph Based Approach for the Question Answering Task Based on Entrance Exams</title>
		<author>
			<persName coords=""><forename type="first">Helena</forename><surname>Gómez-Adorno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Grigori</forename><surname>Sidorov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,428.23,500.56,42.39,8.10;7,142.70,510.88,52.23,8.10">CLEF 2014 Working Notes</title>
		<meeting><address><addrLine>Sheffield</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,128.11,521.20,342.32,8.10;7,142.70,531.52,327.96,8.10;7,142.70,541.96,134.08,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,409.69,521.20,60.73,8.10;7,142.70,531.52,300.00,8.10">CSGS: Adapting a short answer scoring system for multiple-choice reading comprehension exercises</title>
		<author>
			<persName coords=""><forename type="first">Simon</forename><surname>Ostermann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nikolina</forename><surname>Koleva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexis</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Horbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,448.63,531.52,22.03,8.10;7,142.70,541.96,72.53,8.10">CLEF 2014 Working Notes</title>
		<meeting><address><addrLine>Sheffield</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,128.11,552.28,342.63,8.10;7,142.70,562.60,328.01,8.10;7,142.70,572.92,55.43,8.10" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,342.31,552.28,128.42,8.10;7,142.70,562.60,215.33,8.10">LIMSI-CNRS@CLEF 2014: Invalidating Answers for Multiple Choice Question Answering</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Gleize</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anne-Laure</forename><surname>Ligozat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brigitte</forename><surname>Grau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,365.95,562.60,100.97,8.10">CLEF 2014 Working Notes</title>
		<meeting><address><addrLine>Sheffield</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,128.11,583.36,342.57,8.10;7,142.70,593.68,119.84,8.10" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,259.06,583.36,138.79,8.10">Truthteller: Annotating predicate truth</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lotan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,416.31,583.36,54.37,8.10;7,142.70,593.68,51.43,8.10">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="752" to="757" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
