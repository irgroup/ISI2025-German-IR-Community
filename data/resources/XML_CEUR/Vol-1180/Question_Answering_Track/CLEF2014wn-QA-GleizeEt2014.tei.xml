<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,134.77,116.95,345.83,12.62;1,164.69,134.89,285.97,12.62">LIMSI-CNRS@CLEF 2014: Invalidating Answers for Multiple Choice Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,172.55,172.56,60.15,8.74"><forename type="first">Martin</forename><surname>Gleize</surname></persName>
							<email>gleize@limsi.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">LIMSI-CNRS</orgName>
								<address>
									<addrLine>Rue John von Neumann</addrLine>
									<postCode>91405</postCode>
									<settlement>Orsay CEDEX</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Universit√© Paris-Sud</orgName>
								<address>
									<settlement>Orsay</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,249.59,172.56,86.93,8.74"><forename type="first">Anne-Laure</forename><surname>Ligozat</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">LIMSI-CNRS</orgName>
								<address>
									<addrLine>Rue John von Neumann</addrLine>
									<postCode>91405</postCode>
									<settlement>Orsay CEDEX</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">ENSIIE</orgName>
								<address>
									<settlement>Evry</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,372.79,172.56,59.21,8.74"><forename type="first">Brigitte</forename><surname>Grau</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">LIMSI-CNRS</orgName>
								<address>
									<addrLine>Rue John von Neumann</addrLine>
									<postCode>91405</postCode>
									<settlement>Orsay CEDEX</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">ENSIIE</orgName>
								<address>
									<settlement>Evry</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,134.77,116.95,345.83,12.62;1,164.69,134.89,285.97,12.62">LIMSI-CNRS@CLEF 2014: Invalidating Answers for Multiple Choice Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">52E77D12FA00CEFD31703062018E60B5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Question Answering</term>
					<term>Passage Retrieval</term>
					<term>Textual Entailment</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our participation to the Entrance Exams Task of CLEF 2014's Question Answering Track. The goal is to answer multiple-choice questions on short texts. Our system first retrieves passages relevant to the question, through lexical expansion involving a structured use of the Simple English Wiktionary and WordNet. Then it extracts predicate-argument structures (PAS) from each answer choice and aligns them to PAS found in the passages retrieved in the first step. Finally, manually crafted rules are applied to those alignments to try to invalidate answer choices. If enough answer choices are thus invalidated, we make a decision on the remaining answer choices based on their alignment scores with the passages. We submitted several runs in the task, only one of which reached the random baseline (c@1 of 0.25). In the last section, we provide an analysis of the differences between our relatively good results obtained on trial data and the poor performance of our test run.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task focuses on the reading of single documents and identification of the correct answer to a question from a set of possible answer options. The identification of the correct answer requires various kinds of inference and the consideration of previously acquired background knowledge. Japanese University Entrance Exams include questions formulated at various levels of complexity and test a wide range of capabilities. The challenge of "Entrance Exams" aims at evaluating systems under the same conditions humans are evaluated to enter the University. Previously the evaluation campaign Question Answering For Machine Reading Evaluation (QA4MRE at CLEF) <ref type="bibr" coords="1,350.20,585.38,10.52,8.74" target="#b6">[8]</ref> focused on multiple-choice questions designed to evaluate computer systems, but this new task takes on challenges typically offered to humans. It naturally translates into more complex inference phenomena to solve <ref type="bibr" coords="1,291.79,621.25,9.96,8.74" target="#b1">[2]</ref>, and thus usually lower performance of systems: QA4MRE 2013's best run <ref type="bibr" coords="1,297.93,633.20,10.52,8.74" target="#b0">[1]</ref> on the Main task outperformed by a large margin its counterpart on the Entrance Exams pilot task (c@1 of 0.59 on 5-choice questions, compared to 0.42 on 4-choice questions) <ref type="bibr" coords="1,396.82,657.11,9.96,8.74" target="#b4">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Architecture</head><p>The overarching goal of our system is to essentially invalidate as many incorrect answer choices as possible. Although we also introduce some elements of validation of the correct answer, our study of the trial question set revealed that we might have a better chance of finding the right answer by elimination of the wrong candidates. The architecture of our multiple-choice questionanswering system is described in Figure <ref type="figure" coords="2,306.06,206.36,3.87,8.74">1</ref>. Its pipeline is composed of mainly four modules: preprocessing, passage retrieval, predicate-argument extractor and validation/invalidation. The remaining of this section is dedicated to the detailed description of those modules. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preprocessing</head><p>We use Stanford CoreNLP as our main Natural Language annotation tool. Each sentence from the document, questions or answer choices is tagged with Part-Of-Speech <ref type="bibr" coords="2,183.55,549.52,10.52,8.74" target="#b7">[9]</ref> and syntactically parsed <ref type="bibr" coords="2,312.15,549.52,9.96,8.74">[4]</ref>. In addition, we apply coreference resolution <ref type="bibr" coords="2,180.58,561.47,10.52,8.74" target="#b5">[7]</ref> on the whole document. We did not use the Named Entity Recognition component. We also use TruthTeller <ref type="bibr" coords="2,244.51,585.38,9.96,8.74" target="#b3">[5]</ref>, a semantic annotator that assigns truth values to predicate occurrences, on all the sentences. More details about the annotation types are available in its presentation paper, but we generally only use the Predicate Truth annotation, which is the final value assigned by TruthTeller. It is one of P (Positive), U (Uncertain) or N (Negative), and indicates whether the predicate itself is entailed by its containing sentence, in the classical sense of textual entailment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Passage Retrieval</head><p>The passage retrieval module aims at ranking document snippets of 3 to 5 sentences by relevance to the question. Words of the question act as the query. However, it is very rare that words of the question exactly appear in the relevant passage of the document, so we have to use some form of query expansion.</p><p>We use a method similar to <ref type="bibr" coords="3,260.12,187.38,9.96,8.74" target="#b2">[3]</ref>: we basically expand each query and document word with the words of their definition in the Simple English Wiktionary <ref type="bibr" coords="3,462.34,199.33,14.61,8.74" target="#b8">[10]</ref>.</p><p>We can do that recursively, expanding definition words with their own definition, thus defining a kind of word tree. We compute for each (query word, document word) pair a weighted number of common words in their tree. We weigh the words according to their depth in the definition tree.</p><p>Figure <ref type="figure" coords="3,166.18,259.11,4.98,8.74" target="#fig_0">2</ref> shows a portion of the definition trees for the words cat and wolf. The word animal is found in the Simple English Wiktionary definition of both words: we say that it is found at depth 1 -depth 0 being the word itself. The word pet on the other hand is found in the definition of dog, a word of the definition of wolf : we say that it is of depth 2. We add the depth of common words in both trees: animal is a common word of depth 2 and pet is a common word of depth 3 for the pair (cat, wolf ). This roughly captures the intuition that the word animal is more accurate to describe common traits of a cat and a wolf rather than the word pet. We enrich the definition trees with coreference information and WordNet synonyms and antonyms -to complete the coverage of the Simple Wiktionary, and weigh all the words in the tree by the IDF score of the word in the document. We ran TruthTeller on all the Simple English Wiktionary definitions, so we can propagate predicate truth values in the definition tree to compute whether a word in the tree is entailed by the root word. The propagation rules are simple: if a word is of truthvalue P or U, each of its direct definition words keeps the truth value assigned by TruthTeller, if it is N, we take the inverse TruthTeller annotation for the definition word (inverse of P is N, inverse of N is P, and inverse of U is U). We do not use predicate truth information for scoring, it will be used in the validation/invalidation step. With the resulting word-to-word semantic relatedness scores, we compute a 1-to-1 maximum sum alignment of the words using the Integer Linear Programming solver lpsolve. We then rank the aligned passages by alignment scores. This makes up our basic ranking system and it is also used in the predicate-argument alignment described later in the paper. One interesting property of our data is that in general, the order of the passages in the text preserve the order of the questions they're relevant to: if question 2 is after question 1 in the reading test, then its relevant passage in the document is most likely after that of question 1. We use a simple dynamic programming algorithm -omitted here for space-to compute the best sequence of passages conserving this property, and place the passages computed this way at the top of their respective per-question list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Predicate-arguments Aligner</head><p>Predicate-arguments Extractor Predicate-arguments structures (PAS) aim at capturing essentially "Who does what to what?" in a shallow way. We use the Stanford dependency graph from the parsed sentences and take the verbs (Penn POS tag starting with V) as predicates and we classify their dependencies between subject and arguments. We take as subject the nodes in a nsubj or nsubjpass dependency with the verb, and consider the rest as arguments. We enrich subjects and arguments with their own dependencies to not lose out on adjectives, certain numerical determiners and prepositions. When considering dependencies, we filter out determiners (with the DT tag).</p><p>Alignment of answer PAS and passage PAS With the previously described method, we extract PAS for the sentences of each answer choice. We also extract PAS for the sentences in the relevant passages obtained in section 2.2. We align answer PAS and passage PAS with the same alignment method using our semantic relatedness method and the ILP formulation, and rank the alignments by alignment score. Let us note that we align regardless of the function the word plays in the PAS: we can align predicates with subjects and arguments, and subjects with arguments. At this point, we summarize the information available to us in those alignments. For each aligned word pair, we know:</p><p>the function that each word plays in its respective containing PAS: subject, predicate or argument. a semantic relatedness score.</p><p>truth value annotations, as annotated by TruthTeller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Validation/Invalidation</head><p>Algorithm Our goal in this last module is to eliminate as many answer choices as possible without eliminating the right answer. Let K be the maximum number much more difficult, even for a human, to invalidate a wrong answer than it is to validate a correct one with accuracy. In the description of the rules, polarity means the predicate truth value of the common word found in the semantic relatedness score. Compatible polarities are P with P, N with N, and U with P,N,U. Strong alignment means that the word-to-word alignment score is above a threshold, manually set in this system. The validation rules are as follows:</p><p>-Rule 1: Subject and Predicate are strongly aligned in both PAS, and all polarities are compatible. -Rule 2: Predicate and one Argument are strongly aligned in both PAS, and all polarities are compatible.</p><p>The invalidation rules are as follows:</p><p>-Rule 1: One polarity mismatch is found in a strong alignment.</p><p>-Rule 2: Predicates are strongly aligned, but their Subjects are not aligned at all. -Rule 3: The alignment is located at least 2 sentences before the best (question, passage) alignment. We noticed on the trial corpus that the correct answer was usually found after the mention of the question in the document.</p><p>3 Data and results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">CLEF 2014 QA Track: Entrance Exams data and evaluation</head><p>Our data consist of the trial and test sets at CLEF 2014 Question Answering Track, Task 3: Entrance Exams. Both trial and test sets feature the same format: a series of 12 texts, and for each of them, 5 multiple-choice questions to answer: 60 questions in total. There are 4 answer choices possible for each of the questions. This corpus has been extracted from the Tokyo University Entrance Exam in English as a foreign language. Systems are evaluated according to their c@1, defined in equation 1.</p><formula xml:id="formula_0" coords="6,256.57,521.67,224.02,22.31">C@1 = 1 n (n R + n U n R n )<label>(1)</label></formula><p>with n the total number of questions, n R the number of correctly answered questions, n U the number of unanswered questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>In this section, we report the results of our best run on both trial and test question sets. This system uses K = 2 as the maximum number of answer choices allowed to make the decision. Passages are 5 sentences long and the algorithm described in section 2.4 uses a maximum of 10 passages before reaching nondecision. We tried other values of those parameters, but the corresponding runs were performing worse on both trial and data sets, so we do not feel the need to further expose results about them. We focus instead on the differences on trial and test results for the described run. Both datasets contain 60 questions. Table <ref type="table" coords="7,134.77,155.86,4.98,8.74" target="#tab_1">1</ref> reports global results of our system on both sets of questions. As we can see, performance is quite satisfactory on the trial dataset, but really poor -at the level of the random baseline-on the test dataset. We also analyzed in a fine-grained way the accuracy and efficiency of our validation and invalidation rules. Table <ref type="table" coords="7,310.92,328.28,4.98,8.74" target="#tab_2">2</ref> describes the error rate on questions where invalidation rules ended up eliminating a correct answer choice. In each cell, we find the number of questions where the particular rule eliminated a correct answer, and we also find the number of questions it fired on -where it eliminated at least one answer choice. As we can see, all rules misfire more on the test dataset, which seems to correlate well with the overall poor test performance. Validation rules did not fire as often as the invalidation rules (4 times for trial, and 5 times for test). It is possible to frame our rule system as a kind of information retrieval system and evaluate it in term of validation precision and recall, and invalidation precision and recall. For validation, relevant items are the correct answers, and for invalidation, relevant items are the incorrect answer choices. We compare our results in both trial and test datasets with a random baseline, which basically has a 50% chance of validating and invalidating each answer choice and stops under the same conditions as our system (when K or less answer choices remain). Results in term of precision and recall are shown in table <ref type="table" coords="7,425.66,621.25,3.87,8.74" target="#tab_3">3</ref>. As we can see, surprisingly, validation/invalidation does not seem to perform significantly differently from random guesses on test data, while it is not the case at all on trial data. We finally report the performance of our passage retrieval system compared to several baselines. Prior to this participation, we had annotated the correct passages on 45 of the 60 questions of the trial dataset, this allows us to compute the MRR (Mean Reciprocal Rank) of our passage retrieval method. Results are shown in table 4. Baseline 1 is a simple word-overlap-based measure. Baseline 2 is Baseline 2 with TF-IDF weighting and WordNet keyword expansion. We then ran our system with and without taking into account the order preserving property of the reading tests, mentioned at the end of section 2.2. As we can see, our passage retrieval method vastly outperforms typical baselines, and the assumption about order of questions being preserved in the order of passages seems to hold and help quite a bit, at least on trial questions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>Our system has been developed with the invalidation of wrong answer candidates in mind, specifically to answer multiple-choice questions. In the CLEF 2014 evaluation campaign, Question Answering track, the submitted run performed at the level of the random baseline in the Entrance exams task and thus did not reproduce satisfactory trial performance.</p><p>In further works, we plan to study the reasons behind the differences in performance of our system on trial and test results and to better integrate predicatearguments structure selection with passage retrieval, the module of our system which seemed to perform well. Other features considered include a character name resolver addition to coreference resolution, and analyzing discourse relations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,172.80,515.91,267.85,7.89;3,169.35,385.99,276.66,119.88"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Simple English Wiktionary Definition trees of cat and wolf</figDesc><graphic coords="3,169.35,385.99,276.66,119.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,238.52,212.86,138.31,70.16"><head>Table 1 .</head><label>1</label><figDesc>Results on trial and test</figDesc><table coords="7,247.03,230.92,121.28,52.10"><row><cell></cell><cell>Trial Test</cell></row><row><cell cols="2">Questions answered 42 36</cell></row><row><cell>Errors</cell><cell>22 25</cell></row><row><cell>Accuracy</cell><cell>0.48 0.31</cell></row><row><cell>c@1</cell><cell>0.43 0.25</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,218.00,447.55,179.36,59.20"><head>Table 2 .</head><label>2</label><figDesc>Invalidation error on trial and test</figDesc><table coords="7,264.89,465.61,85.57,41.14"><row><cell>Trial Test</cell></row><row><cell>Rule 1 2 / 21 4 / 13</cell></row><row><cell>Rule 2 6 / 25 8 / 22</cell></row><row><cell>Rule 3 8 / 26 10 / 28</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,192.72,117.41,229.92,72.40"><head>Table 3 .</head><label>3</label><figDesc>Precision/recall of validation/invalidation</figDesc><table coords="8,192.72,137.71,229.92,52.10"><row><cell>System</cell><cell cols="3">Random Rules (Trial) Rules (Test)</cell></row><row><cell>Validation precision</cell><cell>0.24</cell><cell>0.36</cell><cell>0.25</cell></row><row><cell>Validation recall</cell><cell>0.40</cell><cell>0.57</cell><cell>0.42</cell></row><row><cell cols="2">Invalidation precision 0.75</cell><cell>0.83</cell><cell>0.74</cell></row><row><cell>Invalidation recall</cell><cell>0.60</cell><cell>0.66</cell><cell>0.56</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,227.50,375.39,160.35,71.90"><head>Table 4 .</head><label>4</label><figDesc>Passage retrieval performance</figDesc><table coords="8,251.58,395.20,112.19,52.10"><row><cell>System</cell><cell>MRR</cell></row><row><cell>Baseline 1</cell><cell>0.36</cell></row><row><cell>Baseline 2</cell><cell>0.45</cell></row><row><cell cols="2">System without order 0.49</cell></row><row><cell>System with order</cell><cell>0.58</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>of answer choices we are allowed to keep to take the final decision. The following algorithm computes whether we take a final decision for the current question. We provide a rough explanation of what goes on in this pre-decision process. We explore all the ranked relevant passages as long as we have not eliminated enough answers. When faced with a new passage, we PAS-align each answer choice with the passage (section 2.3). The ranked alignments go through validation and invalidation rules and the corresponding answer is invalidated if a PAS alignment is found invalid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ALL_ANSWERS</head><p>Final decision If we go through all the passages without invalidating enough answers, we choose not to answer the question. If we have K or less answers remaining, we pick the one with the strongest PAS alignment score found in the passages.</p><p>The rules We manually built 2 validation rules and 3 invalidation rules. They operate on PAS alignments. In the current iteration of our system, the validation rules must be fired simultaneously to validate an answer choice, whereas only one invalidation rule fired is enough to invalidate an answer choice. We consider that it is indeed usually</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="9,142.96,143.59,337.62,7.86;9,151.52,154.54,329.05,7.86;9,151.52,165.50,88.64,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,446.53,143.59,34.04,7.86;9,151.52,154.54,272.73,7.86">Multiple choice question (mcq) answering system for entrance examination</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pakray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bandyopadhyay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,433.02,154.54,47.56,7.86;9,151.52,165.50,59.97,7.86">CLEF 2013 Working Notes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,176.46,337.62,7.86;9,151.52,187.42,329.06,7.86;9,151.52,198.38,329.06,7.86;9,151.52,209.34,329.05,7.86;9,151.52,220.30,325.85,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,238.15,176.46,242.44,7.86;9,151.52,187.42,19.07,7.86">A hierarchical taxonomy for classifying hardness of inference tasks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gleize</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Grau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,377.94,198.38,102.63,7.86;9,151.52,209.34,329.05,7.86;9,151.52,220.30,194.46,7.86">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14). European Language Resources Association (ELRA)</title>
		<editor>
			<persName><forename type="first">)</forename><surname>Chair</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">C C</forename><surname>Choukri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Declerck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Loftsson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Maegaard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Mariani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Moreno</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Odijk</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Piperidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename></persName>
		</editor>
		<meeting>the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14). European Language Resources Association (ELRA)<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,231.26,337.62,7.86;9,151.52,242.22,315.72,7.86;9,139.37,253.17,341.20,7.86;9,151.52,264.13,329.05,7.86;9,151.52,275.09,236.61,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,165.47,242.22,301.77,7.86;9,139.37,253.17,3.58,7.86;9,261.11,253.17,122.33,7.86">Selecting answers with structured lexical expansion and discourse relations 4</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gleize</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Grau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">L</forename><surname>Ligozat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">M</forename><surname>Pho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Illouz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Giannetti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lahondes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,405.35,253.17,75.23,7.86;9,151.52,264.13,273.31,7.86">Proceedings of the 41st Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
	<note>Accurate unlexicalized parsing</note>
</biblStruct>

<biblStruct coords="9,142.96,286.05,337.62,7.86;9,151.52,297.01,183.84,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,282.92,286.05,158.48,7.86">Truthteller: Annotating predicate truth</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lotan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,463.02,286.05,17.55,7.86;9,151.52,297.01,98.27,7.86">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="752" to="757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,307.97,337.62,7.86;9,151.52,318.93,206.39,7.86" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="9,377.51,307.97,103.07,7.86;9,151.52,318.93,80.49,7.86">Overview of qa4mre 2013 entrance exams task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pe√±as</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Forner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kando</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>CLEF</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Working Notes</note>
</biblStruct>

<biblStruct coords="9,142.96,329.89,337.62,7.86;9,151.52,340.85,329.06,7.86;9,151.52,351.80,329.06,7.86;9,151.52,362.76,174.76,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,332.99,329.89,147.59,7.86;9,151.52,340.85,139.04,7.86">The life and death of discourse entities: Identifying singleton mentions</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">C</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,311.73,340.85,168.85,7.86;9,151.52,351.80,329.06,7.86;9,151.52,362.76,90.80,7.86">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="627" to="633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,371.46,337.63,10.13;9,151.52,384.68,240.65,7.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="9,219.28,384.68,172.89,7.86">Overview of qa4mre main task at clef 2013</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sutcliffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pe√±as</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Forner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">√Å</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Forascu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Benajiba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Osenova</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,395.64,337.62,7.86;9,151.52,406.60,329.06,7.86;9,151.52,417.56,329.05,7.86;9,151.52,428.52,329.05,7.86;9,151.52,439.48,106.20,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,371.89,395.64,108.69,7.86;9,151.52,406.60,165.87,7.86">Feature-rich part-of-speech tagging with a cyclic dependency network</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,338.32,406.60,142.26,7.86;9,151.52,417.56,329.05,7.86;9,151.52,428.52,133.43,7.86">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.61,450.43,337.97,8.11;9,151.52,462.04,70.61,7.47" xml:id="b8">
	<monogr>
		<ptr target="http://simple.wiktionary.org/" />
		<title level="m" coord="9,251.42,450.43,122.16,7.86">The simple english wiktionary</title>
		<imprint>
			<publisher>Wikimedia Foundation</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
