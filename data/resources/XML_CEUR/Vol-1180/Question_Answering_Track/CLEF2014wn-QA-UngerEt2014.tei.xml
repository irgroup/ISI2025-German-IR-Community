<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,134.77,117.75,345.83,12.62">Question Answering over Linked Data (QALD-4)</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,147.62,155.47,70.13,8.74"><forename type="first">Christina</forename><surname>Unger</surname></persName>
							<email>cunger@cit-ec.uni-bielefeld.decimiano@cit-ec.uni-bielefeld.deswalter@techfak.uni-bielefeld.de</email>
							<affiliation key="aff0">
								<orgName type="department">CITEC</orgName>
								<orgName type="institution">Bielefeld University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,228.30,155.47,66.12,8.74"><forename type="first">Corina</forename><surname>Forascu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Alexandru Ioan Cuza University of Iasi</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,304.97,155.47,63.63,8.74"><forename type="first">Vanessa</forename><surname>Lopez</surname></persName>
							<email>vanlopez@ie.ibm.com</email>
							<affiliation key="aff2">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<settlement>Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,379.15,155.47,88.59,8.74;1,163.08,167.43,30.72,8.74"><forename type="first">Axel-Cyrille</forename><surname>Ngonga Ngomo</surname></persName>
						</author>
						<author>
							<persName coords="1,204.35,167.43,57.17,8.74"><forename type="first">Elena</forename><surname>Cabrio</surname></persName>
							<email>elena.cabrio@inria.fr</email>
							<affiliation key="aff3">
								<orgName type="department">AKSW</orgName>
								<orgName type="institution">University of Leipzig</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">INRIA Sophia-Antipolis Méditerrané</orgName>
								<address>
									<settlement>Cedex</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,272.09,167.43,71.54,8.74"><forename type="first">Philipp</forename><surname>Cimiano</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CITEC</orgName>
								<orgName type="institution">Bielefeld University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,373.56,167.43,74.25,8.74"><forename type="first">Sebastian</forename><surname>Walter</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CITEC</orgName>
								<orgName type="institution">Bielefeld University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,134.77,117.75,345.83,12.62">Question Answering over Linked Data (QALD-4)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">90482A7A6342DDAEEFEFF35DE2A83A10</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the increasing amount of semantic data available on the web there is a strong need for systems that allow common web users to access this body of knowledge. Especially question answering systems have received wide attention, as they allow users to express arbitrarily complex information needs in an easy and intuitive fashion (for an overview see <ref type="bibr" coords="1,321.37,423.09,10.30,8.74" target="#b3">[4]</ref>). The key challenge lies in translating the users' information needs into a form such that they can be evaluated using standard Semantic Web query processing and inferencing techniques. Over the past years, a range of approaches have been developed to address this challenge, showing significant advances towards answering natural language questions with respect to large, heterogeneous sets of structured data. However, only few systems yet address the fact that the structured data available nowadays is distributed among a large collection of interconnected datasets, and that answers to questions can often only be provided if information from several sources are combined. In addition, a lot of information is still available only in textual form, both on the web and in the form of labels and abstracts in linked data sources. Therefore approaches are needed that can not only deal with the specific character of structured data but also with finding information in several sources, processing both structured and unstructured information, and combining such gathered information into one answer.</p><p>The main objective of the open challenge on question answering over linked data <ref type="foot" coords="1,154.12,612.85,3.97,6.12" target="#foot_0">6</ref> [3] (QALD) is to provide up-to-date, demanding benchmarks that establishe a standard against which question answering systems over structured data can be evaluated and compared. QALD-4 is the fourth instalment of the QALD open challenge, comprising three tasks: multilingual question answering, biomedical question answering over interlinked data, and hybrid question answering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task 1: Multilingual question answering</head><p>Task 1 is the core task of QALD and aims at all question answering systems that mediate between a user, expressing his or her information need in natural language, and semantic data. Given the English DBpedia 3.9 dataset<ref type="foot" coords="2,446.18,227.84,3.97,6.12" target="#foot_1">7</ref> and a natural language question or set of keywords in one of seven languages (English, Spanish, German, Italian, French, Dutch, Romanian), the participating systems had to return either the correct answers, or a SPARQL query that retrieves these answers.</p><p>To get acquainted with the dataset and possible questions, a set of 200 training questions was provided. These questions were compiled from the QALD-3 training and test questions, slightly modified in order to account for changes in the DBpedia dataset. Later, systems were evaluated on 50 different test questions. These questions were mainly devised by the challenge organizers.</p><p>All training questions were manually annotated with keywords, corresponding SPARQL queries and with answers retrieved from the provided SPARQL endpoint. Annotations were provided in an XML format. Each of the questions specifies an ID for the question together with a range of other attributes explained below, the natural language string of the question in the seven languages, keywords in the same languages, a corresponding SPARQL query, as well as the answers this query returns. Along with a unique ID, the following attributes were specified for each question:</p><p>answertype specifies the answer type, which can be one the following: resource (one or many resources, for which the URI is provided), string (a string value), number (a numerical value such as 47 or 1.8), date (a date provided in the format YYYY-MM-DD, e.g. 1983-11-02), boolean (either true or false). aggregation indicates whether any operations beyond triple pattern matching are required to answer the question (e.g., counting, filters, ordering). onlydbo is given only for DBpedia questions and reports whether the query relies solely on concepts from the DBpedia ontology.</p><p>Here is an example from the training set:</p><p>&lt; question id ="36" answertype =" resource " aggregation =" false " onlydbo =" false " &gt; &lt; string lang =" en " &gt; Through which countries does the Yenisei river flow ? &lt;/ string &gt; &lt; string lang =" de " &gt; Durch welche Länder fließt der Yenisei ? &lt;/ string &gt; &lt; string lang =" es " &gt; ¿Por qué países fluye el río Yenisei ? &lt;/ string &gt; ... Of the 200 training questions, 38 questions require aggregation and 74 questions require namespaces other than from the DBpedia ontology. Of the 50 test questions, 15 questions require aggregation and 10 cannot be answered with the DBpedia ontology only. As an additional challenge, 12 training and 2 test questions are out of scope, i.e. they cannot be answered with respect to the dataset.</p><formula xml:id="formula_0" coords="3,135.39,208.99,4.71,7.47">&lt;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Task 2: Biomedical question answering over interlinked data</head><p>Also for the life sciences, linked data plays a bigger and bigger role. Already a tenth of the Linked Open Data cloud<ref type="foot" coords="3,307.58,509.08,3.97,6.12" target="#foot_2">8</ref> consists of biomedical datasets. Especially biomedical data is distributed among a large collection of interconnected datasets, and answers to questions can often only be provided if information from several sources are combined. Task 2 therefore focuses on interlinked data. Given the following three biomedical datasets and a natural language question or set of keywords in English, the participating systems had to return either the correct answers or a SPARQL query that retrieves the answers.</p><p>-SIDER, describing drugs and their side effects http://sideeffects.embl.de -Diseasome, encompassing description of diseases and genetic disorders http://wifo5-03.informatik.uni-mannheim.de/diseasome/ -Drugbank, describing FDA-approved active compounds of medication http://www.drugbank.ca</p><p>The training question set comprised 25 questions over those datasets. All training questions were provided in an XML format similar to the one used for Task 1. Since the focus of the task is on interlinked data, most of the questions require the integration of information from at least two of those datasets. Here is an example query (ommitting prefix definitions), representing the question What are the side effects of drugs used for Tuberculosis? Note that the drugs used for Tuberculosis are retrieved from Diseasome, and their side effects are retrieved from SIDER. The link between the relevant resources in these datasets (bound to ?v2 and ?v3) is established using the OWL property sameAs.</p><p>Later, participating systems were evaluated on 25 similar test questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Task 3: Hybrid question answering</head><p>A lot of information is still available only in textual form, both on the web and in the form of labels and abstracts in linked data sources. Task 3 therefore focuses on the integration of both structured and unstructured information in order to gather answers. Given English DBpedia 3.9, containing both RDF data and free text available in the DBpedia abstracts, and a natural language question or keywords, participating systems had to retrieve the correct answer(s).</p><p>A set of 25 training questions was provided in an XML format that is very similar to the one used for Tasks 1 and 2. However, for this task, not only the RDF triples are relevant, but also the English abstracts, related to a resource by means of the property abstract.</p><p>All questions are annotated with a pseudo query and the correct answers. The pseudo query is like an RDF query but can contain free text as subject, property, or object of a triple. This free text is marked as text:"...". Here is an example pseudo query for the question Give me the currencies of all G8 countries: PREFIX dbo : &lt; http :// dbpedia . org / ontology / &gt; SELECT DISTINCT ? uri WHERE { ? x text :" member of " text :" G8 " . ? x dbo : currency ? uri . } This pseudo query contains two triples: One is an RDF triple retrieving the currency of a country, which is information that is only available as RDF data, not in the abstracts. And the other contains free text reducing the list of countries to those that are a member of the G8, which is information that is contained only in the abstracts, not in the RDF data. For example, the abstract for Canada contains the following sentence:</p><p>Canada is a recognized middle power and a member of many international institutions , including the G7 , G8 , G20 , NATO , NAFTA , OECD , WTO , Commonwealth of Nations , Francophonie , OAS , APEC , and the United Nations .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation measures</head><p>The results submitted by participating systems were automatically compared to the gold standard results and evaluated with respect to precision and recall. For each question q, precision, recall and F-measure were computed as follows:</p><p>Recall(q) = number of correct system answers for q number of gold standard answers for q Precision(q) = number of correct system answers for q number of system answers for q F-Measure(q) = 2 * Precision(q) × Recall(q) Precision(q) + Recall(q)</p><p>On the basis of these measures, overall precision and recall values as well as an overall F-measure value were computed as the average mean of the precision, recall and F-measure values for all questions. In the results reported below, precision, recall and F-measure values refer to the averaged values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Participating systems</head><p>Eight teams participated in QALD-4: four teams from Europe (one from France, one from Germany and two from Romania) and four teams from Asia (three from China and one from South Korea). Six participants took part in Task 1, three participants took part in Task 2, and there was one participant for Task 3 that later withdrew his submission. In the following, we give some details on those participating systems that are also described in working note papers.</p><p>Xser <ref type="bibr" coords="5,175.84,609.29,10.52,8.74" target="#b7">[8]</ref> takes as input a natural language question in English, and retrieves an answer in two steps. First the user query is linguistically analyzed in order to detect predicate argument structures through a semantic parser. Second the query is instantiated with respect to the knowledge base. Besides the DAG dependency parsing it relies on a structured prediction approach implemented using a Collins-style hidden perceptron. The system requires training data but among all participants obtained the highest precision and recall values.</p><p>gAnswer <ref type="bibr" coords="6,197.69,146.00,10.52,8.74" target="#b8">[9]</ref> is a graph-driven question answering system that processes questions in two stages. First, based on the dependency parse of the question, a graph is build that represents the semantic structure of the question. Second, this graph is matched with subgraphs in the RDF dataset. Disambiguation takes place when evaluating subgraph matches. The system achieves real-time performance, requiring an average of 972 miliseconds to answer a question.</p><p>CASIA <ref type="bibr" coords="6,188.32,219.82,10.52,8.74" target="#b6">[7]</ref> proposes an algorithm based on Markov Logic Networks for learning a joint model for detecting phrases, mapping phrases to semantic items, and grouping semantic items into a graph. As a result, each step can be subject to global optimization. The system does not yet process questions which contain numbers and aggregation operations (such as filters, comparisons, or ordering), but shows very promising results on non-aggregation questions. It makes use of the Stanford Named Entity Recognizer, the PATTY and ReVerb resources, as well as thebeast tool<ref type="foot" coords="6,221.95,301.94,3.97,6.12" target="#foot_3">9</ref> for weight learning and MAP inferencing.</p><p>Intui3 <ref type="bibr" coords="6,183.87,317.56,10.52,8.74" target="#b0">[1]</ref> accepts as input a natural language question and constructs its interpretation using syntactic and semantic cues in the question and a target triple store. First, the question is syntactically analyzed and chunked, and the named entities are identified. Then each chunk receives one or more interpretation depending on its type and on additional semantic and syntactic information available for that chunk. The interpretation of the question is then constructed by combining the interpretations assigned to each chunk, based on a set of combination rules that are attached to each type of interpretation. Finally, the question interpretation is mapped to a corresponding SPARQL query, which is then run against a SPARQL endpoint to retrieve the answers.</p><p>ISOFT <ref type="bibr" coords="6,187.55,439.21,10.52,8.74" target="#b5">[6]</ref> follows a template-based approach for transforming natural language questions into SPARQL queries. Based on a linguistic analysis of the input question, query templates and slots are determined, which are then filled by searching for appropriate concepts in the knowledge base, based on string similarity and Explicit Semantic Analysis.</p><p>The Faculty of Computer Science at Alexandru Ioan Cuza University of Iasi, Romania, participated with two systems (RO FII), one tackling question answering over DBpedia and one tackling interlinked biomedical datasets. <ref type="foot" coords="6,451.08,523.42,7.94,6.12" target="#foot_4">10</ref> The former builds on Quepy 11 , a Python tool for transforming natural language questions into SPARQL or MQL queries. The latter comprises three components, based on Service Oriented Architecture principles: a text annotator that receives the question in plain text and returns a list of compound words annotated with POS tags and lemmas (using Standford Core NLP), the triple builder that builds a list of triples given a list of keywords and URIs (currently assembled manu-ally), and a query builder that builds the final SPARQL query on the basis of the list of annotated words and the list of triples.</p><p>GFMed <ref type="bibr" coords="7,192.48,144.40,10.52,8.74" target="#b4">[5]</ref> follows a controlled natural language approach for biomedical question answering. It builds on a Grammatical Framework<ref type="foot" coords="7,401.97,154.78,7.94,6.12" target="#foot_6">12</ref> (GF) grammar for the biomedical datasets DrugBank, Diseasome, and SIDER. GF is a specialpurpose programming language for writing multilingual grammars. For GFMed, an abstract syntax for the biomedical domain, spanning the concepts of the three datasets, as well as two concrete syntaxes that provide linearizations of those concepts, one for English and one for SPARQL, were created manually. In addition, a lexicon for both languages that covers all resource names was automatically constructed based on their labels. The resulting grammar allows to transform English questions into SPARQL queries by parsing the English input, yielding an abstract syntax representation that can then be linearized using the SPARQL concrete syntax. The approach can deal with complex questions and achieves a very high precision, but as any controlled language its coverage is limited, especially it does not easily scale to other domains.</p><p>POMELO <ref type="bibr" coords="7,204.20,312.27,10.52,8.74" target="#b1">[2]</ref> POMELO operates (in contrast to most other approaches) on frames. First, the RDF dataset is converted to frames: The predicates of the RDF triples are mapped to frame predicates while the subjects and objects are mapped to core frame elements. Each question is transformed to a SPARQL query using a four-step approach: First, the query is annotated with semantic and linguistic information using the converted resources. For example, numbers and resources from different datasets are tagged as such. Thereafter, in the question abstraction step, argument and predicate descriptions are used to generate a query template. This template is completed by using owl:sameAs and used to construct a SPARQL query skeleton, which is finally used to generate a final SPARQL query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Tables <ref type="table" coords="7,166.63,492.59,4.98,8.74" target="#tab_2">1</ref> and<ref type="table" coords="7,195.37,492.59,4.98,8.74" target="#tab_3">2</ref> report on the results obtained by the participating systems on Tasks 1 and 2, respectively. The column proc. states for how many of the questions the system provided an answer, right specifies how many of these questions were answered with an F-measure of 1, and part. specifies how many of the questions were answered with an F-measure strictly between 0 and 1.</p><p>The results in Task 1 are comparable to results achieved in earlier challenges, with an average F-measure of 0.33, showing that the level of complexity of the questions is still very demanding. But what has changed with respect to earlier challenges is that question answering systems have become more versatile: There is no particular type of questions anymore that systems struggle with, rather most of them can handle all answer types as well as aggregation. The biggest problem, however, remains the matching of natural language questions to correct vocabulary elements. For example, the questions that all systems struggled with are surprisingly simple with respect to the linguistic structure and the structure of the target query:</p><p>-How deep is Lake Placid? SELECT ?n WHERE { res:Lake Placid (Texas) dbo:depth ?n . } -Which spaceflights were launched from Baikonur?</p><p>SELECT ?uri WHERE { ?uri dbo:launchPad res:Baikonur Cosmodrome . } 6 Future perspectives QALD-4, the fourth edition of the QALD challenge, has attracted a higher number of participants than previous editions, showing that there is a growing interest among researchers to provide end users with an intuitive and easy-to-use access to the huge amount of data present on the Semantic Web. Although one of the aspects of Task 1 was multilinguality, all participating systems worked on English data only. This shows that the multilingual scenario is not yet broadly addressed, although it is starting to attract attention. Similarly, research teams start to look at hybrid question answering, although Task 3 did not have participating systems yet.</p><p>In future challenges, we want to emphasize further aspects of question answering over linked data, such as including statistical question answering (e.g. How much money was spent for public transport in Berlin in 2014?), introducing spoken language in addition to written language (an aspect that is interesting especially for search engines) as well as dialogue-based interaction into the challenge, allowing the system to ask for feedback or clarification, as well as the user to refer to previous questions and answers, thus moving to question answering systems that can exploit the previous interaction context in interpreting new questions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,154.61,116.91,306.13,101.91"><head>Table 1 .</head><label>1</label><figDesc>Results for Task 1: Multilingual question answering over DBpedia Total Proc. Right Part. Recall Precision F-measure</figDesc><table coords="8,186.43,156.14,230.97,62.68"><row><cell>Xser</cell><cell>50</cell><cell>40</cell><cell>34</cell><cell>6</cell><cell>0.71</cell><cell>0.72</cell><cell>0.72</cell></row><row><cell cols="2">gAnswer 50</cell><cell>25</cell><cell>16</cell><cell>4</cell><cell>0.37</cell><cell>0.37</cell><cell>0.37</cell></row><row><cell>CASIA</cell><cell>50</cell><cell>26</cell><cell>15</cell><cell>4</cell><cell>0.40</cell><cell>0.32</cell><cell>0.36</cell></row><row><cell>Intui3</cell><cell>50</cell><cell>33</cell><cell>10</cell><cell>4</cell><cell>0.25</cell><cell>0.23</cell><cell>0.24</cell></row><row><cell>ISOFT</cell><cell>50</cell><cell>28</cell><cell>10</cell><cell>3</cell><cell>0.26</cell><cell>0.21</cell><cell>0.23</cell></row><row><cell cols="2">RO FII 50</cell><cell>50</cell><cell>6</cell><cell>0</cell><cell>0.12</cell><cell>0.12</cell><cell>0.12</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,142.88,232.87,329.58,69.04"><head>Table 2 .</head><label>2</label><figDesc>Results for Task 2: Biomedical question answering over interlinked data</figDesc><table coords="8,183.08,253.67,249.20,48.24"><row><cell></cell><cell cols="7">Total Proc. Right Part. Recall Precision F-measure</cell></row><row><cell>GFMed</cell><cell>25</cell><cell>25</cell><cell>24</cell><cell>1</cell><cell>0.99</cell><cell>1.0</cell><cell>0.99</cell></row><row><cell cols="2">POMELO 25</cell><cell>25</cell><cell>19</cell><cell>3</cell><cell>0.87</cell><cell>0.82</cell><cell>0.85</cell></row><row><cell>RO FII</cell><cell>25</cell><cell>25</cell><cell>4</cell><cell>0</cell><cell>0.16</cell><cell>0.16</cell><cell>0.16</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_0" coords="1,144.73,658.44,198.69,7.47"><p>http://www.sc.cit-ec.uni-bielefeld.de/qald</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_1" coords="2,144.73,658.44,84.73,7.47"><p>http://dbpedia.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_2" coords="3,144.73,658.44,94.64,7.47"><p>http://lod-cloud.net</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_3" coords="6,144.73,625.56,155.34,7.47"><p>http://code.google.com/p/thebeast</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_4" coords="6,144.73,635.88,335.85,7.86;6,144.73,646.84,114.58,7.86"><p>The former was built by Andrei Micu, the latter was built by Claudiu Epure, both supervised by Adrian Iftene.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_5" coords="6,144.73,658.44,127.09,7.47"><p>http://quepy.machinalis.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_6" coords="7,144.73,658.44,164.75,7.47"><p>http://www.grammaticalframework.org</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,138.35,187.42,342.24,7.86;9,146.91,198.38,117.11,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,208.22,187.42,203.10,7.86">Answering natural language questions with Intui3</title>
		<author>
			<persName coords=""><forename type="first">Corina</forename><surname>Dima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,433.28,187.42,47.31,7.86;9,146.91,198.38,88.82,7.86">CLEF 2014 Working Notes Papers</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,209.34,342.22,7.86;9,146.91,220.30,333.67,7.86;9,146.91,231.26,79.57,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,434.55,209.34,46.02,7.86;9,146.91,220.30,223.26,7.86">Description of the POMELO system for the task 2 of QALD-2014</title>
		<author>
			<persName coords=""><forename type="first">Natalia</forename><surname>Thierry Hamon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fleur</forename><surname>Grabar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Frantz</forename><surname>Mougin</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Thiessard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,394.61,220.30,85.97,7.86;9,146.91,231.26,51.28,7.86">CLEF 2014 Working Notes Papers</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,242.22,342.23,7.86;9,146.91,253.17,296.31,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,437.19,242.22,43.39,7.86;9,146.91,253.17,142.73,7.86">Evaluation question answering over linked data</title>
		<author>
			<persName coords=""><forename type="first">Vanessa</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christina</forename><surname>Unger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Philipp</forename><surname>Cimiano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Enrico</forename><surname>Motta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,297.84,253.17,104.59,7.86">Journal of Web Semantics</title>
		<imprint/>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct coords="9,138.35,264.13,342.23,7.86;9,146.91,275.09,331.49,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,435.67,264.13,44.91,7.86;9,146.91,275.09,182.11,7.86">Is question answering fit for the semantic web?: A survey</title>
		<author>
			<persName coords=""><forename type="first">Vanessa</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Victoria</forename><forename type="middle">S</forename><surname>Uren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marta</forename><surname>Sabou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Enrico</forename><surname>Motta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,337.07,275.09,55.39,7.86">Semantic Web</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="125" to="155" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,286.05,342.22,7.86;9,146.91,297.01,285.89,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,223.85,286.05,256.73,7.86;9,146.91,297.01,98.43,7.86">GFMed: Question answering over biomedical linked data with Grammatical Framework</title>
		<author>
			<persName coords=""><forename type="first">Anca</forename><surname>Marginean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,265.68,297.01,138.84,7.86">CLEF 2014 Working Notes Papers</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,307.97,342.23,7.86;9,146.91,318.93,333.67,7.86;9,146.91,329.89,117.11,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,383.23,307.97,97.35,7.86;9,146.91,318.93,268.32,7.86">ISOFT at QALD-4: Semantic similarity-based question answering system over linked data</title>
		<author>
			<persName coords=""><forename type="first">Seonyeong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hyosup</forename><surname>Shim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gary</forename><surname>Geunbae</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,434.15,318.93,46.44,7.86;9,146.91,329.89,88.82,7.86">CLEF 2014 Working Notes Papers</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,340.85,342.22,7.86;9,146.91,351.80,333.68,7.86;9,146.91,362.76,20.99,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,370.17,340.85,110.41,7.86;9,146.91,351.80,172.24,7.86">CASIA@V2: A MLN-based question answering system over linked data</title>
		<author>
			<persName coords=""><forename type="first">He</forename><surname>Shizhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhang</forename><surname>Yuanzhe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,338.30,351.80,138.06,7.86">CLEF 2014 Working Notes Papers</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,373.72,342.23,7.86;9,146.91,384.68,300.17,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,328.24,373.72,152.34,7.86;9,146.91,384.68,113.46,7.86">Answering natural language questions via phrasal semantic parsing</title>
		<author>
			<persName coords=""><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,279.96,384.68,138.84,7.86">CLEF 2014 Working Notes Papers</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,395.64,342.23,7.86;9,146.91,406.60,333.66,7.86;9,146.91,417.56,191.89,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,175.45,406.60,305.13,7.86;9,146.91,417.56,25.25,7.86">Natural langauge question answering over RDF -a graph data driven approach</title>
		<author>
			<persName coords=""><forename type="first">Lei</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruizhe</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haixun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><forename type="middle">Xu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wenqiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,191.98,417.56,118.49,7.86">Proceedings of SIGMOD 2014</title>
		<meeting>SIGMOD 2014</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
