<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,181.70,152.67,231.77,12.64;1,224.69,170.67,145.80,12.64">French run of Synapse Développement at Entrance Exams 2014</title>
				<funder>
					<orgName type="full">Universidad del Pais Vasco</orgName>
				</funder>
				<funder ref="#_hwCGEZu">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder>
					<orgName type="full">Universidad Nacional de Educación a Distancia</orgName>
				</funder>
				<funder ref="#_UamhA7n">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,157.34,210.18,76.74,8.96"><forename type="first">Dominique</forename><surname>Laurent</surname></persName>
							<email>dlaurent@synapse-fr.com</email>
							<affiliation key="aff0">
								<orgName type="department">Synapse Développement</orgName>
								<address>
									<addrLine>5 rue du Moulin-Bayard</addrLine>
									<postCode>31000</postCode>
									<settlement>Toulouse</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,240.78,210.18,67.56,8.96"><forename type="first">Baptiste</forename><surname>Chardon</surname></persName>
							<email>baptiste.chardon@synapse-fr.com</email>
							<affiliation key="aff0">
								<orgName type="department">Synapse Développement</orgName>
								<address>
									<addrLine>5 rue du Moulin-Bayard</addrLine>
									<postCode>31000</postCode>
									<settlement>Toulouse</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,315.47,210.18,52.65,8.96"><forename type="first">Sophie</forename><surname>Nègre</surname></persName>
							<email>sophie.negre@synapse-fr.com</email>
							<affiliation key="aff0">
								<orgName type="department">Synapse Développement</orgName>
								<address>
									<addrLine>5 rue du Moulin-Bayard</addrLine>
									<postCode>31000</postCode>
									<settlement>Toulouse</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,375.17,210.18,62.33,8.96"><forename type="first">Patrick</forename><surname>Séguéla</surname></persName>
							<email>patrick.seguela@synapse-fr.com</email>
							<affiliation key="aff0">
								<orgName type="department">Synapse Développement</orgName>
								<address>
									<addrLine>5 rue du Moulin-Bayard</addrLine>
									<postCode>31000</postCode>
									<settlement>Toulouse</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,181.70,152.67,231.77,12.64;1,224.69,170.67,145.80,12.64">French run of Synapse Développement at Entrance Exams 2014</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E6AAE27E00C530E8954FD97EC0D6BDDE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Question Answering</term>
					<term>Machine Reading</term>
					<term>Natural Language Understanding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article presents the participation of Synapse Développement to the CLEF 2014 Entrance Exam campaign (QA track). Since fifteen years, our company works on Question Answering domain. Recently our work concentrated on Machine Reading and Natural Language understanding. Thus, the Entrance Exam evaluation was an excellent opportunity to measure the results of this work. The developed system is based on a deep syntactic and semantic analysis with anaphora resolution. The results of this analysis are saved in sophisticated structures based on clause description (CDS = Clause Description Structure). For this evaluation, we added a dedicated module to compare CDS from texts, questions and answers. This module measures the degree of correspondence between these elements, taking into account the type of question, which means the type of answer awaited. We participate in English and French languages; this article focuses on the French run. This run obtains the best results (33 good answers on 56) when our run for English was in second place. So, in French, our system can pass the entrance exam for University!</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Entrance Exams evaluation campaign uses real reading comprehension texts coming from Japanese University Entrance Exams (the Entrance Exams corpus for the evaluation is delivered by NII's Todai Robot Project <ref type="bibr" coords="1,351.71,590.27,16.61,8.96">[13]</ref> and NTCIR RITE). These texts are intended to be used to test the level of English of future students and represent an important part in Japanese University Entrance Exams <ref type="foot" coords="1,386.83,612.08,3.24,5.83" target="#foot_0">1</ref> . As claimed by the organizers of this campaign: " The challenge of "Entrance Exams" aims at evaluating systems under the same conditions humans are evaluated to enter the University" 2 . Our Machine Reading system is based on a major hypothesis: The text, in its structure and in its explicit and implied syntactic functions, contains enough information to allow Natural Language Understanding with a good accuracy. So our system does not use any external resources, i.e. Wikipedia, DbPedia and so on. Our system uses only our linguistic modules (parsing, word sense disambiguation, named entities detection and resolution, anaphora resolution) and our linguistic resources (grammatical and semantic information on more than 300,000 words and phrases, global taxonomy on all these words, thesaurus, families of words, converse relation dictionary (for example, "acheter" and "vendre", or "se marier"), and so on). These software modules and linguistic resources are the results of more than twenty years of development and are considered and evaluated as the state of art for French and English.</p><p>Our Machine Reading system and the Multiple-Choice Question-Answering system needed for Entrance Exams use a database built with the results of our analysis that results in a set of Clause Description Structures (CDS) to be described in the second chapter of this article.</p><p>The Entrance Exams corpus was composed this year of 12 texts with a total of 56 questions. Knowing that for each question 4 answers are proposed, the total number of choices/options was 224. Organizers of the evaluation campaign allow the systems to leave some questions unanswered if theses systems are not confident in the correctness of the answer. We did not use this opportunity but we will give in chapter 3 some results when leaving unanswered questions where the probability of the best answer is too low and other results when leaving unanswered questions where the probability of the best answer is not superior or equal to the double of the probability of the second best answer, and we will give results with different translations of the texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Machine Reading System architecture</head><p>For Entrance Exams, similar treatments are made for texts, questions and answers but the results of these treatments are saved in three different databases, allowing the final module to compare the Clause Description Structures (CDS) from text and answers to measure the probability of correspondence between CDS from text and CDS from answers. Figure <ref type="figure" coords="2,212.30,531.23,4.98,8.96">1</ref> shows the the global architecture of our system. Figure <ref type="figure" coords="3,261.38,150.18,3.76,8.96">1</ref>. Description of the system</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Conversion from XML into text format</head><p>The XML format allows our system to distinguish text, questions and answers. So, it's very useful but our different linguistic modules manage only text format. So the first operation is to extract text, then each question and the corresponding answers in text format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Parsing, Word Sense Disambiguation, Named Entities detection</head><p>We use our internal parser which begins by a lexical disambiguation (is it a verb? a noun? a preposition? and so on) and a lemmatization. Then the parser splits the different clauses, groups the phrases, sets the part of speech and searches all grammatical functions (subject, verb, object, direct or indirect, other complements).</p><p>Then, for all polysemous words, a Word Sense Disambiguation module detects the sense of the word. For English, this detection is successful in 82% of word senses, but for French, with a higher number of polysemous words and a higher number of senses for each word, the rate of success is about 87%). The senses disambiguated are directly linked in our internal taxonomy.</p><p>A named entity detector groups the named entities. The Named Entities detected are: names of persons, organizations and locations, but also functions (director, student, etc.), time (relative or absolute), numbers, etc. These entities are linked between them when they refer to the same entity (for example "Dominique Strauss-Kahn" or "DSK", "Toulouse" or "la Ville rose", etc). This module is not very useful for this Entrance Exams campaign except for time entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Anaphora resolution</head><p>In French, we consider as anaphora all the personal pronouns (je, tu, il, elle, nous, vous, ils, le, la, les, leur, me, moi, te, toi, lui, soi, se), all demonstrative pronouns and adjectives (celui, celle, ceux, celles), all possessive pronouns and adjectives (ma, mon, ta, ton, sa, son, nos, notre, vos, votre, leur, leurs) and, of course, the relative pronouns (que, qui, lequel, laquelle, lesquelles, auquel, à laquelle, auxquels, auxquelles, dont).</p><p>During the parsing, the system builds a table with all possible referents for anaphora (proper nouns, common nouns, phrases, clauses, citations) with a lot of grammatical and semantic information (gender, number, type of named entity, category in the taxonomy, sentence where the referent is located, number of references for this referent, etc.) and, after the syntactic parsing and the word sense disambiguation, we resolve the different anaphora in the sentence by comparison with our table of referents. Our results at this step are good, equivalent or best than the state of the art. The pronouns "en" and "y" have the lower rate of success (less than 60%, because they refer frequently to clauses, sentence, sometimes paragraphs, and the resolution is complex for these pronouns). The differentiation between the impersonal form "il" (like in the sentence "il fait beau") and the normal pronoun "il" obtains good results with 93% of good resolution..</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Implied to explicit relations</head><p>When there are coordinate subjects or objects (for example "Papa et Maman"), our system keeps the trace of this coordination. For example with the coordination " Papa et Maman " the system will save three different CDS, one with the coordinate subject and two for each term of the subject. The aim of this division is to find possible answers with only one term of the coordination. But, beyond this very simple decomposition, our analyzer operates more complex operations. For example, in the sentence " bien sûr, plusieurs animaux, surtout les plus jeunes, ont des comportements qui paraissent ludiques ", extracted from third text of this evaluation, our system will add "animals" after "les plus jeunes", this type of completion is very close of anaphora resolution but different because the system tries to add implied information, which are generally nouns or verbs. This mechanism exists also for the CDS structures as described in the next paragraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Making and saving CDS</head><p>We describe in this Section the main features of CDS structures. First we consider the attribute as an object (that could be discussed, but it allows one model of structure only). The main components of the structure are descriptions of a clause, normally compound of a subject, a verb and an object or attribute. Of course the structure allows many other components, for example indirect object, temporal context, spatial context... Each component is a sub-structure with the complete words, the lemma, the possible complements, the preposition if any, the attributes (adjectives) and so on.</p><p>For verbs, if there is some modal verb, only the last verb is considered but the modality relation is kept in the structure. Of course negation or semi-negation (forget to) are also attributes of the verb in the structure. If a passive form is encountered, the real subject becomes the subject of the CDS and the grammatical subject becomes the object. When the system encountered possessive adjective, a specific CDS is created with a link of possession. For example, in the sentence "Il me parlait souvent de sa terre d´origine, située au Wisconsin." where "il" is the referent of a Winnebago Indian, the system creates one CDS with "Indien Winnebago" as subject, "parler" as verb, "Je" as indirect objet and "terre d'origine" as direct object. But the system creates also another CDS with " Indien Winnebago " as subject, "avoir" as verb (possession), "terre d'origine" as object and "Wisconsin" as spatial context.</p><p>New CDS are also created when there is a converse relation. For example, in the sentence "Ne t´en fais pas, papa " dit Patrick.", where "papa" is the author (anaphora resolution from precedent sentences), the system will extract one CDS with "Je" (the author) as subject, "s'en faire" as verb, "père" as object and "Patrick" as complement of "père", but also another CDS with "Patrick" as subject, "être" as verb, "fils" as object and "Je" as complement of "fils". The system manages 347 different converse relations, for example the classical "acheter" and "vendre", or "se marier", or "patron" and "employé", but also geographic terms (sud/nord, dessous/dessus...) and time terms (avant/après, precedent/suivant...). For all these links, two CDS are created.</p><p>Links between CDS are also saved. For example, in the sentence " il est nécessaire d´accroître et d´améliorer les opportunités offertes aux personnes âgées afin qu´elles puissent apporter leur contribution à la bonne marche de la société avec dignité et respect ", we have three CDS ("Il est nécessaire d'accroître", "il est nécessaire d'améliorer" and "elles puissent apporter") but CDS and CDS2 have a relation of aim with CDS3 and CDS3 has a relation of consequence with CDS1 and CDS2. Other relations like "cause", "judgment", "opinion" and so on are also saved and are important when the system matches the CDS of the text and the CDS of the possible answers. At the end, after all these extensions, we can consider that a real semantic role labelling is performed.</p><p>Finally the system saves also "referents", which are proper and common nouns found in the sentences, after anaphora resolution. These referents are especially useful when the system do not find any correspondence between CDS, knowing that the frequencies in text and in usual vocabulary are arguments of the referent structures.</p><p>A specific difficulty of Entrance Exams corpus is that it is frequently spoken language with dialogs like in novels. It needs a deep analysis of the characters as you can imagine with some sentences like " " Et mes amis alors?" "Ne t´en fais pas Elena, tu t´y feras de nouveaux amis." Je ne voulais pas avoir de nouveaux amis, je voulais garder mes anciens amis et rester au brésil avec mes grands-parents, mes tantes et oncles, mes cousins et cousines.", where nothing indicates the author, except "Elena" in the second sentence, which can be considered as the author "Je".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Comparing CDS and Referents</head><p>This part of our system has been partially developed for Entrance Exams evaluation, due to the specificities of this evaluation, specially the triple structure text/questions/answers. Once each text analyzed, each question is analysed, then the four possible answers are analyzed. The questions have generally no anaphora or these anaphors refer to words in the question, but the system needs to consider that "the author" (or, sometimes, "the writer") is "Je" in the text. Anaphors in questions are very common and the referents are in the answer (rarely) or in the question (more commonly). For example, in the answer " Pourquoi l´auteur demanda t-il que Margaret lui envoyât des photos à elle?", the pronoun "elle" refers to "Margaret" in the question. Unfortunately, the translation is false because in English, the question is "Why did the author ask Margaret for her picture?" and the good translation is " Pourquoi l´auteur demanda t-il que Margaret lui envoyât des photos d'elle?".</p><p>When the question is analyzed, besides the CDS structures, the system extracts the type of the question like in our Question Answering system. In Entrance Exams, these types are always non-factual types like cause ("Qu´est ce qui poussa l´auteur à vouloir avoir un correspondant à l´étranger?"), sentiment ("Quelles furent les impressions de l´auteur quand il vit la photo de Margaret?"), aim ("Pourquoi l´auteur demanda t-il que Margaret lui envoyât des photos à elle?"), signification ("Pourquoi l´auteur dit qu´il pourrait " ravaler ses paroles?"), event ("L´expérience a démontré qu´après un certain temps, les rats") and so on. Frequently, parts of the question need to be integrated into the answers. In the last sentence, for example, the nominal group "les rats" needs to be added at the beginning of the answers. In this case, first answer "N´avaient pas de préférence pour un chemin" will become "the rats n´avaient pas de préférence pour un chemin".</p><p>Once the CDS and the type are extracted of the question, referents and temporal and spatial contexts (if they can be extracted from the question) are used to define the part of the text where elements of the answer are the most probable. For example, in the third text where is the precedent question about "les rats", this noun appears only in the second half of the text, so the target of the answers is the second half, not the first one, i.e. CDS of the second half will weight more than CDS from the first half and CDS with rats (the noun or an anaphora referring to this noun) will weight more.</p><p>In a first time, the system eliminates answers where there is no correspondence between CDS, referents and type of question/answer. There are very few cases, only 9 on 224 answers in French. More generally, it seems that the method consisting to reduce the choices between answers by elimination of inadequate answers is extremely difficult to implement. Because, probably, answers are made to test the comprehension of the texts by humans and, frequently, the answer which seems to be the best choice (i.e. which integrates the bigger number of words from the text) is not the good one... and, reciprocally, the answer which seems the farest is frequently the good one! For the answers, two tasks are very important: adding eventually part of the question (described above) and resolution of anaphora. Hopefully the resolution of anaphora is easiest on question and answers than in the text. The number of possible referents is reduced and, testing on the evaluation run, we found that the system made only one error in French (two in English). In fact this error is due to the translation and repeated for all the answers of the question "Où se trouvait la maison de madame Tortino au moment où le nouvel édifice était construit?" All the answers begin by "il", which seems to refer to "nouvel édifice" in the question. In fact, all the answers needed to begin by "elle" which refers to "la maison".</p><p>Equivalences between the subject "I" and a proper noun is not so frequent in the evaluation test as it is in the training corpus. But this equivalence is not so evident for the text 23 (next to last) where this equivalence needs to be deducted from: J´avais seulement sept ans à cette époque mais je m´en rappelle fort bien. " Elena, nous allons au Japon."" And this equivalence is very important because "Elena" is the subject of four questions out of five! To compare CDS of answers and CDS of text, we compare each CDS of text to each CDS of each answer, taking into account a coefficient of proximity of the target and the number of common elements. Subject and verb have bigger weight than object, direct or indirect, which have bigger weight than temporal and spatial context. If the system finds two elements in common, the total is multiplied by 4, if three elements are in common the total is multiplied by 16, etc. The system also increases the total when there is a correspondence with the type of the question. If only one element or no element is common to the CDS, the system takes into account the categories of our ontology, increasing the total if there is a correspondence. The total is slightly increasing if there are common referents. The total is cumulative with all the CDS of the text and finally divided by the number of CDS in the answer (often one, no more than three in the evaluation corpus).</p><p>At the end, we have, for each answer, a coefficient which ranges from 0 to 32792 (in the evaluation test, because there is no upper limit). The answer with the biggest coefficient is considered as the correct answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>Our system answered correctly to 33 questions out of 56 (c@1 = 0.59). The χ² is 34.37 (i.e. a probability of 0,0001 % that these results were obtained randomly). Knowing that, randomly, a system will obtain an average 25% of good answers, in this case 14 good answers. Thus, we outperform random from 19 good answers, which is not a very good result because it means that all our syntactic and semantic methods perform only an improvement of 19 answers out of 42 (total of 56 questions decreased of 14 due to random). Even if this result is the best one, we cannot consider that our main hypothesis is verified. It seems clear for us now that, without pragmatic knowledge and natural language inference, it's impossible to obtain more than 0.6.</p><p>With the run results files, we tested different hypothesis (see Figure <ref type="figure" coords="7,410.35,363.21,3.76,8.96">2</ref>, Results with different filters for answers). In a first hypothesis, we keep only answers where the probability of the best answer is superior or equal to 1000. In this case, we have 19 good answers on 29 questions. Even if the percentage of success is 66%, in fact the c@1 is equal to 0,503, which is lower than the result on 56 questions. If we keep only the questions where the probability of the best answer is superior or equal to 500, we obtain 24 good answers on 38. In this case, results are better: the percentage of success is 63% and the c@1 is equal to 0,567, close to our result of 0,589 on the total of questions. Finally we keep only the answers where the probability of the best answer is almost twice the probability of the second best answer. In this case, we obtain 12 good answers on 18, which is a good result with 67% of successful answers but a c@1is equal to 0.360 because the system answers only 30% of the questions!</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>% successful c@ Finally, keep all the questions and all the answers was the best strategy and our system pass the Entrance Exams for the Japanese University! If we look to the results text by text, on the 12 texts, 9 are superior or equal to 50%, for two texts the result is 40% and for one text 20%.</p><p>Even if, in French, our system obtains a score sufficient to pass the Entrance Exam, there is an area where the computer is clearly superior to the human: speed. The French run is executed in 3.3 seconds, which means a speed of about 2500 words by second. Because we did not try to optimize the code, this speed could be better (the speed of our parser is more than 10000 words by second), specially if we rewrite the comparison between CDS of text and CDS of answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis of results</head><p>Last year <ref type="bibr" coords="8,175.40,243.18,11.69,8.96" target="#b0">[1]</ref> [2] [10] <ref type="bibr" coords="8,223.02,243.18,15.31,8.96" target="#b13">[15]</ref>, like this year, there were 5 participants, but only 10 runs (29 runs this year). On these 10 runs, 3 obtain results superior to random and 7 inferior or equal to random. This year, out of 29 runs, 14 obtain results superior to random and 15 inferior or equal to random. If we consider as a good result needs to be independant of chance with a probability higher than 95%, the χ² needs to be superior or equal to 3.84. Last year, only one run had a χ² superior to 3.84, this year only four runs have a χ² superior to 3.84.</p><p>These calculations demonstrate the difficulty of the task. The fact that more than half of the runs, this year and last year, obtained results inferior or equal to random, shows that classical methods used in Question Answering don't work on these comprehension reading tests. These tests have been written by humans to evaluate the reading comprehension of humans. So, for example, the answer which seems the best, i.e. which includes the higher number of words from the text, is generally a bad answer.</p><p>To demonstrate that with our run, we will take two examples, the first one is very basic, the second one is more complex. As you can imagine, our system finds the good answer in the first case, not in the second case. The easiest question/answer is extracted from text 16:</p><p>Qu´est-ce que l´homme aux lunettes faisait dans la salon de coiffure quand le narrateur le rencontra?</p><p>1. Il se faisait coiffer. 2. Il était en rang à l´extérieur.</p><p>3. Il parlait à d´autres personnes. 4. Il attendait son tour. Some words in the question like "lunettes" or "salon de coiffure" indicate that the target is at about 10% of the text, with the sentences: Je vais vous donner l´exemple d´un homme que j´ai un jour rencontré dans un salon de coiffure de Chicago. Attendant son tour, il se mit à me regarder au travers de ses lentilles binoculaires quand je vins m´asseoir près de lui.</p><p>Even with a "bag of words" method, the answer 4 can be found as the good one, considering the correspondence "attendant son tour ". A simple resolution of anaphora indicates that the subject of "attendant" is "un homme", so the coefficient of confidence becomes very high. For this question, the coefficient of the answer 4 is 1478, and the coefficients for answers 1, 2 and 3 are respectively 47, 148 and 66.</p><p>The second example is considerably more complex and our system didn't find the good answer. It is the first question of the text 22:</p><p>Pourquoi madame Tortino acepta l´offre de l´homme au chapeau melon?</p><p>1. Il lui promis plus de soleil sans lui offrir de l´argent.. 2. Il lui dit qu´il allait construire une maison semblable à la précédente.</p><p>3. Il lui promit qu´elle n´aurait pas à déménager de sa maison. 4. Il lui demanda d´aménager dans un nouvel immeuble situé à la même addresse.</p><p>You will notice that the question includes a spelling error on the word "acepta" (in place of "accepta"). The first answer includes also a grammar error ("promis" in place of "promit") and the fourth answer includes a spelling error on "addresse" (in place of "adresse"). The words " homme au chapeau melon " indicate a target at about 30% of the text, with the sentences:</p><p>Puis un jour, en début de printemps, un homme au chapeau melon se dirigea vers sa porte. Il paraissait différent de ceux qui étaient venus précédemment. Il fit le tour de la maison ombragée, regardant les larges ombres du jardin et inspirant l´air malsain. A première vue, madame Tortino pensa qu´il lui proposerait de l´argent pour acquérir la maison, comme l´avaient fait tous ses prédécesseurs. Mais quand il commença à parler, elle ouvrit grand ses yeux et l´écouta. " Pouvez-vous vraiment le faire?" demanda-t-elle. L´homme fit oui de la tête. " Un haut bâtiment à l´emplacement de ma maison, sans la démolir....?" " effectivement " Dit il. " Votre maison sera sous les mêmes cieux, dans la même rue et à la même adresse. Nous ne toucherons rien à ce qui s´y trouve pas même à Pursifur." " Et aurait-je de l´argent pour acheter plus de semences de tomate et de fleurs, et de la nourriture pour Pursifur?" "Bien sûr," répondit l´homme, en souriant. Madame Tortino fixa l´homme au chapeau melon pendant un long moment et finalement, elle dit, " D´accord!" Et ils se serrèrent les mains. Après le départ de l´homme, Madame Tortino baissa le regard vers Pursifur." N´est ce pas superbe?" dit elle. "J´ai de la peine à croire que cela sera possible!".</p><p>To answer the question, in fact next sentences are needed but we keep here only sentences which are at the target. As you can read, many facts are implied in the text. To choose the good answer (3 for this question), you need to know that if a house is in the same street and at the same address, then there is no moving... except if you need to go from a house in a building (answer 4). You also need to know that saying "D'accord" and "se serrèrent les mains" is similar to "acepta l'offre". Our system returned the answer 2, due to the repetition of "même" and the multiple instances of "construire" in the text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Errors and translations</head><p>Data used for the French campaign includes numerous spell and grammar errors, in the texts, in the questions and in the answers. In order to see the impact of these errors on the system, we correct 213 spelling, typographic and grammar errors and we launched our system on this corrected file. We obtained 36 good answers (36/56 = 0,64), i.e. 3 new good answers that we did not obtain with the original data with er-rors. The difference is not so big because, hopefully, many errors have no impact on the global process.</p><p>The organizers of the campaign asked different translators to translate the evaluation data and they asked us to test our system with 4 additional translation sets (that we will name T1, T2, T3, T4, keeping T0 for the original data used in the evaluation campaign). Note that these translations still include some errors but less than the original data. Some translations contain only grammar errors, probably because the translator used a spelling checker. So we ran the system with these four additional translations, reviewed or not. The Figure <ref type="figure" coords="10,263.26,246.18,4.98,8.96">3</ref>  The correction of errors has a reduced impact on the quality of the results, except for the original evaluation test, probably because the number of errors is reduced on those additional translations. But the difference between the scores is very interesting because this difference comes from the quality of the translations. The translation which gives the best results (T2) is possibly the best translation because the translator tried to have a text in French close to the text in English. For T1, the quality of the translation is good but the translation is a loose translation and the words used in questions and answers are frequently different from the words in the text and different from the direct translation of the words in English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>All the software modules and linguistic resources used in this evaluation exist since many years and are the property of the company Synapse Développement. The parts developed for this evaluation are the Machine Reading infrastructure, some improvements of the resolution of anaphora in English and the complete module to compare CDS from text and answers. No external resources or natural language inference engine have been used.</p><p>With 33 good answers on 56 questions, the results are good and this run is the best run for any language. However, the limitations of the method appear clearly: to obtain more than 2/3 of good answers, pragmatic knowledge and inference are essential.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,153.86,246.18,281.56,102.47"><head></head><label></label><figDesc>below lists the results:</figDesc><table coords="10,153.86,264.66,281.56,83.99"><row><cell></cell><cell>Total of errors</cell><cell>Original text</cell><cell>Reviewed text</cell></row><row><cell>T0</cell><cell>213</cell><cell>33</cell><cell>36</cell></row><row><cell>T1</cell><cell>39</cell><cell>28</cell><cell>29</cell></row><row><cell>T2</cell><cell>28</cell><cell>36</cell><cell>37</cell></row><row><cell>T3</cell><cell>48</cell><cell>34</cell><cell>34</cell></row><row><cell>T4</cell><cell>56</cell><cell>32</cell><cell>33</cell></row><row><cell></cell><cell cols="3">Figure 3. Results with different translations</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,132.14,663.19,338.72,8.10;1,136.10,674.23,130.91,8.10"><p>See in References<ref type="bibr" coords="1,205.38,663.19,10.55,8.10" target="#b2">[3]</ref> and<ref type="bibr" coords="1,237.99,663.19,10.55,8.10" target="#b5">[6]</ref> but also http://www.ritsumei.ac.jp/acd/re/k-rsc/lcs/kiyou/4-5/RitsIILCS_4.5pp.97-116Peaty.pdf</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,124.70,683.36,3.00,5.40;1,132.26,685.27,123.97,8.10"><p>2 http://nlp.uned.es/entrance-exams/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. We acknowledge the support of the <rs type="projectName">CHIST-ERA</rs> project "<rs type="projectName">READERS Evaluation And Development of Reading Systems"</rs> (2012-2016) funded by <rs type="funder">ANR in France</rs> (<rs type="grantNumber">ANR-12-CHRI-0004</rs>) and realized with the collaboration of <rs type="funder">Universidad del Pais Vasco</rs>, <rs type="funder">Universidad Nacional de Educación a Distancia</rs> and</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_UamhA7n">
					<orgName type="project" subtype="full">CHIST-ERA</orgName>
				</org>
				<org type="funded-project" xml:id="_hwCGEZu">
					<idno type="grant-number">ANR-12-CHRI-0004</idno>
					<orgName type="project" subtype="full">READERS Evaluation And Development of Reading Systems&quot;</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>University of Edinburgh. This work benefited from numerous exchanges and discussions with these partners led within the framework of the project.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="11,132.67,242.99,337.77,8.10;11,141.74,254.03,328.74,8.10;11,141.74,265.07,328.84,8.10;11,141.74,275.99,24.09,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,369.66,242.99,100.78,8.10;11,141.74,254.03,75.10,8.10">NAIST at the CLEF 2013 QA4MRE Pilot Task</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,223.61,254.03,243.10,8.10">CLEF 2013 Evaluation Labs and Workshop Online Working Notes</title>
		<meeting><address><addrLine>Valencia -Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-09-26">23 -26 September, 2013 (2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.67,287.06,338.02,8.10;11,141.74,298.10,328.88,8.10;11,141.74,309.02,328.82,8.10;11,141.74,320.06,329.12,8.10;11,141.74,331.10,73.74,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,410.83,287.06,59.86,8.10;11,141.74,298.10,328.88,8.10;11,141.74,309.02,100.33,8.10">Multiple Choice Question (MCQ) Answering System for Entrance Examination, Question Answering System for QA4MRE@CLEF</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pakray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bandyopadhyay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,271.73,309.02,198.83,8.10;11,141.74,320.06,52.43,8.10">CLEF 2013 Evaluation Labs and Workshop Online Working Notes</title>
		<meeting><address><addrLine>Valencia -Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-09-26">2013. 23 -26 September, 2013 (2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.67,342.02,337.95,8.10;11,141.74,353.06,194.59,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,181.64,342.02,288.98,8.10;11,141.74,353.06,16.58,8.10">Testing Listening Comprehension in Japanese University Entrance Examinations</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Buck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,163.82,353.06,48.21,8.10">JALT Journal</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1 &amp; 2</biblScope>
			<biblScope unit="page">1988</biblScope>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.67,364.10,337.95,8.10;11,141.74,375.02,328.76,8.10;11,141.74,386.06,196.33,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,260.61,364.10,210.00,8.10;11,141.74,375.02,132.44,8.10">Using Anaphora resolution in a Question Answering system for Machine Reading Evaluation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Iftene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moruz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Ignat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,280.37,375.02,190.13,8.10;11,141.74,386.06,38.86,8.10">Notebook Paper for the CLEF 2013 LABs Workshop -QA4MRE</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-09-26">23-26 September. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.67,397.10,337.79,8.10;11,141.74,408.02,318.53,8.10" xml:id="b4">
	<monogr>
		<ptr target="http://www.indiana.edu/~best/bweb3/french-grammar-and-reading-comprehension-test/" />
		<title level="m" coord="11,233.01,397.10,233.92,8.10">French Grammar and Reading Comprehension Test</title>
		<imprint/>
		<respStmt>
			<orgName>Indiana University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.67,419.06,337.95,8.10;11,141.74,430.10,328.67,8.10;11,141.74,441.02,328.72,8.10;11,141.74,452.06,64.26,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,200.46,419.06,270.16,8.10;11,141.74,430.10,161.56,8.10">An Investigation of method effects on reading comprehension test performance, The Interface Between Interlanguage</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kobayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,309.62,430.10,160.79,8.10;11,141.74,441.02,152.39,8.10">Pragmatics and Assessment: Proceedings of the 3rd Annual JALT Pan-SIG Conference</title>
		<meeting><address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>Tokyo Keizai University</publisher>
			<date type="published" when="2004">May 22-23, 2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.67,463.10,337.87,8.10;11,141.74,474.04,328.74,8.10;11,141.74,485.08,328.75,8.10;11,141.74,496.12,46.86,8.10" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,276.53,463.10,194.00,8.10;11,141.74,474.04,113.42,8.10">Cross Lingual Question Answering using QRISTAL for CLEF 2005 Working Notes</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Séguéla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nègre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,261.48,474.04,209.00,8.10;11,141.74,485.08,201.40,8.10">CLEF Cross-Language Evaluation Forum, 7th Workshop of the Cross-Language Evaluation Forum, CLEF 2006</title>
		<meeting><address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-09-22">20-22 september 2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.67,507.04,337.87,8.10;11,141.74,518.08,328.88,8.10;11,141.74,529.12,297.96,8.10" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,276.53,507.04,194.00,8.10;11,141.74,518.08,35.68,8.10;11,204.29,518.08,246.40,8.10">Evaluation of Multilingual and Multi-Modal Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Séguéla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nègre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="11,454.06,518.08,16.56,8.10;11,141.74,529.12,113.47,8.10">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">4730</biblScope>
			<biblScope unit="page" from="339" to="350" />
			<date type="published" when="2006">2006. 2007. 2007</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>Cross Lingual Question Answering using QRISTAL for CLEF</note>
</biblStruct>

<biblStruct coords="11,132.67,540.04,337.87,8.10;11,141.74,551.08,328.76,8.10;11,141.74,562.12,298.01,8.10" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,276.53,540.04,194.00,8.10;11,141.74,551.08,34.97,8.10">Cross Lingual Question Answering using QRISTAL for CLEF</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Séguéla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nègre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,202.49,551.08,268.01,8.10;11,141.74,562.12,196.11,8.10">Working Notes, CLEF Cross-Language Evaluation Forum, 7th Workshop of the Cross-Language Evaluation Forum, CLEF 2008</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.40,573.04,337.72,8.10;11,141.74,584.08,329.00,8.10;11,141.74,595.12,328.88,8.10;11,141.74,606.04,60.29,8.10" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,352.75,573.04,117.38,8.10;11,141.74,584.08,105.74,8.10">Question Answering System for Entrance Exams in QA4MRE</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">L T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,256.13,584.08,214.61,8.10;11,141.74,595.12,33.26,8.10">CLEF 2013 Evaluation Labs and Workshop Online Working Notes</title>
		<meeting><address><addrLine>Valencia -Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-09-26">23 -26 September, 2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.40,617.08,337.88,8.10;11,141.74,628.12,24.09,8.10" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="11,203.94,617.08,99.96,8.10">Natural Language Inference</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Maccartney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009-06">June 2009 (2009</date>
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct coords="11,132.40,639.04,338.15,8.10;11,141.74,650.08,328.67,8.10;11,141.74,661.15,24.09,8.10" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,188.66,639.04,281.89,8.10;11,141.74,650.08,200.63,8.10">A Myth of Influence: Japanese University Entrance Exams and Their Effect on Junior and Senior High School Reading Pedagogy</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mulvey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,350.40,650.08,49.94,8.10">JALT Journal</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1999</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.40,149.99,337.80,8.10;12,141.74,161.03,328.77,8.10;12,141.74,172.07,328.85,8.10;12,141.74,182.99,187.51,8.10" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,240.04,161.03,230.47,8.10;12,141.74,172.07,104.51,8.10">Overview of QA4MRE at CLEF 2012: Question Answering for Machine Reading Evaluation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Forner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Á</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sutcliffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sporleder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Forascu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Benajiba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Osenova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,252.77,172.07,217.82,8.10;12,141.74,182.99,23.84,8.10">CLEF 2012 Evaluation Labs and Workshop Working Notes Papers</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-09-20">17-20 September, 2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.40,194.03,338.21,8.10;12,141.74,205.07,328.72,8.10;12,141.74,215.99,210.56,8.10" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,350.12,194.03,120.50,8.10;12,141.74,205.07,98.21,8.10">Overview of QA4MRE at CLEF 2013 Entrance Exams Task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Forner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,246.77,205.07,223.69,8.10;12,141.74,215.99,18.97,8.10">CLEF 2013 Evaluation Labs and Workshop. Online Working Notes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.40,227.03,337.80,8.10;12,141.74,238.07,328.73,8.10;12,141.74,248.99,329.00,8.10;12,141.74,260.03,213.98,8.10" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,241.61,238.07,228.86,8.10;12,141.74,248.99,17.58,8.10">Evaluating Machine Reading Systems through Comprehension Tests</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Forner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Á</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sutcliffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sporleder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Forascu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Benajiba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Osenova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,166.34,248.99,304.40,8.10;12,141.74,260.03,83.56,8.10">LREC 2012 Proceedings of the Eight International Conference on Language Resources and Evaluation</title>
		<meeting><address><addrLine>Istambul</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-05">May, 2012. 2012</date>
			<biblScope unit="page" from="21" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.40,271.07,338.30,8.10;12,141.74,282.02,328.97,8.10;12,141.74,293.06,312.85,8.10" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,235.11,271.07,158.79,8.10">A Simple Measure to Assess Non-response</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Á</forename><surname>Rodrigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,401.23,271.07,69.47,8.10;12,141.74,282.02,255.85,8.10">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Portland, Oregon</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">June 19-24, 2011. 2011</date>
			<biblScope unit="page" from="1415" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.40,304.10,338.09,8.10;12,141.74,315.02,328.59,8.10;12,141.74,326.06,328.93,8.10;12,141.74,337.10,123.56,8.10" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="12,230.78,315.02,235.43,8.10">Question Answering on Web Data : The QA Evaluation in Quero</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Quintard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Galibert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Adda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Grau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Moriceau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Tannier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vilant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,141.74,326.06,300.30,8.10">Proceedings of the Seventh Conference on Language Resources and Evaluation</title>
		<meeting>the Seventh Conference on Language Resources and Evaluation<address><addrLine>Valletta, Malta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-05">May, 2010. 2010</date>
			<biblScope unit="page" from="17" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.40,348.02,338.22,8.10;12,141.74,359.06,329.00,8.10;12,141.74,370.10,328.94,8.10;12,141.74,381.02,24.09,8.10" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="12,228.84,348.02,241.78,8.10;12,141.74,359.06,48.53,8.10">A Rule-based Question Answering System for Reading Comprehension Tests</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Thelen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,196.67,359.06,117.37,8.10;12,340.28,359.06,130.46,8.10;12,141.74,370.10,286.44,8.10">Workshop on Reading Comprehension Tests as Evaluation for computer-Based Language Understanding Systems</title>
		<imprint>
			<date type="published" when="2000">2000. 2000</date>
			<biblScope unit="page" from="13" to="19" />
		</imprint>
	</monogr>
	<note>Proceedings of ANL P/NAACL</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
