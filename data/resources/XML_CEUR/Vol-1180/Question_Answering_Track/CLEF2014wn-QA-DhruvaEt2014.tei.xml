<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,141.29,115.90,332.77,12.90;1,156.70,133.83,301.95,12.90">Solving Open-Domain Multiple Choice Questions with Textual Entailment and Text Similarity Measures</title>
				<funder ref="#_QJ7qwhC">
					<orgName type="full">Volkswagen Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,198.01,171.98,49.28,8.64;1,247.28,170.09,1.83,6.12"><forename type="first">Neil</forename><surname>Dhruva</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Ubiquitous Knowledge Processing Lab (UKP-TUDA</orgName>
								<orgName type="institution">Technische Universität Darmstadt</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,256.42,171.98,63.41,8.64;1,319.83,170.09,1.83,6.12"><forename type="first">Oliver</forename><surname>Ferschke</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Ubiquitous Knowledge Processing Lab (UKP-DIPF) German Institute for International Educational Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Ubiquitous Knowledge Processing Lab (UKP-TUDA</orgName>
								<orgName type="institution">Technische Universität Darmstadt</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,347.00,171.98,62.55,8.64;1,409.54,170.09,1.83,6.12"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Ubiquitous Knowledge Processing Lab (UKP-DIPF) German Institute for International Educational Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Ubiquitous Knowledge Processing Lab (UKP-TUDA</orgName>
								<orgName type="institution">Technische Universität Darmstadt</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,141.29,115.90,332.77,12.90;1,156.70,133.83,301.95,12.90">Solving Open-Domain Multiple Choice Questions with Textual Entailment and Text Similarity Measures</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C7B9EE3A1E8F833005F4CB2DE4AC4FBA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a system for automatically answering opendomain, multiple choice reading comprehension questions about short English narrative texts. The system is based on state-of-the-art text similarity measures, textual entailment metrics and coreference resolution and does not make use of any additional domain specific background knowledge. Each answer option is scored with a combination of all evaluation metrics and ranked according to their overall score in order to determine the most likely correct answer. Our best configuration achieved the second highest score across all competing system in the entrance exam grading challenge with a c@1 score of 0.375.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Question Answering (QA) systems have always been a key focus of Information Retrieval and Natural Language Processing research. Such systems aim to automatically answer questions posed in natural language. The objective of the Entrance Exams task in the CLEF Question Answering Track is the creation of a system for determining the correct answers to a set of multiple choice reading comprehension questions about English narrative texts.</p><p>The task demands a deep understanding of a short passage of text and is closely related to the QA4MRE<ref type="foot" coords="1,233.18,527.64,3.49,6.05" target="#foot_0">1</ref> main task <ref type="bibr" coords="1,283.57,529.31,15.27,8.64" target="#b17">[18]</ref>, which aims to focus on the concept of answer validation. However, the main difference between the main task, and the Entrance Exams task is the fact that no additional knowledge (background material) is provided for the latter. The aim is thus to evaluate automatic answering systems under the same conditions as humans are evaluated, considering that humans have certain background knowledge from real-life experiences. A detailed task definition is provided in Section 2.</p><p>Based on conclusions drawn from a state-of-the-art analysis, which is discussed in Section 3, our system uses three main components: coreference resolution, text similarity and textual entailment. Text-Hypothesis (T-H) pairs are generated for each entrance exam using combinations of sentences from the text, questions about the text and the corresponding answer options. These T-H pairs form the basic processing units for similarity and entailment analyses, which are employed to determine the correct answer to a question. We describe our approach and the architecture of our system in Section 4.</p><p>Finally, we present the results obtained with different system configuration in Section 5 and provide a detailed error analysis in Section 6. We close with a summarization of our findings and a discussion of future research directions in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Definition</head><p>The objective of the task tackled in this paper is to identify the correct answer option for a multiple choice question in a reading comprehension test for a given English narrative text. Since the questions are not restricted to a particular topic, they require a wide range of inference. Additionally, no background information is provided and systems are supposed to determine answers based on common sense knowledge that high school students are supposed to possess.</p><p>The dataset provided in this task is composed of reading comprehension tests taken from Japanese university entrance exams and is available in English, Russian, French, Spanish and Italian. The labeled training data consists of 12 documents with 60 questions and an average of 5 questions for each document. The test data comprises 12 documents with a total of 56 questions. We consider a test item to be a combination of the narrative text, a single question about this text and all answer options for this question. An exam is then the set of all test items with the same text. Each exam is evaluated with a score between 0 and 1 using the c@1 measure <ref type="bibr" coords="2,348.40,399.83,15.27,8.64" target="#b15">[16]</ref>, which is defined as:</p><formula xml:id="formula_0" coords="2,257.86,417.53,91.98,22.31">c@1 = 1 n n c + n u n c n</formula><p>where, n c is the number of correctly answered questions, n u , the number of unanswered questions, and n is the total number of questions. The main idea of this metric is to encourage systems to reduce the number of incorrect answers while maintaining the number of correct ones by leaving some questions unanswered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>The question answering task on university entrance exams was first introduced in this form at the CLEF QA4MRE Lab in 2013. In this section, we review the previous work from the pilot task. Related work on enabling technologies such as textual entailment is introduced later in the paper. The best performing system in the 2013 entrance exam task was presented by Banerjee et al. <ref type="bibr" coords="2,172.51,596.66,10.58,8.64" target="#b2">[3]</ref>. Their answer determination module comprises of named entity recognition, syntactic similarity measures as well as textual entailment. They select the correct answer using a maximum weighted score algorithm, but more importantly, they have successfully developed a technique for avoiding questions for which the system shows low confidence rather than answering them wrongly. Overall, the system achieved a c@1 score of 0.42 by answering 13 of the 23 attempted questions correctly. The runner up system in the performance ranking was presented by Li et al. <ref type="bibr" coords="3,464.00,119.31,16.60,8.64" target="#b11">[12]</ref> with an overall c@1 score of 0.35. The system heavily relies on coreference resolution in the narrative text. Moreover, it includes a sentence extractor to identify the most relevant sentences to a particular question while the final answer is determined with a textual entailment classifier. However, the authors conclude that their sentence extractor was the main bottleneck and that the textual entailment component used in their system did not perform up to the mark.</p><p>Finally, Arthur et al. <ref type="bibr" coords="3,230.79,203.24,11.62,8.64" target="#b1">[2]</ref> employ the same system <ref type="bibr" coords="3,341.98,203.24,11.62,8.64" target="#b0">[1]</ref> they proposed for the QA4MRE 2013 main task <ref type="bibr" coords="3,199.49,215.20,15.27,8.64" target="#b17">[18]</ref>. It makes no use of textual entailment and fails to clear the baseline (c@1 = 0.25) with a c@1 score of 0.22. The authors conclude that the reliance on statistical methods alone, and the absence of logical inference hinders the answer determination ability of their system for this task.</p><p>As discussed by Peñas et al. <ref type="bibr" coords="3,266.61,263.26,15.27,8.64" target="#b14">[15]</ref>, using purely statistical analyses of words to determine the correct answer to a reading comprehension questions does not work well for this task. Using textual entailment, on the other hand, provides a means to carry out necessary inferences to determine the correct answer. Banerjee et al. <ref type="bibr" coords="3,414.53,299.13,10.58,8.64" target="#b2">[3]</ref>, who created the best performing system in the pilot task, also highlight the use of text similarity measures in addition to textual entailment. With these conclusions at hand, we present our system based on textual entailment, text similarity and coreference resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">System Architecture</head><p>We now present the architecture of our question answering system which is based on text similarity measures, textual entailment and coreference resolution. The general idea of the system is to retrieve sentences from the text that are relevant for a given question and to identify the correct answer option among the multiple choices based on textual entailment and similarity between each option and the retrieved sentences.</p><p>Our system builds upon the Darmstadt Knowledge Processing Software Repository (DKPro) <ref type="bibr" coords="3,171.63,465.19,15.27,8.64" target="#b9">[10]</ref>, which is based on the Apache UIMA Framework <ref type="bibr" coords="3,387.19,465.19,10.58,8.64" target="#b5">[6]</ref>. In particular, it uses NLP components from DKPro Core<ref type="foot" coords="3,280.16,475.48,3.49,6.05" target="#foot_1">2</ref> and textual similarity measures from the DKPro Similarity<ref type="foot" coords="3,174.62,487.43,3.49,6.05" target="#foot_2">3</ref>  <ref type="bibr" coords="3,181.10,489.10,11.62,8.64" target="#b3">[4]</ref> framework.</p><p>The system is divided into six main modules: corpus reader, preprocessing, sentence retrieval, answer similarity analysis, entailment analysis, and answer selection. Figure <ref type="figure" coords="3,475.62,513.26,4.98,8.64">1</ref> gives a schematic overview of the system architecture. The individual components are described in the remainder of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Corpus Reader</head><p>The corpus reader parses the provided XML document with the entrance exam data and initializes a separate UIMA Common Analysis Structure (CAS) for each test item. The text of each CAS consists of the narrative text appended with the question and the corresponding answer options. Both the question and the answer options are then marked-up with UIMA annotations which identify the exact span of text and hold additional meta information contained in the XML source document. The final CAS is then passed on to the next module in the pipeline for further processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Preprocessing</head><p>The preprocessing module consists of several NLP components that annotate the text before the similarity and entailment analysis is carried out. All of these components are based on the Stanford CoreNLP<ref type="foot" coords="4,277.50,403.09,3.49,6.05" target="#foot_3">4</ref> package and employed in DKPro Core wrappers. We use the Stanford segmenter and lemmatizer for identifying tokens, sentences and lemmas in the text. The Stanford parser <ref type="bibr" coords="4,299.72,428.67,16.60,8.64" target="#b10">[11]</ref> is furthermore used to annotate parts of speech (PoS) and the constituent structure. Moreover, we observed that most of the narrative texts feature recurring characters (as also noted by Li et al. <ref type="bibr" coords="4,412.42,452.58,14.94,8.64" target="#b11">[12]</ref>). Hence, the Stanford Named Entity Recognizer <ref type="bibr" coords="4,281.12,464.54,11.62,8.64" target="#b6">[7]</ref> is used to identify entities of the Person type in the text. Finally, the Stanford coreference resolver <ref type="bibr" coords="4,356.87,476.49,16.60,8.64" target="#b16">[17]</ref> links the different named entities with other occurrences in the text. Based on the resulting coreference chains, an answer option is then modified, or resolved, such that pronouns in the answer option are replaced with the corresponding proper nouns obtained from the question text. For example:</p><p>Question: What did John want? Answer Option: He wanted a bike. Resolved Answer: John wanted a bike.</p><p>To further improve the sentence retrieval, we also resolve all pronoun entities in the main text. This resolution facilitates the identification of relevant sentences in the text for each question and answer option, since it reduces the lexical variation introduced by referring expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Sentence Retrieval</head><p>We found that the downstream textual entailment module is sensitive to irrelevant information. That is, the longer the text from which a given hypothesis is to be inferred, the more likely the system is to report a falsely positive inference. It is therefore necessary to first identify sentences in the text that are most relevant to a particular question before proceeding with further analyses of the answer options. We use three different text similarity measures to identify sentences from the document that are most relevant a particular question:</p><p>1. Lexical similarity: The Word n-gram Jaccard measure from the DKPro Similarity package implements a generalization of the Ferret measure <ref type="bibr" coords="5,388.36,255.69,16.60,8.64" target="#b12">[13]</ref> to support similarity calculations for token n-grams of arbitrary length using the Jaccard coefficient. We employ the Jaccard measure based on token unigrams. 2. ESA-based similarity: Text similarity based on explicit semantic analysis <ref type="bibr" coords="5,445.45,292.99,11.62,8.64" target="#b7">[8]</ref> using a Wikipedia based index. For the similarity calculation, cosine is used as an inner product along with L2 vector normalization in the cector comparator provided by DKPro Similarity. 3. PoS similarity: A part of speech based measure for calculating the overlap between two bags of PoS. We only consider nouns, verbs and adjectives for the similarity calculations.</p><p>For all similarity measures, the input texts are stopword-filtered using a subset of the English stopword list from the DKPro WSD<ref type="foot" coords="5,316.62,400.64,3.49,6.05" target="#foot_4">5</ref> package. Each similarity measure provides a score between 0 and 1. Using a linear combination of these scores, we select the top k sentences for each question. In our experiments, we empirically found k = 5 to be an optimal value for our setup. We use only these sentences in the following modules to determine the correct answer among all possible options. As a basic processing unit for the following analyses, we define Text-Hypothesis (T-H) pairs. We use each retrieved sentence from the narrative text as T and all answer options along with all possible combinations of the question with each answer option as H. Both for T and H, we use resolved and unresolved version of the text, depending on the system configuration.</p><p>We noticed that, for many questions, the correct answer lies in the sentence before or after the selected sentence. Hence, instead of restricting the set of T entities entirely to the selected sentences, we added an option to include the previous and next sentences as well. These are added as separate T entities to the set. This setting is referred to as S ± later on (see Table <ref type="table" coords="5,226.22,572.53,3.60,8.64" target="#tab_1">1</ref>).</p><p>All downstream analyses are then performed on all possible T-H pairs. Our approach to T-H pair creation is prone to overgeneration, i.e. it will contain irrelevant pairs that can even degrade the system performance. We therefore implemented several selection techniques that only consider particular T-H pairs. An overview of the different configurations will be described in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Textual Entailment</head><p>In order to compute textual entailment on the T-H pairs, we use the EXCITEMENT Open Platform (EOP) <ref type="bibr" coords="6,225.81,148.06,15.27,8.64" target="#b13">[14]</ref>, a UIMA-based framework with implementations of stateof-the-art textual entailment algorithms along with pre-trained models. Each T-H pair is processed using an EOP annotator, which first carries out additional preprocessing on the T-H pair (depending on the model used) and then uses an Entailment Decision Algorithm (EDA) to perform the inference between T and H.</p><p>We use the MaxEntClassificationEDA configuration provided in EOP, which uses a maximum entropy model based on the OpenNLP MaxEnt <ref type="bibr" coords="6,394.07,219.79,11.62,8.64" target="#b4">[5]</ref> package for learning an entailment classifier. Scoring techniques, such as bag of words and bag of lemmas scoring, are used along with an option to employ additional resources including Verb Ocean and WordNet. This enables us to incorporate real-world knowledge to aid the entailment procedure. The classifier was trained on the RTE-3 dataset <ref type="bibr" coords="6,404.11,267.61,10.58,8.64" target="#b8">[9]</ref>. The entailment decisions are binary but provide a confidence score. We store both the decision and the confidence score in the T-H markup of the CAS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Answer Similarity</head><p>While the idea of T-H pairs originally stems from the textual entailment theory, we generalize the idea to similarity analysis as well. As mentioned previously, each sentence (or resolved sentence) is labeled as text (T), while an answer option (or resolved answer option, or a question combined with an answer option) is labeled as the hypothesis. Consequently, similarity measures can be applied to these T-H pairs and corresponding scores can be used to determine the correct answer.</p><p>We explained earlier that we do not only include the answer options (and their resolved variants) in the set of hypotheses, but also a combination of the answer option with the question. The reason for this can be seen in the following example: Question: People are normally regarded as old when Answer Option: they are judged to be old by the society Combined: People are normally regarded as old when they are judged to be old by the society. Relevant Sentence: But in general, people are old when society considers them to be old, that is, when they retire from work at around the age of sixty or sixty-five.</p><p>It is easy to see that the combination of question and answer makes the inference of the correct answer easier as compared to using the answer option alone. We use the same similarity measures that we described earlier in Section 4.3 to measure the similarity between T and H in all T-H pairs. A linear combination of the scores generated by each measure is stored in the markup of each T-H pair and used together with the entailment score for the final answer selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Answer Selection</head><p>The entailment confidence score and the answer similarity scores obtained for each T-H pair are finally used to select the correct answer option. We calculate a correctness score .375 a Sentences are selected based on the top 10 sentences according to their similarity to each answer option rather than the sentence selection procedure described in the text. by using a linear combination of the two scores as mentioned below: correctness score = x(entailment score) + y(similarity score)</p><p>We experimented with different linear combinations of the two scores and found that the best performance is achieved when the entailment score is given a higher weight compared to the similarity score. More specifically, we achieved good results with {x, y} = {2, 1} and {x, y} = {3, 2}. In cases where the entailment confidence exceeded a threshold of .90, we set the weights to {x, y} = {1, 0} thus eliminating the similarity score from the decision process. We finally select the answer option corresponding to the T-H pair with the highest correctness score as the correct answer for a given question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>For the Entrance Exams 2014 task, we submitted a total 7 configurations. In this section, we discuss the individual configurations, the scores achieved with each setup and finally provide a detailed error analysis.</p><p>Each configuration consists of three main components, the sentence selection strategy, the answer similarity analysis and the entailment analysis. The exact setup for each configuration is provided in Table <ref type="table" coords="7,273.66,548.84,3.74,8.64" target="#tab_1">1</ref>. Overall, configuration 7 achieved the best results with a c@1 score of 0.375. Moreover, configurations 3, 5, and 6 performed above or equal to the random baseline of 0.25, while 1, 2, and 4 performed poorly with c@1 scores below the baseline. The reason configuration 1 failed was mainly due to the fact that answer selection was merely performed on the sentences pertaining to the questions, without considering those surrounding the selected sentences. Hence the idea of using the sentence before and after a selected sentence, was not implemented for this configuration.</p><p>Configuration 2, on the other hand, used the previous and next sentences, but failed to clear the baseline. One of the main reasons for this was the fact that sentences chosen as H for the entailment analysis were chosen based on high answer similarity scores rather than the sentence selection procedure based on the question. This resulted in the selection of certain sentences that were related to incorrect answer options, and not directly related to the question. For configurations 4, the linear combination of answer similarity and entailment scores used to determine the final answer was ineffective, and led to incorrect answers. Configurations 3 and 5 did well because the weight assigned to the entailment score was increased, and a lower weight, (0 in case of configuration 3,) was assigned to the answer similarity score. Nonetheless, answer similarity helped determine a few answers where entailment analysis alone had failed, for instance, in configurations 5 and 7.</p><p>Additionally, in all but the last run, we used only the original sentences as T. However, for 7, we introduced the idea of using resolved sentences as T. This improved our scores dramatically, giving a c@1 score of 0.375. As for configuration 6, without the use of resolved answers, it failed to perform as well as 3, 5 or 7.</p><p>This leads us to conclude that coreference resolution in the sentences and answer options is highly beneficial in determining the correct answer. Additionally, using the previous as well as the next sentence in addition to the selected sentence as T is better than using only the selected sentence. The results confirm that many answers can be identified from a sentence close to the selected one using the question text. Finally, when using a linear combination of the answer similarity and entailment scores, a higher weight should be assigned to the entailment score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Error Analysis</head><p>In order to identify systematic errors in the decision making process of our system, we conducted an error analysis on the output of each configuration described in the previous section.</p><p>The implementation of coreference resolution replaces pronouns and phrases describing a person entity with the proper noun. This generates some issues when the phrase describing a person is vital to answer determination. The coreference resolution in the preprocessing module can further be improved by replacing only certain terms, mainly pronouns, in sentences with the corresponding named entities.</p><p>While the sentence extraction works well in most cases, there are two problems that can be encountered. The first problem is that the correct sentence pertaining to a question is not the one with the highest score. Consequently, we select k (=5) sentences rather than just one. Nonetheless, an improvement in identifying the most relevant sentence, and reducing k to 1 or 2 sentences will help to narrow down possible mistakes with answer determination. The second problem is when the question is not sufficient to identify the correct sentence in the text. For example, a question like the following requires sentence selection based on the answer options instead of the question text: Question: The main point the author wishes to make is that While the answer similarity module was able to assist in determining certain answers, it was generally outperformed by the other measures employed in our system, in particular by the entailment module. It particularly underperformed in case of similarly phrased answer options for a given question, which caused false predictions. Consequently, a lower weight was assigned to the similarity score. With an improved linear combination of the various similarity measures integrated in this module, the performance can further be improved.</p><p>While the entailment module based on EOP provides good results, especially in the case of resolved sentences and answer options, it does not perform optimally in certain scenarios. For instance, the module does not perform well when more than one answer options are very similar. This is illustrated by the following example:</p><p>Excerpt: "I'm from Georgia," he said in a loud voice,"and proud of it." "Sorry I made the mistake," I told him, though I just couldn't see what difference it made whether he came from Georgia or Alabama. Question: What mistake did the writer make about the man with glasses? Incorrect Answer: He thought the man came from Georgia Correct Sentence: He thought the man's home state was Alabama Moreover, certain questions require advanced inference based on world knowledge rather than relying on the document surface text alone. In these cases, the correct answers are difficult to determine with the entailment module. For example:</p><p>Excerpt: He was holding onto the arms of his seat so tightly that the blood had left his fingers. He was what is known as a "white-knuckle flier." He would not look out the window, and he was sweating so much that the stewardesses had to keep bringing him towels. Question: What is meant by a "white-knuckle flier?" Answer: It is a person who is extremely nervous on an airplane.</p><p>We also encountered errors when a question and answer pair spanned multiple sentences. Since our system considers at most 3 sentences at a time, i.e. the selected, previous and next sentence, it sometimes fails to obtain the desired inference for the correct answer option. With a more generalized retrieval module that supports larger text windows, we could be able to circumvent this problem but have to take into account the added noise that is introduced by larger spans of text.</p><p>Thus, to summarize, a refined approach to coreference resolution along with an improved answer similarity module will help to improve scores to a great extent. In addition, the entailment module can be trained with different datasets to improve the classifier, while a tighter sentence selection module will help to narrow down possible sentences relevant to a particular question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Future Work</head><p>Automatically answering multiple choice reading comprehension questions is a challenging task. In this paper, we presented a system designed to solve such questions without any domain-specific background knowledge. Our strategy for determining answers to reading comprehension questions is based on three main components: text similarity, textual entailment and coreference resolution.</p><p>As already proposed in related work, textual entailment is the key technology for tackling this task. However, it is sensitive to irrelevant information both in the text and the hypothesis, so a strong filter or retrieval component based on text similarity measures drastically helps to improve the quality of the entailment results. Due to the nature of this task, which focused on reading comprehension questions about narrative texts, coreference resolution furthermore helped to improve the discriminative power of the entailment and similarity scores for the final answer selection.</p><p>With our best configuration, we achieved a c@1 score of 0.375, which puts our system on the second place in the overall performance ranking. Moreover, 3 other configurations performed better than or equal to the random baseline of c@1 = 0.25. In our error analysis, we finally identified the most prominent error types our system encountered.</p><p>As part of future work, our priorities are to refine the coreference resolution module, and to work on improving the performance of the answer similarity module. In addition, the entailment module can be trained on datasets more suited to the task, in order to improve the classifier, while a tighter sentence selection module will help narrow down possible sentences relevant to a question.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,134.77,115.83,345.82,151.65"><head>Table 1 .</head><label>1</label><figDesc>Overview of individual system configurations and their performance. A=Answer, Q=Question, S=Selected sentence in text, S ± =Selected sentence, previous sentence and next sentence. The prefix re indicates the resolved version of an item, e.g. reA indicates a resolved Answer. + indicates concatenation, comma separated series indicate alternatives from which the highest score is chosen.</figDesc><table coords="7,136.16,183.10,343.04,84.37"><row><cell>#</cell><cell>Sentence Selection</cell><cell></cell><cell cols="2">Answer Similarity</cell><cell></cell><cell>Entailment</cell><cell cols="2">Performance</cell></row><row><cell></cell><cell>Sentence Measures</cell><cell cols="3">Text Hyp. Measures</cell><cell cols="2">Text Hyp.</cell><cell cols="2">Corr. Incorr. c@1</cell></row><row><cell>1</cell><cell>Original Jaccard</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>S</cell><cell>A, reA</cell><cell>11 45</cell><cell>.196</cell></row><row><cell>2</cell><cell>Original Jaccard, ESA</cell><cell cols="3">S ± A, reA Jaccard, ESA</cell><cell cols="2">S ±a A, reA, Q+A</cell><cell>11 45</cell><cell>.196</cell></row><row><cell>3</cell><cell>Original Jaccard, ESA</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">S ± A, reA, Q+A</cell><cell>16 40</cell><cell>.286</cell></row><row><cell>4</cell><cell>Original PoS, ESA</cell><cell cols="3">S ± A, reA ESA</cell><cell cols="2">S ± A, reA, Q+A</cell><cell>13 43</cell><cell>.232</cell></row><row><cell>5</cell><cell>Original Jaccard</cell><cell cols="3">S ± A, reA Jaccard, ESA</cell><cell cols="2">S ± A, reA, Q+A</cell><cell>16 40</cell><cell>.286</cell></row><row><cell>6</cell><cell>Original ESA</cell><cell cols="2">S ± A</cell><cell>Jaccard, ESA</cell><cell cols="2">S ± A, Q+A</cell><cell>14 42</cell><cell>.250</cell></row><row><cell>7</cell><cell>Resolved Jaccard, ESA</cell><cell cols="2">reS ± reA</cell><cell>Jaccard, ESA</cell><cell cols="2">reS ± reA, Q+A</cell><cell>21 35</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,657.09,193.26,7.77"><p>Question Answering for Machine Reading Evaluation</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,144.73,646.78,116.01,6.45"><p>http://dkpro-core-asl.googlecode.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,144.73,657.96,129.67,6.45"><p>http://dkpro-similarity-asl.googlecode.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,144.73,657.96,138.36,6.45"><p>http://nlp.stanford.edu/software/corenlp.shtml</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="5,144.73,657.96,103.12,6.45"><p>http://dkpro-wsd.googlecode.com</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work has been supported by the <rs type="funder">Volkswagen Foundation</rs> as part of the <rs type="programName">Lichtenberg-Professorship Program</rs> under grant No. <rs type="grantNumber">I/82806</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_QJ7qwhC">
					<idno type="grant-number">I/82806</idno>
					<orgName type="program" subtype="full">Lichtenberg-Professorship Program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,142.61,428.73,313.21,7.77;10,150.95,439.69,292.98,7.77;10,150.95,450.65,313.60,7.77;10,150.95,461.61,44.08,7.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,355.91,428.73,99.91,7.77;10,150.95,439.69,275.19,7.77">Inter-Sentence Features and Thresholded Minimum Error Rate Training: NAIST at CLEF 2013 QA4MRE</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,150.95,450.65,210.40,7.77">Proceedings of CLEF 2013 Evaluation Labs and Workshop</title>
		<title level="s" coord="10,367.90,450.65,79.34,7.77">Online Working Notes</title>
		<meeting>CLEF 2013 Evaluation Labs and Workshop</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,472.27,305.63,7.77;10,150.95,483.23,306.40,7.77;10,150.95,494.19,138.48,7.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,355.91,472.27,92.33,7.77;10,150.95,483.23,73.45,7.77">NAIST at the CLEF 2013 QA4MRE Pilot Task</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,242.63,483.23,210.41,7.77">Proceedings of CLEF 2013 Evaluation Labs and Workshop</title>
		<title level="s" coord="10,150.95,494.19,79.34,7.77">Online Working Notes</title>
		<meeting>CLEF 2013 Evaluation Labs and Workshop</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,504.85,305.58,7.77;10,150.95,515.81,311.33,7.77;10,150.95,526.77,107.10,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,243.47,504.85,204.71,7.77;10,150.95,515.81,78.17,7.77">Multiple Choice Question (MCQ) Answering System for Entrance Examination</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bhaskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,247.56,515.81,210.41,7.77">Proceedings of CLEF 2013 Evaluation Labs and Workshop</title>
		<title level="s" coord="10,150.95,526.77,80.95,7.77">Online Working Notes</title>
		<meeting>CLEF 2013 Evaluation Labs and Workshop</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,537.43,329.74,7.77;10,150.95,548.39,276.83,7.77;10,150.95,559.35,261.01,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,268.23,537.43,204.12,7.77;10,150.95,548.39,34.12,7.77">DKPro Similarity : An Open Source Framework for Text Similarity</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bär</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zesch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,202.92,548.39,224.87,7.77;10,150.95,559.35,183.86,7.77">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="121" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,570.01,310.37,7.77;10,150.95,580.97,247.53,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,306.90,570.01,146.08,7.77;10,150.95,580.97,71.51,7.77">A maximum entropy approach to natural language processing</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">L</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">J D</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A D</forename><surname>Pietra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,228.38,580.97,95.40,7.77">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="71" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,591.63,312.28,7.77;10,150.95,602.59,322.57,7.77;10,150.95,613.55,57.53,7.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,234.78,591.63,220.12,7.77;10,150.95,602.59,174.48,7.77">UIMA: an architectural approach to unstructured information processing in the corporate research environment</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ferrucci</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,331.60,602.59,110.55,7.77">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="327" to="348" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,624.21,298.59,7.77;10,150.95,635.17,311.65,7.77;10,150.95,646.13,328.77,7.77;10,150.95,657.09,23.90,7.77" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,295.52,624.21,145.68,7.77;10,150.95,635.17,179.25,7.77">Incorporating non-local information into information extraction systems by Gibbs sampling</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,348.56,635.17,114.05,7.77;10,150.95,646.13,194.73,7.77">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics<address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,119.96,325.00,7.77;11,150.95,130.92,327.59,7.77;11,150.95,141.88,240.78,7.77" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,268.38,119.96,199.23,7.77;11,150.95,130.92,90.78,7.77">Computing semantic relatedness using wikipedia-based explicit semantic analysis</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,259.64,130.92,218.91,7.77;11,150.95,141.88,88.45,7.77">Proceedings of The Twentieth International Joint Conference for Artificial Intelligence</title>
		<meeting>The Twentieth International Joint Conference for Artificial Intelligence<address><addrLine>Hyderabad, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1606" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,152.84,325.67,7.77;11,150.95,163.80,293.18,7.77;11,150.95,174.76,196.05,7.77" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,339.42,152.84,128.86,7.77;11,150.95,163.80,73.03,7.77">The third pascal recognizing textual entailment challenge</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,242.08,163.80,202.05,7.77;11,150.95,174.76,101.94,7.77">Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing</title>
		<meeting>the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,185.72,325.03,7.77;11,150.95,196.67,324.52,7.77;11,150.95,207.63,326.93,7.77;11,150.95,218.59,307.45,7.77" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,429.92,185.72,37.35,7.77;11,150.95,196.67,170.54,7.77">Darmstadt knowledge processing repository based on uima</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mühlhäuser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Steimle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Weimer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zesch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,339.98,196.67,135.50,7.77;11,150.95,207.63,326.93,7.77;11,150.95,218.59,202.99,7.77">Proceedings of the First Workshop on Unstructured Information Management Architecture at Biannual Conference of the Society for Computational Linguistics and Language Technology</title>
		<meeting>the First Workshop on Unstructured Information Management Architecture at Biannual Conference of the Society for Computational Linguistics and Language Technology<address><addrLine>Tübingen, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,229.55,316.36,7.77;11,150.95,240.51,300.92,7.77;11,150.95,251.47,105.11,7.77" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,247.08,229.55,109.57,7.77">Accurate unlexicalized parsing</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,374.67,229.55,83.92,7.77;11,150.95,240.51,223.37,7.77">Proceedings of the 41st Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting on Association for Computational Linguistics<address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,262.43,337.76,7.77;11,150.95,273.39,310.12,7.77;11,150.95,284.35,23.90,7.77" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,253.38,262.43,221.03,7.77">Question Answering System for Entrance Exams in QA4MRE</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,163.16,273.39,210.40,7.77">Proceedings of CLEF 2013 Evaluation Labs and Workshop</title>
		<title level="s" coord="11,380.11,273.39,80.96,7.77">Online Working Notes</title>
		<meeting>CLEF 2013 Evaluation Labs and Workshop</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,295.30,338.34,7.77;11,150.95,306.26,303.17,7.77;11,150.95,317.22,219.77,7.77" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,275.98,295.30,204.60,7.77;11,150.95,306.26,303.17,7.77;11,150.95,317.22,27.22,7.77">A theoretical basis to the automated detection of copying between texts, and its practical implementation in the ferret plagiarism and collusion detector</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lyon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Malcolm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,183.82,317.22,160.76,7.77">Plagiarism: Prevention, Practice and Policies</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,328.18,326.95,7.77;11,150.95,339.14,296.42,7.77" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,340.45,328.18,128.73,7.77;11,150.95,339.14,120.21,7.77">Design and realization of a modular architecture for textual entailment</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">G</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zanoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,277.05,339.14,110.55,7.77">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,350.10,321.51,7.77;11,150.95,361.06,329.29,7.77;11,150.95,372.02,23.90,7.77" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,265.63,350.10,182.14,7.77">Overview of QA4MRE 2013 Entrance Exams Task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,150.95,361.06,210.40,7.77">Proceedings of CLEF 2013 Evaluation Labs and Workshop</title>
		<title level="s" coord="11,367.90,361.06,79.34,7.77">Online Working Notes</title>
		<meeting>CLEF 2013 Evaluation Labs and Workshop</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,382.98,326.21,7.77;11,150.95,393.93,323.24,7.77;11,150.95,404.89,132.10,7.77" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="11,236.88,382.98,146.55,7.77">A simple measure to assess non-response</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rodrigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,401.72,382.98,66.73,7.77;11,150.95,393.93,323.24,7.77;11,150.95,404.89,46.08,7.77">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1415" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,415.85,313.98,7.77;11,150.95,426.81,314.63,7.77;11,150.95,437.77,316.24,7.77" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="11,200.76,426.81,160.01,7.77">A multi-pass sieve for coreference resolution</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rangarajan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,378.68,426.81,86.91,7.77;11,150.95,437.77,239.29,7.77">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="492" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.24,448.73,326.71,7.77;11,150.95,459.69,309.38,7.77;11,150.95,470.65,47.06,7.77" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="11,312.15,448.73,156.80,7.77;11,150.95,459.69,16.14,7.77">Overview of QA4MRE Main Task at CLEF 2013</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sutcliffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Forner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,185.57,459.69,210.40,7.77">Proceedings of CLEF 2013 Evaluation Labs and Workshop</title>
		<title level="s" coord="11,402.52,459.69,57.81,7.77;11,150.95,470.65,20.92,7.77">Online Working Notes</title>
		<meeting>CLEF 2013 Evaluation Labs and Workshop</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
