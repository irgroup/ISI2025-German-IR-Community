<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,338.88,116.95,141.68,12.62;1,180.39,134.89,254.55,12.62">at ImageCLEF 2014 Scalable Concept Image Annotation</title>
				<funder>
					<orgName type="full">Fundamental Research Funds for the Central Universities</orgName>
				</funder>
				<funder>
					<orgName type="full">Scientific Research Foundation for the Returned Overseas Chinese Scholars, State Education Ministry</orgName>
				</funder>
				<funder ref="#_Gs5NVsx">
					<orgName type="full">Research Funds of Renmin University of China</orgName>
				</funder>
				<funder ref="#_mzQERGF">
					<orgName type="full">Specialized Research Fund for the Doctoral Program of Higher Education</orgName>
				</funder>
				<funder ref="#_tWKjkmw">
					<orgName type="full">National Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,193.27,172.56,40.80,8.74"><forename type="first">Xirong</forename><surname>Li</surname></persName>
							<email>xirong@ruc.edu.cn</email>
						</author>
						<author>
							<persName coords="1,241.32,172.56,31.36,8.74"><forename type="first">Xixi</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName coords="1,280.90,172.56,46.58,8.74"><forename type="first">Gang</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName coords="1,335.80,172.56,31.51,8.74"><forename type="first">Qin</forename><surname>Jin</surname></persName>
						</author>
						<author>
							<persName coords="1,374.68,172.56,47.46,8.74"><forename type="first">Jieping</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Key Lab of DEKE</orgName>
								<orgName type="laboratory">Multimedia Computing Lab</orgName>
								<orgName type="institution" key="instit1">Renmin University of China</orgName>
								<orgName type="institution" key="instit2">Renmin University of China No</orgName>
								<address>
									<addrLine>59 Zhongguancun Street</addrLine>
									<postCode>100872</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,338.88,116.95,141.68,12.62;1,180.39,134.89,254.55,12.62">at ImageCLEF 2014 Scalable Concept Image Annotation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BAC25D5CC5D4D01D262242476DF0AF70</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Scalable image annotation</term>
					<term>SVM</term>
					<term>adaptive tag selection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we describe our image annotation system participated in the ImageCLEF 2014 scalable concept image annotation task. The system is fully SVM based. Per concept we learn an ensemble of fast intersection kernel SVMs from three sources of training data, all obtained with manual annotation for free. The focus of our experiments this year is to answer the question of how many tags we should use to annotate a novel image. To that end, we introduce adaptive tag selection. In contrast to the common top-k strategy which selects a fixed number of top ranked tags to annotate an unlabeled image, our method estimates the value of k with respect to the image. Given the same concept rankings, the top-5 strategy obtains MF-sample of 0.206, while adaptive tag selection reaches MF-sample of 0.311.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>For automated image annotation, how to adaptively determine a proper number of tags for a specific image is an open problem. The top-k strategy, which simply selects the top k ranked tags per image, is probably the most popular solution. A number of systems in ImageCLEF 2013 <ref type="bibr" coords="1,321.32,483.12,10.50,8.74" target="#b0">[1]</ref> have used this strategy, including our 2013 system <ref type="bibr" coords="1,208.18,495.07,9.95,8.74" target="#b1">[2]</ref>. The fact that the number of relevant tags vary over images makes the top-k strategy suboptimal. Hence, our focus this year is on studying and evaluating strategies for adaptive tag selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The RUC 2014 System</head><p>The main components of our 2014 system, namely visual features, training data, and image annotation models, are described as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Visual features</head><p>For each image, we extract bag of visual words (BoW) using the color descriptor software <ref type="bibr" coords="1,174.07,633.20,9.95,8.74" target="#b2">[3]</ref>. A precomputed codebook of size 4,000 is used to quantize densely sampled SIFT descriptors. We further consider 1x1+1x3 spatial pyramids, resulting in a BoW feature of 16,000 dimensions per image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Training data</head><p>In addition to the 250K web images from ImageCLEF 2013, we leverage two additional two sets, both of which are acquired with manual annotation for free. The first set consists of one million images with user-clicked count, released by MSR Bing <ref type="bibr" coords="2,202.51,175.89,9.95,8.74" target="#b3">[4]</ref>. The other set consists of four million user-tagged images from Flickr. Notice that the data collecting process of the two extra sets were independent of the ImageCLEF dev/test concept lists. So the use of the extra data does not affect the scalability of our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Image annotation models</head><p>Different from our 2013 system <ref type="bibr" coords="2,274.94,261.78,10.50,8.74" target="#b1">[2]</ref> which combines kNN and SVM models, the 2014 edition is fully SVM based. For each dev/test tag ω, we learn an ensemble of two-class SVM classifiers from the three training sets separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Positive example selection.</head><p>As the training sets come from different sources with different (noisy) annotation information, we describe how to select positive training examples for ω from the individual sets.</p><p>For the 250K web images, as they were collected from three web image search engines, namely Google, Yahoo, and Bing, each image x can be described by a triplet &lt; q, r, s &gt;, where q represent a query tag, r is the rank of x in the search results of q returned by an specific search engine s. Because a given image might be retrieved by different queries or by the same query but with different search engines, it can be associated with multiple triplets, denoted as &lt; q i , r i , s i &gt;, i = 1, . . . , l, where l is the number of triplets. To estimate the relevance of x with respect to ω, we propose to compute a search engine based score as</p><formula xml:id="formula_0" coords="2,217.45,442.96,263.13,29.12">relevance search (x, ω) = l i=1 δ(q i , ω) w(s i ) √ r i ,<label>(1)</label></formula><p>where δ(q i , ω) returns 1 if q i and ω are the same, and 0 otherwise. The variable w(s i ) indicates the weight of a specific search engine, which we empirically set to be 1, 0.5, and 0.5 for Google, Yahoo, and Bing, respectively. For the user-clicked set, each image is associated with a textual query and the accumulated count of user click. A larger click count indicates that the image is more likely to be relevant to the query <ref type="bibr" coords="2,314.23,544.47,9.95,8.74" target="#b3">[4]</ref>. We thus match ω with queries and use the corresponding click count as the relevance score.</p><p>For the Flickr set, we use a semantic-based relevance measurement as depicted in <ref type="bibr" coords="2,176.76,580.36,9.95,8.74" target="#b4">[5]</ref>, which computes tagwise similarity between ω and user tags of an image.</p><p>Given the concept ω, we sort images in descending order by their relevance scores w.r.t ω, and preserve the top 1,000 ranked images as positive training examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SVM training.</head><p>As the training data is overwhelmed by negative examples, we learn SVM classifiers by the Negative Bootstrap algorithm <ref type="bibr" coords="2,403.40,657.12,9.95,8.74" target="#b5">[6]</ref>. Different from sampling negative examples at random, Negative Bootstrap iteratively selects negative examples which are most misclassified by present classifiers, and thus most relevant to improve classification. Per iteration, the algorithm randomly samples 10x1,000=10,000 examples to form a candidate set. An ensemble of classifiers obtained in the previous iterations are used to classify each candidate example. The top 1,000 most misclassified examples are selected and used together with the 1,000 positives to train a new classifier. For the consideration of efficiency, we use fast intersection kernel SVMs (fikSVM). For each of the three sets, we conduct Negative Bootstrap with 10 iterations, producing in total 3x10 fikSVMs per concept. These fikSVMs are further compressed into a single model such that the prediction time complexity is independent of the ensemble size.</p><p>As we focus on adaptive tag selection, we do not submit runs to compare the effectiveness of the three training sets. Nevertheless, our preliminary observation from the development set is that models trained on the three sets are complementary to each other to some extent. We therefore combine the models in a linear late fusion manner. As shown in previous studies <ref type="bibr" coords="3,421.71,299.59,10.50,8.74" target="#b1">[2,</ref><ref type="bibr" coords="3,433.92,299.59,7.00,8.74" target="#b6">7]</ref>, weights optimized by coordinate ascent are consistently better than averaging. So we continue this good practice, and learn the fusion weights by coordinate ascent on the development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adaptive tag selection.</head><p>For the 107 dev concepts, we have access to a ground truth set of 1,000 images provided by ImageCLEF 2014 <ref type="bibr" coords="3,410.08,364.63,9.89,8.74" target="#b7">[8]</ref>. We find that finding a cutoff threshold that maximizes the F-concept measure per concept is a good strategy. However, this strategy is inapplicable to novel concepts that have no ground truth data available. In the 2014 task, there are 100 novel concepts, as listed in Table <ref type="table" coords="3,214.25,412.45,3.87,8.74" target="#tab_0">1</ref>. We devise a method that selects the top k ranked tags to annotate an unlabeled image, whilst the value of k is adaptively determined with respect to the test image. The method is based on the following hypothesis. We assume that the dev concept vocabulary and the test concept vocabulary are independent, such that for a given image the ratio of its relevant tags covered in the dev vocabulary equals to the ratio of the relevant tags covered in the test vocabulary. In that regard, we can estimate the number of test concepts according to the number of dev concepts that have been selected. We will detail the method somewhere else <ref type="bibr" coords="3,257.51,508.09,9.95,8.74" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Submitted Runs</head><p>This year we submitted eight runs:</p><p>-RUC 01: Adaptive tag selection, with the scores of test concepts updated with respect to the scores of the selected dev concepts and their cutoff threshold (Adaptive-I). -RUC 02: Adaptive tag selection, with the scores of test concepts updated with respect to the scores of the selected dev concepts (Adaptive-II). The 100 novel concepts: antelope apple arthropod asparagus avocado banana bear berry blood branch bread broccoli buffalo butterfly camel canidae captive carrot cauliflower cervidae cheese cheetah chimpanzee corn crocodile cucumber donkey egg eggplant elephant equidae felidae flamingo fox fried fruit galaxy giraffe gorilla grape hippopotamus human hunting kangaroo knife koala leaf leopard lettuce lion mammal marsupial meat monkey mud mushroom nebula onion orange ostrich pan pasta pear penguin pig pineapple pinniped pool potato pumpkin rabbit raccoon reptile rhino rice rifle roasted rock rodent sausage soup spider spoon squirrel strawberry submarine tiger tomato trunk tuber turtle vegetable walrus warthog watermelon wild wolf yam zebra zoo -RUC 03: Adaptive tag selection, with the scores of test concepts updated with respect to the rank-normalized scores of the selected dev concepts (Adaptive-III).</p><p>-RUC 04: Selecting the top 5 ranked tags as final annotations.</p><p>-RUC 05: The same as RUC 01 except that the output of SVM classifiers learned from the individual data sources have been converted to probabilistic output before fusion.</p><p>-RUC 06: The same as RUC 02 except that the output of SVM classifiers learned from the individual data sources have been converted to probabilistic output before fusion.</p><p>-RUC 07: The same as RUC 03 except that the output of SVM classifiers learned from the individual data sources have been converted to probabilistic output before fusion.</p><p>-RUC 08: The same as RUC 04 except that the output of SVM classifiers learned from the individual data sources have been converted to probabilistic output before fusion. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>The performance scores of the eight runs are summarized in Table <ref type="table" coords="5,424.91,323.29,3.87,8.74" target="#tab_1">2</ref>. Compared to the top-5 strategy (RUC 04 and RUC 08), all the other runs are better. The results clearly show that adaptive tag selection outperforms the top-k strategy.</p><p>As the only difference between RUC 01, RUC 02, RUC 03, and RUC 04 is on how to determine the value of k, which does not change concept ranking, the MAP-samples scores are the same for the four runs. Similarly, RUC 05, RUC 06, RUC 07, and RUC 08 have the same MAP-samples scores. Comparing the two groups of runs, we find that score normalization before fusion is helpful for improving MF-samples and MF-concepts, but causes performance drop in MAP-samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>This paper documents our experiments in the ImageCLEF 2014 Scalable Concept Image Annotation, a testbed for developing a scalable image annotation system with manual annotation for free. A novel component of our 2014 system is adaptive tag selection that determines the number of tags to be selected with respect to a given test image. Adaptive tag selection is better than selecting a fixed number of tags for image annotation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,134.77,116.91,348.90,152.45"><head>Table 1 . The test concept vocabulary defined in the 2014 task.</head><label>1</label><figDesc>It consists of 107 dev concepts for which we have access to a ground truth set of 1,000 images, and 100 novel concepts which have no ground truth available. aerial airplane baby beach bicycle bird boat book bottle bridge building bus car cartoon castle cat chair child church cityscape closeup cloud cloudless coast countryside daytime desert diagram dog drink drum elder embroidery female fire firework fish flower fog food footwear forest furniture garden grass guitar harbor hat helicopter highway horse indoor instrument lake lightning logo male monument moon motorcycle mountain newspaper nighttime outdoor overcast painting park person phone plant portrait poster protest rain rainbow reflection river road sand sculpture sea shadow sign silhouette sky smoke snow soil space spectacle sport sun sunset table teenager toy traffic train tree tricycle truck underwater unpaved vehicle violin wagon water</figDesc><table coords="4,137.84,162.78,103.28,7.89"><row><cell>The 107 dev concepts:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,134.77,116.91,345.82,157.84"><head>Table 2 . Performance of our eight submitted runs.</head><label>2</label><figDesc>Adaptive tag selection clearly outperform the top-k strategy (k = 5).</figDesc><table coords="5,162.65,153.86,290.08,120.89"><row><cell>RunId</cell><cell>Strategy</cell><cell cols="3">MF-samples MF-concepts MAP-samples</cell></row><row><cell>RUC 01</cell><cell>Adaptive-I</cell><cell>28.0</cell><cell>24.1</cell><cell>30.2</cell></row><row><cell>RUC 02</cell><cell>Adaptive-II</cell><cell>27.8</cell><cell>24.1</cell><cell>30.2</cell></row><row><cell>RUC 03</cell><cell>Adaptive-II</cell><cell>27.9</cell><cell>24.0</cell><cell>30.2</cell></row><row><cell>RUC 04</cell><cell>Top-k</cell><cell>21.9</cell><cell>21.9</cell><cell>30.2</cell></row><row><cell>RUC 05</cell><cell>Adaptive-I</cell><cell>31.1</cell><cell>25.0</cell><cell>27.5</cell></row><row><cell>RUC 06</cell><cell>Adaptive-II</cell><cell>29.0</cell><cell>25.2</cell><cell>27.5</cell></row><row><cell>RUC 07</cell><cell>Adaptive-III</cell><cell>29.3</cell><cell>25.3</cell><cell>27.5</cell></row><row><cell>RUC 08</cell><cell>Top-k</cell><cell>20.6</cell><cell>21.5</cell><cell>27.5</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This research was supported by the <rs type="funder">National Science Foundation of China</rs> (No. <rs type="grantNumber">61303184</rs>), the <rs type="funder">Fundamental Research Funds for the Central Universities</rs> and the <rs type="funder">Research Funds of Renmin University of China</rs> (No. <rs type="grantNumber">14XNLQ01</rs>), the <rs type="funder">Specialized Research Fund for the Doctoral Program of Higher Education</rs> (No. <rs type="grantNumber">20130004120006</rs>), and the <rs type="funder">Scientific Research Foundation for the Returned Overseas Chinese Scholars, State Education Ministry</rs>. The authors are grateful to the ImageCLEF coordinators for the benchmark organization efforts [8,10].</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_tWKjkmw">
					<idno type="grant-number">61303184</idno>
				</org>
				<org type="funding" xml:id="_Gs5NVsx">
					<idno type="grant-number">14XNLQ01</idno>
				</org>
				<org type="funding" xml:id="_mzQERGF">
					<idno type="grant-number">20130004120006</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,142.96,143.58,337.64,7.86;6,151.52,154.54,294.67,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,316.79,143.58,163.81,7.86;6,151.52,154.54,135.98,7.86">Overview of the imageclef 2013 scalable concept image annotation subtask</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,309.81,154.54,103.77,7.86">CLEF 2013 working notes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,165.51,337.64,7.86;6,151.52,176.46,329.06,7.86;6,151.52,187.42,87.10,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,405.70,165.51,74.89,7.86;6,151.52,176.46,253.28,7.86">Renmin university of china at imageclef 2013 scalable concept image annotation</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,431.56,176.46,49.02,7.86;6,151.52,187.42,54.50,7.86">ImageCLEF working notes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,198.38,337.61,7.86;6,151.52,209.31,325.62,7.89" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,312.17,198.38,168.40,7.86;6,151.52,209.34,67.32,7.86">Evaluating color descriptors for object and scene recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,226.87,209.34,163.39,7.86">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1582" to="1596" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,220.30,337.63,7.86;6,151.52,231.26,329.07,7.86;6,151.52,242.21,87.01,7.86" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="6,444.75,220.30,35.84,7.86;6,151.52,231.26,325.10,7.86">Clickage: Towards bridging semantic and intent gaps via mining click logs of search engines</title>
		<author>
			<persName coords=""><forename type="first">X</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>ACM MM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,253.18,337.62,7.86;6,151.52,264.11,324.75,7.89" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,350.65,253.18,129.93,7.86;6,151.52,264.14,57.35,7.86">Harvesting social images for biconcept search</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,216.89,264.14,138.45,7.86">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1091" to="1104" />
			<date type="published" when="2012-08">Aug. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,275.09,337.62,7.86;6,151.52,286.03,329.05,7.89;6,151.52,297.01,81.52,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,396.47,275.09,84.10,7.86;6,151.52,286.05,152.70,7.86">Bootstrapping visual categorization with relevant negatives</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Koelma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,314.16,286.05,140.31,7.86">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="933" to="945" />
			<date type="published" when="2013-06">Jun. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,307.97,337.64,7.86;6,151.52,318.93,175.36,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,345.31,307.97,135.29,7.86;6,151.52,318.93,96.84,7.86">Fusing concept detection and geo context for visual search</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,270.44,318.93,22.22,7.86">ICMR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,329.88,337.63,7.86;6,151.52,340.84,329.06,7.86;6,151.52,351.81,91.19,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,264.47,329.88,216.12,7.86;6,151.52,340.84,94.30,7.86">Overview of the ImageCLEF 2014 Scalable Concept Image Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,269.85,340.84,175.89,7.86">CLEF 2014 Evaluation Labs and Workshop</title>
		<title level="s" coord="6,453.97,340.84,26.61,7.86;6,151.52,351.81,58.30,7.86">Online Working Notes</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,362.76,337.63,7.86;6,151.52,373.72,121.87,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="6,323.65,362.76,156.93,7.86;6,151.52,373.72,41.88,7.86">Adaptive top-k tag selection for image annotation</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>submitted</note>
</biblStruct>

<biblStruct coords="6,142.61,384.68,337.96,7.86;6,151.52,393.38,329.06,10.12;6,151.52,406.60,329.07,7.86;6,151.52,417.56,285.61,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,151.52,406.60,226.31,7.86">ImageCLEF 2014: Overview and analysis of the results</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Martinez-Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Patricia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Marvasti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Üsküdarlı</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cazorla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Garcia-Varea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Morell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,402.59,406.60,73.88,7.86">CLEF proceedings</title>
		<title level="s" coord="6,151.52,417.56,141.41,7.86">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
