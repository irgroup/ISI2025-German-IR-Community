<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,156.22,115.96,302.91,12.62;1,192.64,133.89,230.06,12.62">Overview of the ImageCLEF 2014 Scalable Concept Image Annotation Task</title>
				<funder ref="#_4KGMn56">
					<orgName type="full">European Union</orgName>
				</funder>
				<funder>
					<orgName type="full">CLEF initiative</orgName>
				</funder>
				<funder ref="#_kdsT8bj">
					<orgName type="full">Spanish MEC</orgName>
				</funder>
				<funder ref="#_pE5xn9v #_32cScEh #_vjvyp2r #_wWYNmEX #_znqqvUT #_ugNUVzK #_QTyTpKg #_NnM9FJX #_7H6RS4g #_v93yZsB #_kMfMW8e #_FRVUNgv #_VCcy2w3 #_g669Zqh #_pSe9sZy #_t4wCQZK #_jKFeK7P #_pVVhgen #_d8cjUXm">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,222.21,171.56,75.91,8.74"><forename type="first">Mauricio</forename><surname>Villegas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universitat Politècnica de València</orgName>
								<address>
									<addrLine>Camí de Vera s/n</addrLine>
									<postCode>46022</postCode>
									<settlement>València</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,320.81,171.56,72.34,8.74;1,218.56,193.12,28.47,7.86"><roleName>PRHLT</roleName><forename type="first">Roberto</forename><surname>Paredes</surname></persName>
							<email>rparedes@prhlt.upv.es</email>
							<affiliation key="aff0">
								<orgName type="institution">Universitat Politècnica de València</orgName>
								<address>
									<addrLine>Camí de Vera s/n</addrLine>
									<postCode>46022</postCode>
									<settlement>València</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,156.22,115.96,302.91,12.62;1,192.64,133.89,230.06,12.62">Overview of the ImageCLEF 2014 Scalable Concept Image Annotation Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C472BC9B1F7E6807892269274D485E0B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The ImageCLEF 2014 Scalable Concept Image Annotation task was the third edition of a challenge aimed at developing more scalable image annotation systems. Unlike traditional image annotation challenges, which rely on a set of manually annotated images as training data, the participants were only allowed to use data and/or resources that as new concepts to detect are introduced do not require significant human effort (such as hand labeling). The participants were provided with web data consisting of 500,000 images, which included textual features obtained from the web pages on which the images appeared, as well as various visual features extracted from the images themselves. To optimize their systems, the participants were provided with a development set of 1,940 samples and its corresponding hand labeled ground truth for 107 concepts. The performance of the submissions was measured using a test set of 7,291 samples which was hand labeled for 207 concepts among which 100 were new concepts unseen during development. In total 11 teams participated in the task submitting overall 58 system runs. Thanks to the larger amount of unseen concepts in the results the generalization of the systems has been more clearly observed and thus demonstrating the potential for scalability.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="792.0" lry="612.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="792.0" lry="612.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic concept detection within images is a challenging and as of yet unsolved research problem. Over the past decades impressive improvements have been achieved, albeit admittedly not yet successfully solving the problem. Yet, these improvements have been typically obtained on datasets for which all images have been manually, and thus reliably, labeled. For instance, it has become common in past image annotation benchmark campaigns <ref type="bibr" coords="1,387.58,566.19,11.62,8.74" target="#b10">[9,</ref><ref type="bibr" coords="1,399.20,566.19,11.62,8.74" target="#b16">15,</ref><ref type="bibr" coords="1,410.82,566.19,7.75,8.74" target="#b4">3]</ref> to use crowdsourcing approaches, such as the Amazon Mechanical Turk 1 , in order to let multiple annotators label a large collection of images. Nevertheless, crowdsourcing is expensive and difficult to scale to a very large amount of concepts. The image annotation datasets furthermore usually include exactly the same concepts in the training and test sets, which may mean that the evaluated visual concept detection algorithms are not necessarily able to cope with detecting additional concepts beyond what they were trained on. To address these shortcomings a novel image annotation task <ref type="bibr" coords="2,261.40,130.95,15.50,8.74" target="#b20">[19]</ref> was proposed in 2012 for which automatically gathered web data had to be used for concept detection, where the concepts varied between the evaluation sets. The aim of that task was to reduce the reliance of cleanly annotated data for concept detection and rather focus on uncovering structure from noisy data, emphasizing the importance of the need for scalable annotation algorithms able to determine for any given concept whether or not it is present in an image. The rationale behind the scalable image annotation task was that there are billions of images available online appearing on webpages, where the text surrounding the image may be directly or indirectly related to its content, thus providing clues as to what is actually depicted in the image. Moreover, images and the webpages on which they appear can be easily obtained for virtually any topic using a web crawler. In existing work such noisy data has indeed proven useful, e.g. <ref type="bibr" coords="2,248.32,274.41,15.89,8.74" target="#b17">[16,</ref><ref type="bibr" coords="2,264.21,274.41,11.92,8.74" target="#b23">22,</ref><ref type="bibr" coords="2,276.14,274.41,11.92,8.74" target="#b22">21]</ref>.</p><p>This paper presents the overview of the third edition of the Scalable Concept Image Annotation task <ref type="bibr" coords="2,238.18,298.34,15.50,8.74" target="#b20">[19,</ref><ref type="bibr" coords="2,253.68,298.34,11.62,8.74" target="#b21">20]</ref>, one of the four benchmark campaigns organized by ImageCLEF <ref type="bibr" coords="2,205.38,310.30,10.52,8.74" target="#b3">[2]</ref> in 2014 under the CLEF initiative<ref type="foot" coords="2,369.63,308.73,3.97,6.12" target="#foot_1">2</ref> . Section 2 describes the task in detail, including the participation rules and the provided data and resources. Followed by this, Section 3 presents and discusses the results of the submissions. Finally, Section 4 concludes the paper with final remarks and future outlooks.</p><p>2 Overview of the Task</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Motivation and Objectives</head><p>Image concept detection research generally has relied on training data that has been manually, and thus reliably annotated, an expensive and laborious endeavor that cannot easily scale as the number of concepts is increased. As an alternative to clean labeled data, a very large amount of images can be easily gathered from the web, and furthermore, from the webpages that contain the images, text associated with them can be obtained. However, the degree of relationship between the surrounding text and the image varies greatly. Moreover, the webpages can be of any language or even a mixture of languages, and they tend to have many writing mistakes. Overall the data can be considered to be very noisy. Motivated by this need for scalability and the possibility of cheaply obtaining useful data, the ImageCLEF 2014 Scalable Concept Image Annotation task concentrated exclusively on developing annotation systems that rely only on automatically obtained data.</p><p>To illustrate the objective of the evaluation, consider for example that someone searches for the word "rainbow" in a popular image search engine. It would be expected that many results be of landscapes in which in the sky a rainbow is visible. However, other types of images will also appear, see Figure <ref type="figure" coords="2,446.79,625.49,8.49,8.74" target="#fig_1">1a</ref>. The images will be related to the query in different senses, and there might even be  images that do not have any apparent relationship. In the example of Figure <ref type="figure" coords="3,467.88,342.53,8.49,8.74" target="#fig_1">1a</ref>, one image is a text page of a poem about a rainbow, and another is a photograph of an old cave painting of a rainbow serpent. See Figure <ref type="figure" coords="3,413.05,366.44,10.52,8.74" target="#fig_1">1b</ref> for a similar example on the query "sun". As can be observed, the data is noisy, although it does have the advantage that this data can also handle the possible different senses that a word can have, or the different types of images that exist, such as natural photographs, paintings and computer-generated imagery.</p><p>In order to handle the web data, there are several resources that could be employed in the development of scalable annotation systems. Many resources can be used to help match general text to given concepts, amongst which some examples are stemmers, word disambiguators, definition dictionaries, ontologies and encyclopedia articles. There are also tools that can help to deal with noisy text commonly found on webpages, such as language models, stop word lists and spell checkers. And last but not least, language detectors and statistical machine translation systems are able to process webpage data written in various languages.</p><p>In summary, the goal of the scalable image annotation task was to evaluate different strategies to deal with noisy data, so that the unsupervised web data can be reliably used for annotating images for practically any topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Challenge Description</head><p>The challenge 3 consisted of the development of an image annotation system given training data that only included images crawled from the Internet, the corresponding webpages on which they appeared, as well as precomputed visual and textual features. As mentioned in the previous section, the aim of the task was for the annotation systems to be able to easily change or scale the list of concepts used for image annotation. Apart from the image and webpage data, the participants were also permitted and encouraged to use similar datasets and any other automatically obtainable resources to help in the processing and usage of the training data. However, the most important rule was that the systems were not permitted to use any kind of data that had been explicitly and manually labeled for concept detection learning.</p><p>For the development of the annotation systems, the participants were provided with the following:</p><p>-A training dataset of images and corresponding webpages compiled specifically for the task, including precomputed visual and textual features (see Section 2.3). -Source code of a simple baseline annotation system (see Section 2.4).</p><p>-Tools for computing the appropriate performance measures (see Section 2.5).</p><p>-A development set of images with ground truth annotations (including precomputed visual features) for estimating the system performance.</p><p>After a period of three and a half months to work on the development set, a test set of images was released which did not include any ground truth labels. The participants had to use their developed systems to predict the concepts for each of the input images and submit these results to the task organizers. About one month was given to work on the test data and a maximum of 10 submissions (also referred to as runs) were allowed per participating group. Since one of the objectives was that the annotation systems be able to scale or change the list of concepts for annotation, the list of concepts for the test set was not exactly the same as those for the development set. Moreover, each test image had its own list of concepts to detect, so not all images had to be annotated for all the possible concepts. The development set consisted of 1,940 samples labeled for 107 concepts, and the test set consisted of 7,291 samples labeled for 207 concepts (the same 107 concepts from development and 100 additional ones).</p><p>The concepts to be used for annotation were defined as one or more WordNet synsets <ref type="bibr" coords="4,168.41,504.73,9.96,8.74" target="#b5">[4]</ref>. So, for each concept there was a concept name, the type (either noun or adjective), and the sense number(s). Defining the concepts this way, made it straightforward to obtain the concept definition, synonyms, hyponyms, etc. Additionally, for most of the concepts, a link to a Wikipedia article about the respective concept was provided. The complete list of concepts, as well as the number of samples in the test sets, is included in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Dataset</head><p>The dataset<ref type="foot" coords="4,187.62,612.19,3.97,6.12" target="#foot_3">4</ref> used was very similar to the one of the first two editions of the task <ref type="bibr" coords="4,155.83,625.72,15.50,8.74" target="#b20">[19,</ref><ref type="bibr" coords="4,171.33,625.72,11.62,8.74" target="#b21">20]</ref>. To create the dataset, initially a database of over 31 million images was created by querying Google, Bing and Yahoo! using words from the Aspell English dictionary <ref type="bibr" coords="5,220.25,118.99,14.61,8.74" target="#b19">[18]</ref>. The images and corresponding webpages were downloaded, taking care to avoid data duplication. Then, a subset of 500,000 images (to be used as the training set) was selected from this database by choosing the top images from a ranked list. Half of this data was exactly the same as the training set from last year, the additional data was merely intended to supply images for the new concepts that were introduced. The motivation for selecting a subset was to provide smaller data files that would not be so prohibitive for the participants to download and handle. The ranked list was generated by retrieving images from our database using the list of concepts, in essence more or less as if the search engines had only been queried for these. From the ranked list, some types of problematic images were removed, and it was guaranteed that each image had at least one webpage in which they appeared. Unlike the training set, the development and test sets were manually selected and labeled for the concepts being evaluated. For further details on how the dataset was created, please refer to <ref type="bibr" coords="5,199.36,286.37,14.61,8.74" target="#b20">[19]</ref>.</p><p>Textual Data: Since the textual data was to be used only during training, it was only provided for the training set. Four sets of data were made available to the participants. The first one was the list of words used to find the image when querying the search engines, along with the rank position of the image in the respective query and search engine it was found on. The second set of textual data contained the image URLs as referenced in the webpages they appeared in. In many cases the image URLs tend to be formed with words that relate to the content of the image, which is why they can also be useful as textual features. The third set of data were the webpages in which the images appeared, for which the only preprocessing was a conversion to valid XML just to make any subsequent processing simpler. The final set of data were features obtained from the text extracted near the position(s) of the image in each webpage it appeared in.</p><p>To extract the text near the image, after conversion to valid XML, the script and style elements were removed. The extracted text were the webpage title and all the terms closer than 600 in word distance to the image, not including the HTML tags and attributes. Then a weight s(t n ) was assigned to each of the words near the image, defined as</p><formula xml:id="formula_0" coords="5,210.23,537.86,270.36,26.88">s(t n ) = 1 ∀t∈T s(t) ∀tn,m∈T F n,m sigm(d n,m ) ,<label>(1)</label></formula><p>where t n,m are each of the appearances of the term t n in the document T , F n,m is a factor depending on the DOM (e.g. title, alt, etc.) similar to what is done in the work of La Cascia et al. <ref type="bibr" coords="5,273.32,600.80,9.96,8.74" target="#b7">[6]</ref>, and d n,m is the word distance from t n,m to the image. The sigmoid function was centered at 35, had a slope of 0.15 and minimum and maximum values of 1 and 10 respectively. The resulting features include for each image at most the 100 word-score pairs with the highest scores.</p><p>Visual Features: Before visual feature extraction, images were filtered and resized so that the width and height had at most 240 pixels while preserving the original aspect ratio. These raw resized images were provided to the participants but also seven types of precomputed visual features. The first feature set Colorhist consisted of 576-dimensional color histograms extracted using our own implementation. These features correspond to dividing the image in 3 × 3 regions and for each region obtaining a color histogram quantified to 6 bits. The second feature set GETLF contained 256-dimensional histogram based features. First, local color-histograms were extracted in a dense grid every 21 pixels for windows of size 41 × 41. Then, these local color-histograms were randomly projected to a binary space using 8 random vectors and considering the sign of the resulting projection to produce the bit. Thus, obtaining a 8-bit representation of each local color-histogram that can be considered as a word. Finally, the image is represented as a bag-of-words, leading to a 256-dimensional histogram representation. The third set of features consisted of GIST <ref type="bibr" coords="6,389.54,286.37,15.50,8.74" target="#b11">[10]</ref> descriptors. The other four feature types were obtained using the colorDescriptors software <ref type="bibr" coords="6,462.34,298.32,14.61,8.74" target="#b14">[13]</ref>, namely SIFT, C-SIFT, RGB-SIFT and OPPONENT-SIFT. The configuration was dense sampling with default parameters and a hard assignment 1,000 codebook using a spatial pyramid of 1×1 and 2×2 <ref type="bibr" coords="6,334.13,334.19,9.96,8.74" target="#b8">[7]</ref>. Since the vectors of the spatial pyramid were concatenated, this resulted in 5,000-dimensional feature vectors. The codebooks were generated using 1.25 million randomly selected features and the k-means algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Baseline Systems</head><p>A toolkit was supplied to the participants as a performance reference for the evaluation, as well as to serve as a starting point. This toolkit included software that computed the evaluation measures (see Section 2.5) and the implementations of two baselines. The first baseline was a simple random, which is important since any system that gets worse performance than random is useless. The other baseline, referred to as Co-occurrence Baseline, was a basic technique that gives better performance than random, although it was simple enough to give the participants a wide margin for improvement. In the latter technique, when given an input image, obtains its nearest k = 32 images from the training set. Then, the textual features corresponding to these k nearest images are used to derive a score for each of the concepts. This is done by using a concept-word co-occurrence matrix estimated from all of the training set textual features. In order to make the vocabulary size more manageable, the textual features are first processed keeping only English words. Finally, the amount of concepts assigned to the image is variable, the concepts selected are the ones with a score higher than the sum of the mean and the standard deviation for all the concept scores of that image. Since there were seven visual features provided, each one was considered separately for a baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Performance Measures</head><p>Ultimately the goal of an image annotation system is to make decisions about which concepts to assign to given image from a predefined list of concepts. Thus to measure annotation performance what should be considered is how good are those decisions. On the other hand, in practice many annotations systems are based on estimating a score for each of the concepts and then a second technique uses these scores to finally decide which concepts are chosen. For systems of this type a measure of performance can be based only on the concept scores, which considers all aspects of the system except for the technique used for concept decisions, making it an interesting characteristic to measure. For this task, two basic performance measures have been used for comparing the results of the different submissions. The first one is the F-measure (F 1 ), which takes into account the final annotation decisions, and the other is the Average Precision (AP), which considers the concept scores.</p><p>The F 1 is defined as</p><formula xml:id="formula_1" coords="7,277.51,303.14,198.84,22.31">F 1 = 2P R P + R , (<label>2</label></formula><formula xml:id="formula_2" coords="7,476.35,309.88,4.24,8.74">)</formula><p>where P is the precision and R is the recall. In the context of image annotation, the F 1 can be estimated from two different perspectives, one being concept-based and the other sample-based. In the former, one F 1 is computed for each concept, and in the latter one F 1 is computed for each image to annotate. In both cases, the arithmetic mean is used as a global measure of performance, and will be referenced as MF 1 -concepts and MF 1 -samples, respectively. The AP is algebraically defined as</p><formula xml:id="formula_3" coords="7,256.47,423.33,219.88,31.41">AP = 1 |K| |K| k=1 k rank(k) , (<label>3</label></formula><formula xml:id="formula_4" coords="7,476.35,434.60,4.24,8.74">)</formula><p>where K is the ordered set of the ground truth annotations, being the order induced by the annotation scores, and rank(k) is the order position of the k-th ground truth annotation. The fraction k/ rank(k) is actually the precision at the k-th ground truth annotation, and has been written like this to be explicit on the way it is computed. In the cases that there are ties in the scores, a random permutation is applied within the ties. The AP can also be estimated for both the concept-based and sample-based perspectives, however, the concept-based AP is not a suitable measure of annotation performance (it is more adequate for a retrieval scenario), so only the sample-based AP has been considered in this evaluation. As a global measure of performance, also the arithmetic mean is used, which will be referred to as MAP-samples. A bit of care must be taken when comparing systems using the MAP-samples measure. What the MAP-samples turns out saying is that if for a given image the scores are used to sort the concepts, how good would it rank the true concepts for the image. Depending on the system, its scores could or could not be optimal for ranking the concepts. Thus a system with a relatively low MAP-samples, could still have a good annotation performance if the method used to select the concepts is adequate for its concept scores. Because of this, as well as the fact that there can be systems that do not rely on scores, it was optional for the participants of the task to provide scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Participation</head><p>The participation was excellent, although there was a slight decrease in participation with respect to last year. In total, 11 groups took part in the task and submitted overall 58 system runs. Among the 11 participating groups, only 7 of them submitted a corresponding paper describing their system, thus only for these there were specific details available. Last year the participation was 13 groups, 58 runs and 9 papers. The following 11 teams were the ones that participated:</p><p>-INAOE: 5 The team from the Instituto Nacional de Astrofísica, Óptica y Electrónica (Puebla, Mexico) was represented by Hugo Jair Escalante and Luis Pellegrin. -NII: 5 The team from the National Institute of Informatics (Tokyo, Japan) was represented by Duy Dinh Le.</p><p>Table <ref type="table" coords="9,177.20,188.36,4.98,8.74">1</ref> provides the main key details for the best submission of each group that submitted a paper describing their system. This table serves as a summary of the systems, and also is quite illustrative for quick comparisons. For a more in depth look of the annotation systems of each team, please refer to their corresponding paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Scalability Analysis</head><p>Since the objective of this task was to compare annotation systems that are scalable, a very important aspect to evaluate is precisely their scalability. However, unlike the annotation performance, it is difficult to quantify the scalability of a system so that the submissions can be compared in this respect. Therefore, instead of attempting to give a measure for scalability, in this section we make a few comments about possible aspects of the proposed systems in which the scalability could be compromised.</p><p>One characteristic observed in this year's task was that three teams based their system on Convolutional Neural Networks (CNN) pre-trained using Ima-geNet, a dataset which was manually hand labeled for 1,000 WordNet synsets. Two of the teams, MIL and MindLab, used the CNN output of an intermediate layer as a visual feature. There are works in which it has been observed that CNN features perform well in new problems different from the one that was trained for, so in some sense their use does not violate the competition rule of no hand labeled data usage. However, a minor detail is that the ImageNet synsets overlap considerably with the current task's concepts, so the annotation performance for these systems might be a bit optimistic in comparison to the others. The third team that used CNN was MLIA, which employed the synsets predicted by the CNN to clean the concepts automatically assigned using the webpage data. In this case the performance of the system could be greatly affected if the concepts for annotation differ significantly from the ones of ImageNet.</p><p>Also this year most of the teams proposed approaches based on classifiers that need to be learned. In the case of the MIL team, the classifier is multilabel. A multilabel classifier could be problematic since each time the list of concepts to detect changes, the classifier would have to be relearned. However, the PAAPL algorithm of MIL is designed with special consideration of scalability, so in their case it does not seem an issue. The alternative of multilabel is having one classifier per concept, which are learned one concept at a time using positive and negative samples. For scalability, the learning should be based on a selection of negative images so that this process is independent of how many concepts there are. It seems that all of the teams consider this adequately. However, with respect to a multilabel classifier this might not be the optimal approach. When new concepts are introduced it could be advisable to learn new classifiers to consider the relationships between the concepts. However, this relationship could be taken into account in a step after classification which is what the KDEVIR team has done with their constructed ontologies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Annotation Performance Results</head><p>The test set for this year was composed of 4 subsets of samples, each of which had a different list of concepts for annotation. The first subset contained 3,000 images which were exactly the same as last year's development and test sets, and the list of concepts for annotation were also the same 116. The second subset had 1,747 images and the list of concepts were 52 related to the topic animals. The third subset had 479 images and the list of concepts were 41 related to the topic foods. For both the animals and foods subsets all of the concepts for annotation were not among the ones seen in development. The final subset had 2065 images and the list of concepts for annotation were all the 207. Table <ref type="table" coords="10,177.21,309.42,4.98,8.74" target="#tab_2">2</ref> presents the performance measures (mentioned in 2.5) for the baseline techniques and all of the submitted runs by the participants. The table includes the results for the complete test set (referenced as all ) and for three of the mentioned subsets: animals, foods and the one annotated for all the 207 concepts, referenced as ani., food and 207, respectively. Also for the MF 1 -concepts measure the unseen column presents the results for the complete test set, but only considering the 100 concepts that did not appear in the development set. The systems are ordered by performance, beginning at the top with the best performing one. This order was derived by considering for the test set the average rank when comparing all of the systems, using the complete test set for the three performance measures and also MF 1 -concepts unseen. Ties were broken by the average of the same measures. Considering only the performance measures, this ranking indicates that the best system this year was the one developed by KDEVIR.</p><p>For an easier comparison and a more intuitive visualization, the results for the complete test set and all the submissions are presented as graphs in Figure <ref type="figure" coords="10,460.74,488.75,3.87,8.74" target="#fig_2">2</ref>. In the graphs the error bars correspond to the 95% confidence intervals estimated by Wilson's method, employing the standard deviation for the individual measures (for the samples or concepts, and for the average precisions (AP) or F-measures (F 1 ), depending on the case). For the MF 1 -concepts measure two results for each submission is presented, one that includes all concepts and another that considers only the unseen concepts. Similarly Figure <ref type="figure" coords="10,370.49,560.48,4.98,8.74">3</ref> presents the results for all the submissions, but in each case depicting the performance for three of the subsets of the test set: animals, foods and the 207 concepts subset. The fourth subset of the test is intended for comparison of the systems with respect to the previous edition of the task, so this is presented in a separate graph, Figure <ref type="figure" coords="10,472.84,608.30,3.87,8.74">4</ref>, although for space reasons and make the comparison more illustrative, only the best submission of each group is included.</p><p>Finally, in Figure <ref type="figure" coords="10,229.47,644.16,4.98,8.74" target="#fig_5">5</ref> there is for each of the 207 test set concepts, a boxplot for the F 1 when combining all runs. In order to fit all of the concepts in the same graph, for multiple outliers with the same value, only one is shown. The concepts have been sorted by the median performance of all submissions, which in a way orders them by difficulty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Discussion</head><p>As can be observed in Figure <ref type="figure" coords="11,268.39,584.39,3.87,8.74">4</ref>, the performance of the systems has improved somewhat with respect to what was obtained in the previous edition of the task. This year 5 teams obtained all of the performance measures over 30% in contrast to just 3 from last year. An interesting detail is that it seams that the improvements for the MF 1 measures are greater than for the MAP-samples. Thus it can be observed that this year better approaches have been developed for making the final concept annotations decisions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MF1-concepts (%)</head><p>Fig. <ref type="figure" coords="12,154.40,464.29,4.13,7.89">3</ref>: Performance measures for three of the subsets in the test set for all the submissions.</p><p>Observing the results for the complete test set in Figure <ref type="figure" coords="12,402.95,512.66,3.87,8.74" target="#fig_2">2</ref>, the best MAPsamples and MF 1 -samples values are somewhat lower than for last year's test set. This could be related to the fact that this year the list of concepts was larger, thus making the problem a bit more difficult. With respect to the MF 1concepts measure, last year the results were characterized by having relatively large confidence intervals, which made drawing conclusions a bit difficult specially for the unseen concepts. The increase in the number of unseen concepts has made the results clearer. For the three measures the performance for two of the systems submitted by KDEVIR significantly outperforms all of the others. Most impressive is the advantage obtained for the MF 1 -concepts measure and even more if only the unseen concepts are considered, obtaining a performance over 65%. Note that the good performance for the unseen concepts is due to the fact that many of the new concepts were used in the animals and foods subsets </p><formula xml:id="formula_5" coords="13,170.53,246.67,296.32,49.00">U N I M O R E 2 0 1 3 R U C 2 0 1 3 U N E D &amp; U V 2 0 1 3 C E A L I S T 2 0 1 3 U R J C &amp; U N E D 2 0 1 3 M I C C 2 0 1 3 K D E V I R 2 0 1 3 K D E V I R 2 0 1 4 M I L 2 0 1 4 M L I A 2 0 1 4 M i n d L a b 2 0 1 4 R U C 2 0 1 4 D I S A 2 0 1 4 I P L 2 0 1 4</formula><p>Performance (%)</p><p>MAP-samples MF1-samples MF1-concepts Fig. <ref type="figure" coords="13,154.40,310.24,4.13,7.89">4</ref>: Performance measures for the best submission of each group for both this and last year's edition of the task. The results for both years are for exactly the same test set.</p><p>which had smaller concept lists, making the problem a bit easier. Analyzing the key details of the systems presented in Table <ref type="table" coords="13,332.02,380.27,3.87,8.74">1</ref>, it can be noted that the success of the KDEVIR system is most probably due to the usage of concept ontologies both in the training phase for better selecting the images used for optimizing the classifiers and in the testing phase for taking into account the relationships between the concepts. Moreover, the KDEVIR system also employed the technique of last year's winner TPT <ref type="bibr" coords="13,251.84,440.04,14.61,8.74" target="#b13">[12]</ref>, which uses a learning technique that takes into account context, effectively finding a way to exploit the information available in the noisy webpage data. Even though the foods and animals subsets consisted purely of unseen concepts, in the results in Figure <ref type="figure" coords="13,270.19,488.30,4.98,8.74">3</ref> it can be seen that the performance for these is in general much better, mostly because of the known relationship between the size of the list of concepts for annotation and the performance. The smaller the list of concepts, the easier the problem becomes. Moreover, similar to last year in Figure <ref type="figure" coords="13,202.43,536.13,4.98,8.74" target="#fig_5">5</ref> it can be observed that the unseen concepts do not tend to perform worse. The difficulty of each particular concept affects more the performance than the fact that these have not been seen during development, or from another perspective the systems are able to generalize rather well to new concepts.</p><p>Considering both the annotation performance measures and the scalability analysis, it can be declared that this year's winner is the KDEVIR system. The fact that KDEVIR only used the provided visual features shows the characteristic of this evaluation, which in contrast to usual image annotation tasks with labeled training data, this challenge requires work in more fronts in order to get important improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>This paper presented an overview of the ImageCLEF 2014 Scalable Concept Image Annotation task, the third edition of a challenge aimed at developing more scalable image annotation systems. The goal was to develop annotation systems that for training, only rely on unsupervised web data and other cheaply obtainable resources, thus making it easy to add or change the concepts for annotation.</p><p>The participation was similar to last year although with a slight decrease, 11 teams submitted in total 58 system runs. The performance of the submitted systems was somewhat superior to last year's results, in particular improving more for the MF 1 measures, which indicate a greater success in the developed techniques for choosing the final annotated concepts. Thanks to the larger amount of concepts in the test set that were not seen during development, the results for the MF 1 -concepts measure had narrower confidence intervals, so it made the comparison the systems more conclusive. Moreover, by having subsets in the test set which had to be annotated using only unseen concepts, it has been observed that the systems are able to generalize well. The clear winner of this year's evaluation was the KDEVIR <ref type="bibr" coords="14,247.71,334.19,15.50,8.74" target="#b12">[11]</ref> team, which after analyzing the key components of the system it can be observed that most of the success is due to the usage of a classifier learning technique that takes into account context, effectively finding a way to exploit the information available in the noisy webpage data; and the usage of automatically generated concept ontologies both in the training phase for better selecting the images used for optimizing the classifiers and in the testing phase for taking into account the relationships between the concepts.</p><p>The results of the task have been very interesting and show that useful annotation systems can be built using noisy web crawled data. Since the problem requires to cover many fronts, there is still a lot of work that can be done, so it would be interesting to continue this line of research. Papers on this topic should be published, demonstration systems based on these ideas be built and more evaluation of this sort be organized. Also it remains to see how this can be used to complement systems that are based on clean hand labeled data and find ways to take advantage of both the supervised and unsupervised data. Table <ref type="table" coords="16,205.19,151.79,4.13,7.89">1</ref>: Key details of the best system for each of the groups that submitted a paper describing their system.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,215.02,190.51,185.31,7.89;3,223.72,285.57,167.91,7.89"><head>( a )</head><label>a</label><figDesc>Images from a search query of "rainbow".(b) Images from a search query of "sun".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,155.74,306.49,300.80,7.89;3,146.50,210.90,69.17,69.17"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Example of images retrieved by a commercial image search engine.</figDesc><graphic coords="3,146.50,210.90,69.17,69.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="11,147.00,469.27,318.28,7.89"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Performance measures for the complete test set for all the submissions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="15,129.08,464.86,547.08,7.89;15,129.08,475.84,393.08,7.86"><head>Fig. 5 :</head><label>5</label><figDesc>Fig.5: Boxplots for the test set of the per concept annotation F1 (in %) for all runs combined. The plots are ordered by the median performance. Concepts in red font and with an asterisk ( * ) are the ones not seen in development.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="17,134.77,116.63,345.81,18.85"><head>Table 2 :</head><label>2</label><figDesc>Test set performance measures (%) for the baselines and all submissions. The best submission for each team is highlighted with a gray background.The following tables present the 207 concepts used in the ImageCLEF 2014 Scalable Concept Image Annotation task. In the electronic version of this document, each concept name and Wikipedia article name are hyperlinks to webpages of the corresponding WordNet synset and the Wikipedia article, respectively.Concepts seen in both development and test sets:</figDesc><table coords="20,134.77,117.60,341.87,510.65"><row><cell cols="4">A Concept List 2014</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Concept</cell><cell cols="2">WordNet 3 type sense#</cell><cell>Wikipedia article</cell><cell>#test</cell><cell>Concept</cell><cell cols="2">WordNet 3 type sense#</cell><cell>Wikipedia article</cell><cell>#test</cell></row><row><cell>aerial</cell><cell>adj.</cell><cell>1</cell><cell>Aerial photography</cell><cell>112</cell><cell>lightning</cell><cell>noun</cell><cell>1, 2</cell><cell>Lightning</cell><cell>26</cell></row><row><cell>airplane</cell><cell>noun</cell><cell>1</cell><cell>Airplane</cell><cell>33</cell><cell>logo</cell><cell>noun</cell><cell>1</cell><cell>Logo</cell><cell>51</cell></row><row><cell>baby</cell><cell>noun</cell><cell>1</cell><cell>Baby</cell><cell>38</cell><cell>male</cell><cell>noun</cell><cell>2</cell><cell>Male</cell><cell>207</cell></row><row><cell>beach</cell><cell>noun</cell><cell>1</cell><cell>Beach</cell><cell>87</cell><cell>monument</cell><cell>noun</cell><cell>1</cell><cell>Monument</cell><cell>28</cell></row><row><cell>bicycle</cell><cell>noun</cell><cell>1</cell><cell>Bicycle</cell><cell>33</cell><cell>moon</cell><cell>noun</cell><cell>1</cell><cell>Moon</cell><cell>38</cell></row><row><cell>bird</cell><cell>noun</cell><cell>1</cell><cell>Bird</cell><cell>110</cell><cell>motorcycle</cell><cell>noun</cell><cell>1</cell><cell>Motorcycle</cell><cell>33</cell></row><row><cell>boat</cell><cell>noun</cell><cell>1</cell><cell>Boat</cell><cell>144</cell><cell>mountain</cell><cell>noun</cell><cell>1</cell><cell>Mountain</cell><cell>287</cell></row><row><cell>book</cell><cell>noun</cell><cell>2, 1</cell><cell>Book</cell><cell>44</cell><cell>newspaper</cell><cell>noun</cell><cell>3, 1</cell><cell>Newspaper</cell><cell>19</cell></row><row><cell>bottle</cell><cell>noun</cell><cell>1</cell><cell>Bottle</cell><cell>32</cell><cell>nighttime</cell><cell>noun</cell><cell>1</cell><cell>Nighttime</cell><cell>154</cell></row><row><cell>bridge</cell><cell>noun</cell><cell>1</cell><cell>Bridge</cell><cell>79</cell><cell>outdoor</cell><cell>adj.</cell><cell>1, 2</cell><cell>-</cell><cell>2255</cell></row><row><cell>building</cell><cell>noun</cell><cell>1</cell><cell>Building</cell><cell>478</cell><cell>overcast</cell><cell>noun</cell><cell>1</cell><cell>Overcast</cell><cell>137</cell></row><row><cell>bus</cell><cell>noun</cell><cell>1</cell><cell>Bus</cell><cell>45</cell><cell>painting</cell><cell>noun</cell><cell>1</cell><cell>Painting</cell><cell>125</cell></row><row><cell>car</cell><cell>noun</cell><cell>1</cell><cell>Car</cell><cell>135</cell><cell>park</cell><cell>noun</cell><cell>2</cell><cell>Park</cell><cell>47</cell></row><row><cell>cartoon</cell><cell>noun</cell><cell>1</cell><cell>Cartoon</cell><cell>104</cell><cell>person</cell><cell>noun</cell><cell>1</cell><cell>Person</cell><cell>856</cell></row><row><cell>castle</cell><cell>noun</cell><cell>2</cell><cell>Castle</cell><cell>38</cell><cell>phone</cell><cell>noun</cell><cell>1</cell><cell>Phone</cell><cell>26</cell></row><row><cell>cat</cell><cell>noun</cell><cell>1</cell><cell>Cat</cell><cell>38</cell><cell>plant</cell><cell>noun</cell><cell>2</cell><cell>Plant</cell><cell>1261</cell></row><row><cell>chair</cell><cell>noun</cell><cell>1</cell><cell>Chair</cell><cell>64</cell><cell>portrait</cell><cell>noun</cell><cell>1</cell><cell>Portrait</cell><cell>36</cell></row><row><cell>child</cell><cell>noun</cell><cell>1</cell><cell>Child</cell><cell>88</cell><cell>poster</cell><cell>noun</cell><cell>1</cell><cell>Poster</cell><cell>23</cell></row><row><cell>church</cell><cell>noun</cell><cell>2</cell><cell>Church (building)</cell><cell>28</cell><cell>protest</cell><cell>noun</cell><cell>2</cell><cell>Protest</cell><cell>28</cell></row><row><cell cols="2">cityscape noun</cell><cell>1</cell><cell>Cityscape</cell><cell>163</cell><cell>rainbow</cell><cell>noun</cell><cell>1</cell><cell>Rainbow</cell><cell>24</cell></row><row><cell>closeup</cell><cell>noun</cell><cell>1</cell><cell>Closeup</cell><cell>348</cell><cell>rain</cell><cell>noun</cell><cell>1</cell><cell>Rain</cell><cell>41</cell></row><row><cell>cloudless</cell><cell>adj.</cell><cell>1</cell><cell>-</cell><cell>274</cell><cell>reflection</cell><cell>noun</cell><cell>4, 5</cell><cell>Mirror image</cell><cell>149</cell></row><row><cell>cloud</cell><cell>noun</cell><cell>2</cell><cell>Cloud</cell><cell>609</cell><cell>river</cell><cell>noun</cell><cell>1</cell><cell>River</cell><cell>159</cell></row><row><cell>coast</cell><cell>noun</cell><cell>1</cell><cell>Coast</cell><cell>113</cell><cell>road</cell><cell>noun</cell><cell>1</cell><cell>Road</cell><cell>344</cell></row><row><cell cols="2">countryside noun</cell><cell>1</cell><cell>Countryside</cell><cell>117</cell><cell>sand</cell><cell>noun</cell><cell>1</cell><cell>Sand</cell><cell>141</cell></row><row><cell>daytime</cell><cell>noun</cell><cell>1</cell><cell cols="2">Daytime (astronomy) 2186</cell><cell>sculpture</cell><cell>noun</cell><cell>2</cell><cell>Sculpture</cell><cell>79</cell></row><row><cell>desert</cell><cell>noun</cell><cell>1</cell><cell>Desert</cell><cell>49</cell><cell>sea</cell><cell>noun</cell><cell>1</cell><cell>Sea</cell><cell>233</cell></row><row><cell>diagram</cell><cell>noun</cell><cell>1</cell><cell>Diagram</cell><cell>35</cell><cell>shadow</cell><cell>noun</cell><cell>2</cell><cell>Shadow</cell><cell>203</cell></row><row><cell>dog</cell><cell>noun</cell><cell>1</cell><cell>Dog</cell><cell>66</cell><cell>sign</cell><cell>noun</cell><cell>2</cell><cell>Sign</cell><cell>133</cell></row><row><cell>drink</cell><cell>noun</cell><cell>1</cell><cell>Drink</cell><cell>59</cell><cell>silhouette</cell><cell>noun</cell><cell>1</cell><cell>Silhouette</cell><cell>68</cell></row><row><cell>drum</cell><cell>noun</cell><cell>1</cell><cell>Drum</cell><cell>21</cell><cell>sky</cell><cell>noun</cell><cell>1</cell><cell>Sky</cell><cell>1230</cell></row><row><cell>elder</cell><cell>noun</cell><cell>1</cell><cell>Elderly</cell><cell>49</cell><cell>smoke</cell><cell>noun</cell><cell>1</cell><cell>Smoke</cell><cell>43</cell></row><row><cell cols="2">embroidery noun</cell><cell>2</cell><cell>Embroidery</cell><cell>24</cell><cell>snow</cell><cell>noun</cell><cell>2</cell><cell>Snow</cell><cell>175</cell></row><row><cell>female</cell><cell>noun</cell><cell>2</cell><cell>Female</cell><cell>211</cell><cell>soil</cell><cell>noun</cell><cell>2</cell><cell>Soil</cell><cell>247</cell></row><row><cell>fire</cell><cell>noun</cell><cell>3, 1</cell><cell>Fire</cell><cell>62</cell><cell>space</cell><cell>noun</cell><cell>4</cell><cell>Outer space</cell><cell>84</cell></row><row><cell>firework</cell><cell>noun</cell><cell>1</cell><cell>Firework</cell><cell>31</cell><cell>spectacles</cell><cell>noun</cell><cell>1</cell><cell>Spectacles</cell><cell>71</cell></row><row><cell>fish</cell><cell>noun</cell><cell>1</cell><cell>Fish</cell><cell>54</cell><cell>sport</cell><cell>noun</cell><cell>1</cell><cell>Sport</cell><cell>118</cell></row><row><cell>flower</cell><cell>noun</cell><cell>2</cell><cell>Flower</cell><cell>160</cell><cell>sun</cell><cell>noun</cell><cell>1</cell><cell>Sun</cell><cell>92</cell></row><row><cell>fog</cell><cell>noun</cell><cell>2</cell><cell>Fog</cell><cell>57</cell><cell cols="2">sunrise/sunset noun</cell><cell>1, 1</cell><cell>Sunrise/Sunset</cell><cell>90</cell></row><row><cell>food</cell><cell>noun</cell><cell>2, 1</cell><cell>Food</cell><cell>490</cell><cell>table</cell><cell>noun</cell><cell>2</cell><cell>Table (furniture)</cell><cell>49</cell></row><row><cell>footwear</cell><cell>noun</cell><cell>1, 2</cell><cell>Footwear</cell><cell>62</cell><cell>teenager</cell><cell>noun</cell><cell>1</cell><cell>Teenager</cell><cell>45</cell></row><row><cell>forest</cell><cell>noun</cell><cell>1, 2</cell><cell>Forest</cell><cell>235</cell><cell>toy</cell><cell>noun</cell><cell>1</cell><cell>Toy</cell><cell>56</cell></row><row><cell cols="2">furniture noun</cell><cell>1</cell><cell>Furniture</cell><cell>177</cell><cell>traffic</cell><cell>noun</cell><cell>1</cell><cell>Traffic</cell><cell>63</cell></row><row><cell>garden</cell><cell>noun</cell><cell>1</cell><cell>Garden</cell><cell>37</cell><cell>train</cell><cell>noun</cell><cell>1</cell><cell>Train</cell><cell>57</cell></row><row><cell>grass</cell><cell>noun</cell><cell>1</cell><cell>Grass</cell><cell>654</cell><cell>tree</cell><cell>noun</cell><cell>1</cell><cell>Tree</cell><cell>906</cell></row><row><cell>guitar</cell><cell>noun</cell><cell>1</cell><cell>Guitar</cell><cell>20</cell><cell>tricycle</cell><cell>noun</cell><cell>1</cell><cell>Tricycle</cell><cell>15</cell></row><row><cell>harbor</cell><cell>noun</cell><cell>1</cell><cell>Harbor</cell><cell>55</cell><cell>truck</cell><cell>noun</cell><cell>1</cell><cell>Truck</cell><cell>61</cell></row><row><cell>hat</cell><cell>noun</cell><cell>1</cell><cell>Hat</cell><cell>104</cell><cell>underwater</cell><cell>adj.</cell><cell>1, 2</cell><cell>Underwater</cell><cell>84</cell></row><row><cell cols="2">helicopter noun</cell><cell>1</cell><cell>Helicopter</cell><cell>22</cell><cell>unpaved</cell><cell>adj.</cell><cell>1</cell><cell>-</cell><cell>40</cell></row><row><cell>highway</cell><cell>noun</cell><cell>1</cell><cell>Highway</cell><cell>31</cell><cell>vehicle</cell><cell>noun</cell><cell>1</cell><cell>Vehicle</cell><cell>583</cell></row><row><cell>horse</cell><cell>noun</cell><cell>1</cell><cell>Horse</cell><cell>67</cell><cell>violin</cell><cell>noun</cell><cell>1</cell><cell>Violin</cell><cell>23</cell></row><row><cell>indoor</cell><cell>adj.</cell><cell>1</cell><cell>-</cell><cell>357</cell><cell>wagon</cell><cell>noun</cell><cell>1</cell><cell>Wagon</cell><cell>30</cell></row><row><cell cols="2">instrument noun</cell><cell>6</cell><cell>Musical instrument</cell><cell>92</cell><cell>water</cell><cell>noun</cell><cell>6</cell><cell>Water</cell><cell>807</cell></row><row><cell>lake</cell><cell>noun</cell><cell>1</cell><cell>Lake</cell><cell>110</cell><cell></cell><cell cols="3">continues in next page</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,657.44,61.19,7.47"><p>www.mturk.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,144.73,657.44,137.00,7.47"><p>http://www.clef-initiative.eu</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,144.73,656.80,255.79,8.11"><p>Challenge website at http://imageclef.org/2014/annotation</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,144.73,656.80,286.28,8.11"><p>Dataset available at http://risenet.prhlt.upv.es/webupv-datasets</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="8,144.73,656.80,179.72,7.86"><p>No paper describing their system submitted.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>-DISA: [1] The team from the <rs type="institution">Laboratory of Data Intensive Systems and Applications of the Masaryk University (Brno, Czech Republic</rs>) was represented by <rs type="person">Petra Budikova</rs>, <rs type="person">Jan Botorek</rs>, <rs type="person">Michal Batko</rs> and <rs type="person">Pavel Zezula</rs>. -IPL: [14] The team from the <rs type="institution">Information Processing Laboraroty of the Athens University of Economics and Business (Athens, Greece</rs>) was represented by <rs type="person">Spyridon Stathopoulos</rs> and <rs type="person">Theodore Kalamboukis</rs>. -KDEVIR: [11] The team from the <rs type="institution">Computer Science and Engineering department of the Toyohashi University of Technology (Aichi, Japan</rs>), was represented by <rs type="person">Ismat Ara Reshma</rs>, <rs type="person">Md Zia Ullah</rs> and <rs type="person">Masaki Aono</rs>. -MIL: [5] The team from the <rs type="institution">Machine Intelligence Lab of the University of Tokyo (Tokyo, Japan</rs>) was represented by <rs type="person">Atsushi Kanehira</rs>, <rs type="person">Masatoshi Hidaka</rs>, <rs type="person">Yusuke Mukuta</rs>, <rs type="person">Yuichiro Tsuchiya</rs>, <rs type="person">Tetsuaki Mano</rs> and <rs type="person">Tatsuya Harada</rs>. -MindLab: [17] The team from the Machine learning, perception and discovery Lab from the <rs type="institution">Universidad Nacional de Colombia (Bogotá, Colombia</rs>) was represented by <rs type="person">Jorge A. Vanegas</rs>, <rs type="person">John Arevalo</rs>, <rs type="person">Sebastian Otálora</rs>, <rs type="person">Fabián Páez</rs>, <rs type="person">Santiago A. Pérez-Rubiano</rs> and <rs type="person">Fabio A. González</rs>. -MLIA: [23] The team from the <rs type="institution">Department of Advanced Information Technology of the Kyushu University (Fukuoka, Japan</rs>) was represented by <rs type="person">Xing Xu</rs>, <rs type="person">Atsushi Shimada</rs> and <rs type="person">Rin-</rs>ichiro Taniguchi. -RUC: [8] The team from the <rs type="institution">School of Information of the Renmin University of China (Beijing, China</rs>) was represented by <rs type="person">Xirong Li</rs>, <rs type="person">Xixi He</rs>, <rs type="person">Gang Yang</rs>, <rs type="person">Qin Jin</rs> and <rs type="person">Jieping Xu</rs>. -FINKI: 5 The team from the <rs type="institution">Faculty of Computer Science and Engineering of the Ss. Cyril</rs> and <rs type="institution">Methodius University (Skopje, Republic of Macedonia</rs>) was represented by <rs type="person">Ivica Dimitrovski</rs>. -IMC: 5 The team from the <rs type="institution">Institute of Media Computing of the Fudan University (Shanghai, China</rs>) was represented by <rs type="person">Yong Cheng</rs>. KDEVIR #09 KDEVIR #08 KDEVIR #03 <rs type="projectName">KDEVIR</rs> #04 <rs type="projectName">KDEVIR</rs> #10 <rs type="projectName">KDEVIR</rs> #06 <rs type="projectName">KDEVIR</rs> #<rs type="projectName">05 KDEVIR</rs> #<rs type="projectName">02 KDEVIR</rs> #<rs type="grantNumber">07 KDEVIR #01 MIL #03 MIL #02 MIL #01</rs> <rs type="projectName">MindLab</rs> #<rs type="projectName">01 MindLab</rs> #<rs type="grantNumber">02 MLIA #09 MLIA #10 MLIA #08 MLIA #07 MLIA #06 MLIA #03 MLIA #04 MLIA #02 MLIA #01 MLIA #05 DISA #04</rs> <rs type="projectName">DISA</rs> #<rs type="grantNumber">05 DISA #03</rs> <rs type="projectName">DISA</rs> #<rs type="grantNumber">01</rs> <rs type="projectName">DISA</rs> #<rs type="grantNumber">02</rs> <rs type="projectName">RUC</rs> #<rs type="grantNumber">07 RUC</rs> #<rs type="grantNumber">05 RUC</rs> #<rs type="grantNumber">06</rs> <rs type="projectName">RUC</rs> #<rs type="projectName">02 RUC</rs> #<rs type="grantNumber">01</rs> <rs type="projectName">RUC</rs> #<rs type="projectName">03 RUC</rs> #<rs type="grantNumber">04</rs> <rs type="projectName">RUC</rs> #<rs type="grantNumber">08 IPL #09 IPL #08 IPL #10 IPL #07 IPL #04 IPL #03 IPL #05 IPL #06 IPL #02 IPL #01 IMC #01 IMC #02 INAOE #05 INAOE #06 INAOE #02 INAOE #04 INAOE #03 INAOE #01 NII #01 FINKI #01</rs></p></div>
<div><head>Acknowledgments</head><p>The authors are very grateful with the <rs type="funder">CLEF initiative</rs> for supporting ImageCLEF.</p><p>The research leading to these results has received funding from the <rs type="funder">European Union</rs>'s <rs type="programName">Seventh Framework Programme</rs> (<rs type="grantNumber">FP7/2007-2013</rs>) under the tranScriptorium project (#<rs type="grantNumber">600707</rs>) and from the <rs type="funder">Spanish MEC</rs> under the <rs type="projectName">STraDA</rs> project (<rs type="grantNumber">TIN2012-37475-C02-01</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_pE5xn9v">
					<orgName type="project" subtype="full">KDEVIR</orgName>
				</org>
				<org type="funded-project" xml:id="_32cScEh">
					<orgName type="project" subtype="full">KDEVIR</orgName>
				</org>
				<org type="funded-project" xml:id="_vjvyp2r">
					<orgName type="project" subtype="full">KDEVIR</orgName>
				</org>
				<org type="funded-project" xml:id="_wWYNmEX">
					<orgName type="project" subtype="full">KDEVIR</orgName>
				</org>
				<org type="funded-project" xml:id="_znqqvUT">
					<orgName type="project" subtype="full">05 KDEVIR</orgName>
				</org>
				<org type="funded-project" xml:id="_ugNUVzK">
					<idno type="grant-number">07 KDEVIR #01 MIL #03 MIL #02 MIL #01</idno>
					<orgName type="project" subtype="full">02 KDEVIR</orgName>
				</org>
				<org type="funded-project" xml:id="_QTyTpKg">
					<orgName type="project" subtype="full">MindLab</orgName>
				</org>
				<org type="funded-project" xml:id="_NnM9FJX">
					<idno type="grant-number">02 MLIA #09 MLIA #10 MLIA #08 MLIA #07 MLIA #06 MLIA #03 MLIA #04 MLIA #02 MLIA #01 MLIA #05 DISA #04</idno>
					<orgName type="project" subtype="full">01 MindLab</orgName>
				</org>
				<org type="funded-project" xml:id="_7H6RS4g">
					<idno type="grant-number">05 DISA #03</idno>
					<orgName type="project" subtype="full">DISA</orgName>
				</org>
				<org type="funded-project" xml:id="_v93yZsB">
					<idno type="grant-number">01</idno>
					<orgName type="project" subtype="full">DISA</orgName>
				</org>
				<org type="funded-project" xml:id="_kMfMW8e">
					<idno type="grant-number">02</idno>
					<orgName type="project" subtype="full">DISA</orgName>
				</org>
				<org type="funded-project" xml:id="_FRVUNgv">
					<idno type="grant-number">07 RUC</idno>
					<orgName type="project" subtype="full">RUC</orgName>
				</org>
				<org type="funding" xml:id="_VCcy2w3">
					<idno type="grant-number">05 RUC</idno>
				</org>
				<org type="funded-project" xml:id="_g669Zqh">
					<idno type="grant-number">06</idno>
					<orgName type="project" subtype="full">RUC</orgName>
				</org>
				<org type="funded-project" xml:id="_pSe9sZy">
					<idno type="grant-number">01</idno>
					<orgName type="project" subtype="full">02 RUC</orgName>
				</org>
				<org type="funded-project" xml:id="_t4wCQZK">
					<orgName type="project" subtype="full">RUC</orgName>
				</org>
				<org type="funded-project" xml:id="_jKFeK7P">
					<idno type="grant-number">04</idno>
					<orgName type="project" subtype="full">03 RUC</orgName>
				</org>
				<org type="funded-project" xml:id="_pVVhgen">
					<idno type="grant-number">08 IPL #09 IPL #08 IPL #10 IPL #07 IPL #04 IPL #03 IPL #05 IPL #06 IPL #02 IPL #01 IMC #01 IMC #02 INAOE #05 INAOE #06 INAOE #02 INAOE #04 INAOE #03 INAOE #01 NII #01 FINKI #01</idno>
					<orgName type="project" subtype="full">RUC</orgName>
				</org>
				<org type="funding" xml:id="_4KGMn56">
					<idno type="grant-number">FP7/2007-2013</idno>
					<orgName type="program" subtype="full">Seventh Framework Programme</orgName>
				</org>
				<org type="funded-project" xml:id="_kdsT8bj">
					<idno type="grant-number">600707</idno>
					<orgName type="project" subtype="full">STraDA</orgName>
				</org>
				<org type="funding" xml:id="_d8cjUXm">
					<idno type="grant-number">TIN2012-37475-C02-01</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="17,143.63,143.31,28.48,6.14;17,199.35,139.44,55.07,6.14;17,293.05,139.44,52.11,6.14;17,395.20,139.44,55.22,6.14;17,187.52,148.52,291.27,6.14;17,136.56,158.91,337.26,6.12;17,136.56,166.48,337.26,6.12;17,136.56,174.05,26.48,6.12;17,185.36,174.05,288.45,6.12;17,136.56,181.62,18.11,6.12;17,185.36,181.62,288.45,6.12;17,136.56,189.19,31.92,6.12;17,185.36,189.19,288.45,6.12;17,136.56,196.76,19.13,6.12;17,185.36,196.76,288.45,6.12;17,136.56,204.34,27.27,6.12;17,185.36,204.34,288.45,6.12;17,136.56,211.91,29.07,6.12;17,187.35,211.91,286.47,6.12;17,136.56,222.27,339.24,6.12;17,136.56,229.84,339.24,6.12;17,136.56,237.41,21.63,6.12;17,185.36,237.41,290.44,6.12;17,136.56,244.98,339.24,6.12;17,136.56,252.55,339.24,6.12;17,136.56,260.13,21.63,6.12;17,185.36,260.13,290.44,6.12;17,136.56,267.70,339.24,6.12;17,136.56,275.27,339.24,6.12;17,136.56,282.84,339.24,6.12;17,136.56,290.41,27.51,6.12;17,185.36,290.41,290.44,6.12;17,136.56,297.98,339.24,6.12;17,136.56,305.56,31.48,6.12;17,185.36,305.56,290.44,6.12;17,136.56,313.13,339.24,6.12;17,136.56,320.70,27.51,6.12;17,185.36,320.70,290.44,6.12;17,136.56,328.27,27.51,6.12;17,185.36,328.27,290.44,6.12;17,136.56,335.84,339.24,6.12;17,136.56,343.41,27.51,6.12;17,185.36,343.41,290.44,6.12;17,136.56,350.98,339.24,6.12;17,136.56,358.56,27.51,6.12;17,185.36,358.56,290.44,6.12;17,136.56,366.13,21.63,6.12;17,185.36,366.13,290.44,6.12;17,136.56,373.70,27.51,6.12;17,185.36,373.70,290.44,6.12;17,136.56,381.27,25.81,6.12;17,185.36,381.27,290.44,6.12;17,136.56,388.84,25.81,6.12;17,185.36,388.84,290.44,6.12;17,136.56,396.41,27.51,6.12;17,185.36,396.41,290.44,6.12;17,136.56,403.99,27.51,6.12;17,185.36,403.99,290.44,6.12;17,136.56,411.56,27.51,6.12;17,185.36,411.56,290.44,6.12;17,136.56,419.13,25.81,6.12;17,185.36,419.13,290.44,6.12;17,136.56,426.70,23.79,6.12;17,185.36,426.70,290.44,6.12;17,136.56,434.27,23.79,6.12;17,185.36,434.27,290.44,6.12;17,136.56,441.84,23.79,6.12;17,185.36,441.84,290.44,6.12;17,136.56,449.41,23.79,6.12;17,185.36,449.41,290.44,6.12;17,136.56,456.99,23.79,6.12;17,185.36,456.99,290.44,6.12;17,136.56,464.56,23.79,6.12;17,185.36,464.56,290.44,6.12;17,136.56,472.13,23.79,6.12;17,185.36,472.13,290.44,6.12;17,136.56,479.70,25.81,6.12;17,185.36,479.70,290.44,6.12;17,136.56,487.27,25.81,6.12;17,185.36,487.27,290.44,6.12;17,136.56,494.84,23.79,6.12;17,185.36,494.84,290.44,6.12;17,136.56,502.42,19.82,6.12;17,185.36,502.42,290.44,6.12;17,136.56,509.99,19.82,6.12;17,185.36,509.99,290.44,6.12;17,136.56,517.56,23.79,6.12;17,185.36,517.56,290.44,6.12;17,136.56,525.13,19.82,6.12;17,185.36,525.13,290.44,6.12;17,136.56,532.70,19.82,6.12;17,185.36,532.70,288.45,6.12;17,136.56,540.27,19.82,6.12;17,185.36,540.27,288.45,6.12;17,136.56,547.85,22.39,6.12;17,185.36,547.85,290.44,6.12;17,136.56,555.42,19.82,6.12;17,185.36,555.42,288.45,6.12;17,136.56,562.99,22.39,6.12;17,185.36,562.99,290.44,6.12;17,136.56,570.56,19.82,6.12;17,185.36,570.56,290.44,6.12;17,136.56,578.13,19.82,6.12;17,185.36,578.13,288.45,6.12;17,136.56,585.70,19.82,6.12;17,185.36,585.70,288.45,6.12;17,136.56,593.27,32.55,6.12;17,187.35,593.27,288.45,6.12;17,136.56,600.85,32.55,6.12;17,187.35,600.85,288.45,6.12;17,136.56,608.42,32.55,6.12;17,187.35,608.42,288.45,6.12;17,136.56,615.99,32.55,6.12;17,187.35,615.99,81.06,6.12" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="17,143.63,143.31,28.48,6.14;17,199.35,139.44,55.07,6.14;17,293.05,139.44,52.11,6.14;17,395.20,139.44,55.22,6.14;17,187.52,148.52,291.27,6.14">System MAP-samples MF1-samples MF1-concepts all ani. food 207 all ani. food 207 all ani. food 207 unseen</title>
		<idno>8 33.1 67.1 28.9 37.7 29.9 64.9 32.0 54.7 67.1 65.1 31.6 66.1 KDEVIR 8 36.5 33.3 67.1 27.7 37.5 30.2 64.5 31.3 54.8 68.0 64.8 32.1 66.4 MIL 3 36.9 30.9 68.6 23.3 27.5 20.6 53.1 18.0 34.7 34.7 50.4 16.9 36.7 MindLab 1 37.0 43.1 63.0 22.1 25.8 17.0 45.2</idno>
	</analytic>
	<monogr>
		<title level="j" coord="17,136.56,158.91,37.62,6.12">OPP-SIFT</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2 28</biblScope>
			<date type="published" when="1975">1 9.8 8.1 7.7 5.7 9</date>
		</imprint>
	</monogr>
	<note>.4 Random 8.8 12.4 14.3 4.8 3.5 5.2 6.9 1.8 2.6 3.2 6.6 0.8 3.0 KDEVIR 9 36</note>
</biblStruct>

<biblStruct coords="17,208.82,646.28,264.99,6.12;17,136.56,653.85,337.26,6.12;18,134.77,215.37,62.93,10.52" xml:id="b1">
	<analytic>
		<title/>
		<idno>.7 6.3 9.0 2.9 4</idno>
	</analytic>
	<monogr>
		<title level="j" coord="17,208.82,646.28,74.15,6.12">N/A N/A N/A 7</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">8</biblScope>
		</imprint>
	</monogr>
	<note>.7 KDEVIR 1 8.6 12.2 14.1 4.6 4.4 5.4 4.5 3.2 3.0 3.2 6.1 1.8 3.0 References</note>
</biblStruct>

<biblStruct coords="18,142.96,236.71,337.62,7.86;18,151.52,247.66,329.06,7.86;18,151.52,258.62,326.68,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="18,351.27,236.71,129.31,7.86;18,151.52,247.66,210.79,7.86">DISA at ImageCLEF 2014: The search-based solution for scalable image annotation</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Budikova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Botorek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Batko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zezula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,385.78,247.66,94.79,7.86;18,151.52,258.62,170.51,7.86">CLEF 2014 Evaluation Labs and Workshop, Online Working Notes</title>
		<meeting><address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">September 15-18 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.96,268.85,337.62,7.86;18,151.52,277.54,329.06,10.13;18,151.52,290.77,329.06,7.86;18,151.52,301.73,284.59,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="18,151.52,290.77,228.05,7.86">ImageCLEF 2014: Overview and analysis of the results</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Martinez-Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Patricia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Marvasti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Üsküdarlı</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cazorla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Garcia-Varea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Morell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,402.32,290.77,74.13,7.86">CLEF proceedings</title>
		<title level="s" coord="18,151.52,301.73,141.41,7.86">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.96,311.96,337.62,7.86;18,151.52,322.92,329.05,7.86;18,151.52,333.87,329.06,7.86;18,151.52,344.83,132.08,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="18,403.65,311.96,76.93,7.86;18,151.52,322.92,140.19,7.86">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2009.5206848</idno>
	</analytic>
	<monogr>
		<title level="m" coord="18,318.62,322.92,161.96,7.86;18,151.52,333.87,22.38,7.86;18,211.67,333.87,133.64,7.86">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009-06">2009. june 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference</note>
</biblStruct>

<biblStruct coords="18,142.96,355.06,337.62,7.86;18,151.52,366.02,150.15,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="18,234.70,355.06,170.43,7.86">WordNet An Electronic Lexical Database</title>
		<editor>Fellbaum, C.</editor>
		<imprint>
			<date type="published" when="1998-05">May 1998</date>
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge, MA; London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.96,376.25,337.62,7.86;18,151.52,387.21,329.06,7.86;18,151.52,398.17,329.05,7.86;18,151.52,409.13,22.01,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="18,463.04,376.25,17.53,7.86;18,151.52,387.21,235.35,7.86">MIL at ImageCLEF 2014: Scalable System for Image Annotation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kanehira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hidaka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Mukuta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tsuchiya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,406.87,387.21,73.71,7.86;18,151.52,398.17,195.71,7.86">CLEF 2014 Evaluation Labs and Workshop, Online Working Notes</title>
		<meeting><address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">September 15-18 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.96,419.35,337.62,7.86;18,151.52,430.31,329.05,7.86;18,151.52,441.27,329.05,7.86;18,151.52,452.23,116.85,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="18,295.96,419.35,184.62,7.86;18,151.52,430.31,181.87,7.86">Combining textual and visual cues for contentbased image retrieval on the World Wide Web</title>
		<author>
			<persName coords=""><forename type="first">La</forename><surname>Cascia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename></persName>
		</author>
		<idno type="DOI">10.1109/IVL.1998.694480</idno>
	</analytic>
	<monogr>
		<title level="m" coord="18,354.93,430.31,125.64,7.86;18,151.52,441.27,80.94,7.86;18,265.76,441.27,121.53,7.86">Content-Based Access of Image and Video Libraries</title>
		<imprint>
			<date type="published" when="1998">1998. 1998</date>
			<biblScope unit="page" from="24" to="28" />
		</imprint>
	</monogr>
	<note>Proceedings. IEEE Workshop</note>
</biblStruct>

<biblStruct coords="18,142.96,462.46,337.62,7.86;18,151.52,473.42,329.05,7.86;18,151.52,484.38,329.05,7.86;18,151.52,495.34,329.05,7.86;18,151.52,506.30,153.20,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="18,304.46,462.46,176.12,7.86;18,151.52,473.42,209.25,7.86">Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2006.68</idno>
	</analytic>
	<monogr>
		<title level="m" coord="18,383.05,473.42,97.53,7.86;18,151.52,484.38,329.05,7.86;18,262.90,495.34,40.20,7.86">Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
	<note>CVPR &apos;06</note>
</biblStruct>

<biblStruct coords="18,142.96,516.52,337.62,7.86;18,151.52,527.48,329.05,7.86;18,151.52,538.44,293.53,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="18,308.53,516.52,172.05,7.86;18,151.52,527.48,168.94,7.86">Renmin University of China at ImageCLEF 2014 Scalable Concept Image Annotation</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,343.94,527.48,136.63,7.86;18,151.52,538.44,133.88,7.86">CLEF 2014 Evaluation Labs and Workshop, Online Working Notes</title>
		<meeting><address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">September 15-18 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.96,548.67,337.62,7.86;18,151.52,559.63,329.06,7.86;18,151.52,570.59,279.97,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="18,311.07,548.67,169.50,7.86;18,151.52,559.63,125.49,7.86">The CLEF 2011 Photo Annotation and Concept-based Retrieval Tasks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liebetrau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,151.52,570.59,128.77,7.86">CLEF 2011 Labs and Workshop</title>
		<title level="s" coord="18,288.45,570.59,67.93,7.86">Notebook Papers</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Petras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</editor>
		<imprint>
			<date>September</date>
			<biblScope unit="page" from="19" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.61,591.78,337.96,7.86;18,151.52,602.73,329.06,7.86;18,151.52,613.69,119.03,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="18,249.62,591.78,230.95,7.86;18,151.52,602.73,112.89,7.86">Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1011139631724</idno>
	</analytic>
	<monogr>
		<title level="j" coord="18,272.23,602.73,92.79,7.86">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001-05">May 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,142.61,623.92,337.96,7.86;18,151.52,634.88,329.06,7.86;18,151.52,645.84,329.05,7.86;18,151.52,656.80,95.77,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="18,302.56,623.92,178.02,7.86;18,151.52,634.88,309.81,7.86">KDEVIR at ImageCLEF 2014 Scalable Concept Image Annotation Task: Ontology based Automatic Image Annotation</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">A</forename><surname>Reshma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">Z</forename><surname>Ullah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Aono</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,151.52,645.84,268.54,7.86">CLEF 2014 Evaluation Labs and Workshop, Online Working Notes</title>
		<meeting><address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">September 15-18 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.61,119.67,337.96,7.86;19,151.52,130.63,329.06,7.86;19,151.52,141.59,329.06,7.86;19,151.52,152.55,121.36,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="19,195.46,119.67,285.12,7.86;19,151.52,130.63,323.72,7.86">CNRS -TELECOM ParisTech at ImageCLEF 2013 Scalable Concept Image Annotation Task: Winning Annotations with Context Dependent SVMs</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sahbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,165.75,141.59,271.01,7.86">CLEF 2013 Evaluation Labs and Workshop, Online Working Notes</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">September 23-26 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.61,163.51,337.96,7.86;19,151.52,174.47,329.05,7.86;19,151.52,185.43,257.12,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="19,332.91,163.51,147.67,7.86;19,151.52,174.47,107.29,7.86">Evaluating Color Descriptors for Object and Scene Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2009.154</idno>
	</analytic>
	<monogr>
		<title level="j" coord="19,266.08,174.47,214.49,7.86;19,151.52,185.43,45.57,7.86">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1582" to="1596" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.61,196.39,337.96,7.86;19,151.52,207.35,329.06,7.86;19,151.52,218.30,180.80,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="19,300.27,196.39,180.31,7.86;19,151.52,207.35,70.43,7.86">IPL at ImageCLEF 2014: Scalable Concept Image Annotation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Stathopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kalamboukis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,241.64,207.35,238.94,7.86;19,151.52,218.30,21.16,7.86">CLEF 2014 Evaluation Labs and Workshop, Online Working Notes</title>
		<meeting><address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">September 15-18 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.61,229.26,337.96,7.86;19,151.52,240.22,310.26,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="19,259.08,229.26,221.50,7.86;19,151.52,240.22,101.61,7.86">Overview of the ImageCLEF 2012 Flickr Photo Annotation and Retrieval Task</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,274.59,240.22,103.78,7.86">CLEF 2012 working notes</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.61,251.18,337.96,7.86;19,151.52,262.14,329.06,7.86;19,151.52,273.10,329.06,7.86;19,151.52,284.06,117.87,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="19,331.72,251.18,148.86,7.86;19,151.52,262.14,250.23,7.86">80 Million Tiny Images: A Large Data Set for Nonparametric Object and Scene Recognition</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2008.128</idno>
	</analytic>
	<monogr>
		<title level="j" coord="19,411.01,262.14,69.57,7.86;19,151.52,273.10,200.38,7.86">Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1958" to="1970" />
			<date type="published" when="2008-11">nov 2008</date>
		</imprint>
	</monogr>
	<note>IEEE Transactions on</note>
</biblStruct>

<biblStruct coords="19,142.61,295.02,337.96,7.86;19,151.52,305.98,329.05,7.86;19,151.52,316.94,329.05,7.86;19,151.52,327.89,95.77,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="19,176.98,305.98,283.33,7.86">MindLab at ImageCLEF 2014: Scalable Concept Image Annotation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Vanegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Arevalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Otálora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Páez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Pérez-Rubiano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">A</forename><surname>González</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,151.52,316.94,268.54,7.86">CLEF 2014 Evaluation Labs and Workshop, Online Working Notes</title>
		<meeting><address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">September 15-18 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.61,338.85,337.96,7.86;19,151.52,349.81,329.05,7.86;19,151.52,360.77,329.05,7.86;19,151.52,371.73,166.01,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="19,259.48,338.85,221.10,7.86;19,151.52,349.81,53.91,7.86">Image-Text Dataset Generation for Image Annotation and Retrieval</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,355.90,349.81,124.67,7.86;19,151.52,360.77,145.23,7.86">II Congreso Español de Recuperación de Información, CERI 2012</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Berlanga</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Rosso</surname></persName>
		</editor>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">June 18-19 2012</date>
			<biblScope unit="page" from="115" to="120" />
		</imprint>
		<respStmt>
			<orgName>Universidad Politécnica de Valencia</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.61,382.69,337.96,7.86;19,151.52,393.65,329.05,7.86;19,151.52,404.61,329.06,7.86;19,151.52,415.57,329.06,8.11;19,151.52,427.17,108.76,7.47" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="19,261.42,382.69,219.15,7.86;19,151.52,393.65,86.91,7.86">Overview of the ImageCLEF 2012 Scalable Web Image Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<ptr target="http://mvillegas.info/pub/Villegas12_CLEF_Annotation-Overview.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="19,151.52,404.61,293.10,7.86">CLEF 2012 Evaluation Labs and Workshop, Online Working Notes</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Karlgren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Womser-Hacker</surname></persName>
		</editor>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">September 17-20 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.61,437.48,337.96,7.86;19,151.52,448.44,329.06,7.86;19,151.52,459.40,329.07,8.11;19,151.52,471.00,282.92,7.47" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="19,315.56,437.48,165.01,7.86;19,151.52,448.44,167.35,7.86">Overview of the ImageCLEF 2013 Scalable Concept Image Annotation Subtask</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<ptr target="http://mvillegas.info/pub/Villegas13_CLEF_Annotation-Overview.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="19,342.83,448.44,137.75,7.86;19,151.52,459.40,133.19,7.86">CLEF 2013 Evaluation Labs and Workshop, Online Working Notes</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">September 23-26 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.61,481.32,337.96,7.86;19,151.52,492.28,329.05,7.86;19,151.52,503.24,329.06,7.86;19,151.52,514.20,132.08,7.86" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="19,378.65,481.32,101.93,7.86;19,151.52,492.28,173.32,7.86">ARISTA -image search to annotation on billions of web photos</title>
		<author>
			<persName coords=""><forename type="first">X</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2010.5540046</idno>
	</analytic>
	<monogr>
		<title level="m" coord="19,351.72,492.28,128.85,7.86;19,151.52,503.24,84.04,7.86">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010-06">2010. June 2010</date>
			<biblScope unit="page" from="2987" to="2994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.61,525.15,337.96,7.86;19,151.52,536.11,329.06,7.86;19,151.52,547.07,122.41,7.86" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="19,306.46,525.15,174.12,7.86;19,151.52,536.11,166.63,7.86">Large scale image annotation: learning to rank with joint word-image embeddings</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10994-010-5198-3</idno>
	</analytic>
	<monogr>
		<title level="j" coord="19,327.81,536.11,74.39,7.86">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="21" to="35" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,142.61,558.03,337.96,7.86;19,151.52,568.99,329.06,7.86;19,151.52,579.95,271.75,7.86" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="19,331.72,558.03,148.86,7.86;19,151.52,568.99,149.40,7.86">MLIA at ImageCLFE 2014 Scalable Concept Image Annotation Challenge</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Shimada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ichiro Taniguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,321.59,568.99,158.99,7.86;19,151.52,579.95,112.11,7.86">CLEF 2014 Evaluation Labs and Workshop, Online Working Notes</title>
		<meeting><address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">September 15-18 2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
