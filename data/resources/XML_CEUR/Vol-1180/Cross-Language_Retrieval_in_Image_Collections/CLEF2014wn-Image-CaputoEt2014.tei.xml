<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,157.19,115.96,300.97,12.62;1,248.74,133.89,117.87,12.62">Overview of the ImageCLEF 2014 Domain Adaptation Task</title>
				<funder>
					<orgName type="full">Swiss National Science Foundation project Situated Vision (SIVI)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,227.62,171.56,70.76,8.74"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Rome La Sapienza</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,325.54,171.56,57.73,8.74"><forename type="first">Novi</forename><surname>Patricia</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Idiap Research Institute</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,157.19,115.96,300.97,12.62;1,248.74,133.89,117.87,12.62">Overview of the ImageCLEF 2014 Domain Adaptation Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8681138A9842371DB5AE264DFCD18788</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the first edition of the Domain Adaptation Task at ImageCLEF 2014. Domain adaptation refers to the challenge of leveraging over knowledge acquired when learning to recognize given classes on a database, when using a different data collection. We describe the scientific motivations behind the task, the research challenge on which the 2014 edition focused, the data and evaluation metric and results obtained by participants. After a discussion on the lesson learned during this first edition, we conclude with possible ideas for future editions of the task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction and Motivation</head><p>The amount of freely available and annotated image collections is dramatically increased over the last years, thanks to the diffusion of high-quality cameras, and also to the introduction of new and cheap annotation tools such as Mechanical Turk <ref type="bibr" coords="1,158.31,411.77,9.96,8.74" target="#b2">[3]</ref>. Attempts to leverage over and across such large data sources has proved challenging. Indeed, tools like Google GoggleS<ref type="foot" coords="1,341.61,422.15,3.97,6.12" target="#foot_0">3</ref> are able to recognize reliably limited classes of objects, like books or wine labels, but are not able to generalize across generic objects like food items, clothing items and so on. Several authors showed that, for a given task, training on a dataset (e.g. Pascal VOC 07) and testing on another (e.g. ImageNet) produces very poor results, although the set of depicted object categories is the same <ref type="bibr" coords="1,319.46,483.50,15.84,8.74" target="#b9">[10,</ref><ref type="bibr" coords="1,335.30,483.50,11.88,8.74" target="#b12">13,</ref><ref type="bibr" coords="1,347.18,483.50,7.92,8.74" target="#b5">6,</ref><ref type="bibr" coords="1,355.10,483.50,11.88,8.74" target="#b11">12]</ref>. In other words, existing object categorization methods do not generalize well across databases.</p><p>This problem is known in the literature as the domain adaptation challenge, as known in machine learning for speech and language processing <ref type="bibr" coords="1,419.07,519.36,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="1,429.59,519.36,7.01,8.74" target="#b4">5]</ref>. A source domain (S) usually contains a large amount of labeled images, while a target domain (T ) refers broadly to a dataset that is assumed to have different characteristics from the source, and few or no labeled samples. Formally, two domains differ when their probability distributions differ: P S (x, y) = P T (x, y), where x ∈ X indicates the generic image sample and y ∈ Y the corresponding class label. Within this context, the across dataset generalization problem stems from an intrinsic difference between the underlying distributions of the data.</p><p>Addressing this issue would have a tremendous impact on the generality and adaptability of any vision-based annotation system. Current research in domain adaptation focuses on a scenario where -(a) the prior domain (source) consists of one or maximum two databases; -(b) the labels between the source and the target domain are the same, and -(c) the number of annotated training data for the target domain are limited.</p><p>The goal of the Domain Adaptation Task, initiated in 2014 under the Image-CLEF umbrella <ref type="bibr" coords="2,205.27,177.95,9.96,8.74" target="#b3">[4]</ref>, is to push the state of the art in domain adaptation towards more realistic settings, relaxing these assumptions. Our ambition is to provide, over the years, stimulating problems and challenging data collections that might stimulate and support novel research in the field.</p><p>In the rest of the paper we describe the 2014 Domain Adaptation Task (section 2.1), the data and features provided to the participants (section 2.2), and the evaluation metric adopted (section 2.3). Section 3 describes the results obtained while section 4 provides an in depth discussion of the results obtained and identifies possible new directions for the 2015 edition of the task. Conclusions are given in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The 2014 Domain Adaptation Task</head><p>In this section we describe the Domain Adaptation Task proposed in the Image-CLEF 2014 lab. We first outline the research challenge we aimed at addressing (section 2.1). Then, we describe the data collection used and the features provided to all participants (section 2.2) and we describe the evaluation metric used (section 2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Research Challenge</head><p>In the 2014 version (first edition) of the Domain Adaptation Task, we focused on the number of sources available to the system. Current experimental settings, widely used in the community, consider typically one source and one target <ref type="bibr" coords="2,462.34,476.16,14.61,8.74" target="#b9">[10]</ref>, or at most two sources and one target <ref type="bibr" coords="2,307.54,488.11,11.15,8.74" target="#b5">[6,</ref><ref type="bibr" coords="2,318.69,488.11,11.15,8.74" target="#b10">11]</ref>. This scenario is unrealistic: with the wide abundance of annotated resources and data collections that are made available to users, and with the fast progress that is being made in the image annotation community, it is likely that systems will be able to access more and more databases, and therefore to leverage over a much larger number of sources than two, as considered in the most challenging settings today.</p><p>To push research towards more realistic scenarios, the 2014 edition of the Domain Adaptation Task has proposed an experimental setup with four sources, where such sources were built by exploiting existing available resources like the ImageNet, Caltetch256 <ref type="bibr" coords="2,236.54,596.34,10.52,8.74" target="#b6">[7]</ref> databases and so on. Participants were thus requested to build recognition systems for the target classes by leveraging over such source knowledge. We considered a semi-supervised setting, i.e. a setting where the target data, for each class, is limited but annotated. In the next section we describe in details the data used for the sources, the classes contained both in the source and the target, and the target data provided to participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data and Features</head><p>Source and Target Data To define the source and target data, we considered five publicly available databases:</p><p>the Caltech-256 database, consisting of 256 object categories, with a total of 30.607 images; the ImageNet ILSVRC2012 database, organized according to the WordNet hierarchy, with an average of 500 images per node; the PASCAL VOC2012 database, an image data set for object class recognition with 20 object classes; the Bing database, containing all 256 categories from the Caltech-256 one, and augmented with 300 web images per category that were collected through textual search using Bing; and the SUN database, a scene understanding database that contains 899 categories and 130.519 images.</p><p>We then selected twelve classes, common to all the dataset listed above: aereoplane, bike, bird, boat, bottle, bus, car, dog, horse, monitor, motorbike, and people. Figure <ref type="figure" coords="3,221.58,339.58,4.98,8.74" target="#fig_0">1</ref> illustrates the images contained for each class in each of the considered datasets. As sources, we considered 50 images representing the classes listed above from the databases Caltech-256, ImageNet, PASCAL and Bing. The 50 images were randomly selected from all those contained in each of the data collection, for a total of 600 images for each source. As target, we used images taken from the SUN database for each class. We randomly selected 5 images per class for training, and 50 images per class for testing. These data were given to all participants as validation set. The test set consisted of 50 images for each class, for a total of 600, manually collected by us using the class names as textual queries with standard search engines.</p><p>Features Instead of making available directly the images to participants, we decided to release pre-computed features only, in order to keep the focus on the learning aspects of the algorithms in this year's competition. Thus, we represented every image with dense SIFT descriptors (PHOW features) at points on a regular grid with spacing 128 pixels <ref type="bibr" coords="3,316.03,507.29,9.96,8.74" target="#b1">[2]</ref>. At each grid point the descriptors were computed over four patches with different radii, hence each point was represented by four SIFT descriptors. The dense features have been vector quantized into 256 visual words using k-means clustering on a randomly chosen subset of the Caltech-256 database. Finally, all images were converted to 2×2 spatial histograms over the 256 visual words, resulted in 1024 feature dimension. The software used for computing such features is available at www.vlfeat.org.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Evaluation Metrics</head><p>We asked participants to provide the class name for each of the 600 test images released. Results were compared with the ground truth, and a score was assigned as follows: -For each correctly classified image will receive 1 point; -For each misclassified image will receive 0 point.</p><p>We provided to all participants, together with the validation data, a matlab script for evaluating the performance of their algorithms before the official submission, i.e. on the validation data. The script had been tested under Matlab (ver 8.1.0.64) and Octave (ver 3.6.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>While 19 groups registered to the domain adaptation task to receive access to the training and validation data, only 3 groups eventually submitted runs: the XRCE group, the Hubert Curien Lab group and the Idiap group (organizers). They submitted the following algorithms:</p><p>the XRCE group submitted a set of methods based on several heterogeneous methods for domain adaptation, whose predictions were subsequently fused. By combining the output of instance based approaches and metric learning one with a brute force SVM prediction, they obtained a set of heterogeneous classifiers all producing class prediction for the target domain instances. These were combined through different versions of majority voting in order to improve the overall accuracy.</p><p>-The Hubert Curien Lab group did not submit any working notes, neither sent any detail about their algorithm. We are therefore not able to describe it. -The Idiap group submitted a baseline run using a recently introduced learning to learn algorithm <ref type="bibr" coords="5,253.49,167.37,9.96,8.74" target="#b8">[9]</ref>. The approach considers source classifiers as experts, and it combines their confidence output with a high-level cue integration scheme, as opposed to a mid-level one as proposed in <ref type="bibr" coords="5,402.35,191.28,9.96,8.74" target="#b7">[8]</ref>. The algorithm is called High-level Learning to Learn (H-L2L). As our goal was not to obtain the best possible performance but rather to provide an off the shelf baseline against which to compare results of the other participants, we did not perform any parameter tuning.</p><p>Table <ref type="table" coords="5,177.36,260.68,4.98,8.74" target="#tab_0">1</ref> reports the final ranking among groups. Table <ref type="table" coords="5,391.87,260.68,4.98,8.74" target="#tab_1">2</ref> reports the results obtained by the best run submitted by each group, for each of the 12 target classes. We see that XRCE obtained the best score, followed by the Hubert Curien lab. The Idiap baseline obtained the worst score, clearly pointing towards the importance of parameter selection in these kind of benchmark evaluations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis and Discussion</head><p>The clear success of the XRCE group, obtained by combining several domain adaptation methods presented in the literature, seems to indicate that current methods are not able to address effectively the problem of leveraging over multiple sources. Ensemble methods, chosen by at least two teams, appear instead to be a viable option in this setting, whether used to combine the output of various domain adaptation algorithms, whether used to combine several source output confidences.</p><p>The choice made to provide to participants only the features computed from each image, and not the images itself, forced groups to focus on the learning aspects of the problems, but perhaps did not allow for enough flexibility in attacking the problem. We don't plan to repeat this choice in the future editions of the task.</p><p>A last remark should be made on the scarce participation to the task. Even though only three eventually submitted runs, 19 groups expressed interest and registered, in order to access the training and validation data. We believe that this is an indicator of enough interest to push us to organize again the task next year, also collecting feedbacks from the participating and registered groups in order to identify possible problems in the current edition and to offer a more engaging edition of the task in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>The first edition of the Domain Adaptation Task, organized under the Image-CLEF umbrella, focused on the problem of building a classifier in a target domain while leveraging over four different sources. nineteen groups registered for the task, and eventually three groups submitted runs, with the XRCE winning the competition with an ensemble learning based method. For the 2015 edition of the task, we plan to make available to participants the raw images, as opposed to pre-computed features as done in 2014, so to allow for a wider generality of approaches. We will continue to propose data supporting the problem of leveraging from multiple sources, possibly by augmenting the number of classes (which was 12 in the 2014 edition), and/or allowing for a partial overlap of classes between sources and between sources and target, as proposed in <ref type="bibr" coords="6,410.69,545.44,14.61,8.74" target="#b11">[12]</ref>. In order to significantly increase the number of participants to the task next year, we will contact all groups that registered to the task and ask their preferences among these different options.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,141.26,375.23,332.82,7.89;4,134.77,115.83,325.08,244.62"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Exemplar images for the 12 classes from the five selected public databases.</figDesc><graphic coords="4,134.77,115.83,325.08,244.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,137.06,341.10,341.23,62.73"><head>Table 1 .</head><label>1</label><figDesc>Ranking and best score obtained by the three groups that submitted runs.</figDesc><table coords="5,223.44,361.87,168.47,41.96"><row><cell>Rank</cell><cell>Group</cell><cell>Score</cell></row><row><cell>1</cell><cell>XRCE</cell><cell>228</cell></row><row><cell>2</cell><cell cols="2">Hubert Curien Lab Group 158</cell></row><row><cell>3</cell><cell>Idiap</cell><cell>45</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,143.70,462.83,327.94,164.95"><head>Table 2 .</head><label>2</label><figDesc>Class by Class score obtained by the three groups that submitted runs.</figDesc><table coords="5,172.46,483.60,270.42,144.18"><row><cell>class</cell><cell cols="3">Score XRCE Score Hubert Curien Score Idiap</cell></row><row><cell>aereoplane</cell><cell>41</cell><cell>36</cell><cell>3</cell></row><row><cell>bike</cell><cell>12</cell><cell>7</cell><cell>1</cell></row><row><cell>bird</cell><cell>15</cell><cell>15</cell><cell>0</cell></row><row><cell>boat</cell><cell>18</cell><cell>5</cell><cell>4</cell></row><row><cell>bottle</cell><cell>20</cell><cell>25</cell><cell>3</cell></row><row><cell>bus</cell><cell>23</cell><cell>10</cell><cell>6</cell></row><row><cell>car</cell><cell>17</cell><cell>13</cell><cell>7</cell></row><row><cell>dog</cell><cell>8</cell><cell>8</cell><cell>3</cell></row><row><cell>horse</cell><cell>17</cell><cell>6</cell><cell>2</cell></row><row><cell>monitor</cell><cell>28</cell><cell>15</cell><cell>3</cell></row><row><cell>motorbike</cell><cell>12</cell><cell>7</cell><cell>3</cell></row><row><cell>people</cell><cell>17</cell><cell>11</cell><cell>10</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="1,144.73,656.80,160.29,7.86"><p>http://www.google.com/mobile/goggles</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledments</head><p>This work was partially supported by the <rs type="funder">Swiss National Science Foundation project Situated Vision (SIVI)</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,142.96,142.59,337.62,7.86;7,151.52,153.55,329.06,7.86;7,151.52,164.51,115.05,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,310.68,142.59,169.90,7.86;7,151.52,153.55,76.32,7.86">Domain adaptation with structural correspondence learning</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,250.90,153.55,229.68,7.86;7,151.52,164.51,86.38,7.86">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,175.47,337.62,7.86;7,151.52,186.42,93.83,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,276.53,175.47,200.40,7.86">Image classification using random forests and ferns</title>
		<author>
			<persName coords=""><forename type="first">Anna</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ad Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,165.60,186.42,51.08,7.86">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,197.38,337.62,7.86;7,151.52,208.34,329.06,7.86;7,151.52,219.30,42.49,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,323.08,197.38,157.49,7.86;7,151.52,208.34,152.85,7.86">Amazon&apos;s mechanical turk a new source of inexpensive, yet high-quality, data?</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Buhrmester</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kwang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">D</forename><surname>Gosling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,307.38,208.34,151.25,7.86">Perspectives on Psychological Science</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="5" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,230.26,337.62,7.86;7,151.52,238.95,329.06,10.13;7,151.52,252.18,329.06,7.86;7,151.52,263.14,284.59,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,151.52,252.18,228.05,7.86">ImageCLEF 2014: Overview and analysis of the results</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Martinez-Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Patricia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Marvasti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Üsküdarlı</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cazorla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Garcia-Varea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Morell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,402.32,252.18,74.13,7.86">CLEF proceedings</title>
		<title level="s" coord="7,151.52,263.14,141.41,7.86">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,274.10,337.62,7.86;7,151.52,285.05,182.89,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,213.93,274.10,150.27,7.86">Frustratingly easy domain adaptation</title>
		<author>
			<persName coords=""><forename type="first">Iii</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,385.26,274.10,95.32,7.86;7,151.52,285.05,154.23,7.86">Association for Computational Linguistics Conference (ACL)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,296.01,337.62,7.86;7,151.52,306.97,329.05,7.86;7,151.52,317.93,33.30,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,326.20,296.01,154.38,7.86;7,151.52,306.97,74.90,7.86">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,248.25,306.97,232.33,7.86;7,151.52,317.93,33.30,7.86">Proc. CVPR. Extended version considering its additional material</title>
		<meeting>CVPR. Extended version considering its additional material</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,328.89,337.62,7.86;7,151.52,339.85,244.27,7.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="7,289.14,328.89,141.93,7.86">Caltech 256 object category dataset</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>UCB/CSD-04-1366</idno>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>California Institue of Technology</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct coords="7,142.96,350.81,337.62,7.86;7,151.52,361.77,119.52,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,289.29,350.81,191.29,7.86;7,151.52,361.77,22.02,7.86">Multiclass transfer learning from unconstrained priors</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,194.36,361.77,48.01,7.86">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,372.73,337.62,7.86;7,151.52,383.68,217.80,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,251.73,372.73,228.85,7.86;7,151.52,383.68,116.93,7.86">Learning to learn, from transfer learning to domain adaptation: a unifying perspective</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Patricia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,289.57,383.68,51.08,7.86">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,394.64,337.96,7.86;7,151.52,405.60,151.61,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,335.39,394.64,145.18,7.86;7,151.52,405.60,50.34,7.86">Adapting visual category models to new domains</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,223.51,405.60,50.96,7.86">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,416.56,337.96,7.86;7,151.52,427.52,52.21,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,260.76,416.56,176.06,7.86">Frustratingly easy nbnn domain adaptation</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,459.18,416.56,21.39,7.86;7,151.52,427.52,23.55,7.86">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,438.48,337.97,7.86;7,151.52,449.44,291.67,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,393.40,438.48,87.18,7.86;7,151.52,449.44,190.77,7.86">Beyond dataset bias: Multi-task unaligned shared knowledge transfer</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Quadrianto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,363.18,449.44,51.35,7.86">Proc. ACCV</title>
		<meeting>ACCV</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,460.40,334.28,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,257.98,460.40,118.31,7.86">Unbiased look at dataset bias</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,397.13,460.40,51.08,7.86">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
