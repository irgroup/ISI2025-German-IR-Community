<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,137.88,115.96,339.58,12.62;1,291.37,133.89,32.61,12.62">Overview of the ImageCLEF 2014 Robot Vision Task</title>
				<funder ref="#_TUPjpNm">
					<orgName type="full">Spanish Government (MICINN)</orgName>
				</funder>
				<funder ref="#_bXn6Esk #_JR3qseG">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder>
					<orgName type="full">FEDER</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,154.04,171.73,98.59,8.74"><forename type="first">Jesus</forename><surname>Martinez-Gomez</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Castilla-La Mancha</orgName>
								<address>
									<settlement>Albacete</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Malaga</orgName>
								<address>
									<settlement>Malaga</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,269.52,171.73,88.47,8.74"><forename type="first">Ismael</forename><surname>García-Varea</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Castilla-La Mancha</orgName>
								<address>
									<settlement>Albacete</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,368.54,171.73,66.18,8.74"><forename type="first">Miguel</forename><surname>Cazorla</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Alicante</orgName>
								<address>
									<settlement>Alicante</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,267.48,183.69,63.96,8.74"><forename type="first">Vicente</forename><surname>Morell</surname></persName>
							<email>vicente.morell@ua.es</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Alicante</orgName>
								<address>
									<settlement>Alicante</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,137.88,115.96,339.58,12.62;1,291.37,133.89,32.61,12.62">Overview of the ImageCLEF 2014 Robot Vision Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2084F65DDE2B8453CC27EAF5D6ED2B56</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article describes the RobotVision@ImageCLEF 2014 challenge, which addresses two problems: place classification and object recognition. Participants of the challenge were asked to classify rooms on the basis of visual and depth images captured by a Kinect sensor mounted on a mobile robot. They were also asked to detect the appearance or lack of several objects. The proposals of the participants had to answer two questions: "where are you?" (I am in the corridor, in the kitchen, etc.), and "list the objects that you can see?", from a predefined list (I can see a table and a chair but not a computer) when presented with a test frame (a visual and a depth image). The number of times a specific object appears in a frame is not relevant. Two different sequences of frames were provided for training and validation purposes, respectively. The final test sequence included images acquired in a similar but different indoor office environment, which is considered the main novelty of the 2014 edition of the task. In contrast to previous editions of the task, sequences do not represent the temporal continuity in the acquisition procedure and therefore, test frames have to be processed sparsely. The winner of the 2014 edition of the Robot Vision task was the NUDT group, from China.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper describes the ImageCLEF 2014 Robot Vision challenge <ref type="bibr" coords="1,433.57,547.14,9.96,8.74" target="#b8">[9]</ref>, a competition that started in 2009 within the ImageCLEF 4 <ref type="bibr" coords="1,376.97,559.09,10.52,8.74" target="#b0">[1]</ref> as part of the Cross Language Evaluation Forum (CLEF) Initiative 5 . Since its origin, the Robot Vision task has been addressing the problem of place classification for mobile robot localization.</p><p>The 2009@ImageCLEF edition of the task <ref type="bibr" coords="2,337.18,118.99,14.61,8.74" target="#b11">[12]</ref>, with 7 participating groups, defined some details that have been maintained for all the following editions. Participants were given training data consisting of sequences of frames recorded in indoor environments. These training frames were labelled with the name of the rooms they were acquired from. The task consisted in building a system capable to classify test frames using as class the name of the rooms previously seen. Moreover, the system could refrain from making a decision in the case of lack of confidence. Two different subtasks were then proposed: obligatory and optional. The difference between both subtasks was that the temporal continuity of the test sequence could only be exploited in the optional task. The score for each participant submission was computed as the sum of the frames that were correctly labelled minus a penalty that was applied to the frames that were misclassified. No penalties were applied for frames not classified.</p><p>In 2010, two editions of the challenge took place. The second edition of the task, 2010@ICPR <ref type="bibr" coords="2,215.91,289.14,15.50,8.74" target="#b9">[10]</ref> was held in conjunction with ICPR 2010 conference. In that edition, where 9 groups participated, the use of stereo images and two types of different training sequences (easy and hard), that had to be used separately, were introduced. The 2010@ImageCLEF edition <ref type="bibr" coords="2,374.24,325.01,14.61,8.74" target="#b10">[11]</ref>, with 7 participating groups, was focused on generalization: several areas could belong to the same semantic category.</p><p>In 2012 <ref type="bibr" coords="2,185.42,363.65,9.96,8.74" target="#b4">[5]</ref>, stereo images were replaced by images acquired using two types of camera: a perspective camera for visual images and a depth camera (the Microsoft Kinect sensor) for range images. Therefore, each frame consisted of two types of images and the challenge become a problem of multimodal (place) classification. In addition to the use of depth images (using a visual representation), the optional task contained kidnappings and unknown rooms (not previously seen in training sequences) not appeared in the test sequences. Moreover, several techniques for features extraction and cue integration were proposed to the participants.</p><p>In 2013 <ref type="bibr" coords="2,188.52,474.02,9.96,8.74" target="#b5">[6]</ref>, the visual data was changed, providing the traditional RGB images and their corresponding point cloud information. The main difference from 2012 edition was that no depth image was provided but the point cloud itself. The purpose of that was to encourage the participants to make use of 3D image processing techniques, in addition to visual ones, with the aim to obtain better classification results. Furthermore, for some specific rooms, we provided completely dark images for which the use of the 3D information had to be used in order to classify such a room. In addition to the use of the point cloud representation, the 2013 edition of the task included object recognition.</p><p>For the ImageCLEF 2014 Robot Vision challenge, we have introduced two main changes. Firstly, the temporal continuity from the image acquisition has been completely removed in the training, validation and test sequences. That is, consecutive frames in the provided sequences do not represent consecutive frames during the acquisition procedure. The second change is the inclusion of validation and test frames acquired in a different environment. Namely, we acquired new frames in a different building that contains the same type of rooms and objects imaged in the training and part of the validation sequence. Regarding the participation, in this edition, we received a total of 17 runs, from 4 different participant groups. The best result was obtained by the NUDT research group from the National University of Defense Technology, Changsha, China.</p><p>The rest of the paper details the challenge and is organized as follows: Section 2 describes the 2014 ImageCLEF edition of the RobotVision task. Section 3 presents all the participants groups, while the results are reported in Section 4. Finally, in Section 5, the main conclusions are drawn and some ideas for future editions are outlined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The RobotVision Task</head><p>This section describes the details concerning the setup of the ImageCLEF 2014 Robot Vision task. In Section 2.1 a description of training, validation and test sequences is provided. In Section 2.2 the performance evaluation criteria is detailed. Finally, in Section 2.3 a brief description of the baseline visual place classification system provided by the organizers, as well as other relevant details concerning the task are presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Description</head><p>The sixth edition of the Robot Vision task is focused on the use of multimodal information (visual and depth images) with application to semantic localization and object recognition. The main objective of this edition is to address the problem of robot localization in parallel to object recognition from a semantic point of view, with a special focus on generalization. Both problems are inherently related: the objects present in a scene can help to determine the room category and vice versa. Solutions presented should be as general as possible while specific proposals are not desired. In addition to the use of visual data, a 3D point cloud representation of the scene acquired from a Microsoft Kinect device was used, which has shown as a de facto standard in the use of depth images. In this new edition of the task, we introduced strong variations between training and test scenarios with the aim to solve the object recognition and localization problems in parallel and for a great variety of different scenarios.</p><p>Participants were given visual images and depth images in Point Cloud Data (PCD) format. Fig. <ref type="figure" coords="3,222.50,582.78,4.98,8.74" target="#fig_1">2</ref> shows the same scene represented in a visual image and a point cloud data file. Training, validation and test sequence were acquired within two different buildings presenting a similar structure but with some variations in the objects distribution. All the room and object categories included in the test sequence were previously seen during training and validation.</p><p>As for the 2013 edition of the challenge, no sub-tasks were defined and all participants have to prepare their submissions using the same test sequence.   It can be observed that in all sequences, Corridor is the class with higher number of frames. This is because most of the space of the University of Alicante building, suitable for robot navigation, belongs to several corridors. This situation makes it easier the classification of test frames as Corridor while other classes as VisioConference or ElevatorArea are more challenging. The frequency distribution for rooms in the different sequences is graphically presented in Fig. <ref type="figure" coords="5,472.85,420.45,3.87,8.74" target="#fig_2">3</ref>.  The distribution for object categories in the training, validation and test sequences is depicted in Table <ref type="table" coords="5,263.78,608.30,3.87,8.74" target="#tab_3">2</ref>, while frequencies are presented in Fig. <ref type="figure" coords="5,437.98,608.30,3.87,8.74" target="#fig_3">4</ref>. Despite of small variations, it can be observed how classes and objects frequencies are maintained along training, validation and testing sequences.</p><p>The differences between all the room categories can be observed in Fig. <ref type="figure" coords="5,472.86,644.16,3.87,8.74">5</ref>, where a single visual image for each of the 10 room categories is shown. Moreover,  Fig. <ref type="figure" coords="6,155.65,443.84,4.98,8.74">6</ref> shows four examples of visual images for each of the 8 different objects in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Performance Evaluation</head><p>The runs submitted for each participant were compared and sorted according the score assigned to each submission. Every submission consisted of the room category assigned to each test image and the corresponding list of the 8 detected/nondetected objects within that image. As we already mentioned above, the number of times a specific object appears in an image was not relevant to compute the score. The score was computed using the rules shown in Table <ref type="table" coords="6,415.41,572.43,3.87,8.74">3</ref>. For a better understanding of the score computation, an example of three different hypothetical user decisions for a specific test image is shown in Table <ref type="table" coords="6,421.94,596.34,3.87,8.74">4</ref>. Due to the fact that wrong room classifications account negatively to the score, participants were allowed to not providing such information, in which case the score is not affected. The final score was computed as the sum of the score obtained for each individual test frame. According to the test set released the maximum score to be obtained was 7004 points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Participation</head><p>In 2014, 28 participants registered to the Robot Vision task but only 4 submitted, at least, one run accounting for a total of 17 different runs. These participants were:</p><p>-NUDT: National University of Defense Technology, Changsha, China.</p><p>-UFMS CPPP: Federal University of Mato Grosso do Sul, Ponta Pora, Brazil -AEGEAN: University of the Aegean Karlovassi, Greece -SIMD: University of Castilla-La Mancha, Albacete, Spain.</p><p>• Out of competition organizers contribution using the techniques included in the MATLAB proposed script. It can be considered as a baseline result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>This section presents the results of the Robot Vision task of ImageCLEF 2014.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overall Results</head><p>The scores obtained by all the submitted runs are shown in Table <ref type="table" coords="9,426.90,367.42,3.87,8.74" target="#tab_4">5</ref>. The maximum score that could be achieved was 7004 and the winner (NUDT) obtained a score of 4430,25 points. This maximum score is the addition of the maximum score computed from rooms classification (3000) and object recognition (4004). SIMD organizers submission was out-of-competition submission, and it was provided to be considered a baseline score. For this submission, just the techniques proposed in the webpage of the challenge<ref type="foot" coords="10,345.82,141.33,3.97,6.12" target="#foot_0">6</ref> were used. Concretely, it was generated an image descriptor by concatenating both depth and visual histograms. These descriptors were then used as input to train an Online Support Vector Machine <ref type="bibr" coords="10,206.20,178.77,10.52,8.74" target="#b2">[3]</ref> using DOGMA <ref type="bibr" coords="10,289.20,178.77,9.96,8.74" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Details and participants approaches</head><p>A detailed view of the obtained results for the best submissions of the 4 participants is show in Fig. <ref type="figure" coords="10,248.76,244.26,3.87,8.74" target="#fig_4">7</ref>. This figure graphically presents how participants submission performed notoriously better for object recognition than for room classification. For example, the winner of the task achieved 83,78% of the maximum score (3354.75 out of 4004 points) for the object recognition problem, while they just obtained 1075,5 out of 3000 points (35,85%) for the scene classification problem. In relation to the participant approaches, NUDT and UFMS groups submitted a working note with internal details of their submissions. The NUDT proposal <ref type="bibr" coords="10,174.69,564.32,15.50,8.74" target="#b12">[13]</ref> that ranked first followed a spatial pyramid matching approach <ref type="bibr" coords="10,470.09,564.32,10.52,8.74" target="#b3">[4]</ref> based on appearance and shape features. Concretely, they used a Bag of Words (BoW) representation to create the appearance descriptor from dense SIFT features. The shape was represented using Pyramid Histograms of Gradients (PHOG) approach. Shape and appearance descriptors were then concatenated to create a single image descriptor used as input for the classification step. The classification was performed using a multi-class SVM using an one versus all strategy. The CPPP/UFMS proposal <ref type="bibr" coords="11,302.54,118.99,10.52,8.74" target="#b1">[2]</ref> also uses dense SIFT descriptors and the spatial pyramid approach. However, this approach is based on a k-nearest neighbor classifier and no PHOG descriptors are considered. None of the groups used the depth information encoded in the point cloud files that were released in conjunction to the visual images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NUDT</head><p>In view of the obtained results, we can conclude that room classification remains as an open problem when generalization is requested. That is, current approaches (as shown in previous task editions) perform well when the test environment has been previously imaged during training, but they present problems to classify frames acquired in new environments. On the other hand, we should point out the high performance of the submissions when facing the object recognition problem. This can be explained because object recognition does not rely on the scene generalization as for the room classifications. Namely, phones or chairs will always be recognized as their type (a phone or a chair, respectively) independently from the scene where they are placed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>In this paper, the overview of the 2014 edition of the Robot Vision task at ImageCLEF has been presented. We have described the task, which had slightly variations from previous editions, and a detailed analysis of the results obtained for the participants proposals.</p><p>As a novelty for this edition, we have introduced physical changes in the environment where the test sequence has been acquired. That provides an additional component to the classical place classification problem, empathizing in the generalization. According to the obtained results, this novelty has resulted in a notorious decrease on the room classification performance: none of the submission achieved more than 40% of the maximum score. The inclusion of depth images in the participants proposals could have increased the performance of the room classifiers. With respect to the object recognition, it was properly managed by the NUDT group that ranked first.</p><p>As future work, we plan to manage both room classification and object recognition problems jointly. All the participants solutions are based on using the same technique to classify the room and to recognize objects. Both problems are solved without any type of correlation, a different way as humans do. Therefore, future work will focus on making participants classify rooms using as input the list of objects recognized in the scene.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,194.71,254.21,225.93,7.89;4,247.16,115.84,121.04,123.60"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>Fig. 1. Mobile robot platform used for data acquisition.</figDesc><graphic coords="4,247.16,115.84,121.04,123.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,134.77,452.90,73.88,8.77;4,134.77,476.79,345.84,8.74;4,134.77,488.75,345.84,8.74;4,134.77,500.70,345.84,8.74;4,134.77,512.66,345.84,8.74;4,134.77,524.61,345.84,8.74;4,134.77,536.57,345.84,8.74;4,134.77,548.52,345.84,8.74;4,134.77,560.48,345.84,8.74;4,134.77,572.43,345.84,8.74;4,134.77,584.39,345.84,8.74;4,134.77,596.34,345.84,8.74;4,134.77,608.30,345.84,8.74;4,134.77,620.25,345.84,8.74;4,134.77,632.21,345.84,8.74;4,134.77,644.16,239.91,8.74"><head>2. 2</head><label>2</label><figDesc>The Data In the 2014 edition of the RobotVision challenge, a new version of the unreleased Robot Vision dataset was created. This specific dataset consists of three sequences (training, validation and test) of depth and visual images acquired within the following indoor environment: two department buildings at the University of Alicante, in Spain. Visual images were stored in PNG format and depth ones in PCD. Every image in the dataset was manually labelled with its corresponding room category/class and with a list of eight different objects to appear or not within it. The 10 different room categories are: Corridor, Hall, ProfessorOffice, StudentOffice, TechnicalRoom, Toilet, Secretary, VisioConference, Warehouse and ElevatorArea. The 8 different objects are: Extinguisher, Phone, Chair, Printer, Urinal, Bookself, Trash and Fridge. The dataset used in the task includes two labelled sequences used for training and validation with 5000 and 1500 images respectively. The unlabeled sequence used for test consists of 3000 different images. The frequency distribution for room categories in the training, validation and test sequences are depicted in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,170.94,564.52,273.47,7.89"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Class distribution in training, validation and test sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,167.79,408.70,279.77,7.89"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Object distribution in training, validation and test sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="10,202.43,505.88,210.49,7.89"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Detailed results for best groups submissions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,134.77,644.16,345.84,20.69"><head>Table 1 .</head><label>1</label><figDesc>Regarding the building used in the acquisition, all the 5000 training images were acquired in the building A, the same used for the 2013 edition dataset. The validation sequence included 1000 images from building A but 500 new images from building B. Finally, all 3000 test images were acquired in building B.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,134.77,172.44,345.81,161.36"><head>Table 1 .</head><label>1</label><figDesc>Distribution of room categories for the training, validation and test sequences.</figDesc><table coords="5,305.87,193.24,72.54,7.86"><row><cell>Number of frames</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,134.77,115.91,347.68,267.42"><head>Table 2 .</head><label>2</label><figDesc>Distribution of object presences or for the training, validation and test sequences.</figDesc><table coords="6,136.56,147.67,345.89,235.66"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Number of presences / lacks</cell><cell></cell></row><row><cell cols="10">Object Category Training (Building A) Validation (Building A+B) Test (Building B)</cell></row><row><cell cols="2">Extinguisher</cell><cell cols="2">770 / 4230</cell><cell></cell><cell cols="3">238 / 1262</cell><cell cols="2">566 / 2434</cell></row><row><cell>Chair</cell><cell></cell><cell cols="2">1304 / 3696</cell><cell></cell><cell cols="3">471 / 1029</cell><cell cols="2">1070 / 1930</cell></row><row><cell>Printer</cell><cell></cell><cell cols="2">473 / 4527</cell><cell></cell><cell cols="3">139 / 1361</cell><cell cols="2">265 / 2735</cell></row><row><cell>Bookshelf</cell><cell></cell><cell cols="2">802 / 4198</cell><cell></cell><cell cols="3">317 / 1183</cell><cell cols="2">896 / 2104</cell></row><row><cell>Urinal</cell><cell></cell><cell cols="2">162 / 4838</cell><cell></cell><cell cols="3">040 / 1460</cell><cell cols="2">060 / 2940</cell></row><row><cell>Trash</cell><cell></cell><cell cols="2">813 / 4187</cell><cell></cell><cell cols="3">323 / 1177</cell><cell cols="2">797 / 2203</cell></row><row><cell>Phone</cell><cell></cell><cell cols="2">267 / 4733</cell><cell></cell><cell cols="3">113 / 1387</cell><cell cols="2">303 / 2697</cell></row><row><cell>Fridge</cell><cell></cell><cell cols="2">190 / 4810</cell><cell></cell><cell cols="3">034 / 1466</cell><cell cols="2">047 / 2953</cell></row><row><cell>All</cell><cell></cell><cell cols="2">4781 / 35219</cell><cell></cell><cell cols="3">1675 / 10325</cell><cell cols="2">4004 / 19996</cell></row><row><cell></cell><cell cols="2">Training</cell><cell></cell><cell cols="2">Validation</cell><cell></cell><cell></cell><cell>Test</cell></row><row><cell>4%</cell><cell>6%</cell><cell>17%</cell><cell>2%</cell><cell>7%</cell><cell>19%</cell><cell></cell><cell>4%</cell><cell>6%</cell><cell>17%</cell></row><row><cell>16%</cell><cell></cell><cell>3%</cell><cell>14%</cell><cell></cell><cell></cell><cell>2%</cell><cell>16%</cell><cell></cell><cell>3%</cell></row><row><cell></cell><cell></cell><cell>17%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>17%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>19%</cell><cell></cell><cell></cell></row><row><cell>27%</cell><cell></cell><cell>10%</cell><cell>28%</cell><cell></cell><cell>8%</cell><cell></cell><cell>27%</cell><cell></cell><cell>10%</cell></row><row><cell cols="2">Extinguisher</cell><cell>Chair</cell><cell>Printer</cell><cell cols="2">Bookshelf</cell><cell>Urinal</cell><cell>Trash</cell><cell>Phone</cell><cell>Fridge</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,134.77,435.40,357.25,224.58"><head>Table 5 .</head><label>5</label><figDesc>Overall ranking of the runs submitted by the participant groups to the 2014</figDesc><table coords="9,134.77,446.39,357.25,213.59"><row><cell cols="2">Robot Vision task</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Rank Group Name Score Room (% Max) Score Objects (% Max) Score Total (% Max.)</cell></row><row><cell>1</cell><cell>NUDT</cell><cell>1075,5 (35,85)</cell><cell>3354,75 (83,78)</cell><cell>4430,25 (63,25)</cell></row><row><cell>2</cell><cell>NUDT</cell><cell>1060,5 (35,35)</cell><cell>3354,75 (83,78)</cell><cell>4415,25 (63,04)</cell></row><row><cell>3</cell><cell>NUDT</cell><cell>1057,5 (35,25)</cell><cell>3354,75 (83,78)</cell><cell>4412,25 (63,00)</cell></row><row><cell>4</cell><cell>NUDT</cell><cell>1107,0 (36,90)</cell><cell>3276,00 (81,82)</cell><cell>4383,00 (62,58)</cell></row><row><cell>5</cell><cell>NUDT</cell><cell>1107,0 (36,90)</cell><cell>3245,50 (81,06)</cell><cell>4352,50 (62,14)</cell></row><row><cell>6</cell><cell>NUDT</cell><cell>1113,0 (37,10)</cell><cell>3233,75 (80,76)</cell><cell>4346,75 (62,06)</cell></row><row><cell>7</cell><cell>NUDT</cell><cell>1060,5 (35,35)</cell><cell>3096,75 (77,34)</cell><cell>4157,25 (59,36)</cell></row><row><cell>8</cell><cell>NUDT</cell><cell>1057,5 (35,25)</cell><cell>3096,75 (77,34)</cell><cell>4154,25 (59,31)</cell></row><row><cell>9</cell><cell>NUDT</cell><cell>1030,5 (34,35)</cell><cell>2965,25 (74,06)</cell><cell>3995,75 (57,05)</cell></row><row><cell>10</cell><cell>NUDT</cell><cell>1008,0 (33,60)</cell><cell>2870,00 (71,68)</cell><cell>3878,00 (55,37)</cell></row><row><cell>11</cell><cell>UFMS</cell><cell>0219,0 (07,30)</cell><cell>1519,75 (37,96)</cell><cell>1738,75 (24,83)</cell></row><row><cell>12</cell><cell>UFMS</cell><cell>0213,0 (07,10)</cell><cell>1453,00 (36,29)</cell><cell>1666,00 (23,79)</cell></row><row><cell>13</cell><cell>UFMS</cell><cell>0192,0 (06,40)</cell><cell>1460,50 (36,48)</cell><cell>1652,50 (23,59)</cell></row><row><cell>14</cell><cell>UFMS</cell><cell>0150,0 (05,00)</cell><cell>1483,00 (37,04)</cell><cell>1633,00 (23,32)</cell></row><row><cell>15</cell><cell>SIMD</cell><cell>0067,5 (02,25)</cell><cell>0186,25 (04,65)</cell><cell>0253,75 (03,62)</cell></row><row><cell>16</cell><cell>AEGEAN</cell><cell>-405,0 (-13,50)</cell><cell>-995,00 (-24,85)</cell><cell>-1400,00 (-19,99)</cell></row><row><cell>17</cell><cell>AEGEAN</cell><cell>-405,0 (-13,50)</cell><cell>-1001,00 (-25,00)</cell><cell>-1406,00 (-20,07)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_0" coords="10,144.73,656.80,153.38,7.86"><p>http://www.imageclef.org/2014/robot</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>This work has been partially funded by <rs type="funder">FEDER</rs> funds and the <rs type="funder">Spanish Government (MICINN)</rs> through projects <rs type="grantNumber">TIN2010-20900-C04-03</rs>, <rs type="grantNumber">DPI2013-40534-R</rs> and by the <rs type="programName">Innterconecta Programme 2011</rs> project <rs type="grantNumber">ITC-20111030 ADAPTA. 4</rs> http://imageclef.org/ 5 http://www.clef-initiative.eu//</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_TUPjpNm">
					<idno type="grant-number">TIN2010-20900-C04-03</idno>
				</org>
				<org type="funding" xml:id="_bXn6Esk">
					<idno type="grant-number">DPI2013-40534-R</idno>
					<orgName type="program" subtype="full">Innterconecta Programme 2011</orgName>
				</org>
				<org type="funding" xml:id="_JR3qseG">
					<idno type="grant-number">ITC-20111030 ADAPTA. 4</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Additional information provided by the organization</head><p>In addition to all the image sequences, we created a Matlab script to be used as template for participants proposals. This script performs all the steps for generating solutions for the Robot Vision challenge: features generation, training, classification and performance evaluation. Basic features are generated for both visual and depth images (histograms) and training and classification is performed  using an On-line Independent Support Vector Machines <ref type="bibr" coords="8,381.38,632.21,10.52,8.74" target="#b7">[8]</ref> that, in comparison with SVM, dramatically reduces learning time and space requirements at the price of a negligible loss in accuracy.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="11,142.96,623.92,337.62,7.86;11,151.52,632.61,329.06,10.13;11,151.52,645.84,329.07,7.86;11,151.52,656.80,282.54,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,151.52,645.84,229.28,7.86">ImageCLEF 2014: Overview and analysis of the results</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Martinez-Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Patricia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Marvasti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Üsküdarlı</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cazorla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Garcia-Varea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Morell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,404.70,645.84,71.98,7.86">CLEF proceedings</title>
		<title level="s" coord="11,151.52,656.80,141.41,7.86">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,119.67,337.62,7.86;12,151.52,130.63,329.05,7.86;12,151.52,141.59,309.83,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,247.99,130.63,228.27,7.86">CPPP/UFMS at ImageCLEF 2014: Robot Vision Task</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>De Carvalho Gomes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Correia Ribas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Antônio De Castro Junior</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Nunes</forename><surname>Gonçalves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,163.04,141.59,173.38,7.86">CLEF 2014 Evaluation Labs and Workshop</title>
		<title level="s" coord="12,344.46,141.59,88.63,7.86">Online Working Notes</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,152.55,337.62,7.86;12,151.52,163.51,244.90,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="12,399.05,152.55,81.52,7.86;12,151.52,163.51,96.64,7.86">On-line independent support vector machines</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Orabona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Castellini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sandini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1402" to="1412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,174.47,337.62,7.86;12,151.52,185.43,329.07,7.86;12,151.52,196.39,329.07,7.86;12,151.52,207.35,72.82,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,312.80,174.47,167.77,7.86;12,151.52,185.43,193.21,7.86">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,362.13,185.43,118.46,7.86;12,151.52,196.39,44.75,7.86">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,218.30,337.62,7.86;12,151.52,229.26,327.24,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,374.33,218.30,106.24,7.86;12,151.52,229.26,88.17,7.86">Overview of the imageclef 2012 robot vision task</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Martinez-Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Garcia-Varea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,259.15,229.26,190.96,7.86">CLEF (Online Working Notes/Labs/Workshop)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,240.22,337.62,7.86;12,151.52,251.18,329.06,7.86;12,151.52,262.14,116.88,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,416.92,240.22,63.65,7.86;12,151.52,251.18,129.35,7.86">Overview of the imageclef 2013 robot vision task</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Martinez-Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Garcia-Varea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cazorla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,301.32,251.18,174.52,7.86">CLEF 2013 Evaluation Labs and Workshop</title>
		<title level="s" coord="12,151.52,262.14,88.62,7.86">Online Working Notes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,273.10,337.63,7.86;12,151.52,284.06,148.71,7.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="12,204.71,273.10,182.13,7.86">Dogma: a matlab toolbox for online learning</title>
		<author>
			<persName coords=""><surname>Orabona</surname></persName>
		</author>
		<ptr target="http://dogma.sourceforge.net" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,295.02,337.62,7.86;12,151.52,305.98,329.07,7.86;12,151.52,316.94,20.99,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,397.13,295.02,83.45,7.86;12,151.52,305.98,215.60,7.86">Indoor place recognition using online independent support vector machines</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Orabona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Castellini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sandini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,385.57,305.98,48.09,7.86">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,327.89,337.62,7.86;12,151.52,338.85,329.07,7.86;12,151.52,349.81,329.06,7.86;12,151.52,360.77,20.99,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,277.60,327.89,89.29,7.86">The robot vision task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pronobis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,385.34,338.85,46.28,7.86">ImageCLEF</title>
		<title level="s" coord="12,162.76,349.81,134.19,7.86">The Information Retrieval Series</title>
		<editor>
			<persName><forename type="first">Henning</forename><surname>Muller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="185" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,371.73,337.96,7.86;12,151.52,382.69,329.07,7.86;12,151.52,393.65,84.01,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,332.32,371.73,148.25,7.86;12,151.52,382.69,71.84,7.86">Overview of the imageclef@ icpr 2010 robot vision track</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pronobis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,233.32,382.69,243.11,7.86">Recognizing Patterns in Signals, Speech, Images and Videos</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="171" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,404.61,337.96,7.86;12,151.52,415.57,250.33,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,390.38,404.61,90.20,7.86;12,151.52,415.57,68.25,7.86">The robot vision track at imageclef 2010</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pronobis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fornoni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Hi Christensesn</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,228.06,415.57,120.53,7.86">Working Notes of ImageCLEF</title>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,426.52,337.96,7.86;12,151.52,437.48,329.06,7.86;12,151.52,448.44,329.07,7.86;12,151.52,459.40,329.07,7.86;12,151.52,470.36,320.19,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,319.41,426.52,161.17,7.86;12,151.52,437.48,19.21,7.86">Overview of the clef 2009 robot vision track</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pronobis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,432.58,448.44,48.00,7.86;12,151.52,459.40,232.11,7.86">Multilingual Information Access Evaluation II. Multimedia Experiments</title>
		<title level="s" coord="12,451.65,459.40,28.94,7.86;12,151.52,470.36,108.29,7.86">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gareth</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jayashree</forename><surname>Kalpathy-Cramer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Theodora</forename><surname>Tsikrika</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin / Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6242</biblScope>
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,481.32,337.96,7.86;12,151.52,492.28,329.06,7.86;12,151.52,503.24,116.88,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,325.07,481.32,155.50,7.86;12,151.52,492.28,98.57,7.86">NUDT&apos;s Participation in ImageCLEF Robot Vision Challenge</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,296.36,492.28,179.47,7.86">CLEF 2014 Evaluation Labs and Workshop</title>
		<title level="s" coord="12,151.52,503.24,88.62,7.86">Online Working Notes</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
