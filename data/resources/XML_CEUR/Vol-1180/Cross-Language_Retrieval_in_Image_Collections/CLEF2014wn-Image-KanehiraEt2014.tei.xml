<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,216.24,116.95,182.87,12.62;1,171.66,134.89,272.02,12.62">MIL at ImageCLEF 2014: Scalable System for Image Annotation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,146.56,172.63,73.99,8.74"><forename type="first">Atsushi</forename><surname>Kanehira</surname></persName>
							<email>kanehira@mi.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Machine Intelligence Lab</orgName>
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,228.62,172.63,75.89,8.74"><forename type="first">Masatoshi</forename><surname>Hidaka</surname></persName>
							<email>hidaka@mi.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Machine Intelligence Lab</orgName>
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,312.59,172.63,66.16,8.74"><forename type="first">Yusuke</forename><surname>Mukuta</surname></persName>
							<email>mukuta@mi.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Machine Intelligence Lab</orgName>
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,387.36,172.63,76.80,8.74"><forename type="first">Yuichiro</forename><surname>Tsuchiya</surname></persName>
							<email>tsuchiya@mi.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Machine Intelligence Lab</orgName>
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,227.49,184.58,62.38,8.74"><forename type="first">Tetsuaki</forename><surname>Mano</surname></persName>
							<email>mano@mi.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Machine Intelligence Lab</orgName>
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,318.05,184.58,69.82,8.74"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
							<email>harada@mi.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Machine Intelligence Lab</orgName>
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,216.24,116.95,182.87,12.62;1,171.66,134.89,272.02,12.62">MIL at ImageCLEF 2014: Scalable System for Image Annotation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3C5158EBE42D178D693ADA716FE4161A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ImageCLEF</term>
					<term>deep convolutional neural network</term>
					<term>Fisher vector</term>
					<term>Image annotation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this working note, we describe details of our method in ImageCLEF2014 Scalable Concept Image Annotation task. We are given images from the Web and some additional information, including web pages, in which images exist. Using this information, we must construct an annotation system that has high performance and scalability. To assign labels to each image, we use the page title and attributes of an image tag extracted from the web page. As visual features, we propose the use of the combination of two complementary features, which are Fisher Vector and deep convolutional neural network based feature. They are generative and discriminative feature respectively. We then train linear classifiers using Passive-Aggressive with Averaged Pairwise Loss. After training, we calculate the score of each concept for test data and label some concepts having the best scores. Results show that the combination of two features contributes to the improvement of recognition performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>For the ImageCLEF2014 Scalable Concept Image Annotation task <ref type="bibr" coords="1,437.83,513.51,10.20,8.74" target="#b0">[1]</ref>[2], our task is to construct an image annotation system that yields high-performance with scalability.</p><p>As visual features, we use a convolutional neural network (CNN) based feature as well as the Fisher Vector (FV) <ref type="bibr" coords="1,297.61,561.40,9.96,8.74" target="#b2">[3]</ref>. For scalability, our method of assigning labels to training images is simple. We use only the page title and attributes of the image tag extracted from the web page. To train linear classifiers, we use Passive-Aggressive with Averaged Pairwise Loss (PAAPL) <ref type="bibr" coords="1,403.41,597.27,10.52,8.74" target="#b3">[4]</ref> because of its scalability and robustness to noise of label assignment.</p><p>In our experiment, we combine two types of features, which are FV and CNNbased features. Actually, FV is often used in image recognition tasks because of its recognition performance. However, many results of recent studies show that deep CNN achieves high performance on many tasks. Therefore, we expect that the feature, which is the neuron activation pattern in the hidden layers of the network, has high-representational ability.</p><p>These two features are extracted in completely different ways. We obtain FV by coding local descriptors considering their probabilistic distribution. Therefore FV can be regarded as the feature expressing generative information of an image. In contrast to FV because deep CNN based feature is extracted from the network trained for recognition task, we can regard it as a discriminative feature.</p><p>Assuming that these two types of features, which represent different kinds of information, mutually compensate for representational ability, we propose their combined use. Our contribution is the usage of a combination of features that have complementary properties to improve the performance of annotation systems.</p><p>The remainder of this working note is structured as follows. Section 2 presents a description of two types of visual features: FV and deep CNN based features. In Section 3, we explain details of how we obtain labels from training data. Then, in section 4, we introduce a multi-label linear classifier training method: PAAPL. In section 5, we present the results of experiments, using either or both of these visual features. Finally, in section 6, we discuss the analysis of the results obtained in our experiment.</p><p>2 Visual Feature</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Fisher Vector</head><p>As a visual feature, we use Fisher Vector (FV) because FV can achieve better recognition performance than Bag of Visual Words with a linear classifier. In general, the linear classifier is less costly than a nonlinear one such as kernel-SVM when the amount of the training sample increases. Therefore, FV is suitable for this task, which requires scalability.</p><p>In our experiments, we extract four local descriptors: SIFT, GIST, LBP, and C-SIFT. The dimensions of all these local descriptors are reduced to 64 dimensions using Principal Component Analysis (PCA). These local descriptors are densely extracted from five scales of patches (squares 16, 25, 36, 49, 64 pixels on a side) sliding with a step of six pixels. Using some of the obtained local descriptors, we train a Gaussian Mixture Model (GMM) with 256 components, which have diagonal matrices as covariance matrices. After training GMM, we extract FV from each image by calculating the gradient of log-likelihood of local descriptors with respect to parameters of GMM. Then we normalize it using the Fisher information matrix. Power normalization and L2 normalization are applied to the extracted FVs. To include spatial information, we divide images into 1×1, 2×2, and 3×1 cells, extract features from each region and concatenate them into one vector. The final dimension of FV is 262,144 (64 × 256 × 2 × 8).</p><p>As described above, FV is obtained by coding local descriptors considering their probabilistic distribution. Therefore FV can be regarded as a feature expressing generative information of the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep CNN based feature</head><p>In addition to FV, we use a deep convolutional neural network (CNN) based feature extracted from a deep CNN model that had been pre-trained with ImageNet dataset.</p><p>In recent years, many studies that specifically examine deep CNN have shown that such models can perform better than conventional feature representation in object recognition and other tasks. On ImageNet Large Scale Visual Recognition Challenge, one system <ref type="bibr" coords="3,232.17,213.59,10.52,8.74" target="#b4">[5]</ref> dramatically outperformed all other methods including a state-of-the-art method using FV.</p><p>However, training deep architecture of CNN requires large-scale data to prevent overfitting and to ensure generalization ability. Trained models will have low recognition performance if the training data are few.</p><p>For this task, because we must label training data automatically, the number of reliably labeled data we can obtain using our method is roughly 100,000 on the development set and about 200,000 on the test set. This fact implies that the approximate number we can use in training models is only 1,000 per concept, on average. Therefore, because of limitations in the amount of data in the given dataset, it is difficult to train deep CNN models to produce high recognition performance.</p><p>According to <ref type="bibr" coords="3,207.98,358.02,9.96,8.74" target="#b5">[6]</ref>, features extracted from the activation of a CNN pre-trained in supervised fashion can be re-purposed to generic tasks. In his experiment, he shows that such feature representation has such high generality that it outperforms conventional methods in several tasks despite their simple training algorithm.</p><p>In consideration of the discussion presented above, we use feature representations extracted from a deep CNN model pre-trained with the ImageNet dataset. Following the method of <ref type="bibr" coords="3,246.06,442.03,9.96,8.74" target="#b5">[6]</ref>, we extract features from sixth and seventh layers of the network having architecture that is the same as that proposed by <ref type="bibr" coords="3,467.32,453.98,9.96,8.74" target="#b4">[5]</ref>, which won ILSVRC2012 and which includes five convolutional and three fully connected layers. In some layers, the Hinge function is used as the activation function. It is designated as ReLU.</p><p>Contrary to FV, because deep CNN-based features are extracted from the network, which is trained for recognition task, we can regard it as a feature that expresses discriminative information of an image.</p><p>In our experiment, we use DeCAF, an open source library produced by <ref type="bibr" coords="3,455.81,538.32,9.96,8.74" target="#b5">[6]</ref>, to extract deep feature representation. We use features of four types. The feature vector dimensions are 4,096.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Label Assignment</head><p>Because no explicit labels are assigned to training images, we must label them using additional information or external sources. In this section, we describe how to assign labels to images. The pipeline of label assignment is shown in Fig. <ref type="figure" coords="3,472.34,645.16,4.13,8.74" target="#fig_0">1</ref>. We label images in the following two steps. In the first step, we parse the xml files of the web page, in which an image exists. Then we extract page titles and attributes of the image tag, which include src, title, and alt. The hope is that these attributes include important information about what the image represents. Then we split them into a set of single words T . For example, if there is an image tag in an xml file shown in Fig. <ref type="figure" coords="4,434.60,301.94,4.13,8.74" target="#fig_1">2</ref>, then we obtain T = {Queen, Prince, corgi, family, abroad}. In the second step, we collect a set of synonyms and hyponyms for each concept C using WordNet <ref type="bibr" coords="4,255.89,608.61,9.96,8.74" target="#b6">[7]</ref>. We denote the collected sets by W C , which is expressed as</p><formula xml:id="formula_0" coords="4,222.24,657.11,165.30,9.65">W C = {C, synonym(C), hyponym(C)}</formula><p>where synonyms (C) and hyponyms (C) respectively represent sets of synonyms and hyponyms of the concept C. For example, given a target concept "dog", we obtain W dog = {dog, puppy, corgi, ...}.</p><p>We assign the concept C as the label to the image if at least one word in W C appears in T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Training Classifier</head><p>In this section, we introduce the multi-label linear classifier training method Passive-Aggressive with Averaged Pairwise Loss (PAAPL) <ref type="bibr" coords="5,392.56,250.30,9.96,8.74" target="#b3">[4]</ref>. Because PAAPL is based on Passive-Aggressive (PA) <ref type="bibr" coords="5,299.68,262.26,10.52,8.74" target="#b7">[8]</ref> method, which is known to be robust to outliers, PAAPL also has robustness to outliers. In addition, PAAPL has scalability because the trained classifier is linear. Such properties of PAAPL are suitable for our task, in which it is necessary to construct a scalable system that handles data including some outliers. First, we describe the model update rule of PA. Given the t-th training sample, we designate the visual feature by x t . We define Y t as the set of labels assigned to the t-th sample, and Ȳt as the set of labels not assigned. The model (weight) of the linear classifier corresponding to concept label C before updating by the t-th sample is denoted by w C t .</p><p>1. Fetch the t-th training sample. Then compute scores for each label using current models. Because classifiers we are training are linear, scores are given by simple calculation of the inner product of weight and feature. 2. Based on scores, find a combination of labels r t ∈ Y t and s t ∈ Ȳt in the following way. </p><formula xml:id="formula_1" coords="5,271.76,458.04,18.54,9.65">r t =</formula><formula xml:id="formula_2" coords="5,227.46,526.69,252.70,27.34">)) = 0 if w rt t • x t -w st t • x t &gt; 1 1 -(w rt t • x t -w st t • x t ) otherwise</formula><p>4. Update models using hinge-loss according to the following rule.</p><formula xml:id="formula_3" coords="5,254.98,582.83,121.83,54.05">w rt t+1 = w rt t + l 2|x t | 2 + 1 D x t w st t+1 = w st t - l 2|x t | 2 + 1 D x t</formula><p>Therein, D is a Passive-Aggressive parameter that reduces the negative influence of noisy labels.</p><p>Then we describe the method of training classifiers with PAAPL.</p><p>1. Pick the t-th training sample, compute scores for target labels using current models. 2. For a randomly selected combination of labels r t ∈ Y t and s t ∈ Ȳt , hingeloss is calculated as PA and remove r t and s t from Y t and Ȳt . Continue this process until |Y t | = 0 or | Ȳt | = 0. 3. For combinations satisfying the condition that the hinge-loss is not 0, update the models according to the update rule of PA.</p><p>In PAAPL, convergence of models is faster than in PA because PAAPL updates multiple pairs of models for one sample, whereas PA updates only one pair of models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>At the training phase, we first extract visual features and assign labels. Then, linear classifiers are trained. At the test phase, we calculate scores for test images using the linear classifiers we trained. The concepts are labeled on those images. When training classifiers, the number of iterations is set to 5 and Passive Aggressive parameter D is set to 1.0 × 10 5 . After the training process, we average scores from different models trained using different visual features. Also, we decide concepts by selecting those with scores in the top 4% of all given concepts to each test sample. In our experiments, we compare two types of visual features. The settings, except for the combinations of visual features, are fixed throughout all of our experiments.</p><p>First, for each type of feature, we respectively search for the best combination. For FV, we use four local descriptors, SIFT, C-SIFT, GIST, and LBP. Because the respective properties of these four features differ, we try all possible combinations of them. As for deep CNN-based features, we extract them from sixth and seventh layers. From each layer, we obtain features of two types, such as activations and outputs of each unit. Therefore we also obtain four types of visual features. In contrast to the case of FV, we need not try all possible combinations because combinations of features from the same layer do not make sense theoretically: they are expected to have similar properties. As shown in Tables <ref type="table" coords="6,134.77,548.51,32.01,8.74">Table 1</ref> and<ref type="table" coords="6,188.73,548.51,30.90,8.74">Table 2</ref>, in both cases, the results of combining all features achieves higher performance than the others. These results respectively correspond to our Run 1 and Run 2.</p><p>Finally, we combine these two types of visual features. Using the information presented above, we combine all four FVs and four deep CNN based features. The final results are presented in a Table in Table <ref type="table" coords="6,364.47,608.79,3.87,8.74">3</ref>, which correspond to all Runs we submitted. We achieved better performance using both features than by using either one.</p><p>As a result, we achieved the second score among all participants with our best run.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,234.48,218.61,146.39,7.89;4,146.99,119.13,321.40,72.44"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Pipeline of label assignment.</figDesc><graphic coords="4,146.99,119.13,321.40,72.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,243.65,561.64,128.05,7.89;4,261.20,374.28,240.19,159.81"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Attributes of image tag.</figDesc><graphic coords="4,261.20,374.28,240.19,159.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,138.97,455.96,257.98,90.52"><head></head><label></label><figDesc>arg min</figDesc><table coords="5,138.97,455.96,257.98,90.52"><row><cell>r∈Yt</cell><cell cols="2">w r t • x t</cell></row><row><cell cols="2">s t = arg max s∈ Ȳt</cell><cell>w s t • x t</cell></row><row><cell cols="3">3. For a combination of r t and s t , compute the hinge-loss l</cell></row><row><cell>l(w rt t , w st t ; (x t , Y t</cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this working note, we described our annotation method for the ImageCLEF 2014 Scalable Concept Image Annotation task. As visual features, we used FV and deep CNN based feature. Assuming that these two types of features mutually express different kinds of information complementarily, we tried combining them. In our experiment, we showed how the combination of features contributes to the improvement of recognition performance. Results show that the combination of generative features and discriminative features proved effective in image recognition tasks.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="7,138.35,613.96,342.23,7.86;7,146.91,622.65,333.67,10.13;7,146.91,635.88,333.66,7.86;7,146.91,646.84,333.67,7.86;7,146.91,657.80,134.10,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,350.60,635.88,129.98,7.86;7,146.91,646.84,87.38,7.86">ImageCLEF 2014: Overview and analysis of the results</title>
		<author>
			<persName coords=""><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jesus</forename><surname>Martinez-Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mauricio</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Burak</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Novi</forename><surname>Patricia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Neda</forename><surname>Marvasti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Suzan</forename><surname>Üsküdarlı</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roberto</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Miguel</forename><surname>Cazorla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ismael</forename><surname>Garcia-Varea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vicente</forename><surname>Morell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,255.04,646.84,71.25,7.86">CLEF proceedings</title>
		<title level="s" coord="7,333.66,646.84,142.95,7.86">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,120.67,342.23,7.86;8,146.91,131.63,333.67,7.86;8,146.91,142.59,116.87,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,309.26,120.67,171.32,7.86;8,146.91,131.63,131.36,7.86">Overview of the ImageCLEF 2014 Scalable Concept Image Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">Mauricio</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roberto</forename><surname>Paredes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,300.17,131.63,175.66,7.86">CLEF 2014 Evaluation Labs and Workshop</title>
		<title level="s" coord="8,146.91,142.59,88.62,7.86">Online Working Notes</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,153.55,342.23,7.86;8,146.91,164.51,281.01,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,316.82,153.55,163.76,7.86;8,146.91,164.51,76.41,7.86">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,230.97,164.51,168.80,7.86">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,175.47,342.22,7.86;8,146.91,186.42,333.68,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,317.17,175.47,163.40,7.86;8,146.91,186.42,76.75,7.86">Efficient image annotation for automatic sentence generation</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Kuniyoshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,231.10,186.42,221.54,7.86">The 20th ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,197.38,342.23,7.86;8,146.91,208.34,279.67,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,388.63,197.38,91.95,7.86;8,146.91,208.34,161.22,7.86">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,327.99,208.34,19.49,7.86">NIPS</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,219.30,342.23,7.86;8,146.91,230.26,333.66,7.86;8,146.91,241.22,204.67,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="8,226.77,230.26,253.81,7.86;8,146.91,241.22,43.20,7.86">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName coords=""><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.1531</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,138.35,252.18,307.73,7.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="8,202.84,252.18,165.52,7.86">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,263.14,342.23,7.86;8,146.91,274.10,333.68,7.86;8,146.91,285.05,58.87,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,417.98,263.14,62.60,7.86;8,146.91,274.10,89.82,7.86">Online Passive-Aggressive Algorithms</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Keshet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,248.94,274.10,178.59,7.86">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="551" to="585" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
