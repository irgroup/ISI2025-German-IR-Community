<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,156.24,116.95,302.88,12.62;1,202.64,134.89,210.07,12.62">NUDT&apos;s Participation in the Robot Vision Challenge of ImageCLEF2014</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,197.88,172.56,38.47,8.74"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Automatic Control</orgName>
								<orgName type="department" key="dep2">College of Mechatronics</orgName>
								<orgName type="institution">Automation National University of Defense Technology</orgName>
								<address>
									<settlement>Changsha</settlement>
									<region>Hunan</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,244.66,172.56,35.84,8.74"><forename type="first">Jian</forename><surname>Qin</surname></persName>
							<email>qinjian714@126.comfanglinchen</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Automatic Control</orgName>
								<orgName type="department" key="dep2">College of Mechatronics</orgName>
								<orgName type="institution">Automation National University of Defense Technology</orgName>
								<address>
									<settlement>Changsha</settlement>
									<region>Hunan</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,288.52,172.56,55.93,8.74"><forename type="first">Fanglin</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Automatic Control</orgName>
								<orgName type="department" key="dep2">College of Mechatronics</orgName>
								<orgName type="institution">Automation National University of Defense Technology</orgName>
								<address>
									<settlement>Changsha</settlement>
									<region>Hunan</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,372.24,172.56,45.24,8.74"><forename type="first">Dewen</forename><surname>Hu</surname></persName>
							<email>dwhu@nudt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Automatic Control</orgName>
								<orgName type="department" key="dep2">College of Mechatronics</orgName>
								<orgName type="institution">Automation National University of Defense Technology</orgName>
								<address>
									<settlement>Changsha</settlement>
									<region>Hunan</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,156.24,116.95,302.88,12.62;1,202.64,134.89,210.07,12.62">NUDT&apos;s Participation in the Robot Vision Challenge of ImageCLEF2014</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9BF00C2F14B90483BC6C2195C7949BE2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>indoor robot localization</term>
					<term>indoor object recognition</term>
					<term>SPM</term>
					<term>PHOG</term>
					<term>multi-class SVM</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This working note describes the method of the NUDT team for scene classification and object recognition in the ImageCLEF 2014 Robot Vision Challenge. The method is composed of two steps: 1. spatial pyramid match (SPM) and a Pyramid of HOG (Histograms of Oriented Gradient) are incorporated to represent an indoor place image. 2. a multiclass SVM (Support Vector Machine) is utilized to classify an image by one-versus-all binary SVMs. Based on the method, our system wins the championship this year.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the 6th Robot Vision Challenge of ImageCLEF 2014, the task is to address the problem of robot localization in parallel to object recognition by use of visual and depth images. Both problems are inherently related: the objects present in a scene can help to determine the room category and vice versa. In this new edition of the task, strong variations are introduced between training and test scenarios, for instance, a non-previously seen building in the test sequence, variant lighting conditions and different indoor environment distribution.</p><p>This paper describes NUDT's participation in the Robot Vision Challenge. For the image representation, only visual images are utilized and the state-ofthe-art SPM and PHOG are both applied. In addition, nonlinear SVM is used for classificaiton.</p><p>The rest part of this paper is organized as follows. In Section 2, we briefly give a brief overview of the image representation in the task. In Section 3, we describe in detail the classifier which we use in both two problems. In Section 4, experimental setup and results are introduced. In Section 5, we conclude our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Image Representation</head><p>The proposed image representation incorporates appearance and shape. For appearance, we follow the approach of SPM <ref type="bibr" coords="1,324.79,657.11,9.96,8.74" target="#b0">[1]</ref>. SIFT descriptors <ref type="bibr" coords="1,420.56,657.11,10.52,8.74" target="#b1">[2]</ref> of 16 Ã— 16 pixel patches are computed over a dense regular grid with spacing of 8 pixels. We perform k-means clustering of a random subset of patches from the training set to form a visual codebook of V codes, and the corresponding descriptors are assigned to their closest vocabulary codes using the Euclidean distance. In the pyramid layer, multiple codes from inside each sub-region are pooled together into a histogram. Finally, the histograms from all sub-regions are concatenated together to generate SPM. Local place shape is described by HOG <ref type="bibr" coords="2,438.20,191.72,10.52,8.74" target="#b2">[3]</ref> within an image block quantized into B bins. Orientations of edges within a certain angular range are counted, forming each bin in the histogram. Incorporating spatial pooling like SPM, PHOG is formed <ref type="bibr" coords="2,322.45,227.59,9.96,8.74" target="#b3">[4]</ref>. The final image representation is formed by concatenating SPM and PHOG. The total cells at level l in the each pyramid is 4 l . The entire image representation is a vector with dimensionality</p><formula xml:id="formula_0" coords="2,134.77,262.23,83.13,30.55">V L l=0 4 l + B L l=0 4 l .</formula><p>For example, levels up to L = 1, V = 10 visual words and B = 10 bins it will be a 100-vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multiclass classification</head><p>We believe that adopting an appropriate multi-class classifier meets the requirement of promoting the recognition performance. The multi-class SVM using one-versus-all role proves its discrimination in many practical applications and is adopted in our system. For each class s, after the responding SVM classifier has been obtained through training, the final decision function f s (x) for a test sample x has the following form:</p><formula xml:id="formula_1" coords="2,227.85,422.15,252.75,30.32">f s (x) = max( m i=1 a s i y i ker(x i , x) + b s )<label>(1)</label></formula><p>Where ker(x i , x) is the kernel function, a s i , b s are the learned model parameters of each one-versus-all binary SVM, y i is the label of the training sample x i , m is the number of the training samples. Then the ultimate label belongs to the class with the maximal value. In practice, we find that the one-versus-all SVM with nonlinear kernel is suitable for classifying the scenes and objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In the Robot Vision Challenge this year, only one training sequence is provided with 5000 labeled images. An additional validation sequence with 1500 labelled images is provided, and includes 500 images of a non-previously seen building that presents similar room categories and objects as for the training sequences and 1000 images of the old building in the training sequence. The test sequence contains 3000 unlabeled images. In these three sequence, only visual information is considered irrespective of depth information.</p><p>For the task, we utilize the whole images of training sequence for training SVM, and the result is similar to the result of utilizing the same number of images for per class to train SVM. In the application, only grey level cues are used. And a visual codebook of 200 codes is used for appearance, and HOG with range [0, 360] using all orientation is discretized into B = 8 bins for shape. For multi-class classification, we utilize LIBSVM <ref type="bibr" coords="3,335.59,155.86,10.52,8.74" target="#b4">[5]</ref> with one-versus-all classifying rule. Specially, pyramid match kernel <ref type="bibr" coords="3,307.88,167.81,10.52,8.74" target="#b0">[1]</ref> is adopted. Moreover, we limit the number of levels to L = 2 to prohibit over fitting. All experiments are repeated 10 times with all training images. Some example images are shown in Fig. <ref type="figure" coords="3,472.85,191.72,3.87,8.74">1</ref>, Fig. <ref type="figure" coords="3,155.10,203.68,3.87,8.74">2</ref>, Fig. <ref type="figure" coords="3,186.52,203.68,3.87,8.74" target="#fig_1">3</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results on the validation sequence</head><p>We utilize our method on the validation sequence, and compute the respective classification accuracy for each scene category and object class. Detailed results are shown in Table <ref type="table" coords="3,220.64,657.11,4.98,8.74" target="#tab_0">1</ref> and Table <ref type="table" coords="3,275.72,657.11,3.87,8.74" target="#tab_1">2</ref>. Finally, Our team ranks the first out of four teams, results are listed in Table <ref type="table" coords="4,472.85,305.35,3.87,8.74" target="#tab_2">3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In the paper, we introduce our method for the Robot Vision Challenge in Image-CLEF2014. Our method applies the SPM and PHOG for image representation and multi-class SVM for classification, which achieves the best performance among all the participants. However, From the results of the challenge, there are still some scene categories and object classes can not be processed correctly. In addition, obviously, the non-previously seen building influences the performance.</p><p>In the future, we will focus on the further improvement on the classification accuracy and the generalization of our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,171.04,308.76,273.26,7.89;3,177.05,234.95,85.04,63.78"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>Fig. 1. Example images of scene categories in the training sequence</figDesc><graphic coords="3,177.05,234.95,85.04,63.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,218.76,562.30,177.83,7.89;3,177.05,488.49,85.04,63.78"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Example images in the new building</figDesc><graphic coords="3,177.05,488.49,85.04,63.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,158.05,116.91,299.25,43.94"><head>Table 1 .</head><label>1</label><figDesc>The accuracy on per scene category of validation data Scene Cor Elev Hall Pro Sec Stu Tec Toilet Visio Ware Total</figDesc><table coords="4,158.05,152.99,299.13,7.86"><row><cell>accuracy 0.861 0.743 0.814 0.851 0.798 0.825 0.830 0.875 0.712 0.683 0.802</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,134.77,176.20,345.84,125.94"><head>Table 2 .</head><label>2</label><figDesc>The accuracy on per object class of validation data In the ultimate submission for classifying test sequence, we mix the training sequence and the validation sequence into a dataset for training and classify the test sequence. Scene classification and object recognition are realized separately.</figDesc><table coords="4,134.77,198.13,318.57,59.89"><row><cell cols="3">Object Ext Chair Printer Bookshelf Urinal Trash Phone Fridge Total</cell></row><row><cell>accuracy 0.842 0.815 0.713</cell><cell>0.741</cell><cell>0.831 0.804 0.672 0.710 0.798</cell></row><row><cell cols="2">4.2 Results on the test sequence</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,177.79,346.97,259.77,84.64"><head>Table 3 .</head><label>3</label><figDesc>The final results</figDesc><table coords="4,177.79,367.16,259.77,64.45"><row><cell>#</cell><cell>Group</cell><cell cols="3">Score Rooms Score Objects SCORE TOTAL</cell></row><row><cell>1</cell><cell>NUDT</cell><cell>1075.50</cell><cell>3357.75</cell><cell>4430.25</cell></row><row><cell>2</cell><cell>UFMS</cell><cell>219.00</cell><cell>1519.75</cell><cell>1738.75</cell></row><row><cell cols="2">3 Baseline Results</cell><cell>67.5</cell><cell>186.25</cell><cell>253.75</cell></row><row><cell>4</cell><cell>AEGEAN</cell><cell>-405</cell><cell>-995</cell><cell>-1400</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,138.35,646.84,342.23,7.86;4,146.91,657.80,333.67,7.86;5,146.91,120.67,333.68,7.86;5,146.91,131.63,45.06,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,311.15,646.84,169.42,7.86;4,146.91,657.80,195.42,7.86">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,360.91,657.80,119.68,7.86;5,146.91,120.67,44.75,7.86">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,142.59,342.24,7.86;5,146.91,153.55,223.32,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,211.44,142.59,227.56,7.86">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName coords=""><surname>Davidg</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,447.52,142.59,33.07,7.86;5,146.91,153.55,138.34,7.86">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,164.51,342.22,7.86;5,146.91,175.47,333.67,7.86;5,146.91,186.42,267.89,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,254.51,164.51,222.12,7.86">Histograms of oriented gradients for human detection</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,158.58,175.47,169.18,7.86;5,360.34,175.47,120.24,7.86;5,146.91,186.42,87.45,7.86">CVPR 2005. IEEE Computer Society Conference on</title>
		<imprint>
			<date type="published" when="2005-06">2005. June 2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
	<note>Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct coords="5,138.35,197.38,342.23,7.86;5,146.91,208.34,333.67,7.86;5,146.91,219.30,99.01,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,301.24,197.38,179.34,7.86;5,146.91,208.34,18.19,7.86">Image classification using random forests and ferns</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Muoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,183.84,208.34,67.56,7.86;5,283.42,208.34,197.16,7.86;5,146.91,219.30,8.29,7.86">ICCV 2007. IEEE 11th International Conference on</title>
		<imprint>
			<date type="published" when="2007-10">2007. Oct 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct coords="5,138.35,230.26,342.23,7.86;5,146.91,241.22,333.68,7.86;5,146.91,252.18,302.73,9.85" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,306.71,230.26,173.87,7.86;5,146.91,241.22,23.08,7.86">LIBSVM: A library for support vector machines</title>
		<author>
			<persName coords=""><forename type="first">Chih-Chung</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/~cjlin/libsvm" />
	</analytic>
	<monogr>
		<title level="j" coord="5,181.63,241.22,238.72,7.86">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">27</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,263.14,342.23,7.86;5,146.91,271.83,333.67,10.13;5,146.91,285.05,333.66,7.86;5,146.91,296.01,333.67,7.86;5,146.91,306.97,134.10,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,350.60,285.05,129.98,7.86;5,146.91,296.01,87.38,7.86">ImageCLEF 2014: Overview and analysis of the results</title>
		<author>
			<persName coords=""><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jesus</forename><surname>Martinez-Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mauricio</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Burak</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Novi</forename><surname>Patricia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Neda</forename><surname>Marvasti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Suzan</forename><surname>ÃœskÃ¼darlÄ±</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roberto</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Miguel</forename><surname>Cazorla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ismael</forename><surname>Garcia-Varea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vicente</forename><surname>Morell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,255.04,296.01,71.25,7.86">CLEF proceedings</title>
		<title level="s" coord="5,333.66,296.01,142.95,7.86">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,317.93,342.23,7.86;5,146.91,328.89,333.68,7.86;5,146.91,339.85,202.05,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="5,146.91,328.89,217.91,7.86">Overview of the ImageCLEF 2014 Robot Vision Task</title>
		<author>
			<persName coords=""><forename type="first">Jesus</forename><surname>Martinez-Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Miguel</forename><surname>Cazorla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ismael</forename><surname>Garcia-Varea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vicente</forename><surname>Morell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,386.72,328.89,93.87,7.86;5,146.91,339.85,77.14,7.86">CLEF 2014 Evaluation Labs and Workshop</title>
		<title level="s" coord="5,232.09,339.85,88.62,7.86">Online Working Notes</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
