<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,141.54,116.95,332.27,12.62;1,155.54,134.89,304.28,12.62;1,197.48,152.82,220.40,12.62">Towards Content-Based Image Retrieval: From Computer Generated Features to Semantic Descriptions of Liver CT Scans</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,229.33,190.49,72.51,8.74"><forename type="first">Assaf</forename><forename type="middle">B</forename><surname>Spanier</surname></persName>
							<email>assaf.spanier@mail.huji.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hebrew University of Jerusalem</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,324.53,190.49,61.50,8.74;1,157.83,212.05,15.87,7.86"><forename type="first">Leo</forename><forename type="middle">Joskowicz</forename><surname>The</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hebrew University of Jerusalem</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,224.77,212.05,48.51,7.86"><forename type="first">Selim</forename><surname>Benin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hebrew University of Jerusalem</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,141.54,116.95,332.27,12.62;1,155.54,134.89,304.28,12.62;1,197.48,152.82,220.40,12.62">Towards Content-Based Image Retrieval: From Computer Generated Features to Semantic Descriptions of Liver CT Scans</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DDEBD424DECF8C76C49D8CECF2A987DC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The rapid increase of CT scans and the limited number of radiologists present a unique opportunity for computer-based radiological Content-Based Image Retrieval (CBIR) systems. However, the current structure of the clinical diagnosis reports presents substantial variability, which significantly hampers the creation of effective CBIR systems. Researchers are currently looking for ways of standardizing the reports structure, e.g., by introducing uniform User Express (UsE) annotations and by automating the extraction of UsE annotations with Computer Generated (CoG) features. This paper presents an experimental evaluation of the derivation of UsE annotations from CoG features with a classifier that estimates each UsE annotation from the input CoG features. We used the datasets of the ImageCLEF-Liver CT Annotation challenge: 50 training and 10 testing CT scans with liver and liver lesion annotations. Our experimental results on the ImageCLEF-Liver CT Annotation challenge exhibit a completeness level of 95% and accuracy of 91% for 10 unseen cases. This is the second best result obtained in the Liver CT Annotation challenge and only 1% away from the rst place.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>About 68 million CT scans are performed in the USA each year. Clinicians are struggling under the burden of diagnosis and follow-up of such an immense amount of scans. This phenomenon gave rise to a plethora of methods to improve and assist clinicians with the diagnosis process.</p><p>Content based image retrieval (CBIR) is a growing and popular research topic <ref type="bibr" coords="1,159.81,573.43,9.96,8.74" target="#b7">[8]</ref>. The goal of CBIR is to assist physicians with the diagnosis of tumors or other pathologies by finding similar cases to the case at hand. Therefore, CBIR requires efficient search capabilities in a huge database of medical images. The matching criteria are based on image properties and features extracted from image and pathology <ref type="bibr" coords="1,229.14,621.25,10.52,8.74" target="#b0">[1]</ref> and on searching in the clinical reports database.</p><p>Besides the known problem with diagnosis and follow-up of this huge number of scans, there is substantial variability in the structure of the clinical reports provided by the clinicians for each case. This variability hampers the ability to establish an efficient and consistent CBIR system, as uniform reports structure is a major need for such an application.</p><p>The task of standardization the clinical reports for liver and liver lesions has recently been proposed by Kokciyan et al. <ref type="bibr" coords="2,315.92,156.34,9.96,8.74" target="#b0">[1]</ref>. The ONLIRA ontology constitutes a standard that is used to generate multiple-choice User Express (UsE) annotations <ref type="bibr" coords="2,159.13,180.25,10.52,8.74" target="#b8">[9]</ref> consisting of features that clinically characterize the liver and the liver lesion. Note that the UsE annotations are provided by the radiologist, as they cannot be extracted automatically from the image itself. However, the image descriptors, called Computer Generated (CoG) features, can be automatically derived from the image with image processing algorithms <ref type="bibr" coords="2,388.00,228.07,9.96,8.74" target="#b8">[9]</ref>.</p><p>The goal of this work is therefore to use the CoG features to automatically generate the UsE annotations. A major part of this work deals with designing and building a machine learning algorithm that link CoG features to UsE annotations. Training datasets provided by the ImageCLEF-Liver CT challenge <ref type="bibr" coords="2,134.77,288.34,10.52,8.74" target="#b5">[6]</ref> which is part of the ImageCLEF-2014 evaluation campaign <ref type="bibr" coords="2,416.96,288.34,9.96,8.74" target="#b6">[7]</ref>. Additional contributions of this work consist of extending the CoG available features and optimal selection of the most relevant ones to the liver task. Experimental results on the ImageCLEF-Liver CT Annotation challenge exhibit estimation of UsE annotations at a completeness level of 95% and accuracy of 91% for 10 unseen cases. This is the second best result obtained in the Liver CT Annotation challenge <ref type="bibr" coords="2,177.51,360.07,10.52,8.74" target="#b5">[6]</ref> which is part of the ImageCLEF-2014 evaluation campaign <ref type="bibr" coords="2,450.88,360.07,10.52,8.74" target="#b6">[7]</ref> and only 1% away from the first place. We describe each step in detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data Collection</head><p>The input of our algorithm is a set of 50 datasets provided by the imageCLEF 2014 Liver Annotation Task data and has been collected by CaReRa Project (TUBITAK Grant 110E264), Bogazici University, EE Dept., Istanbul, Turkey (www.vavlab.ee.boun.edu.tr). Each dataset includes:</p><p>1. A CT scan that consists the liver region and liver tumors 2. A segmentation of the liver 3. The lesion's bounding box 4. A set of 60 Computer Generated features (CoG) features 5. A set of 73 User Express (UsE) annotations The CoG features can be divided to global image descriptors and pathology descriptors. The global image descriptors cover the basic and liver-wide global statistical properties, such as the mean and variance of the gray-level values, and the liver volume. They are extracted directly from the CT scans and the associated segmentation. The pathology descriptors are computed for each liver lesion. They reflect finer levels of visual information related to individual lesions. UsE annotations are also divided into global and pathology descriptors. Global descriptors are divided into Liver and Vessel groups. These two groups include annotations about the liver itself and its hepatic vasculature. Pathol-  We added 9 features to the 21 pathology features to describe the statistics of the lesion itself. The new features are derived from a refined segmentation of the liver obtained by thresholding the given lesion bounding box with its mean gray level followed by morphological operations. The added CoG features are:</p><p>1. The average gray level intensity values of the healthy part of the liver.</p><p>(LiverGrayMean) 2. The standard deviation of gray level intensity values of the healthy part of the liver. (LiverGrayStd) 3. The average gray level intensity values of the lesion. (LesionGrayMean) 4. The standard deviation of gray level intensity values of the lesion. (Le-sionGrayStd) 5. The lesion's contour mean gray levels. (LesionBounderyGrayMean) 6. The standard deviation of the lesion's contour gray levels.</p><p>(LesionBounderyGrayStd) 7. The average gray level difference between the healthy part of the liver and the lesion. (LesionLiverGrayDiff ) 8. The average gray level difference between the healthy part of the liver and the lesion' contour. (BounderyLiverGrayDiff ) 9. The average gray level difference between the the lesion and its contour.</p><p>(lesionBounderyGrayDiff )</p><p>The result is a modified CoG list with 18 global image descriptors and 30 pathology descriptors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Model Selection</head><p>In this section the classification algorithm to be evaluated is presented: Predictive models can be characterized by two properties: parametric/non-parametric and generative/discriminative. Parametric models have a fixed number of parameters and have the advantage of often being faster to use. However, they tend to rely on stronger assumptions about the nature of the data distributions. In non-parametric classifiers, the number of parameters grows with the size of the training data. Non-parametric classifiers are more flexible but are often computationally intractable for large datasets. As to the generative/discriminative property, the main focus of generative generative models, is not the classification task but to correctly model the probabilistic model. They are called generative since sampling can generate synthetic data points. Discriminative models, however, do not attempt to model the underlying probability distributions, but rather focus on the given task, i.e. the classification itself. Therefore, they may achieve better performance in terms of overall accuracy of the classification task. In general, when the probabilistic distribution assumptions made are correct, the generative model requires less training data than discriminative methods to provide the same performance, but if the probabilistic assumptions are incorrect, discriminative methods will do better <ref type="bibr" coords="6,178.24,335.22,9.96,8.74" target="#b3">[4]</ref>. Table <ref type="table" coords="6,225.57,335.22,4.24,8.74" target="#tab_1">2</ref>.3 shows the characteristics of each classifier.</p><p>For real world datasets, so far there is no theoretically correct, general criterion for choosing between the different models. Therefore, we examined four classifiers, representative of the 4 different families of models <ref type="bibr" coords="6,387.84,372.10,9.96,8.74" target="#b2">[3]</ref>: K-Nearest Neighbors (KNN), Linear Discriminant Analysis (LDA), Logistic Regression (LR), Support Vector Machine (SVM). Note that for each UsE annotation, the outcome of each predictive model consists of classification and subset selection of the optimal CoG features. Therefore, the selection of the CoG features is unique for each model.</p><p>We used the Python scikit-Learn Machine learning tool <ref type="bibr" coords="6,399.92,444.84,10.52,8.74" target="#b4">[5]</ref> to examine the four selected classifiers. For each UsE descriptor, the best predicting classifier and its features was based on leave-one-out cross validation with exhaustive search -i.e. systematically enumerating all possible Combinations CoG features. Note that since we develop two distinct classication models, one for the 18 global CoG features and one for the 30 CoG pathology descriptors, the exhaustive search was performed for each CoG group separately. Three UsE features (Cluster size, Lobe and Segment) were estimated from the image itself and were not part of the learning process (Section 2.6).</p><p>For simplicity, each model was tested with a set of default parameters as define by scikit-learn package <ref type="bibr" coords="6,265.75,565.40,9.96,8.74" target="#b4">[5]</ref>. The parameters for each model are:</p><p>-KNN: K=5, Distance: euclidean distance with no threshold for shrinking.</p><p>-LDA: Euclidean distance, regularization strength of 1.0.</p><p>-LR: L2 penalty, regularization strength of 1.0, tolerance for stopping criteria is 0.0001. -SVM: Penalty parameter of 1.0, RBF kernel with degree of 3 and gamma of 0, tolerance for stopping criteria is 0.0001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Training</head><p>Once the classifier which produced the highest classification was found in the previous step, we trained it using all 50 cases. As a result, for each UsE, we obtain a trained model, consisting of classifier along with optimized sets of CoG features (i.e. the selected features)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Evaluation</head><p>The evaluating phase of the challenge consists of the estimation of UsE annotation from the given CoG features and the images. Unlike the training phase, UsE annotation are not given here to examine our accuracy.</p><p>To apply the resulting classifier in the testing phase on an unseen dataset, we first extract and extend the CoG features according to a scheme described in Section 2.3. Then, for each test case, we apply the prediction model -the one with the highest score -according to the training phase results. The result is a UsE annotation for each unseen case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Estimation of the Lesion Lobe, Lesion Segment, and Cluster Size</head><p>As noted in Section 2.3, the Cluster Size (i.e. the number of lesions inside the lesion bounding box), the Lesion Lobe and the Lesion Segment containing the lesion were not part of the general learning process but were rather estimated from the image itself. The estimation of these fields was performed as follows:</p><p>For the Lesion Lobe: we measure the center of the lesion and the liver. The lesion lobe is estimated as the right lobe if the lesion center is on the right part of the liver, and as the left lobe if the lesion center is on the left part of the liver. In case they were overlapping we estimated it to be the Caudate Lobe.</p><p>The Lesion Segment is estimated as follows: If at a previous stage the estimation was that the lesion is on the right lobe, we assessed that the lesion is located on the fourth segment. Alternatively, if at the previous stage the estimation was that the lesion is on the left lobe, we analyzed whether the lesion is located above or below the center of the liver. If it is located above the center of the liver, we assessed that the lesion is in segment 5-6; and if it is located below the center of the liver -we assessed that the lesion is on segment 7-8.</p><p>For the Cluster Size: We define the Cluster Size to the number of lesion -the value listed in the CoG field NumberofLesions. Except in cases when the value listed in number of lesion is higher than 6, and then we define that the number of lesions to be 6. For the Cluster Size: We define the Cluster Size to the number of lesion -the value listed in the CoG field NumberofLesions. Except in cases when the value listed in number of lesion is higher than 6, and then we define that the number of lesions to be 6.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,134.77,404.94,66.34,10.52;2,134.77,432.68,345.84,8.74;2,134.77,444.63,345.84,8.74;2,134.77,456.59,345.84,8.74;2,134.77,468.54,345.84,8.74;2,134.77,480.50,345.84,8.74;2,134.77,492.45,172.09,8.74"><head>2 MethodA</head><label>2</label><figDesc>major part of this work would deal with developing a machine learning algorithm that would best link CoG features to UsE annotations based on training data-sets. Developing a machine learning algorithm involves four main steps [2]: (1) Data collection; (2) Feature extraction; (3) Model selection/Fitting classifier parameters and; (4) Training the selected model. A diagram illustrating the phases of the process is shown in Fig 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,134.77,452.98,345.82,7.89;3,134.77,463.96,345.81,7.86;3,134.77,474.92,345.81,7.86;3,134.77,485.88,345.81,7.86;3,134.77,496.84,345.81,7.86;3,134.77,507.80,345.81,7.86;3,134.77,518.76,315.34,7.86;3,161.05,116.83,293.25,321.38"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Four main steps in designing and building a machine learning algorithm for the estimation of UsE from CoG and CT images: (a) Data collection: the input to our system consists of CT scans and CoG features and UsE annotation ; (b) Feature extraction: only the most informative CoG features are selected; (c) Model selection: Estimating the most appropriate model for the UsE annotations from CoG features. The model is selected after testing a variety of prediction models and; (d) Training the selected model: The selected models parameters are trained using all 50 cases.</figDesc><graphic coords="3,161.05,116.83,293.25,321.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,134.77,117.41,345.84,189.49"><head>Table 1 .</head><label>1</label><figDesc>Example of the Computer Generated features (CoG) features. The CoG can be divided into two main Descriptor type (a) Global -liver, vessel and all lesions annotations. (b) Pathology -the selected annotated lesion ogy descriptors include two groups: Lesion and Lesion Component and contain annotations about the selected lesion in the liver.A complete list of all UsE and CoG features with their associated descriptor type can be found at<ref type="bibr" coords="4,231.20,286.20,9.96,8.74" target="#b5">[6]</ref>. Representative examples of CoG and UsE are shown in Table2.1 and Table2.1 respectively.</figDesc><table coords="4,150.82,160.59,313.71,54.52"><row><cell cols="3">Descriptors type Group Name</cell><cell>Type</cell><cell>Value</cell></row><row><cell></cell><cell>Liver</cell><cell>LiverVolume</cell><cell>double</cell><cell>12987.6</cell></row><row><cell>Global</cell><cell>Vessel</cell><cell>LiverVariance</cell><cell>double</cell><cell>297.683</cell></row><row><cell></cell><cell cols="3">AllLesions NumberofLesions int</cell><cell>5</cell></row><row><cell>Pathology</cell><cell>Lesion</cell><cell cols="3">HaarWaveletCoef VectorOfDouble 8.4, 3.9, 2.1,</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,134.77,328.90,345.84,336.96"><head>Table 2 .</head><label>2</label><figDesc>Example of the User Express (UsE) annotations . The UsE can be divided into two main groups (a) Global -liver and vessel annotations (b) Pathology -lesion annotationsIn this step we aim at extracting the optimal set of CoG features for the estimation of each UsE annotation. Note that due to the diversity of lesions between and within patients, estimating pathology descriptors is a much more challenging task than estimating global descriptors. Thus, our analysis includes developing two distinct classification models: one for the global CoG features and one for the CoG pathology descriptors.First, we reduce problem dimensionality by omitting 21 high dimension features from the 60 provided CoG (e.g. FourierDescriptors, BoundaryScaleHistogram, BoundaryWindowHistogram etc.). Thus, our analysis includes only CoG features with scalar values. This results in 39 features divided between global and pathology related features.</figDesc><table coords="4,134.77,369.84,337.21,159.05"><row><cell>Descriptors Type</cell><cell>Group Concept</cell><cell>Properties</cell><cell>Values</cell><cell>Indices</cell></row><row><cell>Global</cell><cell>Liver Right Lob Vessel Hepatic Artery</cell><cell cols="2">Right Lobe Size Change Normal Hepatic Artery Lumen Diameter normal</cell><cell>2 2</cell></row><row><cell></cell><cell>Lesion Lesion</cell><cell>is Close to Vein</cell><cell>Other</cell><cell>8</cell></row><row><cell>Pathology</cell><cell>Lesion Lesion</cell><cell>Segment</cell><cell>SegmentI, SegmentII</cell><cell>1,2</cell></row><row><cell></cell><cell>Lesion Capsule</cell><cell cols="2">Is Calcified?(Capsule) True</cell><cell>1</cell></row><row><cell cols="2">2.2 Feature Extraction</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,136.16,116.91,346.44,59.40"><head>Table 3 .</head><label>3</label><figDesc>Four classifiers examined</figDesc><table coords="6,136.16,146.52,346.44,29.78"><row><cell></cell><cell>Generative</cell><cell>Discriminative</cell></row><row><cell cols="2">Parametric Linear Discriminant Analysis (LDA)</cell><cell>Logistic Regression (LR)</cell></row><row><cell>Non-Parametric</cell><cell>K-Nearest Neighbors (KNN),</cell><cell>Support Vector Machine (SVM)</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>Experimental results of applying our method to the ImageCLEF-Liver CT Annotation challenge datasets results with estimation of UsE annotations at a completeness level of 95% and accuracy of 91% for 10 unseen cases.</p><p>Training and test results for each of the classifiers are shown in Table <ref type="table" coords="8,465.11,444.98,3.87,8.74">2</ref>.6. Due to the simplicity of estimating the global UsE annotations, we present their result per group. A detailed presentation is provided for the pathology related features, which is indeed a much more challenging task. It can be seen that all classifiers successfully estimated the global features.</p><p>As mentioned, three additional features were estimated from the images and were not part of the learning process. These features are: ClusterSize, Lesion-Lobe, LesionSegment and their accuracy on the training datasets was 0.75, 0.9, 0.7 respectively. The completeness level of our method is 0.95 due to omitting 3 UsE annotations from the analysis: LesionComposition, Shape and the Margen-Type.</p><p>The optimized sets of CoG features (i.e. the selected features) that were obtained by the model with the highest score after the leave-one-out procedure are shown in Table <ref type="table" coords="8,224.65,600.40,3.87,8.74">5</ref>. Note that the set of added features (Section 2.2) were indeed selected by the model, which proves their necessity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and future work</head><p>We have presented an approach to estimate UsE annotations from CoG features and associated CT scans. We extended the CoG features with 9 additional features to enhance the learning process. In the ImageCLEF-Liver CT Annotation challenge. Our approach provides an average accuracy level of 91% with completeness level of 95% when applied on 10 unseen test cases. This work provides reliable estimation of uniform clinical report from imaging features and therefore it constitutes another step toward automatic CBIR system by enabling efficient search in clinical reports. Future work consists of examining an additional set of classifiers and extending the completeness of our algorithm to estimate the full UsE annotations set and values (e.g. estimating segment 1-3 of the Lesion Segment feature )</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="10,138.35,143.59,342.23,7.86;10,146.91,154.54,333.66,7.86;10,146.91,165.50,234.16,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,450.88,143.59,29.70,7.86;10,146.91,154.54,256.52,7.86">Semantic Description of Liver CT Images: An Ontological Approach</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kokciyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Turkay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Uskudarli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Yolum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Bakir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Acar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,412.25,154.54,68.33,7.86;10,146.91,165.50,139.53,7.86">IEEE Journal of Biomedical and Health Informatics</title>
		<imprint>
			<biblScope unit="volume">2194</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,176.46,342.23,7.86;10,146.91,187.42,148.61,7.86" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="10,369.24,176.46,83.98,7.86">Pattern classification</title>
		<author>
			<persName coords=""><forename type="first">Richard</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">G</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Stork</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>John Wiley</publisher>
			<biblScope unit="page">10</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,198.38,342.23,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="10,222.05,198.38,178.14,7.86">Machine learning: a probabilistic perspective</title>
		<author>
			<persName coords=""><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,209.34,342.23,7.86;10,146.91,220.30,333.66,7.86;10,146.91,231.26,45.06,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,199.95,209.34,280.63,7.86;10,146.91,220.30,104.57,7.86">On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,258.49,220.30,203.15,7.86">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">841</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,242.22,342.22,7.86;10,146.91,253.17,333.66,7.86;10,146.91,264.13,333.66,7.86;10,146.91,275.09,286.56,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,356.29,264.13,124.29,7.86;10,146.91,275.09,38.29,7.86">Scikit-learn: Machine Learning in Python</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Duchesnay</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,192.86,275.09,153.42,7.86">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">28252830</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,283.78,342.24,10.13;10,146.91,297.01,333.66,7.86;10,146.91,307.97,204.34,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,161.72,297.01,187.12,7.86">imageCLEF Liver CT Image Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Marvasti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kökciyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Türkay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Yazı</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Yolum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Üsküdarlı</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Acar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,387.75,297.01,92.83,7.86;10,146.91,307.97,78.17,7.86">CLEF 2014 Evaluation Labs and Workshop</title>
		<title level="s" coord="10,232.91,307.97,89.66,7.86">Online Working Notes</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,318.93,342.23,7.86;10,146.91,327.62,333.67,10.13;10,146.91,340.85,333.66,7.86;10,146.91,351.80,48.91,7.86" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Martinez-Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Patricia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Marvasti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Üsküdarlı</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cazorla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Garcia-Varea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Morell</surname></persName>
		</author>
		<title level="m" coord="10,161.83,340.85,219.75,7.86">ImageCLEF 2014: Overview and analysis of the results</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,362.76,342.22,7.86;10,146.91,373.72,333.66,7.86;10,146.91,384.68,203.77,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,146.91,373.72,329.89,7.86">Content-based image retrieval in radiology: current status and future directions</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">B</forename><surname>Akgl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">L</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Napel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">F</forename><surname>Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Acar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,146.91,384.68,105.19,7.86">Journal of Digital Imaging</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="208" to="222" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,395.64,342.22,7.86;10,146.91,406.60,333.67,7.86;10,146.91,417.56,333.66,7.86;10,146.91,428.52,247.90,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,249.58,406.60,212.29,7.86">Clinical experience sharing by similar case retrieval</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Barzegar Marvasti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">B</forename><surname>Akgl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kkciyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Skdarl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Yolum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Trkay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Bakr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,146.91,417.56,333.66,7.86;10,146.91,428.52,141.67,7.86">Proceedings of the 1st ACM international workshop on Multimedia indexing and information retrieval for healthcare</title>
		<meeting>the 1st ACM international workshop on Multimedia indexing and information retrieval for healthcare</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="67" to="74" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
