<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,157.31,116.75,300.72,12.62;1,266.41,134.69,82.54,12.62">CPPP/UFMS at ImageCLEF 2014: Robot Vision Task</title>
				<funder>
					<orgName type="full">CNPq</orgName>
				</funder>
				<funder>
					<orgName type="full">PET-Fronteira</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,136.22,172.44,48.04,8.74"><forename type="first">Rodrigo</forename><surname>De</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Federal University of Mato Grosso do Sul -Ponta Porã Campus Rua Itibiré</orgName>
								<address>
									<addrLine>Vieira, s/n, CEP</addrLine>
									<postCode>79907-414</postCode>
									<settlement>Ponta Porã -MS</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,187.58,172.44,69.23,8.74"><forename type="first">Carvalho</forename><surname>Gomes</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Federal University of Mato Grosso do Sul -Ponta Porã Campus Rua Itibiré</orgName>
								<address>
									<addrLine>Vieira, s/n, CEP</addrLine>
									<postCode>79907-414</postCode>
									<settlement>Ponta Porã -MS</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,265.50,172.44,86.66,8.74"><forename type="first">Lucas</forename><forename type="middle">Correia</forename><surname>Ribas</surname></persName>
							<email>lucascorreiaribas@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Federal University of Mato Grosso do Sul -Ponta Porã Campus Rua Itibiré</orgName>
								<address>
									<addrLine>Vieira, s/n, CEP</addrLine>
									<postCode>79907-414</postCode>
									<settlement>Ponta Porã -MS</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,360.04,172.44,119.11,8.74;1,236.91,184.40,26.24,8.74"><forename type="first">Amaury</forename><surname>Antônio De Castro Junior</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Federal University of Mato Grosso do Sul -Ponta Porã Campus Rua Itibiré</orgName>
								<address>
									<addrLine>Vieira, s/n, CEP</addrLine>
									<postCode>79907-414</postCode>
									<settlement>Ponta Porã -MS</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,270.84,184.40,107.61,8.74"><forename type="first">Wesley</forename><forename type="middle">Nunes</forename><surname>Gonçalves</surname></persName>
							<email>wesley.goncalves@ufms.br</email>
							<affiliation key="aff0">
								<orgName type="institution">Federal University of Mato Grosso do Sul -Ponta Porã Campus Rua Itibiré</orgName>
								<address>
									<addrLine>Vieira, s/n, CEP</addrLine>
									<postCode>79907-414</postCode>
									<settlement>Ponta Porã -MS</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,157.31,116.75,300.72,12.62;1,266.41,134.69,82.54,12.62">CPPP/UFMS at ImageCLEF 2014: Robot Vision Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0BEFD631C26CC07926E805703328FFA6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Scene recognition</term>
					<term>object recognition</term>
					<term>spatial pyramid matching</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of the CPPP/UFMS group in the robot vision task. We have applied the spatial pyramid matching proposed by Lazebnik et al. This method extends bag-of-visualwords to spatial pyramids by concatenating histograms of local features found in increasingly fine sub-regions. To form the visual vocabulary, kmeans clustering was applied in a random subset of images from training dataset. After that the images are classified using a pyramid match kernel and the k-nearest neighbors. The system has shown promising results, particularly for object recognition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, robotics has achieved important advances, such as the intelligent industrial devices that are increasingly accurate and efficient. Despite the recent advances, most robots still represent the surrounding environment by means of a map with information about obstacles and free spaces. To increase the complexity of autonomous tasks, robots should be able to get a better understanding of images. In particular, the ability to identify scenes such as office, kitchen, as well as objects, is an important step to perform complex tasks <ref type="bibr" coords="1,438.02,541.47,9.96,8.74" target="#b0">[1]</ref>. Thus, place localization and object recognition becomes a fundamental part of image understanding for robot localization <ref type="bibr" coords="1,295.14,565.39,9.96,8.74" target="#b1">[2]</ref>.</p><p>This paper presents the participation of our group in the 6th edition of the Robot Vision challenge<ref type="foot" coords="1,236.22,587.81,3.97,6.12" target="#foot_0">1</ref>  <ref type="bibr" coords="1,244.42,589.38,10.52,8.74" target="#b2">[3,</ref><ref type="bibr" coords="1,256.60,589.38,7.01,8.74" target="#b3">4]</ref>. This challenge addresses the problem of semantic place classification and object recognition. For this task, the bag-of-visualwords approach (BOW) <ref type="bibr" coords="1,241.12,613.29,10.52,8.74" target="#b4">[5,</ref><ref type="bibr" coords="1,253.30,613.29,7.75,8.74" target="#b5">6]</ref> is one of the most promising approaches available. Although the approach have advantages, it also has one major drawback, the absence of spatial information. To overcome this drawback, a spatial pyramid framework combined with local features extractors, such as such as Scale Invariant Feature Transform (SIFT) <ref type="bibr" coords="2,268.97,130.95,10.52,8.74">[7]</ref> and Speeded Up Robust Features (SURF) <ref type="bibr" coords="2,467.32,130.95,9.96,8.74" target="#b7">[8]</ref>, was proposed by Lazebnik et al. <ref type="bibr" coords="2,276.46,142.90,9.96,8.74" target="#b8">[9]</ref>. This method showed significantly improved performance on challenging scene categorization tasks. The image recognition system used in our participation is based on the improved BOW and multi-class classifiers.</p><p>Experimental results have shown that the image recognition system provides promising results, in particular to the object recognition task. Among four systems, the proposed system ranked second using the number of cluster k = 400 and number of images M = 150 for training the vocabulary.</p><p>This paper is described as follows. Section 2 presents the image recognition system used by our group in the robot vision challenge. The experiments and results of the proposed system are described in Section 3. Finally, conclusions and future works are discussed in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Image Recognition System</head><p>In this section, we describe the image recognition system used in the challenge. This system can be described into 3 steps: i) feature extraction; ii) spatial pyramid matching; iii) classification. The following sections describe each step in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Feature Extraction</head><p>In the feature extraction step, the system extracts SIFT descriptors from 16 × 16 patches computed over a grid with spacing of 8 pixels. For each patch i, 128 descriptors are calculated, i.e., it is calculated a vector ϕ i ∈ 128 . To train the visual vocabulary, we perform k-means clustering of a random subset of descriptors D = {ϕ} from the training set according to Equation 1. Throughout the paper, the number of clusters will be referred to as k and the size of the random subset of images will be referred to as M .</p><formula xml:id="formula_0" coords="2,271.11,539.83,209.49,8.74">C = k-means(D)<label>(1)</label></formula><p>where C ∈ Re k×128 represents the clusters. Then, each vector descriptor ϕ i is associated to the closest cluster according to the Euclidean distance (Equation <ref type="formula" coords="2,298.62,586.03,3.87,8.74">2</ref>). The index associated is usually called visual word in the bag-of-visual-word approach.</p><formula xml:id="formula_1" coords="2,264.39,623.17,216.20,21.04">λ i = arg k min j=1 |ϕ i , C i | (2)</formula><p>where |.| is the Euclidean distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Spatial Pyramid Matching</head><p>The pyramid matching was proposed to find an approximate correspondence between two sets, such as histograms. It works placing a sequence of grids over the space and calculating a weighted sum of the number of matches. Consider a sequence of grids at resolution l = 0, . . . , L. A grid at level l has 2 dl cells, where d is the space dimension which in our case is d = 2. In each cell i, it calculates the histogram H l (i) of visual words λ. The number of matches at level l for two image X e Y is given by:</p><formula xml:id="formula_2" coords="3,246.98,230.55,233.61,31.97">I l = 2 dl i=1 min (H l X (i), H l Y (i))<label>(3)</label></formula><p>In order to penalize matches at larger cells, the pyramid match kernel between images X e Y , considering all levels l, is given by:</p><formula xml:id="formula_3" coords="3,235.05,300.07,245.55,30.55">κ L (X, Y ) = 1 2 L I 0 + L l=1 1 2 L-l+1 I l<label>(4)</label></formula><p>The kernel above is calculated to each visual word, such that:</p><formula xml:id="formula_4" coords="3,247.96,361.03,232.64,30.32">K L (X, Y ) = k j=1 κ L (X j , Y j )<label>(5)</label></formula><p>where X j indicates that the kernel will consider only the visual word j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Classification</head><p>The multi-class classification is done with the k-nearest neighbor using the kernel K L described above. Given a test image, it calculates the kernel value for all training images and assigns the room/category of the closest training image. The same procedure is done for object recognition, i.e., it is detected the presence of the objects of the closest training image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>In this section we describe the experiments and results of the proposed system. To train the system, we have used 5000 visual images divided into 10 rooms/categories: Corridor, Hall, ProfessorOffice, StudentOffice, TechnicalRoom, Toilet, Secretary, VisioConference, Warehouse, ElevatorArea. An example of each category can be seen in Figure <ref type="figure" coords="3,291.21,596.34,3.87,8.74">1</ref>. The train dataset also provides 8 objects: Extinguisher, Phone, Chair, Printer, Urinal, Bookself, Trash, Fridge. Examples of images containing each of the objects can be seen in Figure <ref type="figure" coords="3,405.56,620.25,3.87,8.74">2</ref>. The number of images for each room/category and object is summarized in Table <ref type="table" coords="3,426.74,632.21,3.87,8.74" target="#tab_0">1</ref>.</p><p>To test the proposed system, we have used the validation dataset composed by 1500 images. The results for different values of number of cluster k and images  used to obtain the vocabulary M can be seen in Table <ref type="table" coords="5,380.50,299.14,3.87,8.74" target="#tab_1">2</ref>. The final size of the descriptor is given by k × L l=0 2 2l . For L = 2 and k = 300, the final size is 6300. Despite high number of descriptors, the system takes on average 0.9545 seconds to process an image. For each image, our system provided the room category and the presence of objects. The scores shown in the table are the sum of all the scores obtained for the images. The rules shown in Table <ref type="table" coords="5,408.74,358.92,4.98,8.74" target="#tab_2">3</ref> are used when calculating the final score for an image. The room category has been correctly classified: +1.0 points The room category has been wrongly classified:</p><p>-0.5 points The room category has not been classified: 0.0 points For each object correctly detected (True Positive): +1.0 points For each object incorrectly detected (False Positive):</p><p>-0.25 points For each object correctly detected as not present (True Negative): 0.0 points For each object incorrectly detected as not present (False Negative): -0.25 points Finally, the Table <ref type="table" coords="6,228.16,118.99,4.98,8.74" target="#tab_3">4</ref> shows the results for all participations in the robot vision challenge. Three groups have submitted solutions to this challenge and the baseline indicates the results obtained by the dataset provided script (Color &amp; Depth histogram + SVM). For the competition we submitted four runs with different values for the parameters k and M (see Section 2.1). Our system ranked second, achieving 1738.75 points on this task for k = 400 and M = 150. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>This paper described the participation of our group in the Robot vision challenge. In this challenge, the proposed system ranked second among four others systems. Thus, the image recognition system has shown promising results, particularly for object recognition.</p><p>As work, we intend to extend the approach for color images and use other classifiers, such as Support Vector Machine, which it is known to provide better results than k-nearest neighbors. In addition, the system will be applied in the depth images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,134.77,354.45,345.83,7.89;4,134.77,365.44,42.29,7.86"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>Fig. 1. Example of each room/category from the training dataset. Images have 640 × 480 pixels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,134.77,115.91,345.81,147.87"><head>Table 1 .</head><label>1</label><figDesc>Number of images from the training dataset for the room and object recognition.</figDesc><table coords="5,149.85,145.93,312.59,117.85"><row><cell>Category</cell><cell cols="2">N. Images Perc. (%)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Corridor</cell><cell>1833</cell><cell>36.66</cell><cell>Object</cell><cell cols="2">N. Images Perc. (%)</cell></row><row><cell>Hall</cell><cell>306</cell><cell>6.12</cell><cell>Extinguisher</cell><cell>770</cell><cell>15.40</cell></row><row><cell>ProfessorOffice</cell><cell>355</cell><cell>7.10</cell><cell>Chair</cell><cell>1304</cell><cell>26.08</cell></row><row><cell>StudentOffice</cell><cell>498</cell><cell>9.96</cell><cell>Printer</cell><cell>473</cell><cell>9.46</cell></row><row><cell>TechnicalRoom</cell><cell>437</cell><cell>8.74</cell><cell>Bookself</cell><cell>802</cell><cell>16.04</cell></row><row><cell>Toilet</cell><cell>389</cell><cell>7.78</cell><cell>Urinal</cell><cell>162</cell><cell>3.24</cell></row><row><cell>Secretary</cell><cell>336</cell><cell>6.72</cell><cell>Trash</cell><cell>813</cell><cell>16.26</cell></row><row><cell>VisioConferene</cell><cell>364</cell><cell>7.28</cell><cell>Phone</cell><cell>267</cell><cell>5.34</cell></row><row><cell>Warehouse</cell><cell>174</cell><cell>3.48</cell><cell>Fridge</cell><cell>190</cell><cell>3.80</cell></row><row><cell>ElevatorArea</cell><cell>308</cell><cell>6.16</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,157.03,409.95,301.29,93.07"><head>Table 2 .</head><label>2</label><figDesc>Results in the validation dataset for different values of k and M .</figDesc><table coords="5,191.22,429.01,229.85,74.02"><row><cell>Parameters</cell><cell cols="3">Score Rooms Score Object Score Total</cell></row><row><cell>k = 300, nt = 50</cell><cell>952.5</cell><cell>1051</cell><cell>2003.5</cell></row><row><cell>k = 300, nt = 150</cell><cell>960</cell><cell>1044.75</cell><cell>2004.75</cell></row><row><cell>k = 400, nt = 50</cell><cell>952.5</cell><cell>1038.5</cell><cell>1991</cell></row><row><cell>k = 400, nt = 150</cell><cell>963</cell><cell>1049.25</cell><cell>2012.25</cell></row><row><cell>k = 500, nt = 50</cell><cell>948</cell><cell>1014.25</cell><cell>1962.25</cell></row><row><cell>k = 500, nt = 150</cell><cell>966</cell><cell>1042</cell><cell>2008</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,135.35,561.78,344.65,7.89"><head>Table 3 .</head><label>3</label><figDesc>Rules for calculating the score for place localization and object recognition.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,134.77,210.39,345.81,83.86"><head>Table 4 .</head><label>4</label><figDesc>Results for all groups in the robot vision challenge. The baseline indicates the results by using Color &amp; Depth histogram + SVM.</figDesc><table coords="6,186.26,242.15,239.77,52.10"><row><cell>#</cell><cell>Group</cell><cell cols="3">Score Rooms Score Score Total</cell></row><row><cell>1</cell><cell>NUDT</cell><cell>1075.50</cell><cell>3357.75</cell><cell>4430.25</cell></row><row><cell>2</cell><cell>UFMS</cell><cell>219.00</cell><cell>1519.75</cell><cell>1738.75</cell></row><row><cell cols="2">3 Baseline Results</cell><cell>67.5</cell><cell>186.25</cell><cell>253.75</cell></row><row><cell>4</cell><cell>AEGEAN</cell><cell>-405</cell><cell>-995</cell><cell>-1400</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,656.80,153.38,7.86"><p>http://www.imageclef.org/2014/robot</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments.</head><p>RCG and LCR were supported by the <rs type="funder">CNPq</rs> and <rs type="funder">PET-Fronteira</rs>, respectively.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,138.35,569.00,342.23,7.86;6,146.91,579.96,333.66,7.86;6,146.91,590.92,223.52,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,258.96,569.00,221.61,7.86;6,146.91,579.96,37.23,7.86">Appearance-based place recognition for topological localization</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Nourbakhsh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,207.91,579.96,103.48,7.86;6,344.11,579.96,136.47,7.86;6,146.91,590.92,102.70,7.86">Proceedings. ICRA &apos;00. IEEE International Conference on</title>
		<meeting>ICRA &apos;00. IEEE International Conference on</meeting>
		<imprint>
			<date type="published" when="2000">2000. 2000</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1023" to="1029" />
		</imprint>
	</monogr>
	<note>Robotics and Automation</note>
</biblStruct>

<biblStruct coords="6,138.35,601.94,342.23,7.86;6,146.91,612.90,293.40,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,414.86,601.94,65.71,7.86;6,146.91,612.90,128.36,7.86">Overview of the imageclef 2013 robot vision task</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Martinez-Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Garcia-Varea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cazorla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,297.29,612.90,58.30,7.86">Working Notes</title>
		<meeting><address><addrLine>CLEF</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,623.92,342.23,7.86;6,146.91,632.61,333.66,10.13;6,146.91,645.84,333.66,7.86;6,146.91,656.80,285.60,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,146.91,645.84,228.98,7.86">ImageCLEF 2014: Overview and analysis of the results</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Martinez-Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Patricia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Marvasti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Üsküdarlı</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cazorla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Garcia-Varea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Morell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,402.19,645.84,74.26,7.86">CLEF proceedings</title>
		<title level="s" coord="6,146.91,656.80,141.41,7.86">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,119.67,342.23,7.86;7,146.91,130.63,333.67,7.86;7,146.91,141.59,120.90,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,400.33,119.67,80.25,7.86;7,146.91,130.63,135.02,7.86">Overview of the Im-ageCLEF 2014 Robot Vision Task</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Martinez-Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cazorla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Garcia-Varea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Morell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,303.54,130.63,172.28,7.86">CLEF 2014 Evaluation Labs and Workshop</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>Online Working Notes</note>
</biblStruct>

<biblStruct coords="7,138.35,152.55,342.23,7.86;7,146.91,163.51,333.67,7.86;7,146.91,174.47,79.22,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,395.75,152.55,84.83,7.86;7,146.91,163.51,86.88,7.86">Visual categorization with bags of keypoints</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Willamowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,264.74,163.51,211.85,7.86">Workshop on Statistical Learning in Computer Vision</title>
		<imprint>
			<publisher>ECCV</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,185.43,342.23,7.86;7,146.91,196.39,333.66,7.86;7,146.91,207.35,333.66,7.86;7,146.91,218.30,23.04,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,245.13,185.43,235.45,7.86;7,146.91,196.39,33.63,7.86">Video google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,200.93,196.39,279.65,7.86;7,146.91,207.35,110.48,7.86">Proceedings of the Ninth IEEE International Conference on Computer Vision -Volume 2. ICCV &apos;03</title>
		<meeting>the Ninth IEEE International Conference on Computer Vision -Volume 2. ICCV &apos;03<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page">1470</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,146.91,229.26,333.66,7.86;7,146.91,240.20,187.20,7.89" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,197.49,229.26,223.74,7.86">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,428.33,229.26,52.24,7.86;7,146.91,240.22,102.36,7.86">International Journal Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,251.18,342.23,7.86;7,146.91,262.11,213.91,7.89" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,341.44,251.18,135.48,7.86">Speeded-up robust features (surf)</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,146.91,262.14,119.15,7.86">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="346" to="359" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,273.10,342.23,7.86;7,146.91,284.06,333.67,7.86;7,146.91,295.02,333.66,7.86;7,146.91,305.98,285.48,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,306.98,273.10,173.60,7.86;7,146.91,284.06,194.25,7.86">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,361.67,284.06,118.91,7.86;7,146.91,295.02,333.66,7.86;7,146.91,305.98,10.75,7.86">Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR &apos;06</title>
		<meeting>the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR &apos;06<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
