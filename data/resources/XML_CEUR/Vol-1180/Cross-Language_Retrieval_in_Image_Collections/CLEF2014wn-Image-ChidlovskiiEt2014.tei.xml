<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,162.55,115.90,290.25,12.90;1,206.45,133.83,202.46,12.90">Assembling Heterogeneous Domain Adaptation Methods for Image Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,194.21,171.88,68.86,8.64"><forename type="first">Boris</forename><surname>Chidlovskii</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre Europe</orgName>
								<address>
									<addrLine>6 chemin Maupertuis</addrLine>
									<settlement>Meylan</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,269.63,171.88,65.02,8.64"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre Europe</orgName>
								<address>
									<addrLine>6 chemin Maupertuis</addrLine>
									<settlement>Meylan</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,354.02,171.88,67.14,8.64"><forename type="first">Shalini</forename><surname>Gangwar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre Europe</orgName>
								<address>
									<addrLine>6 chemin Maupertuis</addrLine>
									<settlement>Meylan</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,162.55,115.90,290.25,12.90;1,206.45,133.83,202.46,12.90">Assembling Heterogeneous Domain Adaptation Methods for Image Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B75299EBA8CC00AB7B6C39B66F8559E1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we report the contribution of XRCE team to the Domain Adaptation Challenge <ref type="bibr" coords="1,242.85,250.38,14.94,7.77" target="#b6">[10]</ref> organized in the framework of ImageCLEF 2014 competition <ref type="bibr" coords="1,193.72,261.34,9.52,7.77" target="#b5">[9]</ref>. We describe our approach to build an image classification system when a weak image annotation in the target domain is compensated by massively annotated images in source domains. One method is based using several heterogeneous methods for the domain adaptation aimed at the late fusion of the individual predictions. One big class of domain adaptation methods addresses a selective reuse of instances from source domains for target domain. We adopt from this class the adaptive boosting for weighting source instances which learns a combination of weak classifiers in the target domain. Another class of methods aims to transform both target and source domains in a common space. In this class we focused on metric learning approaches aimed at reducing distances between images from the same class and to increase distances of different classes independently if they are from source or target domain. Combined the above approaches with a "brute-force" SVM-based approach we obtain a set of heterogeneous classifiers for class prediction of target instances. In order to improve the overall accuracy, we combine individual classifiers through different versions of majority voting. We describe different series of experiments including those submitted for the official competition and analyze their results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The shortage of labeled data is a fundamental problem in machine learning applications. While huge amounts of unlabeled data is generated and made available in many domains, the cost of acquiring data labels remains high. Domain adaptation addresses this problem by leveraging labeled data in one or more related domains, often referred as "source" domains, when learning a classifier for unseen data in a "target" domain. The domains are assumed to be related but not identical.</p><p>This situation occurs in domains where machine learning components are intensively deployed, such as event detection in videos, entity recognition across different text corpora, object recognition in images acquired in different conditions (see <ref type="bibr" coords="1,450.04,584.71,16.60,8.64" target="#b17">[21]</ref> for a survey of domain adaptation methods and <ref type="bibr" coords="1,319.87,596.66,16.60,8.64" target="#b20">[24]</ref> for a survey on the related field of transfer learning).</p><p>Domain adaptation has also received a significant attention in computer vision applications <ref type="bibr" coords="1,177.04,632.53,16.32,8.64" target="#b13">[17,</ref><ref type="bibr" coords="1,193.35,632.53,12.24,8.64" target="#b14">18,</ref><ref type="bibr" coords="1,205.59,632.53,12.24,8.64" target="#b15">19,</ref><ref type="bibr" coords="1,217.83,632.53,12.24,8.64" target="#b16">20,</ref><ref type="bibr" coords="1,230.06,632.53,12.24,8.64" target="#b18">22,</ref><ref type="bibr" coords="1,242.30,632.53,12.24,8.64" target="#b22">26,</ref><ref type="bibr" coords="1,254.53,632.53,12.24,8.64" target="#b23">27]</ref>, Different aspects have been addressed and various approaches to the domain adaptation have been proposed <ref type="bibr" coords="1,368.08,644.48,11.50,8.64" target="#b3">[7,</ref><ref type="bibr" coords="1,379.58,644.48,11.50,8.64" target="#b21">25]</ref>; this clearly indicates how complex and multi-faceted the problem is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Domain adaptation challenge</head><p>Current research in domain adaptation focuses on a scenario where (a) the prior domain (source) consists of one or maximum two databases (b) the labels between the source and the target domain are the same, and (c) the number of annotated training data for the target domain are limited. The goal of this challenge is to push the state of the art towards more realistic settings, relaxing these assumptions 1 .</p><p>Indeed researchers and teams participating in the ImageCLEF DA 2014 challenge were asked to build recognition systems for the target classes by leveraging the knowledge from four source domains. Both source and target data are provided by exploiting existing available resources.</p><p>Specifically, the participants were provided with image features 2 extracted by the organizers from 600 randomly selected images collected from five different image collections: Caltech-256 [2], ImageNet12 [3], PASCAL-VOC12 <ref type="bibr" coords="2,377.44,270.67,10.58,8.64" target="#b0">[4]</ref>, Bing [1] and SUN <ref type="bibr" coords="2,466.49,270.67,10.58,8.64" target="#b1">[5]</ref>. The organizers selected 12 common classes from each datasets, namely, aeroplane, bike, bird, boat, bottle, bus, car, dog,horse, monitor, motorbike, people. (see Figure <ref type="figure" coords="2,475.61,294.58,4.98,8.64" target="#fig_0">1</ref> for example images for each class and in all collection, showing the variability of classes between the collections). The first four collections from the list are proposed as source domains for which the image features and all the labels were provided. The SUN dataset served as the target domain, with 60 annotated and 600 non-annotated instances. The task was to provide predictions for the non-annotated target data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">XRCE approach to domain adaptation</head><p>We started by analyzing the domain adaptation task and the available features for the different collections. Three key elements of our analysis are the following:</p><p>1. Original images are not available neither for target nor for sources domains; the participants dispose feature sets only. It makes impossible to leverage the intermediate knowledge generated during the feature extraction process, such as the choice of low or high level features or the vocabulary for bags of visual words. 2. Source and target domains are semantically related, however they are different feature-wise. Figure <ref type="figure" coords="2,236.96,501.54,3.46,8.64" target="#fig_1">2</ref>.left compares the PCA projection of four source and target domains made available at phase 1 of the challenge 3 . A similar relationship (see Figure <ref type="figure" coords="2,200.57,525.45,4.27,8.64" target="#fig_1">2</ref>.middle) can be found between PCA projections for the source and target domain data at phase 2. Worse, as shown in Figure <ref type="figure" coords="2,381.79,537.41,3.60,8.64" target="#fig_1">2</ref>.right, the target feature distribution has been changed between phases 1 and 2. This change is likely due to 1 From http://www.imageclef.org/2014/adaptation. 2 These image features were the concatenation of four bag-of-visual words <ref type="bibr" coords="2,412.60,580.16,14.94,7.77" target="#b7">[11]</ref> built on a 2x2 split of the image where the low level features were SIFT descriptors extracted densely from the corresponding image regions. 3 At the beginning of the challenge, the participants were provided with a similar problem configuration but different feature sets in order to familiarize with the problem. In addition all labels for the target set were also available in order to allow participants to evaluate their methods. We will refer to this setting as phase 1. The features for the actual training for the submission were released in phase 2. parameter changes in the feature extraction process hidden from the participants. Consequently, it made impossible any deployment of models learned at phase 1. 3. According to our experiments made with the features released in phase 1, none of existing domain adaptation methods for the visual classification appeared as a perfect match for the challenge task <ref type="bibr" coords="3,296.67,439.70,11.38,8.64" target="#b3">[7,</ref><ref type="bibr" coords="3,308.06,439.70,11.38,8.64" target="#b21">25]</ref>.</p><p>Putting all these elements together, we decided to proceed in two steps. First, we undertook a number of conceptually uncorrelated approaches to domain adaptation. Second, we used ensemble techniques to aggregate individual predictions using different majority voting methods. More precisely, in the first step we tested methods of the following classes:</p><p>Brute force : Cross validation on available training data to identify best combinations of source domains for transferring to the target domain, with the optimal set of parameter values and kernel functions. Instance weighting : Boosting-based instance-transfer domain adaptation to identify different instances in one or more source domains allowing to boost the learning in the target domain. Space transformation : Metric learning-based domain adaptation where the idea is to transform the feature space and bring instances from the same class close to each other, independently if they are from source or target domains.</p><p>The remainder of this working note is organized as follows. In Section 3 we describe three individual domain adaptation methods. Section 4 describes the ensemble methods over heterogeneous classifiers. Section 5 reports evaluation results including the ten submissions to the challenge. We analyze the results and the impact of different components on the overall performance. Finally we outline future work and conclude in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Individual domain adaptation methods</head><p>There are two main cases in domain adaptation, they depend on available data in the target domain. In the unsupervised case, no labeled target instances are available. In the semi-supervised case, a few target instances are labeled. In ImageCLEF DA challenge, 60 labeled target instances are available, this positions the challenge into the second case.</p><p>We suppose that the labeled target dataset T l is drawn from the same distribution as the unlabeled target set T u and hence it will play an important role in building the classification model. However the size of T l is too small to adequately train a good classifier for the test data T u .</p><p>The training data from source domains are more abundant, but the classifiers learned from these data cannot classify the test data well due to different data distributions. Let S 1 , . . . , S N S denote the source domains, where N S ≥ 1 is the number of sources. Let X s k be an instance space of S k (e.g. a subset of S k used for training). We denote by X t the instance space of the target domain (that can take elements from T l , T u or both). Further, we denote by Y = {c 1 , c 2 , . . . , c Nc } the set of category labels common to all domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Brute force</head><p>We start off with the brute force approach which uses available annotated data to assess the relatedness between source domains and the target domain and to test a straightforward domain adaptation scenario. For the N S = 4 different source domains we consider N SC = 2 N S -1 = 15 source combinations SC i , i = 1, . . . , N SC which are generated by an exhaustive enumeration of all possible subsets of source domains, e.g.</p><formula xml:id="formula_0" coords="5,134.77,118.99,255.05,9.65">SC 1 = {S 1 }, SC 6 = {S 1 , S 3 } and SC 15 = {S 1 , S 2 , S 3 , S 4 }.</formula><p>For each source combination SC j , we concatenate the target train set T l with the selected sources SC j and train the SVM models in a cross validation mode to find out the optimal parameter values and kernel functions. Two versions have been implemented, the multi-task SVM and one-against-all binary SVMs, as follows:</p><p>Multi-class SVM: We first used the multi-class LIBSVM package <ref type="foot" coords="5,401.96,187.55,3.49,6.05" target="#foot_0">4</ref> for the multi-class classification in the target domain. In the cross validation with k = 10 folds, we tested different values of SVM parameters µ, ν, C, the standard kernels available with the package and different source combinations SC i we found as the best source combination CS 13 = {S 1 , S 2 , S 4 } with the linear ν-SVM and the parameters ν = 0.12 and C = 0.01. This classifier denoted by f mcsvm allows a gain of 3% compared to the baseline multi-class SVM f bf 0 with no source domains and cross validated parameters. Binarised SVM: We additionally consider the multi-class classification by a composition of one-against-all classifiers running again the LIBSVM package and cross validate to determine the optimal parameters for each of N c = 12 classes in the datasets. Unlike the multi-class SVM above where the optimal parameter values, kernel and the source combination are common for all classes, in the binarized version we identify the best classifier for each class c j ∈ Y which comes with a specific set of parameter values, kernels and source combinations for each class c i . Hence, the multi-class classifier f bsvm is composed of N c binary classifiers f cj bsvm , j = 1, . . . , N c . For an unseen sample x i , it proceeds by applying all classifiers and predicting the label ŷbsvm = c j for which the corresponding classifier reports the highest confidence score:</p><formula xml:id="formula_1" coords="5,262.72,422.77,107.53,19.88">ŷbsvm = argmax cj ∈Y f cj bsvm (x i ).</formula><p>In a probabilistic setting, all class confidence scores are converted into probabilities P (y i = c|f bsvm (x i )).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Instance Transfer with Adaboost</head><p>We extended the Transfer AdaBoost learning algorithm (TrAdaboost) <ref type="bibr" coords="5,410.12,517.70,15.27,8.64" target="#b8">[12]</ref>, an extension of the AdaBoost <ref type="bibr" coords="5,204.96,529.66,16.60,8.64" target="#b11">[15]</ref> for transfer learning by assuming that there is abundant source training data to learn a classifier, but the target domain is different from the source. Hence this approach is can be easily adopted for the domain adaptation.</p><p>AdaBoost aims to boost the accuracy of a weak learner by carefully adjusting the weights of training instances and to learn a classifier accordingly. In TrAdaboost, source and target instances have opposite roles. Target training instances are weighted similarly as in AdaBoost, but when source training instances are wrongly predicted by the learned model due to distribution changes, it is assumed that they could be those that are the most dissimilar to the target instances and therefore TrAdaboost tries to decrease the weights of these instances in order to weaken their impact.</p><p>Algorithm 1 Transfer Adaptive Boosting with one source domain. 1: Initialize the initial source and target weight vectors, w 1 T = (w 1 t 1 , . . . , w 1 t N t ), w 1 S = (w 1 s 1 , . . . , w 1 s Ns ), 2: Set w = (wT , wS), β = 1/(1 + 2 ln Nt/M ) and T = (Tt, Ts). 3: for r = 1, . . . , M do 4:</p><p>Normalize w r = w r /|w r |.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Call Learner on the training set T with the distribution w r to find a hypothesis fr : X → Y which minimizes error for (T, w r ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Calculate the error of hr on Tt : r = min 1 2 ,</p><formula xml:id="formula_2" coords="6,138.55,249.63,334.39,48.83">1 N t i=1 w r t i n i=1 w r t i • [[fr(x t i ) = yi]] . 7: Set β r = 1/2 log((1 -r )/ r ) . 8: Set Γ r = 2(1 -r ). 9:</formula><p>Update the weight vectors:</p><formula xml:id="formula_3" coords="6,225.45,307.71,189.66,23.80">w r+1 s j = Γ r w r s j exp(-β [[fr(x s j ) = yj]]), x s j ∈ Xs, w r+1 t i = w r t i exp(2β r [[fr(x t i ) = yi]]), x t i ∈ Xt.</formula><p>10: end for 11: Output the aggregated estimate ftra(x) = M r=1 β r fr(x) . In a probabilistic setting, these scores are converted into probabilities P (yi = c|ftra(xi)).</p><p>A formal description of the framework is given in Algorithm 1 for the case of using one source domain, N S = 1, but the extension to multiple sources is straightforward. The main idea is that at each iteration round, if a source training instance is mistakenly predicted, the instance may likely conflict with the target training data. Therefore, we decrease its training weight to reduce its effect through multiplying its weight by a strictly positive factor exp(</p><formula xml:id="formula_4" coords="6,241.70,464.13,131.74,9.65">-β[[f r (x i ) = y i ]]) ≤ 1, where [[•]</formula><p>] is the Iverson brackets denoting the indicator function that equals one if its argument is true and zero otherwise. We further add Γ r term inspired by <ref type="bibr" coords="6,279.30,488.36,11.62,8.64" target="#b2">[6]</ref> to address some drawbacks of the initial TrAdaboost <ref type="bibr" coords="6,163.38,500.32,15.27,8.64" target="#b8">[12]</ref>. Due to this, in the next round, the mis-classified source training instances, which are dissimilar to the target ones, will affect the learning process less than in the current round. After several iterations, the source training instances that fit the target ones better will have larger training weights, while the source training instances that are dissimilar to the target ones will have much lower weights. These instances with large training weights tend to help the learning algorithm to train better classifiers.</p><p>As in the previous section, we consider N SC =15 different compositions of available sources to be used as one. We use Algorithm 1 on different combinations by simply concatenating the sources to form the source training set X s . Note the existence of an extension of TrAdaboost to cope with multiple source domains <ref type="bibr" coords="6,398.47,608.62,15.27,8.64" target="#b25">[29]</ref>. This extension probes at each iteration instances from different sources, in order to identify a source which reduces the error the most. In the context of the ImageClef DA challenge, our experience of using TrAdaboost with multiple sources has not been convincing. Concatenating sources was more beneficial that keeping them separately. This phenomenon can be explained by the closeness between the source instances proposed for the challenge (see Figure <ref type="figure" coords="7,184.35,131.27,3.60,8.64" target="#fig_1">2</ref>). In the cases where source domains have very different distributions, keeping them separately might be a better strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Metric learning methods</head><p>Another class of domain adaptation methods tries to jointly transform the source and target domains, either by trying to align domains using PCA projections <ref type="bibr" coords="7,439.09,214.05,16.60,8.64" target="#b14">[18,</ref><ref type="bibr" coords="7,455.70,214.05,12.45,8.64" target="#b12">16,</ref><ref type="bibr" coords="7,468.15,214.05,12.45,8.64" target="#b10">14]</ref> or by learning a metric which can bridge the two domains. For the challenge we experimented with two type of metric learning approaches. On one hand we extended a metric learning technique that optimizes KNN objectives <ref type="bibr" coords="7,369.82,249.92,16.60,8.64" target="#b9">[13,</ref><ref type="bibr" coords="7,386.42,249.92,12.45,8.64" target="#b24">28]</ref> that we will denote by KNN ML. On the other hand, we adopted for domain adaptation the Nearest Class Mean (NCM) and Nearest Class Multiple Centroids (NCMC) classifiers from <ref type="bibr" coords="7,445.80,273.83,15.27,8.64" target="#b19">[23]</ref>.</p><p>Metric learning with KNN objectives. The aim of these approaches is to find a linear transformation W such that, in the new space, distances between examples from the same class are decreased and distances between images from different classes are increased. This is done generally by optimizing the sum of the losses on the training set <ref type="foot" coords="7,145.83,348.42,3.49,6.05" target="#foot_1">5</ref> :</p><formula xml:id="formula_5" coords="7,146.74,373.47,333.85,12.17">L qpn = 1 + d W (x q , x r ) -d W (x q , x n ) + = 1 + 2x q W W (x n -x p ) + (1)</formula><p>where (x q , x p , x n ) are triplets such as the image x p is from the same class as x q while x n is from a different class, d W (x q , x i ) = W x q -W x i 2 2 is the Euclidean distance between image x q and image x i in the transformed space and [a] + denotes max(0, a).</p><p>We consider only projections into a lower dimensional space, i.e. the dimension of the projected vector W x is lower than the dimension of x (in our experiments we used target dimensions of 64 and 128). One advantage of this is to have less parameters to estimate, which is especially important when only relatively small amount of training examples are available; also it generally leads to better performances.</p><p>As optimizing the sum of losses over all possible triplets is typically unfeasible, we approximate the solution through stochastic gradient descent (SGD) <ref type="bibr" coords="7,407.46,508.29,11.62,8.64" target="#b4">[8]</ref> method, where at each step a set of random triplets are selected and W is updated with a fix learning rate and the sub-gradients:</p><formula xml:id="formula_6" coords="7,213.91,558.23,266.68,10.65">∇ W L qpn = 2 [[L qpn &gt; 0]] W X qnp + X qnp<label>(2)</label></formula><p>where X qnp = x q (x n -x p ) . If instead of updating only by a single triplet, for each x q we select a set of random positives-negative pair {(x pi , x ni ), i = 1..m}, we have the same updating formula but X qnp = X q X np , where X np is the matrix concatenating the (x ni -x pi ) vectors, and X q is the matrix concatenating m times the x q vector.</p><p>Metric Learning for Nearest Class Mean. The nearest class mean NCM classifier assigns a document to the class c * ∈ Y with the closest mean:</p><formula xml:id="formula_7" coords="8,176.02,153.12,304.57,18.59">c * = argmin c∈Y d W (x, µ c ) where d W (x, µ c ) = W x -W µ c 2 2 ,<label>(3)</label></formula><p>where d W (x, µ c ) is the Euclidean distance between an image x and the class mean µ c in the transformed space.</p><p>The main idea behind the metric learning for Nearest Class Mean (MLNCM) is to learn a linear projection matrix W such that, in this new space, the instances of the same class are closer to each other, and hence to the correct class mean than to the class centers of the other classes. To optimize such NCM performance objective in the projected space, Mensink et al. <ref type="bibr" coords="8,291.31,259.06,16.60,8.64" target="#b19">[23]</ref> proposed to formulate the NCM classifier as a multi-class soft-max regression problem using a probabilistic model where the probability for a class c given a feature vector x i is defined as:</p><formula xml:id="formula_8" coords="8,208.61,306.54,271.98,25.92">P (y i = c|x i ) = exp -.5d W (x i , µ c ) c ∈Y exp -.5d W (x i , µ c ) .<label>(4)</label></formula><p>Then, to learn the projection matrix W , the log-likelihood of the correct predictions are optimized over the training set using the stochastic gradient descend (SGD) algorithms <ref type="bibr" coords="8,162.87,369.15,10.58,8.64" target="#b4">[8]</ref>. At each step, W is updated with a fixed learning rate in the direction given by the gradient:</p><formula xml:id="formula_9" coords="8,147.28,402.66,329.44,26.80">∇ W L = 1 N i c∈Y P (y i = c|x i ) -[[y i = c]] • W • (x i -µ c )(x i -µ c ) , (<label>5</label></formula><formula xml:id="formula_10" coords="8,476.72,409.71,3.87,8.64">)</formula><p>where N is the number of training examples. At each iteration we are using only a random subset of the training data to update W . In <ref type="bibr" coords="8,161.73,468.32,15.27,8.64" target="#b19">[23]</ref>, Mensink et al. proposed an extension to this method, the Nearest Class Multiple Centroids (NCMC) classifier that allows more flexible class representations by considering multiple cluster means (centroids) for each class. In their case, NCMC represents each class by a set of centroids instead of a single class mean. In our multiple domain case, instead of cluster means, we consider domain specific class means µ d c (averaging over instances from the same class c and same domain d) and we assign a test instance to a given class based on a weighted soft-max distance to these domain specific class means:</p><formula xml:id="formula_11" coords="8,197.62,574.93,282.97,30.72">P (y i = c|x i , µ d c ) = 1 Z N d d=1 w d exp -.5d W (x i , µ d c )<label>(6)</label></formula><p>where N d is the number of domains and</p><formula xml:id="formula_12" coords="8,304.91,618.68,156.66,12.72">Z = c d w d exp -.5d W (x i , µ d c</formula><p>) is the normalizer. Adding domain specific weights w d in Eq. 6 allows giving different importance to each domain (in our experiments we used w t = 2 for the target domain T l and w s k = 1 for the source domains).</p><p>To learn the projection matrix W , we maximize the log-likelihood of correct classification in Eq. 6, for which the gradient w.r.t. W is given by:</p><formula xml:id="formula_13" coords="9,144.21,149.51,336.38,40.97">∇ W L = 1 N i,c,d p(µ c d |x i ) -[[c = y i ]] p(µ c d |x i ) d p(µ d c |x i ) W (x i -µ d c )(x i -µ d c ) ,<label>(7)</label></formula><p>where</p><formula xml:id="formula_14" coords="9,161.74,201.98,153.51,13.70">p(µ c d |x i ) = w d Z exp -.5d W (x i , µ c d )</formula><p>. Again, we use Stochastic Gradient Descent <ref type="bibr" coords="9,158.53,216.36,11.62,8.64" target="#b4">[8]</ref> with fixed learning rate where at each iteration we sample a random subset from the training set (2 * N C was used in the experiments). We denote this learning method by NCMC ML.</p><p>Learning strategies. Let us now suppose that we have the labeled target dataset T l , the unlabeled target data set T u , and the source training set (that can contain one or several source domains S 1 , . . . , S Ns ). We experimented with the above mentioned metric learning approaches using 3 different strategies and different source configurations SC i .</p><p>1. In the first case we simply merged the source combination SC i with the target training data T l and directly optimized the loss in Eq. 2, Eq. 5 or Eq. 7 using all training examples. The rationale behind this is the following. By decreasing interclass distances independently from the domains, allows to exploit more efficiently labeled images from source domains in the projected space to classify target examples and hence it yield to increased performance of distance based classifiers. The methods from this first strategy are denoted by KNN ML, MLNCM ML respectively MLNCMC ML. 2. As we have much less labeled target examples than source ones, their presence is much rearer in the above mentioned random sampling processes. Hence in the iterative approaches they will have less influence in the learning process than the much larger amount of source images. Therefore as a second strategy, we propose to refine W obtained with the first strategy such that we impose at each iteration to have in the sample examples from T l . In the case of KNN, this is done by considering at each step x t q ∈ T l and sampling randomly a set of randomly sampled positive/negative image pairs from the source and then updating W using the triplet sets (x t q , x rj , x nj ), j = 1..N c . In the case of NCM and NCMC we simply ensure that at least one target image from T l is present in the selected batch. The methods corresponding to this second strategy are denoted KNN MLt, NCM MLt and NCMC MLt respectively. 3. Finally, inspired by the adaptive learning proposed in <ref type="bibr" coords="9,372.58,570.04,15.27,8.64" target="#b23">[27]</ref>, we consider an iterative learning strategy where we further adjust the learned metric with the second strategy. To do this, at each step and for each class we add target instances from the unlabeled set T u and remove source instances as follows. For each class c we add the target instance x t i for which P (y i = c * i |x t i ) -P (y i = c ‡ i |x t i ) according to Eq. 4 is the largest difference, where c * i = c is the predicted label of x t i and c ‡ i is the second predicted label of x t i . Also, for each class c we remove the source image x s j from SC i for which c * j = c and P (y j = c * j |x s j ) -P (y j = c ‡ j |x s j ) is the lowest difference. Then we refine W using the updated training set. We do several rounds until the stopping criteria is reached <ref type="foot" coords="10,330.43,129.60,3.49,6.05" target="#foot_2">6</ref> . The methods corresponding to the third strategies are denoted by KNN MLDA, NCM MLDA and NCMC MLDA respectively.</p><p>Classification strategies. In the projected space we have different options to predict the class labels of the target images. As we learned a metric, we can use any distance based classifiers such as KNN, NCM or NCMC based classifications as they do not require further parameter tuning. The only parameter k of the KNN can be fixed (e.g. we used 3 in all experiments) or cross validated. NCM has no parameter and the parameters of the NCMD are the weights w d that can be fixed or tuned. In our experiments we used fixed weights. Note that we experimented also with training Multi-Class SVM in the projected space, but the performance was similar to or below the results obtained in the original space. As our experiments on the dataset provided in phase 1 <ref type="foot" coords="10,410.22,276.03,3.49,6.05" target="#foot_3">7</ref> have shown that NCMC in general performs better than KNN and NCM (up to 3-5%) we only retained the NCMC classifier to compute the class probabilities (using Eq. 6) independently which metric learning method or strategy was used to compute W . Furthermore, from our experiments in phase 1 it was not clear which source combinations are better when we evaluated the NCMC performance in the projected space. Furthermore, the combinations that yielded best accuracies in the original space (using SVM classifiers) were not always the ones that proved to be the best after projection. Therefore, for each of the above described methods, we learned a metric using each source combination SC i individually and computed class predictions P (y i = c * |f SCi ml (x t i )) for the test set T u in the corresponding projected space. Then we computed the late fusion of class predictions over all source combination to get final predictions given a metric learning method. These classifiers denoted by f ml , were used in two modes. Two runs have been used in submission to predict directly the labels, namely, MLNCM MLDA 128 it200 e0.1 p025 and MLNCMC ML 128 it200 e0.1 p025. Also they were used by ensemble methods in combination with other models to generate the combin * runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Ensemble Method</head><p>The combination of the decisions of several classifiers has been proposed as a means of improving the accuracy achieved by any of them. The reasons for combining the outputs of multiple classifiers are compelling, because different classifiers may implicitly represent different useful aspects of a problem, or of the input data, while none of them represents all useful aspects. In the context of instance classification, the idea of combining the decisions of several classifiers has been well explored.</p><p>In contrast to approaches which combine models derived from multiple versions of the same learning method, the specific focus of this paper is on ensemble methods that are able to combine the decisions of multiple classifiers of different types, so-called heterogeneous sets of classifiers.</p><p>Previous sections described three domain adaptation methods which generate one or more classifiers for target instances, namely, obtained by the brute force (f mcsvm and f bsvm ), transfer adaptive boosting (f tra ) and metric learning (f ml ). They form a pool F of several classifiers from each group F = {f 1 , . . . , f N f } where any classifier's output is class scores or class probabilities. We will denote by g(f k , x t i ) the predicted label of f k for x t i and by p(y i = c|f k (x t i )) the class probability scores. When the classifier does not provide such score we can define p(y i = c|f k (x t i )) = 1 when c = g(f k , x t i ) and zero otherwise.</p><p>The first method we tested was the (unweighted) majority voting that combines decisions of individual classifiers, so the global ensemble prediction for target instance x t is:</p><formula xml:id="formula_15" coords="11,228.52,296.25,158.31,22.88">c * = argmax c∈Y f k ∈F [[g k (f k , x t i ) = c]]</formula><p>In the probabilistic setting, we sum up class probabilities and report the class with the highest probability:</p><formula xml:id="formula_16" coords="11,225.36,354.82,164.64,22.88">c * = argmax c∈Y f k ∈F P (y i = c|f k (x t i ))</formula><p>The majority voting method is based solely on the output label or probabilities computed by each classifier. In the case of weighting majority voting, classifiers' opinions may be taken differently, depending on how accurate they have been in the past. Below we establish weights proportional to each classifier's accuracy, so each classifier's output is considered according to its past performance.</p><p>In the probabilistic setting, the fact that instance x t i belongs to class c given that classifier f k output label c for x, i.e. g(f k , x t i ) = c , has an uncertainty which can be expressed as the conditional probability p(y i = c|g(f k , x t i ) = c ). We may use the confusion matrices to approximate the conditional probabilities for all classes c ∈ Y given a classifier f k ∈ F .</p><p>Making assumption of the classifier independence, we use the Bayesian rule to estimate the posterior probability of the class c for the target instance x i from the output of classifiers f k ∈ F :</p><formula xml:id="formula_17" coords="11,187.99,545.91,228.53,27.49">P (y i = c|x t i ) = c ∈Y P y i = c|g(f k , x t i ) = c c ∈Y c ∈Y P y i = c |g(f k , x t i ) = c</formula><p>Finally, the class c * = argmax c∈Y P (y i = c|x t i ) (with the highest probability) is assigned as the weighted majority vote to instance x i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation results</head><p>Both individual and ensemble methods have been tuned by XRCE team during the phase 1 to select best strategies of the challenge and applied to the submission data (phase 2). Among multiple evaluation runs, ten most promising results have been selected for the submission, they all are reported in Table <ref type="table" coords="12,367.39,131.27,3.74,8.64" target="#tab_0">1</ref>. For each submission, the table reports the accuracy, score, divergence and a comment on the method used.</p><p>The divergence has been proposed to measure the deviation of the submitted prediction vector from a vector with equi-weighted classes:</p><formula xml:id="formula_18" coords="12,221.33,191.59,167.69,26.80">div = c∈Y Card({i|g(f, x t i ) = c}) - N N c</formula><p>where N is the number of test images, N c the number of classes, {i|g(f, x t i ) = c} is the set of target instances for which the classifier f predicts class c, Card(A) is the cardinality of the set A and |a| is the absolute value of a. Under the assumption on the equal class distribution in the submission dataset, it may make sense to choose a run whose prediction tends to minimize the divergence. Analyzing the ten submissions made (as well as the non-submitted runs), allowed us to make the following conclusions on the selected strategy and performance of different components of the image classification system we have built for the challenge:</p><p>1. Among individual domain adaptation methods, the brute force performed poorly as expected. It did not merit any individual submission, but participated in various ensembles of classifiers. 2. TrAdaboost (TrA) with SVM as a week learner and the Metric Learning (ML) performed reasonably well, each merited two individual submissions. 3. Ensembles of heterogeneous classifiers has turned to be a right strategy to boost the overall performance. 4. Unweighted majority vote (UMV) on a small selection of classifiers performed the best; these selections include 3 to 6 top performing classifiers from each group of classifiers.</p><p>5. Weighted majority vote (WMV) works well on large sets of classifiers but underperformed against the manual classifier selection<ref type="foot" coords="13,346.16,129.60,3.49,6.05" target="#foot_4">8</ref> with unweighted majority vote. 6. Divergence minimization did not play any important role; making hypotheses on the class distribution and incorporating this prior knowledge in the classification system should be done in a more systematic way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>The strategy of using heterogeneous methods for domain adaptation turned to be a right one. It has allowed our team to build a image classification system in the target domain with an important knowledge transfer from available source domains. Ensembles of heterogeneous classifiers aggregated with different majority voting scenarios has allowed to get high accuracy in the submission runs and to eventually win the ImageCLEF Domain Adaptation competition. The thoughtful analysis of obtained results has also allowed to identify new directions in domain adaptation for image classification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,134.77,346.29,345.81,8.12;3,134.77,357.60,90.98,7.77;3,164.83,115.84,283.47,215.72"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Examples of 12 classes from four source domains (Caletch, InageNet, Pascal, Bing) and the target domain (SUN).</figDesc><graphic coords="3,164.83,115.84,283.47,215.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,134.77,237.20,345.81,8.12;4,134.77,248.50,238.51,7.77;4,121.18,115.84,127.56,95.67"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. PCA projections for source and target domains. a) Target and source data at phase 1; b) Target and source data at phase 2; 3) Target data at phases 1 and 2.</figDesc><graphic coords="4,121.18,115.84,127.56,95.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,134.77,132.57,345.83,8.12;6,150.01,143.59,281.73,8.06;6,134.77,154.49,134.67,8.12"><head>Require:</head><label></label><figDesc>Target training set Tt = (Xt, Y ) of size Nt; source training set Ts = (Xs, Y ), of size Ns; a base learning algorithm Learner; the maximum number of iterations M . Ensure: Target learner f : Xt → Y .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="12,138.95,315.35,329.06,150.04"><head>Table 1 .</head><label>1</label><figDesc>Ten runs submitted by XRCE team.</figDesc><table coords="12,138.95,336.67,329.06,128.72"><row><cell cols="2">Place Score Accu-Run Name</cell><cell>Diver-Comment</cell></row><row><cell></cell><cell>racy</cell><cell>gence</cell></row><row><cell>1 228</cell><cell>38.0 combin6 Np20</cell><cell>108 UMV</cell></row><row><cell>2 228</cell><cell>38.0 combin3 Np18</cell><cell>108 UMV</cell></row><row><cell cols="2">3 226 37.67 combinAll6 Np19</cell><cell>164 UMV</cell></row><row><cell cols="2">4 217 36.17 combin6A Np19</cell><cell>78 UMV + min divergence</cell></row><row><cell cols="3">5 214 35.67 MLNCM MLDA 128 it200 e0.1 p025 174 ML</cell></row><row><cell cols="2">6 212 35.33 combinAll7A Np19</cell><cell>134 WMV</cell></row><row><cell cols="2">7 208 34.67 combin8A Random Np25</cell><cell>78 WMV + min divergence</cell></row><row><cell cols="2">8 185 30.83 MLNCMC ML 128 it200 e0.1 p025</cell><cell>168 ML</cell></row><row><cell cols="2">9 182 30.33 combin2 Np10</cell><cell>134 TrA+UMV</cell></row><row><cell cols="2">10 158 26.33 svmBoost Mul Power f60</cell><cell>186 TrA</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="5,144.73,657.09,142.39,7.77"><p>http://ww.csie.ntu.edu.tw/ cjlin/libsvm/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="7,144.73,646.13,335.85,7.77;7,144.73,657.09,149.20,7.77"><p>The second equality is true if we normalize our features to have L2 norm in the original space, what we have done in all our experiments</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2" coords="10,144.73,613.12,335.85,7.77;10,144.73,624.08,316.78,7.77"><p>The classification performance on the initial training set incurs a stronger degradation than a predefined tolerance threshold or no more target image can be added or source removed.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3" coords="10,144.73,635.17,335.85,7.77;10,144.73,646.13,335.85,7.77;10,144.73,657.09,100.11,7.77"><p>We did 11 fold cross validation on the training set in phase 1. We split the provided the 600 test instances into 10 folds such that each folds contained 5 document per class and we added the training set as 11th fold.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_4" coords="13,144.73,646.13,335.85,7.77;13,144.73,657.09,175.06,7.77"><p>Under manual selection we meant that the selection was made based on our intuitions according to some of the lessons we learned in phase 1.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="13,142.61,396.05,58.07,7.77;13,222.36,396.89,258.22,6.31;13,150.95,407.85,99.08,6.31" xml:id="b0">
	<monogr>
		<ptr target="http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2012/index.html" />
		<title level="m" coord="13,150.96,396.05,45.90,7.77">Pascal-voc12</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,418.06,24.54,7.77" xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Sun</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,429.10,337.96,7.77;13,150.95,440.06,329.63,7.77;13,150.95,450.86,329.63,7.94;13,150.95,461.98,33.62,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,302.13,429.10,178.44,7.77;13,150.95,440.06,50.91,7.77">Adaptive boosting for transfer learning using dynamic updates</title>
		<author>
			<persName coords=""><forename type="first">Samir</forename><surname>Al-Stouhi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chandank</forename><surname>Reddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,150.95,450.86,210.68,7.73">Machine Learning and Knowledge Discovery in Databases</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Gunopulos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Malerba</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">6911</biblScope>
			<biblScope unit="page" from="60" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,473.03,337.96,7.77;13,150.95,483.99,20.17,7.77" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="13,214.70,473.03,194.29,7.77">Domain adaptations for computer vision applications</title>
		<author>
			<persName coords=""><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="13,142.61,494.87,337.98,7.94;13,150.95,505.99,20.17,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,192.36,495.03,223.28,7.77">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,433.80,494.87,41.60,7.73">COMPSTAT</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,517.04,337.97,7.77;13,150.95,526.04,329.63,9.73;13,150.95,538.80,237.30,7.94" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,412.76,528.00,67.82,7.77;13,150.95,538.96,128.91,7.77">ImageCLEF 2014: Overview and analysis of the results</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Martinez-Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Patricia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Marvasti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Üsküdarlı</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cazorla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Garcia-Varea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Morell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,296.01,538.80,66.02,7.73">CLEF proceedings</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.24,550.00,338.34,7.77;13,150.95,560.80,297.24,7.94" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,280.43,550.00,200.15,7.77;13,150.95,560.96,15.16,7.77">Overview of the ImageCLEF 2014 Domain Adaptation Task</title>
		<author>
			<persName coords=""><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Novi</forename><surname>Patricia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,182.85,560.80,154.88,7.73">CLEF 2014 Evaluation Labs and Workshop</title>
		<title level="s" coord="13,344.16,560.80,77.84,7.73">Online Working Notes</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.24,572.01,338.34,7.77;13,150.95,582.81,301.15,7.94" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,367.50,572.01,113.07,7.77;13,150.95,582.97,42.91,7.77">Visual categorization with bags of keypoints</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Willamowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,210.49,582.81,215.70,7.73">ECCV Workshop on Statistical Learning in Computer Vision</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.24,594.02,338.34,7.77;13,150.95,604.81,129.15,7.94" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,361.07,594.02,105.17,7.77">Boosting for transfer learning</title>
		<author>
			<persName coords=""><forename type="first">Wenyuan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gui-Rong</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,150.95,604.81,18.93,7.73">ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="193" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.24,616.02,338.34,7.77;13,150.95,626.82,149.40,7.94" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,434.76,616.02,45.82,7.77;13,150.95,626.98,86.87,7.77">Informationtheoretic metric learning</title>
		<author>
			<persName coords=""><forename type="first">Jason</forename><forename type="middle">V</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brian</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Prateek</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Suvrit</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Inderjit</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,254.28,626.82,18.93,7.73">ICML</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.24,119.96,338.34,7.77;14,150.95,130.76,237.83,7.94" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="14,418.73,119.96,61.85,7.77;14,150.95,130.92,175.99,7.77">Unsupervised visual domain adaptation using subspace alignment</title>
		<author>
			<persName coords=""><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amaury</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marc</forename><surname>Sebban</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,343.70,130.76,18.13,7.73">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.24,141.72,338.35,7.94;14,150.95,152.68,248.93,7.94" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="14,320.06,141.88,118.81,7.77">A short introduction to boosting</title>
		<author>
			<persName coords=""><forename type="first">Yoav</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Abe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,449.94,141.72,30.65,7.73;14,150.95,152.68,153.36,7.73">Journal-Japanese Society For Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">1612</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.24,163.80,338.34,7.77;14,150.95,174.59,99.61,7.94" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="14,310.35,163.80,170.23,7.77;14,150.95,174.76,35.99,7.77">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,203.49,174.59,19.73,7.73">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.24,185.72,338.34,7.77;14,150.95,196.51,309.26,7.94" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="14,318.09,185.72,162.48,7.77;14,150.95,196.67,12.95,7.77">Reshaping visual datasets for domain adaptation</title>
		<author>
			<persName coords=""><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,180.09,196.51,186.40,7.73">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1286" to="1294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.24,207.63,338.34,7.77;14,150.95,218.43,268.45,7.94" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="14,352.25,207.63,128.32,7.77;14,150.95,218.59,119.61,7.77">Domain adaptation for object recognition: An unsupervised approach</title>
		<author>
			<persName coords=""><forename type="first">Raghuraman</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruonan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,287.41,218.43,18.13,7.73">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="999" to="1006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.24,229.55,338.34,7.77;14,150.95,240.35,329.63,7.94;14,150.95,251.47,20.17,7.77" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="14,370.70,229.55,109.88,7.77;14,150.95,240.51,109.65,7.77">Discovering latent domains for multisource domain adaptation</title>
		<author>
			<persName coords=""><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brian</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,277.04,240.35,106.08,7.73">Computer Vision-ECCV 2012</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="702" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.24,262.43,338.34,7.77;14,150.95,273.22,233.60,7.94" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="14,360.99,262.43,119.59,7.77;14,150.95,273.39,102.53,7.77">Robust visual domain adaptation with low-rank reconstruction</title>
		<author>
			<persName coords=""><forename type="first">I-Hong</forename><surname>Jhuo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,270.00,273.22,19.73,7.73">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2168" to="2175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.24,284.35,338.34,7.77;14,150.95,295.30,46.07,7.77" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="14,197.31,284.35,238.54,7.77">A literature survey on domain adaptation of statistical classifiers</title>
		<author>
			<persName coords=""><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="14,142.24,306.26,338.34,7.77;14,150.95,317.06,277.07,7.94" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="14,284.02,306.26,196.55,7.77;14,150.95,317.22,127.20,7.77">What you saw is not what you get: Domain adaptation using asymmetric kernel transforms</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,294.79,317.06,19.73,7.73">CVPR</title>
		<imprint>
			<date type="published" when="2011-06">June 2011</date>
			<biblScope unit="page" from="1785" to="1792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.24,328.18,338.34,7.77;14,150.95,338.98,224.55,7.94" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="14,347.96,328.18,132.62,7.77;14,150.95,339.14,159.27,7.77">Distance-based image classification: Generalizing to new classes at near zero cost</title>
		<author>
			<persName coords=""><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,316.79,338.98,18.28,7.73">PAMI</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.24,349.94,338.35,7.94;14,150.95,360.90,223.68,7.94" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="14,283.87,350.10,108.01,7.77">A survey on transfer learning</title>
		<author>
			<persName coords=""><forename type="first">Jialin</forename><surname>Sinno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qiang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,402.23,349.94,78.36,7.73;14,150.95,360.90,126.30,7.73">Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>IEEE Transactions on</note>
</biblStruct>

<biblStruct coords="14,142.24,372.02,338.34,7.77;14,150.95,382.81,329.63,7.94;14,150.95,393.77,119.29,7.94" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="14,427.48,372.02,53.10,7.77;14,150.95,382.98,155.17,7.77">Visual domain adaptation: An overview of recent advances</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raghuraman</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruonan</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rama</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,313.21,382.81,167.38,7.73;14,150.95,393.77,26.37,7.73">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1019" to="1029" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.24,404.89,338.34,7.77;14,150.95,415.69,329.64,7.94;14,150.95,426.65,329.63,7.94;14,150.95,437.77,51.38,7.77" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="14,362.87,404.89,117.70,7.77;14,150.95,415.85,55.17,7.77">Adapting visual category models to new domains</title>
		<author>
			<persName coords=""><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brian</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,455.43,415.69,20.13,7.73">ECCV</title>
		<title level="s" coord="14,211.65,426.65,127.68,7.73">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Petros</forename><surname>Maragos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nikos</forename><surname>Paragios</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6314</biblScope>
			<biblScope unit="page" from="213" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.24,448.57,338.35,7.94;14,150.95,459.69,20.17,7.77" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="14,290.73,448.73,151.27,7.77">Frustratingly easy nbnn domain adaptation</title>
		<author>
			<persName coords=""><forename type="first">Tatiana</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,457.93,448.57,18.13,7.73">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.24,470.65,338.34,7.77;14,150.95,481.44,146.18,7.94" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="14,261.45,470.65,219.12,7.77;14,150.95,481.61,45.79,7.77">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,203.25,481.44,19.32,7.73">JMLR</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="207" to="244" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.24,492.56,338.34,7.77;14,150.95,503.36,329.63,7.94;14,150.95,514.32,329.63,7.94;14,150.95,525.44,20.17,7.77" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="14,260.10,492.56,203.53,7.77">Multi-source transfer learning with multi-view adaboost</title>
		<author>
			<persName coords=""><forename type="first">Zhijie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shiliang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,430.17,503.36,16.94,7.73">NIPS</title>
		<title level="s" coord="14,182.73,514.32,128.99,7.73">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Tingwen</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Zhigang</forename><surname>Zeng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Chuandong</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Chising</forename><surname>Leung</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">7665</biblScope>
			<biblScope unit="page" from="332" to="339" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
