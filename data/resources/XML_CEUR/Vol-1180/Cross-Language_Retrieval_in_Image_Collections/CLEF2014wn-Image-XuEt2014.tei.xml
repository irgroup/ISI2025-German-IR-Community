<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,149.21,115.96,316.92,12.62;1,206.28,133.89,202.79,12.62">MLIA at ImageCLFE 2014 Scalable Concept Image Annotation Challenge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,192.00,171.56,34.59,8.74"><forename type="first">Xing</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Advanced Information Technology</orgName>
								<orgName type="institution">Kyushu University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,235.17,171.56,71.45,8.74"><forename type="first">Atsushi</forename><surname>Shimada</surname></persName>
							<email>atsushi@limu.ait.kyushu-u.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Advanced Information Technology</orgName>
								<orgName type="institution">Kyushu University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,334.36,171.56,89.02,8.74"><forename type="first">Rin-Ichiro</forename><surname>Taniguchi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Advanced Information Technology</orgName>
								<orgName type="institution">Kyushu University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,149.21,115.96,316.92,12.62;1,206.28,133.89,202.79,12.62">MLIA at ImageCLFE 2014 Scalable Concept Image Annotation Challenge</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A7EBA07F26B9CEE671D18B3B1379C0FD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>imageclef</term>
					<term>image annotation</term>
					<term>social web data</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a large-scale image annotation system for the ImageCLEF 2014 Scalable Concept Image Annotation task. The annotation task, of this year, concentrated on developing annotation algorithms that rely only on data obtained automatically from the web. Since the sophisticated SVM based annotation techniques had been widely applied in the task last year (ImageCLEF 2013), for the task this year, we also adopt the SVM based annotation techniques and put our effort mainly on obtaining more accurate concepts assignment for training images. More specifically, we proposed a two-fold scheme to assign concepts to unlabeled training images: (1) A traditional process which stems the extracted web data of each training image from textual aspect, and make concepts assignment based on the appearance of each concept.</p><p>(2) An additional process which leverages the deep convolutional network toolbox Overfeat to predict labels (in ImageNet nouns) for each training image from visual aspect, then the predicted tags are mapped to concepts in ImageCLEF based on WordNet synonyms and hyponyms with semantic relations. Finally, the allocated concepts for each training image are generated based on a fusion step of the two-fold concepts assignment processes. Experimental results show that the proposed concepts assignment scheme is efficient to improve the assignment results of traditional textual processing and to allocate reasonable concepts for training images. Consequently, with an efficient SVMs solver based on Stochastic Gradient Descent, our annotation systems achieves competitive performance in the annotation task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this year ImageCLEF 2014 <ref type="bibr" coords="1,269.68,575.92,9.96,8.74" target="#b0">[1]</ref>, we participated the Scalable Concept Image Annotation challenge<ref type="foot" coords="1,226.43,586.43,3.97,6.37" target="#foot_0">1</ref>  <ref type="bibr" coords="1,233.45,587.88,15.50,8.74" target="#b8">[10]</ref> which aimed at developing more scalable image annotation system. The goal of this challenge is to develop annotation systems that for training only rely on unsupervised web data and other automatically obtainable resources. In contrast to traditional image annotation evaluations with labeled training data, this challenge requires work in more front, such as handling the noisy data, textual processing and multilabel annotations and scalability to unobserved labels.</p><p>Since this year is the third edition of the annotation challenge, regarding the methodology of annotation system, we can make several observations from the overview reports <ref type="bibr" coords="2,209.90,166.82,10.52,8.74" target="#b7">[9]</ref>  <ref type="bibr" coords="2,223.73,166.82,15.50,8.74" target="#b9">[11]</ref> of previous editions:</p><p>-The best performing system, TPT <ref type="bibr" coords="2,308.88,184.54,9.96,8.74" target="#b4">[6]</ref>, only used provided visual features, which indicated that the visual features provided by the organizers is sufficient enough and the other features extracted by several teams might be complementary. -The top 3 teams (TPT, MIL [4], and UNIMORE <ref type="bibr" coords="2,362.74,231.92,10.79,8.74" target="#b1">[2]</ref>) all utilized SVMs based algorithms to learn separate classifiers for each concept, which was verified to be superior to the K nearest neighbor (KNN) based annotation techniques used by other groups, such as RUC <ref type="bibr" coords="2,309.03,267.79,9.96,8.74" target="#b3">[5]</ref>, MICC <ref type="bibr" coords="2,356.08,267.79,9.96,8.74" target="#b6">[8]</ref>. -The textual processing and concepts assignment for training images were significant, since they directly affected the learning accuracy of concept classifiers.</p><p>The major difference of the challenge this year compared with previous editions is the proportions of "scaled" concepts. In the challenge last year, there are total 116 concepts (95 concepts for development set and 21 more for test set), the proportions of "scaled" concepts are 21 116 ≈ 0.181. On the contrast, in this year, there are total 207 concepts (107 concepts for development set and 100 more for test set), the proportions of "scaled" concepts are 100 207 ≈ 0.483. Thus it implies the significance of annotation system to be scalable and to generalize well to the new concepts.</p><p>To develop a robust and scalable annotation system, we believe that one of the intrinsic issues is to assign more appropriate concepts to training images. Once we have collected more accurate (positive/negative) samples for each concept, it is possible to improve the performance of concepts' classifiers. Thus for the contest, we mainly focus on the issue of accurate concepts assignment for training images. Besides the traditional textual information processing such as stopwords removal and stemming, which have been widely applied in previous editions. We also leverage the recent popular convolutional neural networks (C-NN) <ref type="bibr" coords="2,158.23,512.66,10.52,8.74" target="#b5">[7]</ref> to allocate tags (1K WordNet nouns) for each training images from visual aspect. As the CNN based method utilizes the deep neural network to improve classification task, we can rely on the tags predicted by Overfeat and map the tags to concepts of ImageCLFE vocabulary. Then a late fusion approach is used to decide the final concepts assignment for each training image. Finally, we train a linear SVM classifier for each concept (similar in development and test set) with the visual features provided by the organizers. To tackle the high dimensional large volumes of training data, we adopt the online learning strategy of stochastic gradient descent (SGD). We finally obtain competitive annotation performances in terms of mAP-samples, MF-concepts and MF-samples measures and are ranked the 4th place among all 11 groups on overall measure.</p><p>The rest of the paper is organized as follows. Section 2 demonstrates the architecture of proposed annotation system and we mainly discuss our concepts assignment scheme for training images. In Section 3, we describe our experimental setups and report the evaluation results obtained on both the development and the test sets. And Section 4 includes conclusion and some future direction of our works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed annotation system</head><p>The proposed annotation system is depicted in Figure <ref type="figure" coords="3,372.82,213.38,3.87,8.74" target="#fig_0">1</ref>. To assign more appropriate concepts for training images, we conduct a 2-fold scheme which explicitly leverages the provided textural information semantically (Section 2.1) and the training images visually (Section 2.2). Based on the reliable labeled training images, we further learn SVMs based concept classifiers using standard visual features provided by the organizers. To tackle the high dimensional features and large volumes of data, we use online learning method combined with SGD algorithm. Then we use the learnt stable concept classifiers for concept prediction of images in development and test sets. In the following subsections, we would like to depict the detailed procedure of each module of the diagram in Figure <ref type="figure" coords="3,457.15,320.97,3.87,8.74" target="#fig_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Text Processing Approach</head><p>The organizers of ImageCLFE 2014 provided several kinds of textural features of training images. Following the traditional text processing approach utilized last year, to efficiently process the textual features, we applied multiple filtering on the textural features. Regarding the modules of "Stopword removal and stemming" in Figure <ref type="figure" coords="3,228.25,656.12,3.87,8.74" target="#fig_0">1</ref>, the detailed processing procedures are:</p><p>-"Stopword removal and stemming" is performed on the "scofeats" files, where stopwords, misspelled words, words from different languages other than English, the titles of the original web pages are extracted and parsed. -We then matched the semantic relations of the remaining words with the list of concepts in development set based on WordNet 3.0<ref type="foot" coords="4,381.38,165.37,3.97,6.37" target="#foot_1">2</ref> . We extend the list of concepts with their synonyms, and examine whether current word matches with concept or its synonyms. -The Lucene <ref type="bibr" coords="4,205.62,202.68,10.52,8.74" target="#b2">[3]</ref> stemmer is adopted if the word does not exactly match with the list of concepts.</p><p>The output of the "Result filtering and refinement" produces a candidate set of concepts for each of the training image. Indeed, the processing approach in this subsection could be considered as a baseline as it gives many false negative and false positive concepts to training images. Therefore, besides the textual features of training images, it is reasonable to further consider the visuality of training images. For example, for a training image describing "airplane", and its textural features (web page, title, etc) contain words of "airplane pilot hats", simply applying the text processing approach would result in concepts "airplane", "person", and "hat" to be assigned to the training image. However, if it is possible to estimate the content of image visually in advance, then the unrelated concepts "person", "hat" could be rejected to the training image. Thus, in the next subsection, we would like to introduce a context mapping method to predict tags for training images in advance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Context mapping using CNN</head><p>To estimate the content of training images visually, we take advantages of a recently proposed toolbox Overfeat  In Figure <ref type="figure" coords="5,192.24,267.41,3.87,8.74" target="#fig_1">2</ref>, we give an example of the context mapping using CNN. The tags in blue rectangle are obtained from the previous "text processing" stage. The tags (with confidence scores) in green rectangle are tags predicted from Overfeat. For the context mapping procedure (in practice, we use the path similarity measure in NLTK toolbox as the semantic measure), we can get a candidate concept set {sky, airplane, vehicle, boat} based on the tags in green rectangle .</p><p>For the "Result filtering and refinement" module in Figure <ref type="figure" coords="5,402.93,339.14,3.87,8.74" target="#fig_0">1</ref>, it fuses the candidate concept set from both textual processing approach and context mapping with CNN. Since there are much more number of concepts produced by textual processing approach than context mapping with CNN, however, the concept set from textual processing approach is more coarse. Thus for the fusion strategy, we relied more on the concept set from context mapping with CNN and preserved the concepts with high similarity scores in concept set from textual processing approach. In Figure <ref type="figure" coords="5,224.92,422.83,3.87,8.74" target="#fig_1">2</ref>, the concepts in red rectangle are the final assigned concepts to the training image, which are considered to be semantically related to the training image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Visual features</head><p>Similar as the best result of TPT <ref type="bibr" coords="5,277.99,515.38,10.52,8.74" target="#b4">[6]</ref> in ImageCLFE 2013 annotation task, we use the visual features provided by the organizer including GIST, Color Histogram, SIFT, C-SIFT, RGB-SIFT and OPPONENT-SIFT. For all SIFT-based descriptors, a bag-of-words (BoW) representation is provided. An early fusion is made by concatenating all the features provided (global color histogram, getlf, CSIFT, GIST, opponent SIFT, RGB-SIFT, SIFT) resulting in a 21,312 dimension space. Global features GIST and Color Histogram are normalized using L2 norm, and SIFT-based features are normalized using L1 norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation measures</head><p>For the performance measures used to evaluate the runs, there are three standard measures: mean F-measure for the samples (MF-samples), mean F-measure for the concepts (MF-concepts) and the mean average precision for the samples (MAP-samples). The MF is computed analyzing both the samples (MF-samples) and the concepts (MF-concepts), whereas the MAP is computed analyzing the samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training SVM classifiers for concepts</head><p>Following the SVM based annotation techniques which had achieved best annotation performance last year <ref type="bibr" coords="6,275.44,214.21,10.52,8.74" target="#b1">[2]</ref> [6], again we trained "one-versus-all" SVM classifier for each concept. The popular SVM solvers, such as SVMlight, LibSVM, they are not feasible for training large volumes of data with high dimension, since these batch methods need to pre-load entire training data into memory, to compute gradient in each iteration. Thus it is difficult to directly utilize these SVM solvers. According to the configuration of our machine (an Intel Core i7 2600 CPU (3.4 GHz) and 16 GB RAM), we take into account a better solution by the stochastic gradient descent (SGD) algorithm which is more efficient for training SVM classifiers with large-scale data. Different from the batch method, in the SGD algorithm, training sample is fed one by one to calculate the gradients and update rules of model parameters. Although the SGD algorithm might need more iteration loops to reach convergence, it requires much less memory cost which is more appropriate for large-scale training samples and online learning manner.</p><p>According to the advices in <ref type="bibr" coords="6,271.19,381.58,9.96,8.74" target="#b1">[2]</ref>, we randomize the training data and load the data in chunks which fit in memory, then train the different classifiers on further randomizations of chunks, so that different epochs will get the chunks data with different ordering which leads the learnt classifiers to be stable. We repeat this training process on training set for 5 times to train SVM classifier for each concept of development set and cross validate the F-measure on development set. Then we select the parameters of best performance on development set to further learn classifiers for concepts of test set. To predict concepts for images in development and test sets, we use the trained concepts's classifiers and obtain decision scores for each concept by thresholding the confidence score at zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Inside analysis of annotation results</head><p>We first discuss the proposed 2-fold concept assignments to training images, and evaluate its influence of learning accuracy of concept classifiers. We first conduct experiments on the development set and then extend the 2-fold scheme to test set. Here we consider three settings: (1) "Single-Fold A": the single fold scheme of traditional textual information process ("Stopword removal and stemming" module in Figure <ref type="figure" coords="6,242.83,596.35,3.87,8.74" target="#fig_0">1</ref>). ( <ref type="formula" coords="6,262.01,596.35,4.24,8.74">2</ref>) "Single-Fold B": the single fold scheme of CNN based tag prediction process ("Tag prediction with CNN" and "Context mapping" modules), (3) "Two-Folds": the fusion process of both "Single-Fold A" and "Single-Fold B". We limited the maximum number of concepts assigned to each training image to be 4. Then we use the learned SVMs classifiers from the labeled training data to predict concepts for images in development set (the top 5 ranked concepts are considered to be the final predicted concepts). Table <ref type="table" coords="7,475.62,119.00,4.98,8.74" target="#tab_1">1</ref> shows the annotation performance of three settings, and one of the baselines provided by the organizers is also included for comparison. It can be observed that three settings consistently improve the performance of baseline. In particular, the tags predicted by Overfeat is considerably accurate for training images. "Single-Fold B" outperforms the "Single-Fold A" setting of traditional textual information scheme, which implies the tags is highly coherent with the concepts in ImageCLFE. Moreover, when fusing the two settings to formulate proposed "Two-Folds" setting, the result is further improved on all three measures.  Then we evaluate the effect of "Result filtering and refinement" module. Since in the experiment settings above, we restrict the number (denoted by K) of assigned concepts to each training image as K = 4. And it is reasonable that the value of K could influence the learning accuracy of concept classifiers, as it directly determines the quality of training samples for each concept. Thus, we further vary the value of K (ranges from 1 to 10), and explore the optimal K for concept assignments for training images. The annotation performance on development set with varying K is shown in Figure <ref type="figure" coords="8,364.36,142.91,3.87,8.74" target="#fig_2">3</ref>. It can be observed from Figure <ref type="figure" coords="8,166.69,154.86,4.98,8.74" target="#fig_2">3</ref> that: <ref type="bibr" coords="8,200.34,154.86,12.73,8.74" target="#b0">(1)</ref> The peaks of both MF-concept and MF-sample are reached when K = 6, and peak of MAP-sample reaches the peak when K = 9. (2) The MAP-sample is more sensitive to K since the number of ground truth concepts for each image in development set ranges from 1 to 11 (with average 3.52). Based on these observations, finally we choose K ∈ <ref type="bibr" coords="8,333.30,202.68,10.52,8.74" target="#b4">[6,</ref><ref type="bibr" coords="8,345.48,202.68,7.75,8.74" target="#b5">7,</ref><ref type="bibr" coords="8,354.89,202.68,7.75,8.74" target="#b6">8,</ref><ref type="bibr" coords="8,364.30,202.68,7.75,8.74" target="#b7">9,</ref><ref type="bibr" coords="8,373.71,202.68,12.73,8.74" target="#b8">10]</ref> for our latter submit runs of test set.</p><p>For the test set, we submitted ten runs <ref type="foot" coords="8,321.77,225.15,3.97,6.37" target="#foot_4">5</ref> . Here we would like to present our best 5 runs with baselines provided by organizers and the best runs from the other groups. We can learn from the overall results in Table <ref type="table" coords="8,400.68,250.51,4.98,8.74" target="#tab_2">2</ref> that: (1) All our submitted runs are beyond the best baseline result for the test set according to all measures. Looking into the overall participants results list, our best runs are at position 6, 3 and 5 order by the MF-sample, MF-concept and MAP-sample respectively for the test set, and position 4 for the overall performance. It means that our best runs are competitive compared with other results. However, there is still a considerable gap between our best runs and the topranked runs from KDEVIR group. Although currently we are not able to explore the details of their proposed annotation technique, there are still space to improve our annotation system itself from the following aspects: (1) In our current system, we directly utilized the Overfeat toolbox for tag prediction of training images, a more reasonable choice is that we can generate CNN visual features and directly use these visual features to learn concept classifiers. Indeed, several teams such as MIL and MindLab used the CNN visual features. (2) Currently, the "Context mapping" module only considered mapping the tags from Overfeat to ImageCLEF with its synonymous/hyponyms in WordNet, and the similarity measure from NLTK toolbox might not be precise to map the correct results. An optional choice is modeling the context based similarity measure of tags depending on the Flickr image metadata, which is more efficient to capture the semantic associations from the practical circumstance. (3) Our concept modeling (SVMs based concept classifiers learning) is not elaborately optimized and tuned, because of the limitations of hardware configurations and consumption of resources. Our system capability should be improved if we could overcome these limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we presented our annotation system developed to participate at ImageCLEF 2014 for the Scalable Concept Image Annotation task. Our proposal focus on improving the accuracy of concept assignments for training images. We proposed a 2-fold concept assignments scheme which explicitly leverages the provided textural information semantically (Section 2.1) and the training images visually. To learn concept classifiers, we adopted the sophisticated SVM based model, and took the SGD algorithm to deal with large scale settings of this task. Experimental results show that our proposal on both visual and textual information processing are necessary to build a competitive system. Moreover, we also considered potential future directions to further improve current system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,184.43,533.39,246.50,7.89;3,143.41,352.90,328.68,165.71"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of proposed annotation system architecture</figDesc><graphic coords="3,143.41,352.90,328.68,165.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,167.94,236.03,279.47,7.89;5,152.06,116.03,135.32,105.23"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Example of context mapping using CNN on a training image.</figDesc><graphic coords="5,152.06,116.03,135.32,105.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,168.62,572.81,278.11,7.89;7,143.41,385.39,329.02,172.67"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Annotation performance on development set with varying K.</figDesc><graphic coords="7,143.41,385.39,329.02,172.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,134.76,438.35,345.85,173.56"><head></head><label></label><figDesc>For each of the tag predicted from Overfeat, we calculate its semantic similarity to the concept list of development set, and mapping it to the most similar concept.</figDesc><table coords="5,295.68,120.27,162.17,94.75"><row><cell>sky 0.20241 trimaran 0.150161 speedboat 0.0877134 wing 0.0797028 warplane 0.0761503 paddle 0.0544154 airliner 0.047078 canoe 0.0456207 snowmobile 0.028857 flagpole 0.0329707</cell><cell>instrument space toy cloudless cityscape person cartoon sign protest</cell><cell>airplane sky vehicle cloudless cityscape person cartoon boat</cell></row></table><note coords="4,289.42,438.35,191.17,10.18;4,134.76,451.75,345.84,8.74;4,134.76,463.71,345.84,8.74;4,134.76,474.22,345.84,10.17;4,134.76,487.62,345.84,8.74;4,134.76,499.57,345.85,8.74;4,134.76,511.53,345.84,8.74;4,134.76,523.48,345.85,8.74;4,134.76,535.44,169.10,8.74;4,140.99,555.33,339.61,8.77;4,151.70,567.32,45.74,8.74;4,140.99,579.24,5.73,8.77"><p><p><p><p><p>3 </p>, which is an image recognizer and feature extractor built around a deep convolutional neural network (CNN). We consider this powerful toolbox for two reasons: (1) It achieved competitive classification results on ImageNet 2013 contest 4 . (2) OverFeat convolutional net was trained on WordNet 1K nouns, which is consistent to the concept list of ImageCLFE. Thus it is rational to predict tags for training images based on the Overfeat and mapping the tags to ImageCLFE using a built context mapping rule. Regarding the modules "Tag prediction with CNN" and "Context mapping" in Figure</p>1</p>, the detailed processing procedures are:</p>-For a given training image, we directly use the Overfeat toolbox to predict tags for it. -</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,134.76,247.27,345.82,89.03"><head>Table 1 .</head><label>1</label><figDesc>Annotation results on development set: three settings of textual information processing scheme of concept assignments for training images.</figDesc><table coords="7,196.73,279.03,218.82,57.27"><row><cell>Run</cell><cell cols="3">MF-sample MF-concept MAP-sample</cell></row><row><cell cols="2">Baseline (SIFT) 0.1342</cell><cell>0.2261</cell><cell>0.2254</cell></row><row><cell>Single-Fold A</cell><cell>0.218</cell><cell>0.203</cell><cell>0.3321</cell></row><row><cell>Single-Fold B</cell><cell>0.2693</cell><cell>0.2445</cell><cell>0.3622</cell></row><row><cell>Two-Folds</cell><cell>0.3105</cell><cell>0.3224</cell><cell>0.3781</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,134.76,341.77,345.82,227.32"><head>Table 2 .</head><label>2</label><figDesc>Annotation results of our best 5 runs on the test set, compared best runs of baselines and other groups.</figDesc><table coords="8,193.57,373.53,225.14,195.56"><row><cell>Run</cell><cell cols="3">MF-sample MF-concept MAP-sample</cell></row><row><cell>Baseline (oppsift)</cell><cell>16.7</cell><cell>9.8</cell><cell>20.2</cell></row><row><cell>kdevir 09</cell><cell>37.7</cell><cell>54.7</cell><cell>36.8</cell></row><row><cell>MIL 03</cell><cell>27.5</cell><cell>34.7</cell><cell>36.9</cell></row><row><cell>MindLab 01</cell><cell>25.8</cell><cell>30.7</cell><cell>37</cell></row><row><cell>DISA-MU 04</cell><cell>29.7</cell><cell>19.1</cell><cell>34.3</cell></row><row><cell>RUC 05</cell><cell>31.1</cell><cell>25</cell><cell>27.5</cell></row><row><cell>IPL 09</cell><cell>18.4</cell><cell>15.8</cell><cell>23.4</cell></row><row><cell>IMC-FU 01</cell><cell>16.3</cell><cell>12.5</cell><cell>25.1</cell></row><row><cell>INAOE 05</cell><cell>5.3</cell><cell>10.3</cell><cell>9.6</cell></row><row><cell>NII 01</cell><cell>13</cell><cell>2.3</cell><cell>14.7</cell></row><row><cell>FINKI 01</cell><cell>7.2</cell><cell>4.7</cell><cell>6.9</cell></row><row><cell>MLIA 09</cell><cell>24.8</cell><cell>33.2</cell><cell>27.8</cell></row><row><cell>MLIA 10</cell><cell>24.8</cell><cell>33.2</cell><cell>27.9</cell></row><row><cell>MLIA 08</cell><cell>24.6</cell><cell>33.3</cell><cell>27.4</cell></row><row><cell>MLIA 07</cell><cell>24.4</cell><cell>33.5</cell><cell>26.9</cell></row><row><cell>MLIA 06</cell><cell>24.1</cell><cell>33.6</cell><cell>26.3</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,152.70,657.44,188.29,7.47"><p>http://www.imageclef.org/2014/annotation</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,152.70,635.52,136.51,7.47"><p>http://wordnet.princeton.edu/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,152.70,646.48,263.60,7.47"><p>http://cilvr.nyu.edu/doku.php?id=software:overfeat:start</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,152.70,657.44,273.51,7.47"><p>http://www.image-net.org/challenges/LSVRC/2013/results.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="8,152.70,657.44,225.94,7.47"><p>http://www.imageclef.org/2014/annotation/results</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,142.95,484.15,337.63,7.86;9,151.52,492.84,329.06,10.12;9,151.52,506.06,329.06,7.86;9,151.52,517.02,284.60,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,151.52,506.06,228.05,7.86">ImageCLEF 2014: Overview and analysis of the results</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Martinez-Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Patricia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Marvasti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Üsküdarlı</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cazorla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Garcia-Varea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Morell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,402.32,506.06,74.13,7.86">CLEF proceedings</title>
		<title level="s" coord="9,151.52,517.02,141.41,7.86">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.95,527.44,337.62,7.86;9,151.52,538.40,329.06,7.86;9,151.52,549.36,244.65,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,151.52,538.40,258.46,7.86">Unimore at imageclef 2013: Scalable concept image annotation</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Grana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Manfredi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Martoglia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Mandreoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,433.36,538.40,47.22,7.86;9,151.52,549.36,121.52,7.86">CLEF 2013 Evaluation Labs andWorkshop</title>
		<title level="s" coord="9,280.91,549.36,86.60,7.86">OnlineWorking Notes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.95,559.78,335.32,7.86;9,139.37,570.21,341.21,7.86;9,151.52,581.16,329.06,7.86;9,151.52,592.12,25.60,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,344.89,559.78,65.38,7.86">Lucene in action</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hatcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Gospodnetic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mccandless</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hidaka</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Gunji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,290.34,570.21,190.25,7.86;9,151.52,581.16,41.89,7.86;9,214.89,581.16,265.70,7.86">Mil at imageclef 2013: Scalable system for image annotation</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>CLEF 2013 Evaluation Labs andWorkshop, OnlineWorking Notes</note>
</biblStruct>

<biblStruct coords="9,142.95,602.54,337.64,7.86;9,151.52,613.50,329.07,7.86;9,151.52,624.46,204.34,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,374.91,602.54,105.68,7.86;9,151.52,613.50,212.88,7.86">Renmin university of china at imageclef 2013 scalable concept image annotation</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,386.83,613.50,93.76,7.86;9,151.52,624.46,175.66,7.86">CLEF 2013 Evaluation Labs and Workshop, Online Working Notes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.95,634.88,337.62,7.86;9,151.52,645.84,329.07,7.86;9,151.52,656.80,223.27,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,194.41,634.88,286.16,7.86;9,151.52,645.84,227.10,7.86">Telecom paristech at imageclef 2013 scalable concept image annotation task: Winning annotations with context dependent svms</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sahbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,400.48,645.84,80.11,7.86;9,151.52,656.80,97.12,7.86">CLEF 2013 Evaluation Labs and Workshop</title>
		<title level="s" coord="9,256.46,656.80,89.67,7.86">Online Working Notes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.95,119.68,337.63,7.86;10,151.52,130.64,329.07,7.86;10,151.52,141.60,113.96,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="10,443.44,119.68,37.15,7.86;10,151.52,130.64,324.82,7.86">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno>CoRR abs/1312.6229</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.95,152.55,337.63,7.86;10,151.52,163.51,329.06,7.86;10,151.52,174.47,51.50,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,334.40,152.55,146.18,7.86;10,151.52,163.51,57.14,7.86">Micc at imageclef 2013 image annotation subtask</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Uricchio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bertini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bimbo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,232.00,163.51,248.58,7.86;10,151.52,174.47,22.84,7.86">CLEF 2013 Evaluation Labs andWorkshop, Online Working Notes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.95,185.43,337.63,7.86;10,151.52,196.39,329.06,7.86;10,151.52,207.35,88.64,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,261.91,185.43,218.67,7.86;10,151.52,196.39,93.44,7.86">Overview of the imageclef 2012 scalable concept image annotation subtask</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,267.75,196.39,212.84,7.86;10,151.52,207.35,59.97,7.86">CLEF 2012 Evaluation Labs and Workshop, Online Working Notes</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,218.31,337.97,7.86;10,151.52,229.27,329.06,7.86;10,151.52,240.23,88.64,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,262.57,218.31,218.01,7.86;10,151.52,229.27,94.67,7.86">Overview of the ImageCLEF 2014 Scalable Concept Image Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,268.79,229.27,211.79,7.86;10,151.52,240.23,59.97,7.86">CLEF 2014 Evaluation Labs and Workshop, Online Working Notes</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,251.19,337.97,7.86;10,151.52,262.14,329.07,7.86;10,151.52,273.10,158.27,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,315.60,251.19,164.98,7.86;10,151.52,262.14,133.92,7.86">Overview of the imageclef 2013 scalable concept image annotation subtask</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,305.36,262.14,170.48,7.86">CLEF 2013 Evaluation Labs and Workshop</title>
		<title level="s" coord="10,151.52,273.10,87.99,7.86">Online Working Notes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
