<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,148.45,116.95,318.45,12.62;1,173.53,134.89,268.30,12.62">DISA at ImageCLEF 2014: The search-based solution for scalable image annotation</title>
				<funder ref="#_dPjyf8z">
					<orgName type="full">Czech national</orgName>
				</funder>
				<funder ref="#_aqQNHA2">
					<orgName type="full">METACentrum</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,171.77,172.56,65.27,8.74"><forename type="first">Petra</forename><surname>Budikova</surname></persName>
							<email>budikova@fi.muni.cz</email>
							<affiliation key="aff0">
								<orgName type="institution">Masaryk University</orgName>
								<address>
									<settlement>Brno</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,245.14,172.56,51.55,8.74"><forename type="first">Jan</forename><surname>Botorek</surname></persName>
							<email>botorek@fi.muni.cz</email>
							<affiliation key="aff0">
								<orgName type="institution">Masaryk University</orgName>
								<address>
									<settlement>Brno</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,304.66,172.56,56.53,8.74"><forename type="first">Michal</forename><surname>Batko</surname></persName>
							<email>batko@fi.muni.cz</email>
							<affiliation key="aff0">
								<orgName type="institution">Masaryk University</orgName>
								<address>
									<settlement>Brno</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,388.66,172.56,54.93,8.74"><forename type="first">Pavel</forename><surname>Zezula</surname></persName>
							<email>zezula@fi.muni.cz</email>
							<affiliation key="aff0">
								<orgName type="institution">Masaryk University</orgName>
								<address>
									<settlement>Brno</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,148.45,116.95,318.45,12.62;1,173.53,134.89,268.30,12.62">DISA at ImageCLEF 2014: The search-based solution for scalable image annotation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">59AE8573115B9437E960B98F827DA3EC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents an annotation tool developed by the DISA Laboratory for the ImageCLEF 2014 Scalable Concept Image Annotation challenge. Our solution exploits the search-based annotation paradigm and utilizes several sources of semantic information to determine the relevance of candidate concepts. Rather than relying on the quality of training data, our approach profits from the large quantities of information available in large image collections and semantic knowledge bases. The results achieved by our system confirm that this approach is very promising for scalable image annotation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>While modern technologies allow people to create and store data in many forms (e.g. images, video, etc.), the most natural way of expressing one's need for a specific piece of data is still a text query. Natural language remains the primary means of information transfer both for person-to-person communication and person-to-computer interactions. However, a lot of existing digital data is not associated with any text information that would help users access or categorize the data. The purpose of automatic annotation is to increase the findability of information by bridging the gap between data and query representations.</p><p>Since 2006, the ImageCLEF initiative has been encouraging the development of image annotation tools by organizing competitions on automatic image classification and annotation. In the ImageCLEF 2014 Scalable Concept Image Annotation challenge, participants were required to develop solutions that can annotate common personal images using only automatically obtained training data. Utilization of manually prepared training samples was forbidden to ensure that the solutions would easily scale to larger concept sets.</p><p>This paper presents the annotation tool developed for this task by the DISA Laboratory<ref type="foot" coords="1,183.67,577.47,3.97,6.12" target="#foot_0">1</ref> at Masaryk University. Our solution exploits the search-based annotation paradigm and utilizes several sources of semantic information to determine the probability of candidate concepts. Rather than relying on the quality of training data, our approach profits from the large quantities of information available in large image collections and semantic knowledge bases. The results achieved by our system confirm the strengths of this approach.</p><p>The rest of the paper is structured as follows. First, we briefly review the task definition and discuss which resources can be used. Next, we describe our approach and individual components of our solution. Analysis of results is provided in Section 4. Section 5 concludes the paper and outlines our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Scalable Concept Image Annotation Task</head><p>The problem offered by this year's Scalable Concept Image Annotation (SCIA) challenge <ref type="bibr" coords="2,178.10,226.01,10.52,8.74" target="#b5">[6,</ref><ref type="bibr" coords="2,190.28,226.01,12.73,8.74" target="#b12">13]</ref> is basically a standard annotation task, where an input image needs to be connected to relevant concepts from a fixed set of candidate concepts. The input images are not accompanied by any descriptive metadata such as EXIF or GPS, so that only the visual image content can serve as annotation input. For each test image, there is a list of SCIA concepts from which the relevant ones need to be selected. Each concept is defined by one keyword, a link to relevant WordNet nodes, and, in most cases, a link to a relevant Wikipedia page.</p><p>Annotation tasks of this type have been studied for more than a decade and some impressive results have already been achieved <ref type="bibr" coords="2,362.63,321.89,9.97,8.74" target="#b8">[9]</ref>. However, most existing solutions rely on large amounts of manually labeled training data, which limits the concept-wise scalability of such methods and their applicability to many realworld scenarios. As its name suggests, the Scalable Concept Image Annotation task takes into consideration not only the annotation precision and recall, but also the scalability of annotation techniques. The proposed solutions should be able to adapt easily when the list of concepts is changed, and the performance should generalize well to concepts not observed during development. Therefore, participants were not provided with hand-labeled training data and were not allowed to use resources that require significant manual preprocessing. Instead, they were encouraged to exploit data that can be crawled from the web or otherwise easily obtained.</p><p>Accordingly, the training dataset provided by organizers consists of 500K images downloaded from the web, and the accompanying web pages. The images were obtained by querying popular image search engines (namely Google, Bing and Yahoo) using words in the English dictionary. For each image, the web page that contained the image was downloaded and processed to extract selected textual features. An effort was made to avoid including near duplicates and message images (such as "deleted image") in the dataset, however the dataset can be considered and is supposed to be very noisy. The raw images and web pages were further preprocessed by competition organizers to ease the participation in the task, resulting in several visual and text descriptors as detailed in <ref type="bibr" coords="2,442.17,573.19,14.61,8.74" target="#b12">[13]</ref>.</p><p>The actual competition task consists of annotating 7291 images with different concept lists. Altogether, there are 207 concepts, with the size of individual concept lists ranging from 40 to 207 concepts. Prior to releasing the test image set, which became available a month before the competition deadline, participants were provided with a development set of query images and concept lists, for which a ground truth of relevant concepts was also published. The development set contains 1940 images and only 107 concepts out of the final 207.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Utilization of Additional Resources</head><p>Apart from the 500K set of web images, participants were encouraged to exploit additional knowledge sources such as ontologies, language models, etc., as long as these were not manually prepared and were easily available. Since we found it difficult to decide what level of manual effort is acceptable (e.g. most ontologies are created with significant human participation), we discussed several resources that we were considering with the SCIA task organizers:</p><p>WordNet The WordNet lexical database <ref type="bibr" coords="3,319.30,219.19,10.52,8.74" target="#b7">[8]</ref> is a comprehensive semantic tool interlinking dictionary, thesaurus and language grammar book. The basic building block of WordNet hierarchy is a synset, an object which unifies synonymous words into a single item. On top of synsets, different semantic relations are encoded in the WordNet structure, e.g. hypernymy/hyponymy (super-type and sub-type relation) or meronymy (part-whole relation). Currently, 117 000 synsets are available in the English WordNet 3.0.</p><p>The WordNet is developed manually by language experts, which is not in accordance with the SCIA task rules. However, it can be used to solve the SCIA challenge as it is an existing resource with wide coverage that does not limit the concept-wise scalability of annotation tools. Indeed, many last year's solutions of the SCIA task utilized WordNet to learn about semantic relationships between concepts <ref type="bibr" coords="3,175.22,362.65,14.61,8.74" target="#b13">[14]</ref>.</p><p>ImageNet The ImageNet <ref type="bibr" coords="3,249.99,384.35,10.52,8.74" target="#b6">[7]</ref> is an image database organized according to the WordNet hierarchy (currently only the nouns), in which each node of the hierarchy is depicted by hundreds or even a few thousands of images. Currently, there are about 14M images illustrating 22 000 synsets from selected branches of the WordNet. The images are collected from the web by text queries formulated from the words in the particular synset. In the next step, a crowdsourcing platform is utilized for manual cleaning of the downloaded images.</p><p>According to the organizers, the ImageNet should not be used for solving the SCIA challenge. Although it is easily available, its scope is limited and extending it to other concepts is very expensive in terms of human labor.</p><p>Profiset The Profiset <ref type="bibr" coords="3,231.62,513.65,10.52,8.74" target="#b3">[4]</ref> is a large collection of annotated images available for research purposes. The collection contains 20M high-quality images with rich keyword annotations, which were obtained from a web-site that sells stock images produced by photographers from all over the world. The data contained in the Profiset collection was created manually, however this labor was not focused on providing training data for annotation learning. The Profiset is thus a by-product of another activity and can be seen as ordinary web data downloaded from a well-chosen site. It is also important to mention that the image annotations in Profiset have no fixed vocabulary and their quality is not centrally supervised. At the same time, however, the photographers are interested in selling their photos and are thus motivated to provide rich sets of relevant keywords.</p><p>The organizers agreed that the Profiset can be used as a resource for the SCIA task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach</head><p>Similar to our previous participation in an ImageCLEF annotation competition in 2011 <ref type="bibr" coords="4,191.52,156.57,9.96,8.74" target="#b4">[5]</ref>, the DISA solution is based on the MUFIN Image Annotation software, a tool for general-purpose image annotation which we have been developing for several years now <ref type="bibr" coords="4,258.05,180.48,9.96,8.74" target="#b0">[1]</ref>. The MUFIN Image Annotation tool follows the search-based approach to image annotation, exploiting content-based retrieval in a very large image collection and a subsequent analysis of descriptions of similar images. In 2011, we were experimenting with applying this approach in a task more suited for traditional machine learning (the 2011 Annotation Task offered manually labeled training data). However, we believe that the search-based approach is extremely suitable for the 2014 Scalable Concept Image Annotation.</p><p>The general overview of the solution developed for the SCIA task is provided in Figure <ref type="figure" coords="4,180.71,276.26,3.88,8.74" target="#fig_2">1</ref>. In the first phase, the annotation tool retrieves visually similar images from a suitable image collection. Next, textual descriptions of similar images are analyzed with the help of various semantic resources. The text is split into separate words and transformed into synsets, which are expanded and enhanced by semantic relations. The probability of relevance of each synset is computed with respect to the initial probability value assigned to that synset and the types and amount of relations formed with other synsets. Finally, synsets linked to the candidate concept words (i.e. the words in the list of concepts provided with the particular test image) are ordered by probability and a fixed number of top-ranking ones is selected as the final image description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Retrieval of Similar Images</head><p>The search-based approach to image annotation is based on the assumption that in a sufficiently large collection, images with similar content to any given query image are likely to appear. If these can be identified by a suitable content-based retrieval technique, their metadata such as accompanying texts, labels, etc. can be exploited to obtain text information about the query image.</p><p>In our solution, we utilize the MUFIN similarity search system <ref type="bibr" coords="4,430.42,495.01,10.52,8.74" target="#b1">[2]</ref> to index and search images. The MUFIN system exploits state-of-the-art metric indexing structures <ref type="bibr" coords="4,180.93,518.92,15.49,8.74" target="#b11">[12]</ref> and enables fast retrieval of similar images from very large collections. The visual similarity of images is measured by a weighted combination of five MPEG7 global visual descriptors as detailed in <ref type="bibr" coords="4,356.92,542.83,14.62,8.74" target="#b10">[11]</ref>. For each test image, the k most similar images are selected; if more datasets are used, the most similar images from all searches are merged, sorted by visual distance, and the k best are selected. The values of k are discussed in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Collections</head><p>The choice of image collection(s) over which the contentbased retrieval is evaluated is a crucial factor of the whole annotation process. There should be as many images as possible in the chosen collection, the images should be relevant for the domain of the queries, and their descriptions should be rich and precise. Naturally, these requirements are in a conflict -while it is   relatively easy to obtain large collections of image data (at least in the domain of general-purpose images appearing in personal photo-galleries), it is very difficult to automatically collect images with high-quality descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word probability computation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text analysis</head><p>Our solution utilizes two annotated image collections -the 20M Profiset database (introduced in Section 2) and the 500K set of training images provided by organizers (we denote this collection as the SCIA trainset). The Profiset represents a large collection of general-purpose images with as precise annotations as can be achieved in a non-controlled environment. The SCIA trainset is smaller and the quality of text data is much lower; on the other hand, it has been designed to contain images for all keywords from the SCIA task concept lists, which makes it a very good fallback for topics not sufficiently covered in Profiset.</p><p>We further considered the 14M ImageNet collection which provides reliable linking between visual content and semantics of images, but we found out that this resource is not acceptable for the SCIA task due to its low scalability (as discussed in Section 2). We also experimented with the 100M CoPhIR image dataset <ref type="bibr" coords="5,170.12,633.20,10.52,8.74" target="#b2">[3]</ref> that was built automatically by downloading Flickr photos, but we found the text metadata to be too noisy for the purpose of automatic image annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">From Similar Images to SCIA concepts</head><p>In the second phase of the annotation process, the descriptions of images returned by content-based retrieval need to be analyzed and linked to SCIA concepts of a given query to decide about their (ir)relevance. During this phase, our solution relies mainly on the WordNet semantic structure, but we also employ several other resources. The following sections explain how we link keywords from similar images' annotations to WordNet synsets and how the probability of individual synsets is computed. Various parameters of the whole process are summarized in Table <ref type="table" coords="6,229.21,222.89,3.88,8.74" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selection of Initial Keywords</head><p>Having retrieved the set of similar images, we first divide their text metadata into separate words and compute the frequency of each word. In case of Profiset data, we use directly the keyword annotations of individual images, whereas for SCIA trainset we utilize the scofeat descriptors extracted from the respective web pages <ref type="bibr" coords="6,310.76,299.88,14.62,8.74" target="#b12">[13]</ref>. This way, we obtain a set of initial keywords.</p><p>This set can be further enriched by adding a fixed number of most frequently co-occurring words for each initial word. The lists of co-occurring words were obtained using the method described in <ref type="bibr" coords="6,305.75,347.70,15.50,8.74" target="#b9">[10]</ref> applied to the ukWac corpus<ref type="foot" coords="6,445.71,346.13,3.97,6.12" target="#foot_1">2</ref> , which contains about 2 billion words crawled from the .uk Web domain. Only words which occur at least 5000 times in the corpus and do not begin with a capital letter (indicating a name) were eligible for the co-occurrence lists.</p><p>For each keyword in the extended set, we then compute its initial probability, which depends on the frequency of the keyword in descriptions of similar images and eventually the probability of its co-occurrence with other initial keywords. Finally, only the n most probable keywords are kept for further processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Matching Keywords to WordNet</head><p>The set of keywords with their associated probabilities contains rich information about query image content, but it is difficult to work with this representation since we have no information about semantic connections between individual words. Therefore, we need to transform the keywords into semantically connected objects. Since we have chosen the WordNet hierarchy as a corner stone for our analysis, each initial keyword is mapped to a relevant WordNet synset. However, there are often more possible meanings of a given word and thus more candidate synsets. Therefore, we use a probability measure based on the cntlist<ref type="foot" coords="6,341.00,554.63,3.97,6.12" target="#foot_2">3</ref> frequency values to select the most probable synset for each keyword. This measure is based on the frequency of words in a particular sense in semantically tagged corpora and expresses a relative frequency of a given synset in general text. To avoid false dismissals, several highly probable synsets may be selected for each keyword (see Table <ref type="table" coords="6,468.96,604.03,3.88,8.74" target="#tab_0">1</ref>). Each selected synset is assigned a probability value computed as a product of the WordNet normalized frequency and the respective keyword's initial probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exploitation of WordNet Relationships</head><p>By transforming keywords into synsets, we are able to group words with the same meaning and thus increase the probability of recognizing a significant topic. Naturally, this can be further improved by analyzing semantic relationships between the candidate synsets. In the DISA solution of the SCIA task, we exploit the following four WordNet relationships to create a candidate synset graph:</p><p>-Hypernymy (generalization, IS-A relationship): the fundamental relationship utilized in WordNet to build a hierarchy of nouns and some verb groups. It represents upward direction in the generalization/specialization object tree organization. E.g. dog is a hypernym of words poodle and Dalmatian. -Hyponymy (specialization relationship, the opposite of hypernymy): downward direction in the generalization/specialization tree. E.g. car is a hyponym of motor vehicle.</p><p>-Holonymy (has-parts relationship): upward direction in the part/whole hierarchy. E.g. wheeled vehicle is a holonym of wheel. -Meronymy (is-a-part-of relationship, the opposite of holonymy): downward direction in the part/whole tree. E.g. steering wheel is a meronym of car.</p><p>To build the candidate synset graph, we first apply the upward-direction relationships (i.e. hypernymy and holonymy) in a so-called expansion mode, when all synsets that are linked to any candidate synset by these relationships are added to the graph; this way, the candidate graph is enriched by upper level synsets in the potentially relevant WordNet subtrees. However, we are not interested in some of the upper-most levels that contain very general concepts such as entity, physical entity, etc. Therefore, we also utilize the Visual Concept Ontology (VCO)<ref type="foot" coords="7,164.65,426.62,3.97,6.12" target="#foot_3">4</ref> in this step, which is designed as a complementary tool to WordNet and provides a more compact hierarchy of concepts related to image content. Synsets not covered by the VCO are considered to be too general and therefore are not included in the candidate graph. The VCO was created semi-automatically on top of WordNet and its structure is independent of the SCIA task, therefore its utilization is not in conflict with the SCIA scalability requirement.</p><p>After the expansion step, the other two relationships are utilized in an enhancement mode that only adds new links to the graph based on the relationships between synsets that already are present in the graph. Finally, the candidate graph is submitted to an iterative algorithm that updates the probabilities of individual synsets so that synsets with high number of links receive higher probabilities and vice versa.</p><p>Final Concept Selection At the end of the candidate graph processing, the system produces a set of candidate synsets with updated probabilities. The final annotation result is then formed by the k most probable concepts from the intersection of this set with the list of SCIA concepts provided for the particular query image. The matching between candidate synsets and the SCIA concepts is based on the definition of SCIA concepts provided by the organizers, which contains links to WordNet. However, as we detected some missing links (e.g. concept water was linked with meaning H 2 O but not with body of water), we manually added several links to this definition, thus creating the extended concept definition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Tuning of the System</head><p>As described in the previous sections, there are many parameters in the DISA annotation system for which suitable values need to be selected. To determine these values, we performed many experiments using the development data and annotation quality measures provided by the SCIA organizers. To increase the reliability of experimental results, we utilized three different query sets: 1) the whole development set of 1940 images as provided by the organizers, 2) a subset of 100 queries randomly selected from the development set, and 3) a manually selected subset of 100 images for which the visual search provided semantically relevant results. These three test sets are significantly different, which was reflected in the absolute values of quality measures, but the overall trends observed in all experiments were consistent. Table <ref type="table" coords="8,313.57,559.38,4.98,8.74" target="#tab_0">1</ref> summarizes the values of parameters that were tested and the optimal values determined by the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">DISA Submissions at ImageCLEF</head><p>For the actual SCIA competition, we submitted five results produced by different variants of our system. Apart from the optimal set of parameters determined by experiments on development data, we chose several other settings to verify the influence of selected parameters on the overall performance. The values that were modified in some competition runs are highlighted by italics in Table <ref type="table" coords="9,472.84,119.99,3.88,8.74" target="#tab_0">1</ref>.</p><p>The individual run settings were as follows:</p><p>- Originally, we planned to submit two more runs but we didn't manage to prepare them in time due to some technical difficulties. The SCIA organizers kindly allowed us to evaluate these runs as well, even though they are not included in the official result list:</p><p>-DISA-MU 06: the same configuration as DISA-MU 04, but 35 similar images were utilized. -DISA-MU 07: the same configuration as DISA-MU 04, with 3 co-occurring words added to each initial word during the selection of keywords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion of Results</head><p>The global evaluation of our submissions is presented in Table <ref type="table" coords="9,413.15,467.99,3.88,8.74" target="#tab_1">2</ref>. As expected, the best results were achieved by the primary run DISA-MU 04. Using both our observations from the development phase and the competition results, we can conclude the following facts about search-based annotation:</p><p>-The search-based approach is a suitable solution for the SCIA task. To achieve good annotation quality, the search-based approach requires a large dataset with rich annotations, which was in our case represented by Profiset. In a comparison between solutions based only on Profiset and only on SCIA trainset, Profiset clearly dominates due to its size. However, the best results were achieved when both datasets were utilized. The optimal number of similar images is 20-25. -The utilization of statistic data for expansion of image descriptions did not improve the quality of annotations. Evidently, the addition of frequently cooccurring words rather introduced noise into the descriptions of image content. A straightforward utilization of keyword co-occurrence statistics such as suggested here is thus not a viable way to annotation system improvement. -The utilization of semantic relationships between candidate concepts helps to improve the quality of annotations. All relationships that we examined have proved their usefulness. Also, a careful mapping between the WordNetbased output of our annotation system and SCIA concepts is important for the precision of final annotation. Unfortunately, we did not manage to tune the mapping very well, as our extended concept definition slightly decreased the quality of competition results (DISA-MU 02 vs. DISA-MU 01).</p><p>In comparison with other competing groups, our best solution ranked rather high in both sample-based mean F-measure and sample-based MAP. Especially the sample-based MAP achieved by the run DISA-MU 04 was very close to the overall best result (DISA-MU 04 -MAP 34.3, best result kdevir 09 -MAP 36.8). The results for concept-based mean F-measure are less competitive, which does not come as a surprise. In general, the search-based approach works well for frequent terms, whereas concepts for which there are few examples are difficult to recognize. Furthermore, the MPEG7 similarity is more suitable for scenes and dominant objects than for details which were sometimes required by SCIA (e.g. a park photo with a very small bench was labeled as furniture in the development data). Overall, the best results were obtained for scenes (sunrise/sunset, sky, forest, outdoor) and more general concepts (mammal, fruit, flower).</p><p>The set of query images utilized in the SCIA competition is composed of four distinct subsets of images that also deserve to be examined in more detail. Sub-set1 consists of 1000 images that were present in the development set, Subset2 contains 2000 new images. Subset3 and Subset4 contain a mix of development and new images, and consist of 2226 and 2065 images, respectively. Each subset is accompanied by a different list of concepts, as detailed in Table <ref type="table" coords="10,451.56,633.20,3.88,8.74">3</ref>. The differences between individual subsets allow us to asses the concept-wise scalability of solutions by comparing the annotation results over these subsets. In</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,281.06,230.62,24.64,5.77;5,278.94,237.47,28.91,5.77;5,286.27,244.31,14.24,5.78;5,350.19,270.48,35.97,5.78;5,354.42,277.34,27.57,5.77;5,348.80,234.04,37.70,5.78;5,358.61,240.89,18.08,5.77;5,143.69,232.05,110.13,5.14;5,143.69,238.22,30.46,5.13;5,143.69,250.54,98.90,5.13;5,143.69,256.71,30.46,5.13;5,143.69,269.04,91.78,5.13;5,143.69,275.19,30.47,5.14;5,143.69,281.36,3.86,5.13"><head></head><label></label><figDesc>duck: small wild or domesticated swimming bird ; probability: p1 bird: warm-blooded egg-laying vertebrates ; probability: p2 lake: a body of water surrounded by land ; probability: p3 ...</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,223.74,416.17,167.88,7.89"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Architecture of the DISA solution</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,151.70,153.72,328.89,8.74;9,151.70,165.68,328.89,8.74;9,151.70,177.63,207.48,8.74;9,140.99,189.54,339.60,8.77;9,151.70,201.52,328.89,8.74;9,151.70,213.48,90.58,8.74;9,140.99,225.38,339.60,8.77;9,151.70,237.36,142.01,8.74;9,140.99,249.27,339.60,8.77;9,151.70,261.25,328.89,8.74;9,151.70,273.21,192.78,8.74;9,140.99,285.11,339.60,8.77;9,151.70,297.09,90.58,8.74"><head></head><label></label><figDesc>DISA-MU 01 -the baseline DISA solution: content-based retrieval only on Profiset collection, 25 similar images, hypernymy and hyponymy relationships only, original definition of SCIA concepts. -DISA-MU 02: the same configuration as DISA-MU 01, but with extended definition of SCIA concepts, which should improve the final selection of concepts for annotation. -DISA-MU 03: content-based retrieval on both Profiset and SCIA trainset, otherwise same as DISA-MU 02. -DISA-MU 04 -the primary run: content-based retrieval on both datasets, 25 similar images, hypernymy, hyponymy, holonymy and meronymy relationships, extended definition of SCIA concepts. -DISA-MU 05: the same configuration as DISA-MU 04, but only 15 similar images were utilized.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,139.42,116.91,331.35,196.40"><head>Table 1 .</head><label>1</label><figDesc>Annotation tool parameters</figDesc><table coords="8,139.42,140.01,331.35,173.30"><row><cell>Annotation phase</cell><cell>Parameter</cell><cell cols="2">Tested values</cell><cell></cell><cell>Development best</cell></row><row><cell>Similar images</cell><cell>datasets</cell><cell cols="2">Profiset, trainset, both</cell><cell>SCIA</cell><cell>both</cell></row><row><cell>retrieval</cell><cell># of similar images</cell><cell cols="2">10, 15, 20, 25</cell><cell></cell><cell>25</cell></row><row><cell></cell><cell># of co-occurring words</cell><cell>0-5</cell><cell></cell><cell></cell><cell>0</cell></row><row><cell>Text analysis</cell><cell cols="2">max # of synsets per word 1-10</cell><cell></cell><cell></cell><cell>7</cell></row><row><cell></cell><cell># of initial synsets</cell><cell>100-500</cell><cell></cell><cell></cell><cell>200</cell></row><row><cell>Semantic</cell><cell></cell><cell cols="3">hypernymy, hypo-</cell></row><row><cell>probability</cell><cell>relationships</cell><cell>nymy,</cell><cell cols="2">holonymy,</cell><cell>all</cell></row><row><cell>computation</cell><cell></cell><cell cols="2">meronymy</cell><cell></cell></row><row><cell>Final concepts</cell><cell cols="3">extended concept definition true/false</cell><cell></cell><cell>true</cell></row><row><cell>selection</cell><cell># of best results</cell><cell>5-30</cell><cell></cell><cell></cell><cell>7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,134.77,117.41,345.83,190.01"><head>Table 2 .</head><label>2</label><figDesc>DISA results in SCIA 2014: mean F-measure for the samples (MF-samples); mean F-measure for the concepts (MF-concepts); and the mean average precision for the samples (MAP-samples). The values between the square brackets correspond to the 95 % confidence intervals.</figDesc><table coords="10,139.42,172.75,335.68,134.67"><row><cell>Run</cell><cell>MF-samples</cell><cell>MF-concepts</cell><cell>MAP-samples</cell></row><row><cell>DISA-MU 01</cell><cell>27.9 [27.4-28.5]</cell><cell>15.4 [14.0-18.1]</cell><cell>31.6 [31.0-32.2]</cell></row><row><cell>DISA-MU 02</cell><cell>27.5 [27.0-28.1]</cell><cell>15.3 [14.0-18.0]</cell><cell>31.9 [31.3-32.5]</cell></row><row><cell>DISA-MU 03</cell><cell>28.5 [28.0-29.1]</cell><cell>18.9 [17.4-21.6]</cell><cell>32.9 [32.3-33.5]</cell></row><row><cell>DISA-MU 04</cell><cell cols="2">29.7 [29.2-30.3] 19.1 [17.5-21.8]</cell><cell>34.3 [33.8-35.0]</cell></row><row><cell>DISA-MU 05</cell><cell>28.4 [27.9-29.0]</cell><cell cols="2">20.3 [18.8-23.0] 32.3 [31.7-32.9]</cell></row><row><cell>DISA-MU 06</cell><cell>28.7 [28.2-29.3]</cell><cell>18.2 [16.7-20.9]</cell><cell>33.4 [32.8-34.0]</cell></row><row><cell>DISA-MU 07</cell><cell>27.9 [27.4-28.4]</cell><cell>17.8 [16.3-20.4]</cell><cell>32.9 [32.4-33.5]</cell></row><row><cell>best result (kdevir 09)</cell><cell>37.7 [37.0-38.5]</cell><cell>54.7 [50.9-58.3]</cell><cell>36.8 [36.1-37.5]</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,657.80,86.06,7.86"><p>http://disa.fi.muni.cz</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="6,144.73,646.84,112.99,7.86"><p>http://wacky.sslmit.unibo.it</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="6,144.73,657.80,255.91,7.86"><p>https://wordnet.princeton.edu/wordnet/man/cntlist.5WN.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="7,144.73,657.80,108.59,7.86"><p>http://disa.fi.muni.cz/vco/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported by the <rs type="funder">Czech national</rs> research project <rs type="grantNumber">GBP103/12/G084</rs>. The hardware infrastructure was provided by the <rs type="funder">METACentrum</rs> under the programme <rs type="grantNumber">LM 2010005</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_dPjyf8z">
					<idno type="grant-number">GBP103/12/G084</idno>
				</org>
				<org type="funding" xml:id="_aqQNHA2">
					<idno type="grant-number">LM 2010005</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>case of DISA, the trends for all runs are similar to those of the primary run DISA-MU 04, shown in Table <ref type="table" coords="11,267.03,265.70,3.88,8.74">3</ref>. We can observe that the DISA annotation system can adapt very well to previously unseen concepts, which is demonstrated by Subset3 results. The lower annotation quality observed for Subset4 is caused by increased difficulty of the annotation task, which grows with the number of candidate concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>In this study, we have described the DISA solution of the 2014 Scalable Concept Image Annotation challenge. The presented annotation tool applies similaritybased retrieval on annotated image collections to retrieve images similar to a given query, and then utilizes semantic resources to detect dominant topics in the descriptions of similar images. The DISA annotation tool utilizes the Profiset collection of annotated images, word occurrence statistics automatically extracted from large text corpora, the WordNet lexical database, and the VCO ontology.</p><p>All of these resources are freely available and were created independently of the SCIA task, so the scalability objective is achieved.</p><p>The competition results show that the search-based approach to annotation applied by DISA can be successfully used to identify dominant concepts in images. While the quality of results achieved by the DISA annotation tool is not as high as we would wish, especially in the view of concept-based precision and recall, the strong advantages of our solution lie in the fact that it requires minimum training and easily scales to new concepts. The mean average precision of annotation per sample achieved by our system was only slightly worse than the overall best result.</p><p>The semantic search-based annotation can be further developed in several directions. First, we would like to find better measures of visual similarity that could be used in the similarity-search phase, since the relevance of retrieved images is crucial for the whole annotation process. Second, we plan to extend the set of semantic relationships exploited in the annotation process, using e.g. specialized ontologies or Wikipedia. Finally, we also intend to develop a more sophisticated method of final results selection.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="12,142.96,216.56,337.63,7.86;12,151.52,227.51,329.07,7.86;12,151.52,238.47,329.07,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,356.57,216.56,124.02,7.86;12,151.52,227.51,234.59,7.86">Content-based annotation and classification framework: a general multi-purpose approach</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Batko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Botorek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Budikova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zezula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,407.44,227.51,73.15,7.86;12,151.52,238.47,198.89,7.86">17th International Database Engineering &amp; Applications Symposium</title>
		<meeting><address><addrLine>IDEAS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="58" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,248.91,337.63,7.86;12,151.52,259.87,329.07,7.86;12,151.52,270.83,233.05,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,260.38,259.87,215.92,7.86">Building a web-scale image similarity search system</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Batko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Falchi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lucchese</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Perego</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rabitti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sedmidubský</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zezula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,151.52,270.83,142.42,7.86">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="599" to="629" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,281.27,337.64,7.86;12,151.52,292.23,329.07,7.86;12,151.52,303.19,124.25,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="12,151.52,292.23,229.58,7.86">CoPhIR: a test collection for content-based image retrieval</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bolettieri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Falchi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lucchese</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Perego</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Piccioli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rabitti</surname></persName>
		</author>
		<idno>CoRR abs/0905.4627v2</idno>
		<ptr target="http://cophir.isti.cnr.it" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,313.63,337.63,7.86;12,151.52,324.59,329.07,7.86;12,151.52,335.55,175.45,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,300.13,313.63,180.46,7.86;12,151.52,324.59,65.89,7.86">Evaluation platform for content-based image retrieval systems</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Budikova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Batko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zezula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,239.30,324.59,241.29,7.86;12,151.52,335.55,67.93,7.86">International Conference on Theory and Practice of Digital Libraries (TPDL</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="130" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,345.99,337.63,7.86;12,151.52,356.95,272.20,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,298.02,345.99,182.57,7.86;12,151.52,356.95,17.17,7.86">MUFIN at ImageCLEF 2011: Success or Failure?</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Budikova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Batko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zezula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,185.84,356.95,129.65,7.86">CLEF 2011 Labs and Workshop</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>Notebook Papers</note>
</biblStruct>

<biblStruct coords="12,142.96,367.39,337.64,7.86;12,151.52,376.08,329.07,10.13;12,151.52,389.31,329.07,7.86;12,151.52,400.27,284.61,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,151.52,389.31,228.05,7.86">ImageCLEF 2014: Overview and analysis of the results</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Martinez-Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Patricia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Marvasti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Üsküdarlı</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cazorla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Garcia-Varea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Morell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,402.34,389.31,74.13,7.86">CLEF proceedings</title>
		<title level="s" coord="12,151.52,400.27,141.42,7.86">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,410.71,337.63,7.86;12,151.52,421.67,329.07,7.86;12,151.52,432.63,268.60,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,384.46,410.71,96.13,7.86;12,151.52,421.67,109.02,7.86">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,281.12,421.67,199.47,7.86;12,151.52,432.63,127.81,7.86">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,443.07,337.63,7.86;12,151.52,454.03,25.60,7.86" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="12,234.71,443.07,173.00,7.86">WordNet: An Electronic Lexical Database</title>
		<editor>Fellbaum, C.</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,464.47,337.63,7.86;12,151.52,475.43,329.07,7.86;12,151.52,486.39,141.94,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,327.01,464.47,153.58,7.86;12,151.52,475.43,103.36,7.86">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,274.47,475.43,206.12,7.86;12,151.52,486.39,48.77,7.86">Advances in Neural Information Processing Systems (NIPS 2012)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,496.83,337.97,7.86;12,151.52,507.79,329.07,7.86;12,151.52,518.74,329.07,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,296.13,496.83,184.46,7.86;12,151.52,507.79,187.09,7.86">Determining compositionality of expresssions using various word space models and methods</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Krčmář</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Ježek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pecina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,361.33,507.79,119.26,7.86;12,151.52,518.74,254.52,7.86">Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the Workshop on Continuous Vector Space Models and their Compositionality</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="64" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,529.19,337.97,7.86;12,151.52,540.14,329.07,7.86;12,151.52,551.10,238.77,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,343.03,529.19,137.57,7.86;12,151.52,540.14,134.30,7.86">Visual image search: Feature signatures or/and global descriptors</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lokoc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Novák</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Batko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Skopal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,307.64,540.14,172.95,7.86;12,151.52,551.10,97.85,7.86">5th International Conference on Similarity Search and Applications</title>
		<meeting><address><addrLine>SISAP</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="177" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,561.54,337.97,7.86;12,151.52,572.50,329.07,7.86;12,151.52,583.46,25.60,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,294.93,561.54,185.66,7.86;12,151.52,572.50,97.55,7.86">Large-scale similarity data management with distributed metric index</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Batko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zezula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,256.90,572.50,160.34,7.86">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="855" to="872" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,593.90,337.97,7.86;12,151.52,604.86,329.07,7.86;12,151.52,615.82,88.65,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,262.58,593.90,218.01,7.86;12,151.52,604.86,94.67,7.86">Overview of the ImageCLEF 2014 Scalable Concept Image Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,268.80,604.86,211.80,7.86;12,151.52,615.82,59.96,7.86">CLEF 2014 Evaluation Labs and Workshop, Online Working Notes</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,626.26,337.97,7.86;12,151.52,637.22,329.07,7.86;12,151.52,648.18,118.34,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,308.31,626.26,172.28,7.86;12,151.52,637.22,142.95,7.86">Overview of the ImageCLEF 2013 Scalable Concept Image Annotation Subtask</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,301.88,637.22,173.95,7.86">CLEF 2013 Evaluation Labs and Workshop</title>
		<title level="s" coord="12,151.52,648.18,89.67,7.86">Online Working Notes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
