<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,136.57,125.77,342.10,3.63;1,243.62,143.70,128.07,3.63">MindLab at ImageCLEF 2014: Scalable Concept Image Annotation</title>
				<funder ref="#_cF24ZPv">
					<orgName type="full">Microsoft Research LACCIR</orgName>
				</funder>
				<funder ref="#_zR8bzEM #_GvUmbxZ">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_KhF7gkY">
					<orgName type="full">Colciencias</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,137.67,170.89,73.71,9.96"><forename type="first">Jorge</forename><forename type="middle">A</forename><surname>Vanegas</surname></persName>
							<email>javanegasr@unal.edu.co</email>
							<affiliation key="aff0">
								<orgName type="laboratory">MindLab Research Group</orgName>
								<orgName type="institution">Universidad Nacional de Colombia</orgName>
								<address>
									<settlement>Bogotá</settlement>
									<country key="CO">Colombia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,219.47,170.89,55.96,9.96"><forename type="first">John</forename><surname>Arevalo</surname></persName>
							<email>jearevaloo@unal.edu.co</email>
							<affiliation key="aff0">
								<orgName type="laboratory">MindLab Research Group</orgName>
								<orgName type="institution">Universidad Nacional de Colombia</orgName>
								<address>
									<settlement>Bogotá</settlement>
									<country key="CO">Colombia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,283.24,170.89,76.33,9.96"><forename type="first">Sebastian</forename><surname>Otálora</surname></persName>
							<email>jsotaloram@unal.edu.co</email>
							<affiliation key="aff0">
								<orgName type="laboratory">MindLab Research Group</orgName>
								<orgName type="institution">Universidad Nacional de Colombia</orgName>
								<address>
									<settlement>Bogotá</settlement>
									<country key="CO">Colombia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,367.38,170.89,51.47,9.96"><forename type="first">Fabián</forename><surname>Páez</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">MindLab Research Group</orgName>
								<orgName type="institution">Universidad Nacional de Colombia</orgName>
								<address>
									<settlement>Bogotá</settlement>
									<country key="CO">Colombia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,426.84,170.89,45.78,9.96;1,222.95,182.85,61.90,9.96"><forename type="first">Santiago</forename><forename type="middle">A</forename><surname>Pérez-Rubiano</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">MindLab Research Group</orgName>
								<orgName type="institution">Universidad Nacional de Colombia</orgName>
								<address>
									<settlement>Bogotá</settlement>
									<country key="CO">Colombia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,312.30,182.85,80.13,9.96"><forename type="first">Fabio</forename><forename type="middle">A</forename><surname>González</surname></persName>
							<email>fagonzalezo@unal.edu.co</email>
							<affiliation key="aff0">
								<orgName type="laboratory">MindLab Research Group</orgName>
								<orgName type="institution">Universidad Nacional de Colombia</orgName>
								<address>
									<settlement>Bogotá</settlement>
									<country key="CO">Colombia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,136.57,125.77,342.10,3.63;1,243.62,143.70,128.07,3.63">MindLab at ImageCLEF 2014: Scalable Concept Image Annotation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CDDFDF1FDB81BB448949A4E34EB59E4D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ImageCLEF</term>
					<term>Visual Features</term>
					<term>Convolutional Neural Networks</term>
					<term>Multi-label Annotation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of the MindLab research group of Universidad Nacional de Colombia at the ImageCLEF 2014 Scalable Concept Image Annotation challenge. Our strategy mainly relies in finding a good visual representation based on deep convolutional neural networks. Despite the simplicity of the proposed classifier which allows to deal with the large-scale nature of this task, we can achieve good performance (our proposed approach achieved the best MAP) thanks to the rich visual representation based on learned features.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper describes the participation of MindLab research group of Universidad Nacional de Colombia in the 2014 version of the Scalable Concept Image Annotation challenge at ImageCLEF <ref type="bibr" coords="1,298.64,440.26,10.51,9.96" target="#b5">[6,</ref><ref type="bibr" coords="1,309.14,440.26,7.00,9.96" target="#b0">1]</ref>. Our first motivation was to evaluate the use of learned features via deep convolutional neural networks (DCNN). The main strategy is based on transfer learning <ref type="bibr" coords="1,327.71,464.17,9.95,9.96" target="#b6">[7]</ref>, by using in this domain a neural network trained in another similar domain. Current state-of-the-art results on ImageNet, the largest image classification challenge, are based on a DCNN trained in a supervised fashion. Moreover, in the last years multiple works using DCNN significantly improve upon the best performance in the literature for multiple image databases, showing the promising potential of systems based on DCNN <ref type="bibr" coords="1,167.56,535.90,9.95,9.96" target="#b4">[5]</ref>. The success of DCNN is attributed to their capability to learn a rich mid-level image representation. Some works have shown that it is possible to learn to extract this rich mid-level representation from one domain and use this knowledge to improve the performance in other related domain <ref type="bibr" coords="1,413.35,571.76,9.95,9.96" target="#b3">[4]</ref>.</p><p>In this work we proposed a transfer learning approach by using a convolutional network that was trained over a million of images of the ImageNet dataset to enrich the visual representation of the images from this particular domain.</p><p>The rest of the paper is organized as follows: Section 2 describes the characteristics of the dataset; Section 3 describes our multi-label annotation approach; Section 4 presents the experimental results; and finally, Section 6 presents some concluding remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Dataset</head><p>The dataset is composed by a subset of images extracted from a database of millions of images downloaded from the Internet. For each image, the corresponding web page that contained the image is available o↵ering a set of unstructured and noisy related text. This is a large training dataset composed by 500,000 images with meta-data but without labels. To evaluate the proposed systems, two sets with di↵erent list of images and corresponding concepts are provided: Development Set. This set is annotated and composed by 1,000 images labeled with 107 di↵erent concepts.</p><p>Test Set. Is an unlabeled set composed by 4,122 unique images and 207 possible concepts.</p><p>To validate the scalability of the proposed systems, the list of concepts are di↵erent for the development and test sets, moreover, within each set the list of concepts will not be the same for all images.</p><p>3 Multi-label Annotation Model</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Visual Representation</head><p>Although several sets of pre-processed visual features are provided by the challenge organizers, our strategy is based on building our own visual representation based on DCNN. And, for this strategy, we rely in the theory of transfer learning which is based in the ability of a system to recognize and apply knowledge learned in previous domains to novel domains, which share some commonality.</p><p>We use the Yangqing Jia et al. <ref type="bibr" coords="2,291.84,509.46,10.50,9.96" target="#b1">[2]</ref> (Ca↵e) pretrained network to represent images. Ca↵e is an open source implementation of the winning convolutional network architecture of the ImageNet challenge proposed by Krizhevsky et al. <ref type="bibr" coords="2,134.77,545.33,9.95,9.96" target="#b2">[3]</ref>. This network was trained over a million of images annotated with 1,000 ImageNet classes.</p><p>This convolutional network has 60 million parameters and has an architecture composed by eight layers: five convolutional layers and three fully-connected. The output of the last fully-connected layer is the input for a 1000-way soft-max layer which produces a distribution over the 1000 classes (normalized scores).</p><p>Each image is scaled so that the smallest dimension has 227 pixels preserving the original aspect ratio. This raw scaled image is given as input to the network. We used the last fully-connected layer activations, composed by 4096 neurons, as the visual representation for each image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Text preprocessing</head><p>As text representation for images we processed the provided word-score features based on term frequency, DOM attributes and word distance to the image. This text is preprocessed using stop-words removal and stemming, generating a final list of words for each image that is used as textual annotation. Notice that this training set is noisy, that means that a lot of incorrect words could be associated to an image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Label Assignment for training set</head><p>The process to assign labels to images is as follows: the list of query concepts are stemmed and compared to the list of words of the textual annotation obtained in 3.2, if the query concept is presented in the list of words assigned to an image, this concept is assigned as label to the corresponding image. After the label assignment process, if some image does not have any concept, then this image is removed from the original training set. This leads to a total of 383,815 filtered training images to train the annotation model for the development set composed by 107 concepts; and a total of 427,444 training images for the test set composed by 207 concepts. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Multi-label Annotation</head><p>Once we extracted a training set composed by the filtered images which are represented by the visual features generated through the convolutional neural network and annotated with its corresponding concepts, we trained a logistic regression model with multiple outputs, which produces a distribution over the 207 di↵erent concepts of the test set. Later, visual features are extracted for the test images and the annotations are predicted by using the trained logistic regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Decision</head><p>The logistic regression layer produces a distribution with denotes the probability of belonging to each concept, in order to give a final decision in the annotation, it is necessary to define a threshold value. To define an appropriate value for this threshold we perform an exploration by using the development dataset. Figure <ref type="figure" coords="4,134.77,142.24,4.98,9.96" target="#fig_1">2</ref> shows the results of the exploration, using three di↵erent strategies: 1) the output of the logistic regression is normalized by samples assigning 1 to the maximum value and 0 the minimum value 2a; 2) the output for each concept is normalized by setting 1 to the maximum value achieved among all samples (2b), and 3) the logistic regression output is used directly, without normalization (2c). The di↵erent performance curves in figure2 gives clues to select the score normalization method. Normalizing either by concept or sample give better re-sults than no normalization at all. And among those two types of normalization, sample normalization yields a curve which is less sensitive to slight threshold variations, allowing more tolerance in the choice of the threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental results</head><p>We submitted 2 runs, where the only di↵erence is the strategy used for threshold assigning:</p><p>Run 1 (MindLab 01): In this run we assigned the best threshold found in the exploration performed in the development set using per sample normalization. This threshold was used for all concepts.</p><p>Run 2 (MindLab 02): In this run the output for each concept is also normalized per sample as in the first run, but two types of thresholds are used. The best threshold found for the development concepts with sample normalization was used for the new concepts in the test set. For the concepts present in development and test, an specific threshold was used for each concept. This specific threshold was fine tuned for each development concept to achieve the best performance in the development images.</p><p>The o cial results of both submitted runs are reported in Table <ref type="table" coords="5,413.09,343.05,3.87,9.96" target="#tab_0">1</ref>, also, the best result obtained among all the submissions is reported for comparison. As can be seen from the table, our strategy achieved a better result in MAP value than the best submission obtained among all the participants, but a more adequate strategy is required for selecting the final annotations. Figures <ref type="figure" coords="5,407.21,390.87,4.98,9.96" target="#fig_2">3</ref> and<ref type="figure" coords="5,434.36,390.87,4.98,9.96" target="#fig_3">4</ref> show the obtained results of precision and recall for both submissions grouping by concept or sample. When comparing performance of both submissions grouping by concept (figures 3a and 4a), an improvement in recall is evident for the second submission. This can be attributed to the specific threshold used for the concepts on the second submission. This improvement in recall is also present when comparing the performance of both submissions grouping by sample (figures 3b and 4b). But this comparison also reveals a drawback of the strategy used for the second submission, as precision drops significantly. This results bring forward the need to evaluate other strategies for threshold selection, which do not su↵er this kind of disadvantages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work, we proposed a method for multi-label annotation. Despite the simplicity of the proposed classifier which allows to deal with the large-scale nature of this task, we can achieve a good performance (our proposed approach achieved the best MAP) thanks to the richness of the visual representation based on learned features via deep convolutional neural networks.</p><p>The experimental results showed that a good performance can be achieved by applying knowledge from other similar domain (transfer learning).   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,173.13,449.90,269.10,9.96;3,134.77,373.93,345.82,65.11"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Label Assignment process to images in the training set</figDesc><graphic coords="3,134.77,373.93,345.82,65.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,134.77,568.62,345.87,9.96;4,134.77,580.58,345.88,9.96;4,134.77,592.53,345.80,9.96;4,134.77,604.49,234.71,9.96;4,219.69,402.33,172.91,140.49"><head>Fig. 2 :</head><label>2</label><figDesc>Fig.2: Threshold exploration for three strategies: threshold based on sample normalization (2a), threshold based on concept normalization (2b) and threshold using no normalization (2c). Performance reported in mean F-measure for samples (mFSamp) and mean F-measure for concepts (mfConc).</figDesc><graphic coords="4,219.69,402.33,172.91,140.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,134.77,402.55,345.83,9.96;6,134.77,414.51,345.85,9.96;6,134.77,426.46,77.47,9.96;6,137.83,250.07,172.92,126.68"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Results for submission MindLab 01. Average Precision (AP), Recall (RECL), Precision (PREC) and F-measure values are reported for samples (3b) and concepts (3a)</figDesc><graphic coords="6,137.83,250.07,172.92,126.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,134.77,621.28,345.83,9.96;6,134.77,633.24,345.85,9.96;6,134.77,645.19,77.47,9.96;6,137.83,468.80,172.92,126.68"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Results for submission MindLab 02. Average Precision (AP), Recall (RECL), Precision (PREC) and F-measure values are reported for samples (4b) and concepts (4a)</figDesc><graphic coords="6,137.83,468.80,172.92,126.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,134.77,133.37,345.84,257.28"><head>Table 1 :</head><label>1</label><figDesc>Performance measures of the submitted runs for the Scalable Concept Image Annotation task [37.0-38.5] 54.7 [50.9-58.3] 36.8 [36.1-37.5]</figDesc><table coords="6,137.08,167.62,341.18,47.62"><row><cell>Run</cell><cell cols="4">Position MF-samples (%) MF-concepts (%) MAP-samples (%)</cell></row><row><cell>MindLab 01</cell><cell>8</cell><cell>25.8 [25.2-26.3]</cell><cell>30.7 [28.2-34.0]</cell><cell>37.0 [36.4-37.6]</cell></row><row><cell>MindLab 02</cell><cell>10</cell><cell>24.8 [24.2-25.3]</cell><cell>31.7 [29.2-34.8]</cell><cell>37.0 [36.4-37.6]</cell></row><row><cell></cell><cell>1</cell><cell>37.7</cell><cell></cell><cell></cell></row></table><note coords="6,161.28,382.07,129.07,8.57;6,344.64,382.07,126.57,8.57"><p>(a) Results grouping by concept (b) Results grouping by sample</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was partially funded by project <rs type="projectName">Multimodal Image Retrieval</rs> to Support Medical Case-<rs type="projectName">Based Scientific Literature Search</rs>, ID <rs type="grantNumber">R1212LAC006</rs> by <rs type="funder">Microsoft Research LACCIR</rs> and <rs type="person">Jorge Vanegas</rs> and <rs type="person">John Arevalo</rs> also thanks for doctoral grant supports <rs type="grantNumber">Colciencias 617/2013</rs>. <rs type="person">Sebastian Otálora</rs> also thanks <rs type="funder">Colciencias</rs> for its support through the grant " <rs type="programName">Jóvenes Investigadores 2012</rs>" in call <rs type="grantNumber">566</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_zR8bzEM">
					<orgName type="project" subtype="full">Multimodal Image Retrieval</orgName>
				</org>
				<org type="funded-project" xml:id="_cF24ZPv">
					<idno type="grant-number">R1212LAC006</idno>
					<orgName type="project" subtype="full">Based Scientific Literature Search</orgName>
				</org>
				<org type="funding" xml:id="_KhF7gkY">
					<idno type="grant-number">Colciencias 617/2013</idno>
					<orgName type="program" subtype="full">Jóvenes Investigadores 2012</orgName>
				</org>
				<org type="funding" xml:id="_GvUmbxZ">
					<idno type="grant-number">566</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,138.34,257.95,342.19,8.57;7,146.90,267.05,333.65,10.43;7,146.90,279.87,333.62,8.57;7,146.90,290.43,333.67,8.97;7,146.90,301.78,133.90,8.57" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,350.58,279.87,129.94,8.57;7,146.90,290.83,87.34,8.57">ImageCLEF 2014: Overview and analysis of the results</title>
		<author>
			<persName coords=""><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jesus</forename><surname>Martinez-Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mauricio</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Burak</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Novi</forename><surname>Patricia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Neda</forename><surname>Marvasti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Suzan</forename><surname>Üsküdarlı</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roberto</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Miguel</forename><surname>Cazorla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ismael</forename><surname>Garcia-Varea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vicente</forename><surname>Morell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,255.10,290.43,71.23,8.97">CLEF proceedings</title>
		<title level="s" coord="7,333.71,290.83,142.90,8.57">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.34,312.74,342.21,8.57;7,146.90,323.70,202.77,8.57" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="7,211.23,312.74,269.32,8.57;7,146.90,323.70,41.41,8.57">Ca↵e: An open source convolutional architecture for fast feature embedding</title>
		<author>
			<persName coords=""><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<ptr target="http://ca↵e.berkeleyvision.org/" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.34,334.66,342.15,8.57;7,146.90,345.23,301.33,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,388.89,334.66,91.60,8.57;7,146.90,345.62,160.99,8.57">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geo↵rey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,327.74,345.23,19.47,8.97">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.34,356.58,342.17,8.57;7,146.90,367.14,305.81,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,337.23,356.58,143.28,8.57;7,146.90,367.54,233.02,8.57">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,399.78,367.14,23.13,8.97">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.34,378.50,342.17,8.57;7,146.90,389.06,333.70,8.97;7,146.90,400.02,333.65,8.97;7,146.90,411.37,74.17,8.57" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,234.23,378.50,231.51,8.57">Multi-column deep neural networks for image classification</title>
		<author>
			<persName coords=""><forename type="first">Jurgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,146.90,389.06,333.70,8.97;7,146.90,400.02,108.32,8.97">Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), CVPR &apos;12</title>
		<meeting>the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), CVPR &apos;12<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.34,422.33,342.17,8.57;7,146.90,432.90,333.70,8.97;7,146.90,443.86,116.74,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,309.22,422.33,171.29,8.57;7,146.90,433.29,131.29,8.57">Overview of the ImageCLEF 2014 Scalable Concept Image Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">Mauricio</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roberto</forename><surname>Paredes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,300.20,432.90,175.66,8.97">CLEF 2014 Evaluation Labs and Workshop</title>
		<title level="s" coord="7,146.90,443.86,88.54,8.97">Online Working Notes</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.34,455.21,342.21,8.57;7,146.90,466.17,333.62,8.57;7,146.90,476.73,333.64,8.97;7,146.90,488.09,20.95,8.57" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,146.90,466.17,333.62,8.57;7,146.90,477.13,13.92,8.57">Cross-domain learning from multiple sources: A consensus regularization perspective</title>
		<author>
			<persName coords=""><forename type="first">Fuzhen</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuhong</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhongzhi</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,170.51,476.73,230.05,8.97">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1664" to="1678" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
