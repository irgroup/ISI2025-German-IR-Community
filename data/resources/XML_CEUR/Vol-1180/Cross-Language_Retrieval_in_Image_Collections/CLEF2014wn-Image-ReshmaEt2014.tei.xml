<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,137.89,116.95,339.56,12.62;1,222.33,134.89,170.71,12.62;1,146.31,152.82,322.72,12.62">KDEVIR at ImageCLEF 2014 Scalable Concept Image Annotation Task: Ontology-based Automatic Image Annotation</title>
				<funder ref="#_xsBY8WP">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_DWFVQ6j">
					<orgName type="full">Ministry of Education, Culture, Sports, Science and Technology (MEXT)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,186.26,190.68,82.19,8.74"><forename type="first">Ismat</forename><forename type="middle">Ara</forename><surname>Reshma</surname></persName>
							<email>reshma1@kde.cs.tut.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Toyohashi University of Technology</orgName>
								<address>
									<addrLine>1-1 Hibarigaoka, Tempaku-Cho</addrLine>
									<postCode>441-8580</postCode>
									<settlement>Toyohashi, Aichi</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,279.01,190.68,58.67,8.74"><roleName>Md</roleName><forename type="first">Zia</forename><surname>Ullah</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Toyohashi University of Technology</orgName>
								<address>
									<addrLine>1-1 Hibarigaoka, Tempaku-Cho</addrLine>
									<postCode>441-8580</postCode>
									<settlement>Toyohashi, Aichi</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,367.61,189.10,59.17,10.32"><forename type="first">Masaki</forename><surname>Aono</surname></persName>
							<email>aono@tut.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Toyohashi University of Technology</orgName>
								<address>
									<addrLine>1-1 Hibarigaoka, Tempaku-Cho</addrLine>
									<postCode>441-8580</postCode>
									<settlement>Toyohashi, Aichi</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,137.89,116.95,339.56,12.62;1,222.33,134.89,170.71,12.62;1,146.31,152.82,322.72,12.62">KDEVIR at ImageCLEF 2014 Scalable Concept Image Annotation Task: Ontology-based Automatic Image Annotation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">958540FA43E18FBB85FE402DAF6ECE7B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Concept Detection</term>
					<term>Classification</term>
					<term>Image Annotation</term>
					<term>Ontology</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we describe our participation in the Image-CLEF 2014 Scalable Concept Image Annotation task. In this participation, we propose a novel approach of automatic image annotation by using ontology at several steps of supervised learning. In this regard, we construct tree-like ontology for each annotating concept of images using WordNet and Wikipedia as primary source of knowledge. The constructed ontologies are used throughout the proposed framework including several phases of training and testing of one-vs-all SVMs classifier. Experimental results clearly demonstrate the effectiveness of the proposed framework.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Due to the explosive growth of digital technologies, collections of images are increasing tremendously in every moment. The ever growing size of the image collections has evolved the necessity of image retrieval (IR) systems; however, the task of IR from a large volume of images is formidable since binary stream data is often hard to decode, and we have very limited semantic contextual information about the image content.</p><p>To enable the user for searching images using semantic meaning, automatically annotating images with some concepts or keywords using machine learning is a popular technique. During last two decades, there are a large number of researches being lunched using state-of-the-art machine learning techniques <ref type="bibr" coords="1,467.87,585.39,8.49,8.74" target="#b0">[1]</ref><ref type="bibr" coords="1,476.36,585.39,4.24,8.74" target="#b1">[2]</ref><ref type="bibr" coords="1,476.36,585.39,4.24,8.74" target="#b2">[3]</ref><ref type="bibr" coords="1,134.76,597.34,7.75,8.74" target="#b3">[4]</ref> (e.g. SVMs, Logistic Regression). In such efforts, most often each image is assumed to have only one class label. However, this is not necessarily true for real world applications, as an image might be associated with multiple semantic tags. Therefore, it is a practical and important problem to accurately assign multiple labels to one image. To alleviate above problem i.e. to annotate each image with multiple labels, a number of research have been carried out; among them adopting probabilistic tools such as the Bayesian methods is popular <ref type="bibr" coords="2,457.36,119.99,7.75,8.74" target="#b4">[5]</ref><ref type="bibr" coords="2,465.11,119.99,3.87,8.74" target="#b5">[6]</ref><ref type="bibr" coords="2,468.98,119.99,7.75,8.74" target="#b6">[7]</ref>. More review can be found in <ref type="bibr" coords="2,260.84,131.95,10.52,8.74" target="#b7">[8,</ref><ref type="bibr" coords="2,273.02,131.95,7.01,8.74" target="#b8">9]</ref>. However, accuracy of such approach depends on expensive human labeled training data.</p><p>Fortunately, some initiatives have been taken to reduce the reliability on manually labeled image data <ref type="bibr" coords="2,246.31,170.76,13.05,8.74" target="#b9">[10]</ref><ref type="bibr" coords="2,259.36,170.76,4.35,8.74" target="#b10">[11]</ref><ref type="bibr" coords="2,259.36,170.76,4.35,8.74" target="#b11">[12]</ref><ref type="bibr" coords="2,263.71,170.76,13.05,8.74" target="#b12">[13]</ref> by using cheaply gathered web data. Although the "semantic gaps" between low-level visual features and high-level semantics still remain and accuracy is not improved remarkably.</p><p>In order to reduce the dependencies of human-labeled image data, Image-CLEF <ref type="bibr" coords="2,165.02,221.52,15.50,8.74" target="#b13">[14]</ref> has been organizing the photo annotation and retrieval task for the last several years, where training data is a large collection of Web images without ground truth labels. Despite the proposed methods in this task shown encouraging performance on a large scale dataset, unfortunately none of them utilizes the sematic relations among annotating concepts. In this paper, we describe the participation of KDEVIR at ImageCLEF 2014 Scalable Concept Image Annotation Task <ref type="bibr" coords="2,181.05,293.25,14.61,8.74" target="#b14">[15]</ref>, where, we proposed a novel approach, ontology based supervised learning that exploits both low-level visual features and high-level semantic information of images during training and testing. The evaluation results reveal the effectiveness of proposed framework.</p><p>The rest of the paper is organized as follows: Section 2 describes the proposed framework. Section 3 describes our submitted runs to this task as well as comparison results with other participants' runs. Finally, Concluded remarks and some future directions of our work are described in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual Features Metadata</head><p>Given Training Data: Large Scale Web Image Corpus </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-processing of Training Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concepts</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Framework</head><p>In this section, we describe our method for annotating images with a list of semantic concepts. We divide our method into four steps: 1) Constructing Ontology, 2) Pre-processing of Training Data, 3) Training Classifier, and 4) Generating</p><p>Annotations. An overview of our proposed framework is depicted in Fig. <ref type="figure" coords="3,454.02,180.01,3.87,8.74" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Constructing Ontology</head><p>Ontologies are the structural frameworks for organizing information about the world or some part of it. In computer science and information science, ontology is defined as an explicit, formal specification of a shared conceptualization <ref type="bibr" coords="3,465.11,254.20,15.50,8.74" target="#b15">[16,</ref><ref type="bibr" coords="3,134.76,266.16,12.73,8.74" target="#b16">17]</ref> and it formally represents knowledge as a set of concepts within a domain, and the relationships between those concepts. To utilize these relationships in image annotation, we construct ontology for each concept of a predefined list of concept used to annotate images.</p><p>In real world, an image might contain multiple objects (aka concepts) in a single frame, where concepts are inter-related and maintain a natural way of being co-appearance. We use these hypotheses to construct ontologies for concepts. In this regard, we utilize WordNet <ref type="bibr" coords="3,294.90,349.89,15.50,8.74" target="#b17">[18]</ref> and Wikipedia as primary sources of knowledge. However, WordNet and Wikipedia themselves have some limitations which cause obstacles to construct ontology using its. For example, WordNet considers very small number of conceptual relations and very few cross-POS (Parts of Speech) pointers among words; on the other hand, Wikipedia contains wide range of semantic information, however, is not structured as WordNet and prone to contain noises, as of being free to edit for all expert and non-expert contributor. As, both of the sources have some limitations, during knowledge extraction we choose those parts of both sources which are less prone to noise and semantically more confident. Thus, take the advantage of both structured representation of WordNet and wide diversity of semantic relations of Wikipedia.</p><p>Let C be a set of concepts. We will construct a tree-like <ref type="bibr" coords="3,389.02,481.45,15.50,8.74" target="#b18">[19]</ref> ontology for each concept c c ∈ C. In order to build ontologies, first of all, we select some types of relations including: 1) taxonomical R t , 2) bionomical R b , 3) food habitual R f h , and 4) weak hierarchical, R wh . The first and fourth types of relations define relations among any types of concepts, where second and third types define relations among concepts which are biological living things. The relations are extracted empirically according to our observations on WordNet and hundreds of Wikipedia articles. According to the semantic confidence, the order of relation types is:</p><formula xml:id="formula_0" coords="3,174.27,577.09,103.20,9.65">R t &gt; R b &gt; R f h &gt; R wh .</formula><p>For each type of relations, we extract a set of relations as listed below:</p><formula xml:id="formula_1" coords="3,134.76,609.16,307.61,45.66">-R t = {inHypernymPathOf, superClassOf } -R b ={habitat, inhabit, liveIn, foundOn, foundIn, locateAt, nativeTo} -R f h ={liveOn, feedOn} -R wh ={kindOf, typeOf, representationOf, methodOf,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>appearedAt, appearedIn, ableToProduce}</head><p>Finally, we apply some "if-then" type inference rules to add an edge from a parent-concept to a child-concept by leveraging the above relations as illustrated in Fig. <ref type="figure" coords="4,199.28,143.90,3.87,8.74" target="#fig_1">2</ref>. In addition, for some concepts, especially adjectives (e.g. indoor, outdoor), which have neither much lexical information in WordNet, nor any Wikipedia articles, we manually determine the relations to other concepts. </p><formula xml:id="formula_2" coords="4,224.06,206.46,165.61,48.57">! " ! ! # ! $ % &amp; ' % ! Parent-concept Child-concept</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pre-processing of Training Data</head><p>For a given list of concepts, we select the most weighted images for each concept from the noisy training images by exploiting their metadata (details about metadata are given in <ref type="bibr" coords="4,213.16,371.79,15.50,8.74" target="#b14">[15]</ref>) and pre-constructed concept ontologies. In this regards, first of all, we detect the nouns and adjectives from metadata using WordNet followed by singularizing with Pling Stemmer<ref type="foot" coords="4,335.99,394.25,3.97,6.37" target="#foot_0">1</ref> . Secondly, detected terms from metadata: Web text (scofeat), keywords, and URLs are weighted by BM25 <ref type="bibr" coords="4,462.33,407.65,14.61,8.74" target="#b19">[20]</ref>, mean reciprocal rank (MRR), and a constant weight,ϑ ∈ (0, 1) respectively, which is followed by detecting concepts from the weighted sources on appearance basis. Thus, we have three lists of possible weighted concepts from three different sources of metadata for each image. We take the inverted index of image-wise weighted concepts, thus generate the concept-wise weighted images. To aggregate the images for a concept from three sources, we normalize the weight of images, and linearly combine the normalized BM25 (nBM25) weight, normalized MRR (nMRR), and constant weight ϑ to generate the final weight of images. From the resultant aggregated list of images, top-m images are primarily selected for each concept.</p><p>Finally, in order to increase the recall, we merge the primarily selected training images of each concept with its predecessor concepts of highest semantic confident (i.e. predecessors connected by r t ∈ R t ) by leveraging our concept ontologies. Thus, we enhance training images per-concept as well as number of annotated concepts per-image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training Classifier</head><p>This subsection is partially inspired by the winner of ImageCLEF 2013 <ref type="bibr" coords="4,448.27,637.69,14.61,8.74" target="#b20">[21]</ref>.</p><p>Image annotation is a multi-class multi-label classification problem; current state-of-the-art classifiers are not able to solve this problem in their usual format. Towards this problem, we propose a novel technique of using ontologies during different phases of learning a classifier. In this regard, we choose Support Vector Machines (SVMs) as a classifier for its robustness of generalization. We subdivide the whole problem into several sub-problems according to the number of concepts, i.e. train SVMs for each concept separately, since using a large dataset at a time is not rational in terms of memory and time. Areas surrounded by dashed line represents five different communities, "highway", "road", "unpaved", "soil", and "nighttime". Here, black solid-lines represent contextual connections, where, red dashed-lines represent semantic connections emerged from ontological information. Even though image (a) is in "highway" community, it should also belong to "nighttime" community as its one of the tag is "moon", which is semantically related to nighttime. At the same way, all the images of "unpaved" should also belong to "road" and "soil" as unpaved is a characteristic of a kind of road and unpaved-road contains soil. Likewise, "highway" community also belongs to "road" community, as highway is a kind of road.</p><p>Another problem is that, along with the different parameters, the classification accuracy of SVMs depends on the positive and negative examples which are used to train the classifier. It is obvious that if classifiers are trained with wrong examples, the prediction will be wrong. However, selecting appropriate training example is formidable without any semantic clues. For example, if we train a classifier about "soil" without taking into account semantic inter-links with other concepts, one might choose only the "soil" community of Fig. <ref type="figure" coords="6,475.61,191.73,4.98,8.74" target="#fig_2">3</ref> as positive examples, and the remaining are as negative examples. However, it might result in wrongly trained model, since semantically "unpaved" contains soil, which should not be in negative example. To handle this issue, we use our pre-constructed concept ontologies. We randomly select with replacement n-folds positive image examples for each concept from its image list and negative examples from image lists of other concepts which are not its successor of strong or weak semantic confident in its ontology.</p><p>From the n-folds positive and negative examples, we train n probabilistic one-vs-all SVM models for each concept, where n ∈ <ref type="bibr" coords="6,359.17,299.80,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="6,371.35,299.80,11.62,8.74" target="#b9">10]</ref>. We use LIBSVM <ref type="bibr" coords="6,465.09,299.80,15.50,8.74" target="#b21">[22]</ref> to learn the SVM models. As kernel, two hybrid kernels are plugged in, instead of using the default choice linear kernel or Gaussian kernel, since image classification is a nonlinear problem and distribution of image data is unknown. We choose histogram intersection kernel (HIK) <ref type="bibr" coords="6,345.12,347.62,15.50,8.74" target="#b22">[23]</ref> as primary kernel which is further used to generate two other hybrid kernels. The HIK is defined as:</p><formula xml:id="formula_3" coords="6,230.59,381.31,250.00,30.21">k HI (h (a) , h (b) ) = l ∑ q=1 min(h (a) q , h (b) q )<label>(1)</label></formula><p>where h (a) and h (b) are two normalized histograms of l bins; in context of image data, two feature vectors of l dimensions. One of the hybrid kernels is convex combination of HIKs (CCHIK) generated from low level visual features of image defined as:</p><formula xml:id="formula_4" coords="6,255.90,484.94,224.69,31.07">K (0) = 1 |F | |F | ∑ s=1 K HI (f s )<label>(2)</label></formula><p>where K HI (f s ) is a HIK matrix, computed from feature vector type f s ∈ F ; F is a set of visual feature types (details about used visual features are given in <ref type="bibr" coords="6,458.47,540.19,14.76,8.74" target="#b14">[15]</ref>); and |F | is the number of elements in F . In this task |F | = 7. Another hybrid kernel is context dependent kernel (CDK) <ref type="bibr" coords="6,412.81,564.58,15.50,8.74" target="#b20">[21,</ref><ref type="bibr" coords="6,429.97,564.58,11.62,8.74" target="#b23">24]</ref>, defined as:</p><formula xml:id="formula_5" coords="6,248.83,586.21,227.52,12.45">K (t+1) = K (0) + γP K (t) P ′ (<label>3</label></formula><formula xml:id="formula_6" coords="6,476.35,589.93,4.24,8.74">)</formula><p>where K (0) is the CCHIK kernel, P is the left stochastic adjacency matrix between images with each entry proportional to the number of shared labels, and γ ≥ 0. Unlike the original CDK, here, we consider semantic links emerged from ontological information along with contextual links (as shown in Fig. <ref type="figure" coords="6,440.01,645.16,3.87,8.74" target="#fig_2">3</ref>). These kernels are plugged into the SVMs for training and testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Generating Annotations</head><p>The trained models generated in the previous subsection are used to predict annotations. Given a test image with its visual features (which are similar types of training images' visual features) and URLs as metadata, the system finds out the concepts from URLs on appearance basis as did before for URLs of training images. The visual features and detected concepts are used to calculate kernel values mentioned in previous subsection, which are in turn used for predicting annotations. For the given test image, if a model of particular concept responds positively, the image is considered as voted by current model i.e. the corresponding concept is primarily selected for annotation. At the same time, the tracks of predicted probability and vote are kept. This process is repeated for all learned models of all concepts. The concept-wise predicted probabilities and votes are accumulated for n-models. In second level selection, empirical thresholds for accumulated probabilities and votes are used to select more relevant annotations.</p><p>In third level, we take top-k weighted concepts, and finally, the test image is annotated with the selected concepts along with their predecessor concepts in concept ontologies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">KDEVIR Runs and Comparative Results</head><p>We submitted total ten runs, which are differ from each other in terms of: use of ontology or not, if used, then in terms of used relation types of different semantic confident during final stage of generating annotation; used kernel (e.g. CCHIK, CDK); number of primarily selected training images, m; and number of trained models for each concept, n. The configurations of all runs are given in Table <ref type="table" coords="7,175.37,441.22,3.87,8.74" target="#tab_1">1</ref>, where, runs are arranged according to their original name to ease the flow of description. All the parameters used in our proposed framework were set empirically to obtain optimal F-measure based on sample (MF-samples) of corresponding run on development set. Details about all the performance measures are given in <ref type="bibr" coords="7,231.79,489.04,14.61,8.74" target="#b14">[15]</ref>. In Fig. <ref type="figure" coords="7,180.65,501.42,3.87,8.74">4</ref>, and 5, comparisons of our runs (denoted KDEVIR-*) and other participants' runs are illustrated. It reveals the most effectiveness of our proposed approach over other participants' runs. Among the our submitted runs, in Run 1 and 7, we did not exploit semantic information from ontology to compare the effectiveness of our proposed ontology-based approach over ontology-free ordinary one-vs-all SVMs setting with CCHIK and CDK respectively. The comparison results depict that proposed approach tremendously outperform the ordinary one-vs-all SVMs setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we described the participation of KDEVIR at ImageCLEF 2014 Scalable Concept Image Annotation task, where, we proposed a novel approach     </p><formula xml:id="formula_7" coords="8,140.62,164.28,332.74,121.86">-MU_02 DISA-MU_03 DISA-MU_04 DISA-MU_05 FINKI_01 IMC-FU_01 IMC-FU_02 INAOE_01 INAOE_02 INAOE_03 INAOE_04 INAOE_05 INAOE_06 IPL_01 IPL_02 IPL_03 IPL_04 IPL_05 IPL_06 IPL_07 IPL_08 IPL_09 IPL_10 KDEVIR_01 KDEVIR_02 KDEVIR_03 KDEVIR_04 KDEVIR_05 KDEVIR_06 KDEVIR_07 KDEVIR_08 KDEVIR_09 KDEVIR_10 MIL_01 MIL_02 MIL_03 MindLab_01 MindLab_02 MLIA_01 MLIA_02 MLIA_03 MLIA_04 MLIA_05 MLIA_06 MLIA_07 MLIA_08 MLIA_09 MLIA_10 NII_01 RUC_01 RUC_02 RUC_03 RUC_04 RUC_05 RUC_06 RUC_07 RUC_08 MF-samples (%)<label>(a)</label></formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,246.79,653.53,121.78,8.74"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Proposed Framework</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,149.73,272.84,315.40,10.05"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Connecting concepts c c and c d ∈ C according to relation, r * ∈ R *</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,134.76,533.98,345.84,8.74;5,134.76,545.94,345.84,8.74;5,134.76,557.89,345.84,8.74;5,134.76,569.85,345.85,8.74;5,134.76,581.80,345.84,8.74;5,134.76,593.76,345.84,8.74;5,134.76,605.71,345.84,8.74;5,134.76,617.67,345.83,8.74;5,134.76,629.62,345.84,8.74;5,134.76,641.58,345.84,8.74;5,134.76,653.53,74.25,8.74"><head>Fig. 3 :</head><label>3</label><figDesc>Fig.3: This figure illustrates examples of images (taken from Flickr) and their social tag links. Areas surrounded by dashed line represents five different communities, "highway", "road", "unpaved", "soil", and "nighttime". Here, black solid-lines represent contextual connections, where, red dashed-lines represent semantic connections emerged from ontological information. Even though image (a) is in "highway" community, it should also belong to "nighttime" community as its one of the tag is "moon", which is semantically related to nighttime. At the same way, all the images of "unpaved" should also belong to "road" and "soil" as unpaved is a characteristic of a kind of road and unpaved-road contains soil. Likewise, "highway" community also belongs to "road" community, as highway is a kind of road.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="9,134.76,662.21,345.84,8.74;9,134.76,674.17,345.84,8.74;9,134.76,686.09,345.84,8.77;9,134.76,698.08,345.83,8.74;9,134.76,710.00,345.83,8.77;9,134.76,721.96,345.83,8.77;9,134.76,733.91,345.83,8.77;9,134.76,745.87,345.83,8.77;9,134.76,757.82,345.83,8.77;9,134.76,769.81,345.84,8.74;9,134.76,781.76,345.85,8.74;9,134.76,793.72,345.83,8.74;9,134.76,805.67,86.11,8.74"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: These figures illustrate a comparison (as released by the ImageCLEF 2014 organizers in http://www.imageclef.org/2014/annotation/results) of our runs (denoted KDEVIR-*) and other participants' runs on three different subsets of the test set in terms of mean F-measures for samples (MF-samples). Acronyms stand for RUC: Renmin U. of China, DISA-MU: Masaryk U. in Czech Republic, MIL: Tokyo U., MindLab: National U. of Colombia, MLIA: Kyushu U. in Japan, IPL: Athens U. of Economics and Business, IMC-FU: Fudan U. in China, NII: National Institute of Informatics in Japan, FINKI: Ss.Cyril and Methodius U. in Macedonia, INAOE: National Institute of Astrophysics, Optics and Electronics in Mexico. (a) for the subset of test set seen during development, (b) for the subset of test set unseen during development, and (c) for the subset of test set, which contains both seen and unseen samples during development</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,134.76,128.33,345.85,92.46"><head>Table 1 :</head><label>1</label><figDesc>Configurations of submitted runs. The run pairs Run {1, 2} and Run {7, 8} were conducted to show the effectiveness of using ontology (proposed method) for CCHIK and CDK respectively; while, the run pairs Run {5, 6} and Run {8, 9} were conducted to show the effect of considering most semantic confident relation type R t over all relation types (R t , R b , R f h , R wh ) for CCHIK and CDK respectively. Run 3 and 4 were conducted to show the effect of selecting different number of training images during primary selection. We aggregated the decision of models from Run 3 and 6 at Run 10.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,144.73,657.80,244.77,7.86"><p>http://www.mpi-inf.mpg.de/yago-naga/javatools/index.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This research was partially supported by the <rs type="funder">Ministry of Education, Culture, Sports, Science and Technology (MEXT)</rs>, <rs type="grantName">Grant-in-Aid</rs> (<rs type="grantNumber">B</rs>) <rs type="grantNumber">26280038</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_DWFVQ6j">
					<idno type="grant-number">B</idno>
					<orgName type="grant-name">Grant-in-Aid</orgName>
				</org>
				<org type="funding" xml:id="_xsBY8WP">
					<idno type="grant-number">26280038</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Ontology for annotating images using ontologies at several phases of supervised learning from large scale noisy training data.</p><p>The evaluation result reveals that our proposed approach achieved the most effective and best performance among 58 submitted runs in terms of MF-samples and MF-concepts. Moreover, according to the MAP-samples it produced comparable result, although we did not prioritize the annotated concepts came from semantic relation (i.e. we assigned the same weights of originally predicted concepts to their corresponding semantically emerged concepts in annotation of a particular image). In future, we will consider fuzzy relations among concepts in ontologies to facilitate more robust ranking of annotation, thus increase the MAP, and incorporate distributed framework to ensure scalability.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="10,142.95,624.92,337.63,7.86;10,151.52,635.88,329.06,7.86;10,151.52,646.84,329.06,7.86;10,151.52,657.80,140.28,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,363.60,624.92,116.98,7.86;10,151.52,635.88,280.95,7.86">Fast multi-class image annotation with random windows and multiple output randomized trees</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dumont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Marée</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wehenkel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Geurts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,459.19,635.88,21.39,7.86;10,151.52,646.84,329.06,7.86">Proc. International Conference on Computer Vision Theory and Applications (VISAPP)</title>
		<meeting>International Conference on Computer Vision Theory and Applications (VISAPP)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="196" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.95,120.67,337.64,7.86;11,151.52,131.60,329.06,7.89;11,151.52,142.59,32.25,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,282.64,120.67,197.95,7.86;11,151.52,131.63,117.84,7.86">Parallelizing multiclass support vector machines for scalable image annotation</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">K</forename><surname>Alham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,278.51,131.63,147.36,7.86">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="367" to="381" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.95,153.56,337.63,7.86;11,151.52,164.49,170.88,7.89" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,226.87,153.56,249.52,7.86">Incorporating multiple svms for automatic image annotation</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,151.52,164.52,81.43,7.86">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="728" to="741" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.95,175.48,337.62,7.86;11,151.52,186.41,240.98,7.89" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,285.68,175.48,194.90,7.86;11,151.52,186.44,30.27,7.86">Content-based image classification using a neural network</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">B</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,190.18,186.44,112.87,7.86">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="287" to="300" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.95,197.41,337.62,7.86;11,151.52,208.37,329.06,7.86;11,151.52,219.33,329.07,7.86;11,151.52,230.29,86.13,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,275.31,197.41,205.27,7.86;11,151.52,208.37,260.77,7.86">A novel approach to auto image annotation based on pairwise constrained clustering and semi-naïve bayesian model</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,434.25,208.37,46.33,7.86;11,151.52,219.33,86.31,7.86;11,270.62,219.33,206.05,7.86">MMM 2005. Proceedings of the 11th International</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="page" from="322" to="327" />
		</imprint>
	</monogr>
	<note>Multimedia Modelling Conference</note>
</biblStruct>

<biblStruct coords="11,142.95,241.25,337.63,7.86;11,151.52,252.21,329.06,7.86;11,151.52,263.17,269.59,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,287.16,241.25,193.43,7.86;11,151.52,252.21,169.58,7.86">Image content annotation using bayesian framework and complement components analysis</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Fotouhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,340.79,252.21,67.49,7.86;11,437.80,252.21,42.78,7.86;11,151.52,263.17,136.10,7.86">ICIP 2005. IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">1193</biblScope>
		</imprint>
	</monogr>
	<note>Image Processing</note>
</biblStruct>

<biblStruct coords="11,142.95,274.14,337.63,7.86;11,151.52,285.10,170.74,7.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="11,311.02,274.14,169.57,7.86;11,151.52,285.10,136.69,7.86">Automatic image annotation and retrieval using crossmedia relevance models</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.95,296.06,337.63,7.86;11,151.52,307.02,274.74,7.86" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4894</idno>
		<title level="m" coord="11,356.43,296.06,124.15,7.86;11,151.52,307.02,112.03,7.86">Deep convolutional ranking for multilabel image annotation</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,142.95,317.99,337.64,7.86;11,151.52,328.92,202.92,7.89" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,289.44,317.99,191.15,7.86;11,151.52,328.95,23.96,7.86">A review on automatic image annotation techniques</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,183.56,328.95,81.43,7.86">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="346" to="362" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,339.91,337.97,7.86;11,151.52,350.87,329.06,7.86;11,151.52,361.83,325.66,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,332.43,339.91,148.15,7.86;11,151.52,350.87,228.15,7.86">Hierarchical clustering of www image search results using visual, textual and link information</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,404.74,350.87,75.84,7.86;11,151.52,361.83,232.39,7.86">Proceedings of the 12th annual ACM international conference on Multimedia</title>
		<meeting>the 12th annual ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="952" to="959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,372.80,337.97,7.86;11,151.52,383.73,185.54,7.89" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,300.78,372.80,142.95,7.86">Training highly multiclass classifiers</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,450.23,372.80,30.35,7.86;11,151.52,383.76,121.70,7.86">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1" to="48" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,394.72,337.97,7.86;11,151.52,405.65,295.23,7.89" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,295.46,394.72,185.12,7.86;11,151.52,405.68,137.11,7.86">Large scale image annotation: learning to rank with joint word-image embeddings</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,297.24,405.68,69.14,7.86">Machine learning</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="35" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,416.65,337.97,7.86;11,151.52,427.61,329.06,7.86;11,151.52,438.57,236.17,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,335.48,416.65,145.10,7.86;11,151.52,427.61,36.88,7.86">Annosearch: Image auto-annotation by search</title>
		<author>
			<persName coords=""><forename type="first">X</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,212.64,427.61,171.88,7.86">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1483" to="1490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,449.53,337.97,7.86;11,151.52,458.23,329.06,10.12;11,151.52,471.45,329.06,7.86;11,151.52,482.41,285.61,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,151.52,471.45,226.29,7.86">ImageCLEF 2014: Overview and analysis of the results</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Martinez-Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Patricia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Marvasti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Üsküdarlı</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cazorla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Garcia-Varea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Morell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,402.57,471.45,73.88,7.86">CLEF proceedings</title>
		<title level="s" coord="11,151.52,482.41,141.41,7.86">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,493.38,337.97,7.86;11,151.52,504.34,329.06,7.86;11,151.52,515.29,91.19,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,264.44,493.38,216.14,7.86;11,151.52,504.34,94.32,7.86">Overview of the ImageCLEF 2014 Scalable Concept Image Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,269.86,504.34,175.88,7.86">CLEF 2014 Evaluation Labs and Workshop</title>
		<title level="s" coord="11,453.95,504.34,26.63,7.86;11,151.52,515.29,58.30,7.86">Online Working Notes</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,526.26,337.98,7.86;11,151.52,537.19,323.65,7.89" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="11,213.38,526.26,267.21,7.86;11,151.52,537.22,33.60,7.86">Toward principles for the design of ontologies used for knowledge sharing?</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">R</forename><surname>Gruber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,189.20,537.22,196.51,7.86">International journal of human-computer studies</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="907" to="928" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,548.19,337.97,7.86;11,151.52,559.12,253.62,7.89" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="11,323.79,548.19,156.79,7.86;11,151.52,559.15,32.07,7.86">Knowledge engineering: principles and methods</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">R</forename><surname>Benjamins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fensel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,192.26,559.15,123.29,7.86">Data &amp; knowledge engineering</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="161" to="197" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,570.11,337.98,7.86;11,151.52,581.04,81.77,7.89" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="11,205.10,570.11,152.19,7.86">Wordnet: a lexical database for english</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,364.05,570.11,116.54,7.86">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,592.04,337.97,7.86;11,151.52,603.00,329.06,7.86;11,151.52,613.96,329.06,7.86;11,151.52,624.91,13.82,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="11,246.14,592.04,234.45,7.86;11,151.52,603.00,43.11,7.86">Sentiment learning on product reviews via sentiment ontology tree</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Gulla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,219.83,603.00,260.75,7.86;11,151.52,613.96,279.90,7.86">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="404" to="413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,635.88,337.97,7.86;11,151.52,646.84,329.06,7.86;11,151.52,657.80,32.25,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="11,373.36,635.88,107.22,7.86;11,151.52,646.84,172.97,7.86">Okapi at trec-7: automatic ad hoc, filtering, vlc and interactive track</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Willett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,335.66,646.84,115.17,7.86">Nist Special Publication SP</title>
		<imprint>
			<biblScope unit="page" from="253" to="264" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,120.67,337.97,7.86;12,151.52,131.63,329.06,7.86;12,151.52,142.59,329.06,7.86;12,151.52,153.55,213.15,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="12,195.45,120.67,285.13,7.86;12,151.52,131.63,257.43,7.86">Cnrs-telecom paristech at imageclef 2013 scalable concept image annotation task: Winning annotations with context dependent svms</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sahbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,433.82,131.63,46.76,7.86;12,151.52,142.59,220.40,7.86">CLEF 2013 Evaluation Labs and Workshop, Online Working Notes</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Overview of the ImageCLEF</publisher>
			<date type="published" when="2013">September 23-26 2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,164.51,337.97,7.86;12,151.52,175.44,305.81,7.89" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="12,256.41,164.51,190.56,7.86">Libsvm: a library for support vector machines</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,458.82,164.51,21.76,7.86;12,151.52,175.47,241.62,7.86">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,186.43,337.97,7.86;12,151.52,197.36,98.27,7.89" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="12,272.16,186.43,58.70,7.86">Color indexing</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Swain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">H</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,342.17,186.43,138.40,7.86;12,151.52,197.39,23.34,7.86">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="32" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,208.35,337.97,7.86;12,151.52,219.27,329.07,7.89;12,151.52,230.26,60.92,7.86" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="12,310.63,208.35,169.95,7.86;12,151.52,219.30,34.95,7.86">Context-dependent kernels for object classification</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sahbi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">Y</forename><surname>Audibert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Keriven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,193.27,219.30,262.34,7.86">Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="699" to="708" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>IEEE Transactions on</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
