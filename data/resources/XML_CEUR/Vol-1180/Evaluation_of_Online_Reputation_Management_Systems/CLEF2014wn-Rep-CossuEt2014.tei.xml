<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,155.87,116.95,303.61,12.62">LIA@Replab 2014 : 10 methods for 3 tasks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,146.66,154.63,77.47,8.74"><forename type="first">Jean-Valère</forename><surname>Cossu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">et des Pays de Vaucluse 39 chemin des Meinajaries</orgName>
								<orgName type="institution" key="instit1">LIA</orgName>
								<orgName type="institution" key="instit2">Université d&apos;Avignon</orgName>
								<address>
									<addrLine>Agroparc BP 91228</addrLine>
									<postCode>84911</postCode>
									<settlement>Avignon cedex 9</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,232.18,154.63,54.21,8.74"><forename type="first">Kilian</forename><surname>Janod</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">et des Pays de Vaucluse 39 chemin des Meinajaries</orgName>
								<orgName type="institution" key="instit1">LIA</orgName>
								<orgName type="institution" key="instit2">Université d&apos;Avignon</orgName>
								<address>
									<addrLine>Agroparc BP 91228</addrLine>
									<postCode>84911</postCode>
									<settlement>Avignon cedex 9</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,294.59,154.63,82.34,8.74"><forename type="first">Emmanuel</forename><surname>Ferreira</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">et des Pays de Vaucluse 39 chemin des Meinajaries</orgName>
								<orgName type="institution" key="instit1">LIA</orgName>
								<orgName type="institution" key="instit2">Université d&apos;Avignon</orgName>
								<address>
									<addrLine>Agroparc BP 91228</addrLine>
									<postCode>84911</postCode>
									<settlement>Avignon cedex 9</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,384.34,154.63,64.99,8.74"><forename type="first">Julien</forename><surname>Gaillard</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">et des Pays de Vaucluse 39 chemin des Meinajaries</orgName>
								<orgName type="institution" key="instit1">LIA</orgName>
								<orgName type="institution" key="instit2">Université d&apos;Avignon</orgName>
								<address>
									<addrLine>Agroparc BP 91228</addrLine>
									<postCode>84911</postCode>
									<settlement>Avignon cedex 9</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,278.19,166.59,58.97,8.74"><forename type="first">Marc</forename><surname>El-Bèze</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">et des Pays de Vaucluse 39 chemin des Meinajaries</orgName>
								<orgName type="institution" key="instit1">LIA</orgName>
								<orgName type="institution" key="instit2">Université d&apos;Avignon</orgName>
								<address>
									<addrLine>Agroparc BP 91228</addrLine>
									<postCode>84911</postCode>
									<settlement>Avignon cedex 9</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,155.87,116.95,303.61,12.62">LIA@Replab 2014 : 10 methods for 3 tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8A46B918E2B11EA63DD1B43E48800DD4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present the participation of the Laboratoire Informatique d'Avignon (LIA) to RepLab 2014 edition <ref type="bibr" coords="1,384.87,261.89,9.22,7.86" target="#b1">[2]</ref>. RepLab is an evaluation campaign for Online Reputation Management Systems. LIA has produced an important number of experiments for every tasks of the campaign: Reputation Dimensions and both Author Categorization and Author Ranking sub-tasks from Author Profiling. Our approaches rely on a large variety of machine learning methods. We have chosen to mainly exploit tweet contents. In several of our experiments we have also added selected meta-data. A fewer number of our proposals have integrated external information by using provided background messages.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>RepLab addresses the challenging problem of online Reputation analysis, i.e. mining and understanding opinions about companies and individuals by extracting information conveyed in tweets. Here, the end-user application is monitoring the reputation of several entities from Twitter messages. This year the organizers defined two tasks, namely Reputation Dimensions and Author Profiling. The last one is divided in two sub-tasks respectively Author Categorization and Author Ranking. In this context, LIA's participants have proposed several methods to automatically annotate tweets according to this problematic. We took part into each task. The rest of this paper is structured as follows. In section 2, we briefly discuss about data-set and RepLab tasks. In section 3, we present the LIA's submitted systems. Then in section 4, performances are reported before concluding and discussing some future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Tasks and Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Reputation Dimensions</head><p>Data The corpus consists of the same multilingual collection of tweets as last edition <ref type="bibr" coords="1,171.63,626.00,10.52,8.74" target="#b0">[1]</ref> referring to a set of 61 entities spread in four domains: automotive, banking, universities and music/artists. Replab 2014 will use only the automotive http://lia.univ-avignon.fr/ and banking subsets (31 entities). These tweets cover a period going from the 1 st of June 2012 to the 31 st of December 2012. Entitie's canonical names have been used as queries to extract tweets from a larger database. For each entity, at least 2,200 tweets have been collected. The 700 first tweets have been taken to compose the training set, and the other ones are used as test set. Consequently, tweets concerning each of the four tasks are not homogeneously distributed in the data-set. The corpus also provides additional background tweets for each entity (up to 50,000, with a large variability across entities). Each tweet is categorized into one of the following reputation dimensions: Products/Services, Innovation, Workplace, Citizenship, Governance, Leadership, Performance and Undefined</p><p>We have selected 3,000 tweets from the training collection to build a development set. As shown in table <ref type="table" coords="2,273.59,251.70,4.98,8.74" target="#tab_0">1</ref> there is bias with one class. The Reputation Dimensions is a classification tasks that consists in categorizing tweets according to their reputation dimension. The standard categorization provided by the Reputation Institute<ref type="foot" coords="2,319.27,480.45,3.97,6.12" target="#foot_0">1</ref> is used as a gold standard. We may question about what is exactly the meaning of this task since there is a doubt on how the reference has been produced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Author Profiling</head><p>Data For the author profiling task, the data set consists of over 8,000 Twitter profiles (all with at least 1,000 followers) related to the automotive and banking domains. Each profile consists of :</p><p>author name profile URL the last 600 tweets published by the author at crawling time Reputation experts have manually identified the opinion makers (i.e. authors with reputation influence) and annotated them as "Influencer". All those profiles that are not considered opinion makers were assigned the "Non-Influencer" label. Profiles for thoses it was not possible to perform a classification into one of these categories have been labeled as "Undecidable". Each opinion maker has been categorized as journalist, professional, authority, activist, investor, company, or celebrity. The data has been split into training and test sets, the proportion is respectively 30% and 70% .</p><p>Author Categorization goal's is to classify Twitter profiles by type of author: journalist, professional, authority, activist, investor, company or celebrity. The systems' output is a list of profile identifiers with the assigned categories, one per profile. Note that this sub-task has been evaluated only over the profiles annotated as "Influencer" in the "Author Ranking" gold standard.</p><p>Author Ranking objective's is to find out which authors have more reputation influence (who the influencers or opinion makers are) and which profiles are less influential or have no influence at all. For a given domain (e.g. automotive or banking), the system's output had to be a ranking of profiles according to their probability of being an opinion maker with respect to the concrete domain, optionally including the corresponding weights. Some aspects that determine the influence of an author in Twitter -from a reputation analysis perspective -can be the number of followers, the number of comments on a domain or the type of author.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approaches</head><p>In this section we propose descriptions of the LIA's approaches used in this edition. Among our 10 approaches, note that parts were also used in the last edition <ref type="bibr" coords="3,171.10,490.10,9.96,8.74" target="#b3">[4]</ref>. As some systems are a combination of several methods our systems list can be found resumed in Table <ref type="table" coords="3,292.93,502.05,3.87,8.74" target="#tab_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Cosine distance with TF-IDF and Gini purity criteria</head><p>We proposed a supervised classification method based on a cosine distance computed over vectors built using discriminant features like Term Frequency-Inverse Document Frequency (TF-IDF) <ref type="bibr" coords="3,281.49,577.51,14.61,8.74" target="#b11">[13]</ref>, <ref type="bibr" coords="3,307.30,577.51,15.50,8.74" target="#b10">[12]</ref> using the Gini purity criteria <ref type="bibr" coords="3,462.34,577.51,14.61,8.74" target="#b12">[14]</ref>. This system consists in two steps. First the text is cleaned by removing hypertext links and punctuation marks and we generate a list of n-grams by using the Gini purity criteria. During this step stop-lists (from Oracle's website) 2 for both English and Spanish have been used. In the second step we creates terms (words or [2/3]-grams) models for each class by using term frequency with the TF-IDF and Gini criterion. A cosine distance measures the similarity of a given tweet by comparing its bag of words to the whole bag built for each class and ranks tweets according to this measure. This classification process takes into account the following meta-data :</p><p>1. user id; 2. entity id / domain id;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hidden Markov Models</head><p>Hidden Markov models (HMM) have been widely used for categorization <ref type="bibr" coords="4,462.34,230.64,14.61,8.74" target="#b13">[15]</ref>.</p><p>For each class k, a language model Lmk is built from the train set. The language model Lmk is made of uni-gram probabilities and of probabilities Pk(w -h), where histories h are obtained from chunks automatically selected . Conditional probabilities are estimated from the annotated tweets of the train set assuming that a term is considered as a unique event even though it is occurring several in a tweet (or used by an author). As before meta-data were included into the classification process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Poisson modeling</head><p>Another approach inspired by the method used for the fast match component of a speech recognition system <ref type="bibr" coords="4,276.20,372.15,10.52,8.74" target="#b2">[3]</ref> has been also applied in parallel : although the corpus is not so small, it is interesting to use the Poisson law since it is well suited to take into account the sparse distribution of relevant features f mainly for the under populated classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Naive use of continuous Word2Vec model[8]</head><p>Word2vec is an unsupervised algorithm that give a fixed length vector representation for words. Word2vec proved their ability to extract semantics relation between words <ref type="bibr" coords="4,195.05,477.79,12.85,8.74" target="#b8">[9]</ref>. This mean that "king"'s vector is closer to "queen"'s vector than "cat"'s vector. We exploit naively this information to do an unsupervised classification. At first, two wor2vec models where built <ref type="bibr" coords="4,371.63,501.70,15.50,8.74">[11]</ref>. The first model was made for English from the Brown corpus and every English tweet contained in the background corpus. The second model was made for Spanish from various resources <ref type="bibr" coords="4,177.24,537.56,10.52,8.74" target="#b6">[7]</ref> and Spanish tweets in the background corpus. The label "Products &amp; Services" was split in two during classification and re-merge later. Then a naive hypothesis was made.</p><p>The hypothesis was that the name of each class (citizenship,innovation ... ) represents the meaning of the class and so the vector representation of a tweet wich can be classified must be somehow close to the vector representation of the class name. To achieve this class names were translated from english to spanish manualy and each tweets were preprocessed (like tockenization and stop word removing ...).</p><p>Then each words is labeled with the closest class and the majority class give the tweet a label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Multilayer Perceptron</head><p>This classifier use two Word2vec models, one for English and one for Spanish and a multilayer perceptron (MLP) A multilayer perceptron is a feed-forward neural network model. In MLP each neurons use a nonlinear activation function. MLP are train with back-propagation. Our MLP used a 1 input layer with 2500 units, 1 hidden layout with 200 units , 1 output layout with 8 units and L2 normalization. The input was a 5 Words vectors concatenated. So each tweet had to be split with a five words sliding window. Each word is replaced by its Word2vec <ref type="bibr" coords="5,195.34,230.85,10.52,8.74" target="#b7">[8]</ref> representation inside of the sliding window. Then the MLP is trained with the concatenated vector made from the sliding window as intput and with the tweet's label as output. During the classification task the Multilayer Perceptron labeled each window. The final label for the entire tweet is chosen by majority rule from the different windows given a tweet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Conditional random field [6]</head><p>CRFs represent a log-linear model, normalized at the sentence level. CRFs, though very comparable, have many advantages over hidden Markov models and maximum entropy Markov models (MEMM). HMMs model the joint portability between the observed sequences and tag sequences while CRFs are based on the conditional probability of tags considering the entire sequence. MEMM also maximize this conditional probability but only for local states. In our case, CRFs model the probability between class and words as follows:</p><formula xml:id="formula_0" coords="5,225.22,441.25,255.38,30.20">P (c N 1 |w N 1 ) = 1 Z N n=1 H(c n-1 , c n , w n+2 n-2 )<label>(1)</label></formula><p>with</p><formula xml:id="formula_1" coords="5,200.96,495.67,279.64,30.20">H(c n-1 , c n , w n+2 n-2 ) = M m=1 λ m • h m (c n-1 , c n , w n+2 n-2 )<label>(2)</label></formula><p>Log-linear models are based on feature functions h m representing the information extracted from the training corpus, λ are estimated during the training process, Z is a normalization term given by:</p><formula xml:id="formula_2" coords="5,241.78,584.46,238.81,33.29">Z = c N 1 N n=1 H(c n-1 , c n , w n+2 n-2 )<label>(3)</label></formula><p>The tweets from the training set were used to train our CRF tagger with unigram (5 neighbors) and bigram features. Then a CRF tagged each unigram in every tweets and decision for the final tweet's label is made by majority 4 Submissions and results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Systems</head><p>Ten methods compose the LIA's set of submissions. For reading convenience, these methods are summed up in table 2 and refer to a method number used in results table presented above. We now compare our result with regards to the baselines and also to the best score in a given task. As shown (in table <ref type="table" coords="6,242.71,573.43,4.43,8.74" target="#tab_2">3</ref>) most of our runs, ranked according to F-Score are situated between the SVM and most frequent baselines. All our systems are under the SVM baseline. As our systems were biased by the most frequent class we mainly performed bad in term of per-class F-score (computed with precision and recall) although they are not so bad in terms of accuracy. Runs 2 and 1 used separate models for both English and Spanish languages while runs 4 and 3 used a global model. Run 1 also use the background tweets. The run 6 only used tweet's Word2vec information. Adding other source of information will make the system do better decision. Likewise we can try to add more hidden layer now that we have more training data or add an unsupervised phase of pretraining. The Naive run (Run 3) did not perform well compared to others. On one hand its ability to infer meanings and semantic distance between words bring new information to the system. On the other hand due to our hypothesis this system bring a lot of noises. Word2vec have already proved that they are able to summarize information contain in a document[10] and thanks to the MLP we know that there is usefull information for this task in the Word2vec model. With this information there is many things we want to do in order validate/invalidate our usage of Word2vec model. The combination (run 5) has not been able to produce a good selection rules since it performances remains lower than the best system taken alone mostly due to the noise given by the Naive system. Classes distribution (in table <ref type="table" coords="7,281.65,445.92,4.43,8.74" target="#tab_3">4</ref>) explains the low performance level our systems (shown in table 3) since they are all biased to Products&amp;Services. As an interesting result we can notice that the Naive run (run 3) over-estimated the Performance class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Reputation Dimensions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Author Profiling</head><p>Author Categorization Ranked according (table <ref type="table" coords="7,365.75,530.71,4.43,8.74" target="#tab_4">5</ref>) to the average accuracy only one system is better than both "most frequent" and "Machine Learning" (SVM) baselines. One our of system is near the SVM baseline for "Automotive" accuracy while it outperforms the "Banking" accuracy of the baseline. A second system is far behind the baselines while the combination is worse.</p><p>Run 1 used two different systems combinations depending on the language. For English tweets HMM and Poisson were combined. Whereas in spanish Cosine was added to the above combination because there was less data.</p><p>In the second run combined Cosine and HMM where trained with global models without separating languages. Here again our combination (run 3) has not been able to produce a good selection rules since it does worse than all systems taken alone.</p><p>Both baselines produced interesting results since they performed well. Since they are over all other candidates we can consider them as very strong baselines. Another interesting fact is that the "Stockholder" users were not found by any systems.</p><p>With regard to the label distribution in the training set, we decided to have an harmonization post-process of our systems output for this task. The postprocess consist for each output to consider the second hypothesis of the system in the following case :</p><p>-The best hypothesis is an over populated class<ref type="foot" coords="8,357.96,264.71,3.97,6.12" target="#foot_2">3</ref> -The second hypothesis is an under populated class -The score differential between the both hypothesis is not significant In this case the system will full-up small classes despite it has a better confidence in a bigger class. Although this strategy implies as sacrifice some losses in terms of accuracy, it allows the system to be better with small classes. Depending on the chosen evaluation metric this strategy can perform well.</p><p>Author Ranking The run uses the same interesting double combination of Poisson and HMM for both English and Spanish tweets as in "Author Categorization" task. We interpreted this task as a binary classification problem for each author. System considered if each tweet in the author bag of tweets in opinionated or not. Considering now the majority label the system decides whether the user is "opinion maker" or not. To rank users we use the probability of the "opinion maker" label on his bag of tweets. In case of parity we add the probability of a HMM system trained with global models.</p><p>As in the Author Categorization task our Author Ranking output was postprocessed in order the obtain an approaching ratio of "opinion maker" as the training set. Since there were only 2 classes in this task, our post-process can be considered as an offset and threshold set on the probability of one class. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and perspectives</head><p>In this paper we have presented the systems as well as the performances reached by the Laboratoire Informatique d'Avignon to RepLab 2014. We have presented a large variety of approaches and observed logically a large variety of system performances even about one system in several tasks. Our results are good in both subtasks of "Author Profiling" but it seems like we missed something in the "Reputation Dimensions" We have also proposed several combinations of systems in order to benefit from the diversity of information considered by our runs but it did not worked as expected. Sign that our results could still be improved by looking for another way of considering the data and our systems output during both classification and merging processes. While the mass of data has caused us many troubles, in a future work, we will propose to automatically summarize tweets clusters or users profiles in order to reduce our representation and perform a faster classification. As we have already done on the ImagiWeb dataset <ref type="bibr" coords="9,172.51,548.04,10.52,8.74" target="#b4">[5]</ref> we intend to apply an active learning strategy to answer the Replab issue.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,208.83,294.39,197.70,116.73"><head>Table 1 .</head><label>1</label><figDesc>Classes distribution in the training set.</figDesc><table coords="2,226.89,315.18,158.50,95.93"><row><cell>Label</cell><cell>Number of tweets</cell></row><row><cell>Citizenship</cell><cell>2209</cell></row><row><cell>Governance</cell><cell>1303</cell></row><row><cell>Innovation</cell><cell>216</cell></row><row><cell>Leadership</cell><cell>297</cell></row><row><cell>Performance</cell><cell>943</cell></row><row><cell>Products &amp; Services</cell><cell>7898</cell></row><row><cell>Undefined</cell><cell>2228</cell></row><row><cell>Workplace</cell><cell>468</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,157.72,116.91,296.83,138.65"><head>Table 2 .</head><label>2</label><figDesc>LIA's systems for RepLab 2014</figDesc><table coords="6,157.72,137.71,296.83,117.85"><row><cell cols="2"># Method Description</cell></row><row><cell>1</cell><cell>HMM with TF-IDF and Gini purity criteria</cell></row><row><cell>2</cell><cell>Cosine distance with TF-IDF and Gini purity criteria</cell></row><row><cell>3</cell><cell>Poisson with TF-IDF and Gini purity criteria</cell></row><row><cell>4</cell><cell>Merge of HMM and Cosine (global models)</cell></row><row><cell>5</cell><cell>Merge of HMM, Poisson and Cosine (per lang specific models)</cell></row><row><cell>6</cell><cell>Multilayer Perceptron</cell></row><row><cell>7</cell><cell>Conditional random field</cell></row><row><cell>8</cell><cell>Naive Word2vec</cell></row><row><cell>9</cell><cell>Merge of Multilayer Perceptron, CRF, Naive and 4</cell></row><row><cell>10</cell><cell>Merge of 4 and 5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,149.76,420.19,315.83,116.73"><head>Table 3 .</head><label>3</label><figDesc>Submitted runs to Reputation Dimensions Task ordered by F-Score.</figDesc><table coords="6,214.42,440.99,183.45,95.93"><row><cell>#Run-ID</cell><cell>#Method</cell><cell cols="2">F-Score Accuracy</cell></row><row><cell>-</cell><cell>Best</cell><cell>0,489</cell><cell>0,695</cell></row><row><cell>-</cell><cell cols="2">SVM Baseline 0,380</cell><cell>0,622</cell></row><row><cell>Run 2</cell><cell>6</cell><cell>0,258</cell><cell>0,612</cell></row><row><cell>Run 1</cell><cell>7</cell><cell>0,258</cell><cell>0,607</cell></row><row><cell>Run 5</cell><cell>9</cell><cell>0,238</cell><cell>0,595</cell></row><row><cell>Run 4</cell><cell>4</cell><cell>0,160</cell><cell>0,549</cell></row><row><cell>-</cell><cell cols="2">Naive Baseline 0,152</cell><cell>-</cell></row><row><cell>Run 3</cell><cell>8</cell><cell>0,121</cell><cell>0,356</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,170.99,292.17,273.36,116.73"><head>Table 4 .</head><label>4</label><figDesc>Classes distribution in gold-standard and systems output.</figDesc><table coords="7,171.56,312.97,269.14,95.93"><row><cell>Label</cell><cell cols="7">Run 1 Run 2 Run 3 Run 4 Run 5 Gold Baseline</cell></row><row><cell>Citizenship</cell><cell cols="3">4578 3303 7485</cell><cell>855</cell><cell cols="2">3188 5027</cell><cell>3263</cell></row><row><cell cols="4">Governance 1209 1226 1372</cell><cell>465</cell><cell cols="2">507 3395</cell><cell>2131</cell></row><row><cell>Innovation</cell><cell>54</cell><cell>5</cell><cell>337</cell><cell>38</cell><cell>18</cell><cell>306</cell><cell>27</cell></row><row><cell>Leadership</cell><cell>286</cell><cell>46</cell><cell>117</cell><cell>72</cell><cell>120</cell><cell>744</cell><cell>352</cell></row><row><cell cols="2">Performance 916</cell><cell cols="3">1070 10765 266</cell><cell cols="2">1284 1598</cell><cell>668</cell></row><row><cell cols="8">Prod &amp; Svc 20713 24513 12922 29233 25696 15903 19920</cell></row><row><cell>Undefined</cell><cell cols="2">2720 1186</cell><cell>6</cell><cell>567</cell><cell cols="2">383 4349</cell><cell>5303</cell></row><row><cell>Workplace</cell><cell>1154</cell><cell>281</cell><cell>58</cell><cell>136</cell><cell cols="2">434 1124</cell><cell>241</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,134.77,578.12,345.82,83.86"><head>Table 5 .</head><label>5</label><figDesc>Submitted runs to Author Categorization Task ordered by Average accuracy.</figDesc><table coords="8,158.50,598.92,295.26,63.06"><row><cell>#Run-ID</cell><cell>#Method</cell><cell cols="5">Automotive Banking Misc Average F-Score</cell></row><row><cell>Run 1</cell><cell>5</cell><cell>0,445</cell><cell cols="4">0,502 0,461 0,473 0,319</cell></row><row><cell>-</cell><cell>Baseline-SVM</cell><cell>0,426</cell><cell>0,494</cell><cell>-</cell><cell>0,460</cell><cell>0,302</cell></row><row><cell>-</cell><cell>MF-Basline</cell><cell>0,450</cell><cell cols="3">0,420 0,51 0,435</cell><cell>-</cell></row><row><cell>Run 2</cell><cell>4</cell><cell>0,356</cell><cell cols="3">0,397 0,376 0,377</cell><cell>0,294</cell></row><row><cell>Run 3</cell><cell>10</cell><cell>0,292</cell><cell cols="3">0,308 0,369 0,300</cell><cell>0,255</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,170.99,116.91,273.36,138.65"><head>Table 6 .</head><label>6</label><figDesc>Classes distribution in gold-standard and systems output.</figDesc><table coords="9,192.98,137.71,226.32,117.85"><row><cell>Label</cell><cell cols="5">Run 1 Run 2 Run 3 Gold Baseline</cell></row><row><cell cols="2">Public Institution 24</cell><cell>36</cell><cell>60</cell><cell>90</cell><cell>78</cell></row><row><cell>NGO</cell><cell>181</cell><cell>190</cell><cell>331</cell><cell>233</cell><cell>49</cell></row><row><cell>Stockholder</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>7</cell><cell>0</cell></row><row><cell>Sportsmen</cell><cell>157</cell><cell>219</cell><cell>364</cell><cell>208</cell><cell>7</cell></row><row><cell>Journalist</cell><cell>859</cell><cell cols="3">1407 1700 991</cell><cell>708</cell></row><row><cell>Employee</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>14</cell><cell>0</cell></row><row><cell>Undecidable</cell><cell cols="2">1972 1264</cell><cell cols="2">515 1412</cell><cell>2851</cell></row><row><cell>Celebrity</cell><cell>39</cell><cell>318</cell><cell>347</cell><cell>208</cell><cell>0</cell></row><row><cell>Professional</cell><cell cols="4">1492 1278 1291 1546</cell><cell>1144</cell></row><row><cell>Company</cell><cell>151</cell><cell>165</cell><cell>269</cell><cell>222</cell><cell>82</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,134.77,269.23,345.81,72.90"><head>Table 7 .</head><label>7</label><figDesc>Submitted run, best run and baseline to Author Ranking Task ordered by Average MAP.</figDesc><table coords="9,187.46,300.99,237.35,41.14"><row><cell cols="5">#Run-ID #Method Automotive Banking Average MAP</cell></row><row><cell>Best</cell><cell>-</cell><cell>0,721</cell><cell>0,410</cell><cell>0,565</cell></row><row><cell>Run 1</cell><cell>5</cell><cell>0,502</cell><cell>0,450</cell><cell>0,476</cell></row><row><cell>Baseline</cell><cell>-</cell><cell>0,370</cell><cell>0,385</cell><cell>0,378</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.73,647.48,292.84,7.47;2,144.73,658.44,99.85,7.47"><p>http://www.reputationinstitute.com/about-reputation-institute/ the-reptrak-framework</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,144.73,658.44,103.56,7.47"><p>http://docs.oracle.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="8,144.73,535.30,335.85,7.86;8,144.73,546.26,104.51,7.86"><p>The notion of over or under population is considerd with regards to the class distribution in the training set.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,142.96,614.11,337.63,7.86;9,151.52,625.07,329.07,7.86;9,151.52,636.03,143.88,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,398.54,614.11,82.04,7.86">Overview of RepLab</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amigó</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Corujo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Meij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,176.95,625.07,303.65,7.86;9,151.52,636.03,112.64,7.86">Evaluating Online Reputation Management Systems CLEF 2013 Labs and Workshop Notebook Papers</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,646.84,337.62,7.86;9,151.52,657.80,329.07,7.86;10,151.52,120.67,329.07,7.86;10,151.52,131.63,317.52,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,398.69,657.80,81.90,7.86;10,151.52,120.67,237.30,7.86">author profiling and reputation dimensions for Online Reputation Management</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amigó</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carrillo-De-Albornoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Chugur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Corujo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Meij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Spina</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Overview</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Replab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,405.20,120.67,75.39,7.86;10,151.52,131.63,214.91,7.86">Proceedings of the Fifth International Conference of the CLEF initiative</title>
		<meeting>the Fifth International Conference of the CLEF initiative<address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-09">2014. 2014. sep</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,142.59,337.63,7.86;10,151.52,153.55,329.07,7.86;10,151.52,164.51,133.74,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,400.18,142.59,80.40,7.86;10,151.52,153.55,255.14,7.86">Obtaining candidate words by polling in a large vocabulary speech recognition system</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L</forename><surname>Bahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bakis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">V</forename><surname>De Souza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,422.01,153.55,58.58,7.86;10,151.52,164.51,54.90,7.86">Proceedings of ICASSP 1988</title>
		<meeting>ICASSP 1988</meeting>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="489" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,175.47,337.62,7.86;10,151.52,186.42,329.07,7.86;10,151.52,197.38,329.06,7.86;10,151.52,208.34,37.14,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,342.88,186.42,137.72,7.86;10,151.52,197.38,219.95,7.86">LIA@RepLab 2013 An evaluation campaign for Online Reputation Management Systems</title>
		<author>
			<persName coords=""><forename type="first">J.-V</forename><surname>Cossu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Bigot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bonnefoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Morchid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Bost</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Senay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Dufour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Bouvier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-M</forename><surname>Torres-Moreno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>El-Bèze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,378.97,197.38,36.96,7.86">CLEF&apos;13)</title>
		<imprint>
			<date type="published" when="2013-09">September 2013</date>
			<biblScope unit="page" from="23" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,219.30,337.64,7.86;10,151.52,230.26,329.07,7.86;10,151.52,241.22,87.31,7.86" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="10,414.62,219.30,65.97,7.86;10,151.52,230.26,329.07,7.86;10,151.52,241.22,35.04,7.86">reputation monitoring on Twitter with active learning automatic annotation Techreport hal-01002818</title>
		<author>
			<persName coords=""><forename type="first">J.-V</forename><surname>Cossu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>El-Bèze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sanjuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-M E-</forename><surname>Torres-Moreno</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-04">April 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,252.18,337.62,7.86;10,151.52,263.14,252.52,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="10,374.12,252.18,106.46,7.86;10,151.52,263.14,248.52,7.86">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,274.10,337.63,7.86;10,151.52,285.05,329.06,7.86;10,151.52,296.01,12.23,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,359.00,274.10,121.58,7.86;10,151.52,285.05,43.55,7.86">Investigaciones lingüísticas en lexicografía</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">F</forename><surname>Lara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">H</forename><surname>Chande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">I G</forename><surname>Hidalgo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,226.32,285.05,254.26,7.86">Colegio de México, Centro de Estudios Lingüísticos y Literarios</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,306.97,337.63,7.86;10,151.52,317.93,329.06,7.86;10,151.52,328.89,21.64,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,399.68,306.97,80.91,7.86;10,151.52,317.93,166.61,7.86">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName coords=""><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,338.70,317.93,136.79,7.86">Proceedings of Workshop at ICLR</title>
		<meeting>Workshop at ICLR</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,339.85,337.63,7.86;10,151.52,350.81,307.00,7.86;10,134.77,361.77,7.85,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,358.25,339.85,122.33,7.86;10,151.52,350.81,145.38,7.86">Linguistic Regularities in Continuous Space Word Representations</title>
		<author>
			<persName coords=""><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,316.34,350.81,111.91,7.86">Proceedings of NAACL HLT</title>
		<meeting>NAACL HLT</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,219.03,361.77,261.55,7.86;10,151.52,372.73,89.52,7.86;10,134.77,381.42,345.82,10.13;10,151.52,394.64,329.05,7.86;10,151.52,405.60,303.73,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,338.27,361.77,142.31,7.86;10,151.52,372.73,89.52,7.86;10,134.77,381.42,345.82,10.13;10,151.52,394.64,30.67,7.86">Distributed Representations of Sentences and Documents 11. Radim Řehůřek and Petr Sojka Software Framework for Topic Modelling with Large Corpora</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Mikolov</surname></persName>
		</author>
		<ptr target="http://is.muni.cz/publication/884893/en" />
	</analytic>
	<monogr>
		<title level="m" coord="10,190.61,394.64,289.96,7.86;10,151.52,405.60,45.21,7.86">Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks</title>
		<meeting>the LREC 2010 Workshop on New Challenges for NLP Frameworks</meeting>
		<imprint>
			<publisher>ELRA</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="45" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,416.56,337.97,7.86;10,151.52,427.52,329.05,7.86;10,151.52,438.48,79.66,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,205.95,416.56,274.63,7.86;10,151.52,427.52,31.92,7.86">Understanding inverse document frequency: on theoretical arguments for IDF</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,200.75,427.52,105.24,7.86">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="503" to="520" />
			<date type="published" when="2004">2004</date>
			<publisher>Emerald Group Publishing Limited</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,449.44,337.98,7.86;10,151.52,460.40,255.99,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,253.39,449.44,215.15,7.86">Term weighting approaches in automatic text retrieval</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,151.52,460.40,165.89,7.86">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="513" to="523" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,471.36,337.98,7.86;10,151.52,482.32,329.06,7.86;10,151.52,493.27,98.42,7.86" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="10,388.90,471.36,91.69,7.86;10,151.52,482.32,245.20,7.86">Opinion detection as a topic classification problem In in Textual Information Access</title>
		<author>
			<persName coords=""><forename type="first">J.-M</forename><surname>Torres-Moreno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>El-Beze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bellot</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Bechet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>John Wiley &amp; Son</publisher>
			<biblScope unit="page">337</biblScope>
		</imprint>
	</monogr>
	<note>Chapter 9</note>
</biblStruct>

<biblStruct coords="10,142.61,504.23,337.97,7.86;10,151.52,515.19,329.06,7.86;10,151.52,526.15,329.05,7.86;10,151.52,537.11,36.86,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,233.94,504.23,246.64,7.86;10,151.52,515.19,115.85,7.86">Automatic Text Classification Based on Hidden Markov Model and Support Vector Machine</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,283.21,515.19,197.37,7.86;10,151.52,526.15,283.27,7.86">Proceedings of The Eighth International Conference on Bio-Inspired Computing: Theories and Applications (BIC-TA)</title>
		<meeting>The Eighth International Conference on Bio-Inspired Computing: Theories and Applications (BIC-TA)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="217" to="224" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
