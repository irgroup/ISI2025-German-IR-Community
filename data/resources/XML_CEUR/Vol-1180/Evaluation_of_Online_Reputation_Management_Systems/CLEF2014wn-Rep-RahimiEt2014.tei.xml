<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,168.24,183.06,275.10,11.78;1,224.64,199.86,162.32,11.78">The STAVICTA Group Report for RepLab 2014 Reputation Dimensions Task</title>
				<funder ref="#_J2TCgYj">
					<orgName type="full">Swedish Research Council (Vetenskapsrådet)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,163.20,236.65,55.18,8.53"><forename type="first">Afshin</forename><surname>Rahimi</surname></persName>
							<email>afshin.rahimi@lnu.se</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Linnaeus University</orgName>
								<address>
									<settlement>Växjö</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="laboratory">StaViCTA Project Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,232.55,236.65,65.84,8.53"><forename type="first">Magnus</forename><surname>Sahlgren</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Gavagai AB</orgName>
								<address>
									<settlement>Stockholm, Stockholm</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="laboratory">StaViCTA Project Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,312.47,236.65,59.42,8.53"><forename type="first">Andreas</forename><surname>Kerren</surname></persName>
							<email>andreas.kerren@lnu.se</email>
							<affiliation key="aff2">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Linnaeus University</orgName>
								<address>
									<settlement>Växjö</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="laboratory">StaViCTA Project Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,386.15,236.65,52.73,8.53"><forename type="first">Carita</forename><surname>Paradis</surname></persName>
							<email>carita.paradis@englund.lu.se</email>
							<affiliation key="aff3">
								<orgName type="department">Center for Languages and Linguistics</orgName>
								<orgName type="institution">Lund University</orgName>
								<address>
									<settlement>Lund</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="laboratory">StaViCTA Project Group</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,168.24,183.06,275.10,11.78;1,224.64,199.86,162.32,11.78">The STAVICTA Group Report for RepLab 2014 Reputation Dimensions Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C5977D536AA4BC0FD7C96EDDAA6BFD0D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>short text categorization</term>
					<term>sentiment analysis</term>
					<term>reputation monitoring</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we present our experiments on the RepLab 2014 Reputation Dimension task. RepLab is a competitive challenge for Reputation Management Systems. RepLab 2014's reputation dimensions task focuses on categorization of Twitter messages with regard to standard reputation dimensions (such as performance, leadership, or innovation). Our approach only relies on the textual content of tweets and ignores both metadata and the content of URLs within tweets. We carried out several experiments focusing on different feature sets including bag of n-grams, distributional semantics features, and deep neural network representations. The results show that bag of bigram features with minimum frequency thresholding work quite well in reputation dimension task especially with regards to average F1 measure over all dimensions where two of our four submitted runs achieve highest and second highest scores. Our experiments also show that semi-supervised recursive autoencoders outperform other feature sets used in our experiments with regards to accuracy measure and is a promising subject of future research for improvements.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Twitter has become a good source of data for opinion mining systems. Not only does the length restriction of tweets (140 characters) encourage users to keep their messages concise (this is of course not always the case), the characteristics of the medium itself promote opinionated content; its simplicity, brevity, and velocity makes Twitter an ideal channel for users to express opinions about current events. Using the vast amount of data Twitter provides, there has been several attempts to apply machine learning on various applications including, but not limited to, predicting election results <ref type="bibr" coords="2,188.98,245.77,14.37,8.53" target="#b19">[20]</ref>, <ref type="bibr" coords="2,210.00,245.77,14.37,8.53" target="#b21">[22]</ref>, monitoring brands' reputation <ref type="bibr" coords="2,348.24,245.15,10.95,9.34" target="#b6">[7]</ref>, <ref type="bibr" coords="2,366.29,245.15,17.56,9.34" target="#b17">[18]</ref> and forecasting stock prices <ref type="bibr" coords="2,169.44,256.91,12.10,9.34" target="#b3">[4]</ref> Many of these attempts rely on sentiment analysis (or opinion mining), which is usually cast as a classification problem over the categories positive, negative, and neutral <ref type="bibr" coords="2,190.03,280.57,14.46,8.53" target="#b12">[13]</ref>. However, for many applications such as Reputation Classification <ref type="bibr" coords="2,143.28,291.85,10.28,8.53" target="#b0">[1,</ref><ref type="bibr" coords="2,157.19,291.85,7.94,8.53" target="#b1">2]</ref> positive/negative categories are too simplistic and current interest has drifted towards more complex sentiment palletes like that of the RepTrak® model <ref type="bibr" coords="2,426.84,303.13,15.97,8.53" target="#b14">[15]</ref> that is adopted in the RepLab reputation dimensions task.</p><p>RepLab 2014 <ref type="bibr" coords="2,208.26,325.69,10.79,8.53" target="#b1">[2]</ref> is an evaluation campaign addressing the challenge of categorizing tweets related to several brands with regards to standard reputation dimensions introduced by the RepTrak® model. These dimensions/categories are:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Products/Services</head><p>There's a nice BMW in front of my window.... I think I'm gonna steal it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Innovation</head><p>Wait! They're integrating Siri into cars. Mercedes, Honda, GM, Toyota etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Workplace</head><p>What's going on at the Nissan plant? Undefined, which covers tweets not relating to any of the other 7 categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ford music. In my car!</head><p>The rest of this article is organized as follows. Section 2 presents the dataset; section 3 summarizes our experiments with regards to the reputation dimension task; section 4 provides both submitted and unsubmitted results, and section 5 briefly concludes our work and discusses future improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset</head><p>RepLab 2014 uses Twitter data in English and Spanish. For the reputation dimensions task the dataset is the same as in RepLab 2013 and consists of a collection of tweets related to 61 entities/brands in four different industries. The RepLab 2014 dataset only uses tweets in the automotive and banking subsets. For each entity at least 2200 tweets are downloaded and annotated from which 700 tweets are used for the training set and the last 1500 tweets are reserved for the test set. As Twitter terms of service does not permit distribution of tweet contents, the id of tweets are provided to be used in retrieving tweets directly from Twitter. However, since some tweets may have been deleted or changed to private by users, the actual number of retrieved tweets will possibly be lower than the initial number of annotated tweets. Training tweets are categorized with regards to the 8 mentioned categories.</p><p>For each entity there are a number of uncategorized background tweets that can be used in different ways (e.g. for unsupervised feature learning).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>We performed several experiments to evaluate the performance of various feature sets and various classification algorithms for the reputation dimension task. The feature sets we used in these experiments can be roughly categorized into the following 3 groups with regards to representation type: bag of words representations, distributional representations and deep neural network representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Bag of Words Representations</head><p>Bag of words is arguably the most common form of representation of textual content; each text is represented as a feature vector where the elements record (some function of) the frequencies of the words in the text. Although there have been many attempts to devise more sophisticated forms of text representations, bag of words representations have remained the standard form of text representation for classification purposes. The main reason for this is their simplicity, coupled with the fact that they produce competitive results not only in text classification, but also in many other tasks such as information retrieval, clustering, question answering, etc. The main drawback of these models is the very assumption that makes them so simple: Assuming that we can have a representation of a piece of text by considering it as a bag of words and that we can completely ignore the sequence and the structure of the words in a text in favor of the simplicity of representation. To relax this overly naïve assumption, we used bag of ngram models to incorporate local sequential and structural information up into the representation. Unigrams, bigrams, trigrams and 4-grams were used in different experiments. Previous research has shown that in some tasks unigrams perform better than higher order n-grams <ref type="bibr" coords="3,191.26,627.82,14.44,8.53" target="#b12">[13]</ref>. Using bigrams and higher order n-grams as features in text classification tasks introduces a lot more new rare features many of which occur in just one or two documents especially when the training data is not very big. These rare features have very high Inverse Document Frequency (IDF) because they occur in few documents which means that they will get high scores using TF-IDF weighting:</p><formula xml:id="formula_0" coords="4,242.16,192.40,154.67,22.37">( , , ) = ( , ) ( , ) × log | | | |</formula><p>Where t is a term, d is a document and D is the document collection. The first product term is Term Frequency (TF) and the second product term is Inverse Document Frequency (IDF). Here n(t, d) is the number of times term t occurs in document d, |D| is the total number of documents/tweets and |d| is the number of documents/tweets that a term is occurred in. As is shown in the formula rare terms have low d and so result in a big IDF. These new features introduce a lot of noise to the classification task and consequently decrease accuracy. In order to alleviate this problem one working solution is to use a minimum document frequency with which the features that occur in just few documents are removed before a TF-IDF transformation. We found that removing the n-grams that occur in just one document improves accuracy in the RepLab dataset. To prevent over-fitting a 10-fold crossvalidation was used and the resulting accuracies were averaged into an overall accuracy for each setting.</p><p>In addition to using words, we tried to enrich our feature sets with named entities. As an example, in the sentence John left Ford was converted to Person left Organization in order to get more generalized features. We used the Stanford named entity recognizer tool <ref type="bibr" coords="4,204.23,416.63,11.08,8.53" target="#b4">[5]</ref> to tag both training and test set to be used later as features. We used named entities both as extra features and as a replacement for the named entities of tweets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Distributional Representations</head><p>Distributional Semantic Models (DSM) are word representations that try to capture the semantic similarity of words using their distribution in language. The idea which is known as the Distributional Hypothesis is that words with similar distribution have similar meaning <ref type="bibr" coords="4,206.86,519.83,14.56,8.53" target="#b16">[17]</ref>. Here the word 'distribution' means the collection of occurrences of a word within a context where context can be a very narrow window of size 1 around that word or a large textbook the words occur in.</p><p>As input to the DSM, we concatenated English Wikipedia, Spanish Wikipedia, the RepLab training and background sets. It should be noted that the test set was not included in the corpus. we used the Random Indexing framework, which is an efficient method for building DSM models for big data, since it uses fixed-dimensional vectors whose dimensionality is much lower than the representational dimensionality of the data <ref type="bibr" coords="4,162.00,607.21,10.05,8.53" target="#b7">[8]</ref>. We used Random Indexing with 2048-dimensional vectors, and documents (Wikipedia articles or tweets) as word contexts. After building the model we used Positive PMI to normalize weights in order to disfavor highly frequent words.</p><p>To come up with a vector-based representation for each tweet out of the word representations two different approaches were applied: summing word vectors and concatenating word vectors. In the summing approach the representations of words of a tweet were fetched from the DSM model and summed to form a 2048-dimensional compositional vector representing the semantic content of that tweet. Vector addition is a very simple but comparatively effective approach to form compositional DSM representations <ref type="bibr" coords="5,202.99,208.80,9.97,8.53" target="#b5">[6]</ref>. In the concatenation approach we concatenated the first 20 word representations of each tweet. If a tweet had less than 20 words zero valued vectors were concatenated at the end the vector, resulting in 40960-dimensional vector representing each tweet. The tweet vectors were then used as features of the training and test sets. We also carried out an experiment with a combination of both bag of words feature set and DSM feature set. In our second approach</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Deep Neural Network Representation</head><p>Deep Neural Networks are producing state of the art results in many Machine Learning fields including Computer Vision, Speech Recognition, Natural Language Processing and Music Recognition. Recursive Autoencoders have been shown to produce good results in sentiment analysis tasks <ref type="bibr" coords="5,299.18,345.83,14.42,8.53" target="#b20">[21]</ref>. We reproduced Socher et al.'s experiment with the reputation dimensions dataset. We also used Theano's <ref type="bibr" coords="5,406.80,357.11,11.07,8.53" target="#b2">[3]</ref> implementation of Deep Belief Networks in order to compare the abstract feature sets provided by these deep representations to bag of words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We used scikit-learn <ref type="bibr" coords="5,223.12,433.43,14.29,8.53" target="#b13">[14]</ref>, a collection of simple and efficient tools for machine learning in Python, for doing feature extraction, weight normalization, and classification. The deep learning experiments are evaluated by partitioning the training set into two random train and test sets by ratio of 9 to 1. Other experiments have been evaluated by 10-fold cross validation. The gold standard final test set consisted of 7 unbalanced categories (excluding undefined category). The distribution of tweets in these 7 categories is shown in table <ref type="table" coords="5,234.50,501.11,3.45,8.53" target="#tab_1">1</ref>.   <ref type="table" coords="6,273.34,298.56,4.74,8.53" target="#tab_4">3</ref> indicate that our models outperform the baseline model with regards to both accuracy and f measure and also perform close to the best results from other participants (uofTr_RD_4, DAE_RD_1 and LyS_RD_1). Some runs including uofTr_RD_4, DAE_RD_1 and LyS_RD_1 perform better than our runs with regards to accuracy but our runs perform better with regards to macro averaged f measure. Given the skewed distribution of categories in table <ref type="table" coords="6,406.96,354.96,4.74,8.53" target="#tab_1">1</ref> it is important for a classifier to perform well with regards to f measure too because if someone just classified all tweets in Products &amp; Services class it would achieve about 56% accuracy. The final results in table <ref type="table" coords="6,252.20,388.80,4.74,8.53" target="#tab_4">3</ref> show that all our runs which use bag of bigram models perform quite well with regards to F measure and in the same time achieve reasonable accuracies too. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Our goal in these experiments was to evaluate different feature sets with regards to the reputation dimensions task. We carried out several experiments with bag of word representations, DSM representations and deep learning representations. Our results show that higher order n-gram features such as trigrams do not perform better than bigrams. We assume the reason for this is data sparseness; higher-order n-grams pro-vide more specific features, but if the data is not big enough (i.e. if the occurrence counts of the n-grams are uncertain) they will only introduce noise to the representations. They also show that in order to reduce noise introduced by bigrams, minimum frequency thresholding should be applied. Removing bigrams that occur just once in the corpus is the best minimum threshold on the RepLab dataset and this improvement resulted in highest and second highest scores in the RepLab reputation dimensions challenge with regards to average F1 over all dimensions. We also used named entity features in several experiments but the resulting accuracy was lower than not using them at all. In <ref type="bibr" coords="8,222.62,231.36,15.68,8.53" target="#b18">[19]</ref> similar results are reported both for replacing NER features with real names and for adding them to bag of word models. Although named entity features resulted in lower accuracy the generalized features they provide is a good subject of future research in domain adaptation tasks.</p><p>As our results show the DSM representations do not perform better than bag of word models. Although such models can encode semantic content, summing or concatenating them is shown here not to perform well in the reputation dimensions task. However, recent works <ref type="bibr" coords="8,207.57,310.32,14.86,8.53" target="#b9">[10,</ref><ref type="bibr" coords="8,225.45,310.32,11.97,8.53" target="#b10">11,</ref><ref type="bibr" coords="8,240.22,310.32,12.52,8.53" target="#b11">12]</ref> indicate that word vectors produced by neural networkbased models can be used to improve text representations for classification results. The composition of word vectors into sentence/document vectors is another subject of future research. While Deep Belief Networks did not produce good results, semi-supervised recursive autoencoders <ref type="bibr" coords="8,195.93,366.71,15.75,8.53" target="#b20">[21]</ref> performed quite well according to accuracy measure. We did not submit deep learning results in the RepLab challenge but as the results show they can produce promising representations and consequently are a subject of future research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,213.84,523.08,184.03,111.93"><head>Table 1 .</head><label>1</label><figDesc>The distribution of classes in the gold test set</figDesc><table coords="5,242.16,543.51,126.87,91.50"><row><cell>Category</cell><cell>Percent</cell></row><row><cell cols="2">Products &amp; Services 56.60034879</cell></row><row><cell>Citizenship</cell><cell>17.89158985</cell></row><row><cell>Governance</cell><cell>12.08314055</cell></row><row><cell>Performance</cell><cell>5.68743994</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,143.28,242.17,325.01,64.92"><head>Table 2</head><label>2</label><figDesc>summarizes the main results of our experiments. As can be seen in table2the bag of bigram model outperforms the DSM model and LinearSVC outperforms other classifiers. The only classifier that works better than LinearSVC with bigram features is socher-recursive-autoencoders which achieved a high accuracy of 0.83 but because we did not evaluate the model by cross-validation we did not submit that run for the task. Final results shown in table</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,164.88,444.61,281.71,201.09"><head>Table 2 .</head><label>2</label><figDesc>Experimental results for RepLab 2014 reputation dimension classification</figDesc><table coords="6,216.00,476.34,179.09,169.37"><row><cell>Method</cell><cell cols="2">Accuracy F1</cell></row><row><cell>BoW-unigram-RidgeClassifier</cell><cell>0.758</cell><cell>0.742</cell></row><row><cell>BoW-unigram-LinearSVC</cell><cell>0.760</cell><cell>0.750</cell></row><row><cell>BoW -bigram-RidgeClassifier</cell><cell>0.766</cell><cell>0.750</cell></row><row><cell>BoW -bigram-LinearSVC</cell><cell>0.770</cell><cell>0.759</cell></row><row><cell cols="2">BoW -bigram-PassiveAggressive 0.755</cell><cell>0.749</cell></row><row><cell>BoW-bigram-MultinomialNB</cell><cell>0.750</cell><cell>0.742</cell></row><row><cell>MultinomialNB-bigram-NER</cell><cell>0.740</cell><cell>0.728</cell></row><row><cell>BoW -trigram-RidgeClassifier</cell><cell>0.764</cell><cell>0.748</cell></row><row><cell>BoW -trigram-LinearSVC</cell><cell>0.767</cell><cell>0.756</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,175.68,315.75,259.26,220.76"><head>Table 3 .</head><label>3</label><figDesc>Final evaluation of submitted runs over test data</figDesc><table coords="7,175.68,347.22,259.26,189.29"><row><cell>Method</cell><cell cols="2">Accuracy Macro Averaged F1</cell></row><row><cell>baseline-dimensions-bow-presence-SVM</cell><cell>0.622</cell><cell>0.380</cell></row><row><cell>* uogTr_RD_4</cell><cell>0.731</cell><cell>0.473</cell></row><row><cell>DAE_RD_1*</cell><cell>0.723</cell><cell>0.390</cell></row><row><cell>LyS_RD_1*</cell><cell>0.716</cell><cell>0.477</cell></row><row><cell>run1: BoW -bigram-LinearSVC</cell><cell>0.695</cell><cell>0.489</cell></row><row><cell>run2: BoW-bigram-MultinomialNB</cell><cell>0.685</cell><cell>0.475</cell></row><row><cell>run3: BoW-bigram-PassiveAggressive</cell><cell>0.661</cell><cell>0.482</cell></row><row><cell>run4: BoW-bigram-RidgeClassifier</cell><cell>0.703</cell><cell>0.469</cell></row><row><cell cols="3">*The best results from other participants with regards to accuracy</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This work has been funded through the project <rs type="projectName">StaViCTA</rs> by the framework grant "the <rs type="programName">Digitized Society -Past, Present</rs>, and Future" with No. <rs type="grantNumber">2012-5659</rs> from the <rs type="funder">Swedish Research Council (Vetenskapsrådet)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_J2TCgYj">
					<idno type="grant-number">2012-5659</idno>
					<orgName type="project" subtype="full">StaViCTA</orgName>
					<orgName type="program" subtype="full">Digitized Society -Past, Present</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,150.83,518.54,317.57,7.72;8,159.36,528.87,308.76,7.72;8,159.36,539.18,165.10,7.72" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,242.02,518.54,226.38,7.72;8,159.36,528.87,25.12,7.72">Overview of replab 2013: Evaluating online reputation monitoring systems</title>
		<author>
			<persName coords=""><forename type="first">Enrique</forename><surname>Amigó</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,193.92,528.87,274.20,7.72;8,159.36,539.18,12.27,7.72">Information Access Evaluation. Multilinguality, Multimodality, and Visualization</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="333" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,150.83,549.51,317.36,7.72;8,159.36,559.83,309.09,7.72;8,159.36,570.14,132.23,7.72" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,242.89,549.51,225.30,7.72;8,159.36,559.83,138.95,7.72">Overview of replab 2014: Author profiling and reputation dimensions for Online Reputation Management</title>
		<author>
			<persName coords=""><forename type="first">Enrique</forename><surname>Amigó</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,309.12,559.83,159.33,7.72;8,159.36,570.14,91.22,7.72">Proceedings of the Fifth International Conference of the CLEF Initiative</title>
		<meeting>the Fifth International Conference of the CLEF Initiative</meeting>
		<imprint>
			<date type="published" when="2014-09">Sep. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,150.83,580.46,317.63,7.72;8,159.36,590.78,230.85,7.72" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,239.57,580.46,176.77,7.72">Theano: a CPU and GPU math expression compiler</title>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,426.48,580.46,41.98,7.72;8,159.36,590.78,193.90,7.72">Proceedings of the Python for scientific computing conference (SciPy)</title>
		<meeting>the Python for scientific computing conference (SciPy)</meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,150.83,601.34,317.48,7.72;8,159.36,611.67,171.10,7.72" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,326.09,601.34,135.01,7.72">Twitter mood predicts the stock market</title>
		<author>
			<persName coords=""><forename type="first">Johan</forename><surname>Bollen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Huina</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaojun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,159.36,611.67,115.87,7.72">Journal of Computational Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,150.83,621.74,317.59,7.72;8,159.36,632.06,309.03,7.72;8,159.36,642.62,309.07,7.72" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,387.41,621.74,81.01,7.72;8,159.36,632.06,232.71,7.72">Incorporating non-local information into information extraction systems by gibbs sampling</title>
		<author>
			<persName coords=""><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trond</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,402.96,632.06,65.43,7.72;8,159.36,642.62,239.36,7.72">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics 25</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics 25</meeting>
		<imprint>
			<date type="published" when="2005-06">Jun. 2005</date>
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,150.83,140.54,317.37,7.72;9,159.36,150.87,309.33,7.72;9,159.36,161.42,157.90,7.72" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,238.97,140.54,229.23,7.72;9,159.36,150.87,11.61,7.72">Computing semantic compositionality in distributional semantics</title>
		<author>
			<persName coords=""><forename type="first">Emiliano</forename><surname>Guevara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,179.04,150.87,289.65,7.72;9,159.36,161.42,133.49,7.72">Proceedings of the Ninth International Conference on Computational Semantics. Association for Computational Linguistics</title>
		<meeting>the Ninth International Conference on Computational Semantics. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,150.83,171.75,317.60,7.72;9,159.36,181.82,295.18,7.72" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,248.85,171.75,174.12,7.72">Twitter power: Tweets as electronic word of mouth</title>
		<author>
			<persName coords=""><forename type="first">Bernard</forename><forename type="middle">J</forename><surname>Jansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,433.44,171.75,34.99,7.72;9,159.36,181.82,205.88,7.72">Journal of the American society for information science and technology</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2169" to="2188" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,150.83,192.14,317.56,7.72;9,159.36,202.70,309.07,7.72;9,159.36,213.02,119.04,7.72" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,352.82,192.14,115.57,7.72;9,159.36,202.70,93.12,7.72">Random indexing of text samples for latent semantic analysis</title>
		<author>
			<persName coords=""><forename type="first">Pentti</forename><surname>Kanerva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jan</forename><surname>Kristofersson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anders</forename><surname>Holst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,261.84,202.70,206.59,7.72;9,159.36,213.02,50.62,7.72">Proceedings of the 22nd annual conference of the cognitive science society</title>
		<meeting>the 22nd annual conference of the cognitive science society</meeting>
		<imprint>
			<date type="published" when="2000-08">Aug. 2000</date>
			<biblScope unit="page" from="1036" to="1037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,150.83,223.35,139.06,7.72;9,306.45,223.35,135.81,7.72" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="9,306.45,223.35,101.51,7.72">From Words to Understanding</title>
		<author>
			<persName coords=""><forename type="first">Jussi</forename><surname>Karlgren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Magnus</forename><surname>Sahlgren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,150.47,233.66,317.94,7.72;9,159.36,243.99,163.18,7.72" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.4053</idno>
		<title level="m" coord="9,286.80,233.66,181.61,7.72;9,159.36,243.99,18.44,7.72">Distributed Representations of Sentences and Documents</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,150.47,254.30,317.98,7.72;9,159.36,264.63,161.01,7.72" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="9,260.14,254.30,208.31,7.72;9,159.36,264.63,17.41,7.72">Efficient estimation of word representations in vector space</title>
		<author>
			<persName coords=""><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,150.47,274.95,317.94,7.72;9,159.36,285.51,242.85,7.72" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,242.25,274.95,226.15,7.72;9,159.36,285.51,33.21,7.72">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName coords=""><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,200.88,285.51,176.35,7.72">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,150.47,295.83,317.90,7.72;9,159.36,305.90,309.09,7.72;9,159.36,316.22,308.84,7.72;9,159.36,326.78,60.93,7.72" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,352.74,295.83,115.63,7.72;9,159.36,305.90,131.34,7.72">Thumbs up?: sentiment classification using machine learning techniques</title>
		<author>
			<persName coords=""><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shivakumar</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,300.00,305.90,168.45,7.72;9,159.36,316.22,162.43,7.72">Proceedings of the ACL-02 conference on Empirical methods in natural language processing</title>
		<meeting>the ACL-02 conference on Empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,150.47,337.10,317.98,7.72;9,159.36,347.42,163.65,7.72" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,248.94,337.10,139.79,7.72">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName coords=""><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,399.36,337.10,69.09,7.72;9,159.36,347.42,85.17,7.72">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,150.47,357.75,317.73,7.72;9,159.36,368.07,309.34,7.72;9,159.36,378.39,118.53,7.72" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="9,389.27,357.75,78.93,7.72;9,159.36,368.07,246.44,7.72">RepTrak™ pulse: Conceptualizing and validating a short-form measure of corporate reputation</title>
		<author>
			<persName coords=""><forename type="first">Leonard</forename><forename type="middle">J</forename><surname>Ponzi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Naomi</forename><forename type="middle">A</forename><surname>Fombrun</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Gardberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,414.96,368.07,53.74,7.72;9,159.36,378.39,50.83,7.72">Corporate Reputation Review</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="35" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,150.47,388.71,317.89,7.72;9,159.36,399.02,309.05,7.72;9,159.36,409.58,153.57,7.72" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="9,231.96,388.71,124.02,7.72">An introduction to random indexing</title>
		<author>
			<persName coords=""><forename type="first">Magnus</forename><surname>Sahlgren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,366.34,388.71,102.02,7.72;9,159.36,399.02,309.05,7.72;9,159.36,409.58,80.64,7.72">Methods and Applications of Semantic Indexing Workshop at the 7th International Conference on Terminology and Knowledge Engineering</title>
		<imprint>
			<date type="published" when="2005-08-16">16 Aug. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,150.47,419.90,317.98,7.72;9,159.36,429.98,48.94,7.72" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="9,235.00,419.90,102.06,7.72">The distributional hypothesis</title>
		<author>
			<persName coords=""><forename type="first">Magnus</forename><surname>Sahlgren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,346.32,419.90,115.22,7.72">Italian Journal of Linguistics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="54" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,150.47,440.31,317.98,7.72;9,159.36,450.87,150.44,7.72" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="9,314.23,440.31,132.14,7.72">Semantic sentiment analysis of twitter</title>
		<author>
			<persName coords=""><forename type="first">Hassan</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Harith</forename><surname>Alani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,456.00,440.31,12.45,7.72;9,159.36,450.87,91.11,7.72">The Semantic Web-ISWC 2012</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="508" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,150.47,461.19,317.49,7.72;9,159.36,471.51,222.93,7.72" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="9,311.35,461.19,156.61,7.72;9,159.36,471.51,26.22,7.72">Alleviating data sparsity for twitter sentiment analysis</title>
		<author>
			<persName coords=""><forename type="first">Hassan</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Harith</forename><surname>Alani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="9,194.15,471.51,143.76,7.72">CEUR Workshop Proceedings (CEUR-WS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,150.47,481.83,317.71,7.72;9,159.36,492.14,309.11,7.72;9,159.36,502.46,43.42,7.72" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="9,300.77,481.83,167.41,7.72;9,159.36,492.14,38.87,7.72">Predicting the 2011 dutch senate election results with twitter</title>
		<author>
			<persName coords=""><forename type="first">Erik</forename><forename type="middle">Tjong</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,207.12,492.14,232.77,7.72">Proceedings of the Workshop on Semantic Analysis in Social Media</title>
		<meeting>the Workshop on Semantic Analysis in Social Media</meeting>
		<imprint>
			<date type="published" when="2012-04">Apr. 2012</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="53" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,150.47,512.78,317.70,7.72;9,159.36,523.10,309.05,7.72;9,159.36,533.67,115.83,7.72" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="9,243.02,512.78,225.15,7.72;9,159.36,523.10,41.49,7.72">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,209.76,523.10,258.65,7.72;9,159.36,533.67,37.87,7.72">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011-07">Jul. 2011</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,150.47,543.99,318.19,7.72;9,159.36,554.06,190.07,7.72" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="9,253.61,543.99,215.05,7.72;9,159.36,554.06,83.87,7.72">Predicting Elections with Twitter: What 140 Characters Reveal about Political Sentiment</title>
		<author>
			<persName coords=""><forename type="first">Andranik</forename><surname>Tumasjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,252.72,554.06,26.55,7.72">ICWSM</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="178" to="185" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
