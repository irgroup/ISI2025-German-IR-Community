<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,211.37,169.24,172.60,12.00">Notebook for PAN at CLEF 2016</title>
				<funder ref="#_nBraQT8">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,273.29,207.72,60.28,8.89"><forename type="first">Mirco</forename><surname>Kocher</surname></persName>
							<email>mirco.kocher@unine.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Neuch√¢tel</orgName>
								<address>
									<addrLine>rue ; Emile Argand</addrLine>
									<postCode>11 2000</postCode>
									<settlement>Neuch√¢tel</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,211.37,169.24,172.60,12.00">Notebook for PAN at CLEF 2016</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">66AAA5565A05CB767D2BD147CEE3CEE6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes and evaluates an effective unsupervised author clustering authorship linking model called SPATIUM-L1. The suggested strategy can be adapted without any problem to different languages (such as Dutch, English, and Greek) in different genres (e.g., newspaper articles and reviews). As features, we suggest using the m most frequent terms of each text (isolated words and punctuation symbols with m at most 200). Applying a simple distance measure, we determine whether there is enough indication that two texts were written by the same author. The evaluations are based on six test collections (PAN AUTHOR CLUSTERING task at CLEF 2016).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The authorship attribution problem is an interesting problem in computational linguistics but also in applied areas such as criminal investigation and historical studies where knowing the author of a document (such as a ransom note) may be able to save lives. With the Web 2.0 technologies, the number of anonymous or pseudonymous texts is increasing and in many cases one person writes in different places about different topics (e.g., multiple blog posts written by the same author). Therefore, proposing an effective algorithm to the authorship problem presents a real interest. In this case, the system must regroup all texts by the same author (written according to different genres) into the same group or cluster. A justification supporting the proposed answer and a probability that the given answer is correct can be given to improve the confidence attached to the response <ref type="bibr" coords="1,269.45,552.25,54.90,9.05" target="#b6">(Savoy, 2016)</ref>.</p><p>This author clustering task is more demanding than the classical authorship attribution problem. Given a document collection the task is to group documents written by the same author such that each cluster corresponds to a different author. The number of distinct authors whose documents are included is not given. This task can also be viewed as establishing authorship links between documents and is related to the PAN 2015 task of authorship verification.</p><p>This paper is organized as follows. The next section presents the test collections and the evaluation methodology used in the experiments. The third section explains our proposed algorithm called SPATIUM-L1. In the last section, we evaluate the proposed scheme and compare it to the best performing schemes using six different test collections. A conclusion draws the main findings of this study.</p><p>To evaluate the effectiveness of a clustering algorithm, the number of tests must be large and run on a common test set. To create such benchmarks, and to promote studies in this domain, the PAN CLEF evaluation campaign was launched <ref type="bibr" coords="2,397.75,200.35,73.04,9.06;2,124.82,211.76,21.57,9.05" target="#b9">(Stamatatos et al., 2016)</ref>. Multiple research groups with different backgrounds from around the world have participated in the PAN CLEF 2016 campaign. Each team has proposed a clustering strategy that has been evaluated using the same methodology. The evaluation was performed using the TIRA platform, which is an automated tool for deployment and evaluation of the software <ref type="bibr" coords="2,302.93,257.83,82.67,9.06" target="#b2">(Gollub et al., 2012)</ref>. The data access is restricted such that during a software run the system is encapsulated and thus ensuring that there is no data leakage back to the task participants <ref type="bibr" coords="2,357.07,280.78,85.89,9.06" target="#b5">(Potthast et al., 2014)</ref>. This evaluation procedure also offers a fair evaluation of the time needed to produce an answer.</p><p>During the PAN CLEF 2016 evaluation campaign, six collections were built each containing six problems (training + testing). In each problem, all the texts matched the same language, are in the same genre, and are single-authored, but they may differ in text-length and can be cross-topic. The number of distinct authors is not given. In this context, a problem is defined as:</p><p>Given a collection of up to 100 documents, identify authorship links and groups of documents by the same author. The six collections are a combination of one of three languages (English, Dutch, or Greek) and one of two genres (newspaper articles or reviews). An overview of these collections is depicted in Table <ref type="table" coords="2,249.22,418.79,3.76,9.05" target="#tab_0">1</ref>. The training set will be used to evaluate our approach and the test set will be used in order to be able to compare our results with those of the PAN CLEF 2016 campaign. For each benchmark we have three problems in the training dataset containing the same number of texts with the exact corresponding number given under the label "Texts". The number of distinct authors for each problem is indicated in the column "Authors", and the number of authors with only a single document under the label "Single". For example, with the English newspaper collection (training set), 50 texts are written by 35 authors and in this text subset we can find 27 authors who wrote only one single article. These metrics are not available for the test corpora because the datasets remained undisclosed thanks to the TIRA system. We only know that the same combinations of language and genre are present.</p><p>When inspecting the training collection of Dutch reviews, the number of words available is rather small (in mean 130 words for each document). Overall, there are many authors who only wrote a single text, so the number of authors per problem is rather large. This means we should only cluster two documents if there are enough signs for a single authorship.</p><p>During the PAN CLEF 2016 campaign, a system must return two outputs in a JSON structure. First, the detected groups have to be written to a file indicating the author clustering. Each document has to belong to exactly one cluster, thus the clusters have to be non-overlapping. Second, a list of document pairs with a probability of having the same author has to be written to another file representing the authorship links.</p><p>As performance measure, two evaluation measures were used during the PAN CLEF campaign. The first performance measure is the BCubed F-Score <ref type="bibr" coords="3,389.11,299.02,81.68,9.06">(Amigo et al., 2007)</ref> to evaluate the clustering output. This value is the harmonic mean of the precision and recall associated to each document. The document precision represents how many documents in the same cluster are written by the same author. Symmetrically, the recall associated to one document represents how many documents from that author appear in its cluster.</p><p>As another measure, the PAN CLEF campaign adopts the mean average precision (MAP) measure for the authorship links between document pairs <ref type="bibr" coords="3,402.53,379.54,68.30,9.06;3,124.82,391.07,21.57,9.05" target="#b4">(Manning et al., 2008)</ref>. This evaluation measure provides a single-figure measure of quality across recall levels. The MAP is roughly the average area under the precision-recall curve for a set of problems. Therefore, this measure gives more emphasis on the first positions and a misclassification with a lower probability is less penalized.</p><p>Considering the six benchmarks as a whole, we have 18 problems to solve and 18 problems to train (pre-evaluate) our system. Because there are many authors with only a single document, we can compare our approach with a na√Øve baseline, which clusters each text in an individual cluster. This means the document precision is always 100%. The documents recall is lower, but should still be competitive due to the low number of expected clusters. Furthermore, random scores are assigned for all combinations in the authorship links.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Simple Clustering Algorithm</head><p>To solve the clustering problem, we suggest an unsupervised approach based on a simple feature extraction and distance measure called SPATIUM-L1 (Latin word meaning distance). The selected stylistic features correspond to the top m most frequent terms (isolated words without stemming but with the punctuation symbols). For determining the value of m, previous studies have shown that a value between 200 and 300 tends to provide the best performance <ref type="bibr" coords="3,310.37,628.84,63.24,9.05" target="#b1">(Burrows, 2002</ref>; Savoy 2016). Some documents were rather short and we further excluded the words only appearing once in the text. This filtering decision was taken to prevent overfitting to single occurrences. The effective number of terms m was set to at most 200 terms but was in most cases well below. With this reduced number the justification of the decision will be simpler to understand because it will be based on words instead of letters, bigrams of letters or combinations of several representation schemes or distance measures.</p><p>To measure the distance between one document A and another text B, SPATIUM-L1 uses the L1-norm as follows:</p><formula xml:id="formula_0" coords="4,218.33,196.45,251.02,13.32">‚àÜ(ùê¥, ùêµ) = ‚àÜ ùê¥ùêµ = ‚àë |ùëÉ ùê¥ [ùë° ùëñ ] -ùëÉ ùêµ [ùë° ùëñ ]| ùëö ùëñ=1<label>(1)</label></formula><p>where m indicates the number of terms (words or punctuation symbols), and PA[ti] and PB[ti] represent the estimated occurrence probability of the term ti in the first text A and in the other text B respectively. To estimate these probabilities, we divide the term occurrence frequency (tfi) by the length in tokens of the corresponding text (n), Prob[ti] = tfi / n, without smoothing and therefore accepting a 0.0 probability. To verify whether the resulting ‚àÜ ùê¥ùêµ value is small or rather large, we need to have a comparison. To achieve this, the distance from A to all other k texts from the current problem was calculated. If this ‚àÜ ùê¥ùêµ value is 2.0 standard deviations below the average of all distances, then this is a first indication of an author link. Since the m terms are always selected from the first text, the ‚àÜ ùê¥ùêµ value might be different from the ‚àÜ ùêµùê¥ value. We therefore calculate the distance of text B with all other k texts and if this ‚àÜ ùêµùê¥ value is as well 2.0 standard deviations below the average of all distances, then this is our second indication of an author link. The exact difference to the mean divided by the standard deviation is used to calculate how much the indication weights, where a higher number means more evidence of a shared authorship. For example, in the second English review problem we cluster document 9 together with document 50. The ‚àÜ 9;50 value is 32, while the average ‚àÜ 9;ùêµ value to all other texts is 45 with a standard deviation of 5.6, which results in a first indication of (45 -32)/5.6 = 2.3. A higher value means more evidence of a shared authorship.</p><p>For the grouping stage we follow the transitivity rule. If we have enough indication that the texts A and B are written by the same author and we also have indication that the documents B and C have a single authorship, then we will group A, B, and C together even if we don't have enough evidence that A and C have the same writer.</p><p>For the author link, on the other hand, we only report A-B and B-C as a having the same author in this scenario, while leaving out A-C due to the absence of any previous sign for a single authorship. Furthermore, since this step allows a ranked listing of the author links, we assigned the highest probability to the text pair where we have the most evidence. A rather low probability is attributed to document pairs where we only have partial indication of a shared authorship.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>Since our system is based on an unsupervised approach we were able to directly evaluate it using the training set. In Table <ref type="table" coords="4,298.86,618.40,3.71,9.05" target="#tab_1">2</ref>, we have reported the same performance measure applied during the PAN CLEF campaign, namely the BCubed F-Score and the MAP. Each collection consists of three sets of problems and we report the average of them. The final score is the mean between the two reported metrics. The algorithm returns the best results for the Greek Review collection with a final score of 0.5590 followed by the Greek Article and Dutch Article corpora. The worst result is achieved with the two English collections which are slightly worse than the Dutch Review corpus. For the two Dutch collections we can clearly see the difference in text length reflected in the final score, as the newspaper corpus contains almost 10 times more words and achieves a noteworthy higher value. Our approach achieves an F-Score that is slightly higher than the one from the na√Øve baseline, but a significantly higher MAP.</p><p>The test set is then used to rank the performance of all 7 participants in this task. Based on the same evaluation methodology, we achieve the results depicted in Table <ref type="table" coords="5,465.79,391.55,4.98,9.05" target="#tab_2">3</ref> corresponding to the six test corpora.</p><p>As we can see, the final score with the Greek Review corpus is the highest as expected from the training set. The results we achieved in the two English collection is as low as in the training set. On the other hand, the Greek result achieved for the newspaper part is only slightly worse than the estimation from the training set. Generally, we see a very similar performance when comparing it with the training set. Therefore, the system seems to perform stable independent of the underlying text collection and is not over-fitted to the data. To put those values in perspective we can see in Table <ref type="table" coords="5,363.31,651.04,4.98,9.05" target="#tab_3">4</ref> our result in comparison with the top three of all participants using macro-averaging for the effectiveness measures and showing the total runtime. We have also added our na√Øve baseline as described above. As in the training collections, our approach achieves an F-Score that is slightly higher than the one from the na√Øve baseline, but a significantly higher MAP. Therefore, some documents were wrongly clustered together, which decreases the document precision part of the BCubed F-Score. But we cluster many documents correctly together (increases document recall) and assign them a high score for their authorship link (increases MAP). Overall, this is beneficial and we are ranked second out of eight approaches. The runtime only shows the actual time spent to classify the test set. On TIRA there was the possibility to first train the system using the training set which had no influence on the final runtime. Since we have an unsupervised system it did not need to train any parameters, but this possibility might have been used by other participants. Overall, we achieve excellent results using a rather simple and fast approach in comparison with the other solutions <ref type="foot" coords="6,198.74,406.29,3.00,5.45" target="#foot_0">1</ref> .</p><p>In text categorization studies, we are convinced that a deeper analysis of the evaluation results is important to obtain a better understanding of the advantages and drawbacks of a suggested scheme. By just focusing on overall performance measures, we only observe a general behavior or trend without being able to acquire a better explanation of the proposed assignment. To achieve this deeper understanding, we could analyze some problems extracted from the English corpus. Usually, the relative frequency (or probability) differences with very frequent words such as when, is, in, that, to, or it can explain the decision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper proposes a simple unsupervised technique to solve the author clustering problem. As features to discriminate between the proposed author and different candidates, we propose using at most the top 200 most frequent terms (words and punctuations). This choice was found effective for other related tasks such as authorship attribution <ref type="bibr" coords="6,219.57,609.85,67.28,9.05" target="#b1">(Burrows, 2002)</ref>. Moreover, compared to various feature selection strategies used in text categorization <ref type="bibr" coords="6,317.84,621.40,72.27,9.05" target="#b8">(Sebastiani, 2002)</ref>, the most frequent terms tend to select the most discriminative features when applied to stylistic studies <ref type="bibr" coords="6,124.82,644.32,54.90,9.05" target="#b7">(Savoy, 2015)</ref>. In order to take the author linking decision, we propose using a simple distance measure called SPATIUM-L1 based on the L1 norm.</p><p>The proposed approach tends to perform very well in three different languages (Dutch, English, and Greek) and in two genres (newspaper articles and reviews, but keeping the same genre inside a given test collection). Such a classifier strategy can be described as having a high bias but a low variance <ref type="bibr" coords="7,328.83,184.03,78.71,9.06" target="#b3">(Hastie et al., 2009)</ref>. Changing the training data does not change a lot the decision. However, the suggested approach ignores other significant information such as mean sentence length, POS (part of speech) distribution, or topical terms. Even if the proposed system cannot capture all possible stylistic features (bias), changing the available data does not modify significantly the overall performance (variance).</p><p>It is common to fix some parameters (such as time period, size, genre, or length of the data) to minimize the possible source of variation in the corpus. However, our goal was to present a simple and unsupervised approach without many predefined arguments.</p><p>With SPATIUM-L1 the proposed clustering could be clearly explained because it is based on a reduced set of features on the one hand and, on the other, those features are words or punctuation symbols. Thus the interpretation for the final user is clearer than when working with a huge number of features, when dealing with n-grams of letters or when combing several similarity measures. The SPATIUM-L1 decision can be explained by large differences in relative frequencies of frequent words, usually corresponding to functional terms.</p><p>To improve the current classifier, we will investigate the consequence of some smoothing techniques, the effect of other distance measures, and different feature selection strategies. In the latter case, we want to maintain a reduced number of terms. In a better feature selection scheme, we can take account of the underlying text genre, as for example, the most frequent use of personal pronouns in narrative texts. As another possible improvement, we can ignore specific topical terms or character names appearing frequently in an author profile, and terms that can be selected in the feature set without being useful in discriminating between authors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,146.18,467.88,302.75,105.06"><head>Table 1 .</head><label>1</label><figDesc>PAN  </figDesc><table coords="2,146.18,467.88,302.75,105.06"><row><cell></cell><cell></cell><cell cols="3">CLEF 2016 training corpora statistics</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Training Sets</cell></row><row><cell>Corpus</cell><cell>Texts</cell><cell>Authors</cell><cell>Single</cell><cell>Words</cell></row><row><cell>English Newspaper</cell><cell>50</cell><cell cols="2">35; 25; 43 27; 17; 37</cell><cell>741; 745; 734</cell></row><row><cell>English Reviews</cell><cell>80</cell><cell cols="2">55; 70; 40 39; 62; 17</cell><cell>969; 1080; 1020</cell></row><row><cell>Dutch Newspaper</cell><cell>57</cell><cell cols="3">51; 28; 40 46; 20; 32 1,086; 1,334; 1,026</cell></row><row><cell>Dutch Reviews</cell><cell cols="3">100 54; 67; 91 31; 44; 83</cell><cell>128; 135; 126</cell></row><row><cell>Greek Newspaper</cell><cell>55</cell><cell cols="2">28; 38; 48 10; 26; 42</cell><cell>756; 750; 735</cell></row><row><cell>Greek Reviews</cell><cell>55</cell><cell cols="2">50; 28; 40 46; 13; 29</cell><cell>534; 646; 756</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,199.10,149.47,207.43,133.17"><head>Table 2 .</head><label>2</label><figDesc>Evaluation for the six training collections</figDesc><table coords="5,199.10,164.48,197.23,118.16"><row><cell>Corpus</cell><cell>Final F-Score MAP</cell></row><row><cell cols="2">English Newspaper 0.4116 0.7915 0.0317</cell></row><row><cell>English Review</cell><cell>0.4144 0.8036 0.0252</cell></row><row><cell>Dutch Newspaper</cell><cell>0.4720 0.8230 0.1210</cell></row><row><cell>Dutch Review</cell><cell>0.4285 0.8201 0.0369</cell></row><row><cell>Greek Newspaper</cell><cell>0.4804 0.8239 0.1368</cell></row><row><cell>Greek Review</cell><cell>0.5590 0.8480 0.2700</cell></row><row><cell>Overall</cell><cell>0.4610 0.8184 0.1036</cell></row><row><cell>Na√Øve Baseline</cell><cell>0.4169 0.8115 0.0222</cell></row><row><cell></cell><cell>.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,199.10,512.52,199.46,121.29"><head>Table 3 .</head><label>3</label><figDesc>Evaluation for the six test collections.</figDesc><table coords="5,199.10,527.53,197.23,106.28"><row><cell>Corpus</cell><cell>Final F-Score MAP</cell></row><row><cell cols="2">English Newspaper 0.4295 0.8159 0.0431</cell></row><row><cell>English Review</cell><cell>0.4207 0.8199 0.0214</cell></row><row><cell>Dutch Newspaper</cell><cell>0.4291 0.8160 0.0421</cell></row><row><cell>Dutch Review</cell><cell>0.4302 0.8135 0.0468</cell></row><row><cell>Greek Newspaper</cell><cell>0.4370 0.8191 0.0548</cell></row><row><cell>Greek Review</cell><cell>0.4814 0.8467 0.1160</cell></row><row><cell>Overall</cell><cell>0.4379 0.8218 0.0540</cell></row><row><cell>Na√Øve Baseline</cell><cell>0.4187 0.8209 0.0165</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,152.42,247.52,290.35,81.32"><head>Table 4 .</head><label>4</label><figDesc>Evaluation comparison.</figDesc><table coords="6,152.42,259.52,290.35,69.32"><row><cell cols="2">Rank User</cell><cell cols="4">Final F-Score MAP Runtime (h:m:s)</cell></row><row><cell>1</cell><cell>bagnall16</cell><cell cols="3">0.4956 0.8223 0.1689</cell><cell>63:03:59</cell></row><row><cell>2</cell><cell>kocher16</cell><cell cols="3">0.4379 0.8218 0.0540</cell><cell>00:01:50</cell></row><row><cell>3</cell><cell>Na√Øve Baseline</cell><cell cols="3">0.4187 0.8209 0.0165</cell><cell>00:00:34</cell></row><row><cell>4</cell><cell>sari16</cell><cell cols="3">0.4176 0.7952 0.0399</cell><cell>00:07:48</cell></row><row><cell cols="2">‚Ä¶ ‚Ä¶</cell><cell>‚Ä¶</cell><cell>‚Ä¶</cell><cell>‚Ä¶</cell><cell>‚Ä¶</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="6,130.10,686.14,147.96,8.18"><p>http://www.tira.io/task/author-clustering/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The author wants to thank the task coordinators for their valuable effort to promote test collections in author clustering. This research was supported, in part, by the <rs type="funder">NSF</rs> under Grant #<rs type="grantNumber">200021_149665/1</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_nBraQT8">
					<idno type="grant-number">200021_149665/1</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,146.58,613.69,324.01,9.05;7,160.82,625.12,309.77,9.05;7,160.82,636.63,154.53,9.06" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,401.47,613.69,69.12,9.05;7,160.82,625.12,305.74,9.05">A comparison of Extrinsic Clustering Evaluation Metrics based on Formal Constraints</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Artiles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,160.82,636.63,85.60,9.06">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="461" to="486" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,146.58,648.16,324.06,9.05;7,160.82,659.67,288.47,9.06" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,246.41,648.16,224.24,9.05;7,160.82,659.68,71.74,9.05">Delta: A Measure of Stylistic Difference and a Guide to Likely Authorship</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Burrows</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,241.97,659.67,137.72,9.06">Literary and Linguistic Computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="267" to="287" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,146.58,671.20,323.80,9.05;7,160.82,682.72,309.45,9.05;8,160.82,149.47,304.91,9.06;8,465.79,148.40,5.04,5.90;8,160.82,160.99,128.97,9.06" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,342.94,671.20,127.45,9.05;7,160.82,682.72,264.12,9.05">Ousting Ivory Tower Research: Towards a Web Framework for Providing Experiments as a Service</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Burrows</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,399.07,149.47,66.66,9.06;8,465.79,148.40,5.04,5.90;8,160.82,160.99,73.09,9.06">SIGIR. The 35 th International ACM</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Hersh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Callan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Maarek</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1125" to="1126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,146.58,172.51,324.15,9.06;8,160.82,184.03,309.87,9.06;8,160.82,195.56,46.68,9.05" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<title level="m" coord="8,362.59,172.51,108.15,9.06;8,160.82,184.03,208.32,9.06">The Elements of Statistical Learning. Data Mining, Inference, and Prediction</title>
		<meeting><address><addrLine>New York (NY</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,146.58,206.95,324.10,9.06;8,160.82,218.47,209.51,9.06" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="8,407.11,206.95,63.57,9.06;8,160.82,218.47,85.60,9.06">Introduction to Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Raghaven</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sch√ºtze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,146.58,230.00,323.59,9.05;8,160.82,241.52,309.81,9.05;8,160.82,253.04,309.44,9.05;8,160.82,264.59,309.36,9.05;8,160.82,275.98,309.96,9.06;8,160.82,287.51,36.77,9.05" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,190.62,241.52,280.01,9.05;8,160.82,253.04,228.12,9.05">Improving the Reproducibility of PAN&apos;s Shared Tasks: -Plagiarism Detection, Author Identification, and Author Profiling</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="8,188.97,275.98,185.28,9.06">CLEF. Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Lupu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hall</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Handbury</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Toms</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">8685</biblScope>
			<biblScope unit="page" from="268" to="299" />
			<date type="published" when="2014">2014</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,146.58,299.03,323.93,9.05;8,160.82,310.54,309.92,9.06;8,160.82,322.07,45.96,9.05" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,237.15,299.03,229.47,9.05">Estimating the Probability of an Authorship Attribution</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,160.82,310.54,277.76,9.06">Journal of American Society for Information Science &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1462" to="1472" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,146.58,333.59,323.85,9.05;8,160.82,344.98,310.09,9.06;8,160.82,356.51,17.61,9.05" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,233.44,333.59,237.00,9.05;8,160.82,344.99,91.27,9.05">Comparative Evaluation of Term Selection Functions for Authorship Attribution</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,263.09,344.98,153.85,9.06">Digital Scholarship in the Humanities</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="246" to="261" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,146.58,368.03,323.61,9.05;8,160.82,379.54,150.36,9.06" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,252.38,368.03,213.73,9.05">Machine Learning in Automatic Text Categorization</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,160.82,379.54,96.06,9.06">ACM Computing Survey</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,151.20,391.07,318.94,9.05;8,160.82,402.47,309.71,9.05;8,160.82,413.99,309.89,9.05;8,160.82,425.50,189.93,9.06" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,291.89,402.47,178.64,9.05;8,160.82,413.99,43.14,9.05">Clustering by Authorship Within and Across Documents</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tschuggnall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Verhoeven</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Daelemans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Specht</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="8,227.44,413.99,238.79,9.05">Working Notes Papers of the CLEF 2016 Evaluation Labs</title>
		<title level="s" coord="8,160.82,425.50,119.81,9.06">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
