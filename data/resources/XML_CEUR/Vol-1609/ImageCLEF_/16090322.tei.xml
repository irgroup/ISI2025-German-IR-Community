<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,166.24,115.96,282.87,12.62;1,269.03,133.89,77.29,12.62">Image annotation and two paths to text illustration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,136.12,172.08,71.63,8.74"><forename type="first">Herv√©</forename><surname>Le Borgne</surname></persName>
							<email>herve.le-borgne@cea.fr</email>
							<affiliation key="aff0">
								<orgName type="department">LIST</orgName>
								<orgName type="laboratory">Laboratory of Vision and Content Engineering</orgName>
								<orgName type="institution">CEA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,215.88,172.08,69.46,8.74"><forename type="first">Etienne</forename><surname>Gadeski</surname></persName>
							<email>etienne.gadeski@cea.fr</email>
							<affiliation key="aff0">
								<orgName type="department">LIST</orgName>
								<orgName type="laboratory">Laboratory of Vision and Content Engineering</orgName>
								<orgName type="institution">CEA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,293.35,172.08,47.10,8.74"><forename type="first">Ines</forename><surname>Chami</surname></persName>
							<email>ines.chami@cea.fr</email>
							<affiliation key="aff0">
								<orgName type="department">LIST</orgName>
								<orgName type="laboratory">Laboratory of Vision and Content Engineering</orgName>
								<orgName type="institution">CEA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,349.03,172.08,48.44,8.74"><forename type="first">Thi</forename><surname>Quynh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">LIST</orgName>
								<orgName type="laboratory">Laboratory of Vision and Content Engineering</orgName>
								<orgName type="institution">CEA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,400.78,172.08,37.94,8.74"><forename type="first">Nhi</forename><surname>Tran</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">LIST</orgName>
								<orgName type="laboratory">Laboratory of Vision and Content Engineering</orgName>
								<orgName type="institution">CEA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,446.75,172.08,32.49,8.74;1,174.28,184.04,53.07,8.74"><forename type="first">Youssef</forename><surname>Tamaazousti</surname></persName>
							<email>youssef.tamaazousti@cea.fr</email>
							<affiliation key="aff0">
								<orgName type="department">LIST</orgName>
								<orgName type="laboratory">Laboratory of Vision and Content Engineering</orgName>
								<orgName type="institution">CEA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,235.50,184.04,109.16,8.74"><forename type="first">Alexandru</forename><forename type="middle">Lucian</forename><surname>Ginsca</surname></persName>
							<email>alexandru.ginsca@cea.fr</email>
							<affiliation key="aff0">
								<orgName type="department">LIST</orgName>
								<orgName type="laboratory">Laboratory of Vision and Content Engineering</orgName>
								<orgName type="institution">CEA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,371.94,184.04,69.13,8.74"><forename type="first">Adrian</forename><surname>Popescu</surname></persName>
							<email>adrian.popescu@cea.fr</email>
							<affiliation key="aff0">
								<orgName type="department">LIST</orgName>
								<orgName type="laboratory">Laboratory of Vision and Content Engineering</orgName>
								<orgName type="institution">CEA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,166.24,115.96,282.87,12.62;1,269.03,133.89,77.29,12.62">Image annotation and two paths to text illustration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AEE1CCD32917A86C03DD3F5CBC45396F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our participation to the ImageCLEF 2016 scalable concept image annotation main task and Text Illustration teaser. Regarding image annotation, we focused on better localizing the detected features. For this, we identified the saliency of the image to collect a list of potential interesting places into the image. We also added a specific human attribute detector that boosted the results of the best performing team in 2015. For the text illustration, we proposed two complementary approaches. The first one relies on semantic signatures that give a textual description of an image. This description is further matched to the textual query. The second approach learns a common latent space, in which visual and textual features are directly comparable. We propose a robust description, as well as the use of an auxiliary dataset to improve retrieval. While the first approach only uses external data, the second one was mainly learned from the provided training dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper describes our participation to the ImageCLEF 2016 <ref type="bibr" coords="1,426.55,487.70,15.50,8.74" target="#b26">[27]</ref> scalable concept image annotation main task (IAL: image annotation and localization) and Text Illustration teaser that are described in detail in <ref type="bibr" coords="1,391.30,511.61,9.96,8.74" target="#b4">[5]</ref>.</p><p>Regarding image annotation, we improved our 2015 system <ref type="bibr" coords="1,414.50,524.09,10.51,8.74" target="#b3">[4]</ref> and focused on better localizing the detected features. In 2015, we proposed a concept localization pipeline which uses the spatial information that CNNs offer. To improve this, we identified the saliency of the image to collect a list of potential interesting places from the image, then detected the concepts found in these boxes. We also added a specific human attribute detector that boosted the results of the best performing team in 2015.</p><p>For text illustration, we proposed two complementary approaches. The first one relies on semantic signatures that give a textual description of an image. This description is further matched to the textual query. The second approach relies on the learning of a common latent space, in which visual and textual features are directly comparable, using a robust description and an auxiliary dataset to improve retrieval. While the first approach only uses external data, the second one was mainly learned from the provided training dataset.</p><p>This manuscript is organized as follows. Section 2 deals with our participation to the image annotation and localization subtask, while Section 3 is dedicated to the text illustration teaser. Each time, we discuss some limits of the task itself that are important to better understand the results. Then we present the method(s) and finally comment the results of the campaign.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Image annotation task</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dataset limitation</head><p>As highlighted last year by the team that obtained the best results <ref type="bibr" coords="2,444.03,266.12,14.61,8.74" target="#b14">[15]</ref>, the development set of the image annotation subtask (and it is probably the case for the test set as well) suffers from severe limitations due to the crowd-sourcing ground-truth annotation. They explain the annotation are inconsistent, incomplete, sometimes incorrect and there are even some cases that are "impossible" according to the assumptions of the task (e.g. the fact there are at most 100 concepts per image).</p><p>It seems these issues have not been addressed in the 2016 development set, thus the results are still subject to some limitations. However, on the other hand, the task is thus consistent with last year's results and we can directly compare the improvement from one year to another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Method</head><p>In this section, we detail the training and testing frameworks that we used. Our method is based upon deep CNNs which have lately shown outstanding performances in diverse computer vision tasks such as object classification, localization and action recognition <ref type="bibr" coords="2,235.53,471.13,15.50,8.74" target="#b19">[20,</ref><ref type="bibr" coords="2,252.68,471.13,7.01,8.74" target="#b6">7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training</head><p>Data. We collected a set of roughly 251, 000 images (1, 000 images per concept) from the Bing Images search engine. For each concept we used its name and its synonyms (if present) to query the search engine. This dataset is of course noisy but some works showed it is not a big issue to train a deep CNN <ref type="bibr" coords="2,413.92,560.58,10.52,8.74" target="#b5">[6,</ref><ref type="bibr" coords="2,426.10,560.58,11.62,8.74" target="#b28">29]</ref>. We used this additional data to train a 16-layer CNN <ref type="bibr" coords="2,333.58,572.54,15.50,8.74" target="#b20">[21]</ref> and the 50-layer ResNet <ref type="bibr" coords="2,462.33,572.54,14.61,8.74" target="#b9">[10]</ref>. We used 90% of the dataset for training and 10% for validation.</p><p>Network Settings. The networks were initialized with ImageNet weights. The initial learning rate is set to 0.001 and the batch size is set to 256. The last layer (the classifier) is trained from scratch, i.e. it is initialized with random weights sampled from a Gaussian distribution (œÉ = 0.01 and ¬µ = 0) and its learning rate is 10 times larger than for other layers. During training, the dataset is enhanced with random transformations: RGB jittering, scale jittering, contrast adjustment, JPEG compression and flips. It is known that data augmentation leads to better models <ref type="bibr" coords="3,237.99,142.90,15.50,8.74" target="#b29">[30]</ref> and reduces overfitting. Finally, the networks take a 224 √ó 224 RGB image as input and produce 251 outputs, i.e. the number of concepts. The models were trained on a single Nvidia Titan Black with our modified version of the Caffe framework <ref type="bibr" coords="3,316.31,178.77,14.60,8.74" target="#b13">[14]</ref>.</p><p>Localizing concepts. We provide two approaches to detect the concepts and localize them.</p><p>The first method, named FCN, is the same as described in <ref type="bibr" coords="3,408.49,233.50,9.96,8.74" target="#b3">[4]</ref>. It is a simple and efficient framework where the concept detection and localization are done simultaneously with a unique forward pass of the image to process. More indepth information about this framework can be found in <ref type="bibr" coords="3,385.35,269.36,9.96,8.74" target="#b3">[4]</ref>.</p><p>The second method is based upon the generic object detector EdgeBoxes <ref type="bibr" coords="3,466.81,281.47,14.61,8.74" target="#b30">[31]</ref>, which takes an image as input and produces R regions where visual objects are likely to appear (objectness detection). In our experiments, we extracted a maximum of 100 regions per image then fed each one to the CNN models. We finally kept the concept that had the highest probability among the 251 concepts. Therefore this framework outputs R predictions per image.</p><p>In addition to these methods, we also used a face detector <ref type="bibr" coords="3,406.46,353.36,15.50,8.74" target="#b25">[26]</ref> to categorize more precisely the faces, eyes, noses and mouths. We extracted those features on all images and aggregated the results to the boxes detected by the CNN frameworks. It was our belief that it would slightly boost our accuracy since these kind of "objects" are quite hard to capture even with a good generic object detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Combination of runs</head><p>We also combined some runs by simply concatenating detected the boxes. When the number of boxes was above the allowed limit (100) we randomly removed some of them (this case was very rare).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Results</head><p>We submitted ten runs to the campaign, with settings that allow to measure the benefit of different choices. We studied the influence of three parameters:</p><p>our last year's method used to localize the concepts <ref type="bibr" coords="3,378.87,551.58,10.52,8.74" target="#b3">[4]</ref> compared to the use of EdgeBoxes <ref type="bibr" coords="3,213.47,563.53,15.50,8.74" target="#b30">[31]</ref> the CNN architecture, by comparing VGG <ref type="bibr" coords="3,334.74,575.64,15.50,8.74" target="#b20">[21]</ref> and ResNet <ref type="bibr" coords="3,404.62,575.64,15.50,8.74" target="#b9">[10]</ref> that obtained good results at the ILSVRC campaign in 2014 and 2015. the use of a face part detector <ref type="bibr" coords="3,286.30,599.71,15.50,8.74" target="#b25">[26]</ref> Results of individual runs are reported in Table <ref type="table" coords="3,346.06,620.10,3.87,8.74" target="#tab_0">1</ref>.</p><p>On the ILSVRC challenge, VGG had 7.3% classification error and ResNet obtained 5.7%. On the 2016 ImageCLEF dataset, we obtain similar results with VGG and ResNet when we use FCN to localize the concepts and VGG is better  with EdgeBoxes. Regarding the VGG-based scores, we noticed that our results are about 8 points better than last year, showing the benefit of the new learning process.</p><p>The use of a face part detector does not significantly improve our results and they are even lower when we use the EdgeBoxes-based localization. This is quite surprising since a similar process boosted the results of last year's best performing team from 30.39 to 65.95 <ref type="bibr" coords="4,266.42,416.79,14.61,8.74" target="#b14">[15]</ref>. However, regarding these results, we should notice this "boost" was observed on the test dataset only (on the development set, the performances were more or less the same with and without the body part detection). It is also hard to explain how the results can increase by 35 points while the body-part detector deals with less than 10 classes among 250. Following a discussion with the organizers of the campaign, it seems that there was a bug in the evaluation script (fixed since then, and probably reported on this year's overview <ref type="bibr" coords="4,220.94,500.47,10.79,8.74" target="#b4">[5]</ref>) and that detecting body parts is finally not very interesting, making our results in line with other participants' ascertainment. However, there are still some unexpected results with regards to the concepts that are directly concerned with face part detection. In Table <ref type="table" coords="4,365.40,536.34,3.87,8.74" target="#tab_1">2</ref>, we report the results for four of these concepts and two different settings (results of ResNet+EdgeBoxes are similar to VGG+EdgeBoxes). With FCN, the behavior is in line with expectation. On the contrary with EdgeBoxes, the concepts mouth and eye are perfectly detected, that is quite unlikely. Although there are obvious issues with EdgeBoxes as explained below, this strange result may be due to a remaining bug in the evaluation script.</p><p>The most disappointing result is that the use of the EdgeBoxes-based localization gives globally lower results than the FCN one. A possible reason is that EdgeBoxes generates much more boxes than FCN and that a significant part leads to wrong concept estimation, hence penalizing the global score.</p><p>Last, we evaluated the combination of runs, as reported in Table <ref type="table" coords="5,446.40,118.99,3.87,8.74" target="#tab_2">3</ref>. Once again, the results are quite disappointing since the more we combine, the lower the results are. Since the combination of runs is a simple concatenation of the boxes found by each run, it is not clear to us how the results can decrease (the mAP should be at least better than the weaker run). It probably results from the way the results are evaluated but unfortunately the exact method used is not available. 3 Text illustration</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task realism</head><p>The task consists of matching textual queries to images without using the textual features derived from the web page the images belong to, although these last ones are available since they are part of the 500k noisy dataset. In practice, the queries were mainly obtained by removing the HTML tags from these web pages, and retaining all the remaining text. It thus raises an issue with regards to the realism of the task. Indeed, when one wants to illustrate a text in practice, she would submit the interesting part of the text as query to the system. It does not make sense to add some noisy data in the query such as that coming from the generic task bar as in the query --/--diUdSrlGyv7zF4 that starts with: Of course, it is hard for the organizers to extract this "relevant text" at a large scale, since there are 180, 000 queries. However, this could be part of the task: if the query was the actual HTML page, the system could include an automatic search of the relevant text by using the DOM structure of the query. Fig. <ref type="figure" coords="6,153.45,241.54,3.87,8.74">1</ref>: The semantic signature principle: an image is described in terms of likelihood to be similar to some concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Semantic signature approach</head><p>Specific object detectors have been developed for a long time, to be able to recognize e.g. faces <ref type="bibr" coords="6,206.79,324.61,14.61,8.74" target="#b27">[28]</ref>, pedestrians <ref type="bibr" coords="6,280.06,324.61,10.52,8.74" target="#b2">[3]</ref> or buildings <ref type="bibr" coords="6,347.62,324.61,14.61,8.74" target="#b17">[18]</ref>. More recently, it has been proposed to use a set of object or concept detectors as image descriptors <ref type="bibr" coords="6,447.93,336.57,15.50,8.74" target="#b23">[24,</ref><ref type="bibr" coords="6,465.09,336.57,11.62,8.74" target="#b16">17]</ref>, introducing the "semantic features". With this approach, images are described into a fixed size vector space as it is the case with Bag-of-visual words, Ficher Kernels <ref type="bibr" coords="6,171.31,372.44,15.50,8.74" target="#b12">[13]</ref> or even when one uses the last fully connected layer of a CNN as a feature <ref type="bibr" coords="6,177.75,384.39,14.61,8.74" target="#b19">[20]</ref>. However, contrary to these approaches, each dimension of a semantic feature is associated to a precise concepts that makes sense for a human (Fig. <ref type="figure" coords="6,158.54,408.30,3.87,8.74">1</ref>). The "semantic signature approach" to text illustration thus consists in:</p><p>-(i) extracting relevant concept from images of reference -(ii) expressing the corresponding concepts with words and index them -(iii) matching the query to the index textually During the campaign, we tested several alternatives for each of these steps.</p><p>Regarding step (i), our system is based on recently published work <ref type="bibr" coords="6,447.93,488.32,15.50,8.74" target="#b22">[23,</ref><ref type="bibr" coords="6,465.09,488.32,11.62,8.74" target="#b21">22]</ref>, that is itself an extension of the Semfeat descriptor <ref type="bibr" coords="6,355.87,500.27,9.96,8.74" target="#b5">[6]</ref>. Relying on powerful midlevel-features such as <ref type="bibr" coords="6,228.97,512.23,14.61,8.74" target="#b20">[21]</ref>, this semantic feature is a large set of linear classifiers that are built automatically. The authors showed that keeping a small part of the K most active concepts and setting the others to zero (sparsification) led to a more efficient descriptor for an image retrieval task. However, there are two limitation to this approach: first, the value of K has to be fixed in advance; secondly, sparsification is not efficient in a classification context, in the sense that the performance obtained is below that of the mid-level feature it is built on. For these reasons, <ref type="bibr" coords="6,198.77,595.91,15.50,8.74" target="#b22">[23]</ref> proposed to compute K for each image independently, with regard to the actual content of the image. The principle is to keep the "dominant concepts" only, when we are confident on their detection.</p><p>For step (ii), we considered two sets of concepts. The first one, based on WordNet, contains 17, 467 concepts, each being described by its main synset (one word). The second set is that collected automatically in <ref type="bibr" coords="6,415.10,656.12,9.96,8.74" target="#b5">[6]</ref>. It contains around 30, 000 concepts derived from Flickr groups, each described by three words.</p><p>For the textual matching step (iii), we considered classical inverse indexing, computing the query-document similarity from the "weight/score" associated to each indexed document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Text-image common space approach</head><p>The design of common latent spaces has been proposed for a while <ref type="bibr" coords="7,435.77,215.43,15.50,8.74" target="#b15">[16,</ref><ref type="bibr" coords="7,452.93,215.43,11.62,8.74" target="#b18">19]</ref>, in particular in the case of textual and visual modality <ref type="bibr" coords="7,366.81,227.38,15.50,8.74" target="#b11">[12,</ref><ref type="bibr" coords="7,383.97,227.38,12.73,8.74" target="#b31">32,</ref><ref type="bibr" coords="7,398.37,227.38,12.73,8.74" target="#b10">11,</ref><ref type="bibr" coords="7,412.75,227.38,7.75,8.74" target="#b1">2,</ref><ref type="bibr" coords="7,422.16,227.38,7.01,8.74" target="#b7">8]</ref>. Given two modalities, let say a visual and a textual modality described by their respective features, the general idea is to learn a latent common sub-space of both feature spaces, such that visual points are directly comparable to textual ones. One of the recent popular approach is to use Canonical Correlated Analysis (CCA), in particular in its kernelised version (KCCA) <ref type="bibr" coords="7,326.54,287.16,9.96,8.74" target="#b8">[9]</ref>.</p><p>Let us consider N data samples</p><formula xml:id="formula_0" coords="7,292.65,297.54,120.11,12.32">{(x T i , x I i )} N i=1 ‚äÇ R d T √ó R d I ,</formula><p>simultaneously represented in two different vector spaces. The purpose of CCA is to find maximally correlated linear subspaces of these two vector spaces. More precisely, if one notes X T ‚àà R d T and X I ‚àà R d I two random variables, CCA simultaneously seeks directions w T ‚àà R d T and w I ‚àà R d I that maximize the correlation between the projections of x T onto w T and of x I onto w I ,</p><formula xml:id="formula_1" coords="7,214.31,380.43,266.28,25.19">w * T , w * I = arg max w T ,w I w T C T I w I w T C T T w T w I C II w I<label>(1)</label></formula><p>where C T T , C II denote the autocovariance matrices of X T and X I respectively, while C T I is the cross-covariance matrix. The solutions w * T and w * I are found solution of an eigenvalue problem. The d eigenvectors associated to the d largest eigenvalues define maximally correlated d-dimensional subspaces in R d T and respectively R d I . Even though these are linear subspaces of two different spaces, they are often referred to as "common" representation space.</p><p>KCCA aims to remove the linearity constraint by using the "kernel trick" to first map the data from each initial space to the reproducing kernel Hilbert space associated to a selected kernel and then looking for correlated subspaces in these RKHS.</p><p>In this space, textual and visual documents are directly comparable, thus it is possible to perform cross-modal retrieval <ref type="bibr" coords="7,331.31,548.52,15.50,8.74" target="#b11">[12,</ref><ref type="bibr" coords="7,348.46,548.52,12.73,8.74" target="#b10">11,</ref><ref type="bibr" coords="7,362.85,548.52,7.75,8.74" target="#b1">2,</ref><ref type="bibr" coords="7,372.26,548.52,3.51,8.74" target="#b7">8</ref>]. However, it has been recently found that the learned common space may not represent adequately all data <ref type="bibr" coords="7,157.32,572.43,14.61,8.74" target="#b24">[25]</ref>. It has thus be proposed a more robust representation data within the common space consisting in coding the original visual and textual point with respect to a codebook (Figure <ref type="figure" coords="7,266.39,596.34,8.30,8.74">2a</ref>). This method, named Multimedia Aggregated Correlated components (MACC) is detailed in <ref type="bibr" coords="7,343.28,608.30,14.61,8.74" target="#b24">[25]</ref>. Another contribution that "'compensates" the defaults of the representation space is to project a bi-modal auxiliary dataset into the common space and use the known text-image connections of this dataset as a "pivot" to link e.g. a textual query to an appropriate image of the reference database (Figure <ref type="figure" coords="7,310.77,656.12,8.58,8.74">2b</ref>). Fig. <ref type="figure" coords="8,153.45,331.96,3.87,8.74">2</ref>: Illustration of (a) the approach of <ref type="bibr" coords="8,315.84,331.96,15.50,8.74" target="#b24">[25]</ref> to describe robustly the bi-modal documents into a common description space (b) the pivot principle using an auxiliary database MACC considers the use of two datasets. A first training dataset T is used to learn the common space with a KCCA. Due to computational issues, the number of documents that can be used to learn this space is limited to few ten thousands. Hence, at a quite large scale such as that of the text illustration subtask, it is important to use a second auxiliary dataset A to "compensate" the possible limitation of the initial learning.</p><p>Once the settings of the common latent space are chosen, the principle of the text illustration is quite straightforward, as illustrated in Figure <ref type="figure" coords="8,424.33,476.54,3.87,8.74">3</ref>. Regarding the images of the reference database, we extract the same feature as that used during learning, namely the FC7 fully connected layer of VGG <ref type="bibr" coords="8,409.99,500.45,14.61,8.74" target="#b20">[21]</ref>. This vector is projected on the latent space and the MACC signature is computed then stored into the reference database.</p><p>Regarding the textual query, we process the raw text in order to fix the defaults identified in Section 3.1. We first remove the stopwords using the Stanford NLTK package <ref type="bibr" coords="8,223.71,560.48,10.52,8.74" target="#b0">[1]</ref> that contains lists of stopwords in several languages. As months, days and numbers are not included in the stopwords list from NLTK, we also filtered them out as they are hard to illustrate and might add noise to our model. Additionally, we also removed words containing special characters such as ""' that are often found in noisy words. Furthermore, we combined the stopwords list with a part of speech tagger developed in the NLTK library. NLTK Pos Tag categorizes our set of words and labels each word according to its grammatical properties. In order to take the more descriptive words, we choose to keep only nouns (proper and common) and adjectives. Fig. <ref type="figure" coords="9,153.45,370.59,3.87,8.74">3</ref>: General principle to illustrate a textual query from a purely visual database, using a common space and MACC representation.</p><p>For each word i of the resulting vocabulary, we extract a word2vec vectorial representation t i . Then, we compute a weight w i equal to its tfidf value. For a document d, we select the k terms in the textual description that have the largest weights. We then compute a unique vector v d , representing d, from the k selected words describing a document, weighting each t i with its corresponding weight w i , resulting into the Weighted Arithmetic Mean (WAM) v d = n i=1 witi n i=1 wi . As said above, the classical KCCA algorithm does only support some dozen thousand document to learn the latent common space. To get around this limitation, we first proceed to a selection of the training data, in order to build a corpus with a diversified vocabulary. To do so, we divide our train set of 300k documents into 10 groups of 30k documents each. We then clustered every group of textual features with K-Means and compute 100 clusters per group. To build a 20k documents corpus for instance, we select for each group 20 random documents per cluster (20K = n groups n clusters n doc = 10x100x20). Similarly, we build diversified sets for the pivotal basis by selecting a certain number of random documents per cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Submission and Results</head><p>We submitted four individual runs and three runs that merge them differently. Some synthetic results are presented in Table <ref type="table" coords="9,341.64,656.12,3.87,8.74" target="#tab_4">4</ref>. Globally, the recall at 100 is low, that is explained by the difficulty of the task as well as the noise in the queries. We run the method described in Section 3.2 with a semantic signature computed with CBS and both the WordNet and FlickRgroups vocabularies. We obtained better results with the smaller vocabulary of WordNet. The basic classifiers of the Wornet-based semantic features are learned with "cleaner" annotated images then those based on the FlickRGroups. However, since the original Semfeat paper <ref type="bibr" coords="10,200.35,371.75,10.52,8.74" target="#b5">[6]</ref> showed better or similar results with both types of classifiers, we suggest that in the current task the (small) difference of performance may be due to a better coverage of the vocabulary with respect to the queries.</p><p>The approach based on the common space and the MACC representation lead to significantly better results. A first run Wam5 used 22k images for T , 64k for A and 5 best words were retained to build the training and testing textual features. In the second run Wam7 we used the same training dataset to learn the common space but A was extended to 164k documents while we retained up to 30k words to build the textual training features. For the textual testing features, we kept only 10 words to build the WAM, because of the noisy aspect of the query data.</p><p>The experiments we run on a development dataset (not reported) extracted from the 300k development images suggest that a large part of the improvement between Wam5 and Wam7 is due to the growth of the auxiliary dataset A.</p><p>By merging several runs, the results are marginally improved. The run mergeA concatenates 10 best results of CBS+WordNet and Wam5 with 80 best of Wam7. We then also consider run wam8 similar to Wam7 but its testing textual queries that were built with five best words (instead of 10 for Wam7). The run mergeB merges the 10 best results of CBS+WordNet, Wam5 and W am8 with the 70 best of Wam7. Finally, mergeC concatenates the 5 best results of CBS+FlickRGroups, the 10 best results of CBS+WordNet and Wam5, the 15 best of Wam8 and 60 best of Wam7. While the settings are quite different between the three merging methods, the results are similar, showing that the results is mainly due to the first answers of Wam7.</p><p>We presented the results to the Image Annotation and Localization subtask and the Text Illustration teaser. The results to IAL are good in comparison to other participants, but the contribution proposed in 2016 did not lead to significantly better results than our 2015 system. It is partially due to the fairer evaluation of the task, but since the exact method of evaluation is not released, it is hard to fully interpret the results, in particular why the combination of runs decreases the mAP. Regarding the Text Illustration teaser, we proposed two methods based on recently published work. The results are globally low, due to the difficulty of the task in general<ref type="foot" coords="11,215.93,235.93,3.97,6.12" target="#foot_0">1</ref> and the very noisy queries in particular.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="8,177.38,301.05,87.69,7.86;8,357.26,301.05,73.74,7.86"><head></head><label></label><figDesc>(a) robust description (b) pivot principle</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="9,165.95,115.84,283.47,243.23"><head></head><label></label><figDesc></figDesc><graphic coords="9,165.95,115.84,283.47,243.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,134.77,127.36,345.82,76.39"><head>Table 1 :</head><label>1</label><figDesc>Results of individual runs submitted in terms of mAP (√ó100) with 0.5 overlap. We report results including the facepart detection (face) or not (raw ).</figDesc><table coords="4,238.09,151.26,139.18,52.50"><row><cell></cell><cell cols="2">FCN EdgeBoxes</cell></row><row><cell>VGG</cell><cell>raw face 37.66 -</cell><cell>32.16 31.75</cell></row><row><cell>ResNet</cell><cell>raw 37.35 face 37.84</cell><cell>27.18 24.14</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,134.77,229.64,345.82,86.95"><head>Table 2 :</head><label>2</label><figDesc>Results (mAP √ó100) for two individual runs and four concepts concerned by the face part detector.</figDesc><table coords="4,214.79,253.53,185.78,63.06"><row><cell></cell><cell cols="4">VGG + EdgeBoxes ResNet + FCN</cell></row><row><cell></cell><cell>raw</cell><cell>face</cell><cell>raw</cell><cell>face</cell></row><row><cell cols="2">mouth 100</cell><cell>54</cell><cell>2</cell><cell>55</cell></row><row><cell>eye</cell><cell>100</cell><cell>36</cell><cell>33</cell><cell>37</cell></row><row><cell>nose</cell><cell>25</cell><cell>28</cell><cell>7</cell><cell>42</cell></row><row><cell>face</cell><cell>67</cell><cell>75</cell><cell>50</cell><cell>76</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,203.68,234.37,208.00,53.47"><head>Table 3 :</head><label>3</label><figDesc>Results for three combinations of runs.</figDesc><table coords="5,208.66,246.71,194.97,41.14"><row><cell>Methods combined</cell><cell>mAP (0.5 overlap) √ó100</cell></row><row><cell>All VGG-based</cell><cell>32.76</cell></row><row><cell>All ResNet-based</cell><cell>27.11</cell></row><row><cell>All (both above)</cell><cell>21.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,151.70,536.60,311.96,68.51"><head></head><label></label><figDesc>Taakbalk Navigation Subnavigation Content home Who is who organisational chart contact intranet nederlands zoekterm: Navigatie About K.U.Leuven Education Research Admissions Living in Leuven Alumni Libraries Faculties, Departments &amp; Schools International cooperation Virology -home Current Labmembers Former Labmembers Research Projects Publications Contact Us Where To Find Us Courses (...)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,210.90,180.16,193.55,97.71"><head>Table 4 :</head><label>4</label><figDesc>Results of the seven run submitted.</figDesc><table coords="10,241.50,192.50,129.29,85.37"><row><cell>Method</cell><cell>Recall @ 100</cell></row><row><cell>CBS + wordnet</cell><cell>1.44</cell></row><row><cell>CBS + flickrgroup</cell><cell>1.74</cell></row><row><cell>Wam5</cell><cell>2.47</cell></row><row><cell>Wam7</cell><cell>4.33</cell></row><row><cell>MergeA</cell><cell>4.47</cell></row><row><cell>MergeB</cell><cell>4.51</cell></row><row><cell>MergeC</cell><cell>4.50</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="11,144.73,645.84,335.86,7.86;11,144.73,656.80,335.87,7.86"><p>the other participant to the task obtained outstanding results around 80%. If they actually used the same data as us, we'll of course revised the interest of our methods!</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,142.96,290.63,337.64,7.86;11,151.52,301.59,78.84,7.86" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="11,270.06,290.63,163.16,7.86">Natural language processing with Python</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Loper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>O&apos;Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,312.28,337.63,7.86;11,151.52,323.24,329.07,7.86;11,151.52,334.20,187.30,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,220.11,323.24,260.48,7.86;11,151.52,334.20,59.20,7.86">On the role of correlation and abstraction in cross-modal multimedia retrieval</title>
		<author>
			<persName coords=""><forename type="first">Costa</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Coviello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Rasiwasia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,217.36,334.20,30.84,7.86">TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="521" to="535" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,344.90,337.63,7.86;11,151.52,355.86,109.04,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,238.14,344.90,213.29,7.86">Histograms of oriented gradients for human detection</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,151.52,355.86,29.18,7.86">CVPR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,366.56,337.63,7.86;11,151.52,377.52,329.07,7.86;11,151.52,388.47,149.77,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,324.13,366.56,156.46,7.86;11,151.52,377.52,176.75,7.86">Cea list&apos;s participation to the scalable concept image annotation task of imageclef</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Gadeski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Le Borgne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,370.91,377.52,109.67,7.86;11,151.52,388.47,121.10,7.86">CLEF2015 Working Notes. CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,399.17,337.64,7.86;11,151.52,410.13,329.07,7.86;11,151.52,421.09,329.07,7.86;11,151.52,429.78,272.66,10.13" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,287.51,410.13,193.08,7.86;11,151.52,421.09,117.61,7.86">Overview of the ImageCLEF 2016 Scalable Concept Image Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ramisa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dellandrea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,293.35,421.09,187.25,7.86;11,151.52,432.05,110.74,7.86">CLEF2016 Working Notes. CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>√âvora, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08">September 5-8 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,442.75,337.64,7.86;11,151.52,453.71,329.07,7.86;11,151.52,464.66,143.60,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,454.84,442.75,25.75,7.86;11,151.52,453.71,147.34,7.86">Largescale image mining with flickr groups</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">L</forename><surname>Ginsca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Le Borgne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Kanellos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,319.72,453.71,160.87,7.86;11,151.52,464.66,114.94,7.86">21th International Conference on Multimedia Modelling (MMM 15)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,475.36,337.64,7.86;11,151.52,486.32,329.07,7.86;11,151.52,497.28,297.07,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,354.18,475.36,126.41,7.86;11,151.52,486.32,205.73,7.86">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,379.58,486.32,101.01,7.86;11,151.52,497.28,268.40,7.86">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,507.98,337.63,7.86;11,151.52,518.94,329.07,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,320.84,507.98,159.75,7.86;11,151.52,518.94,188.01,7.86">A multi-view embedding space for modeling internet images, tags, and their semantics</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,346.63,518.94,21.62,7.86">IJCV</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="233" />
			<date type="published" when="2014-01">Jan 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,529.63,337.63,7.86;11,151.52,540.59,329.07,7.86;11,151.52,551.55,47.10,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,358.58,529.63,122.00,7.86;11,151.52,540.59,199.39,7.86">Canonical correlation analysis: An overview with application to learning methods</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">R</forename><surname>Hardoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">R</forename><surname>Szedmak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,358.41,540.59,65.15,7.86">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2639" to="2664" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,562.25,337.98,7.86;11,151.52,573.21,248.72,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,293.02,562.25,187.57,7.86;11,151.52,573.21,175.80,7.86">Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,348.02,573.21,23.55,7.86">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,583.91,337.98,7.86;11,151.52,594.87,329.07,7.86;11,151.52,605.82,115.66,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,325.25,583.91,155.34,7.86;11,151.52,594.87,190.00,7.86">Framing image description as a ranking task: Data, models and evaluation metrics</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,349.54,594.87,131.06,7.86;11,151.52,605.82,35.78,7.86">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="page" from="853" to="899" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,616.52,337.98,7.86;11,151.52,627.48,315.57,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,260.79,616.52,219.80,7.86;11,151.52,627.48,172.51,7.86">Learning the relative importance of objects from tagged images for retrieval and cross-modal search</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,331.03,627.48,21.63,7.86">IJCV</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="134" to="153" />
			<date type="published" when="2012-11">Nov 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,119.67,337.98,7.86;12,151.52,130.63,329.07,7.86;12,151.52,141.59,200.19,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,440.89,119.67,39.70,7.86;12,151.52,130.63,186.99,7.86">Aggregating local image descriptors into compact codes</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,345.75,130.63,134.84,7.86;12,151.52,141.59,100.36,7.86">Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1704" to="1716" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,152.05,337.97,7.86;12,151.52,163.01,329.07,7.86;12,151.52,173.97,154.44,7.86" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="12,237.79,163.01,238.19,7.86">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,142.62,184.42,337.98,7.86;12,151.52,195.38,225.22,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,304.35,184.42,176.24,7.86;12,151.52,195.38,69.13,7.86">Automatic image annotation using weakly labelled web data</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Kakar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Y S</forename><surname>Chia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,241.89,195.38,106.17,7.86">CLEF2015 Working Notes</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,205.84,337.97,7.86;12,151.52,216.80,317.47,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,181.35,205.84,256.17,7.86">Multimedia content processing through cross-modal association</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,459.20,205.84,21.39,7.86;12,151.52,216.80,181.46,7.86">Proc. ACM international conference on Multimedia</title>
		<meeting>ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="604" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,227.25,337.98,7.86;12,151.52,238.21,321.52,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="12,309.06,227.25,171.53,7.86;12,151.52,238.21,250.37,7.86">Object bank: A high-level image representation for scene classification &amp; semantic feature sparsification</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,422.74,238.21,21.63,7.86">NIPS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,248.67,337.97,7.86;12,151.52,259.63,329.07,7.86;12,151.52,270.58,196.30,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="12,401.82,248.67,78.77,7.86;12,151.52,259.63,161.25,7.86">Detecting the presence of large buildings in natural images</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Malobabiƒá</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Le Borgne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">E</forename><surname>O'connor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,334.07,259.63,146.53,7.86;12,151.52,270.58,167.63,7.86">Content-Based Multimedia Indexing (CBMI), 2005 International Workshop on</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,281.04,337.97,7.86;12,151.52,292.00,158.60,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="12,410.96,281.04,69.62,7.86;12,151.52,292.00,30.97,7.86">Multimodal deep learning</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,203.51,292.00,26.75,7.86">ICML</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,302.46,337.97,7.86;12,151.52,313.41,329.07,7.86;12,151.52,324.37,306.00,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="12,410.07,302.46,70.52,7.86;12,151.52,313.41,200.44,7.86">Cnn features offthe-shelf: An astounding baseline for recognition</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,375.56,313.41,105.03,7.86;12,151.52,324.37,255.18,7.86">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2014-06">June 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,334.83,337.97,7.86;12,151.52,345.79,190.79,7.86" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="12,278.93,334.83,201.66,7.86;12,151.52,345.79,69.82,7.86">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>CoRR abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,356.24,337.98,7.86;12,151.52,367.20,329.07,7.86;12,151.52,378.16,165.13,7.86" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="12,344.59,356.24,136.01,7.86;12,151.52,367.20,103.21,7.86">Diverse concept-level features for multi-object classification</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tamaazousti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Le Borgne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hudelot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,277.83,367.20,202.77,7.86;12,151.52,378.16,42.04,7.86">International Conference on Multimedia retrieval (ICMR&apos;16)</title>
		<meeting><address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,388.62,337.97,7.86;12,151.52,399.58,329.07,7.86;12,151.52,410.54,234.55,7.86" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="12,342.60,388.62,137.99,7.86;12,151.52,399.58,173.53,7.86">Constrained local enhancement of semantic features by content-based sparsity</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tamaazousti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Le Borgne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,345.88,399.58,134.71,7.86;12,151.52,410.54,111.47,7.86">International Conference on Multimedia retrieval (ICMR&apos;16)</title>
		<meeting><address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,420.99,337.97,7.86;12,151.52,431.95,313.66,7.86" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="12,334.12,420.99,146.47,7.86;12,151.52,431.95,60.66,7.86">Efficient object category recognition using classemes</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Szummer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,233.39,431.95,203.11,7.86">European Conference on Computer Vision. ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,442.41,337.97,7.86;12,151.52,453.37,329.07,7.86;12,151.52,464.33,233.56,7.86" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="12,327.98,442.41,152.61,7.86;12,151.52,453.37,90.13,7.86">Aggregating image and text quantized correlated components</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">Q N</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Le Borgne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Crucianu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,266.19,453.37,214.41,7.86;12,151.52,464.33,81.97,7.86">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Las Vegas, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06">June 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,474.78,337.98,7.86;12,151.52,485.74,329.07,7.86;12,151.52,496.70,273.21,7.86" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="12,395.65,474.78,84.95,7.86;12,151.52,485.74,273.09,7.86">Real-time Multi-view Facial Landmark Detector Learned by the Structured Output SVM</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>U≈ôiƒç√°≈ô</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Franc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sugimoto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Hlav√°ƒç</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,448.47,485.74,32.12,7.86;12,151.52,496.70,239.67,7.86">BWILD &apos;15: Biometrics in the Wild 2015 (IEEE FG 2015 Workshop</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,507.16,337.97,7.86;12,151.52,518.11,329.07,7.86;12,151.52,529.07,329.07,7.86;12,151.52,540.03,329.07,7.86;12,151.52,550.99,198.38,7.86" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="12,449.27,529.07,31.32,7.86;12,151.52,540.03,206.00,7.86">General Overview of ImageCLEF at the CLEF 2016 Labs</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garc√≠a Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schaer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bromuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ramisa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dellandrea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">H</forename><surname>Toselli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Snchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="12,366.20,540.03,114.40,7.86;12,151.52,550.99,27.78,7.86">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,561.45,299.58,7.86" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="12,236.56,561.45,134.25,7.86">Robust real-time object detection</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,391.90,561.45,21.62,7.86">IJCV</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,571.90,337.97,7.86;12,151.52,582.86,329.07,7.86" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="12,363.83,571.90,116.76,7.86;12,151.52,582.86,155.31,7.86">Effective training of convolutional networks using noisy web images</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">D</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">L</forename><surname>Ginsca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Le Borgne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,327.56,582.86,24.96,7.86">CBMI</title>
		<meeting><address><addrLine>prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,593.32,337.98,7.86;12,151.52,604.28,147.75,7.86" xml:id="b29">
	<monogr>
		<title level="m" type="main" coord="12,333.19,593.32,147.41,7.86;12,151.52,604.28,22.38,7.86">Deep image: Scaling up image recognition</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<idno>CoRR abs/1501.02876</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,614.73,337.97,7.86;12,151.52,625.69,277.39,7.86" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="12,256.32,614.73,205.29,7.86">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,151.52,625.69,203.12,7.86">ECCV. European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014-09">September 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,636.15,337.97,7.86;12,151.52,647.11,328.49,7.86" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="12,451.28,636.15,29.31,7.86;12,151.52,647.11,165.34,7.86">Bag-ofmultimedia-words for image classification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Znaidia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Shabou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Le Borgne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hudelot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,337.56,647.11,25.60,7.86">ICPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1509" to="1512" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
