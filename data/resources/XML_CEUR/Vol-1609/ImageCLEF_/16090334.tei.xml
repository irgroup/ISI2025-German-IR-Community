<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,145.96,116.95,323.44,12.62">UDEL CIS at ImageCLEF Medical Task 2016</title>
				<funder ref="#_5ETNvbp">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,135.68,155.53,53.91,8.74"><forename type="first">Pengyuan</forename><surname>Li</surname></persName>
							<email>pengyuan@udel.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Sciences</orgName>
								<orgName type="institution">University of Delaware</orgName>
								<address>
									<settlement>Newark</settlement>
									<region>DE</region>
									<country>US</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,196.85,155.53,62.49,8.74"><forename type="first">Scott</forename><surname>Sorensen</surname></persName>
							<email>sorensen@udel.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Sciences</orgName>
								<orgName type="institution">University of Delaware</orgName>
								<address>
									<settlement>Newark</settlement>
									<region>DE</region>
									<country>US</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,267.22,155.53,88.61,8.74"><forename type="first">Abhishek</forename><surname>Kolagunda</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Sciences</orgName>
								<orgName type="institution">University of Delaware</orgName>
								<address>
									<settlement>Newark</settlement>
									<region>DE</region>
									<country>US</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,364.14,155.53,69.39,8.74"><forename type="first">Xiangying</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Sciences</orgName>
								<orgName type="institution">University of Delaware</orgName>
								<address>
									<settlement>Newark</settlement>
									<region>DE</region>
									<country>US</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,441.20,155.53,38.47,8.74;1,201.66,167.48,22.14,8.74"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Sciences</orgName>
								<orgName type="institution">University of Delaware</orgName>
								<address>
									<settlement>Newark</settlement>
									<region>DE</region>
									<country>US</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,232.65,167.48,103.80,8.74"><forename type="first">Chandra</forename><surname>Kambhamettu</surname></persName>
							<email>chandrak@udel.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Sciences</orgName>
								<orgName type="institution">University of Delaware</orgName>
								<address>
									<settlement>Newark</settlement>
									<region>DE</region>
									<country>US</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,347.12,167.48,61.99,8.74"><forename type="first">Hagit</forename><surname>Shatkay</surname></persName>
							<email>shatkay@udel.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Sciences</orgName>
								<orgName type="institution">University of Delaware</orgName>
								<address>
									<settlement>Newark</settlement>
									<region>DE</region>
									<country>US</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,145.96,116.95,323.44,12.62">UDEL CIS at ImageCLEF Medical Task 2016</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DC49436D93D0CC0E170EA3541BA0A753</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Compound figure detection</term>
					<term>Compound figure separation</term>
					<term>Visual Information</term>
					<term>Biomedical image classification</term>
					<term>Connected Component Analysis</term>
					<term>Neural Networks</term>
					<term>ImageCLEF 2016</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Figures play an important role within biomedical publications. A typical and essential first step toward using images is the detection of compound figures and their separation into panels. In Image-CLEF'16 our team has participated in the compound figure detection and separation tasks, where we utilized a method based on connected component analysis (CCA) to detect and to separate compound figures, while extending CCA in several ways to improve correct detection of subfigures while avoiding over-fragmentation. We have also participated in the Subfigure Classification task, where we employed an array of global image characteristics and a merge-split strategy coupled with neural network classifiers. We describe here the methods used in each task and analyze the performance of our system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Our team has participated in the ImageCLEF <ref type="bibr" coords="1,340.09,491.46,140.50,8.74;1,134.77,503.42,44.69,8.74">'16 Medical Compound Figure Detection,</ref><ref type="bibr" coords="1,182.55,503.42,28.12,8.74">Figure</ref>  <ref type="figure" coords="1,213.75,503.42,46.52,8.74">Separation</ref> and Subfigure Classification tasks <ref type="bibr" coords="1,412.40,503.42,9.96,8.74" target="#b7">[8]</ref>. For the first two, our approach is based on Connected Component Analysis (cca), extending and significantly improving upon our work from ImageCLEF'15 <ref type="bibr" coords="1,425.14,527.33,14.61,8.74" target="#b19">[20]</ref>. For the classification task, we employed primarily global features based on color and gradient statistics along with a merge-split strategy, aiming to improve performance on classes that tend to be confused with one another. In this report we focus primarily on the compound figure detection and separation tasks, and briefly go into the classification task. The details of our approach and discussion of performance are provided in the following sections. In Section 2 we discuss our approach to compound figure separation; Section 3 presents our method for compound figure detection, while Section 4 discusses our approach to subfigure classification; Section 5 summarizes the work.</p><p>These authors contributed equally to the work. About 40% to 50% of figures in biomedical documents are compound figures consisting of multiple panels <ref type="bibr" coords="2,261.74,156.65,9.96,8.74" target="#b5">[6]</ref>. Segmenting compound figures into constituent panels is an essential first step for harvesting information from different panels. Current compound figures separation techniques can be classified into two main methods: The first is based on finding gaps in the figure <ref type="bibr" coords="2,395.91,192.51,15.50,8.74" target="#b12">[13,</ref><ref type="bibr" coords="2,413.06,192.51,7.75,8.74" target="#b0">1,</ref><ref type="bibr" coords="2,422.47,192.51,7.75,8.74" target="#b2">3,</ref><ref type="bibr" coords="2,431.89,192.51,7.75,8.74" target="#b1">2,</ref><ref type="bibr" coords="2,441.29,192.51,7.76,8.74" target="#b3">4,</ref><ref type="bibr" coords="2,450.70,192.51,12.73,8.74" target="#b20">21,</ref><ref type="bibr" coords="2,465.09,192.51,11.62,8.74" target="#b17">18]</ref>. Gaps are identified by finding sharp peaks in axis projections along the figure. As this can lead to both over-and under-fragmentation, figure caption analysis and separation analysis has been used as an aid for determining the correct number of panels (fragments).</p><p>In contrast, rather than look for separators we employ Connected Component Analysis (cca) <ref type="bibr" coords="2,203.12,264.40,14.61,8.74" target="#b15">[16]</ref>, which aims to directly identify large well-connected regions within the figure. However, small connected objects that are not connected to the rest of the image (such as legends or labels) may also lead to over-fragmentation. To address this issue, we "adopt" small fragments in a post-processing step.</p><p>The above methods do not address stitched compound figures, that is, figures in which panels directly touch one another and are not separated by gaps. Santosh, et al <ref type="bibr" coords="2,196.34,336.29,15.50,8.74" target="#b14">[15]</ref> first proposed a method to separate stitched compound figure based on Line Segment Detector. However, their method requires first manually separating stitched compound figures from the rest of the dataset, as opposed to a fully automated classification process. In this paper, we propose a figure separation scheme based on connected component analysis. To avoid overfragmentation, we develop a separation quality assessment step. Furthermore, we employ the susan edge detector <ref type="bibr" coords="2,278.48,408.02,15.50,8.74" target="#b16">[17]</ref> to separate stitched compound figures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Methods</head><p>Compound figures consist of several panels, typically separated by gaps, which appear as vertical or horizontal light/dark bands; however, such gaps may be blurry or too thin to recognize. We first preprocess compound figures by resizing, adjusting, and cropping them to make the gaps in the images clearer and broader. We then apply Connected Component Analysis (cca) to separate compound figures into constituents panels. As part of this step, (to which we refer throughout the rest of this report as cca), we set thresholds to eliminate small objects and keep only the main components as individual panels, as we have done before <ref type="bibr" coords="2,188.48,555.06,14.61,8.74" target="#b19">[20]</ref>.</p><p>Notably, this process may not be effective in several cases, namely: individual panels whose contents may not be well-connected, very blurry panels, and stitched compound figures. For the first two, which often occur in displayed graphics, we apply susan edge detection 1 on the original compound figure, thus sharpening the blurry components and keeping the connectivity within the panels. For stitched compound figures, we apply susan edge detection to find the positions where adjacent pixels are sharply changed. The boundary can then be detected by finding the peak value of the sum projections that are calculated by summing the pixel values along the horizontal and the vertical directions. Some compound figures are separated by partial gaps, where boundaries do not cross the entire image; we separate such figures by finding the projection with the highest peak value along only one direction. The projections are then calculated for the separated sub-figures, and the separation is recursively repeated until the highest peak of projections falls below a threshold.</p><p>As a last step, we employ a separation quality assessment step that we have developed to prevent over-and under-segmentation. It consists of five steps: 1) Merge overlapping panels; 2) Temporarily eliminate small disconnected components; 3) Recover missing panels; 4) Separation of potentially overlapping regions; 5) Small component recovery. The complete framework is shown in Fig. <ref type="figure" coords="3,134.77,275.65,3.87,8.74" target="#fig_0">1</ref>, and is further described below. Image Preprocessing Gaps in compound figures typically separate sub-panels into clear individual components. However, some panels may be positioned too close to one another, or a thin gap may be noisy or blurred, making separation hard. To address this issue, we first scale-up the original image I, of size m×n, to 2m×2n, using nearest neighbor interpolation, which broadens the separating gaps. Notably, gaps in compound figures are not always white or black, that is, the intensity of pixels in the gaps is non-binary. To make the gap clearer, we adjust the intensity of figures, remapping pixel intensity that is below a lower threshold T low or above an upper threshold T high in the resized image I resized to 0 and 1 values, respectively. This enhances contrast in the image so that gaps, which are the lightest or the darkest bands in the figure, will be clearer. In our experiments, we set T low to 0.05 and T high to 0.95.</p><p>Connected Component Analysis Compound figure segmentation implies identifying panels that can be separated by boundaries and contents. We assume that the gaps among panels are white (which can be reversed later by inverting pixel values). We thus generate a mask of image foreground M by setting a threshold t and mapping all grayscale pixels whose value is below t to 0 and all those whose value exceeds t to 1. In our experiments the threshold t is set to 0.95. We then detect the connected components in M by using the Connected Component Labeling method <ref type="bibr" coords="4,264.95,167.81,9.96,8.74" target="#b8">[9]</ref>. This method is based on labeling gray values of similar intensity of adjacent pixels using the same label. A connected component is a set of pixels in which each pair of adjacent pixels share the same label. In the labeling process we employ here, we use 4-way adjacency to calculate connectivity. A panel bounding box is then set around the smallest rectangle that contains all pixels in each connected component. To detect panels in images with black gaps we reverse the gray value of every pixel in the image.</p><p>Using connected component analysis may generate some small boundingboxes due to small and unconnected objects in the image, such as text. We thus set two thresholds to initially eliminate bounding boxes of very small height or width: t height = height/20, t width = width/20, where width and height are the total figure width and height.</p><p>Connected Component Analysis on SUSAN edge image If the figure cannot be separated through the step above, we employ a classifier that labels it as either a stitched compound figure or a compound figure with gaps. We define here a gap as a row or a column whose minimum gray value is higher than 0.95. If a gap is found in the figure, the latter is labeled as a compound figure with gaps. Otherwise, it is labeled as stitched.</p><p>Two kinds of images with gaps cannot be directly separated by the cca method: images that have very blurry components and images that have components with very low internal connectivity. To address the first issue, we apply susan edge detector to the preprocessed image, which sharpens blurry components in the image. Still, the corresponding susan edge image, denoted I susan , may have poor connectivity. To enhance connectivity of components in I susan , we dilate the connected regions within the susan edge image using the minimum gap-width in the image as the dilation factor. After dilating, the connectivity of connected-regions within the dilated susan edge image is increased. We then apply the cca method to the dilated image.</p><p>Projection on SUSAN edge image For stitched compound figures separation, identifying panel boundaries is the main challenge. To detect these boundaries we propose a separation method based on the application of susan edge detector to the figure. The edge detector is first applied to the preprocessed image, which highlights the boundary between panels. Thus the objective becomes that of detecting boundaries shown in the resulting susan-edge image. Given a susan-edge image I susan , if I(x, y) is a detected edge we have I susan (x, y) = 1.</p><p>Therefore, summing the pixel value along the horizontal and the vertical directions gives rise to two projections: P roj0 • and P roj90 • . The panel separation is done along the horizontal or the vertical line that goes through the highest projection position. For figures with complex layout, the boundary between panels may not cross the whole figure; in this case we recursively separate the figure along one direction at a time, where the projection peak value is at least 0.7 of the height or the width of the region currently considered for separation.</p><p>Separation Quality Assessment a. Merge Overlapping Panels: Because cca is based on the connectivity of components in the image, some bounding boxes may overlap. For example, the bounding box of legends may overlap with the bounding box of corresponding line graphs. Therefore, we choose to merge two bounding boxes when their overlap ratio is larger than 0.1. b. Temporarily Eliminate Small Disconnected Components: In the cca separation we try to eliminate noise and text by setting a threshold for the size of bounding boxes. In this step we eliminate the subfigures that are too small when compared with the biggest bounding box from the separation result. When the height or width of a subfigure is less than the 1/5 of maximum height or width of all subfigures, this subfigure will be removed. c. Recover Missing Panels: Due to blurriness in some of the compound figures, some panels will be recognized while others may be omitted/missed in our separation process. Therefore, we propose a missing subfigure recovery method. We assume that the missing panel is similar in size and symmetric in position to present panels. We thus check for each present panel whether there is enough space for another bounding box to its left, right, above or below. The candidate panels are expected to have similar black area ratio, and the boundary of candidate panels are verified by checking if all pixels in the boundary have a variance less than 5. d. Separation of Overlapping Regions: We calculate the dark area overlap. If the overlap-rate is less than 0.5 we consider the separation to be incorrect. e. Small Component Recovery: While we have eliminated text and some relatively small regions from our separation result, there may be some essential parts to be eliminated. We will merge those bounding boxes to their closest qualified bounding box. To do this we employ several rules.</p><p>1. If merging changes both height and width of a qualified bounding box -do not merge. 2. If merging changes more than 20% of height or width of a qualified bounding box -do not merge. 3. If the change of height or width for a qualified bounding box is more than 20%, this qualified bounding box keeps its original size. 4. Each small bounding box can only be merged once.</p><p>Last, we experiment with an automatic cropping scheme that simply subtracts the position of the top-left corner of the top-left bounding box, from the coordinates of all bounding boxes. This ensures that all top-left panels are anchored at position [0, 0], which reflects the bounding boxes position in the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Runs</head><p>Here we describe each of our submitted runs for the Figure <ref type="figure" coords="6,396.20,142.86,46.52,8.74">Separation</ref>  -FS.run7: The framework proposed in Fig. <ref type="figure" coords="6,336.70,312.14,4.98,8.74" target="#fig_0">1</ref> without adopting small objects (small component recovery).</p><p>-FS.run8: The framework proposed in Fig. <ref type="figure" coords="6,335.36,336.63,4.98,8.74" target="#fig_0">1</ref> with small component recovery.</p><p>-FS.run9: The results from FS.run8 with automatic cropping employed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Results and Discussion</head><p>Results from all our runs are shown in Table <ref type="table" coords="6,341.30,573.43,3.87,8.74" target="#tab_1">1</ref>. The simple Connected Component Analysis approach (originally used in <ref type="bibr" coords="6,335.18,585.38,15.50,8.74" target="#b19">[20]</ref>) achieves a relatively low accuracy of 73.57%. Preprocessing slightly improved the accuracy, at the cost of fewer images being successfully separated. Adding separation quality assessment further improves accuracy. Combining all our methods improves results, where the highest accuracy is achieved using the complete framework, along with automatic cropping, with a total accuracy of 84.43%. As we were the only team participating in this task, these results are not comparable to other groups.</p><p>This task was first introduced in 2015 <ref type="bibr" coords="7,307.99,143.01,9.96,8.74" target="#b6">[7]</ref>, aiming to identify whether a figure is compound or not. We use a text-based method applied to the figure-caption, image-based methods, and combination methods for compound figure detection. We first introduce the methods, followed by a description of the 10 runs submitted to ImageCLEF 2016 Compound Figure Detection task. We then show and discuss the results from each run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Methods</head><p>Text-based compound figure detection Captions in compound figures often contain specific delimiters, separating the description of each panel. That is, figure caption that contain at least two of these delimiters are often captions of compound figures. Therefore, we employ a text-based compound figure detection method by extracting these delimiters from the captions. To select the delimiters, we manually analyzed captions of figures from the training set, and identified a list of delimiters that frequently occur in compound figure captions. These delimiters are used to detect compound figures, that is, a figure is classified as compound if the corresponding caption has more than two of these delimiters.</p><p>A list demonstrating the typical delimiters used is shown in Table <ref type="table" coords="7,426.01,358.42,3.87,8.74">2</ref>. -First, the bounding box with maximum area divided by the area of the image (Maximum Bounding box Area f 1 ). -Second, the sum of the number of solid columns and rows in the image (Number of Separating Lines f 2 ). -Last, we form two binary-vectors: a row-based vector where the value 1 is placed at each position in which a solid column starts (0 in all other positions), and a column-based vector where the value 1 is placed at each position in which a solid row starts (0 in all other positions). We multiply each of these vectors by a Gaussian whose standard deviation is proportional to the image size, thus giving more weight to solid rows and columns that are at the center of the figure (as opposed to the margins). We sum all elements of the weighted vectors and take the result as a third feature, (which we call the solid axis matrix, denoted f 3 ).</p><p>Combined methods As both caption and image provide important information for deciding whether a figure is compound, we use several combinationmethods based on features extracted from both captions and images. One combined method is a simple logical 'OR' (union) of the two predictions. In this case, if either the text-based detection method or the image-based method labels a figure as compound, the figure is assigned the compound label.</p><p>The second combined method employs a Decision Tree classifier on the textbased and image-based prediction results. Each figure is represented as a twodimensional binary vector r 1 , r 2 , in which r 1 is the label assigned by the text-based method (where 0 denotes non-compound and 1 compound). Similarly, r 2 is the label assigned by the image-based method. All figures in the training set are used by the J48 algorithm on Weka <ref type="bibr" coords="8,325.19,330.44,15.50,8.74" target="#b9">[10]</ref> to build a decision tree model.</p><p>The third combined method employs a Decision Tree classifier on the textbased prediction results, image-based prediction results along with the three additional image features listed before. Thus, each figure is represented as a 5dimensional vector r 1 , r 2 , f 1 , f 2 , f 3 . r 1 and r 2 are as in the above decision tree, while f 1 , f 2 and f 3 are the three additional features described earlier. All figures in the training set are used by the J48 algorithm (Weka implemntation) to build a decision tree model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Runs</head><p>We have submitted 10 runs in total for the compound figure detection subtask, as follows:</p><p>-FD.run01: Uses only the text-based detection method.</p><p>-FD.run02: Uses the figure separation result from FS.run4 (see Sec. -FD.run08: Uses the logical OR relationship (union) over the results from FD.run1 and FD.run3. -FD.run09: Represents each figure in the test-set as a five-dimensional vector based on the results of FD.run1, FD.run2 and the three additional features, then employs the Decision Tree classifier built over the training set. -FD.run10: Represents each figure in the test-set as a five-dimensional vector based on the results of FD.run1, FD.run3 and the three additional features, then employs the Decision Tree classifier built over the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results and Discussion</head><p>Our results are given in Table <ref type="table" coords="9,272.01,252.14,3.87,8.74" target="#tab_2">3</ref>. We achieve an accuracy of 90.74% using our combined textual and visual approach that represents images as 5 dimensional vectors using feature extraction, showing that the combined method improves upon the purely visual methods. This result is among the very top achieved on this task this year. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Subfigure classification</head><p>We have used two techniques to address subfigure classification.</p><p>-Training a hierarchical classifier using merge-split approach.</p><p>-Fine tuning a pre-trained deep-network.</p><p>As our deep learning approach has not proven effective in this setting, we will not discuss it further in the interest of focusing on the hierarchical approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Methods</head><p>The complexity of a classifier is typically proportional to the number of classes in the data and the amount of training data required is proportional to the complexity of the classifier. Due to limited training data available, and classifier limitations, some of the classes are not easily distinguished from one another. To combat this we have augmented the ImageCLEF training dataset with images collected from the web. Furthermore we have developed an approach to target specific cases of easily mis-classified images. Given a classifier and training data we use a merge-split approach, in which classes that tend to be easily confused are merged into a single class, and the classifier is re-trained accordingly. Another classifier is then trained to classify (split) each merged class into its constituents. Details of the image features and the classifier used are discussed in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Features</head><p>The global image feature used is a combination of 4 types of feature-vectors that capture color distribution, contrast, gradient orientations and distribution of local patterns in an image, as follows:</p><p>-The first type of feature vectors consist of information regarding the mean and co-variance of the color histogram. We compute histograms with 10 bins for each color channel and compute the mean and co-variance across these histograms. This results in a 10 and a 9-dimensional vector, respectively, both of which are normalized. -The second set of components is based on a Histogram of Oriented Gradients <ref type="bibr" coords="10,151.70,344.58,10.52,8.74" target="#b4">[5]</ref> computed over the entire image. This forms a 31-dimensional vector. -The third type of feature-vector consists of properties derived from gray level co-occurrence matrix (GLCM) <ref type="bibr" coords="10,315.00,367.98,15.50,8.74" target="#b10">[11]</ref> with 8 gray levels on the image. The properties include contrast, correlation, energy and homogeneity. This forms a 4-dimensional vector. -The last set of features is the histogram of local binary pattern (LBP) <ref type="bibr" coords="10,465.10,403.32,15.50,8.74" target="#b13">[14]</ref> computed over the entire image, giving rise to a 58-dimensional vector.</p><p>The VLFeat library was used for feature extraction <ref type="bibr" coords="10,365.35,433.14,14.61,8.74" target="#b18">[19]</ref>. Each of these feature vectors is normalized and they are all concatenated to form a global feature representation for the given image. Before feature extraction, the images are preprocessed to remove bright and dark borders with constant intensity as they do not contribute to image information content. The cropped images are then re-sized to be of size 64×64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classifier and Training</head><p>We use a neural network multi-class classifier <ref type="bibr" coords="10,462.32,520.74,14.61,8.74" target="#b11">[12]</ref>. The network has a hidden layer with 10 neurons, the input layer has 112 units (same as the size of the feature vector). Stochastic gradient descent is used for training the network. The target error is set to 0.01 and the maximum iterations number to 1000. Training a classifier using the merge-split approach involves the following steps:</p><p>-Train the neural network over the given training data.</p><p>-Use the confusion matrix to merge classes that are easily confused: For each class A if more than 10% of its instances are classified as class B, then A and B are merged. Note that more than 2 classes can be merged into a combined class. Also note that there may be classes that are not merged or merged classes consisting of 2 or more individual classes.</p><p>-Train a base-classifier to distinguish between the new set of classes, obtained by the merging process above. -Train a subordinate classifier to split each of the merged-classes, that is, classify their instances into the original constituent classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results and Discussion</head><p>The merge-split training approach was used to train a set classifiers. 10 different classifiers were trained using random initialization and the one that did best on the training data was used on the ImageCLEF test data. We used two approaches for doing this:</p><p>-Sequential: the training steps described in Section 4.1 were used sequentially to train 10 different classifiers and the best among them was picked; -Parallel: the merge step was repeated 10 times and the best classifier was chosen before performing the split step 10 times with the selected set of classes. We trained models on both the original training data and our augmented training data. The results on imageCLEF 2016 test data are shown below.</p><p>We submitted 4 runs with classifiers trained using the merge-split method:</p><p>- We also submitted 3 runs with classifiers trained on the augmented dataset using our deep learning approach (Runs 5-7). These are different models that were trained in the same manner, with varying number of iterations. Table <ref type="table" coords="11,475.61,485.55,4.98,8.74" target="#tab_3">4</ref> shows results on ImageCLEF test data. We have presented our approach for addressing the Compound For subfigure classification we developed an approach based on global features extracted from the images. The features include statistics derived from color, gray level, edge orientation and local patterns. Once images were represented through these features, we used a merge-split scheme and trained neural network classifiers. All the techniques presented throughout this paper offer important capabilities to document analysis approaches that seek to extract information from figures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,202.67,437.15,210.02,7.89;3,138.40,307.54,338.58,114.84"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An outline of the figure separation approach</figDesc><graphic coords="3,138.40,307.54,338.58,114.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,141.43,389.47,332.50,7.89;7,208.80,410.27,60.82,7.86;7,342.89,410.27,38.84,7.86;7,186.93,421.62,104.56,7.86;7,307.00,421.62,107.53,7.86;7,185.98,432.98,229.84,7.86;7,195.91,444.34,230.40,7.86;7,199.29,455.70,79.84,7.86;7,307.25,455.70,110.10,7.86;7,134.77,495.25,345.82,8.77;7,134.77,507.24,345.83,8.74;7,134.77,519.19,345.82,8.74;7,134.77,531.15,345.83,8.74;7,134.77,543.10,345.83,8.74;7,134.77,555.06,245.66,8.74"><head>Table 2 .</head><label>2</label><figDesc>Representative delimiters used for text-based compound figure detection Delimiter Kind Examples Arabic numeric delimiters 1, 2, 3, 4, or 1), 2), 3), 4), Roman numeric delimiters I, II, III, IV, or i, ii, iii, iv, Alphabetic delimiters a, b, c, d, or (A), (B), (C), (D), Direction delimiters top, bottom, middle, upper Image-based compound figure detection The image-based compound figure detection method we employ uses the result from the figure separation method (see Section 2) to classify the figures as compound vs. non-compound. If a figure can be separated into multiple panels, it is labeled compound, otherwise, it is labeled non-compound. In addition to the separation-based decision, we extract three features for compound figure detection:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,445.63,512.77,19.37,8.74;8,140.99,524.87,324.01,8.77;8,140.99,537.00,339.60,8.77;8,151.70,548.99,328.89,8.74;8,151.70,560.94,328.89,8.74;8,151.70,572.90,91.15,8.74;8,140.99,585.00,339.60,8.77;8,151.70,596.99,328.89,8.74;8,151.70,608.94,177.39,8.74;8,140.99,621.04,349.13,8.77;8,140.99,633.17,339.60,8.77;8,151.70,645.16,328.89,8.74;8,151.70,657.11,177.39,8.74"><head></head><label></label><figDesc>2.2). -FD.run03: Uses the figure separation result from FS.run7 (see Sec. 2.2). -FD.run04: Represents each figure in the test-set as a 5-dimensional vector based on the results of FD.run2, and the three features mentioned in Sec. 3.1 (Image-Based detection), then employs the Decision Tree classifier built over the training set. -FD.run05: Represents each figure in the test-set as a two-dimensional vector based on the results of FD.run1 and FD.run2, then employs the Decision Tree classifier built over the training set. -FD.run06: Uses logical OR (Union) over the results from FD.run1 and FD.run2. -FD.run07: Represents each figure in the test-set as a two-dimensional vector based on the results of FD.run1 and FD.run3, then employs the Decision Tree classifier built over the training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="11,151.70,399.06,293.27,8.74;11,140.99,412.12,298.30,8.77;11,140.99,425.20,284.22,8.77;11,140.99,438.29,278.54,8.77"><head>Run 1 :</head><label>1</label><figDesc>Sequentially trained classifier on ImageCLEF training data. -Run 2: Sequentially trained classifier on augmented training data. -Run 3: Parallel trained classifier on ImageCLEF training data. -Run 4: Parallel trained classifier on augmented training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="12,424.09,145.75,56.50,8.74;12,134.77,157.71,345.82,8.74;12,134.77,169.66,345.82,8.74;12,134.77,181.62,345.82,9.30;12,134.77,193.57,345.82,8.74;12,134.77,205.53,345.83,8.74;12,134.77,217.48,345.82,8.74;12,134.77,229.44,345.83,8.74;12,134.77,241.39,345.82,8.74;12,134.77,253.35,345.83,8.74;12,134.77,265.30,345.83,8.74;12,134.77,277.26,345.82,8.74;12,134.77,289.21,145.98,8.74"><head></head><label></label><figDesc>Figure Separation, Compound Figure Detection, and Subfigure Classification tasks at Im-ageCLEF'16 Medical. Our approach for figure separation is based on connected component analysis, and extends it via preprocessing steps, susan edge detection for improved segmentation and for addressing stitched-figure segmentations, and separation quality assessment. For compound figure detection, we have developed and employed text-based, visual and combined methods. The text based method leverages common delimiters typically found in captions of compound figures. The visual separation techniques employ our figure separation approach to identify images that can or cannot be split into panels. We use decision trees to combine the methods. We were the only team to submit an entry to the Figure Separation task, and our results on the Compound Figure Detection task are among the very top this year.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,140.99,142.86,339.60,166.03"><head></head><label></label><figDesc>task: -FS.run1: Connected Component Analysis. -FS.run2: Combination of the image preprocessing and Connected Component Analysis. -FS.run3: Combination of separation quality assessment and Connected Component Analysis. -FS.run4: Combination of image processing, separation quality assessment and Connected Component Analysis. -FS.run5: Combination of image preprocessing, separation quality assessment, Connected Component Analysis and Connected component analysis method over results of susan edge image. -FS.run6: Combination of image preprocessing, separation quality assessment, Connected Component Analysis and projection on susan edge image.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,170.70,391.50,273.95,128.19"><head>Table 1 .</head><label>1</label><figDesc>Compound Figure Separation (FS) results, shown per run</figDesc><table coords="6,180.77,412.79,247.97,106.89"><row><cell>Run</cell><cell cols="3">Run Type Correctly Classified (%) # of unseparated</cell></row><row><cell cols="2">FS.run1 Visual</cell><cell>73.57</cell><cell>162</cell></row><row><cell cols="2">FS.run2 Visual</cell><cell>74.30</cell><cell>146</cell></row><row><cell cols="2">FS.run3 Visual</cell><cell>75.27</cell><cell>202</cell></row><row><cell cols="2">FS.run4 Visual</cell><cell>74.38</cell><cell>243</cell></row><row><cell cols="2">FS.run5 Visual</cell><cell>81.23</cell><cell>85</cell></row><row><cell cols="2">FS.run6 Visual</cell><cell>84.03</cell><cell>13</cell></row><row><cell cols="2">FS.run7 Visual</cell><cell>84.08</cell><cell>9</cell></row><row><cell cols="2">FS.run8 Visual</cell><cell>83.04</cell><cell>9</cell></row><row><cell cols="2">FS.run9 Visual</cell><cell>84.43</cell><cell>9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,171.86,320.34,271.63,142.74"><head>Table 3 .</head><label>3</label><figDesc>Compound Figure Detection (FD) results, shown per run</figDesc><table coords="9,215.27,341.64,181.74,121.44"><row><cell>Run</cell><cell cols="2">Run Type Correctly Classified (%)</cell></row><row><cell cols="2">FD.run01 Textual</cell><cell>85.47</cell></row><row><cell cols="2">FD.run02 Visual</cell><cell>89.29</cell></row><row><cell cols="2">FD.run03 Visual</cell><cell>69.82</cell></row><row><cell cols="2">FD.run04 Visual</cell><cell>89.64</cell></row><row><cell cols="2">FD.run05 Mixed</cell><cell>90.39</cell></row><row><cell cols="2">FD.run06 Mixed</cell><cell>52.25</cell></row><row><cell cols="2">FD.run07 Mixed</cell><cell>85.47</cell></row><row><cell cols="2">FD.run08 Mixed</cell><cell>69.06</cell></row><row><cell cols="2">FD.run09 Mixed</cell><cell>90.39</cell></row><row><cell cols="2">FD.run10 Mixed</cell><cell>90.74</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="11,225.75,531.26,163.86,105.77"><head>Table 4 .</head><label>4</label><figDesc>Subfigure Classification results</figDesc><table coords="11,245.60,552.06,124.15,84.97"><row><cell cols="2">Run Correctly Classified (%)</cell></row><row><cell>Run 1</cell><cell>72.46</cell></row><row><cell>Run 2</cell><cell>71.53</cell></row><row><cell>Run 3</cell><cell>68.17</cell></row><row><cell>Run 4</cell><cell>68.79</cell></row><row><cell>Run 5</cell><cell>15.62</cell></row><row><cell>Run 6</cell><cell>53.16</cell></row><row><cell>Run 7</cell><cell>53.24</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.73,646.84,335.87,8.37;2,144.73,657.79,149.40,7.86"><p>We have experimented with several edge-detection methods and found susan to have the best performance in this context.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>This work was partially supported by <rs type="funder">NIH</rs> grant <rs type="grantNumber">R56LM011354A</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_5ETNvbp">
					<idno type="grant-number">R56LM011354A</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,142.96,491.93,337.64,7.86;12,151.52,502.89,329.07,7.86;12,151.52,513.85,324.03,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,450.50,491.93,30.10,7.86;12,151.52,502.89,324.86,7.86">Exploring use of images in clinical articles for decision support in evidence-based medicine</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">V</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">R</forename><surname>Thoma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,163.04,513.85,247.62,7.86">Proc. of SPIE Document Recognition and Retrieval (DRR&apos;08)</title>
		<meeting>of SPIE Document Recognition and Retrieval (DRR&apos;08)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page">68150Q</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,525.18,337.64,7.86;12,151.52,536.14,329.07,7.86;12,151.52,547.10,329.07,7.86;12,151.52,558.05,149.69,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,151.52,536.14,329.07,7.86;12,151.52,547.10,109.09,7.86">Image retrieval from scientific publications: Text and image content processing to separate multipanel figures</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Apostolova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">R</forename><surname>Thoma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,270.79,547.10,209.80,7.86;12,151.52,558.05,59.98,7.86">Journal of the Association for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="893" to="908" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,569.38,337.64,7.86;12,151.52,580.34,329.07,7.86;12,151.52,591.30,275.92,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,371.79,569.38,108.80,7.86;12,151.52,580.34,278.17,7.86">Automatic segmentation of subfigure image panels for multimodal biomedical document retrieval</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">R</forename><surname>Thoma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,448.83,580.34,31.76,7.86;12,151.52,591.30,212.56,7.86">Proc. of SPIE Document Recognition and Retrieval (DRR&apos;11)</title>
		<meeting>of SPIE Document Recognition and Retrieval (DRR&apos;11)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page">78740Z</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,602.63,337.63,7.86;12,151.52,613.59,329.07,7.86;12,151.52,624.55,270.82,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,201.21,613.59,279.38,7.86;12,151.52,624.55,65.68,7.86">Separating compound figures in journal articles to allow for subfigure classification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chhatkuli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Markonis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Foncubierta-Rodrguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Meriaudeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,236.37,624.55,123.90,7.86">Proc. of SPIE Medical Imaging</title>
		<meeting>of SPIE Medical Imaging</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">86740</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,635.88,337.63,7.86;12,151.52,646.84,329.07,7.86;12,151.52,657.79,57.34,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,249.96,635.88,214.22,7.86">Histograms of oriented gradients for human detection</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,151.52,646.84,324.17,7.86">Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<meeting>of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR&apos;05)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,120.67,337.64,7.86;13,151.52,131.63,329.07,7.86;13,151.52,142.59,173.09,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,151.52,131.63,191.18,7.86">Overview of the ImageCLEF 2013 medical tasks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,361.07,131.63,119.51,7.86;13,151.52,142.59,144.35,7.86">Working Notes of CLEF 2013 (Cross Language Evaluation Forum)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,153.55,337.64,7.86;13,151.52,164.51,329.07,7.86;13,151.52,175.46,102.98,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,344.74,153.55,135.85,7.86;13,151.52,164.51,105.74,7.86">Overview of the ImageCLEF 2015 medical classification task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bromuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,282.08,164.51,198.51,7.86;13,151.52,175.46,74.24,7.86">Working Notes of CLEF 2015 (Cross Language Evaluation Forum)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,186.42,337.63,7.86;13,151.52,197.38,329.07,7.86;13,151.52,208.34,102.98,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,413.72,186.42,66.86,7.86;13,151.52,197.38,121.65,7.86">Overview of the ImageCLEF 2016 medical task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schaer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bromuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,291.61,197.38,188.98,7.86;13,151.52,208.34,74.24,7.86">Working Notes of CLEF 2016 (Cross Language Evaluation Forum)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,219.30,332.13,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="13,291.78,219.30,95.84,7.86">Digital image processing</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">C</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">E</forename><surname>Woods</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Prentice-Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,230.26,337.98,7.86;13,151.52,241.22,329.07,7.86;13,151.52,252.18,73.21,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,464.72,230.26,15.88,7.86;13,151.52,241.22,156.56,7.86">The weka data mining software: an update</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Reutemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,319.32,241.22,157.40,7.86">ACM SIGKDD explorations newsletter</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10" to="18" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,263.14,337.98,7.86;13,151.52,274.09,20.99,7.86" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="13,264.32,263.14,104.66,7.86">Computer and robot vision</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Haralick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Addison-Wesley Pub. Co</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,285.05,337.98,7.86;13,151.52,296.01,150.06,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="13,210.52,285.05,266.20,7.86">A scaled conjugate gradient algorithm for fast supervised learning</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Møller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,151.52,296.01,64.79,7.86">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="525" to="533" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,306.97,337.98,7.86;13,151.52,317.93,329.07,7.86;13,151.52,328.89,329.07,7.86;13,151.52,339.85,105.96,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="13,376.94,306.97,103.65,7.86;13,151.52,317.93,325.08,7.86">Searching online journals for fluorescence microscope images depicting protein subcellular location patterns</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">F</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Velliste</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Porreca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,162.97,328.89,317.62,7.86;13,151.52,339.85,40.80,7.86">Proc. of IEEE International Symposium on Bioinformatics and Bioengineering (BIBE&apos;01)</title>
		<meeting>of IEEE International Symposium on Bioinformatics and Bioengineering (BIBE&apos;01)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="119" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,350.81,337.98,7.86;13,151.52,361.77,329.07,7.86;13,151.52,372.73,222.61,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,323.61,350.81,156.98,7.86;13,151.52,361.77,226.25,7.86">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pietikinen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Maenpaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,385.33,361.77,95.26,7.86;13,151.52,372.73,133.46,7.86">IEEE Trans. on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,383.68,337.98,7.86;13,151.52,394.64,329.07,7.86;13,151.52,405.60,197.68,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="13,354.73,383.68,125.86,7.86;13,151.52,394.64,66.36,7.86">Stitched multipanel biomedical figure separation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">C</forename><surname>Santosh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">K</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">R</forename><surname>Thoma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,240.07,394.64,240.53,7.86;13,151.52,405.60,139.82,7.86">Proc. of the IEEE International Symposium on Computer-Based Medical Systems (CBMS&apos;15)</title>
		<meeting>of the IEEE International Symposium on Computer-Based Medical Systems (CBMS&apos;15)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="54" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,416.56,337.98,7.86;13,151.52,427.52,329.07,7.86;13,151.52,438.48,231.29,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="13,319.45,416.56,161.15,7.86;13,151.52,427.52,73.77,7.86">Integrating image data into biomedical text categorization</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Shatkay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Blostein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,243.99,427.52,236.60,7.86;13,151.52,438.48,74.02,7.86">Proc. of the Int. Conf. on Intelligent Systems for Molecular Biology (ISMB&apos;06)</title>
		<meeting>of the Int. Conf. on Intelligent Systems for Molecular Biology (ISMB&apos;06)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="446" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,449.44,337.97,7.86;13,151.52,460.40,282.89,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="13,279.65,449.44,200.94,7.86;13,151.52,460.40,26.52,7.86">SUSAN-a new approach to low level image processing</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Brady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,185.92,460.40,168.11,7.86">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="78" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,471.36,337.98,7.86;13,151.52,482.31,329.07,7.86;13,151.52,493.27,109.51,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="13,280.57,471.36,200.03,7.86;13,151.52,482.31,97.86,7.86">Compound figure separation combining edge and band separator detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Taschwer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Marques</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,267.33,482.31,213.26,7.86;13,151.52,493.27,42.08,7.86">Proc. of International Conf. on Multimedia Modelling (MMM&apos;16)</title>
		<meeting>of International Conf. on Multimedia Modelling (MMM&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="162" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,504.23,337.97,7.86;13,151.52,515.19,329.07,7.86;13,151.52,526.15,20.99,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="13,274.70,504.23,205.89,7.86;13,151.52,515.19,66.76,7.86">VLFeat: an open and portable library of computer vision algorithms</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Fulkerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,236.20,515.19,194.35,7.86">Proc. of ACM Int. Conf. on Multimedia (MM&apos;10)</title>
		<meeting>of ACM Int. Conf. on Multimedia (MM&apos;10)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1469" to="1472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,537.11,337.98,7.86;13,151.52,548.07,329.07,7.86;13,151.52,559.03,259.82,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="13,436.61,537.11,43.99,7.86;13,151.52,548.07,271.70,7.86">CIS UDEL working notes on imageclef 2015: Compound figure detection task</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kolagunda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Shatkay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Kambhamettu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,446.35,548.07,34.25,7.86;13,151.52,559.03,231.08,7.86">Working Notes of CLEF 2015 (Cross Language Evaluation Forum)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,569.99,337.98,7.86;13,151.52,580.94,329.07,7.86;13,151.52,591.90,133.76,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="13,246.28,569.99,234.32,7.86;13,151.52,580.94,142.35,7.86">A novel figure panel classification and extraction method for document image understanding</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,305.66,580.94,174.93,7.86;13,151.52,591.90,57.94,7.86">International Journal of Data Mining and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="36" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
