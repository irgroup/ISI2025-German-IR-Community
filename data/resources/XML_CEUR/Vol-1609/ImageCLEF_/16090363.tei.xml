<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,161.52,115.96,292.32,12.62;1,192.64,133.89,230.07,12.62">MRIM-LIG at ImageCLEF 2016 Scalable Concept Image Annotation Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,150.03,171.56,66.85,8.74"><forename type="first">Maxime</forename><surname>Portaz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ. Grenoble Alpes</orgName>
								<orgName type="institution" key="instit2">LIG</orgName>
								<address>
									<postCode>F-38000</postCode>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">LIG</orgName>
								<address>
									<postCode>F-38000</postCode>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,240.23,171.56,71.32,8.74"><forename type="first">Mateusz</forename><surname>Budnik</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ. Grenoble Alpes</orgName>
								<orgName type="institution" key="instit2">LIG</orgName>
								<address>
									<postCode>F-38000</postCode>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">LIG</orgName>
								<address>
									<postCode>F-38000</postCode>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,334.89,171.56,75.41,8.74"><forename type="first">Philippe</forename><surname>Mulhem</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ. Grenoble Alpes</orgName>
								<orgName type="institution" key="instit2">LIG</orgName>
								<address>
									<postCode>F-38000</postCode>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">LIG</orgName>
								<address>
									<postCode>F-38000</postCode>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,433.64,171.56,31.69,8.74;1,279.61,183.51,38.88,8.74"><forename type="first">Johann</forename><surname>Poignant</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ. Grenoble Alpes</orgName>
								<orgName type="institution" key="instit2">LIG</orgName>
								<address>
									<postCode>F-38000</postCode>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">LIG</orgName>
								<address>
									<postCode>F-38000</postCode>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,161.52,115.96,292.32,12.62;1,192.64,133.89,230.07,12.62">MRIM-LIG at ImageCLEF 2016 Scalable Concept Image Annotation Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EBCD6578DD78F83AF5AFD3721D8F9486</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Convolutional Neural Networks</term>
					<term>Landmark face detection</term>
					<term>ImageNet</term>
					<term>TRECVID</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of the the MRIM research Group of the LIG laboratory in the ImageCLEF scalable concept image annotation subtask 1. We made use of a classical framework to annotate the 500K images of this task: we tuned an existing Convolutional Neural Network model to learn the 251 concepts and to locate bounding boxes of such concepts, and we applied a specific process to handle faces and face parts. Because of time constraints, we fully processed 35% of the full corpus (i.e. 180K images), and partially the remaining images of the corpus. For our first participation to this task, the results obtained show that we have to manage the localization in a more effective way.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The first participation of the MRIM group from the LIG laboratory at the Im-ageCLEF 2016 <ref type="bibr" coords="1,202.23,452.88,10.52,8.74" target="#b6">[7]</ref> scalable concept image annotation subtask 1 <ref type="bibr" coords="1,412.85,452.88,10.52,8.74" target="#b2">[3]</ref> is presented. Our approach was to use a classical framework based on face detection <ref type="bibr" coords="1,452.23,464.84,10.52,8.74" target="#b7">[8]</ref> followed by facial landmarks detection <ref type="bibr" coords="1,295.04,476.79,10.52,8.74" target="#b5">[6]</ref> for faces and face parts (eyes, nose and mouth), and to rely on convolutional neural networks <ref type="bibr" coords="1,380.21,488.75,10.52,8.74" target="#b3">[4]</ref> for each of the 251 concepts.</p><p>The ImageCLEF 2016 scalable concept image annotation subtask 1 consists of finding the location of 251 classes of objects in a corpus of 500K images. This task is challenging because of the difficulty of finding accurate location of objects in large sets of images. The objective is to assign a maximum of 100 bounding boxes per image, each bounding box being associated to one or more of the 251 concepts proposed. It is also possible to provide a confidence value for each of the tagging defined. The visual concepts defined for this subtasks do not match fully with concepts coming from the well known ImageNet database <ref type="bibr" coords="1,418.83,596.34,9.96,8.74" target="#b0">[1]</ref>, so specific work has to be done to be able to tackle these concepts.</p><p>Because of time needed to process the whole corpus, we fully processed around 35% of the full image corpus (i.e. 180K images), and partially the remaining of the corpus. The results obtained are then negatively impacted by this partial processing.</p><p>The rest of this paper is organized as follows. In section 2, we define our approach: we mainly rely on convolutional neural networks for "classical" concepts, with a specific process dedicated to faces. Then, in section 3, we detail the results obtained, as well as some additional elements dedicated to analyzing our results in more detail. We conclude in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview</head><p>The overall process applied for detection and localization of concepts in images is described in figure <ref type="figure" coords="2,251.58,256.62,3.87,8.74" target="#fig_0">1</ref>. We generate possible bounding boxes, then apply Convolutional Neural Networks for each of the 251 concepts. For face and face part detection, we use face and facial landmarks detection. Such approaches have been successfully used by several participants during the 2015 campaign of ImageCLEF concept annotation task. We finally rank all the labeled bounding boxes by score or by size, depending on the run. This ranking is used as filtering to reduce the number of boxes per image, as we take only up to 100 boxes for each image (a limit chosen by the organizers).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Convolutional Neural Networks</head><p>We used a Deep Residual Convolutional Neural Network (ResNet) with 152 layers presented by Microsoft in the ImageNet'16 challenge <ref type="bibr" coords="2,406.20,403.50,9.96,8.74" target="#b3">[4]</ref>. The network was finetuned to match the 251 labels from ImageClef. Only the final layer was retrained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Processed</head><p>A first step in the learning process was to map, when possible, the 251 CLEF concepts into concepts from existing image collections, namely the ImageNet concepts. From the full set C of 251 concepts, 224 are mapped directly to Im-ageNet concepts, and for each of the 27 remaining concepts we acquired 4519 images from Bing API using the concept name as query. We do not filter manually the resulting set of images.</p><p>As described in figure <ref type="figure" coords="2,246.36,541.83,3.87,8.74" target="#fig_1">2</ref>, we also define a second set of images to increase the quality of the concept detection. This second set also includes both Bing API and the validation set (2000 images, 10000 tagged bounding boxes) provided by the organizers of the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN Processing</head><p>One specificity of our proposal is to define a two-step learning process (basically two finetuning stages) as a way to increase the effectiveness of the concept detection. The CNN network comes pre-trained on the ImageNet dataset <ref type="bibr" coords="2,450.76,644.16,9.96,8.74" target="#b0">[1]</ref>. We used two validation sets: a) the first one is the set provided by the organizers of the ImageCLEF task, and b) a second one that we defined to assess the quality of the training on "clean" images. The first finetuning step is evaluated on these two validation sets. While during the second learning step the first set (a) is used for training as well as some additional images (which were crawled from the Internet) for the concepts with the lowest recognition rate. After the second finetuning, the system is tested only on the (b) validation set. In other words:</p><p>-On our first set of training images, learn the last layer of CNN, then evaluate (success@1 success@5) on the two validation sets; -During the second learning stage, for the low quality recognition concepts, we generate the second set of 200 additional training images per concept. As described above, we also add the validation set (a) provided by CLEF. We retrain the network on this combined and extended set. At the end of these two steps, we obtained the results presented in table <ref type="table" coords="4,472.85,383.70,3.87,8.74" target="#tab_0">1</ref>. The two first rows of this table present the results after the first tuning step. The remaining two rows give the results after the second phase of finetuning. The second step seems to significantly increase the performance on the Bing validation set.</p><p>The ImageCLEF validation set was included in the training set at the second stage of tuning. That is why a surprisingly strong result (denoted with "*"), compared to the first tuning, is obtained: it does not generalize and was included just for illustrative purposes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concept Localization</head><p>We used the work of Uijlings, van de Sande, Gevers and Smeulders <ref type="bibr" coords="4,422.29,644.16,10.52,8.74" target="#b4">[5]</ref> to perform selective search to define bounding box detection. The idea is mainly to define a priori a set of bounding boxes that are expected to contain one visual concept. The selective search use Felzenswalb algorithm <ref type="bibr" coords="5,346.97,130.95,10.52,8.74" target="#b1">[2]</ref> for image segmentation. In our runs, we use a width for Gaussian kernel of 0.8, and a scale factor of 500. The minimum size for a box is set to 200 pixels. These constant give a average of 517 boxes per image. Each of these boxes will be used as an input image on which the CNN will be applied to detect objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Actual Processing Achieved</head><p>Due to time constraints, we applied the full process to 180k images: selective search and clustering of bounding boxes, and CNN detection on each of the selected boxes. On average, the number of boxes generated per image is 517. For each of the remaining images (320K images), we applied detection on: a) the full image, and b) a small subset of the initial boxes selected randomly. On average, the number of boxes generated per image for each remaining image is 8. Overall, we processed 95 millions of boxes for our submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Face Detection</head><p>The detection and localization of parts of faces is achieved through a two step process:</p><p>-Frontal faces are detected using the "classical" Viola and Jones approach <ref type="bibr" coords="5,470.07,361.50,10.52,8.74" target="#b7">[8]</ref> based on cascade of simple Haar-like features; -Then 8 facial landmarks <ref type="bibr" coords="5,260.10,384.68,10.52,8.74" target="#b5">[6]</ref> are detected on these faces. They correspond to the 2 mouth corners, 4 eye canthus, the tip of the nose and the center of the face. We used then simple heuristics to define faces, eyes, noses and mouths bounding boxes based on these landmarks.</p><p>All images of the ImageCLEF corpus are processed using the above steps. With such process, at least one faces is detected on 64642 of the 510K images (12.7% of the whole corpus). A total of 91102 faces "boxes" are detected on these images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation Results</head><p>The runs submitted by the MRIM-LIG team are the following:</p><p>-RUN1 LIG DLo: Annotation using the Convolutional Neural Network described in part 2.2, with a ranking of the bounding boxes according to the confidence value; -RUN2 LIG DLo: Annotation using the CNN described in part 2.2, with a ranking of the bounding boxes according to the surfaces of the boxes; -RUN3 LIG Fo: Annotation of the face parts only, using the Viola/Jones approach described in part 2.3; -RUN4 LIG DLF: Annotation using both the CNN and face parts detection, with a ranking of the bounding boxes according to the confidence value; -RUN5 LIG DLF: Annotation using both the CNN and face parts detection, with a ranking of the bounding boxes according to the surfaces of the boxes;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Official Results</head><p>The official MAP at 0% overlap and MAP at 50% overlap results of our runs are presented in table <ref type="table" coords="6,216.04,150.83,3.87,8.74" target="#tab_1">2</ref>. We find that the run RUN5 (that fuses the face parts and deep learning results, ranking based on surfaces) achieves our best result (rank 11 for overlap 0, and rank 9 for overlap 0.5). At overlap 0.5, our second best result is RUN4 (that fuses the face parts and deep learning results, ranking based on confidence values). The difference between RUN5 and RUN4 are negligible. We suppose that comes from the fact that only 180K images where fully processed, and for the remaining ones we did not have more than 100 boxes, the ranking only plays a role when we obtain more than 100 boxes. The same holds also for our runs RUN1 and RUN2 (based only on deep learning features).</p><p>Compared to the runs of other participants, we find that our general runs that integrate deep learning do not obtain very high results. This can be explained by the fact that, as mentioned before, the whole proposed process was applied only on 180K images of the 510K images of the corpus.</p><p>As expected, our run RUN3, that detects only face parts has a very low overall result, ranked 23 for both overlap 0 and overlap 0.5. When considering the additional official measures related to the minimum number of boxes per image, we see a plateau above a minimum of 20 boxes. This shows that when a image has less than 20 boxes in the ground truth set our proposal has difficulty to find relevant concepts or boxes. This can be also attributed to the fact that we did not fully process the whole corpus, as explained earlier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Detailed analysis of face parts results</head><p>Here we try to give additional insight into the results obtained when considering only the face elements from deep learning and predefined face extraction approaches <ref type="bibr" coords="6,175.71,596.34,10.52,8.74" target="#b7">[8,</ref><ref type="bibr" coords="6,187.89,596.34,7.01,8.74" target="#b5">6]</ref>. In table 3, we present the average precision results obtained for our overlap ranking approaches runs RUN2 (deep learning only), RUN3 (face parts only), and RUN5 (fusion), for the concepts mouth, eye, nose and f ace.</p><p>One interesting point that we get from table 3 is that, for the MAP at 0 and for the f ace concept, the deep learning approach (RUN2) outperforms both the predefined detection (RUN3) and fusion (RUN5). We recall that face </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>M AP 0 M AP 0.5 mouth eye nose face mouth eye nose face RUN2 0.1502 0.4053 0.1964 0.8947 0.08336 0.2955 0.1607 0,5722 RUN3 0.6787 0.7078 0.7172 0.8416 0.5578 0.4177 0.6941 0.8172 RUN5 0.2082 0.6699 0.7076 0.8663 0.1366 0.4198 0.6804 0.7216 already a concept available in ImageNet. However, for the other concepts this is not the case. When the localization is evaluated, then the predefined detection outperforms the deep learning approach. When considering the fusion run (RUN5), we see that most of the time such fusion does not work properly as it does not seem to boost the results. The only case when the fusion outperforms the other runs is for MAP 0.5 for the eye, and the increment is marginal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Current limitations of the scalable concept annotation task</head><p>After checking the official global results and the per concept results, we feel that:</p><p>-The size of the ground truth seems small: many concept results aP values are equal to 1 (or exactly 0.5, 0.25, etc.), leading to think that there are only very few ground truth regions defined for most concepts. A collaborative annotation interface open to participants may be a good idea to get more ground truth, leading to results that are more statistically valid. In this case, it should be possible to force a minimum number of examples for each concept in the ground truth; -The ground truth is not released by the organizers after the official results.</p><p>Even if we understand the reason why the organizers do that, such ground truth may be of a great help for the participant to study why and when their approach fail. Alternatively, a bigger and more representative validation set should be very helpful to participants;. -Without obtaining the ground truth, we think that the number of boxes per concept in the ground truth should be released, so that participants may have cues about their results per concept; -Even if the name of the task is "scalable concept annotation", we wonder if it should be possible to get, in addition to the existing measures, other measures that are able to focus on the runs submitted: limiting the evaluation on the concepts detected is already possible by averaging a posteriori the aP of a subset of concepts, but it is impossible for the participants that were not able for any reason to process all the images to evaluate the quality of such runs only on the subset of image processed.</p><p>For our first participation in the Image CLEF scalable concept detection, we used classical approaches based on convolutional networks as well as specific elements related to the detection of parts of faces. Selective search was applied on the images in a way to detect concepts from CNNs. Because only a subset (35%) of the whole corpus was fully processed, the official results we obtain are not as high as they could have been. We found that the fusion of predefined face part extraction and deep learning detection did not give positive results: such fusion has to be studied in more detail in the future. The elements related to the definition of localization has also to be studied in the future to allow fast detection of such boxes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,228.53,460.16,158.30,7.89;3,152.06,115.83,311.24,329.55"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. MRIM-LIG Annotation System</figDesc><graphic coords="3,152.06,115.83,311.24,329.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,229.77,349.63,155.81,7.89;4,203.93,115.84,207.50,219.03"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. MRIM-LIG two steps learning.</figDesc><graphic coords="4,203.93,115.84,207.50,219.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,214.07,511.34,187.21,74.09"><head>Table 1 .</head><label>1</label><figDesc>Post-tuning evaluation results</figDesc><table coords="4,214.07,532.14,187.21,53.29"><row><cell cols="4">Tuning set Validation set success@1 success@5</cell></row><row><cell>1</cell><cell>ImageCLEF</cell><cell>0.1523</cell><cell>0.3418</cell></row><row><cell>1</cell><cell>Bing</cell><cell>0.6521</cell><cell>0.8435</cell></row><row><cell>2</cell><cell cols="2">ImageCLEF* 0.6290</cell><cell>0.6290</cell></row><row><cell>2</cell><cell>Bing</cell><cell>0.7337</cell><cell>0.9333</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,183.57,349.67,244.26,85.45"><head>Table 2 .</head><label>2</label><figDesc>Official overlap evaluation results</figDesc><table coords="6,183.57,370.47,244.26,64.65"><row><cell>Run</cell><cell cols="2">MAP 0 rank (on 30)</cell><cell cols="2">MAP 0.5 rank (on 30)</cell></row><row><cell cols="2">RUN5 0.2084</cell><cell>11</cell><cell>0.1353</cell><cell>9</cell></row><row><cell cols="2">RUN2 0.2051</cell><cell>12</cell><cell>0.1317</cell><cell>11</cell></row><row><cell cols="2">RUN4 0.2030</cell><cell>13</cell><cell>0.1351</cell><cell>10</cell></row><row><cell cols="2">RUN1 0.1998</cell><cell>14</cell><cell>0.1309</cell><cell>12</cell></row><row><cell cols="2">RUN3 0.0123</cell><cell>23</cell><cell>0.0104</cell><cell>23</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,252.08,115.91,111.20,7.89"><head>Table 3 .</head><label>3</label><figDesc>Face parts results</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,138.35,305.98,342.25,7.86;8,146.91,316.93,333.68,7.86;8,146.91,327.89,255.12,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,387.53,305.98,93.07,7.86;8,146.91,316.93,111.02,7.86">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,280.17,316.93,170.86,7.86;8,146.91,327.89,136.11,7.86">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct coords="8,138.35,338.85,342.24,7.86;8,146.91,349.81,280.23,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,325.60,338.85,154.99,7.86;8,146.91,349.81,14.75,7.86">Efficient graph-based image segmentation</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,169.43,349.81,168.11,7.86">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="181" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,360.77,342.25,7.86;8,146.91,371.73,333.68,7.86;8,146.91,382.69,333.68,7.86;8,146.91,391.38,270.61,10.13" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,284.40,371.73,196.19,7.86;8,146.91,382.69,133.21,7.86">Overview of the ImageCLEF 2016 Scalable Concept Image Annotation Challenge</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ramisa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dellandrea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="8,299.21,382.69,181.39,7.86;8,146.91,393.65,46.41,7.86">CLEF2016 Working Notes, CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Évora, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08">September 5-8 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,404.61,342.25,7.86;8,146.91,415.56,118.97,7.86" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="8,300.51,404.61,176.16,7.86">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>CoRR, abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,426.52,342.24,7.86;8,146.91,437.48,275.39,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,401.72,426.52,78.87,7.86;8,146.91,437.48,71.10,7.86">Selective search for object recognition</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,226.03,437.48,168.11,7.86">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,448.44,342.24,7.86;8,146.91,459.40,333.68,7.86;8,146.91,470.36,305.05,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,302.08,448.44,178.52,7.86;8,146.91,459.40,91.57,7.86">Detector of facial landmarks learned by the structured output SVM</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Uřičář</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Franc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Hlaváč</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,259.13,459.40,221.46,7.86;8,146.91,470.36,213.95,7.86">VISAPP &apos;12: Proceedings of the 7th International Conference on Computer Vision Theory and Applications</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="547" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,481.32,342.24,7.86;8,146.91,492.28,333.68,7.86;8,146.91,503.24,333.68,7.86;8,146.91,514.19,333.68,7.86;8,146.91,525.15,124.61,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,377.70,503.24,102.90,7.86;8,146.91,514.19,137.98,7.86">General Overview of Im-ageCLEF at the CLEF 2016 Labs</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schaer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bromuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ramisa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dellandrea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">M J</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">H</forename><surname>Toselli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-A</forename><surname>Snchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="8,295.17,514.19,143.92,7.86">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,536.11,342.25,7.86;8,146.91,547.07,333.68,7.86;8,146.91,558.03,333.68,7.86;8,146.91,568.99,48.76,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,246.30,536.11,234.29,7.86;8,146.91,547.07,30.33,7.86">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,200.42,547.07,172.12,7.86;8,406.59,547.07,74.00,7.86;8,146.91,558.03,236.58,7.86">CVPR 2001. Proceedings of the 2001 IEEE Computer Society Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">511</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision and Pattern Recognition</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
