<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,184.96,116.95,245.44,12.62;1,232.24,134.89,150.88,12.62">Joint Learning of CNN and LSTM for Image Captioning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,203.42,172.56,58.74,8.74"><forename type="first">Yongqing</forename><surname>Zhu</surname></persName>
							<email>yongqing.zhu@vipl.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology Chinese Academy of Sciences</orgName>
								<orgName type="laboratory">Key Laboratory of Intelligent Information Processing</orgName>
								<address>
									<addrLine>No.6 Kexueyuan South Road Zhongguancun, Haidian District</addrLine>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,270.38,172.56,57.38,8.74"><forename type="first">Xiangyang</forename><surname>Li</surname></persName>
							<email>xiangyang.li@vipl.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology Chinese Academy of Sciences</orgName>
								<orgName type="laboratory">Key Laboratory of Intelligent Information Processing</orgName>
								<address>
									<addrLine>No.6 Kexueyuan South Road Zhongguancun, Haidian District</addrLine>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,335.01,172.56,28.59,8.74"><forename type="first">Xue</forename><surname>Li</surname></persName>
							<email>xue.li@vipl.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology Chinese Academy of Sciences</orgName>
								<orgName type="laboratory">Key Laboratory of Intelligent Information Processing</orgName>
								<address>
									<addrLine>No.6 Kexueyuan South Road Zhongguancun, Haidian District</addrLine>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,370.85,172.56,36.25,8.74"><forename type="first">Jian</forename><surname>Sun</surname></persName>
							<email>sun.jian@vipl.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology Chinese Academy of Sciences</orgName>
								<orgName type="laboratory">Key Laboratory of Intelligent Information Processing</orgName>
								<address>
									<addrLine>No.6 Kexueyuan South Road Zhongguancun, Haidian District</addrLine>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,231.09,184.51,59.17,8.74"><forename type="first">Xinhang</forename><surname>Song</surname></persName>
							<email>xinhang.song@vipl.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology Chinese Academy of Sciences</orgName>
								<orgName type="laboratory">Key Laboratory of Intelligent Information Processing</orgName>
								<address>
									<addrLine>No.6 Kexueyuan South Road Zhongguancun, Haidian District</addrLine>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,317.71,184.51,66.55,8.74"><forename type="first">Shuqiang</forename><surname>Jiang</surname></persName>
							<email>shuqiang.jiang@vipl.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology Chinese Academy of Sciences</orgName>
								<orgName type="laboratory">Key Laboratory of Intelligent Information Processing</orgName>
								<address>
									<addrLine>No.6 Kexueyuan South Road Zhongguancun, Haidian District</addrLine>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,184.96,116.95,245.44,12.62;1,232.24,134.89,150.88,12.62">Joint Learning of CNN and LSTM for Image Captioning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A7386DC33AC79CA56D8DE4C15C4F3C25</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Convolutional neural network</term>
					<term>Long Short-Term Memory</term>
					<term>Image caption</term>
					<term>Joint learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we describe the details of our methods for the participation in the subtask of the ImageCLEF 2016 Scalable Image Annotation task: Natural Language Caption Generation. The model we used is the combination of a procedure of encoding and a procedure of decoding, which includes a Convolutional neural network(CNN) and a Long Short-Term Memory(LSTM) based Recurrent Neural Network. We first train a model on the MSCOCO dataset and then fine tune the model on different target datasets collected by us to get a more suitable model for the natural language caption generation task. Both of the parameters of CNN and LSTM are learned together.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the rapid development of Internet technologies and extensive access to digital cameras, we are surrounded by a huge number of images, accompanied with a lot of related text. However, the relationship between the surrounding text and images varies greatly, how to close the loop between vision and language is a challenging problem for the task of scalable image annotation <ref type="bibr" coords="1,414.99,525.61,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="1,427.17,525.61,7.01,8.74" target="#b1">2]</ref>.</p><p>It is easy for our human beings to describe a picture after a glance of it. However, it is not easy for a computer to do the same work. Though great progress has been achieved in visual recognition, it is still far away from generating descriptions that a human can compose. The approaches automatically generating sentence descriptions can be divided into three categories. The first method is template-based <ref type="bibr" coords="1,203.25,597.34,10.52,8.74" target="#b2">[3,</ref><ref type="bibr" coords="1,215.42,597.34,7.01,8.74" target="#b3">4]</ref>. These approaches often rely heavily on sentence templates, so the generated sentences lack variety. The second method is retrieval-based <ref type="bibr" coords="1,470.07,609.29,10.52,8.74" target="#b4">[5,</ref><ref type="bibr" coords="1,134.77,621.25,7.01,8.74" target="#b5">6]</ref>. The advantage of these methods is that the captions are more human-like. However, it is not flexible to add or remove words based on the content of the target image. Recently, many researchers have used the combination of CNN and LSTM to translate an image into a linguistic sentence <ref type="bibr" coords="1,392.75,657.11,10.52,8.74" target="#b6">[7,</ref><ref type="bibr" coords="1,404.92,657.11,7.01,8.74" target="#b7">8]</ref>.</p><p>Our method is based on deep models proposed by Vinyals <ref type="bibr" coords="2,400.67,119.99,10.52,8.74" target="#b6">[7]</ref> which takes advantage of Convolutional Neural Network (CNN) for image encoding and Long-Short Term Memory based Recurrent Neural Network (LSTM) for sentence decoding. We firstly train a model on the MSCOCO <ref type="bibr" coords="2,352.89,155.86,10.52,8.74" target="#b8">[9]</ref> dataset. We then fine tune the model on different datasets to make the model more suitable for the target task. In training and finetuning, the parameters of both CNN and LSTM are learned together.</p><p>Next, we introduce our methods in Section 2, followed by our experimental results in Section 3. At last, the section 4 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>The model we use contains two types of neural networks, as illustrated in Figure <ref type="figure" coords="2,134.77,299.50,3.87,8.74" target="#fig_0">1</ref>. The first stage is CNN for image encoding and the second stage is Long-Short Term Memory(LSTM) based Recurrent Neural Network for sentence encoding <ref type="bibr" coords="2,134.77,323.41,10.52,8.74" target="#b6">[7,</ref><ref type="bibr" coords="2,146.95,323.41,7.01,8.74" target="#b7">8]</ref>. For CNN, we use the pre-trained VGGNet <ref type="bibr" coords="2,341.93,323.41,15.50,8.74" target="#b9">[10]</ref> for feature extraction. Using the VGGNet , we transform the pixels inside an image to a 4096-dimensional vector. After getting the visual features, we train an LSTM to obtain linguistic captions. In the LSTM training procedure, we change the parameters of both CNN and LSTM together. At last we fine tune the pre-trained model to get a more suitable model for the natural language caption generation task.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training stage</head><p>As shown in Figure <ref type="figure" coords="3,227.25,140.36,3.87,8.74" target="#fig_0">1</ref>, we use the pre-trained VGGNet <ref type="bibr" coords="3,387.52,140.36,15.50,8.74" target="#b9">[10]</ref> for CNN feature extraction. We first train the LSTM on corpora with paired image and sentence captions, such as MSCOCO <ref type="bibr" coords="3,259.76,164.27,10.52,8.74" target="#b8">[9]</ref> and Flickr30k <ref type="bibr" coords="3,337.07,164.27,14.61,8.74" target="#b10">[11]</ref>. In the training procedure of the LSTM, we change not only the parameters of the LSTM model, but also the parameters of the CNN model, which is a joint learning of CNN and LSTM. We then fine tune our model on different datasets as described in Section 3. At last, We use the trained models to predict linguistic sentence of a given image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predicting stage</head><p>The process of predicting an image is shown in Figure <ref type="figure" coords="3,371.49,262.81,3.87,8.74" target="#fig_1">2</ref>. To generate a sentence caption for an image, we get the CNN features of an image b v , set the first hidden state h 0 =0, x 0 to the START vector and compute the hidden state h 1 and predict the first word y 1 . Then we use the word y 1 predicted by our model and set its embedding vector as x 1 , the previous hidden state h 1 , and then compute the hidden state h 2 and use it to predict the next word y 2 . The process is repeated until the END token is generated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Submitted Runs</head><p>We first use the LSTM implementation from the NeuralTalk project <ref type="bibr" coords="3,448.86,391.23,9.96,8.74" target="#b6">[7]</ref>. We train models separately on the MSCOCO <ref type="bibr" coords="3,320.93,403.18,10.52,8.74" target="#b8">[9]</ref> dataset, the Flickr8k <ref type="bibr" coords="3,429.84,403.18,15.50,8.74" target="#b11">[12]</ref> dataset and the Flickr30k <ref type="bibr" coords="3,217.89,415.14,15.50,8.74" target="#b10">[11]</ref> dataset. Then we use the model to predict the images in the ImageCLEF 2016 validation set. The results are shown in Table <ref type="table" coords="3,454.92,427.09,3.87,8.74" target="#tab_0">1</ref>. We use Meteor <ref type="bibr" coords="3,187.04,439.05,15.50,8.74" target="#b12">[13]</ref> to evaluate sentences generated by a model. Validation set we use here is the 2000 images and their corresponding sentences provided by the organizers. Because the performance of the model on the MSCOCO dataset is better than the other dataset, So we use the model trained on the MSCOCO as our pre-trained model. We then do experiments to decide whether jointly learn the parameters of CNN and LSTM together or fixed the CNN and just learn the parameters of the LSTM. Firstly, we train a model which only learns the parameters in LSTM, then we use the model to predict the images in the validation set. The training set we use is the MSCOCO dataset, and the test set we use is the provided validation set. For comparison, we train a new model which not only learns the parameters of the LSTM but also fine tunes the CNN model. We then use the second model to predict the images in the validation set. And the results are shown in Table <ref type="table" coords="4,204.01,155.86,3.87,8.74" target="#tab_1">2</ref>. The results demonstrate that the joint learning of CNN and LSTM has a significant improvement in performance. To make full use of the MSCOCO dataset, we jointly train a model using all of the examples in MSCOCO dataset, not just using the train split. The results are shown in Table <ref type="table" coords="4,366.86,304.18,3.87,8.74" target="#tab_2">3</ref>. It is demonstrated that more data can result in better performance.</p><p>At last, we fine tune the jointly learned model on different datasets to get a more suitable model for the natural language caption generation task. We use the model trained on all of the examples on MSCOCO as the baseline, and fine tune the model on different datasets. We firstly fine tune our model on a very small dataset. In this experiment, we use 1500 images and their sentences in the validation set as a training set and use the remaining 500 images to evaluate the performance of the fine tuned model. The results are shown in Table <ref type="table" coords="4,455.63,400.26,3.87,8.74" target="#tab_2">3</ref>. We also fine tune the baseline model on a big dataset, which is the combination of Flickr30K and Flickr8K. We can see that fine tuning on a big dataset can get a better performance. We use the model obtained in the previous step to generate image captions on all the 510123 target images. This time, we manually select 1000 satisfactory pairs of image and generated sentence from all the generated captions and add them to the fine-tuning dataset. As shown in Table <ref type="table" coords="4,454.46,471.99,3.87,8.74" target="#tab_2">3</ref>, the results show that this pipeline has the best performance. We use the model fine tuned in the combination of Flickr8K, Flickr30K and the selected 1000 examples as our final model.</p><p>Figure <ref type="figure" coords="4,180.76,520.24,4.98,8.74" target="#fig_2">3</ref> is the illustration of the generated image captions by different models. The results qualitatively demonstrate that our final model can generate more satisfactory captions that reveal the content of the corresponding images. We submitted four runs in the natural language caption generation task: id sentence2.txt is our baseline method. The model is trained only using all the examples in the MSCOCO dataset. The median score (provided by the server) of the generated sentences is 0.1676 (The model is used twice to generate both the two runs, so id sentence3.txt is the same as id sentence2.txt).</p><p>id sentence.txt is the results generated by the model which is firstly trained only using the examples in the MSCOCO dataset and then fine tuned on the combination of Flickr8K and Flickr30K. The median score of the generated sentences is 0.1710. id sentence4.txt is the results generated by our final model which is firstly trained only using the examples in the MSCOCO dataset and then fine tuned on the combination of Flickr8K, Flickr30k and the manually selected examples. And the median score of the generated sentences is 0.1711. This submitted run is the best of our submitted runs and also the best one for natural language caption generation of ImageCLEF 2016. After performing the experiments above, we get the following conclusions. By learning the parameters in CNN and LSTM, the performance of the model can be greatly improved. When we just change the parameters of LSTM, the accuracy on the test set is 0.133. However, when we change parameters in both neural network, the accuracy is 0.165. Secondly,more data can result in better performance. We train a model only on the training split of the MSCOCO dataset and the score of the generated sentences is 0.137. However, when we use all the data in the MSCOCO dataset, the score is 0.165. Thirdly, when fine-tuning the model only using a small dataset, the result we get is worse. When we fine tuned our model only using 3/4 of the validation set, the result on the remaining 1/4 of the validation set is 0.116 which is worse than the model before fine-tuning. However, the datasets we use to train our model don't include all the concepts of the ImageCLEF 2016, so some sentences predicted by our model might be weird. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Acknowledgments</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,226.85,475.77,161.66,7.89;2,134.77,533.41,353.10,83.70"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An illustration of training stage.</figDesc><graphic coords="2,134.77,533.41,353.10,83.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,222.50,631.88,170.36,7.89"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. An illustration of predicting stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,134.77,619.37,345.83,7.89;5,134.77,630.36,238.50,7.86;5,134.77,333.85,360.19,270.75"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. An illustration of the generated image captions. Sentence 1 is generated by our final model. Sentence 2 is generated by our baseline model.</figDesc><graphic coords="5,134.77,333.85,360.19,270.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,134.77,345.61,345.83,8.74;6,134.77,357.57,345.83,8.74;6,134.77,369.52,345.83,8.74;6,134.77,381.48,345.82,8.74;6,134.77,393.43,211.10,8.74"><head></head><label></label><figDesc>This work was supported in part by the National Basic Research Program of China (973 Program): 2012CB316400, the National Natural Science Foundation of China: 61532018, 61322212, the National Hi-Tech Development Program (863 Program) of China: 2014AA015202. This work is also funded by Lenovo Outstanding Young Scientists Program (LOYS).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,166.83,518.54,278.62,62.73"><head>Table 1 .</head><label>1</label><figDesc>The performance of training a LSTM on different datasets.</figDesc><table coords="3,223.70,539.34,167.95,41.94"><row><cell cols="2">Training data Test data</cell><cell>Accuracy</cell></row><row><cell cols="3">MSCOCO validation set 0.1326719463</cell></row><row><cell>Flickr8k</cell><cell cols="2">validation set 0.1168440478</cell></row><row><cell cols="3">Flickr30k validation set 0.1231898764</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,166.20,186.29,282.96,51.38"><head>Table 2 .</head><label>2</label><figDesc>The performance of joint learning CNN and LSTM or not.</figDesc><table coords="4,166.20,207.09,282.96,30.58"><row><cell cols="2">Change parameters in CNN Training data Test data</cell><cell>Accuracy</cell></row><row><cell>NO</cell><cell cols="2">MSCOCO validation set 0.1326719463</cell></row><row><cell>YES</cell><cell cols="2">MSCOCO validation set 0.1646048292</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,160.50,576.52,294.35,85.45"><head>Table 3 .</head><label>3</label><figDesc>The performance of fine tuning the model on different datasets</figDesc><table coords="4,162.81,597.32,289.74,64.65"><row><cell>Train</cell><cell>Test</cell><cell>Accuracy</cell></row><row><cell>Only the training split of MSCOCO</cell><cell cols="2">validation set 0.1368988204</cell></row><row><cell>MSCOCO(baseline)</cell><cell cols="2">validation set 0.1646048292</cell></row><row><cell>FT(3/4 validation set)</cell><cell cols="2">1/4 validation set 0.1159403729</cell></row><row><cell>FT(Flickr8k+Flickr30k)</cell><cell cols="2">validation set 0.1738749916</cell></row><row><cell cols="3">FT(Flickr8k+Flickr30K+1000 selected) validation set 0.2042696662</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,142.96,441.78,337.63,7.86;6,151.52,452.74,329.07,7.86;6,151.52,463.70,329.07,7.86;6,151.52,472.39,329.07,10.13;6,151.52,485.62,81.95,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,151.52,463.70,324.76,7.86">Overview of the ImageCLEF 2016 Scalable Concept Image Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Josiah</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fei</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arnau</forename><surname>Ramisa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emmanuel</forename><surname>Dellandrea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mauricio</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,164.54,474.66,239.17,7.86">CLEF2016 Working Notes, CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Évora, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08">September 5-8 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,495.79,337.63,7.86;6,151.52,506.74,329.07,7.86;6,151.52,517.70,329.07,7.86;6,151.52,528.66,329.07,7.86;6,151.52,539.62,329.07,7.86;6,151.52,550.58,196.33,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,461.07,528.66,19.52,7.86;6,151.52,539.62,211.89,7.86">General Overview of ImageCLEF at the CLEF 2016 Labs</title>
		<author>
			<persName coords=""><forename type="first">Mauricio</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roger</forename><surname>Schaer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefano</forename><surname>Bromuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Josiah</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fei</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arnau</forename><surname>Ramisa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emmanuel</forename><surname>Dellandrea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alejandro</forename><forename type="middle">H</forename><surname>Toselli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joan-Andreu</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Enrique</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="6,371.20,539.62,109.39,7.86;6,151.52,550.58,27.78,7.86">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,560.75,337.64,7.86;6,151.52,571.71,329.07,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,151.52,571.71,269.06,7.86">Baby talk: Understanding and generating simple image descriptions</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,427.80,571.71,23.34,7.86">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,581.87,337.63,7.86;6,151.52,592.83,329.07,7.86;6,151.52,603.79,53.11,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,223.54,592.83,252.81,7.86">Every picture tells a story: Generating sentences from images</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hejrati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,151.52,603.79,23.24,7.86">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,613.96,337.64,7.86;6,151.52,624.92,329.07,7.86;6,151.52,635.88,329.07,7.86;6,151.52,646.84,329.07,7.86;6,151.52,657.79,142.32,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,200.94,624.92,203.44,7.86">Collective generation of natural image descriptions</title>
		<author>
			<persName coords=""><forename type="first">Polina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,423.88,624.92,56.71,7.86;6,151.52,635.88,329.07,7.86;6,151.52,646.84,110.79,7.86">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers -Volume 1,ACL &apos;12</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers -Volume 1,ACL &apos;12<address><addrLine>Stroudsburg,PA,USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="359" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,120.67,337.63,7.86;7,151.52,131.63,76.03,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,265.53,120.67,215.06,7.86;7,151.52,131.63,11.14,7.86">Nonparametric method for datadriven image captioning</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,181.97,131.63,16.14,7.86">ACL</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,142.59,337.64,7.86;7,151.52,153.55,235.14,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,440.78,142.59,39.81,7.86;7,151.52,153.55,162.22,7.86">Show and Tell: A Neural Image Caption Generator</title>
		<author>
			<persName coords=""><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,333.65,153.55,23.16,7.86">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,164.51,337.63,7.86;7,151.52,175.46,329.07,7.86;7,151.52,186.42,329.07,7.86;7,151.52,197.38,20.99,7.86" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="7,361.94,175.46,118.66,7.86;7,151.52,186.42,187.63,7.86">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName coords=""><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03044</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,142.96,206.07,337.64,10.13;7,151.52,219.30,329.07,7.86;7,151.52,230.26,91.15,7.86" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Doll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.0312</idno>
		<title level="m" coord="7,232.77,219.30,178.31,7.86">Microsoft coco: Common objects in context</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,142.62,241.22,337.98,7.86;7,151.52,252.18,230.23,7.86" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="7,285.47,241.22,195.12,7.86;7,151.52,252.18,69.82,7.86">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,142.62,263.14,337.98,7.86;7,151.52,274.09,329.07,7.86;7,151.52,285.05,96.75,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,368.30,263.14,112.30,7.86;7,151.52,274.09,329.07,7.86;7,151.52,285.05,37.35,7.86">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,196.69,285.05,22.01,7.86">TACL</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.62,296.01,337.97,7.86;7,151.52,306.97,329.07,7.86;7,151.52,317.93,62.41,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,330.32,296.01,150.26,7.86;7,151.52,306.97,187.58,7.86">Framing image description as a ranking task: data, models and evaluation metrics</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,349.79,306.97,130.81,7.86;7,151.52,317.93,34.08,7.86">Journal of Artificial Intelligence Research</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.62,328.89,337.98,7.86;7,151.52,339.85,329.07,7.86;7,151.52,350.81,152.38,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,269.38,328.89,211.21,7.86;7,151.52,339.85,123.76,7.86">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,297.84,339.85,182.75,7.86;7,151.52,350.81,124.28,7.86">Proceedings of the EACL 2014 Workshop on Statistical Machine Translation</title>
		<meeting>the EACL 2014 Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
