<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,181.25,116.90,252.87,12.68;1,208.06,134.83,199.23,12.68">NovaSearch at ImageCLEFmed 2016 Subfigure Classification Task</title>
				<funder>
					<orgName type="full">NVIDIA Corporation</orgName>
				</funder>
				<funder ref="#_eSxKtNp">
					<orgName type="full">NOVA LINCS</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,230.60,172.50,62.39,8.80"><forename type="first">David</forename><surname>Semedo</surname></persName>
							<email>df.semedo@campus.fct.unl.pt</email>
						</author>
						<author>
							<persName coords="1,315.67,172.50,69.08,8.80"><forename type="first">João</forename><surname>Magalhães</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science Faculty of Science</orgName>
								<orgName type="laboratory">NOVA LINCS</orgName>
								<orgName type="institution">Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Universidade NOVA de Lisboa</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,181.25,116.90,252.87,12.68;1,208.06,134.83,199.23,12.68">NovaSearch at ImageCLEFmed 2016 Subfigure Classification Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">66E2610D22D1F2BC7D4FAAA208515934</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Medical Modality Classification</term>
					<term>Deep Learning</term>
					<term>Convolutional Neural Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the NovaSearch team participation in the ImageCLEF 2016 Medical Task in the subfigure classification subtask.</p><p>Deep learning techniques have proved to be very effective in automatic representation learning and classification tasks with general data. More specifically, convolutional neural networks (CNNs) have surpassed humanlevel performance in the ImageNET classification task, making them a promising model for the task of medical modality classification. We assess how each model behave when dealing with medical images, by developing three different models, with different depths and components, and analyse the impact of these factors in the performance. One of the key ingredients for the effectiveness of CNNs (and deep learning in general) is the use of large amounts of data for training. This subtask scenario is completely different, due to the small size of the dataset, implying a significant risk of overfitting. We apply state-of-the-art techniques developed to reduce overfitting in these networks to our models and evaluate their effectiveness. Our best model achieves 65.31% accuracy on the test set using only the training data provided.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper describes the NovaSearch team submissions, from the Faculty of Science and Technology of Universidade Nova de Lisboa, to the ImageCLEF 2016 <ref type="bibr" coords="1,158.19,561.41,15.49,8.80" target="#b15">[16]</ref> Medical task <ref type="bibr" coords="1,236.28,561.41,9.96,8.80" target="#b1">[2]</ref>. This task consists of five subtasks: compound figure detection, multi-label classification, figure separation, subfigure classification and caption prediction. We addressed the subfigure classification task, which aims to classify medical images within a given set of modalities.</p><p>We were interested in evaluating deep learning methods, namely convolutional neural networks (CNNs), in the specific scenario of this subtask. More concretely, we wanted to evaluate their effectiveness when dealing with medical images, which possess very distinct characteristics compared to general images, and for which they have proved lately to be very effective.</p><p>The remainder of this paper is organised as follows. In section 2 we describe our approach. More specifically, we present and discuss in detail the models and techniques used in our submitted runs. The configuration and results of each run are presented and discussed in section 3. Additionally, we compare our classifier results with the results achieved by the winning team. Finally, we draw conclusions and present future work perspectives on section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Regarding medical diagnostic and research, effective medical image retrieval systems (MIRS) can be a valuable tool for aiding clinicians. Furthermore, images in biomedical literature usually are compound figures with multiple panels each, with an image from a given modality (e.g. in figure <ref type="figure" coords="2,362.11,283.74,3.87,8.80" target="#fig_0">1</ref>), promoting human readability/interpretation by grouping correlated images, but making automatic retrieval more difficult. Adapted from <ref type="bibr" coords="2,193.90,535.46,13.51,7.92" target="#b16">[17]</ref>.</p><p>ImageCLEFmed promotes research in this direction, by proposing a set of tasks, which result from splitting the problem of building a MIRS. Concretely, an effective MIRS must be capable of identifying and separate compound figures from biomedical articles and classify each subfigure with a given modality.</p><p>Knowing the modality of a medical image has been shown to be important to improve the performance of MIRS <ref type="bibr" coords="2,283.01,645.10,9.96,8.80" target="#b7">[8]</ref>. The subfigure task aims at classifying each subfigure, from a collection of figures from compound images found in biomedical articles from PubMed Central<ref type="foot" coords="3,263.97,118.37,3.97,6.16" target="#foot_0">1</ref> , into 30 modalities structured hierarchically (the hierarchy is defined in <ref type="bibr" coords="3,234.20,131.89,10.29,8.80" target="#b6">[7]</ref>).</p><p>We tackled the subfigure task with a deep neural network classifiers. Deep neural networks have recently achieved very good results in representation learning and classification of images <ref type="bibr" coords="3,278.75,167.88,10.51,8.80" target="#b8">[9,</ref><ref type="bibr" coords="3,290.93,167.88,12.73,8.80" target="#b10">11,</ref><ref type="bibr" coords="3,305.32,167.88,12.73,8.80" target="#b13">14,</ref><ref type="bibr" coords="3,319.70,167.88,7.75,8.80" target="#b4">5,</ref><ref type="bibr" coords="3,329.11,167.88,7.01,8.80" target="#b3">4]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dataset</head><p>The provided training dataset consists of a collection of 6776 labelled medical images. Deep learning methods have achieved great results using very large datasets like the ImageNet challenge dataset <ref type="bibr" coords="4,293.79,155.80,15.49,8.80" target="#b9">[10]</ref> which contains roughly 1.2 million images from 1000 classes with approximately 1000 images per class. However, in the present task, the size of the dataset imposes a big challenge due to its size, specially for deep learning methods, which tend to overfit with small datasets. The fact that the dataset is highly unbalanced, making this a very challenging task.</p><p>We intend to evaluate how CNNs behave in such a scenario, which is clearly different from the scenarios in which they excelled, and experiment state-of-theart techniques used to improve their performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Convolutional Neural Networks</head><p>Convolutional neural networks are a type of neural network which have very interesting characteristics. These networks are able to automatically learn highlevel and hierarchical representations from data which eliminates the necessity of selecting a good set of low-level and high-level features to describe each image by hand.</p><p>By definition, CNNs make some assumptions (which are correct in general) regarding the stationarity of statistics and locality of pixel dependencies, allowing for a reduction in the number of connections and consequently the number of parameters to learn <ref type="bibr" coords="4,227.51,394.47,10.51,8.80" target="#b0">[1,</ref><ref type="bibr" coords="4,239.69,394.47,7.01,8.80" target="#b8">9]</ref>. Through depth and breadth one can control their capacity of identifying high-level data representations and relationships between the input and the output. From this reduction in the number of parameters and with current GPUs processing power, the task of training deep convolutional networks is feasible. The motivation for building and training deep CNNs is based on the fact that as the number of layers (depth) of the network increases, so the capacity of detecting more high-level details does, in principle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Dealing with Unbalanced and Small Datasets</head><p>Despite the fact that this task involves classifying medical images, which have a set of exclusive characteristics of their own, the challenges of working with unbalanced and small datasets is a general problem in machine learning. In this section we describe a set of techniques that we applied to our models in order to take into account the training dataset characteristics.</p><p>Large networks tend to overfit. Traditional techniques to solve the overfitting problem consist of stopping the training procedure as soon as the validation error starts increasing (early-stopping) or in using regularisation techniques like adding an extra term in the function to be minimised or limiting the complexity of the model <ref type="bibr" coords="4,192.87,621.19,9.96,8.80" target="#b0">[1]</ref>.</p><p>Recently, a technique named Dropout was proposed in <ref type="bibr" coords="4,389.01,633.14,15.49,8.80" target="#b11">[12]</ref> which has shown to be very effective in reducing overfitting in large networks, leading to performance improvements. Dropout can be interpreted as a stochastic regularization technique which essentially consists in randomly dropping neurons and their respective connections during training. The idea is to prevent neurons from coadapting too much to data, making overfitting less likely.</p><p>Another technique we used to address overfitting and the class imbalance problem was data augmentation. More concretely, we performed real time data augmentation, in the sense that new images are generated from sample images at each training batch construction. The following operations are applied randomly to sampled images: horizontal/vertical shifting and horizontal/vertical flipping.</p><p>For classes which have few examples, the network may not be able to learn discriminative properties and may fail to generalise. Furthermore, in section 2.1 we pointed out that some classes (e.g. CFIG) dominate the dataset, which means that the majority of the weight updates during the training phase will be based in examples of these classes. It is therefore important to avoid focusing learning in some classes or we take the risk that the network will classify classes with few examples into a dominating class. We attempt to address this problem by modifying the loss function and making it a weighted loss function. We assign weights to each class such that it is worse to misclassify an image from a class with few examples.</p><p>Let P w be the ideal number of examples of each class c i assuming that the dataset is perfectly balanced. Then, P w is obtained as follows:</p><formula xml:id="formula_0" coords="5,284.96,364.61,195.64,22.31">P w = N N C<label>(1)</label></formula><p>where N is the dataset size and N C is the total number of classes. The weight w i for a class c i is computed using the following expression:</p><formula xml:id="formula_1" coords="5,287.14,423.19,193.46,23.23">w i = P w |S i |<label>(2)</label></formula><p>where |S i | is the cardinality of the set of samples of class c i from the training dataset. After computing the weights, each w i is normalised such that w i ∈ [0, 1].</p><p>By computing the weights with the expression above, we somehow simulate training with a perfectly balanced dataset. This is based on the fact that the values of the gradient to be back-propagated will be amplified for classes with few examples and reduced for the remaining ones.</p><p>In our first experiments these technique did not yield any performance improvement. In fact it deteriorated the performance of our models. After some experiments we concluded that the problem was that the network was severely misclassifying images from dominating classes due to the fact the respective weights are very small. To address this issue we introduced a lower bound on the weights values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">CNN Developed Models</head><p>Our submitted runs are based on essentially three different CNN models which we describe in the following sections and that will denoted from now on by V GG 1 -CNN, V GG 2 -CNN and P ReLU -CNN.</p><p>For all the network models, the input consists of an 224x224 matrix. The last layer of the networks has the sof tmax activation function with dimension 30 (number of modalities of the medical classification subtask). The sof tmax function enforces the constraint that outputs must lie between 0 and 1, and the sum of all output values is equal to 1, allowing the outputs of the network to be interpreted as posterior probabilities for categorical target variables.</p><p>For the special case of medical images, and considering the ImageCLEFmed hierarchy, images may share characteristics which in principle can help the classifier discriminate between modalities (e.g. certainly all Radiology images share some characteristics). However, taking this into account would require a different and possibly more complex approach. By using sof tmax we do not model the fact that the modalities are structured hierarchically, and the final model is not an hierarchical classifier, but an unstructured one. This relaxation has proved to be effective while not increasing the model conceptual complexity <ref type="bibr" coords="6,436.42,275.77,10.51,8.80" target="#b8">[9,</ref><ref type="bibr" coords="6,448.60,275.77,12.73,8.80" target="#b10">11,</ref><ref type="bibr" coords="6,462.98,275.77,11.62,8.80" target="#b13">14]</ref>.</p><p>VGG-like models Both V GG 1 -CNN and V GG 2 -CNN models are inspired in the VGG model proposed in <ref type="bibr" coords="6,264.02,318.67,15.49,8.80" target="#b10">[11]</ref>   This model consists of a deep network with several identically parametrised convolutional layers using small receptive fields (3x3). By using small receptive fields a deeper network can be achieved while keeping the number of parameters equivalent to more shallow networks. Max-pooling is performed after some convolutional layers. For all hidden layers the activation function used is the Rectified Linear Unit <ref type="bibr" coords="7,230.72,179.71,10.51,8.80" target="#b8">[9]</ref> (ReLU) which has shown to yield faster training since it does not saturate as the tanh and sigmoid activation functions.</p><p>We performed some modifications mainly due to computational constraints but also due to overfitting. The original best performing models have 16 and 19 weight layers respectively, and roughly 140 million parameters each. This induces large training times and requires a large dataset such that the network can effectively learn all the parameters and generalize. The medical subfigures dataset is very small compared to the ILSVRC dataset and overfitting becomes a serious issue. Therefore, we reduced the number of convolutional layers and removed one of the fully connected layers. Additionally, we reduced the number of channels in each convolutional layer, in a way that the new values are still proportional to the original model. All the remaining components and characteristics of the architecture like the max-pooling (and its shape), dropout and activation functions are preserved.</p><p>Figure <ref type="figure" coords="7,182.69,348.47,4.98,8.80" target="#fig_3">3</ref> presents both model architectures. The difference between both models is in the depth (V GG 2 -CNN has greater depth). Just like in the original VGG model, dropout is used between the last convolutional layer and the first fully connected layer, and between this and the softmax layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PReLU with Batch Normalization Model</head><p>The ReLU activation function has a reduced likelihood of suffering from gradient vanishing. This is due to the fact that derivatives through the rectifier remain large whenever the unit is active, unlike other activation functions like sigmoid and tanh. However, when the value of the argument of the ReLU function is not positive, the gradient will be 0 and it will not be able to learn.</p><p>Recently, the Parametric ReLU (PReLU) was proposed in <ref type="bibr" coords="7,405.60,490.11,10.51,8.80" target="#b4">[5]</ref> which consists in changing the slope of the ReLU function for inputs in R ≤0 , by multiplying it by a coefficient a i . The coefficient a i is treated as a learnable parameter. The final expression is: f (x i ) = max(0, x i ) + a i min(0, x i )</p><p>With these expression, when the inputs of the function are in R ≤0 , the gradient will not be 0. In deep neural networks, despite the fact that in pre-processing steps the network inputs are normalised (shifted to zero-mean and unit variance), the distribution of each layer's inputs changes during training in function of the parameters of the previous layers. This problem is referred as internal covariate shift <ref type="bibr" coords="7,175.15,645.10,9.96,8.80" target="#b5">[6]</ref>. Consequently, one needs to use lower learning rates and carefully initialise the parameters, slowing down training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,134.77,524.49,345.83,7.93;2,134.77,535.46,76.03,7.92;2,222.64,339.66,170.10,170.10"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example of compound image with 4 subfigures, from a biomedical article.Adapted from<ref type="bibr" coords="2,193.90,535.46,13.51,7.92" target="#b16">[17]</ref>.</figDesc><graphic coords="2,222.64,339.66,170.10,170.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,134.77,510.09,345.82,7.93;3,134.77,521.06,187.52,7.92"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Training set class distribution. The absolute number of examples from each modality is represented at the top of each bar.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,170.85,616.04,309.74,8.80;4,134.77,119.93,345.83,8.80;4,134.77,131.89,270.26,8.80"><head>Figure 2</head><label>2</label><figDesc>depicts the class distribution. It is clear from the figure that the dataset is highly unbalanced, specially towards the CFIG (Statistical figures, graphs, charts) class which represents ≈ 42% of the examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,134.77,614.89,345.83,7.93;6,134.77,625.86,345.82,7.92;6,134.77,636.82,89.98,7.92"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. V GG1-CNN and V GG2-CNN models architecture. The shaded area represents the architecture of model V GG1-CNN and the outer box represents the architecture of model V GG2-CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,112.73,318.67,370.29,268.53"><head></head><label></label><figDesc>which achieved top results in the ImageNET Large Scale Visual Recognition Challenge 2014 (ILSVRC 2014).</figDesc><table coords="6,112.73,366.96,370.29,220.24"><row><cell cols="2">VGG 2 -CNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">VGG 1 -CNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Dropout</cell></row><row><cell>Input -&gt; 224x224</cell><cell>Convolution 32 @ 3x3 Convolution 32 @ 3x3 Max-Pooling 2x2</cell><cell>ReLU ReLU</cell><cell>Convolution 64 @ 3x3 Convolution 64 @ 3x3 Max-Pooling 2x2</cell><cell>ReLU ReLU</cell><cell>Convolution 128 @ 3x3 Convolution 128 @ 3x3 Max-Pooling 2x2</cell><cell>ReLU ReLU</cell><cell>Dropout</cell><cell></cell><cell>ReLU Dense (512)</cell><cell>Dense (30) softmax</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Convolution 256 @ 3x3 256 @ 3x3 Convolution</cell><cell>ReLU ReLU</cell><cell>Dropout</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Max-Pooling 2x2</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,144.73,635.82,335.86,7.92;3,144.73,646.78,335.87,7.92;3,144.73,657.74,229.50,7.92"><p>PubMed Central (http://www.ncbi.nlm.nih.gov/pmc/) is a subset of PubMed, one of the major research databases containing scholarly articles that have been published within the biomedical and life sciences journal literature.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements This research was partially supported by the project <rs type="funder">NOVA LINCS</rs> <rs type="grantNumber">Ref. UID/CEC/04516/2013</rs>. We gratefully acknowledge the support of <rs type="funder">NVIDIA Corporation</rs> with the donation of the Titan X GPU used for this research.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_eSxKtNp">
					<idno type="grant-number">Ref. UID/CEC/04516/2013</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To address these problems in <ref type="bibr" coords="8,281.48,350.35,10.51,8.80" target="#b5">[6]</ref> the Batch Normalisation technique is proposed. It essentially consists of performing mini-batch normalisation at intermediate nodes of the network.</p><p>We developed a CNN model (P ReLU -CNN) which uses both PReLU's and batch normalisation techniques. The model can be seen in figure <ref type="figure" coords="8,421.99,398.37,3.87,8.80">4</ref>. Both these techniques increase the computational requirements during training, therefore, we had to reduce the network depth to be able to train the network in feasible time for the submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Networks Hyperparameters</head><p>The three proposed models are trained using stochastic gradient descent with Nesterov momentum <ref type="bibr" coords="8,227.26,489.01,15.49,8.80" target="#b12">[13]</ref> and learning rate decay is applied. The training hyperparameters values are the same and can be seen in Table <ref type="table" coords="8,390.34,500.97,3.87,8.80">1</ref>. These values were defined empirically. Our model implementation allows enabling or disabling dropout. In section 3 in which we describe our submitted runs, we point out which ones use it. The probability of dropping units between the last convolutional layer and the first fully connected layer is 0.25, between the first fully connected layer and the sof tmax layer is 0.5. Xavier initialisation algorithm <ref type="bibr" coords="9,364.35,143.84,10.51,8.80" target="#b2">[3]</ref> was used for initialising all the weights of our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Data Pre-processing</head><p>As stated in the previous section, despite the fact that our models architecture is based on the VGG model which takes as input an RGB image (the image matrix has one dimension for each color channel) we only used one channel, to reduce the training time. All images are resized to 224x224. Additionally, the elements of each image matrix were scaled to the interval [0, 1].</p><p>We split training data into 70/30 training and validation splits, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Submitted Runs and Results</head><p>All our models were trained using exclusively the training dataset provided. The dataset has the same sample images as the set defined by the merge of the training and test sets from the 2015 edition of the subfigure task. Therefore, for fixing parameters we used the 2015 training/test split since it will have a similar distribution.</p><p>Training CNNs is a very computationally demanding task. Recently several efficient libraries that take advantage of GPU computing capabilities to speedup computations, have been developed. We implemented our models using Keras 2 along with Theano <ref type="bibr" coords="9,222.25,404.90,15.49,8.80" target="#b14">[15]</ref> (Python) libraries. Keras is a neural networks python library that features a rich set of components for developing neural networks models and Theano is an efficient numerical computation library with GPU support.</p><p>We submitted four runs for the subfigure classification task. Table <ref type="table" coords="9,441.80,452.72,4.98,8.80">2</ref> depicts each run configuration and the epoch in which the lowest validation error was obtained. For all runs we fixed the number of epochs to 500. Hyperparameters are also shared among all runs (see section 2.4) apart from dropout on run #2 which 2 Keras python library: https://github.com/fchollet/keras is not enabled. We verified that except for run #1 which achieves the lowest validation error at roughly half of the total number of epochs, in remaining runs it is achieved in late epochs.</p><p>The results (accuracy on validation and test datasets) for each run are presented in table 3. Our models are not in par with the best results achieved for the task in the current edition.</p><p>A first observation is that our models performance both on validation and test sets are very similar, which indicates that training splits were suited, i.e., have an identical distribution as the test set. The deeper model with dropout (run #3) achieved the best result (65.31% accuracy) taking ≈ 6 hours to train. The exact same model with the same configuration but without dropout (run #2) took almost 6 hours to train and achieved approximately less 3% accuracy. Therefore, it is clear that dropout does contribute to performance improvements. From the results table, we can also see that the run #1 got almost as good results as run #3 despite the fact that its model is not as deep, taking ≈ 4 hours to train. This is an indication that we are probably observing overfitting on both models.</p><p>To assess this issue we focused on the best performing run model (#3). We plotted in the y-axis the error obtained in the training and validation splits across the 500 epochs (x -axis) for the model used in our best run (#3). The plot is shown in figure <ref type="figure" coords="10,202.70,524.92,3.87,8.80">5</ref>. The error curves are very noisy but it is visible that the model is able to gradually converge towards smaller errors until it starts overfitting. We believe that using a low learning rate (0.005) was crucial to achieve this convergence.</p><p>Although this model uses both dropout and data augmentation techniques, which help regularising the model, it is clear that overfitting behaviour can be verified from epoch 100. The training dataset class skewness and size contribute to this behaviour. Therefore, a different approach than the one we used for this subtask is needed to deal with overfitting and help the model generalise. The remaining models also suffer from overfitting.</p><p>Regarding run #4 which uses the P ReLU -CNN model, it achieved the third best result on the test set, with an accuracy of 63.8%, outperforming model V GG 1 -CNN, despite being a less deeper model and taking significantly more time to train (≈ 21 hours). This is very likely a consequence of using both PReLUs and batch normalisation techniques, although in order to confirm this fact, additional experiments using a network with the same depth as the ones from models V GG 1 -CNN and V GG 2 -CNN are required such that models are comparable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and Future Work</head><p>In this paper we developed three deep learning models for tackling the subfigure medical modality classification subtask. We performed a set of experiments using CNNs, which excel with general images in scenarios where large amounts of data are available but not with small and highly unbalanced datasets. Our objective was to assess their effectiveness in classifying medical images with respect to this task challenging scenario. A set of state-of-the-art techniques for dealing with overfitting in CNNs were used in the models developed. Our best model achieved an accuracy of 65.30% using only training data provided, which we believe its a good result given the challenge, making CNNs a promising tool for medical image modality classification. We observed severe overfitting in our models despite our efforts in reducing it, therefore, a different approach is needed. Namely, one technique would be to modify the batch construction and ensure that batches are constructed by sampling from the dataset with all the 30 classes being uniformly distributed. The key idea is that the probability of each batch having a sample from a given class is the same for all classes, possibly reducing overfitting on the most dominant classes.</p><p>A better hyperparameters tuning protocol must be used so that we understand the true influence of each hyperparameter in the results achieved. Additionally, we may be losing important properties of images by not using all the three color channels, therefore our experiments should be repeated with the input of all the models being an RGB image.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="12,142.95,323.61,337.64,7.92;12,151.52,334.57,133.14,7.92" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<title level="m" coord="12,379.83,323.61,100.76,7.92;12,151.52,334.57,60.72,7.92">Deep learning. Book in preparation for</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.95,346.20,337.64,7.92;12,151.52,357.16,329.08,7.92;12,151.52,368.12,218.62,7.92" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,151.52,357.16,189.62,7.92">Overview of the ImageCLEF 2016 medical task</title>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Seco</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roger</forename><surname>Schaer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefano</forename><surname>Bromuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,360.49,357.16,120.11,7.92;12,151.52,368.12,144.31,7.92">Working Notes of CLEF 2016 (Cross Language Evaluation Forum)</title>
		<imprint>
			<date type="published" when="2016-09">September 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.95,379.75,337.64,7.92;12,151.52,390.71,329.07,7.92;12,151.52,401.67,329.07,7.92;12,151.52,412.63,105.86,7.92" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,297.82,379.75,182.77,7.92;12,151.52,390.71,111.40,7.92">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName coords=""><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,293.55,390.71,187.04,7.92;12,151.52,401.67,205.60,7.92">Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS&apos;10)</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics (AISTATS&apos;10)</meeting>
		<imprint>
			<publisher>Society for Artificial Intelligence and Statistics</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.95,424.25,337.64,7.92;12,151.52,435.21,261.54,7.92" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="12,391.24,424.25,89.35,7.92;12,151.52,435.21,83.90,7.92">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01497</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,142.95,446.84,337.64,7.92;12,151.52,457.80,329.08,7.92;12,151.52,468.76,329.07,7.92;12,151.52,479.72,182.57,7.92" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,407.26,446.84,73.34,7.92;12,151.52,457.80,291.50,7.92">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName coords=""><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,461.78,457.80,18.82,7.92;12,151.52,468.76,258.21,7.92">2015 IEEE International Conference on Computer Vision, ICCV 2015</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">December 7-13, 2015. 2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.95,491.35,337.63,7.92;12,151.52,502.31,329.08,7.92;12,151.52,513.27,329.07,7.92;12,151.52,524.22,128.32,7.92" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,300.95,491.35,179.63,7.92;12,151.52,502.31,205.97,7.92">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName coords=""><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,382.12,502.31,98.47,7.92;12,151.52,513.27,244.83,7.92">Proceedings of the 32nd International Conference on Machine Learning, ICML 2015</title>
		<meeting>the 32nd International Conference on Machine Learning, ICML 2015<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07-11">6-11 July 2015. 2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.95,535.85,337.64,7.92;12,151.52,546.81,329.07,7.92;12,151.52,557.77,329.07,7.92;12,151.52,568.73,318.53,7.92" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,373.33,546.81,107.26,7.92;12,151.52,557.77,329.07,7.92;12,151.52,568.73,102.96,7.92">Evaluating performance of biomedical image retrieval systems -an overview of the medical image retrieval task at ImageCLEF 2004-2014</title>
		<author>
			<persName coords=""><forename type="first">Jayashree</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sameer</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,262.96,568.73,178.81,7.92">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.95,580.36,337.64,7.92;12,151.52,591.32,329.07,7.92;12,151.52,602.28,329.07,7.92;12,151.52,613.24,329.07,7.92;12,151.52,624.19,93.21,7.92" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,370.59,580.36,110.00,7.92;12,151.52,591.32,284.38,7.92">Automatic image modality based classification and annotation to improve medical image retrieval</title>
		<author>
			<persName coords=""><forename type="first">Jayashree</forename><surname>Kalpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">-</forename><surname>Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">R</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,455.86,591.32,24.73,7.92;12,151.52,602.28,329.07,7.92;12,151.52,613.24,161.37,7.92">MED-INFO 2007 -Proceedings of the 12th World Congress on Health (Medical) Informatics -Building Sustainable Health Systems</title>
		<meeting><address><addrLine>Brisbane, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-08-24">20-24 August, 2007. 2007</date>
			<biblScope unit="page" from="1334" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.95,635.82,337.63,7.92;12,151.52,646.78,329.07,7.92;12,151.52,657.74,210.48,7.92" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,389.07,635.82,91.51,7.92;12,151.52,646.78,161.74,7.92">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,333.66,646.78,146.93,7.92;12,151.52,657.74,109.92,7.92">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
	<note>NIPS 2012</note>
</biblStruct>

<biblStruct coords="13,142.61,120.62,337.98,7.92;13,151.52,131.58,329.07,7.92;13,151.52,142.54,329.07,7.92;13,151.52,153.49,294.87,7.92" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,267.66,142.54,208.73,7.92">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName coords=""><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,151.52,153.49,200.23,7.92">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,164.45,337.98,7.92;13,151.52,175.41,234.67,7.92" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="13,327.39,164.45,153.20,7.92;13,151.52,175.41,114.42,7.92">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>ICLR, abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,186.37,337.98,7.92;13,151.52,197.33,329.07,7.92;13,151.52,208.29,217.66,7.92" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="13,214.17,197.33,262.77,7.92">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName coords=""><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,151.52,208.29,83.58,7.92">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014-01">January 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,219.25,337.98,7.92;13,151.52,230.21,329.08,7.92;13,151.52,241.17,329.07,7.92;13,151.52,252.12,190.49,7.92" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="13,452.23,219.25,28.36,7.92;13,151.52,230.21,239.38,7.92">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,409.34,230.21,71.25,7.92;13,151.52,241.17,267.03,7.92">Proceedings of the 30th International Conference on Machine Learning, ICML 2013</title>
		<meeting>the 30th International Conference on Machine Learning, ICML 2013<address><addrLine>Atlanta, GA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06-21">16-21 June 2013. 2013</date>
			<biblScope unit="page" from="1139" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,263.08,337.98,7.92;13,151.52,274.04,329.07,7.92;13,151.52,285.00,329.07,7.92;13,151.52,295.96,20.99,7.92" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,456.47,274.04,24.12,7.92;13,151.52,285.00,99.10,7.92">Going deeper with convolutions</title>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,270.60,285.00,204.78,7.92">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,306.92,337.98,7.92;13,151.52,317.88,291.60,7.92" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="13,307.79,306.92,172.80,7.92;13,151.52,317.88,111.90,7.92">A Python framework for fast computation of mathematical expressions</title>
		<author>
			<persName coords=""><forename type="first">Theano</forename><surname>Development</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Team</forename><surname>Theano</surname></persName>
		</author>
		<idno>arXiv e-prints, abs/1605.02688</idno>
		<imprint>
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,328.84,337.98,7.92;13,151.52,339.80,329.08,7.92;13,151.52,350.75,329.07,7.92;13,151.52,361.71,329.07,7.92;13,151.52,372.67,329.08,7.92;13,151.52,383.63,196.29,7.92" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="13,461.07,361.71,19.52,7.92;13,151.52,372.67,211.90,7.92">General Overview of ImageCLEF at the CLEF 2016 Labs</title>
		<author>
			<persName coords=""><forename type="first">Mauricio</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roger</forename><surname>Schaer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefano</forename><surname>Bromuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Josiah</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fei</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arnau</forename><surname>Ramisa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emmanuel</forename><surname>Dellandrea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alejandro</forename><forename type="middle">H</forename><surname>Toselli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joan-Andreu</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Enrique</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="13,371.20,372.67,109.39,7.92;13,151.52,383.63,27.77,7.92">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,394.59,337.98,7.92;13,151.52,405.55,329.07,7.92;13,151.52,416.51,329.07,7.92;13,151.52,427.47,20.99,7.92" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="13,266.26,394.59,214.33,7.92;13,151.52,405.55,329.07,7.92;13,151.52,416.51,211.27,7.92">Berberine chloride can ameliorate the spatial memory impairment and increase the expression of interleukin-1beta and inducible nitric oxide synthase in the rat model of alzheimer&apos;s disease</title>
		<author>
			<persName coords=""><forename type="first">Feiqi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Caiyun</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,369.88,416.51,73.31,7.92">BMC neuroscience</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">78</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
