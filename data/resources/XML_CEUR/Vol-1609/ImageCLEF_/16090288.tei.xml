<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,145.32,142.31,320.82,11.85;1,165.96,158.75,279.84,11.85;1,277.56,175.07,56.37,11.85">Using Machine Learning Techniques, Textual and Visual Processing in Scalable Concept Image Annotation Challenge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,143.76,211.12,98.57,8.48"><forename type="first">Alexandru-Gabriel</forename><surname>Cristea</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UAIC</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,248.52,211.12,91.92,8.48"><forename type="first">Mădălin-Marian</forename><surname>Savoaia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UAIC</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,346.80,211.12,120.79,8.48"><roleName>Ionela</roleName><forename type="first">Monica-Andreea</forename><surname>Martac</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UAIC</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,144.96,222.04,53.69,8.48"><forename type="first">Cristina</forename><surname>Pătraș</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UAIC</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,204.72,222.04,98.30,8.48"><forename type="first">Alexandru-Ovidiu</forename><surname>Scutaru</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UAIC</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,309.24,222.04,100.40,8.48"><forename type="first">Constantin-Emilian</forename><surname>Covrig</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UAIC</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,416.04,222.04,50.35,8.48"><forename type="first">Adrian</forename><surname>Iftene</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UAIC</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,145.32,142.31,320.82,11.85;1,165.96,158.75,279.84,11.85;1,277.56,175.07,56.37,11.85">Using Machine Learning Techniques, Textual and Visual Processing in Scalable Concept Image Annotation Challenge</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">77D864690F909F9029F952A469555FE0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Machine learning</term>
					<term>Text processing</term>
					<term>Visual Processing</term>
					<term>Text Generation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes UAIC 1 's system built for participating in the Scalable Concept Image Annotation challenge 2016. We submitted runs for Subtask 1 (Image annotation and localisation), for Subtask 2 (Natural language caption generation) and for Subtask 3 (Content Selection). For the first subtask we used an ontology created last year with relations between concepts and their synonyms, hyponyms and hypernyms and also with relations between concepts and related words, but additional we used another resources and machine learning techniques. For the second subtask, we created a resource that contains triplets (concept1, verb, concept2), where concepts are from the list of concepts provided by the organizers and verb is a relation between concepts. With this resource we build sentences in which concept1 is subject, verb is predicate and concept2 is complement. For Subtask 3, we transform the input file in a form used by Subtask 2 and then we used the component developed by our team for Subtask 2.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In 2016, UAIC group participated again in CLEF labs in few ImageCLEF tasks <ref type="bibr" coords="1,457.32,512.56,10.91,8.48" target="#b0">[1]</ref> and in this way we continued our previous participation from 2013 when we participated in Plant Identification task <ref type="bibr" coords="1,298.08,534.16,11.03,8.48" target="#b1">[2]</ref> and from 2015 when we participated in Scalable Annotation task <ref type="bibr" coords="1,240.00,544.96,10.12,8.48" target="#b2">[3]</ref>. Like in the 2015 campaign, the Scalable Concept Image Annotation challenge (from Image CLEF 2016 -Image Annotation 2 ) task in 2016 aims to develop systems that receive as input an image and produce as output a prediction of which concepts are present in that image, selected from a predefined list of concepts. Similar to last year, the participants must describe images, localize the different concepts in the images and generate a description of the scene. This year the task was composed by two subtasks using a data source with 510,123 web page items (Subtask 1 and Subtask 2) and a data source with 10,003 entries for Subtask 3. For each item we have a corresponding web page, an image and the keywords extracted from the web page. The participants must annotate and localize concepts and/or generate sentence descriptions for all 510,123 items. More details about challenge from 2016 are in <ref type="bibr" coords="2,208.44,183.76,11.03,8.48" target="#b3">[4]</ref> and details about challenge from 2015 are in <ref type="bibr" coords="2,391.56,183.76,10.03,8.48" target="#b4">[5]</ref>.</p><p>In 2014, three teams <ref type="bibr" coords="2,240.48,194.44,48.95,8.48">[6, 7 and 8]</ref> based their system on Convolutional Neural Networks (CNN) pre-trained using ImageNet <ref type="bibr" coords="2,321.00,205.24,10.12,8.48" target="#b8">[9]</ref>. Also, most of the teams proposed approaches based on classifiers that need to be learned <ref type="bibr" coords="2,356.88,216.16,11.03,8.48" target="#b5">[6]</ref> or based on classification with constructed ontologies <ref type="bibr" coords="2,256.68,226.96,14.54,8.48" target="#b9">[10]</ref>. In 2015, all top 4 groups used CNNs in their pipeline for the feature description <ref type="bibr" coords="2,275.52,237.76,10.03,8.48" target="#b4">[5]</ref>.</p><p>The rest of the paper is structured as follows: Section 2 details the general architecture of our system, Section 3 presents the results and an error analysis, while the last Section discusses the conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System components</head><p>In 2016, UAIC submitted runs for image annotation and localisation (Subtask 1), for generation of a textual description of an image (Subtask 2) and for generation of a textual description avoiding processing of images (Subtask 3). For that, we built a system, consisting in modules specialized for text processing and visual processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Subtask 1 -Textual processing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Google Translate</head><p>This module is used for translating non-English words from the initial file into English for the purpose of textual concept identification. The motivation for this component is related to our built resources, which are presented in the next sections: these resources are only for English. It is using the Google Translation API v2 service <ref type="foot" coords="2,169.80,491.60,2.81,5.11" target="#foot_2">3</ref> , a file with English words and a cache file. The file with English words contains 363,802 words and we use an AVL Tree for storing them in memory to have O(log n) access time thus not wasting time. The cache file contains pairs of words in a foreign language and their English translation, for loading it in memory we use a HashMap which has a theoretical access time of O(1).</p><p>For translating a word from the initial file, we consider the following cases:</p><p>• Case 1: If the current word is found in the AVL Tree with English words then the program uses the word as it is and seeks to the next one from the initial file. If the word is not found then it will search for it in the cache file (Case 2). • Case 2: If the current word is in the HashMap cache with translated words then the program uses the translated form and seeks to the next one from the initial file. If not it will call the Google Translate API (Case 3).</p><p>• Case 3: The program will request a translation from the Google Translate service with auto-detect as source language and English as target language and then it will use the returned result and it will also add it to the HashMap cache and file cache.</p><p>Example for Case 3 (the detected language is Spanish):</p><p>Other language: ... At the end of the translation process the cache file contained 935,827 pairs (initial_word, translated_word) and the entire process took around 19 hours to finish from an empty cache file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Stop-words Elimination</head><p>This component receives a file with 510,123 lines and tries to remove every stopword from every line with its associated number which represents its frequency. We consider additional elimination of classical stop-words (the, from, it, he, be, is, has, …) and the elimination of all words with one or two characters. We consider for stopwords a file with 667 entries.</p><p>For example, for input line: In the end, from a total of 45,771,971 words in the input file, 17,613,121 stop words were eliminated. From 2 files summing up 480.7 Mb we obtained one file with 339.6 Mb.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Concept Identification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our ontology</head><p>At this step, we start to build an ontology with relations between the initial 250 concepts and related words to them (this file is based on similar file build by us in 2015 <ref type="bibr" coords="4,165.24,194.44,9.90,8.48" target="#b2">[3]</ref>). For that, we used the WordNet<ref type="foot" coords="4,305.40,194.24,2.81,5.11" target="#foot_3">4</ref>  <ref type="bibr" coords="4,311.52,194.44,15.59,8.48" target="#b10">[11]</ref> and we extracted in average around three synonyms and an average of five words that are somehow related to the concepts (automatically extracted from WordNet (hyponyms or hypernyms) and manually verified by human annotators or manually added by human annotators).</p><p>For example, for the concept airplane we have the following information: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DuckDuckGo Ontology</head><p>Because, our ontology is still limited, in terms of concept, synonyms and relations between them (the ontology was built semi-manually, and it depends by the annotators experience and quality of work), we decided to build automatically another ontology. At this step, we start to build an ontology with related words to the initial 250 concepts. For that we used www.DuckDuckGo.com.</p><p>For example, for the concept apple we have the following information:</p><p>Synonyms and lexical family: cider, russet, crab, pear, stayman, fruit, quince, banana, lemon, mango, melon, orange, peach, berry, cherry, grape, plum, strudel, blackberry, currant, pomegranate, pumpkin, raspberry, strawberry, apples, almond, blueberry, carrot, cocoanut, coconut, cranberry, glacier, lemons, malus pumila, oranges, orchard apple tree, peaches, pears, potato, potatoes, turnips, alar, core, tree, eve, mac, winesap, eris, mom, pie</p><p>With this files we execute on the processed file with 510.123 lines (after steps 2.1.1 and 2.1.2) a module which identifies related concepts for every line. For that, for each word from a line, we try to find a way to connect it to concepts. That implies searching for it in the list of concepts (case 1) or in our ontology (case 2) or in DuckDuckGo ontology (case 3). If a match is found, the word is replaced with its related concept and placed in the output file along with its initial number (case 1 and case 2) or with a lower value (case 3). All words that could not be associated with any concept have been eliminated along with their number. At the next step, we sum the frequencies for the same concept for current line. After that we put in output, for current line, the initial ID and 251 bits corresponding to the list of concepts, where every bit 1 means that the corresponding concept is identified.</p><p>For example, for the following input line: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The output looks like:</head><p>000bRjJGbnndqJxV 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 ... Regarding the infrastructure used by our group, firstly, all programs were run on 2core i5 Intel processors and it's took around 500 minutes. Secondly, all programs were run distributed on 5 different computers, all of them with a similar configurations, and the execution time was in average under 200 minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Subtask 1 -Visual Processing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Face recognition</head><p>On every computer used by our team, we downloaded the archived images indexed and hashed based on the source site. For the Face Recognition part we ran the old JJIL algorithm written in Java. The results of this algorithm were either merged with the results from the textual / translation processing module or used individually with the Machine Learning Object Recognition which we are going to talk about later. For example for image from Fig. <ref type="figure" coords="6,279.96,140.44,3.45,8.48" target="#fig_1">1</ref>, the output after we use the JJIL API is: n05600637 0.7:225x90+300+300, 0.7:855x420+300+300, 0.7:960x120+240+240, 0.7:960x120+200+200</p><p>After obtaining the path, we wrote the subtask ID (1), the ID of the Image and the result based on the Textual Processing / Translation combined (or not) with the Face Recognition API and the Machine Learning Object Recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Final output line example:</head><p>1 J41e3GF7p7rGdsj4 n02709367 0.7:0x0+640+427 n05563770 0.7:0x0+640+427 n02774152 0.7:0x0+640+427 ... Downloading the 510,123 images took 3-4 hours for each computer. Then, the face recognition algorithm was executed on 13 computers for sets of 30,000 -40,000 images to reduce the total duration. The face recognition API needs approximately 1 -2 seconds for each image to identificate faces and to write the result in the output file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Object recognition with Machine Learning</head><p>Object Recognition with Machine Learning was achieved through the API TensorFlow <ref type="foot" coords="6,188.64,509.48,2.81,5.11" target="#foot_4">5</ref> . It is an API written in Python for Linux based systems that has 1,000 inner concepts created through Neural Network learning. We mapped semiautomatically these 1,000 concepts by finding a logical "connection" between them and the ImageClef's 251 concepts. The mapped file (JSON key:value) contained the TensorFlow's concepts (string) and the ImageClef's concepts ID represented by the "line position -1" . (line position = the position of the ID in the official concepts list).</p><p>Example: "fireboat":30, "gondola":30, "speedboat":30,</p><p>The "30" represents n02858304 boat.n.01. After the mapping part, we used a Virtual Machine with a Linux Based OS to run the API that had sets of 30,000 -50,000 images and the mapped JSON as input. The algorithm analysed the Images and the JSON and generated an intermediate output file which contained on each line the Image ID followed by 251 characters of 1 or 0 separated by space. (if the API recognised any of its concepts in the Image, the character was 1; else, 0).</p><p>The Python script could use from 1 to 10 threads for faster execution and it took about 50 hours for a set of 50,000 images. After obtaining and mixing/ordering the parts of the intermediate results, we combined the intermediate results (ours and the textual processing one) through logical OR (1 OR 0 = 1, 0 OR 0 = 0 …), using a fast C program written by us, and afterwards we generated the final output files by using the Java algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Subtask 2 -Text Generation</head><p>For subtask 2 we are given an input file containing 510,123 lines, each of them respecting the following pattern: The line begins with the image's code, followed by a space character and after that by a list of 251 values of 0 or 1, separated by space. Each value of 0/1 from the list corresponds to a concept from the list of concepts given by the organizers: a value of 1 means that the concept belongs to appears in that image, while a value of 0 means that the concept is not in the photo.</p><p>We grouped the 251 concepts into 43 categories. For these categories, we built manually, with human annotators, a matrix that gives us the most suitable verb for linking every possible pair of 2 categories (if a suitable verb exists for 2 given categories). With all these resources we built sentences with form: (concept1, verb, concept2), in which concept1 is subject, verb is predicate and concept2 is complement.</p><p>For example, in our matrix are the following types of triplets:</p><p>-Body_part -wearing -accessories; -Animal -drinking -drink; -insect -in on -land vehicle; • -animal -near -man made object; -animal -playing -sport_item_or_toy. First of all, we read the entire input file and we map each line into an object containing the image's code and an array with the concepts which are present in that image. Then for every line we consider the following cases: Case 1 -the matrix contains verbs that link the concepts in the image. In this case, a phrase that describes an image will contain maximum 3 sentences in form (concept1, verb, concept2).</p><p>Case 2 -the matrix doesn't contain verbs that link the concepts in the image. We will build sentences with the following format: "The image contains"/ "has"/ "includes" + concept", so that a final output phrase for an image looks like: "The image contains an X, and has a Y and includes a Z.", where X, Y and Z are concepts from that image. If there are duplicates among the concepts that will appear in the phrase describing the image, we will compress the phrase into:</p><p>-The image contains 3 X's.</p><p>-if X = Y = Z -The image contains 2X's and the image has a Z. -if X = Y ≠ Z. And so on.</p><p>Case 3 -the image has not concepts. We return "Empty sentence".</p><p>Finally, the output for this subtask will look like:</p><formula xml:id="formula_0" coords="8,143.28,204.96,91.35,7.22">2 000qUQAfomr0QAm4</formula><p>The ribbon is on a letter, the letter is near a bag and the house is near a garden. 2 dZ29Q2T0HBwvNQIi The image contains a telephone, has a cap and includes a beach. 2 00hMe3sGLZSXzJKH The image contains 1 temple and 1 fan.</p><p>where 2 is the prefix identifying the current subtask, followed by image's code and the phrase generated by the algorithm. In this way, we have improved the rate of success in creating sentences -only the images with no concepts will cause an "Empty sentence" description.</p><p>The program used was single threaded and took about 11 hours to complete all 510,123 input lines on a dual core processor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Subtask 3 -Content Selection</head><p>The input for Subtask 3 consist of a file having 10,003 lines, with data for 450 images. After parsing the input given, we obtained (as in the precedent subtask) a list of images, each image having a list of concepts that appears in that image. From this point on, we used the sentence generation algorithm from Subtask 2 to form phrases for each of 450 images. In order to select the triplets (concept1, verb, concept2), we consider the combination between concepts with highest frequency. Then, we consider maximum 4 triplets, in cases when we have a verb in our matrix, for considered concepts. The program used was single threaded and took about 15 minutes to run on a dual core processor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results and Evaluation</head><p>For the 2016 task, our team submitted 7 runs for Subtask 1, 2 runs for Subtask 2 and 1 run for Subtask 3 <ref type="bibr" coords="8,217.80,529.36,10.12,8.48" target="#b3">[4]</ref>. The description and duration for every run is presented in bellow Table. Based on visual processing using only the Face Recognition algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">hour</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run 2</head><p>Based on the textual processing for the Ontology 1, 2 and the Translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="27">hours</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run 3</head><p>Obtained through the Face Recognition and Machine Learning Object Recognition APIs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Description Duration Run 4</head><p>Mixed visual processing / textual processing output. The visual part has been made with the Face Recognition API, Machine Learning Obj. Rec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">days</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run 5</head><p>Textual processing output using our ontology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">hours</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run 6</head><p>Textual processing output using our ontology and translation 27 hours Run 7</p><p>Textual processing output using DuckDuckGo ontology 7 hours</p><p>The translation part of the project has been made possible by the Google Translation Service <ref type="foot" coords="9,216.84,235.64,2.81,5.11" target="#foot_5">6</ref> . The Face Recognition algorithm we used is the old one revised / optimized to analyse the images faster and safer. TensorFlow Script<ref type="foot" coords="9,400.44,246.44,2.81,5.11" target="#foot_6">7</ref> was written so it can use multiple threads. It also had a "recheck and continue" option. This was useful in case that the Virtual Machine broke / didn't respond well so the script checked the old result saved before being closed by force and continued it.</p><p>The analysation took most of our time and computing power because it ran in a Virtual Machine. The fact that it is Python based made it even slower. Afterwards, mixing the results between Tensor Flow and Textual Processing took some more time because the lines in the intermediate output files were not in the same order so we used Linux's bash commands for sets difference, sorting, cut-in etc. to fix the problem.</p><p>Finally, the final output files were obtained running the Java algorithm with or without the Face Recognition option and later we validated them with the official bash script. In the case of Subtask 2, we submitted two runs with different files like input from Subtask 1. We consider for input files two cases: Run 2 and Run 4 from Subtask 1. For Subtask 3 we submitted Run 10, which took around 15 minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Evaluation for Subtask 1</head><p>Table <ref type="table" coords="9,168.00,532.24,4.67,8.48" target="#tab_5">3</ref> below gives the results for the runs from Subtask 1 described above. More details are in <ref type="bibr" coords="9,193.56,543.16,10.12,8.48" target="#b3">[4]</ref>. As we can see from Table <ref type="table" coords="10,257.76,140.44,3.45,8.48" target="#tab_5">3</ref>, the R1, R3 and R4 have the same results. We didn't add to this table the rest of runs, because the values for them are 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation for Subtask 2 and Subtask 3</head><p>Tables <ref type="table" coords="10,170.76,203.92,4.67,8.48" target="#tab_6">4</ref> and<ref type="table" coords="10,194.04,203.92,4.67,8.48" target="#tab_7">5</ref> from below give the results for the runs from Subtask 2noisy track and Subtask 2 clean track described above. We can see how R9 is better than R8. For R10, we remark positive the good value for precision, but also we remark negative the lower value for recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>This document details the system developed by the UAIC team for the 2016 edition of ImageClef's contest on Image Annotation (Face Recognition). The entire system has modules for each of the 3 subtasks: Concepts Detection and Annotation (Subtask 1), Natural Language Generation (Subtask 2) and Bounding Boxes Concepts Identification (Subtask 3). The Subtask 1 is the main component of the system and consists in textual processing of each website's content (which also means translating words that are not in English) and concept identification, and also visual processing. This one represents Face Recognition and Object Recognition through Machine Learning.</p><p>For Subtask 2 and for Subtask 3, we used a matrix built by us, with relations between concepts in form (concept1, verb, concept2), in which concept1 is subject, verb is predicate and concept2 is complement. When concepts cannot be linked using this matrix, we use a generic phrase in which the concepts are enumerated.</p><p>For further development of the system we think that finding APIs and adapting them in a more thoughtful and efficient manner to identify concepts is an important target. We should aim to have more concepts identified more precisely for each image. As for the translation part we think that the Google Translation Service works well enough for the moment, even though it takes some time to finish "its jobs".</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,143.28,183.36,325.11,7.22;5,143.28,192.96,325.11,7.22;5,143.28,202.56,274.11,7.22"><head></head><label></label><figDesc>000bRjJGbnndqJxV 75 timepiece 7988 time 3252 thesauru 1595 device 1569 timepiece 1513 measuring 1286 instrument 1285 clock 1268 wheel 1232 noun 1170 balance 1162 legend 1105 ...</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,224.64,644.79,162.17,7.70;5,158.88,433.32,293.88,203.64"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An example of image that contains faces</figDesc><graphic coords="5,158.88,433.32,293.88,203.64" type="vector_box" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,143.28,258.72,325.11,74.66"><head></head><label></label><figDesc>Synonyms and lexical family: bombardier, landing, helicopter, jet, flight, aircraft, cabin, flying, airliner, pilot, gunner, blimp, airport, wing, fighter, terminal, hangar, cockpit,</figDesc><table coords="4,143.28,287.76,324.99,45.62"><row><cell cols="2">flotilla,</cell><cell cols="2">maneuverable,</cell><cell>passenger,</cell><cell>alien,</cell><cell>fly,</cell><cell>overhead,</cell></row><row><cell cols="7">airborne, carrier, albatross, tank, hijacking, aerospace,</cell></row><row><cell cols="7">refuel, immigrate, aviator, fledged, airman, levitate, mobile,</cell></row><row><cell>moth,</cell><cell cols="2">gull,</cell><cell>ballooning,</cell><cell>hover,</cell><cell cols="2">stewardess,</cell><cell>spacecraft,</cell></row><row><cell cols="7">butterfly, sortie, pterosaur, volant, submarine, footloose.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,163.32,562.23,290.18,34.84"><head>Table 1 :</head><label>1</label><figDesc>Description of runs for Subtask 1</figDesc><table coords="8,163.32,579.15,290.18,17.92"><row><cell>Description</cell><cell>Duration</cell></row><row><cell>Run 1</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,148.92,398.31,315.37,45.40"><head>Table 2 :</head><label>2</label><figDesc>Description of runs for Subtask 2</figDesc><table coords="9,148.92,415.23,315.37,28.48"><row><cell></cell><cell>Description</cell><cell>Duration</cell></row><row><cell>Run 8</cell><cell cols="2">Based on Cases 1, 2, 3 presented in 2.3, using like input file the Run 2 11 hours</cell></row><row><cell>Run 9</cell><cell>Similar to Run 8, with input file Run 4</cell><cell>11 hours</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,174.84,564.99,262.46,45.38"><head>Table 3 :</head><label>3</label><figDesc>Results of UAIC's runs from Subtask 1</figDesc><table coords="9,174.84,581.91,262.46,28.46"><row><cell>% Overlap with GT labels</cell><cell>R1</cell><cell>R3</cell><cell>R4</cell></row><row><cell>50 %</cell><cell>0.001526</cell><cell>0.001526</cell><cell>0.001526</cell></row><row><cell>0 %</cell><cell>0.00281</cell><cell>0.00281</cell><cell>0.00281</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="10,223.32,236.67,165.02,76.70"><head>Table 4 :</head><label>4</label><figDesc>Results of UAIC's runs from Subtask 2</figDesc><table coords="10,241.32,253.59,137.30,59.78"><row><cell></cell><cell>R8</cell><cell>R9</cell></row><row><cell>MEAN</cell><cell>0.0896</cell><cell>0.0934</cell></row><row><cell>STDDEV</cell><cell>0.0297</cell><cell>0.0249</cell></row><row><cell>MEDIAN</cell><cell>0.0870</cell><cell>0.0915</cell></row><row><cell>MIN</cell><cell>0.0161</cell><cell>0.0194</cell></row><row><cell>MAX</cell><cell>0.2230</cell><cell>0.2514</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="10,223.32,327.99,165.02,55.94"><head>Table 5 :</head><label>5</label><figDesc>Results of UAIC's runs from Subtask 3</figDesc><table coords="10,247.20,344.91,121.70,39.02"><row><cell></cell><cell>R10</cell></row><row><cell>Mean F</cell><cell>0.4982 +-0.1782</cell></row><row><cell>Mean P</cell><cell>0.4597 +-0.1553</cell></row><row><cell>Mean R</cell><cell>0.5951 +-0.2592</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,148.32,635.10,175.83,7.66"><p>University "Alexandru Ioan Cuza" of Iasi, Romania</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,148.32,644.94,279.74,7.66"><p>ImageCLEF 2016 -Image Annotation: http://www.imageclef.org/2016/annotation</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,148.32,644.94,252.04,7.66"><p>Google Translation service: https://cloud.google.com/translate/v2/libraries</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,148.32,644.94,188.66,7.66"><p>WordNet: http://wordnetweb.princeton.edu/perl/webwn</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="6,148.32,644.94,313.03,7.66"><p>Tensor Flow Script: https://www.tensorflow.org/versions/r0.8/tutorials/deep_cnn/index.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="9,148.32,635.10,229.00,7.66"><p>Google Translation service: https://cloud.google.com/translate/docs</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="9,148.32,644.94,313.03,7.66"><p>Tensor Flow Script: https://www.tensorflow.org/versions/r0.8/tutorials/deep_cnn/index.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. Special thanks go to all colleagues from the <rs type="institution">Faculty of Computer Science</rs>, second year, group B2, who were involved in this project.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="11,146.43,245.10,322.00,7.66;11,160.20,254.70,308.23,7.66;11,160.20,264.42,308.11,7.66;11,160.20,274.26,307.97,7.66;11,160.20,283.98,114.41,7.66" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,341.16,264.42,127.15,7.66;11,160.20,274.26,76.30,7.66">General Overview of ImageCLEF at the CLEF 2016 Labs</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Henning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Stefano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ramisa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dellandrea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Toselli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="11,369.00,274.26,99.17,7.66;11,160.20,283.98,24.84,7.66">Lecture Notes in Computer Science</title>
		<idno type="ISSN">0302-9743</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,146.44,293.70,321.83,7.73;11,160.20,303.42,308.30,7.66;11,160.20,313.02,308.23,7.66;11,160.20,322.86,192.41,7.66" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,430.20,293.70,38.06,7.66;11,160.20,303.42,308.30,7.66">Combining image retrieval, metadata processing and naive Bayes classification at Plant Identification</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Șerban</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sirițeanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gheorghiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Iftene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Alboaie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Breabăn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,184.20,313.02,284.23,7.66;11,160.20,322.86,44.42,7.66">Notebook Paper for the CLEF 2013 LABs Workshop -ImageCLEF -Plant Identification</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-09">2013. September. 2013</date>
			<biblScope unit="page" from="23" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,146.44,332.58,322.00,7.66;11,160.20,342.30,308.07,7.66;11,160.20,352.02,308.19,7.66;11,160.20,361.74,251.45,7.66" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,235.08,342.30,233.19,7.66;11,160.20,352.02,74.08,7.66">Using Textual and Visual Processing in Scalable Concept Image Annotation Challenge</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Calfa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Silion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">P</forename><surname>Acatrinei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">I</forename><surname>Lupu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">E</forename><surname>Cozma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Pădurariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Iftene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,251.88,352.02,216.51,7.66;11,160.20,361.74,123.04,7.66">Working Notes of CLEF 2015 -Conference and Labs of the Evaluation forum -ImageCLEF2015</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="volume">1391</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,146.43,371.46,322.00,7.66;11,160.20,381.18,308.19,7.66;11,160.20,390.90,286.85,7.66" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,264.72,381.18,203.67,7.66;11,160.20,390.90,55.02,7.66">Overview of the ImageCLEF 2016 Scalable Concept Image Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ramisa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dellandrea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,221.16,390.90,90.02,7.66">CLEF2016 Working Notes</title>
		<title level="s" coord="11,316.92,390.90,101.80,7.66">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,146.43,400.74,322.00,7.66;11,160.20,410.46,307.99,7.66;11,160.20,420.06,308.27,7.66;11,160.20,429.78,308.23,7.66;11,160.20,439.62,80.45,7.66" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,227.40,410.46,240.79,7.66;11,160.20,420.06,154.69,7.66">Overview of the ImageCLEF 2015 Scalable Image Annotation, Localization and Sentence Generation task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dellandrea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,334.68,420.06,133.79,7.66;11,160.20,429.78,79.36,7.66">CLEF2015 Working Notes -CEUR Workshop Proceedings</title>
		<title level="s" coord="11,247.44,429.78,67.62,7.66">Publisher CEUR-WS</title>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">September 8-11. (2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,146.43,449.34,321.83,7.66;11,160.20,459.06,308.23,7.66;11,160.20,468.78,22.49,7.66" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,255.48,449.34,212.79,7.66;11,160.20,459.06,55.38,7.66">Overview of the ImageCLEF 2014 Scalable Concept Image Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,233.52,459.06,148.54,7.66">CLEF 2014 Evaluation Labs and Workshop</title>
		<title level="s" coord="11,388.56,459.06,76.22,7.66">Online Working Notes</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,146.43,478.50,321.99,7.66;11,160.20,488.22,308.08,7.66;11,160.20,497.94,271.49,7.66" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,442.56,478.50,25.87,7.66;11,160.20,488.22,195.50,7.66">MIL at ImageCLEF 2014: Scalable System for Image Annotation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kanehira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hidaka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Mukuta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tsuchiya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,370.92,488.22,97.36,7.66;11,160.20,497.94,129.28,7.66">CLEF 2014 Evaluation Labs and Workshop, Online Working Notes</title>
		<meeting><address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">September 15-18. (2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,146.43,507.66,321.99,7.66;11,160.20,517.50,308.18,7.66;11,160.20,527.22,308.23,7.66;11,160.20,536.82,22.49,7.66" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,160.20,517.50,245.77,7.66">MindLab at ImageCLEF 2014: Scalable Concept Image Annotation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Vanegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Arevalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Otálora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Páez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Pérez-Rubiano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">A</forename><surname>González</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,425.88,517.50,42.50,7.66;11,160.20,527.22,188.92,7.66">CLEF 2014 Evaluation Labs and Workshop, Online Working Notes</title>
		<meeting><address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">September 15-18. (2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,146.43,546.54,321.99,7.66;11,160.20,556.38,308.19,7.66;11,160.20,566.10,192.65,7.66" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,310.92,546.54,157.51,7.66;11,160.20,556.38,99.87,7.66">MLIA at ImageCLEF 2014 Scalable Concept Image Annotation Challenge</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Shimada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ichiro Taniguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,281.04,556.38,187.35,7.66;11,160.20,566.10,50.42,7.66">CLEF 2014 Evaluation Labs and Workshop, Online Working Notes</title>
		<meeting><address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">September 15-18. (2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,150.37,575.82,318.14,7.66;11,160.20,585.54,308.27,7.66;11,160.20,595.26,304.34,7.66" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,381.84,575.82,86.67,7.66;11,160.20,585.54,94.76,7.66">ImageNet: A large scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2009.5206848</idno>
	</analytic>
	<monogr>
		<title level="m" coord="11,271.92,585.54,145.15,7.66;11,446.28,585.54,22.19,7.66;11,160.20,595.26,80.67,7.66">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009-06">2009. June. (2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference</note>
</biblStruct>

<biblStruct coords="11,150.37,604.98,318.06,7.66;11,160.20,614.70,22.49,7.66" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="11,211.32,604.98,148.08,7.66">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
