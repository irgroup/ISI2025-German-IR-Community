<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,151.88,115.96,311.60,12.62;1,230.57,133.89,154.22,12.62">INAOE&apos;s participation at ImageCLEF 2016: Text Illustration Task</title>
				<funder ref="#_A3Dhkz4">
					<orgName type="full">CONACyT</orgName>
				</funder>
				<funder ref="#_CARgwAt">
					<orgName type="full">CONACYT</orgName>
				</funder>
				<funder>
					<orgName type="full">Clasificación y recuperación de imágenes mediante técnicas de minería de textos</orgName>
				</funder>
				<funder ref="#_CrK2yvP">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,219.44,171.56,58.54,8.74"><forename type="first">Luis</forename><surname>Pellegrin</surname></persName>
							<email>pellegrin@inaoep.mx</email>
							<affiliation key="aff0">
								<orgName type="department">Óptica y Electrónica (INAOE)</orgName>
								<orgName type="institution">Instituto Nacional de Astrofísica</orgName>
								<address>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,285.39,171.56,105.50,8.74"><forename type="first">A</forename><surname>Pastor López-Monroy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Óptica y Electrónica (INAOE)</orgName>
								<orgName type="institution">Instituto Nacional de Astrofísica</orgName>
								<address>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,196.60,183.51,86.16,8.74"><forename type="first">Hugo</forename><forename type="middle">Jair</forename><surname>Escalante</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Óptica y Electrónica (INAOE)</orgName>
								<orgName type="institution">Instituto Nacional de Astrofísica</orgName>
								<address>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,309.87,183.51,108.88,8.74"><forename type="first">Manuel</forename><surname>Montes-Y-Gómez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Óptica y Electrónica (INAOE)</orgName>
								<orgName type="institution">Instituto Nacional de Astrofísica</orgName>
								<address>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,151.88,115.96,311.60,12.62;1,230.57,133.89,154.22,12.62">INAOE&apos;s participation at ImageCLEF 2016: Text Illustration Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1B0D71C306B0C6EC1EC6046D8D639F5C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>text illustration</term>
					<term>image retrieval</term>
					<term>document representation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we describe the participation of the Language Technologies Lab of INAOE at ImageCLEF 2016 teaser 1: Text Illustration (TI). The goal of the TI task consists in finding the best image that describes a given document query. For evaluating this task, there is a dataset containing web pages having text and images. We address the TI as a purely Information Retrieval (IR) task, for a given document query we search for the most similar web pages and use the associated images to them as illustrations. In this way, queries are used to retrieve related images from web pages, but the retrieval result are only the associated images. For this, we represent the web pages and queries using state-of-the-art text representations. Those representations incorporate information that allows us to exploit textual or semantic aspects. According to ImageCLEF 2016 evaluation, the proposed approach holds the best performance for the TI task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Since 2010, ImageCLEF promotes research into annotation of images using noisy web page data. Following the same path, for the 2016 edition <ref type="bibr" coords="1,405.46,488.75,10.52,8.74" target="#b0">[1]</ref> two new tasks were introduced as teasers: Text illustration and Geolocation, this paper focuses in the former. The goal of the Text Illustration task consists in finding the best illustration, from a set of reference images, for a given text-document. Unlike the problem of illustrating a sentence formed by few words, the TI is a much more challenging task. The reason of this is that we want to illustrate a whole document (i.e. web page) including a number of different topics. In this regard, the used dataset consists in images embedded in web pages.</p><p>We address the TI problem as an Information Retrieval (IR) task. The hypothesis is that related web pages have related images. Thus, the document queries to be illustrated are a target set of web pages, which we illustrate using the embedded images of the retrieved web pages. For this, we bring two popular representations from the IR field, that do not take into account visual characteristics of images. On the one hand, the bag-of-words representation defines each document as histograms of word occurrences. On the other hand, the Word2vec representation incorporates distributional semantics to text documents with learned word vectors <ref type="bibr" coords="2,279.10,130.95,9.96,8.74" target="#b1">[2]</ref>. Finally, as work in progress we experiment with a third novel multimodal representation, where the textual and visual information are used to produce a multimodal representation of the queries. Such representation, allows us to directly retrieve images from a reference image dataset. The official results in the evaluation are encouraging and lays the background for future avenues of inquiry.</p><p>The remainder of the paper is organized as follows. Section 2 describes our method; Section 3 shows the obtained experimental results; finally, in Section 4 some conclusions of this work are presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Text illustration using an IR-based approach</head><p>To approach the TI task we consider the following elements in our strategy. Let Q = {q 1 , . . . , q m } be the set of document queries to be illustrated. Also, let D = {(d 1 , I 1 ), . . . , (d n , I n )} be the set of web pages of d n documents and I n images pairs in the collection. Finally, let V = {w 1 , . . . , w r } be the textual features extracted from documents in the reference collection D. The general process of the proposed approach has two stages. The first consist in representing each query q j and each document d i into the same space R r . In the second stage, each query q j ∈ Q is used to retrieve the k most similar web pages {(d h , I h ) : (d h , I h ) ∈ D} to q j . The final result only considers the I h elements as the resultant illustration set. The rest of this section explains the stages in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Representing documents</head><p>The first stage in our strategy requires computing query vectors q j = w 1 , . . . , w r and document vectors d i = w 1 , . . . , w r in a space R r . For this, we relied in two different textual representations exploiting word occurrences (i.e., in BoW) and co-occurrences (i.e., in Word2vec) in documents, as described below. Note that |r| is defined according to each representation. For the case of BoW, |r| = |V|. In the case of Word2vec, |r| is number of hidden neurons used to represent the learned word vectors.</p><p>Bag-of-Words (BoW) Under BoW each document is represented by taking each word in the vocabulary as an attribute to build document vectors d i = w 1 , . . . , w r . Intuitively, the BoW is an histogram representing word frequencies in each document. BoW representation was built filtering terms with high frequency and using TF-IDF (Term Frequency Inverse Document Frequency) weighting scheme <ref type="bibr" coords="2,251.33,614.09,9.96,8.74" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word2vec Adaptation</head><p>The purpose of Word2vec is to build accurate representations of words in a space R r . The main goal is that semantically related words should have similar word vectors in R r <ref type="bibr" coords="3,334.66,118.99,9.96,8.74" target="#b1">[2]</ref>. For instance, Paris vector are close to Berlin vector, since both are capitals. Surprisingly, Mikolov et. a.l (2013) also showed other generalizations using specific lineal operations. For example, France-Paris+Berlin result in a very close vector to Germany. In this paper, we exploit the use of learned word vectors from Wikipedia using Word2vec <ref type="bibr" coords="3,467.30,166.81,9.96,8.74" target="#b1">[2]</ref>. For our experiments, the learned word vectors from each document are used to compute the average document vector as in <ref type="bibr" coords="3,328.86,190.72,9.96,8.74" target="#b3">[4]</ref>. The idea is that the average of those word representations, should capture rich notions of semantic relatedness and compositionality of the whole document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Retrieval stage</head><p>In this stage, a document query q j under a specific representation is used to retrieve a set of relevant items {(d h , I h ) : (d h , I h ) ∈ D}. Note that only the textual information from web pages and textual queries are used in the retrieval stage, but the reported results correspond to the immersed images in the retrieved items. For the retrieval stage we used the cosine similarity measure, which is defined in Equation <ref type="formula" coords="3,223.74,322.81,3.87,8.74" target="#formula_0">1</ref>.</p><formula xml:id="formula_0" coords="3,204.64,342.65,275.95,23.25">similarity(q j , d i ) = cosine(q j , d i ) = q j * d i ||q j ||||d i ||<label>(1)</label></formula><p>where q j , d i are the representations of the document query q j , and the i th document d i from the collection, respectively. This equation iterates over all documents from D, then the images associated to the k most similar documents to q j are used to illustrate it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head><p>In this section we present quantitative and qualitative results of the proposed approach in the TI task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Quantitative results</head><p>In Table <ref type="table" coords="3,175.44,524.61,3.87,8.74">1</ref>, it can be seen the performance of the proposed representations for TI. The table reports scores from the metric proposed in <ref type="bibr" coords="3,381.82,536.57,9.96,8.74" target="#b4">[5]</ref>, where basically the recall is evaluated at the k-th rank position (R@K) of the ground truth images. Several values of k are reported, in Table <ref type="table" coords="3,312.53,560.48,4.98,8.74">1</ref> we can see the scores that correspond to the test set.</p><p>Our best score is reported by run1, which uses the BoW under a TF-IDF weighting scheme filtering 5% of the highest frequent terms. The results obtained by run1 validate our hypothesis that related images appear in related web pages. On the other hand, the run2 and run3 report scores obtained by Word2vec representation (denoted as d2v), both runs use also a filtering of 5%, but with and without TF-IDF weighting respectively. In these latter results, we consider that the representation is affected by noise when increasing the number of used word to build it. Although, a Word2vec representation helps to retrieve similar documents (as is showed in Figure <ref type="figure" coords="4,265.23,130.95,3.87,8.74" target="#fig_0">1</ref>), we have found that this representation is more confident with short documents or in specific domains. However, in documents with diversity of topics the performance decrease (see Figure <ref type="figure" coords="4,405.57,154.86,4.43,8.74">2</ref>) because of the great variety of different words involved.</p><p>Table <ref type="table" coords="4,246.52,196.36,3.87,8.74">1</ref>: Recall@K for full 180K test set.</p><p>Team RUN Recall (%) R@1 R@5 R@10 R@25 R@50 R@75 R@100 Baseline chance 0.00 0.00 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Qualitative results</head><p>In this subsection we compare the proposed representation. In Figure <ref type="figure" coords="4,434.72,351.21,3.87,8.74" target="#fig_0">1</ref>, we show top retrieved images that illustrate the document query under two representations. In this case, the document query consists in a short text, we can see that both representations show relevant images to illustrate the text. An interesting output is obtained by run3 that shows diversity on retrieved images. On the other hand, Figure <ref type="figure" coords="4,270.97,644.16,4.98,8.74">2</ref> shows a long document used as query. Again, the outputs of run1 and run3 are compared. Despite that in the documents are included great quantity of topics, the image retrieval of run1 is effective, instead the image retrieval of run3 includes few relevant images. Taking as examples the Figures <ref type="figure" coords="5,171.02,142.90,4.98,8.74" target="#fig_0">1</ref> and<ref type="figure" coords="5,200.47,142.90,3.87,8.74">2</ref>, we can see that the quantity of terms and rich vocabularies contained in the documents is an important factor for selecting the representation. While Word2vec representation seems to be robust in short documents or documents in a specific domain, the BoW representation plus weighting TF-IDF shows to be a better option for the case of long documents. Fig. <ref type="figure" coords="5,153.45,433.54,3.87,8.74">2</ref>: Given the text document (top), the top of retrieved images. First row, output images from run1. Second row, output images from run3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Work in progress: representing documents in a visual space</head><p>We have worked with a visual representation but it is not reported in Table <ref type="table" coords="5,472.84,512.66,3.87,8.74">1</ref>. Unfortunately, we were not able to submit a run because of the tight time for the deadline. Nevertheless, we also present an in-house evaluation showing qualitative results.</p><p>For representing documents in a visual space, we used a multimodal representation M composed by visual prototypes. The construction of M is performed in an unsupervised way by using images immersed in web pages. The idea is that images can be represented by two different modalities: a visual representation extracted from the image I, and a textual representation extracted from the web pages D. In M for every word in D a visual prototype is formed, where each prototype is a distribution over visual representation (more detail of this approach in <ref type="bibr" coords="5,146.75,644.16,10.30,8.74" target="#b5">[6]</ref>). We used a reference image dataset (training set of <ref type="bibr" coords="5,393.12,644.16,10.79,8.74" target="#b0">[1]</ref>) for construction of M.</p><p>The aim of this representation is to include the visual information in the text illustration. Under this representation, the words from a given document query are seen in function of its visual representation. First, using visual prototypes of words extracted from a query, then an average visual prototypes is formed. Second, using average visual prototype as query, then we retrieve some related images. In other words, the document query is translated to a visual document and used it to retrieve images, as a CBIR (Content-Based Image Retrieval) task.</p><p>In Figure <ref type="figure" coords="6,194.10,203.07,3.87,8.74" target="#fig_1">3</ref>, we show one favorable case for the visual representation. However, the average visual prototype in this case is formed by three words of the document query with the highest weight. For this kind of representation, we have observed that the more terms in the document query, the more noisy the visual representation is. As conclusion, a visual document representation is formed only by few words, so it is necessary a keyword extraction process on document query. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this paper we presented an IR approach to address the Text Illustration task. The documents are defined as textual, semantic or visual representations. The performed experiments under these different representations give an initial point of comparison for future approaches. According to the performed evaluation we conclude that, related web pages have related images, then it is possible to retrieve highly relevant elements using IR techniques. On the one hand, the BoW obtained outstanding performances because of the filtering of high frequent terms and the discriminative information captured by TF-IDF weighting scheme. On the other hand, Word2vec representation did not obtain reliable representations because of the great diversity of words involved in web pages. Such diversity makes difficult to build accurate document representations using the simple average of words. Our perspectives for future work include exploring relationships between representation to incorporate mix information (textual-visual) and adding a keyword extraction for the document query.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,134.77,596.20,345.82,8.74;4,134.77,608.16,282.25,8.74"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Given the text document (top), the top of retrieved images. First row, output images from run1. Second row, output images from run3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,134.77,452.14,345.82,8.74;6,134.77,464.09,119.22,8.74;6,149.74,295.05,315.88,99.22"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Given the text document (top), the top of retrieved images. Output images using visual representation.</figDesc><graphic coords="6,149.74,295.05,315.88,99.22" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This work was supported by <rs type="funder">CONACYT</rs> under project grant <rs type="grantNumber">CB-2014-241306</rs> (<rs type="funder">Clasificación y recuperación de imágenes mediante técnicas de minería de textos</rs>). The first author was supported by <rs type="funder">CONACyT</rs> under scholarship No. <rs type="grantNumber">214764</rs>, and <rs type="person">López-Monroy</rs> thanks for doctoral scholarship <rs type="grantNumber">CONACyT-México 243957</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_CARgwAt">
					<idno type="grant-number">CB-2014-241306</idno>
				</org>
				<org type="funding" xml:id="_A3Dhkz4">
					<idno type="grant-number">214764</idno>
				</org>
				<org type="funding" xml:id="_CrK2yvP">
					<idno type="grant-number">CONACyT-México 243957</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,138.35,288.04,342.24,7.86;7,146.91,299.00,333.68,7.86;7,146.91,309.96,333.68,7.86;7,146.91,318.65,114.26,10.13" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,271.10,299.00,209.49,7.86;7,146.91,309.96,93.03,7.86">Overview of the ImageCLEF 2016 Scalable Concept Image Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ramisa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dellandrea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,261.77,309.96,218.82,7.86;7,146.91,320.92,14.79,7.86">CLEF2016 Working Notes. CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Évora, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,331.88,342.24,7.86;7,146.91,342.81,232.49,7.89" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="7,340.45,331.88,140.14,7.86;7,146.91,342.84,101.88,7.86">Efficient estimation of word representations in vector space</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>CoRR abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,353.80,342.24,7.86;7,146.91,364.73,213.92,7.89" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,201.75,353.80,278.84,7.86;7,146.91,364.75,32.07,7.86">A statistical interpretation of term specificity and its application in retrieval</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">S</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,186.63,364.75,105.74,7.86">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="11" to="21" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,375.71,342.24,7.86;7,146.91,386.65,122.50,7.89" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="7,247.56,375.71,228.47,7.86">Distributed representations of sentences and documents</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno>CoRR abs/1405.4053</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,397.63,342.24,7.86;7,146.91,408.56,324.56,7.89" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,318.93,397.63,161.66,7.86;7,146.91,408.59,168.87,7.86">Framing image description as a ranking task: Data, models and evaluation metrics</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,323.85,408.59,69.95,7.86">J. Artif. Int. Res</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="853" to="899" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,419.55,342.24,7.86;7,146.91,430.51,333.68,7.86;7,146.91,441.47,333.68,7.86;7,146.91,452.43,96.72,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,257.94,430.51,222.65,7.86;7,146.91,441.47,71.80,7.86">INAOE-UNAL at ImageCLEF 2015: Scalable Concept Image Annotation</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Pellegrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Vanegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Arevalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Beltrán</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">J</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes-Y-Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>González</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,242.99,441.47,233.38,7.86">CLEF2015 Working Notes. CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
