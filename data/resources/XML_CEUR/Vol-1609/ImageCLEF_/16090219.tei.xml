<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,137.98,115.96,339.40,12.62">Overview of the ImageCLEF 2016 Medical Task</title>
				<funder ref="#_DShw8qS">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
				<funder>
					<orgName type="full">National Library of Medicine</orgName>
					<orgName type="abbreviated">NLM</orgName>
				</funder>
				<funder>
					<orgName type="full">Lister Hill National Center for Biomedical Communications</orgName>
					<orgName type="abbreviated">LHNCBC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,165.66,153.76,106.97,8.74"><forename type="first">Alba</forename><forename type="middle">G</forename><surname>Seco De Herrera</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Lister Hill National Center for Biomedical Communications</orgName>
								<orgName type="institution">National Library of Medicine</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,283.18,153.76,57.48,8.74"><forename type="first">Roger</forename><surname>Schaer</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Applied Sciences Western Switzerland (HES-SO)</orgName>
								<address>
									<settlement>Sierre</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,351.22,153.76,71.88,8.74"><forename type="first">Stefano</forename><surname>Bromuri</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Open University of the Netherlands</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,270.01,165.72,68.10,8.74"><forename type="first">Henning</forename><surname>MÃ¼ller</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Applied Sciences Western Switzerland (HES-SO)</orgName>
								<address>
									<settlement>Sierre</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,137.98,115.96,339.40,12.62">Overview of the ImageCLEF 2016 Medical Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9511D818C5216D0D56A2AE50DC3C8D43</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ImageCLEFmed</term>
					<term>compound figure detection</term>
					<term>multi-label classification</term>
					<term>figure separation</term>
					<term>modality classification</term>
					<term>caption detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ImageCLEF is the image retrieval task of the Conference and Labs of the Evaluation Forum (CLEF). ImageCLEF has historically focused on the multimodal and language-independent retrieval of images. Many tasks are related to image classification and the annotation of image data as well. The medical task has focused more on image retrieval in the beginning and then retrieval and classification tasks in subsequent years. In 2016 a main focus was the creation of meta data for a collection of medical images taken from articles of the the biomedical scientific literature. In total 8 teams participated in the four tasks and 69 runs were submitted. No team participated in the caption prediction task, a totally new task. Deep learning has now been used for several of the ImageCLEF tasks and by many of the participants obtaining very good results. A majority of runs was submitting using deep learning and this follows general trends in machine learning. In several of the tasks multimodal approaches clearly led to best results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>ImageCLEF has organized image retrieval evaluation campaigns since 2003 and a medical task was added in 2004 <ref type="bibr" coords="1,271.17,540.80,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="1,283.34,540.80,7.01,8.74" target="#b1">2]</ref>. With a focus on multimodal and languageindependent retrieval of images, the databases used have evolved strongly over the years and many of the datasets have gotten larger as well. Several medical tasks have been organized over the years <ref type="bibr" coords="1,312.89,576.67,7.75,8.74" target="#b2">[3]</ref><ref type="bibr" coords="1,320.63,576.67,3.87,8.74" target="#b3">[4]</ref><ref type="bibr" coords="1,324.51,576.67,7.75,8.74" target="#b4">[5]</ref>, ranging from the classification of medical images to retrieval of single images or entire cases. This year's tasks are an evolution from the tasks that were organized in 2015 <ref type="bibr" coords="1,380.16,600.58,9.96,8.74" target="#b5">[6]</ref>. The main objective with the current data sets and tasks is to make the large amount of visual content that is shared in the biomedical open access literature available in an easier way by generating meta data. PubMed Central 4 (PMC) makes a large amount of currently over 4 million articles available including text and figures in a structured form. The collection is growing strongly with over 200'000 articles being added in 2014 alone and with a quickly increasing tendency. Basically no metadata are available for the figures besides the figure captions and global information on the articles including global MeSH (Medical Subject Headings) terms. A major problem is that about half of the available figures contain more than one subfigure, so are compound or multi-pane figures. The tasks in 2016 aims at first detecting, whether a figure is a compound figure, then trying to separate the compound figures into their parts or extract image type information for all subfigures of a compound figure. Then, a modality classification tries to detect the image type, that ranges from medical modalities (e.g. X-ray, MRI, CT) to general image types such as graphs and flow charts. All these tasks can help to generate metadata for the almost 4 million images available via PMC. Including the extracted subfigures this will likely amount to over 10 million medical figures that are available and currently only little exploited.</p><p>This article first describes the five tasks that were organized in 2016, then describes the data sets, ground truth and participation. The conclusions summarize the main lessons learned from the evaluation campaign. Finally, a little outlook is given into the limitations and a possible future of the task.</p><p>2 Tasks, Data Sets, Ground Truth, Participation This section gives an overview of each of the five subtasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compound Figure Detection</head><p>As a first step for the retrieval of compound figures and its subfigures, compound figure detection is necessary. This subtask was introduced in 2015 and the goal is to identify whether a figure is a compound figure or not (see Figure <ref type="figure" coords="2,246.79,534.56,3.87,8.74" target="#fig_0">1</ref>). The task is not easy, as compound figures can or not have dominant subfigures and do not always have clear separating lines.</p><p>The subtask provides a set of compound and non-compound figures from the biomedical literature of PMC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compound Figure Separation</head><p>The goal of this subtask is to separate the compound figures into subfigures to be able to work with the subfigures independently. This subtask was introduced in 2013.     Multi-label Classification Compound figures are often an aggregate belonging to multiple classes, as they can show multiple perspectives concerning a medical problem. This aggregation is not random, a compound figure is an aggregation of sub-figures representing a relationship with a clear semantic meaning. Goal of the task was to see whether it is possible to learn the components of a figure without learning from single images representing the image types, but from other labelled compound figures. Techniques such as deep learning should work well on such tasks. Figure <ref type="figure" coords="4,181.81,215.94,4.98,8.74" target="#fig_6">3</ref> is an example of such an image in which multiple perspectives of the same set of cells are shown. Bromuri et al. <ref type="bibr" coords="4,214.66,535.26,10.52,8.74" target="#b6">[7]</ref> formulates the general multi-label problem as follows: Let X be the domain of observations and let L be the finite set of labels. Given a training set</p><formula xml:id="formula_0" coords="4,226.88,560.48,253.71,9.65">T = {(x 1 , Y 1 ), (x 2 , Y 2 ), ..., (x n , Y n )} (x i â X, Y i â L) i.i.d.</formula><p>drawn from an unknown distribution D, the goal is to learn a multi-label classifier h : X â 2 L . However, it is often more convenient to learn a real-valued scoring function of the form f : X Ã L â R. Given an instance x i and its associated label set Y i , a working system will attempt to produce larger values for labels in Y i than those that are not in Y i , i.e. f (x i , y 1 ) &gt; f (x i , y 2 ) for any y 1 â Y i and y 2 / â Y i . By the use of the function f (â¢, â¢), we can obtain a multi-label classifier: h(x i ) = {y|f (x i , y) &gt; Î´, y â L}, where Î´ is a threshold to infer from the training set. The function f (â¢, â¢) can also be adapted to a ranking function rank f (â¢, â¢), which maps the outputs of f (x i , y) for any y</p><formula xml:id="formula_1" coords="5,134.77,118.99,345.83,21.61">â L to {1, 2, ..., |L|} such that if f (x i , y 1 ) &gt; f (x i , y 2 ) then rank f (x i , y 1 ) &lt; rank f (x i , y 2 ).</formula><p>Multi-label performance measures are generally different from those used in the single label tasks. In <ref type="bibr" coords="5,247.54,154.86,9.96,8.74" target="#b5">[6]</ref>, we introduced the Hamming loss. The Hamming loss evaluates how many times an observation-label pair is misclassified. The score lies between 0 and 1, where 0 is the best:</p><formula xml:id="formula_2" coords="5,239.87,196.94,240.72,30.32">hloss S (h) = 1 m m i=1 |h(x i ) Y i | |L| .<label>(1)</label></formula><p>represents the symmetric difference.</p><p>In 2016, we also introduce the mean of the F-Measures of all the labels belonging to a figure, where for each i label we define the F-Measure to be:</p><formula xml:id="formula_3" coords="5,221.82,280.17,258.77,23.22">F -M easure i = 2 * precision i * recall i precision i + recall i<label>(2)</label></formula><p>This allows us to understand if there is an unbalanced distribution of the labels that leads to the classifier overfitting to the majority class. -[Dxxx] Diagnostic images:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subfigure Classification</head><formula xml:id="formula_4" coords="5,140.99,444.61,223.12,220.25">â¢ [DRxx] Radiology (7 categories): â¢ [DRU S] Ultrasound â¢ [DRM R] Magnetic Resonance â¢ [DRCT ] Computerized Tomography â¢ [DRXR] X-Ray, 2D Radiography â¢ [DRAN ] Angiography â¢ [DRP E] PET â¢ [DRCO] Combined modalities in one image -[DV xx] Visible light photography (3 categories): â¢ [DV DM ] Dermatology, skin â¢ [DV EN ] Endoscopy â¢ [DV OR] Other organs -[DSxx] Printed signals, waves (3 categories): â¢ [DSEE] Electroencephalography â¢ [DSEC] Electrocardiography â¢ [DSEM ] Electromyography -[DM xx] Microscopy (4 categories): â¢ [DM LI] Light microscopy â¢ [DM EL] Electron microscopy â¢ [DM T R] Transmission microscopy â¢ [DM F L] Fluorescence microscopy -[D3DR] 3D reconstructions (1 category) -[Gxxx]</formula><p>Generic biomedical illustrations (12 categories):</p><p>â¢ [GT AB] Tables and forms     Caption Prediction Thanks to the technical advances of cloud computing many large and data intensive applications have become possible. Modern GPUs (Graphical Processing Units) have made massively parallel computing of simple operations possible and lead to a revival of methods based on neural networks with more complex and deeper architectures. There has been a strong hype around such Deep Learning techniques <ref type="bibr" coords="7,302.16,484.85,9.96,8.74" target="#b8">[9]</ref>. One of the most successful application of Deep Learning is that of deep captioning <ref type="bibr" coords="7,328.01,496.81,14.61,8.74" target="#b9">[10]</ref>.</p><formula xml:id="formula_5" coords="6,158.68,185.33,190.26,135.57">â¢ [GP LI] Program listing â¢ [GF IG] Statistical figures, graphs, charts â¢ [GSCR] Screenshots â¢ [GF LO] Flowcharts â¢ [GSY S] System overviews â¢ [GGEN ] Gene sequence â¢ [GGEL] Chromatography, Gel â¢ [GCHE] Chemical structure â¢ [GM AT ] Mathematics, formula â¢ [GN CP ] Non-clinical photos â¢ [GHDR] Hand-drawn sketches</formula><p>The purpose of the caption prediction task is to mimic the ability of a medical professional to recognize figures in a medical text and provide a description of these figures. We believe that this is be an important task that can lead to future applications in medical image information retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Datasets</head><p>The dataset used in this task is a subset of images contained in articles from the biomedical literature extracted from the PMC. The trainining sets were obtained merging the training and test sets of the ImageCLEFmed 2015 subtasks <ref type="bibr" coords="7,161.00,632.21,9.96,8.74" target="#b5">[6]</ref>. Therefore, in 2016 a larger number of figures were distributed than in 2015. Image captions were also provided in addition to all images. For the compound figure detection subtask 21,000 figures were labelled as compound figures or non-compound for the training set and 3,456 for the test set. A subset of the compound figures of the compound figure detection subtask was distributed to be separated into subfigures for the figure separation subtask. 6,783 and 1,614 were distributes as training and test sets, respectively. In 2016, more stitched compound figures were added making the subtask more complicated. For the multi-label classification, a subset of the compound figures were distributed containing 1,568 in the training set and 1,083 in the test set. These compound figures were separated into subfigures and distributed for the subfigure classification subtask. The naming of the subfigures was done in a way that if the compound figure ID is "1297-9686-42-10-3", then the corresponding subfigure IDs are "1297-9686-42-10-3-1", "1297-9686-42-10-3-2", "1297-9686-42-10-3-3" and "1297-9686-42-10-3-4" on the case of four subfigures. This resulted in 6,776 subfigures in the training set and 4,166 subfigures in the test set.</p><p>The data distributed to the participants for the caption prediction subtask involved 10,000 images from diagnostic imaging category and relative captions. We figured that diagnostic images might be of the highest relevance in this context. The test set comprised another 10,000 diagnostic images but the captions were not included for these images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Participation</head><p>72 groups registered and obtained access to the data. The same number of groups as in 2015 submitted results to the medical task (8 groups in total). Groups participated from four continents, so the regional spread was high.</p><p>Despite that the number of groups that registered in 2016 being smaller than in 2015, the number of submitted runs increased. 15 runs were submitted to the compound figure detection task, 3 runs to the multi-label classification task, 9 runs to the figure separation task and 42 runs to the subfigure separation task. There were unfortunately no participants in the new caption prediction task.</p><p>The following groups submitted at least one run:</p><p>This section provides the results obtained by the participants in each of the subtasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Compound Figure Detection</head><p>Table <ref type="table" coords="9,161.37,202.59,4.98,8.74" target="#tab_0">1</ref> shows the results obtained for the compound figure detection task. Three groups participated in the subfigure detection subtask obtaining an accuracy of up to 92.70% using a multi-modal approach, followed by a visual approach submitted for the same group, DUTIR. DUTIR applied deep convolutional neural networks on vectors trained on the words of all captions using Word2Vec. Five deep convolutional neural networks were also applied on the resized images.</p><p>CIS UDEL <ref type="bibr" coords="9,200.10,476.19,14.61,8.74" target="#b10">[11]</ref>, also achieved its best results using a multi-modal approach. In the textual approach the group extracted a set of delimiters from the captions. In the visual approach, CIS UDEL applied the output of a figure separation approach to classify the figures into compound and non-compound. Finally, the results were fused using several methods, such as a logical union and a decision tree classifier.</p><p>MLKD submitted a single run in this subtask achieving the best results using only text information. The textual approach is based on the caption and on the text citing the figure inside the article followed by the use of a random forest classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Figure Separation</head><p>Table <ref type="table" coords="9,163.14,632.21,4.98,8.74" target="#tab_1">2</ref> shows the results for the figure separation subtask. In 2016 only one group participated in the compound figure separation task, CIS UDEL <ref type="bibr" coords="9,462.33,644.16,14.61,8.74" target="#b10">[11]</ref>, achieving very good results up to an accuracy of 84.43%. Similar results to 2015 were obtained although the difficulty of the subtask was increased in 2016. CIS UDEL applied a connected component analysis to separate the compound figures. A post-processing step was applied to avoid over-fragmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-label Classification</head><p>This year two groups submitted runs for the multi-label classification task. The BMET group achieved the best Hamming loss (0.0131) and both groups achieved a F-Measure of 0.32. Table <ref type="table" coords="10,255.09,346.20,4.98,8.74" target="#tab_2">3</ref> summarises these results. The BMET group also submitted a working notes article <ref type="bibr" coords="10,406.68,464.50,14.61,8.74" target="#b11">[12]</ref>, highlighting the use of Deep Learning and CNNs to classify the images with multiple labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Subfigure Classification</head><p>This subtask was the most popular task in 2016 with seven groups participating. The results achieved by the participants are shown in Table <ref type="table" coords="10,403.29,536.57,3.87,8.74" target="#tab_3">4</ref>. As in the compound detection task best results were obtained by a multi-modal approach, followed by visual and textual approaches. BCSG <ref type="bibr" coords="10,364.43,560.48,15.50,8.74" target="#b12">[13]</ref> achieved the best accuracy of 88.43% by applying multiple visual features and deep convolutional neural networks (CNN). Figure captions and paper full text were also used for the classification. To remove unimportant visual words information gain is used for feature selection. MLKD achieved the best results using a text analysis approach. Similar approaches as in the compound figure detection subtask were applied. Best results on visual approaches were obtained by BCSG followed by IPL <ref type="bibr" coords="10,154.38,644.16,14.61,8.74" target="#b13">[14]</ref>. IPL adopted various state-of-the-art visual features, such as, Bag-of-Visual-Words computed with pyramid-histogram-of-visual-word descriptors and quad-treebag-of-colors. BMET <ref type="bibr" coords="12,298.18,118.99,15.50,8.74" target="#b11">[12]</ref> applied a method similar to the one they used for the multi-label classification task based on CNNs. NWPU also based its method on deep CNNs. A hierarchical approach was used that first classified the figures into diagnostic images and generic biomedical illustrations through a deep CNN. Then, two other deep CNNs were trained to finish the classification of diagnostic images and generic biomedical illustrations,respectively. CIS UDEL <ref type="bibr" coords="12,188.08,190.72,15.50,8.74" target="#b10">[11]</ref> also applied a hierarchical classifier using multiple visual descriptors. Neural networks were used as a classifier. Finally, NovaSearch <ref type="bibr" coords="12,445.64,202.68,15.50,8.74" target="#b14">[15]</ref> also applied three different CNN models in their approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In 2016, the ImageCLEF medical task proposed 5 subtasks. One of the subtasks was organized for the first time, the caption prediction subtask. Unfortunately, no participants finally submitted results to the task. This year, more figures were added to the database in the other four subtasks that had already been run in the past. In total, there were eight participants who submitted results, the same number as in 2015 but more runs were submitted in 2016 compared to 2015. The best accuracy obtained was very good in three of the tasks: 92.70% in the compound detection subtask using a multi-modal approach; 84.43% in the figure separation subtask using a visual approach; and 88.43% in the subfigure detection subtask using a multi-modal approach. For the multi-label subtask, the BMET group obtained 0.0135 Hamming loss and a F-Measure of 0.32 using a deep learning approach based on CNNs.</p><p>The clear novelty and trend in 2016 is the use of neural network models or deep learning for classification subtasks obtaining very good results in general. CIS UDEL was the only participant of the 2016 figure separation subtask separating the images using a connected component analysis. The main novelty concerning the multi-label task in 2016 was the use of fine-tuned CNNs to perform the multi-label classification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,134.77,382.01,117.27,8.77;2,134.77,398.00,164.46,8.74;2,140.99,412.93,129.46,8.77;2,140.99,423.90,134.52,8.77;2,140.99,434.86,198.23,8.77;2,140.99,445.82,186.42,8.77;2,140.99,456.78,92.65,8.77"><head>2. 1</head><label>1</label><figDesc>The Tasks in 2016 Five subtasks were organized in 2016: compound figure detection; compound figure separation; multi-label classification with image types; subfigure classification into image types; caption prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,335.39,620.25,145.20,8.74;2,134.77,632.21,345.82,8.74;2,134.77,644.16,345.83,8.74;2,134.77,656.12,161.61,8.74"><head>Figure 2</head><label>2</label><figDesc>shows a simple example of a compound figure separated by blue lines. There are many more challenging examples where the separating lines are not straight or where a large number of sub figures is put into a single figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,187.08,273.20,77.53,7.86"><head>( a )</head><label>a</label><figDesc>Compound figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="3,349.27,273.20,94.12,7.86"><head></head><label></label><figDesc>(b) Non-compound figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="3,188.14,297.58,239.08,7.89;3,150.42,154.98,150.85,113.39"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Examples of compound and non-compound figures.</figDesc><graphic coords="3,150.42,154.98,150.85,113.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="3,134.77,604.93,345.82,7.89;3,134.77,615.91,65.15,7.86;3,176.79,391.73,261.78,198.42"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Example of a compound figure separated into subfigures, showing the blue separation lines.</figDesc><graphic coords="3,176.79,391.73,261.78,198.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="4,134.77,473.19,345.82,7.89;4,134.77,484.18,345.83,7.86;4,134.77,495.14,304.39,7.86;4,175.69,260.00,263.98,198.42"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Example of a compound figure with multiple related subparts that was used for the multi-label classification subtask. This figure contain the labels "Light microscopy" (subfigures A and C) and "Fluorescence microscopy" (subfigures B and D).</figDesc><graphic coords="4,175.69,260.00,263.98,198.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="5,258.63,354.02,221.97,8.74;5,134.77,365.98,345.83,8.74;5,134.77,377.93,345.82,8.74;5,134.77,389.89,345.82,8.74;5,134.77,401.84,345.83,8.74;5,134.77,413.80,318.64,8.74"><head></head><label></label><figDesc>The figure classification task was already run in a slightly different configuration from 2011 to 2013. In 2015 and 2016 the subtask focuses on the modality classification of subfigures extracted from the compound figures distributed for the multi-label classification subtask. This subtask aims to classify figures into the 30 classes of the hierarchy shown in Figure 4. The class codes with descriptions are the following ([Class code] Description):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="6,134.77,598.02,345.82,7.89;6,134.77,609.00,175.00,7.86;6,156.50,356.47,302.36,226.77"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The image class hierarchy that was developed for document images occurring in the biomedical open access literature [8].</figDesc><graphic coords="6,156.50,356.47,302.36,226.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="6,149.71,644.16,330.88,8.74;6,134.77,656.12,31.16,8.74"><head>Figure 5</head><label>5</label><figDesc>Figure 5 shows four subfigures from a compound figured from two different classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="7,165.55,234.06,111.10,7.86;7,324.08,234.06,137.31,7.86"><head></head><label></label><figDesc>(a) Light microscopy subfigure. (b) Fluorescence microscopy subfigure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="7,167.64,366.62,110.59,7.86;7,323.27,366.62,137.31,7.86"><head></head><label></label><figDesc>(c) Light microscopy subfigure.(d) Fluorescence microscopy subfigure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12" coords="7,180.96,391.00,253.44,7.89;7,149.09,248.40,147.70,113.38"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Example of subfigures belonging to a compound figure.</figDesc><graphic coords="7,149.09,248.40,147.70,113.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="9,169.87,233.73,275.61,145.73"><head>Table 1 .</head><label>1</label><figDesc>Results of the runs of the compound figure detection task.</figDesc><table coords="9,203.11,253.40,209.13,126.07"><row><cell>Group</cell><cell>Run</cell><cell cols="2">Run Type Accuracy</cell></row><row><cell>DUTIR</cell><cell>CFD DUTIR Mixed AVG</cell><cell>mixed</cell><cell>92.70</cell></row><row><cell cols="2">CIS UDEL CFDRun10</cell><cell>mixed</cell><cell>90.74</cell></row><row><cell cols="2">CIS UDEL CFDRun09</cell><cell>mixed</cell><cell>90.39</cell></row><row><cell cols="2">CIS UDEL CFDRun05</cell><cell>mixed</cell><cell>90.39</cell></row><row><cell cols="2">CIS UDEL CFDRun07</cell><cell>mixed</cell><cell>85.47</cell></row><row><cell cols="2">CIS UDEL CFDRun8</cell><cell>mixed</cell><cell>69.06</cell></row><row><cell cols="2">CIS UDEL CFDRun06</cell><cell>mixed</cell><cell>52.25</cell></row><row><cell>MLKD</cell><cell>CFD2</cell><cell>textual</cell><cell>88.13</cell></row><row><cell>DUTIR</cell><cell cols="2">CFD DUTIR Textual CNN textual</cell><cell>87.03</cell></row><row><cell>DUTIR</cell><cell cols="2">CFD DUTIR Textual RNN textual</cell><cell>86.05</cell></row><row><cell cols="2">CIS UDEL CFDRun01</cell><cell>textual</cell><cell>85.47</cell></row><row><cell>DUTIR</cell><cell>CFD DUTIR Visual CNNs</cell><cell>visual</cell><cell>92.01</cell></row><row><cell cols="2">CIS UDEL CFDRun04</cell><cell>visual</cell><cell>89.64</cell></row><row><cell cols="2">CIS UDEL CFDRun02</cell><cell>visual</cell><cell>89.29</cell></row><row><cell cols="2">CIS UDEL CFDRun03</cell><cell>visual</cell><cell>69.82</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,167.76,115.91,279.83,97.91"><head>Table 2 .</head><label>2</label><figDesc>Results of the runs submitted to the figure separation task.</figDesc><table coords="10,237.12,135.58,141.12,78.24"><row><cell>Run</cell><cell cols="2">Group Run Type Accuracy</cell></row><row><cell cols="2">CIS UDEL FS.run9 visual</cell><cell>84.43</cell></row><row><cell cols="2">CIS UDEL FS.run7 visual</cell><cell>84.08</cell></row><row><cell cols="2">CIS UDEL FS.run6 visual</cell><cell>84.03</cell></row><row><cell cols="2">CIS UDEL FS.run8 visual</cell><cell>83.04</cell></row><row><cell cols="2">CIS UDEL FS.run5 visual</cell><cell>81.23</cell></row><row><cell cols="2">CIS UDEL FS.run3 visual</cell><cell>75.27</cell></row><row><cell cols="2">CIS UDEL FS.run4 visual</cell><cell>74.83</cell></row><row><cell cols="2">CIS UDEL FS.run2 visual</cell><cell>74.30</cell></row><row><cell cols="2">CIS UDEL FS.run1 visual</cell><cell>73.57</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="10,154.40,375.35,306.55,59.79"><head>Table 3 .</head><label>3</label><figDesc>Multi-label classification subtasks.</figDesc><table coords="10,154.40,394.00,306.55,41.14"><row><cell>Run</cell><cell cols="3">Group Hamming Loss F-Measure</cell></row><row><cell>MLC-BMET-multiclass-test-max-all</cell><cell>BMET</cell><cell>0.0131</cell><cell>0.295</cell></row><row><cell cols="2">MLC-BMET-multiclass-test-prob-max-all BMET</cell><cell>0.0135</cell><cell>0.320</cell></row><row><cell>MLC2</cell><cell>MLKD</cell><cell>0.0294</cell><cell>0.320</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="11,136.16,198.65,393.57,370.14"><head>Table 4 .</head><label>4</label><figDesc>Results of the runs submitted to the subfigure classification task.</figDesc><table coords="11,136.16,227.53,70.28,6.12"><row><cell>Run</cell><cell>Group</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p><rs type="institution">-BMET</rs> (<rs type="institution">Institute of Biomedical Engineering and Technology, University of Sydney, Australia)</rs>; -<rs type="institution">CIS UDEL (Computer &amp; Information Sciences, University of Delaware, Newark, USA)</rs>; -DUTIR (<rs type="institution">Department of Computer Science and Engineering, Dalian University of Technology, China.</rs>)*; -<rs type="institution">FHDO BCSG</rs> (<rs type="institution">FHDO Biomedical Computer Science Group, University of Applied Science and Arts, Dortmund, Germany</rs>); -IPL (<rs type="institution">Athens University of Economics and Business, Greece)</rs>; -MLKD (<rs type="institution">Department of Informatics, Aristotle University of Thessaloniki, Greece</rs>)*; -NOVASearch (<rs type="institution">NOVA LINCS</rs>, <rs type="institution">Department of Computer Science Faculty of Science and Technology, University NOVA of Lisbon, Portugal</rs>)*; -NWPU (<rs type="institution">Northwestern Polytechnical University, China</rs>)*; Participants marked with a star had not participated in the medical task in 2015. Acknowledgements This research was supported in part by the <rs type="programName">Intramural Research Program</rs> of the <rs type="funder">National Institutes of Health (NIH)</rs>, <rs type="funder">National Library of Medicine (NLM)</rs>, and <rs type="funder">Lister Hill National Center for Biomedical Communications (LHNCBC)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_DShw8qS">
					<orgName type="program" subtype="full">Intramural Research Program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,142.96,612.96,337.64,7.86;12,151.52,623.92,329.07,7.86;12,151.52,634.88,329.07,7.86;12,151.52,645.84,329.07,7.86;12,151.52,656.80,258.61,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,321.13,612.96,159.46,7.86;12,151.52,623.92,55.90,7.86">The CLEF 2004 cross-language image retrieval track</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,227.32,634.88,253.27,7.86;12,151.52,645.84,186.28,7.86">Multilingual Information Access for Text, Speech and Images: Result of the fifth CLEF evaluation campaign</title>
		<title level="s" coord="12,412.61,645.84,67.98,7.86;12,151.52,656.80,98.76,7.86">Lecture Notes in Computer Science (LNCS</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</editor>
		<meeting><address><addrLine>Bath, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3491</biblScope>
			<biblScope unit="page" from="597" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,119.67,337.64,7.86;13,151.52,130.63,329.07,7.86;13,151.52,141.59,329.07,7.86;13,151.52,152.55,329.07,7.86;13,151.52,163.51,313.03,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="13,449.27,141.59,31.32,7.86;13,151.52,152.55,188.21,7.86">General overview of ImageCLEF at the CLEF 2015 labs</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>GarcÃ­a Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bromuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kazi Mohammed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Uskudarli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">B</forename><surname>Marvasti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Aldana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D M</forename><surname>RoldÃ¡n GarcÃ­a</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,360.28,152.55,116.11,7.86">Working Notes of CLEF 2015</title>
		<title level="s" coord="13,151.52,163.51,141.41,7.86">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,174.47,337.63,7.86;13,151.52,185.43,329.07,7.86;13,151.52,196.39,329.07,7.86;13,151.52,207.34,106.67,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,234.78,185.43,245.81,7.86;13,151.52,196.39,72.70,7.86">Overview of the ImageCLEF 2012 medical image retrieval and classification tasks</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>GarcÃ­a Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,245.60,196.39,234.99,7.86;13,151.52,207.34,27.77,7.86">Working Notes of CLEF 2012 (Cross Language Evaluation Forum)</title>
		<imprint>
			<date type="published" when="2012-09">September 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,218.30,337.63,7.86;13,151.52,229.26,329.07,7.86;13,151.52,240.22,283.29,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,210.81,229.26,189.99,7.86">Overview of the ImageCLEF 2013 medical tasks</title>
		<author>
			<persName coords=""><forename type="first">GarcÃ­a</forename><surname>Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Demner Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,421.22,229.26,59.37,7.86;13,151.52,240.22,204.39,7.86">Working Notes of CLEF 2013 (Cross Language Evaluation Forum)</title>
		<imprint>
			<date type="published" when="2013-09">September 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,251.18,337.64,7.86;13,151.52,262.14,329.07,7.86;13,151.52,273.10,329.07,7.86;13,151.52,284.03,269.83,7.89" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,264.01,262.14,216.58,7.86;13,151.52,273.10,324.67,7.86">Evaluating performance of biomedical image retrieval systems: Overview of the medical image retrieval task at ImageCLEF 2004-2014</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>GarcÃ­a Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,151.52,284.06,183.45,7.86">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page" from="55" to="61" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,295.02,337.64,7.86;13,151.52,305.98,329.07,7.86;13,151.52,316.93,153.13,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,364.92,295.02,115.67,7.86;13,151.52,305.98,122.59,7.86">Overview of the ImageCLEF 2015 medical classification task</title>
		<author>
			<persName coords=""><forename type="first">GarcÃ­a</forename><surname>Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bromuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,294.40,305.98,186.20,7.86;13,151.52,316.93,74.23,7.86">Working Notes of CLEF 2015 (Cross Language Evaluation Forum)</title>
		<imprint>
			<date type="published" when="2015-09">September 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,327.89,337.63,7.86;13,151.52,338.85,329.07,7.86;13,151.52,349.78,304.91,7.89" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,401.45,327.89,79.14,7.86;13,151.52,338.85,329.07,7.86;13,151.52,349.81,82.00,7.86">Multi-label classification of chronically ill patients with bag of words and supervised dimensionality reduction algorithms</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bromuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zufferey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hennebert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">I</forename><surname>Schumacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,241.71,349.81,137.04,7.86">Journal of Biomedical Informatics</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="165" to="175" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,360.77,337.64,7.86;13,151.52,371.73,329.07,7.86;13,151.52,382.69,122.35,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,437.12,360.77,43.47,7.86;13,151.52,371.73,310.29,7.86">Creating a classification of image types in the medical literature for visual categorization</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,151.52,382.69,89.30,7.86">SPIE Medical Imaging</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,393.65,337.63,7.86;13,151.52,404.61,288.61,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,202.87,393.65,148.60,7.86">Deep learning and cultural evolution</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,375.65,393.65,104.94,7.86;13,151.52,404.61,98.93,7.86">Genetic and Evolutionary Computation Conference</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>GECCO</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,415.56,337.97,7.86;13,151.52,426.50,267.57,7.89" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="13,348.11,415.56,132.48,7.86;13,151.52,426.52,136.69,7.86">Deep captioning with multimodal recurrent neural networks (m-rnn)</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno>CoRR abs/1412.6632</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,437.48,337.98,7.86;13,151.52,448.44,329.07,7.86;13,151.52,457.13,329.07,10.13;13,151.52,470.36,37.37,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,201.85,448.44,179.76,7.86">UDEL CIS working notes in ImageCLEF 2016</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sorensen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kolagunda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Kambhamettu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Shatkay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,401.33,448.44,79.26,7.86;13,151.52,459.40,145.92,7.86">CLEF2016 Working Notes. CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Ãvora, Portugal, CEUR-WS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08">September 5-8 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,481.32,337.98,7.86;13,151.52,492.28,329.07,7.86;13,151.52,500.97,329.07,10.13;13,151.52,514.19,22.02,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="13,325.85,481.32,154.75,7.86;13,151.52,492.28,194.59,7.86">Subfigure and multi-label classification using a fine-tuned convolutional neural network</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lyndon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,370.77,492.28,109.83,7.86;13,151.52,503.24,121.86,7.86">CLEF2016 Working Notes. CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Ãvora, Portugal, CEUR-WS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08">September 5-8 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,525.15,337.97,7.86;13,151.52,536.11,329.07,7.86;13,151.52,544.80,329.07,10.13;13,151.52,558.03,37.37,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="13,265.53,525.15,215.05,7.86;13,151.52,536.11,229.62,7.86">Traditional feature engineering and deep learning approaches at medical classification task of ImageCLEF 2016</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,401.20,536.11,79.39,7.86;13,151.52,547.07,145.92,7.86">CLEF2016 Working Notes. CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Ãvora, Portugal, CEUR-WS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08">September 5-8 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,568.99,337.98,7.86;13,151.52,577.68,329.07,10.13;13,151.52,590.91,86.55,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,284.75,568.99,129.57,7.86">IPL at CLEF 2016 medical task</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Valavanis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kalamboukis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,437.46,568.99,43.13,7.86;13,151.52,579.95,189.27,7.86">CLEF2016 Working Notes. CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Ãvora, Portugal, CEUR-WS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08">September 5-8 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.61,601.87,337.97,7.86;13,151.52,610.56,329.07,10.13;13,151.52,623.78,189.62,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="13,268.94,601.87,211.65,7.86;13,151.52,612.82,47.55,7.86">NovaSearch at ImageCLEFmed 2016 subfigure classification task</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Semedo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>MagalhÃ£es</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,219.48,612.82,228.17,7.86">CLEF2016 Working Notes. CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Ãvora, Portugal, CEUR-WS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08">September 5-8 2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
