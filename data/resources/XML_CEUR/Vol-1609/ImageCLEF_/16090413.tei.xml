<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,192.50,115.96,230.36,12.62">IPL at CLEF 2016 Medical Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,150.42,153.63,81.11,8.74"><forename type="first">Leonidas</forename><surname>Valavanis</surname></persName>
							<email>valavanisleonidas@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="laboratory">Information Processing Laboratory</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<addrLine>76 Patission Str</addrLine>
									<postCode>10434</postCode>
									<settlement>Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,239.23,153.63,97.07,8.74"><forename type="first">Spyridon</forename><surname>Stathopoulos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="laboratory">Information Processing Laboratory</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<addrLine>76 Patission Str</addrLine>
									<postCode>10434</postCode>
									<settlement>Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,363.56,153.63,101.38,8.74"><forename type="first">Theodore</forename><surname>Kalamboukis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="laboratory">Information Processing Laboratory</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<addrLine>76 Patission Str</addrLine>
									<postCode>10434</postCode>
									<settlement>Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,192.50,115.96,230.36,12.62">IPL at CLEF 2016 Medical Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2161040CCCF28B243C195F4E74C0DB49</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>pyramid-histogram of visual words</term>
					<term>bag of visual words</term>
					<term>bag of colors</term>
					<term>early-fusion</term>
					<term>late-fusion</term>
					<term>textual-classification</term>
					<term>support vector machines</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we present the image classification techniques performed by the IPL Group for the subfigure classification subtask of ImageCLEF 2016 Medical Task. For the visual representation of images, various state-of-the-art visual features, such as, Bag of Visual Words computed with pyramid-histogram of-visual-words descriptors and quadtree bag-of-colors were adopted. We present the results of our runs and our extensive experiments applying early or late fusion on the results obtained from a multi-class linear kernel support vector machine. Our top run was ranked 3rd among 34 runs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image classification is perhaps the most important and challenging task within the field of computer vision with applications in several domains. A broad area of image-processing approaches is directed by image classification, the automated assignment of unknown images into a set of predefined categories.</p><p>In the medical domain Content Based Image Retrieval (CBIR) plays an important role in supporting diagnosis, treatment and teaching <ref type="bibr" coords="1,406.73,512.66,9.96,8.74" target="#b0">[1]</ref>. Visual image classification into a relatively small number of classes, has shown to deliver good results in several benchmarks. Approaches combining both visual and textual techniques for classification have shown to be promising in medical image classification tasks. Here we should mention the substantial contribution of the Im-ageCLEFmed task <ref type="bibr" coords="1,220.08,572.43,10.52,8.74" target="#b1">[2]</ref> focusing on medical images over a decade on the CBIR and classification tasks.</p><p>The ImageCLEF 2016 Medical Task, <ref type="bibr" coords="1,316.60,596.34,9.96,8.74" target="#b2">[3]</ref>, consists of 5 subtasks: compound figure detection, figure separation, multi-label classification, subfigure classification, caption prediction. Subfigures extracted from compound images are classified into 30 heterogeneous classes ranging from diagnosis images to various biomedical illustrations. Some image categories were represented by few training examples, thus the enrichment of the original collection was necessary in order to counteract the imbalanced dataset. Over the past years of the contest there was a large class of compound images that contained sub-images of several modalities something which made it difficult to train a classifier. This year there are no compound images in the subfigure classification subtask. However, both, the train and the test sets remain unbalanced with one very large category (GFIG, 2085) and some other categories that contain just few images(GPLI 2) or <ref type="bibr" coords="2,146.97,190.72,33.35,8.74">(DSEE,</ref><ref type="bibr" coords="2,183.63,190.72,7.75,8.74" target="#b2">3)</ref>. This year our group participated only in the subfigure classification subtask. Details of this task can be found in the overview paper <ref type="bibr" coords="2,378.55,214.88,10.52,8.74" target="#b2">[3]</ref> and the web page of the contest<ref type="foot" coords="2,184.96,225.26,3.97,6.12" target="#foot_0">1</ref> . Our approach to classification is based on merging two well known models, that of the BoW, <ref type="bibr" coords="2,251.18,238.79,10.52,8.74" target="#b3">[4]</ref> and a generalized version of bag of colors (BoC), <ref type="bibr" coords="2,134.77,250.75,10.52,8.74" target="#b4">[5]</ref> approach combined with early or late fusion which gave us the third best performing position.</p><p>In the next section we present a detailed description of the modelling techniques and data fusion used. In section 4, the classification tools and parameters are described as well as the submission runs with our results. Finally, Section 5 concludes our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Image Visual Representation</head><p>Inspired from text retrieval, the Bag-of-visual Words (BoW) approach has shown promising results in the field of image retrieval and classification. In this vein, we based our approach to the BoW model for the image classification task. In this section, we describe the methodology used for the visual and textual representation of images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Pyramid Histogram of Visual Words (PHOW)</head><p>PHOW is an extension of the BoW model used for image classification. In this model, we identify small regions (local interest points) known as, salient image patches that contain rich local information of the image. To extract such keypoints, the SIFT <ref type="bibr" coords="2,211.74,505.04,10.52,8.74" target="#b5">[6]</ref> or the Dense SIFT <ref type="bibr" coords="2,311.94,505.04,10.52,8.74" target="#b6">[7]</ref> descriptors are employed. However, the number of features extracted from local interest points may vary, depending on the image. In order to have a fixed number of feature dimensions, a visual codebook is created by clustering the extracted local interest points of a number of sample images, using the k-means clustering algorithm. Each cluster (visual word) represents a different local pattern, which shares similar interest points. The histogram of an image, is created by performing a vector quantization which assigns each key-point to its closest cluster (visual word) <ref type="bibr" coords="2,390.10,588.73,9.96,8.74" target="#b7">[8]</ref>. However, as it is known, the BoW model loses the spatial information of the local descriptors due to the clustering which, limits severely their discriminative power. Pyramid Histogram of Visual Words (PHOW) addresses this problem by dividing the image into increasingly fine sub-regions of equal size, which are called pyramids.</p><p>The histogram of visual words is computed in each local sub-region of the image and in the sequel they are concatenated into a single feature vector <ref type="bibr" coords="3,431.68,130.95,9.96,8.74" target="#b8">[9]</ref>. For our experiments, we partition the image into 2 × 2 and 4 × 4 sub-regions and then combine the generated quantizations. As for the size of the visual codebook, after experimentation with several values, we selected 1536 visual words. Thus each image was represented with a vector of 30720 features (2x2x1536 + 4x4x1536).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Quad-Tree Bag-of-Colors Model(QBoC)</head><p>With the BoC model <ref type="bibr" coords="3,234.82,253.06,10.52,8.74" target="#b4">[5]</ref> a color vocabulary is learned from a sub-set of the image collection. This vocabulary is used to extract the color histograms for each image. Through experiments, it has been shown that using a learned color vocabulary improves retrieval performance over a flat color space quantization. Furthermore, this model is succesfully fused with the SIFT descriptor into a compact binary signature <ref type="bibr" coords="3,251.12,312.84,15.50,8.74" target="#b9">[10]</ref> increasing further the performance of classification. The BoC model was used for classification of biomedical images in <ref type="bibr" coords="3,446.17,324.79,15.50,8.74" target="#b10">[11]</ref> and it was shown that it is combined successfully with the BoW-SIFT model in a late fusion manner. Similarly to the BoW model the main drawback with the BoC is the lack of spatial information. Furthermore, it is evident that the construction of the vocabulary and in particular the selection of its size is another weak point of the algorithm. To address this problem, we have extended the BoC model applying a quad-tree-decomposition of images <ref type="bibr" coords="3,344.52,396.52,14.61,8.74" target="#b11">[12]</ref>. Quad-Tree decomposition sub-divides an image into regions of homogeneous colors. Each time the image is split into four equal size squares and the process continues until we reach a sub-region of size 1 × 1 pixel (see figure <ref type="figure" coords="3,312.05,432.39,8.58,8.74" target="#fig_0">1b</ref>). To speed up the pre-processing of the images the Quad-Tree decomposition may end when we reach a sub-region of 2 × 2 pixels. Similar colors within a sub-region are quantized into the same color. This is tuned with an extra parameter which, was set to 0.15 in all our runs. In both models the TFIDF weights of visual words were calculated and the image vectors were normalized with the L 1 norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Textual Representation</head><p>The text representation for the sub-figure images is derived from the caption of their corresponding compound figures. The caption of a compound figure is assigned to all its constituent subfigures. This makes it difficult to distinguish between sub-images, and is a point to be improved in the future. For text retrieval we used the vector space model with TFIDF weights of the terms. While we didn't submit runs using textual information due to a misunderstanding, experimentation outside competition showned that stemming significantly drops the performance of categorization (see section 4.4). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Image Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiments Settings</head><p>All our experiments were conducted using several combinations of the two models described in section 2. For the classification the LibLinear classifier<ref type="foot" coords="4,456.00,364.30,3.97,6.12" target="#foot_1">2</ref> was employed, an open source library for large scale linear classification <ref type="bibr" coords="4,431.23,377.83,14.61,8.74" target="#b12">[13]</ref>. Linear SVMs are in general much faster to train and predict than the non-linear and can approximate large scale non-linear SVMs using a suitable feature map. Efficient feature mapping can be achieved using additive kernels, commonly employed in computer vision, with the homogeneous kernel map being the most common <ref type="bibr" coords="4,462.32,425.65,14.61,8.74" target="#b13">[14]</ref>. The homogeneous kernel map includes the intersection, Hellinger's, Jensen Shannon, Chi2, which allows large scale training of non-linear SVMs. The transformation of the data results into a compact linear representation which reproduces the desired kernel to a very good level of approximation. This transformation makes the use of linear SVM solvers feasible 3 , 4 . In our experiments, the homogeneous kernel mapping of VLFeat is used and more specifically Chi2 kernel. The implementation of VLFeat does not require any parameters but experiments have shown that results can be improved slightly by changing the Gamma parameter. The Gamma parameter sets the homogeneity degree of the kernel. The SVM model was tuned using n-fold cross validation to find the best cost. Lib-Linear has an embedded grid search which conducts n-fold cross validation with different costs and finds the best one. Besides from the cost parameter, that is discovered using grid search, bias multiplier and kernel type were given. Results were not greatly affected when varying bias multiplier or kernel type. After experimentation using several parameters, results yielded better performance with cost 10, Gamma 0.5 and the L2-Regularized L2-loss support vector classification kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Early and Late Data Fusion</head><p>In early fusion also referred to as feature fusion, <ref type="bibr" coords="5,348.90,181.68,14.60,8.74" target="#b14">[15]</ref>, image representation features extracted from different models are integrated into a single unified representation. Normalization techniques may be applied before the integration so that features are on the same scale. There is only one learning phase that handles all multimodal features together. Five of our submitted runs were conducted using early fusion. In late fusion also referred to as decision level fusion, multiple probabilistic output scores obtained from separate classifiers are combined into a single vector to form the final decision. Models are trained and classified separately and their respective outputs are combined to form the final decision. In contrast to the early fusion, late fusion requires two learning phases and there is a potential loss of correlation in the mixed feature space. Nevertheless, late fusion does not suffer from the integration problem early fusion does and can be easily used due to its simplicity and scalability. Five of our submitted runs were conducted using late fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Submitted Runs and Results</head><p>In this year's contest we submitted ten visual runs for the subfigure classification subtask. The results are presented in table 1. Early tests on the learning curves of our model on the imageCLEF 2013 dataset shown that the test error drops continuously with increasing the training instances. This suggests that with a larger dataset the test error would drop even more. Thus we have enriched the poorest train categories with new images. These categories were the following 14 The name of each run describes the methods and the parameters used in the run. For example, the first run in table 1, corresponds to an early fusion experiment on the enriched dataset of the -QBoC model, using a quad tree decomposition of the image terminating at a block of size 1 × 1 and a codebook of 256 colors in the RGB color space and the -BoW model, with the default PHOW 2 -level descriptor with 1536 features.</p><p>A color option is used to compute the color variant of the descriptor, i.e. RGB. The value of the parameter "default", denotes that the gray-scale variant of the descriptor is computed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head><p>From the confusion matrix, in figure <ref type="figure" coords="6,291.62,475.64,3.87,8.74" target="#fig_2">2</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper we presented the image classification techniques performed by the IPL Group for the subfigure classification subtask at ImageCLEF 2016 Medical Task. For our runs, we used Early and Late Fusion on two bag-of-visual-words models. The first model was a novel generalized version of the BoC model, and the second was the classical BoW with the PHOW descriptor to represent images. Our experiments show that using Early or Late Fusion performs better than any of the two models on their own. Providing visual image representation with textual representation, proved to be beneficial for classification accuracy. The results so far with our new approach of the QBoC model are encouraging and several new directions have emerged which need further investigation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,134.77,265.42,345.83,8.77;4,134.77,277.41,28.23,8.74;4,312.68,122.76,155.62,116.72"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Representation of image 1471-2202-11-1-4-2 (a) original image; (b) QBoC image.</figDesc><graphic coords="4,312.68,122.76,155.62,116.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,143.99,459.56,336.60,8.74;5,134.77,471.52,345.83,8.74;5,134.77,483.47,39.41,8.74;5,140.99,505.82,339.60,8.77;5,151.70,517.81,328.89,8.74;5,151.70,529.76,33.21,8.74;5,140.99,541.78,339.59,8.77;5,151.70,553.76,328.89,8.74;5,151.70,565.72,214.39,8.74"><head></head><label></label><figDesc>/30: DRAN, DRCO, DRCT, DRPE, DRUS, DRXR, DSEC, DSEE, DSEM, DVDM, DVEN, GFLO, GMAT, GPLI. Thus we have used two datasets with our runs: -Original Dataset: The original training collection distributed for the subgure classication task in ImageCLEF2016 Medical task containing 6776 images and the -Enriched Dataset: The original training collection was enriched with 482 images from the ImageCLEF 2013 Modality Classication training collection [16]. The enriched dataset contains 7258 images .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,251.62,390.46,112.12,8.77;6,186.64,198.85,242.08,181.56"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Confusion matrix.</figDesc><graphic coords="6,186.64,198.85,242.08,181.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,295.50,475.64,185.09,8.74;6,134.77,487.59,345.83,8.74;6,134.77,499.55,345.83,8.74;6,134.77,511.50,345.83,8.74;6,134.77,523.46,79.07,8.74;6,255.34,523.46,225.25,8.74;6,134.77,535.42,345.83,8.74;6,134.77,547.37,345.83,8.74;6,134.77,559.33,166.40,8.74;6,149.71,572.43,330.88,8.74;6,134.77,584.39,345.83,8.74;6,134.77,596.34,345.82,8.74;6,134.77,608.30,345.83,8.74;6,134.77,620.25,345.83,8.74;6,134.77,632.21,345.82,8.74;6,134.77,644.16,345.82,8.74;6,134.77,656.12,50.67,8.74"><head></head><label></label><figDesc>, corresponding to our first run, we see that in three categories, zero true positive examples were assigned. These categories were: the PET (DRPE) where the majority of the examples were classified into Computerized Tomography (DRCT) and the Electrocardiogpaphy (DSEC) and Electromyography categories where most of the examples were classified as statistical figures, graphs and charts (GFIG). These three categories happen to have the smallest learning sets even after the enrichment with new images with 30, 39, and 23 train images each.Although we submitted runs exclusively for visual categorization, for completeness, we present here our results for textual and mixed classification. Our textual representation of images was based on a naive TFIDF bag of words model with stopword removal and stemming. The textual classification on the enriched dataset attained an accuracy of 63.68% with stemming and 70.07% without stemming. Our mixed results combining QBoC with PHOW and Text in an early fashion mode, with weights (0.5, 0.3, 0.2) respectively, attained accuracy 86.9%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,141.38,117.74,332.46,117.73"><head>Table 1 .</head><label>1</label><figDesc>IPL submitted visual runs on subfigure classification.</figDesc><table coords="7,141.38,117.74,32.79,7.01"><row><cell>Run ID</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.73,657.44,174.17,7.47"><p>http://www.imageclef.org/2016/medical</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,144.73,624.57,211.83,9.21"><p>https://www.csie.ntu.edu.tw/ ~cjlin/liblinear/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,144.73,635.53,254.20,9.21"><p>http://www.robots.ox.ac.uk/ ~vgg/software/homkermap/#r1</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,144.73,646.48,320.10,7.47;4,144.73,657.44,84.73,7.47"><p>http://vision.princeton.edu/pvt/SiftFu/SiftFu/SIFTransac/vlfeat/doc/ api/homkermap.html</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,142.96,440.40,337.63,7.86;7,151.52,451.36,329.07,7.86;7,151.52,462.30,201.30,7.89" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,375.49,440.40,105.10,7.86;7,151.52,451.36,329.07,7.86;7,151.52,462.32,18.39,7.86">A review of content-based image retrieval systems in medical applications -clinical benefits and future directions</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Michoux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bandon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Geissbühler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,177.68,462.32,99.52,7.86">I. J. Medical Informatics</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,472.82,337.64,7.86;7,151.52,483.78,329.07,7.86;7,151.52,494.73,329.07,7.86;7,151.52,505.69,237.20,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,264.01,483.78,216.58,7.86;7,151.52,494.73,329.07,7.86;7,151.52,505.69,16.79,7.86">Evaluating performance of biomedical image retrieval systems-an overview of the medical image retrieval task at ImageCLEF review-2014</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,176.60,505.69,183.44,7.86">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,516.19,337.64,7.86;7,151.52,527.15,329.07,7.86;7,151.52,538.11,153.13,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,416.14,516.19,64.46,7.86;7,151.52,527.15,121.28,7.86">Overview of the ImageCLEF 2016 medical task</title>
		<author>
			<persName coords=""><forename type="first">García</forename><surname>Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Schaer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bromuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,293.48,527.15,187.11,7.86;7,151.52,538.11,74.23,7.86">Working Notes of CLEF 2016 (Cross Language Evaluation Forum)</title>
		<imprint>
			<date type="published" when="2016-09">September 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,548.60,337.64,7.86;7,151.52,559.56,168.25,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,244.56,548.60,236.04,7.86;7,151.52,559.56,38.24,7.86">A bayesian hierarchical model for learning natural scene categories</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,211.74,559.56,26.62,7.86">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="524" to="531" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,570.05,337.63,7.86;7,151.52,581.01,329.07,7.86;7,151.52,591.97,252.07,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,299.91,570.05,160.94,7.86">Bag-of-colors for improved image search</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wengert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,151.52,581.01,278.45,7.86">Proceedings of the 19th International Conference on Multimedia 2011</title>
		<meeting>the 19th International Conference on Multimedia 2011<address><addrLine>Scottsdale, AZ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-12-01">November 28 -December 1, 2011. 2011</date>
			<biblScope unit="page" from="1437" to="1440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,602.47,337.63,7.86;7,151.52,613.43,329.07,7.86;7,151.52,624.39,285.49,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,202.27,602.47,209.82,7.86">Object recognition from local scale-invariant features</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,432.52,602.47,48.07,7.86;7,151.52,613.43,217.56,7.86;7,457.04,613.43,23.55,7.86;7,151.52,624.39,10.75,7.86">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
	<note>ICCV &apos;99</note>
</biblStruct>

<biblStruct coords="7,142.96,634.88,337.63,7.86;7,151.52,645.84,329.07,7.86;7,151.52,656.80,247.22,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,312.43,634.88,168.16,7.86;7,151.52,645.84,37.06,7.86">Image classification using random forests and ferns</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Muñoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,214.15,645.84,266.45,7.86;7,151.52,656.80,16.79,7.86">IEEE 11th International Conference on Computer Vision, ICCV 2007</title>
		<meeting><address><addrLine>Rio de Janeiro, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">October 14-20, 2007. 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,119.67,337.64,7.86;8,151.52,130.63,329.07,7.86;8,151.52,141.59,329.07,7.86;8,151.52,152.55,129.77,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,378.99,119.67,101.60,7.86;8,151.52,130.63,175.94,7.86">Evaluating bag-of-visualwords representations in scene classification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">W</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,350.12,130.63,130.47,7.86;8,151.52,141.59,281.24,7.86">Proceedings of the International Workshop on Workshop on Multimedia Information Retrieval. MIR &apos;07</title>
		<meeting>the International Workshop on Workshop on Multimedia Information Retrieval. MIR &apos;07<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="197" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,163.51,337.63,7.86;8,151.52,174.44,329.07,7.89;8,151.52,185.43,25.60,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="8,234.55,163.51,246.04,7.86;8,151.52,174.47,227.92,7.86">What you need to know about the state-of-the-art computational models of object-vision: A tour through the models</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Khaligh-Razavi</surname></persName>
		</author>
		<idno>CoRR abs/1407.2776</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,196.39,337.98,7.86;8,151.52,207.32,289.17,7.89" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,292.38,196.39,188.21,7.86;8,151.52,207.34,23.54,7.86">Improving bag-of-features for large scale image search</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,183.07,207.34,168.17,7.86">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="316" to="336" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,218.30,337.97,7.86;8,151.52,229.26,329.07,7.86;8,151.52,240.22,156.00,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,339.36,218.30,141.23,7.86;8,151.52,229.26,100.19,7.86">Bag-of-colors for biomedical document image classification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Markonis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,274.38,229.26,206.21,7.86;8,151.52,240.22,49.70,7.86">Medical Content-Based Retrieval for Clinical Decision Support</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="110" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,251.18,337.98,7.86;8,151.52,262.14,116.82,7.86" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="8,297.61,251.18,182.98,7.86;8,151.52,262.14,46.13,7.86">Quadtree representation and compression of spatial data</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Düntsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gediga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,273.10,337.98,7.86;8,151.52,284.03,307.35,7.89" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,401.85,273.10,78.75,7.86;8,151.52,284.06,112.07,7.86">Liblinear: A library for large linear classification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><forename type="middle">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,271.23,284.06,83.91,7.86">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008-06">June 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,295.02,337.98,7.86;8,151.52,305.95,281.55,7.89" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="8,270.40,295.02,205.48,7.86">Efficient additive kernels via explicit feature maps</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,151.52,305.98,163.41,7.86">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="480" to="492" />
			<date type="published" when="2012-03">March 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,316.93,337.98,7.86;8,151.52,327.89,329.07,7.86;8,151.52,338.85,329.07,7.86;8,151.52,349.81,245.73,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="8,317.80,316.93,162.79,7.86;8,151.52,327.89,178.85,7.86">Information fusion for combining visual and textual image retrieval in imageclef@icpr</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Depeursinge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,351.54,327.89,129.05,7.86;8,151.52,338.85,329.07,7.86;8,151.52,349.81,32.70,7.86">Proceedings of the 20th International Conference on Recognizing Patterns in Signals, Speech, Images, and Videos. ICPR&apos;10</title>
		<meeting>the 20th International Conference on Recognizing Patterns in Signals, Speech, Images, and Videos. ICPR&apos;10<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="129" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,360.77,337.97,7.86;8,151.52,371.73,329.07,7.86" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="8,469.58,360.77,11.01,7.86;8,151.52,371.73,192.56,7.86">In: Overview of the ImageCLEF 2013 medical tasks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="CEUR-WS" />
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">1179</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
