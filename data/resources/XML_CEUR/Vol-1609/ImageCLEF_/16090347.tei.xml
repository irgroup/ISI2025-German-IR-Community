<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,150.48,115.96,314.41,12.62;1,217.64,133.89,180.08,12.62">MayoBMI at ImageCLEF 2016 Handwritten Document Retrieval Task</title>
				<funder ref="#_mjwRxFz">
					<orgName type="full">National Library of Medicine</orgName>
					<orgName type="abbreviated">NLM</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,143.81,171.56,35.39,8.74"><forename type="first">Sijia</forename><surname>Liu</surname></persName>
							<email>liu.sijia@mayo.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Section of Biomedical Informatics</orgName>
								<orgName type="institution">Mayo Clinic</orgName>
								<address>
									<postCode>55905</postCode>
									<settlement>Rochester</settlement>
									<region>MN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,186.83,171.56,62.60,8.74"><forename type="first">Yanshan</forename><surname>Wang</surname></persName>
							<email>wang.yanshan@mayo.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Section of Biomedical Informatics</orgName>
								<orgName type="institution">Mayo Clinic</orgName>
								<address>
									<postCode>55905</postCode>
									<settlement>Rochester</settlement>
									<region>MN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,258.29,171.56,62.40,8.74"><forename type="first">Saeed</forename><surname>Mehrabi</surname></persName>
							<email>mehrabi.saeed@mayo.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Section of Biomedical Informatics</orgName>
								<orgName type="institution">Mayo Clinic</orgName>
								<address>
									<postCode>55905</postCode>
									<settlement>Rochester</settlement>
									<region>MN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,328.89,171.56,56.68,8.74"><forename type="first">Dingcheng</forename><surname>Li</surname></persName>
							<email>li.dingcheng@mayo.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Section of Biomedical Informatics</orgName>
								<orgName type="institution">Mayo Clinic</orgName>
								<address>
									<postCode>55905</postCode>
									<settlement>Rochester</settlement>
									<region>MN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,412.19,171.56,59.36,8.74"><forename type="first">Hongfang</forename><surname>Liu</surname></persName>
							<email>liu.hongfang@mayo.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Section of Biomedical Informatics</orgName>
								<orgName type="institution">Mayo Clinic</orgName>
								<address>
									<postCode>55905</postCode>
									<settlement>Rochester</settlement>
									<region>MN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,150.48,115.96,314.41,12.62;1,217.64,133.89,180.08,12.62">MayoBMI at ImageCLEF 2016 Handwritten Document Retrieval Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">87CB77DBE7BF05EA287E9722FA17FC33</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>handwriting recognition</term>
					<term>hyphenation detection</term>
					<term>text retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this working note, we introduce our participation at the ImageCLEF 2016 Handwritten Document Retrieval Task. We mainly focused on hyphenation detection using line images and information retrieval using n-best results. The hyphenation detection step utilizes extracted image features from beginning and end of a line and a binary classifier to determine if a line contains hyphenation. Then the spell correction step is used to eliminate spelling errors from the concatenation of a broken word from the end of a line and the beginning of the next line. The final text retrieval step employs a suffix stripping algorithm to normalize the word tense and form and TF-IDF scheme to rank the retrieved relevant segment results of our submission.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>For the ImageCLEF 2016 Handwritten Scanned Document Retrieval Task <ref type="bibr" coords="1,459.56,464.84,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="1,470.07,464.84,7.01,8.74" target="#b1">2]</ref>, our aim is to develop a document retrieval system to retrieve the relevant segments and word bounding boxes for given string of test queries. An intuitive solution for this task generally includes three components: handwritten text recognition, keyword spotting and text retrieval. To obtain relatively accurate transcripts from document images, image pre-processing methods such as image binarization and text line extraction are generally used <ref type="bibr" coords="1,414.74,536.57,10.52,8.74" target="#b2">[3,</ref><ref type="bibr" coords="1,425.26,536.57,7.01,8.74" target="#b3">4]</ref>. Based on whether trying to generate transcripts from the handwritten text images as an intermediate step, there are mainly two categories of solutions: recognition based approaches and keyword spotting based approaches. For the recognition based approaches, there are two kinds of models commonly used in the state-ofart handwritten recognition systems for historical documents: Recurrent Neural Network (RNN) with Connectionist Temporal Classification (CTC) <ref type="bibr" coords="1,441.63,608.30,10.96,8.74">[5,</ref><ref type="bibr" coords="1,452.59,608.30,7.31,8.74" target="#b5">6]</ref> and Hidden Markov Model (HMM) <ref type="bibr" coords="1,276.07,620.25,10.79,8.74" target="#b6">[7,</ref><ref type="bibr" coords="1,286.86,620.25,7.20,8.74" target="#b7">8,</ref><ref type="bibr" coords="1,294.06,620.25,7.20,8.74">9]</ref>. Both of these models can achieve high recognition accuracy, which is measured in word and character error rates. For keyword based approaches, depending on whether the query is a word image or a string in the dataset, systems can either query the keyword by comparing The green texts are the ground truth, and the red texts are the recognition errors. In (a), the ending word "portable" is missing in 1-best. In line (b), the prediction of text "fixed Book. Taken" is mistakenly recognized as "by Each Silver " and then ending word by hyphenation "Exche-" is also mistakenly recognized as "The". In line (c), the recognition system mistakenly added an ending hyphenation "=-" to the line. the features of query images with the indexed features of existing images in the dataset or transcribe the document before retrieving the query string. Language models like n-gram and HMM <ref type="bibr" coords="2,277.83,435.03,15.50,8.74" target="#b9">[10]</ref> can also be used to improve the overall keyword spotting performance.</p><p>Although the handwritten document recognition has attracted much attention in the areas of image processing and machine learning, the impact of hyphenated words is not well studied. Hyphenated words, also known as broken words, are the words supposed to appear at the end of a text line while broken and continued at the beginning of the next line because of the manuscript writer's intention to save space. Hyphens will be used to mark such broken words at the end of line by "-" or "=" and at beginning of a line by ":". Hyphenations may cause recognition errors if they are not processed properly. Example line images, recognition errors in 1-best transcripts and the corresponding ground truth from the task dataset are shown in Fig. <ref type="figure" coords="2,288.55,569.48,3.87,8.74">1</ref>. We are interested in this working note to describe a solution to detect words with hyphenation in segmented line images, which can further be used to improve the retrieval result of given strings of queries.</p><p>The rest of this working note is organized as follows. First, we introduce our proposed methods in Section 2, including steps of image preprocessing of line segmented image, hyphenation detection, spell correction using training transcript and text retrieval. Second, the evaluation of the effectiveness of hyphenation detection methods and the official task evaluation of our submitted run are discussed in Section 3. Finally, we conclude our work and proposed some future works and improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Methods</head><p>In our proposed solution to ImageCLEF 2016 Handwritten Document Retrieval Task, we mainly focused on how to detect the hyphenated words, how to do spell correction for detected hyphenated words and how to retrieve the relevant segments based on 1-best results. In this section, we will elaborate the details of each step. We first describe the preprocessing step of how the segmented grayscale line images is normalized into fixed-height binary images (Section 2.1). Then using normalized line images, a hyphenation detection methods is proposed (Section 2.2) to detect lines with hyphenation, followed by spell correction (Section 2.3) and text retrieval step (Section 2.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preprocessing</head><p>In this step, our goal for the preprocessing step is to obtain noise and slant free binary line images. In some related works, skew correction may also be necessary before the recognition or word spotting step. However, in the task dataset, lines are well segmented and with only negligible slope, which ease us from skew correction. The slant of written lines are removed by applying a two dimensional affine transformation to the original line images. The transformation matrix is arbitrarily chosen based on the observation from random selected line images in the training set. Afterwards, a global threshold is applied to the slant corrected grayscale line images to generate the binary line images. We also resize the line images to a fixed height of 30, and the width is scaled proportionally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Hyphenation Detection</head><p>In order to detect lines with beginning and ending hyphenations, image windows at both the beginning and end of each line image are obtained. Several image features are then extracted from the image windows, and various binary classifiers are used to detect hyphenations. As a binary classification problem, according to the writing style of the document writer, lines containing both the end hyphens and beginning hyphens in the next line are considered lines with hyphenation. Lines with such hyphenations are labeled as positive, while the others are labeled as negative. This strict labeling rule is helpful to eliminate false positives in the prediction results.</p><p>Several binary classification methods are used on the image features in the training set. To evaluate the performance of these methods, precision, recall and F-score are tested as metrics in the development set. In this task, the ground truth transcripts of both training and development set are provided, thus the ground truth labels can be obtained and used to compare with the hyphenation detection results.</p><p>To represent the binary line image windows as feature vectors, a set of local features are extracted from the beginning windows and ending windows for each line. These features have been used in previous works on keyword spotting approaches <ref type="bibr" coords="4,186.61,178.77,14.61,8.74" target="#b9">[10]</ref>. There are 8 features of the image window describing whether they contain hyphens. They are: the horizontal and vertical locations of the first non-background pixel in the window, the horizontal and vertical locations of the last non-background pixel in the window, the average intensity, the second order moment and the coordinates of the window centroid. Further, the window is cropped with the tight rectangular bounding box by removing all the lines with no non-background pixel on the boundary. Then the average and second order moment of intensity and the window centroid are recalculated and combined with previous features. Besides, the number of non-background pixels are summed up by each line and column, which generates pixel histograms horizontally and vertically. In this work, we use a window with width of 20 and height of 30. Therefore, a feature vector with 8 + 4 + 20 + 30 = 62 dimensions are extracted to represent the window. For each line, both the ending window of the current line and the beginning window of the next line are used to determine whether the line contains hyphenation or not. Thus, the dimension of feature vector of each line is doubled, resulting in a feature vector of 2 × 62 = 124 dimensions for each line. The feature vector is then used as the input of various machine learning methods.</p><p>The classifiers investigated are implemented in Scikit-Learn <ref type="bibr" coords="4,438.21,393.96,14.61,8.74" target="#b10">[11]</ref>. The classifiers which are tested on development set are: 5 Nearest Neighbors, Decision Tree, Random Forest, AdaBoost, Naive Bayes with Gaussian kernel. Among these classifiers, our experiment in development set suggested that AdaBoost can provide the best prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Spell Correction</head><p>Once the hyphenation of a certain line is detected, the last word of the current line and the first word of the next line are concatenated. The text from the two word windows are extracted from the 1-best result, and all the special characters are removed before concatenation.</p><p>From the 1-best documents in both the training and development dataset, the accuracy of recognition without hyphen is relatively high. However, for hyphenated words, word spotting algorithms tend to predict the given word image as the most similar complete word, instead of considering it as only the front part or the back part of a word. For example, the word "testimony", if broken as "testi-mony" into the end and beginning of two lines, the latter part is more likely to be predicted as "many" which is a complete word, instead of "mony" which is a suffix in ground truth. The corrections with maximum likelihood to the current prediction in the dictionary is chosen. If no matched correction under these criteria is found, the original word is remained unchanged. The implementation of the spell corrector can be found in <ref type="bibr" coords="5,174.33,370.25,14.61,8.74" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Text Retrieval</head><p>Figure <ref type="figure" coords="5,167.93,434.97,4.98,8.74">2</ref> shows the overview of information retrieval system that we utilize to retrieve the relevant segments. The system basically consists of two components: indexing and retrieval. In the indexing component, we annotate the textual documents and then create the segments for indexing. In the retrieval component, the queries are used as input to the retrieval model, and then the relevant segments are retrieved from the indexing database. In the following we will describe each component in detail.</p><p>The textual document are preprocessed before indexing. In the submitted run, we utilized the 1-best transcript document as the input for the retrieval system. According to the definition of segments in this task, 6 consecutive lines in the document are concatenated to create the segments. Specifically, the segments are defined by a sliding window with the size of 6 that moves one line at a time until the bottom of the sliding window is reached at the end of the document. Prior to creating the segments, we add the bounding boxes information into the 1-best transcript documents. More precisely, words are separated in each line, and subsequently attached the line number and bounding boxes to each word as the prefix and suffix, respectively. The format of each annotated word is defined as:</p><formula xml:id="formula_0" coords="5,262.00,656.12,218.59,8.74">L T W H X Y, (<label>Format 1)</label></formula><p>where L is the line number, T represents the word, W and H are the width and height of the bounding box, and X and Y are the left-top coordinate of the box, respectively. We used the software package Elasticsearch<ref type="foot" coords="6,351.67,153.54,3.97,6.12" target="#foot_0">1</ref> to index the segments. A total of 16939 segments were indexed into three fields: "ID", "contents" and "annotations". In the "ID" field, the ID of the first line in each segment was used to distinguish the segments. In the "contents" field, we only indexed the words, i.e., W in format 1, in the segments. We applied the following hyphen-rule to take the hyphenated words into consideration:</p><p>If line i ends with any of =, -, and :, and line i + 1 starts with either -or :, then the last term in line i and the first term in line i + 1 are considered a hyphenated word.</p><p>(Rule 1) The broken words are concatenated together after removing the hyphens. The spell corrector described in previous section is then used. Before indexing, we also applied the Porter stemmer <ref type="bibr" coords="6,281.64,314.46,15.50,8.74" target="#b12">[13]</ref> to the "contents" field. By doing so, the words like "abuses", "abusing" and "abused" were also retrieved given the query "abuse". In the "annotations" field if rule 1 was found, the corresponding annotation terms represented in Format 1 were connected by "=:". Therefore, for each segment the number of words in the "contents" field is equivalent to the number of annotation terms in the "annotations" field.</p><p>In the retrieval component, we utilize the Term Frequency -Inverse Document Frequency (TF-IDF) scheme <ref type="bibr" coords="6,322.87,398.40,15.50,8.74" target="#b13">[14]</ref> to rank the segments in the "contents" field and retrieved the top 30 segments. According to the positions of the matched word in the "contents" field, the corresponding annotation terms are retrieved from the "annotations" field. Using the annotation terms, the bounding boxes as well as the queries are created as the final submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Evaluation</head><p>In this section, we elaborate the details of experiments and the performance of hyphenation detection and ImageCLEF 2016 Handwritten Scanned Document Retrieval Task evaluation.</p><p>The task dataset is a subset of scanned and manually transcribed manuscripts written by Jeremy Bentham under the Transcribe Bentham project <ref type="bibr" coords="6,433.92,552.60,11.15,8.74" target="#b1">[2,</ref><ref type="bibr" coords="6,445.07,552.60,11.15,8.74" target="#b14">15]</ref> For the hyphenation detection step, the ground truth are extracted from transcripts of training and development set. As expected, the dataset is significantly unbalanced. In the training set, there are only 810 positive samples in 9645 lines, the proportion of positive samples is 8.4%. In development set, there are only 853 positive samples in 10589 total samples. The percentage of positive samples is 8.0%. The trained models from training set are used for the classification of the development set. The precision, recall and F-score metrics of the development set is shown in Table <ref type="table" coords="7,314.32,178.77,3.87,8.74" target="#tab_0">1</ref>. From the experiment results we can observe that AdaBoost is the best performed classifier for hyphenation detection in the proposed feature set. The detection algorithm does not perform well in development set, thus we do not include it into the final submission. The spell corrector is still used in text retrieval step to handle the hyphens already in the 1-best results. We submitted one run as the task submission, and the official evaluation results are shown in Table <ref type="table" coords="7,257.08,405.52,3.87,8.74" target="#tab_1">2</ref>. We noticed there is a significant drop from the results of the development set to those of the test set. The similar performance decreases can be also found in the results of the baseline system, which uses exact string matching and should be robust among different datasets if these datasets are of similar characteristics and quality. The reason for this significant performance difference is because the test set is considerably more difficult than the development set, where the bounding box is much less accurate and the images quality is lower. We discussed our participation of ImageCLEF 2016 Handwritten Scanned Document Retrieval Task. Our submission run is based on a three step process, hyphenation detection, spell correction and information retrieval. The hyphenation detection step utilizes extracted image features from beginning and end of lines and a binary classifiers to determine if a line contains hyphenation or not. Then the spell correction step is used to eliminate spelling errors the concatenation of a broken word from a line ending and the line beginning of the next line. The spell correction step uses only the vocabulary from the transcript of training set. A final information retrieval step employs a suffix stripping algorithm to normalize the word tense and form and TF-IDF scheme to rank the retrieved 30 segments as the output of our system.</p><p>There are several future works that can be investigated to improve the performance of our system. First, more line image features can be considered as the input of supervised binary classification methods. Second, larger lexicon can be utilized in both the hyphenation detection step and spell correction step. Due to the task restriction, only the vocabulary in the training set can be used, and the use of external data for learning a language model is prohibited. A larger lexicon of the whole dataset or external data rather than only the training set will improve the effectiveness of the spell corrector.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,134.77,287.31,345.83,8.74;2,134.77,299.26,345.82,8.74;2,134.77,311.22,345.83,8.74;2,134.77,323.17,345.83,8.74;2,134.77,335.13,345.82,8.74;2,134.77,347.08,345.83,8.74;2,134.77,359.04,47.60,8.74"><head>1-best: 6 .Fig. 1 :</head><label>61</label><figDesc>Fig.1: Three line examples of errors in 1-best predictions of development set. The green texts are the ground truth, and the red texts are the recognition errors. In (a), the ending word "portable" is missing in 1-best. In line (b), the prediction of text "fixed Book. Taken" is mistakenly recognized as "by Each Silver " and then ending word by hyphenation "Exche-" is also mistakenly recognized as "The". In line (c), the recognition system mistakenly added an ending hyphenation "=-" to the line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,206.46,247.36,202.44,8.74"><head>Fig. 1 Fig. 2 :</head><label>12</label><figDesc>Fig. 2: Information Retrieval System Overview</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,456.22,552.60,24.37,8.74;6,134.77,564.55,345.83,8.74;6,134.77,576.51,345.83,8.74;6,134.77,588.46,345.83,8.74;6,134.77,600.42,345.82,8.74;6,134.77,612.38,61.77,8.74"><head></head><label></label><figDesc>. The task dataset consists of three subsets: training, development and test set. The training and development set contains 9645 and 10589 manually segmented line images, respectively. The test set contains the 10589 line images in the development set and 6355 line images exclusive in test set, resulting in a total of 16944 lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,223.07,278.99,169.23,75.39"><head>Table 1 :</head><label>1</label><figDesc>Hyphenation detection results</figDesc><table coords="7,223.18,291.33,168.99,63.06"><row><cell></cell><cell>Precision Recall F-score</cell></row><row><cell cols="2">Nearest Neighbor 0.514 0.237 0.325</cell></row><row><cell>Decision Tree</cell><cell>0.545 0.225 0.319</cell></row><row><cell>Random Forest</cell><cell>0.340 0.412 0.373</cell></row><row><cell>AdaBoost</cell><cell>0.301 0.650 0.411</cell></row><row><cell>Naive Bayes</cell><cell>0.477 0.263 0.339</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,169.35,531.59,276.66,87.35"><head>Table 2 :</head><label>2</label><figDesc>ImageCLEF Handwritten Scanned Document Retrieval Task evaluation results in percentage (%)</figDesc><table coords="7,231.60,555.88,152.16,63.06"><row><cell></cell><cell>Development</cell><cell>Test</cell></row><row><cell></cell><cell cols="2">Segment Box Segment Box</cell></row><row><cell>gAP</cell><cell cols="2">25.76 18.40 2.53 1.02</cell></row><row><cell>mAP</cell><cell cols="2">23.41 18.37 2.85 1.67</cell></row><row><cell cols="3">gNDCG 33.05 25.71 6.96 3.42</cell></row><row><cell cols="3">mNDCG 26.56 22.24 3.62 2.48</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="6,144.73,656.80,185.90,7.86"><p>https://www.elastic.co/products/elasticsearch</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5">Acknowledgement</head><p>The authors gratefully acknowledge the support from the <rs type="funder">National Library of Medicine (NLM)</rs> grant <rs type="grantNumber">R01LM11934</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_mjwRxFz">
					<idno type="grant-number">R01LM11934</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,142.96,472.23,337.64,7.86;8,151.52,483.19,329.07,7.86;8,151.52,494.15,329.07,7.86;8,151.52,505.10,329.07,7.86;8,151.52,516.06,199.40,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,449.27,494.15,31.32,7.86;8,151.52,505.10,204.09,7.86">General Overview of ImageCLEF at the CLEF 2016 Labs</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schaer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bromuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ramisa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dellandrea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">H</forename><surname>Toselli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="8,367.02,505.10,113.57,7.86;8,151.52,516.06,27.78,7.86">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,526.59,337.63,7.86;8,151.52,537.55,329.07,7.86;8,151.52,546.24,329.07,10.13;8,151.52,559.47,43.00,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,417.99,526.59,62.61,7.86;8,151.52,537.55,263.92,7.86">Overview of the ImageCLEF 2016 Handwritten Scanned Document Retrieval Task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">H</forename><surname>Toselli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Vidal</surname></persName>
		</author>
		<ptr target=".org" />
	</analytic>
	<monogr>
		<title level="m" coord="8,437.46,537.55,43.13,7.86;8,151.52,548.51,189.27,7.86">CLEF2016 Working Notes. CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Évora, Portugal, CEUR-WS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-09">Sep 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,569.99,337.64,7.86;8,151.52,580.93,329.07,7.89" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,277.47,569.99,198.87,7.86">Text line extraction for historical document images</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Saabni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Asi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>El-Sana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,151.52,580.95,111.11,7.86">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="23" to="33" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,591.48,337.63,7.86;8,151.52,602.44,329.07,7.86;8,151.52,613.40,329.07,7.86;8,151.52,624.35,212.66,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,308.25,591.48,172.34,7.86;8,151.52,602.44,230.00,7.86">Text extraction from gray scale historical document images using adaptive local connectivity map</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Setlur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Govindaraju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,409.37,602.44,71.22,7.86;8,151.52,613.40,224.93,7.86">Proc. Eighth Int. Conf. Document Analysis and Recognition. ICDAR 05</title>
		<meeting>Eighth Int. Conf. Document Analysis and Recognition. ICDAR 05<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005-08">August 2005</date>
			<biblScope unit="page" from="794" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,634.88,143.78,7.86;8,309.67,634.88,170.92,7.86;8,151.52,645.84,329.07,7.86;8,151.52,656.80,193.50,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,309.67,634.88,170.92,7.86;8,151.52,645.84,184.71,7.86">Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,347.98,645.84,132.61,7.86;8,151.52,656.80,78.18,7.86">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="545" to="552" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,119.67,337.64,7.86;9,151.52,130.61,224.45,7.89" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="9,362.49,119.67,118.11,7.86;9,151.52,130.63,93.30,7.86">Citlab ARGUS for historical handwritten documents</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strauß</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Grüning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Leifert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Labahn</surname></persName>
		</author>
		<idno>CoRR abs/1412.3949</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,141.59,337.64,7.86;9,151.52,152.52,329.07,7.89;9,151.52,163.51,13.82,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,354.66,141.59,125.93,7.86;9,151.52,152.55,126.13,7.86">Lexicon-free handwritten word spotting using character HMMs</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Frinken</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,289.37,152.55,113.96,7.86">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="934" to="942" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,174.47,337.64,7.86;9,151.52,185.43,307.60,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,342.96,174.47,133.51,7.86">Efficient Exemplar Word Spotting</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Almazán</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fornés</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Valveny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,151.52,185.43,213.72,7.86">Procedings of the British Machine Vision Conference</title>
		<meeting>edings of the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="1" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,196.39,337.63,7.86;9,151.52,207.32,329.07,7.89;9,151.52,218.30,41.47,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,315.88,196.39,164.71,7.86;9,151.52,207.34,175.90,7.86">Handwritten word-spotting using hidden Markov models and universal vocabularies</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Rodríguez-Serrano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,340.25,207.34,83.03,7.86">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2106" to="2116" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,229.26,116.09,7.86;9,273.76,229.26,206.84,7.86;9,151.52,240.22,329.07,7.86;9,151.52,251.15,329.07,7.89;9,151.52,262.14,51.70,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,273.76,229.26,206.84,7.86;9,151.52,240.22,324.78,7.86">Using a statistical language model to improve the performance of an hmm-based cursive handwriting recognition system</title>
		<author>
			<persName coords=""><forename type="first">U</forename><forename type="middle">V</forename><surname>Marti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,151.52,251.18,297.36,7.86">International Journal of Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="65" to="90" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,273.10,337.98,7.86;9,151.52,284.06,329.07,7.86;9,151.52,295.02,329.07,7.86;9,151.52,305.95,324.32,7.89" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,394.55,295.02,86.04,7.86;9,151.52,305.98,73.64,7.86">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,233.84,305.98,155.11,7.86">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,316.93,337.97,8.11;9,151.52,328.54,18.83,7.47" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Norvig</surname></persName>
		</author>
		<ptr target="http://norvig.com/spell-correct.html" />
		<title level="m" coord="9,196.41,316.93,126.74,7.86">How to write a spelling corrector</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,338.83,328.84,7.89" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,208.89,338.85,130.20,7.86">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,347.02,338.85,34.98,7.86">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,349.81,337.97,7.86;9,151.52,360.77,63.99,7.86" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="9,255.72,349.81,179.93,7.86">Introduction to modern information retrieval</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Mcgill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>McGraw-Hill, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,371.73,337.97,7.86;9,151.52,382.66,253.02,7.89" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="9,236.55,371.73,244.04,7.86;9,151.52,382.69,76.51,7.86">Building a volunteer community: Results and findings from transcribe bentham</title>
		<author>
			<persName coords=""><forename type="first">Tim</forename><surname>Causer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,236.86,382.69,118.74,7.86">Digital Humanities Quarterly</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
