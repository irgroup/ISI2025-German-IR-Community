<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,156.22,116.95,302.91,12.62;1,192.64,134.89,230.07,12.62">Overview of the ImageCLEF 2016 Scalable Concept Image Annotation Task</title>
				<funder ref="#_hkJ2Daz">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,135.51,172.56,67.26,8.74"><forename type="first">Andrew</forename><surname>Gilbert</surname></persName>
						</author>
						<author>
							<persName coords="1,210.35,172.56,45.44,8.74"><forename type="first">Luca</forename><surname>Piras</surname></persName>
						</author>
						<author>
							<persName coords="1,263.30,172.56,52.77,8.74"><forename type="first">Josiah</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName coords="1,324.92,172.56,31.14,8.74"><forename type="first">Fei</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<persName coords="1,364.36,172.56,60.80,8.74"><forename type="first">Arnau</forename><surname>Ramisa</surname></persName>
						</author>
						<author>
							<persName coords="1,433.49,172.56,46.35,8.74;1,141.02,184.51,45.18,8.74"><forename type="first">Emmanuel</forename><surname>Dellandrea</surname></persName>
						</author>
						<author>
							<persName coords="1,194.04,184.51,79.86,8.74"><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
						</author>
						<author>
							<persName coords="1,281.84,184.51,75.91,8.74"><forename type="first">Mauricio</forename><surname>Villegas</surname></persName>
						</author>
						<author>
							<persName coords="1,380.44,184.51,93.90,8.74"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
						</author>
						<title level="a" type="main" coord="1,156.22,116.95,302.91,12.62;1,192.64,134.89,230.07,12.62">Overview of the ImageCLEF 2016 Scalable Concept Image Annotation Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D9588782D1F0DFC138E1D2EFD8B9C5F4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Since 2010, ImageCLEF has run a scalable image annotation task, to promote research into the annotation of images using noisy web page data. It aims to develop techniques to allow computers to describe images reliably, localise different concepts depicted and generate descriptions of the scenes. The primary goal of the challenge is to encourage creative ideas of using web page data to improve image annotation. Three subtasks and two pilot teaser tasks were available to participants; all tasks use a single mixed modality data source of 510,123 web page items for both training and test. The dataset included raw images, textual features obtained from the web pages on which the images appeared, as well as extracted visual features. Extracted from the Web by querying popular image search engines, the dataset was formed. For the main subtasks, the development and test sets were both taken from the "training set". For the teaser tasks, 200,000 web page items were reserved for testing, and a separate development set was provided. The 251 concepts were chosen to be visual objects that are localizable and that are useful for generating textual descriptions of the visual content of images and were mined from the texts of our extensive database of image-webpage pairs. This year seven groups participated in the task, submitting over 50 runs across all subtasks, and all participants also provided working notes papers. In general, the groups' performance is impressive across the tasks, and there are interesting insights into these very relevant challenges.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>How can you use large-scale noisy data to improve image classification, caption generation and text illustration? This challenging question is the basis of this year's image annotation challenge. Every day, users struggle with the everincreasing quantity of data available to them. Trying to find "that" photo they took on holiday last year, the image on Google of their favourite actress or band, or the images of the news article someone mentioned at work. There are a huge number of images that can be cheaply found and gathered from the Internet. However, more valuable is mixed-modality data, for example, web pages containing both images and text. A significant amount of information about the image is present on these web pages and vice-versa. However, the relationship between the surrounding text and images varies greatly, with much of the text being redundant and unrelated. Despite the obvious benefits of using such information in automatic learning, the weak supervision it provides means that it remains a challenging problem. Fig. <ref type="figure" coords="1,248.39,657.11,4.98,8.74" target="#fig_0">1</ref> illustrates the expected results of the task. The Scalable Concept Image Annotation task is a continuation of the general image annotation and retrieval task that has been part of ImageCLEF since its very first edition in 2003. In the early years the focus was on retrieving relevant images from a web collection given (multilingual) queries, from 2006 onwards annotation tasks were also held, initially aimed at object detection, but more recently also covering semantic concepts. In its current form, the 2016 Scalable Concept Image Annotation task is its fifth edition, having been organized in 2012 <ref type="bibr" coords="2,158.47,359.15,14.61,8.74" target="#b23">[24]</ref>, 2013 <ref type="bibr" coords="2,204.21,359.15,14.61,8.74" target="#b25">[26]</ref>, 2014 <ref type="bibr" coords="2,249.95,359.15,14.61,8.74" target="#b24">[25]</ref>, and 2015 <ref type="bibr" coords="2,315.52,359.15,9.96,8.74" target="#b7">[8]</ref>. In the 2015 edition <ref type="bibr" coords="2,420.46,359.15,9.96,8.74" target="#b7">[8]</ref>, the image annotation task was expanded to concept localization and also natural language sentential description of images. In this year's edition, we further introduced a text illustration 'teaser' task, to evaluate systems that analyse a text document and select the best illustration for the text from a large collection of images provided. As there is an increased interest in recent years in research combining text and vision, the new tasks introduced in both the 2015 and 2016 editions aim at further stimulating and encouraging multimodal research that uses both text and visual data for image annotation and retrieval.</p><p>This paper presents the overview of the fifth edition of the Scalable Concept Image Annotation task <ref type="bibr" coords="2,258.86,478.72,15.84,8.74" target="#b23">[24,</ref><ref type="bibr" coords="2,274.69,478.72,11.88,8.74" target="#b25">26,</ref><ref type="bibr" coords="2,286.57,478.72,11.88,8.74" target="#b24">25,</ref><ref type="bibr" coords="2,298.45,478.72,7.92,8.74" target="#b7">8]</ref>, one of the three benchmark campaigns organized by ImageCLEF <ref type="bibr" coords="2,248.76,490.68,15.50,8.74" target="#b21">[22]</ref> in 2016 under the CLEF initiative <ref type="foot" coords="2,414.60,489.10,3.97,6.12" target="#foot_0">1</ref> . Section 2 describes the task in detail, including the participation rules and the provided data and resources. Section 3 presents and discusses the results of the submissions received for the task. Finally, Section 4 concludes the paper with final remarks and future outlooks.</p><p>2 Overview of the Task</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Motivation and Objectives</head><p>Image annotation has relied on training data that has been manually, and thus reliably annotated. Annotating training data is an expensive and laborious endeavour that cannot be easily scaled, particularly as the number of concepts  grows. However, images for any topic can be cheaply gathered from the Web, along with associated text from the web pages that contain the images. The degree of relationship between these web images and the surrounding text varies considerably, i.e., the data are very noisy, but overall these data contain useful information that can be exploited to develop annotation systems. Figure <ref type="figure" coords="3,447.70,424.81,4.98,8.74" target="#fig_2">2</ref> shows examples of typical images found by querying search engines. As can be seen, the data obtained are useful and furthermore a wider variety of images is expected, not only photographs but also drawings and computer generated graphics. This diversity has the advantage that this data can also handle the different possible senses that a word can have or the various types of images that exist. Likewise, there are other resources available that can help to determine the relationships between text and semantic concepts, such as dictionaries or ontologies. There are also tools that can contribute to deal with noisy text commonly found on web pages, such as language models, stop word lists and spell checkers.</p><p>Motivated by the need for exploiting this useful (albeit noisy) data, the Im-ageCLEF 2016 Scalable Concept Image Annotation task aims to develop techniques to allow computers to describe images reliably, localise the different concepts depicted in the images, generate a description of the scene and select images to illustrate texts. The primary objective of the 2016 edition is to encourage creative ideas of using noisy, web page data so that it can be used to improve various image annotation tasks -concept annotation and localization, selecting important concepts to be described, generating natural language descriptions, and retrieving images to illustrate a text document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Challenge Description</head><p>This year the challenge<ref type="foot" coords="4,235.86,139.28,3.97,6.12" target="#foot_1">2</ref> consisted of 3 subtasks and a teaser task. <ref type="foot" coords="4,423.67,139.28,3.97,6.12" target="#foot_2">3</ref>1. Subtask 1 (Image Annotation and Localization): The image annotation task remains the same as the 2015 edition. Participants are required to develop a system that receives as input an image and produces as output a prediction of which concepts are present in that image, selected from a predefined list of concepts. Like the 2015 edition, they should also output bounding boxes indicating where the concepts are located within the image. 2. Subtask 2 (Natural Language Caption Generation): This subtask was geared towards participants interested in developing systems that generate textual descriptions directly with an image as input. For example, by using visual detectors to identify concepts and generating textual descriptions from the detected concepts, or by learning neural sequence models in a joint fashion to create descriptions conditioned directly on the image. Participants used their own image analysis methods, for example by using the output of their image annotation systems developed for Subtask 1. They are also encouraged to augment their training data with the noisy content of the web page. 3. Subtask 3 (Content Selection): This subtask was primarily designed for those interested in the Natural Language Generation aspects of Subtask 2 while avoiding visual processing of images. It concentrated on the content selection phase when generating image descriptions, i.e. which concepts (from all possible concepts depicted) should be selected, and mentioned in the corresponding description? Gold standard input, bounding boxes labelled with concepts for each test image was provided, and participants were expected to develop systems that predict the bounding box instances most likely to be mentioned in the corresponding image descriptions. Unlike the 2015 edition, participants were not required to generate complete sentences but were only requested to provide a list of bounding box instances per image. 4. Teaser task (Text Illustration): This pilot task is designed to evaluate the performance of methods for text-to-image matching. Participants were asked to develop a system to analyse a given text document and find the best illustration for it from a set of all available images. At test time, participants were provided as input a selection of text documents as queries, and the goal was to select the best illustration for each text from a collection of 200,000 images.</p><p>As a common dataset, participants were provided with 510,123 web images, the corresponding web pages on which they appeared, as well as precomputed visual and textual features (see Sect. 2.4). As in the 2015 task, external training data such as ImageNet ILSVRC2015 and MSCOCO is also allowed, and participants were also encouraged to use other resources such as ontologies, word disambiguators, language models, language detectors, spell checkers, and automatic translation systems.</p><p>We observed in the 2015 edition that this large-scale noisy web data was not used as much as we anticipated -participants mainly used external training data. To encourage participants to utilise the provided data for training, in this edition participants were expected to produce two sets of related results:</p><p>1. using only external training data; 2. using both external data and the noisy web data of 510,123 web pages.</p><p>The aim is for participants to improve the performance of externally trained systems, using the provided noisy web data. However none of the participants submitted results, only on the supplied noisy training; this is probably due to the fact the groups are chasing the optimal image annotation results, and not actively attempting to research into using the noisy training data.</p><p>Development datasets: In addition to the training dataset and visual/textual features mentioned above, the participants were provided with the following for the development of their systems:</p><p>-A development set of images (a small subset of the training data) with ground truth labelled bounding box annotations and precomputed visual features for estimating the system performance for Subtask 1. -A development set of images with at least five textual descriptions per image for Subtask 2. -A subset of the development set above for Subtask 3, with gold standard inputs (bounding boxes labelled with concepts) and correspondence annotation between bounding box inputs and terms in textual descriptions. -A development set for the Teaser task, with approximately 3,000 image-web page pairs. This set is disjoint from the 510,123 noisy dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Concepts</head><p>For the three subtasks, the 251 concepts were retained from the 2015 edition. They were chosen to be visual objects that are localizable and that are useful for generating textual descriptions of the visual content of images. They include animate objects such as people, dogs and cats, inanimate objects such as houses, cars and balls, and scenes such as city, sea and mountains. With the concepts mined from the texts of our database of 31 million image-webpage pairs <ref type="bibr" coords="5,462.32,597.34,14.61,8.74" target="#b22">[23]</ref>.</p><p>Nouns that are subjects or objects of sentences are extracted and mapped onto WordNet synsets <ref type="bibr" coords="5,211.21,621.25,9.96,8.74" target="#b6">[7]</ref>. In addition, filtered to 'natural', basic-level categories (dog rather than a Yorkshire terrier ), based on the WordNet hierarchy and heuristics from a large-scale text corpora <ref type="bibr" coords="5,278.97,645.16,14.61,8.74" target="#b27">[28]</ref>. The organisers manually shortlisted the final list of concepts such that they were (i) visually concrete and localizable;</p><p>(ii) suitable for use in image descriptions; (iii) at an appropriate 'every day' level of specificity that was neither too general nor too specific. The complete list of concepts, as well as the number of samples in the test sets, is included in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Dataset</head><p>The dataset this year<ref type="foot" coords="6,228.95,205.56,3.97,6.12" target="#foot_3">4</ref> was built on the 500,000 image-webpage pairs from the 2015 edition. The 2015 dataset used was very similar to previous three editions of the task <ref type="bibr" coords="6,182.83,231.04,15.90,8.74" target="#b23">[24,</ref><ref type="bibr" coords="6,198.73,231.04,11.93,8.74" target="#b25">26,</ref><ref type="bibr" coords="6,210.66,231.04,11.93,8.74" target="#b24">25]</ref>. To create the dataset, a database of over 31 million images was created by querying Google, Bing and Yahoo! using words from the Aspell English dictionary <ref type="bibr" coords="6,219.33,254.95,14.61,8.74" target="#b22">[23]</ref>. The images and corresponding web pages were downloaded, taking care to avoid data duplication. Then, a subset of 500,000 images was selected from this database by choosing the top images from a ranked list.</p><p>For further details on the dataset creation, please refer to <ref type="bibr" coords="6,385.08,290.82,14.61,8.74" target="#b23">[24]</ref>. By retrieving images from our database using the list of concepts, the ranked list was generated, in essence, more or less as if the search engines was queried. From the ranked list, some types of problematic images were removed, and each image had at least one web page in which they appeared.</p><p>To incorporate the teaser task this year, the 500,000 image dataset from 2015 was augmented with 10,123 new image-webpage pairs, taken from a subset of the BreakingNews dataset <ref type="bibr" coords="6,278.46,374.65,15.50,8.74" target="#b15">[16]</ref> which we developed, expanding the size of the dataset to 510,123 image-webpage pairs. The aim of generating the Break-ingNews dataset was to further research into image and text annotation, where the textual descriptions are loosely related to their corresponding images. Unlike the main subtasks, the textual descriptions in this dataset do not describe real image content but provide connotative and ambiguous relations that may not be directly inferred from images. More specifically, the documents and images were obtained from various online news sources such as BBC News and The Guardian. For the teaser task, a random subset of image-article pairs was selected from the original dataset, and we ensured that each image corresponds to only one text article. The reports were converted to a 'web page' via a generic template.</p><p>Like last year, for the main subtasks the development and test sets were both taken from the "training set". Both sets were retained from last year, making the evaluation for the three subtasks comparable across both 2015 and 2016 editions. To generate these sets, a set of 5,520 images was selected using a CNN trained to identify images suitable for sentence generation. Crowd-sourcing, annotated the images in three stages: (i) image level annotation for the 251 concepts; (ii) bounding box annotation; (iii) textual description annotation. A subset of these samples was then selected for subtask 3 and further annotated by the organisers with correspondence annotations between bounding box instances and terms in textual descriptions.</p><p>The development set for the main subtask contained 2,000 samples, out of which 500 samples were further annotated and used as the development set for subtask 3. Only 1,979 samples from the development set include at least one bounding box annotation. The number of textual descriptions for the development set ranged from 5 to 51 per image (with a mean of 9.5 and a median of 8 descriptions). The test set for subtasks 1 and 2 contains 3,070 samples, while the test set for subtask 3 comprises 450 samples which are disjoint from the test set of subtasks 1 and 2.</p><p>For the teaser task, 3,337 random image-article pairs were selected from the BreakingNews dataset as the development set; these are disjoint from the 10,123 selected in the main dataset. Again, each image corresponds to only one article.</p><p>Like last year, the training and the test images were all contained within the 510,123 images. In the case of the teaser task, we divided the dataset into 310,123 for training and 200,000 for testing, where all 10,123 documents from the BreakingNews dataset were contained within the 200,000 test set. Participants of the teaser task were thus not allowed to explore the data for these 200,000 test documents.</p><p>The training and development sets for all tasks were released approximately three months before the submission deadline. For subtasks 1 and 2, participants were expected to provide classification/generate a description for all 510,123 images. The test data for subtask 3 was released one week before the submission deadline. While the train/test split for the teaser tasks was provided right from the beginning, the test input was only released 1.5 months before the deadline. The test data were 180,000 text documents extracted from a subset of the web pages in the 200,000 test split. Text extraction was performed using the get text() method of the Beautiful Soup library<ref type="foot" coords="7,339.98,398.43,3.97,6.12" target="#foot_4">5</ref> , after removal of unwanted elements (and their content) such as script or style. A maximum of 10 submissions per subtask (also referred to as runs) was allowed per participating group.</p><p>Textual Data: Four sets of data were made available to the participants. The first one was the list of words used to find the image when querying the search engines, along with the rank position of the image in the respective query and search engine used. The second set of textual data contained the image URLs as referenced in the web pages they appeared in. In many cases, the image URLs tend to be formed with words that relate to the content of the image, which is why they can also be useful as textual features. The third set of data was the web pages in which the images appeared, for which the only preprocessing was a conversion to valid XML just to make any subsequent processing simpler. The final set of data were features obtained from the text extracted near the position(s) of the image in each web page it appeared in.</p><p>To extract the text near the image, after conversion to valid XML, the script and style elements were removed. The extracted texts were the web page title, and all the terms closer than 600 in word distance to the image, not including the HTML tags and attributes. Then a weight s(t n ) was assigned to each of the words near the image, defined as</p><formula xml:id="formula_0" coords="8,210.23,138.77,270.36,26.88">s(t n ) = 1 ∀t∈T s(t) ∀tn,m∈T F n,m sigm(d n,m ) ,<label>(1)</label></formula><p>where t n,m are each of the appearances of the term t n in the document T , F n,m is a factor depending on the DOM (e.g. title, alt, etc.) similar to what is done in the work of La Cascia et al. <ref type="bibr" coords="8,277.07,201.70,14.61,8.74" target="#b9">[10]</ref>, and d n,m is the word distance from t n,m to the image. The sigmoid function was centered at 35, had a slope of 0.15 and minimum and maximum values of 1 and 10 respectively. The resulting features include for each image at most the 100 word-score pairs with the highest scores.</p><p>Visual Features: Before visual feature extraction, images were filtered and resized so that the width and height had at most 240 pixels while preserving the original aspect ratio. These raw resized images were provided to the participants but also eight types of precomputed visual features. The first feature set Colorhist consisted of 576-dimensional colour histograms extracted using our implementation. These features correspond to dividing the image in 3×3 regions and for each region obtaining a colour histogram quantified to 6 bits. The second feature set GETLF contained 256-dimensional histogram based features. First, local color-histograms were extracted in a dense grid every 21 pixels for windows of size 41 × 41. Then, these local color-histograms were randomly projected to a binary space using eight random vectors and considering the sign of the resulting projection to produce the bit. Thus, obtaining an 8-bit representation of each local color-histogram that can be regarded as a word. Finally, the image is represented as a bag-of-words, leading to a 256-dimensional histogram representation. The third set of features consisted of GIST <ref type="bibr" coords="8,373.72,434.39,15.50,8.74" target="#b12">[13]</ref> descriptors. The following four feature types were obtained using the colorDescriptors software <ref type="bibr" coords="8,462.33,446.35,14.61,8.74" target="#b18">[19]</ref>, namely SIFT, C-SIFT, RGB-SIFT and OPPONENT-SIFT. The configuration was dense sampling with default parameters and a hard assignment 1,000 dimension codebook using a spatial pyramid of 1 × 1 and 2 × 2 <ref type="bibr" coords="8,384.82,482.21,14.61,8.74" target="#b10">[11]</ref>. Concatenation of the vectors of the spatial pyramid resulted in 5,000-dimensional feature vectors. The codebooks were generated using 1.25 million randomly selected features and the k-means algorithm. Moreover, finally, CNN feature vectors have been provided computed as the seventh layer feature representations extracted from a deep CNN model pre-trained with the ImageNet dataset <ref type="bibr" coords="8,382.25,541.99,15.50,8.74" target="#b16">[17]</ref> using the Berkeley Caffe library<ref type="foot" coords="8,189.62,552.37,3.97,6.12" target="#foot_5">6</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Performance Measures</head><p>Subtask 1 Ultimately the goal of an image annotation system is to make decisions about which concepts to assign and localise to a given image from a predefined list of concepts. Consideration on how to measure annotation performance should be how good and accurate are those decisions. Ideally, a recall measure would also be used to penalise a system that has additional false positive output. However given difficulties and unreliability of the hand labelling of the concepts for the test images it was not possible to guarantee all concepts were labelled. However, the labels present are assumed to be accurate and of a high quality.</p><p>The annotation and localization of Subtask 1 were evaluated using the PAS-CAL VOC <ref type="bibr" coords="9,184.44,191.72,10.52,8.74" target="#b5">[6]</ref> style metric of intersection over union (IoU), IoU is defined as</p><formula xml:id="formula_1" coords="9,258.51,212.14,222.08,23.22">IoU = |BB f g ∩ BB gt | |BB f g ∪ BB gt | (2)</formula><p>Where BB is a rectangle bounding box, f g is a foreground proposed annotation label, gt is the ground truth label of the concept. It calculates the area of intersection between the foreground in the proposed output localization and the ground-truth bounding box localization, divided by the area of their union.</p><p>IoU is superior to a more simple measure of the percentage of correctly labelled pixels as IoU is normalised by the size of the object automatically and penalises segmentation's that include the background. Causing small changes in the percentage of correctly labelled pixels to correspond to large differences in IoU, and as the dataset has a wide variation in object size, the performance increases from our approach are more reliably measured. The evaluation of the ground truth and proposed output overlap was recorded from 0% to 90%. At 0%, this is equivalent to an image level annotation output, and 50% is the standard PASCAL VOC style metric used. The localised IoU is then used to compute the mean average precision (MAP) of each concept independently. The MAP is reported both per concept and averaged over all concepts. In comparison to previous years, the MAP was averaged over all possible concept labels in the test data, instead of just the concepts the participant used. This was to penalise correctly approaches that only contained a subset of a full approach such as a face detector, as these were producing unrepresentative performances overall MAP, however, registering on only a few concepts.</p><p>Subtask 2 Subtask 2 was evaluated using the Meteor evaluation metric <ref type="bibr" coords="9,467.31,497.09,9.96,8.74" target="#b3">[4]</ref>, which is an F -measure of word overlaps taking into account stemmed words, synonyms, and paraphrases, with a fragmentation penalty to penalise gaps and word order differences. This measure was chosen as it was shown to correlate well with human judgments in evaluating image descriptions <ref type="bibr" coords="9,401.62,544.91,9.96,8.74" target="#b4">[5]</ref>. Please refer to Denkowski and Lavie <ref type="bibr" coords="9,230.58,556.87,10.52,8.74" target="#b3">[4]</ref> for details about this measure.</p><p>Subtask 3 Subtask 3 was evaluated with the fine-grained metric for content selection which we introduced in last year's edition. Please see <ref type="bibr" coords="9,413.87,597.34,10.52,8.74" target="#b7">[8]</ref> or <ref type="bibr" coords="9,440.73,597.34,15.50,8.74" target="#b26">[27]</ref> for a detailed description. The content selection metric is the F 1 score averaged across all 450 test images, where each F 1 score is computed from the precision and recall averaged over all gold standard descriptions for the image. Intuitively, this measure evaluates how well the sentence generation system selects the correct concepts to be described against gold standard image descriptions. Formally, let</p><formula xml:id="formula_2" coords="10,134.77,117.60,283.56,13.20">I = {I 1 , I 2 , ...I N } be the set of test images. Let G Ii = {G Ii 1 , G Ii 2 , .</formula><p>.., G Ii M } be the set of gold standard descriptions for image I i , where each G Ii m represents the set of unique bounding box instances referenced in gold standard description m of image I i . Let S Ii be the set of unique bounding box instances referenced by the participant's generated sentence for image I i . The precision P Ii for test image I i is computed as:</p><formula xml:id="formula_3" coords="10,254.00,192.00,226.60,30.03">P Ii = 1 M M m |G Ii m ∩ S Ii | |S Ii |<label>(3)</label></formula><p>where |G Ii m ∩ S Ii | is the number of unique bounding box instances referenced in both the gold standard description and the generated sentence, and M is the number of gold standard descriptions for image I i .</p><p>Similarly, the recall R Ii for test image I i is computed as:</p><formula xml:id="formula_4" coords="10,254.06,289.78,226.53,30.03">R Ii = 1 M M m |G Ii m ∩ S Ii | |G Ii m |<label>(4)</label></formula><p>The content selection score for image I i , F Ii , is computed as the harmonic mean of P Ii and R Ii :</p><formula xml:id="formula_5" coords="10,262.79,354.24,217.80,23.89">F Ii = 2 × P Ii × R Ii P Ii + R Ii<label>(5)</label></formula><p>The final P , R and F scores are computed as the mean P , R and F scores across all test images. The advantage of the macro-averaging process in equations ( <ref type="formula" coords="10,408.53,410.02,4.24,8.74" target="#formula_3">3</ref>) and ( <ref type="formula" coords="10,442.23,410.02,4.24,8.74" target="#formula_4">4</ref>) is that it implicitly captures the relative importance of the bounding box instances based on how frequently to which they are referred across the gold standard descriptions.</p><p>Teaser task For the teaser task, participants are requested to rank the 200,000 test images according to their distance to each input text document. Recall at the k-th rank position (R@k) of the ground truth image were used as the performance metrics. The testing of several values of k was performed, and participants were asked to submit the top 100 ranked images. Please refer to Hodosh et al. <ref type="bibr" coords="10,196.26,538.04,10.52,8.74" target="#b8">[9]</ref> for more details about the metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Participation</head><p>This year the participation was not so good as 2015 where it increased considerably in previous years. In total seven groups took part in the task and submitted overall 50 system runs. All seven participating groups submitted a working paper describing their system, thus for these there were specific details available:</p><p>-CEA LIST: <ref type="bibr" coords="11,202.96,119.99,10.52,8.74" target="#b1">[2]</ref> The team from CEA, LIST, Laboratory of Vision and Content Engineering, France, represented by Herve Le Borgne, Etienne Gadeski, Ines Chami, Thi Quynh Nhi Tran, Youssef Tamaazousti, Alexandru Lucian Gînscȃ and Adrian Popescu. -CNRS TPT: <ref type="bibr" coords="11,210.11,167.77,15.50,8.74" target="#b17">[18]</ref> The team from CNRS TELECOM ParisTech, France, represented by Hichem Sahbi. -DUTh: <ref type="bibr" coords="11,183.10,191.63,10.52,8.74" target="#b0">[1]</ref> The team from Democritus University of Thrace, DUTh, Greece, was represented by Georgios Barlas, Maria Ntonti and Avi Arampatzis. -ICTisia: <ref type="bibr" coords="11,188.19,215.50,15.50,8.74" target="#b28">[29]</ref> The team from Key Laboratory of Tables 7, 8, 9 and 10 provide the main key details for some the top groups submission describing their system for each subtask. These tables serve as a summary of the systems, and are also quite illustrative for quick comparisons. For a more in-depth look at the systems of each team, please refer to their corresponding paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results for Subtask 1: Image Annotation and Localization</head><p>Unfortunately subtask 1 had a lower participation than last year, however there were some excellent results showing improvements over previous years. All submissions were able to provide results on all 510,123 images, indicating that all groups have developed systems that are scalable enough to annotate large amounts of images. However one group only processed 180K image (MRIM-LIG <ref type="bibr" coords="11,158.70,537.56,15.50,8.74" target="#b14">[15]</ref>) due to computational constraints. Final results are presented in Table 1 in terms of mean average precision (MAP) over all images of all concepts, with both 0% overlap (i.e. no localization) and 50% overlap.</p><p>Three of the four groups have achieved good performance across the dataset, in particular, the approach of CEA LIST. An excellent result given the challenging nature of the images used and the wide range of concepts provided. The graph in Figure <ref type="figure" coords="11,207.77,609.29,4.98,8.74" target="#fig_3">3</ref> shows the performance of each submission for an increasing amount of overlap of the ground truth labels. All the approaches show a steady drop off in performance which is encouraging, illustrating that the approaches do not fail to detect some concepts correctly even with a high degree of accuracy. Even 90% overlap with the ground truth the MAP for CEA LIST was 0.20,  which is impressive. The results from the groups seem encouraging, and the approaches use a now standard CNN as their foundation. Improved neural network structures such as the proposed approach from VGG <ref type="bibr" coords="12,370.85,464.26,14.61,8.74" target="#b19">[20]</ref>, have provided much of this improvement.</p><p>CEA LIST used a recent deep learning framework <ref type="bibr" coords="12,373.66,489.74,14.61,8.74" target="#b19">[20]</ref>, however, focused on improving the localisation of the concepts. They attempted to use a face body part detector, boosted by last year's results. However, the use of a face detector was oversold in the previous years results and didn't improve the performance. They used EdgeBoxes a generic objectness object detector, however the performance also didn't increase as expected in the test runs. They hypothesise that this could be due to the generation of many more candidate bounding boxes, and a significant number estimate the concept incorrectly. MRIM-LIG also used a classical deep learning framework and the object localisation of <ref type="bibr" coords="12,432.49,585.38,14.61,8.74" target="#b20">[21]</ref>, where an apriori set of bounding boxes are defined which are expected to contain a single concept each. They also investigated the false lead on performance improvement through face detection, with a similar lack of performance increase. Finally CNRS focused on concept detection and used label enrichment to increase the training data quantity in conjunction with an SVM and VGG <ref type="bibr" coords="12,465.09,645.16,15.50,8.74" target="#b19">[20]</ref> deep network. As each group could submit ten different approaches, in general, the best-submitted approaches contained a fusion of all the various components of their proposed approaches. Some of the test images have nearly 100 ground truth labelled concepts, and due to limited resources, some of the submitted groups might not have labelled all possible concepts in each image. However, Fig. <ref type="figure" coords="13,351.05,167.81,4.98,8.74" target="#fig_4">4</ref> shows a similar performance between groups as previously in Fig. <ref type="figure" coords="13,304.32,179.77,3.87,8.74" target="#fig_3">3</ref>. An improvement over previous years where groups struggled to annotate the 500K images fully. Much of the difference between the groups, is their ability to localise the concepts effectively. The ten concepts with the highest average MAP across the groups, with 0% overlap with the bounding box are in general human-centric: face, hair, arm, woman, tree, man, car, ship, dress and airplane. These are nonrigid classes that are being detected on the image, however not yet successfully localised in the picture as well. With the constraint of 50% overlap with the ground truth bounding box, the list becomes more based around defined objects: car, aeroplane, hair, park, floor, boot, sea, street, face and tree. These are objects that have a rigid shape that can be learnt. Table <ref type="table" coords="13,385.01,522.86,4.98,8.74" target="#tab_2">2</ref> shows numerical examples of the most successfully localised concepts, together with the percentage of concept occurrence per image in the test data. No method managed to localise 38 concepts, these include the concepts: nut, mushroom, banana, ribbon, planet, milk, orange fruit and strawberry. These are smaller and less represented concepts, in both the test and validation data, in generally occurring in less that 2% of the test images. In fact, many of these concepts were poorly localised in the previous years challenge too, making this an area to direct the challenge objectives in future years.</p><p>Discussion for subtask 1 From a computer vision perspective, we would argue that the ImageCLEF challenge has two key differences in its dataset construc- tion to that of the other popular data sets ImageNet <ref type="bibr" coords="14,374.34,324.91,15.50,8.74" target="#b16">[17]</ref> and MSCOCO <ref type="bibr" coords="14,462.32,324.91,14.61,8.74" target="#b11">[12]</ref>. All three are working on detection and classification of concepts within images. However, the ImageCLEF dataset is created from Internet web pages, providing a fundamental difference to the other popular datasets. The web pages are unsorted and unconstrained meaning the relationship or quality of the text and image about a concept can be very variable. Therefore, instead of a high-quality Flickr style photo of a car from ImageNet, the image in the ImageCLEF dataset could be a fuzzy abstract car shape in the corner of the image. Allowing the ImageCLEF image annotation challenge to provide additional opportunities to test proposed approaches on. Another important difference is that in addition to the image, text data from web pages can be used to train and generate the output description of the image in a natural language form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results for Subtask 2: Natural Language Caption Generation</head><p>For subtask 2, participants were asked to generate sentence-level textual descriptions for all 510,123 training images. Two teams, ICTisia and UAIC, participated in this subtask. Table <ref type="table" coords="14,288.11,525.61,4.98,8.74">3</ref> shows the Meteor scores, for all submitted runs by both participants. The Meteor score for the human upper-bound was estimated to be 0.3385 via leave-one-out cross validation, i.e. by evaluating one description against the other descriptions for the same image and repeating the process for all descriptions.</p><p>ICTisia achieved the better Meteor score of 0.1837, by building on the stateof-the-art joint CNN-LSTM image captioning system, but fine-tuning the parameters of the image CNN as well as the LSTM. On the other hand, UAIC, who also participated last year, improved on their Meteor score with 0.0934 compared to their best performance from last year (0.0813). They generated image descriptions using a template-based approach and leveraged external ontologies and CNNs to improve their results compared to their submissions from last year.</p><p>Table <ref type="table" coords="15,165.08,128.32,4.13,7.89">3</ref>: Results for subtask 2, showing the Meteor scores for all runs from both participants. We consider the mean Meteor score as the primary measure, but for completeness, we also present the median, min and max scores. Neither teams have managed to bridge the gap between system performance and the human upper-bound this year, showing that there is still scope for further improvement on the task of generating image descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Team</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results for Subtask 3: Content Selection</head><p>For subtask 3 on content selection, participants were provided with gold standard labelled bounding box inputs for 450 test images, released one week before the submission deadline. Participants were expected to develop systems capable of predicting, for each image, the bounding box instances (among the gold standard input) that will be mentioned in the gold standard human-authored textual descriptions.</p><p>Two teams, DUTh and UAIC, participated in this task. Table <ref type="table" coords="15,430.37,464.81,4.98,8.74" target="#tab_4">4</ref> shows the F -score, Precision and Recall across 450 test images for each participant, both of whom submitted only a single run. The generation of a random per image baseline by selecting at most three bounding boxes from the gold standard input at random was perfomed. Like subtask 2, a human upper-bound was computed via leave-one-out cross validation. The results for these are also shown in Table <ref type="table" coords="15,472.85,524.58,3.87,8.74" target="#tab_4">4</ref>. As observed, both participants performed significantly better than the random baseline. Compared against the human upper-bound, like subtask 2, much work can still be done to improve further the performance on the task.</p><p>Unlike the previous two subtasks, neither team used neural networks directly for content selection. DUTh achieved a higher F -score compared to the best performing team from last year (0.5459 vs. 0.5310), by training SVM classifiers to predict whether a bounding box instance is important or not, using various image descriptors. UAIC used the same system as subtask 2, and while they did not significantly improve on their F -score from last year, their recall score showed a slight increase. An interesting note is that both teams this year seem to have concentrated on recall R at the expense of a lower precision P , in contrast to last year's best performing team who used an LSTM to achieve high precision but with a much lower recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Results for Teaser task: Text Illustration</head><p>Two teams, CEA LIST and INAOE, participated in the teaser task on text illustration. Participants were provided with 180,000 text documents as input, and for each document were asked to provide the top 100 ranked images that correspond to the document (from a collection of 200,000 images). Table <ref type="table" coords="16,447.65,363.90,4.98,8.74">5</ref> shows the recall at different ranks k (R@k), for a selected subset of 10,112 input documents comprised of news articles from the BreakingNews dataset (see Sect. 2.4). Table <ref type="table" coords="16,161.81,399.76,4.98,8.74" target="#tab_5">6</ref> shows the same results, but on the full 180,000 test documents. Because the full set of test documents were extracted from generic web pages, the domain of the text varies. As such, they may consist of noisy documents such as text from navigational links or advertisements.</p><p>Table <ref type="table" coords="16,163.48,479.14,4.13,7.89">5</ref>: Results for Teaser 1: Text Illustration. Recall@k for a selected subset of test set</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Team</head><p>Run Recall (%) R@1 R@5 R@10 R@25 R@50 R@75 R@100 Random Chance -0.00 0.00 Team Run Recall (%) R@1 R@5 R@10 R@25 R@50 R@75 R@100 Random Chance -0.00 0.00 This task yielded some interesting results. Bearing in mind the difficulty of the task (selecting one correct image from 200,000 images), CEA LIST yielded a respectable score that is clearly better than chance performance. The recall also increased as the rank k is increased. CEA LIST's approach involves mapping visual and textual modalities onto a common space and combining this method with a semantic signature. INAOE on the other hand produced excellent results with run 1, which is a retrieval approach based on a bag-of-words representation weighted with tf-idf, achieving a recall of 37% even at rank 1 and almost 80% at rank 100 (in Table <ref type="table" coords="17,218.24,435.84,3.87,8.74">5</ref>). In contrast, their runs based on a neural network trained word2vec representation achieved a much lower recall, although it did increase to 29.59% at rank 100. Comparing Tables <ref type="table" coords="17,317.72,459.75,4.98,8.74">5</ref> and<ref type="table" coords="17,344.66,459.75,3.87,8.74" target="#tab_5">6</ref>, both teams performed better on the larger test set of 180,000 generic (and noisy) web text than the smaller test set of 10,112 restricted to news articles. Although interestingly INAOE's bag-of-words approach performed worse at smaller ranks (1-10) for the full test set compared to the news article test set, although still significantly better than their word2vec representation. This increase in overall scores, despite the significant increase in the size of the test set, suggests that there may be some slight overfitting to the training data with most of the methods.</p><p>It should be noted that the results of both teams are not directly comparable, as INAOE based their submission on the assumption that the webpages for test images are available at test time while CEA LIST did not. This assumption made the text illustration problem significantly less challenging since the test documents were extracted directly from these webpages, hence the superior performance by INAOE. On hindsight, this should have been specified more clearly in our task description for a level playing field. As such we do not consider one method being superior over the other, but instead concentrate on the technical contributions of each team.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Limitations of the challenge</head><p>There are two major limitations that we have identified with the challenge this year. Very few of the groups used the provided data set and features, we found this surprising, considering the state of the art CNN features and many others were included. However, this is likely to be due to the complexity and challenge of the 510,123 web page based images. Given they were from the Internet with little, a large number of the images are poor representations of the concept. In fact, some participants annotated a significant amount of their more comprehensive training data, as their learning process assumes perfect or near perfect training examples, it will fail. As the number of classes increases and become more varied annotating all comprehensive data will be made more difficult.</p><p>Another shortcoming of the overall challenge is the difficulty of ensuring the ground truth has 100% of concepts labelled, thus allowing a recall measure to be used. Especially problematic as the concepts selected include fine-grained categories such as eyes and hands that are small but frequently occur in the dataset. Also, it was difficult for annotators to reach a consensus in annotating bounding boxes for less well-defined categories such as trees and field. Given the current crowd-source based hand-labelling of the ground truth, the concepts have missed annotations. Thus, in this edition, a recall measure is not evaluated for subtask 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>This paper presented an overview of the ImageCLEF 2016 Scalable Concept Image Annotation task, the fifth edition of a challenge aimed at developing more scalable image annotation systems. The focus of the three subtasks and teaser task available to participants had the goal to develop techniques to allow computers to annotate the images reliably, localise the different concepts depicted in the images, select important concepts to be described, generate a description of the scene, and retrieve a relevant image to illustrate a text document.</p><p>The participation was lower than the previous year, however, in general, the performance of the submitted systems was somewhat superior to last year's results for subtask 1. In part probably due to the increased CNN usage as the feature representation had improved localisation techniques. The clear winner of this year's subtask 1 evaluation was the CEA LIST <ref type="bibr" coords="18,365.78,549.52,10.52,8.74" target="#b1">[2]</ref> team, which focused on using a state of the art CNN architecture and then also investigated improved localisation of the concepts which helped provide a good performance increase. In contrast to subtask 1, the participants for subtask 2 did not significantly improve the results from last year. The approaches used were very similar to those of last year. For subtask 3, both participating teams concentrated on achieving high recall with traditional approaches like SVM's, compared to last year's winning team which focused on obtaining high precision with a neural network approach. For the pilot teaser task of text illustration, both participating teams performed respectably, with different techniques proposed with varied results. Because of the ambiguity surrounding one aspect of the task description, the results of the teams are not directly comparable.</p><p>The results of the task have been interesting and show that useful annotation systems can be built using noisy web-crawled data. Since the problem requires to cover many fronts, there is still much work, so it would be interesting to continue this line of research. Papers on this topic should be published, demonstration systems based on these ideas be built and more evaluation of this sort be organised. Also, it remains to see how this can be used to complement systems that are based on clean hand-labelled data and find ways to take advantage of both the supervised and unsupervised data. For each concept it has been trained "one-versus-all" SVM classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Concept List 2016</head><p>The following tables present the 251 concepts used in the ImageCLEF 2016 Scalable Concept Image Annotation task. In the electronic version of this document, each concept name is a hyperlink to the corresponding WordNet synset webpage. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,134.77,229.19,345.82,7.89;2,134.77,240.17,44.57,7.86;2,134.77,116.83,345.80,100.87"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Image annotation and localization of concepts and natural language caption generation.</figDesc><graphic coords="2,134.77,116.83,345.80,100.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,215.02,191.50,185.32,7.89;3,223.72,286.57,167.92,7.89"><head>( a )</head><label>a</label><figDesc>Images from a search query of "rainbow".(b) Images from a search query of "sun".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,155.74,307.49,300.81,7.89;3,146.50,211.90,69.17,69.17"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Example of images retrieved by a commercial image search engine.</figDesc><graphic coords="3,146.50,211.90,69.17,69.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="12,134.77,390.98,345.83,7.89;12,134.77,401.97,41.31,7.86;12,134.77,229.96,325.08,149.54"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Increasing percentage of ground truth bounding box overlap of submissions for sub task 1</figDesc><graphic coords="12,134.77,229.96,325.08,149.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="13,134.77,382.55,345.83,7.89;13,134.77,393.53,40.47,7.86;13,134.77,221.53,325.08,149.54"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: MAP performance with a minimum number of ground truth bounding boxes per Image</figDesc><graphic coords="13,134.77,221.53,325.08,149.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="11,134.02,215.50,346.58,152.06"><head></head><label></label><figDesc>Intelligent Information Processing, Institute of Computing Technology Chinese Academy of Sciences, China, represented by Yongqing Zhu, Xiangyang Li, Xue Li, Jian Sun, Xinhang Song and Shuqiang Jiang. -INAOE: [14] The team from Instituto Nacional de Astrofısica, Optica y Electronica (INAOE), Mexico was represented by Luis Pellegrin, A. Pastor López-Monroy, Hugo Jair Escalante and Manuel Montes-Y-Gómez. -MRIM-LIG: [15] The team from LIG -Laboratoire d'Informatique de Grenoble, and CNRS Grenoble, France, was represented by Maxime Portaz, Mateusz Budnik, Philippe Mulhem and Johann Poignant. -UAIC: [3] The team from UAIC: Faculty of Computer Science, "Alexandru Ioan Cuza" University, Romania, represented by Alexandru Cristea and Adrian Iftene.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="12,220.99,128.32,173.37,74.64"><head>Table 1 :</head><label>1</label><figDesc>Subtask 1 results.</figDesc><table coords="12,220.99,149.64,173.37,53.32"><row><cell>Group</cell><cell cols="2">0% Overlap 50% Overlap</cell></row><row><cell>CEA LIST</cell><cell>0.54</cell><cell>0.378</cell></row><row><cell>MRIM-LIG</cell><cell>0.21</cell><cell>0.14</cell></row><row><cell>CNRS</cell><cell>0.25</cell><cell>0.11</cell></row><row><cell>UAIC</cell><cell>0.003</cell><cell>0.002</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="14,166.64,128.32,282.08,161.11"><head>Table 2 :</head><label>2</label><figDesc>Successfully localised Concepts ranked by 0.5 BB Overlap</figDesc><table coords="14,166.64,149.64,282.08,139.79"><row><cell>Concept</cell><cell cols="2">Ave MAP across all Groups</cell><cell>% of Occurrence</cell></row><row><cell></cell><cell cols="3">0.5 BB Overlap 0.5 BB Overlap in test images</cell></row><row><cell>Ship</cell><cell>0.61</cell><cell>0.57</cell><cell>28.0%</cell></row><row><cell>Car</cell><cell>0.62</cell><cell>0.55</cell><cell>25.3%</cell></row><row><cell>Airplane</cell><cell>0.60</cell><cell>0.55</cell><cell>3.2%</cell></row><row><cell>Hair</cell><cell>0.74</cell><cell>0.52</cell><cell>93.0%</cell></row><row><cell>Park</cell><cell>0.41</cell><cell>0.52</cell><cell>13.9%</cell></row><row><cell>Floor</cell><cell>0.41</cell><cell>0.51</cell><cell>13.4%</cell></row><row><cell>Boot</cell><cell>0.43</cell><cell>0.59</cell><cell>4.2%</cell></row><row><cell>Sea</cell><cell>0.45</cell><cell>0.49</cell><cell>8.8%</cell></row><row><cell>Street</cell><cell>0.54</cell><cell>0.47</cell><cell>18.0%</cell></row><row><cell>Face</cell><cell>0.75</cell><cell>0.47</cell><cell>95.7%</cell></row><row><cell>Street</cell><cell>0.64</cell><cell>0.45</cell><cell>59.9%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="16,134.77,128.32,345.82,96.56"><head>Table 4 :</head><label>4</label><figDesc>Results for subtask 3, showing the content selection scores for all runs from all participants.</figDesc><table coords="16,183.74,160.59,247.87,64.28"><row><cell>Team</cell><cell>Content Selection Score Mean F Mean P Mean R</cell></row><row><cell cols="2">Human 0.7445 ± 0.1174 0.7690 ± 0.1090 0.7690 ± 0.1090</cell></row><row><cell cols="2">DUTh 0.5459 ± 0.1533 0.4451 ± 0.1695 0.7914 ± 0.1960</cell></row><row><cell cols="2">UAIC 0.4982 ± 0.1782 0.4597 ± 0.1553 0.5951 ± 0.2592</cell></row><row><cell cols="2">Baseline 0.1800 ± 0.1973 0.1983 ± 0.2003 0.1817 ± 0.2227</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="16,146.98,533.76,321.40,118.25"><head>Table 6 :</head><label>6</label><figDesc>Results for Teaser 1: Text Illustration. Recall@k for full 180K test set</figDesc><table coords="16,312.90,533.76,155.48,7.86"><row><cell>0.01</cell><cell>0.01</cell><cell>0.03</cell><cell>0.04</cell><cell>0.05</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="19,136.39,271.39,335.41,299.31"><head>Table 7 :</head><label>7</label><figDesc>Key details of the best system for top performing groups (subtask 1).</figDesc><table coords="19,136.39,282.02,335.41,288.68"><row><cell>System</cell><cell>Visual Features</cell><cell>Other Used Resources</cell><cell>Training Data Processing Highlights</cell><cell>Annotation Technique Highlights</cell></row><row><cell></cell><cell></cell><cell></cell><cell>They collected a set of</cell><cell></cell></row><row><cell>CEA LIST [2]</cell><cell>16-layer CNN 50-layer ResNet</cell><cell>* Bing Image Search</cell><cell>roughly 251,000 images (1,000 images per concept) from the Bing Images search engine. For each concept they used its name and its synonyms (if present) to query the search engine. They used 90% of the dataset for training and</cell><cell>They used EdgeBoxes, a generic objectness object detector, extracting a maximum of 100 regions per image then feeding each one to the CNN models. The concept that had the highest probability among the 251 concepts it has been kept.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>10% for validation.</cell><cell></cell></row><row><cell>MRIM-LIG [15]</cell><cell>152-layer ResNet</cell><cell>* Bing Image Search</cell><cell>Two-step learning process using two validation sets. First set of training images,learn the last layer of CNN. Retrain using 200 additional training images defined by the authors according to the low quality recognition concepts</cell><cell>An apriori set of bounding boxes which are expected to contain a single concept each is defined. Each of these boxes have been used as an input image on which the CNN has been applied to detect objects. Localization of parts of faces is achieved through the Viola and Jones approach and facial landmarks detection.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>2,000 images of the dev</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>set have been used in</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>order to enrich the labels</cell><cell></cell></row><row><cell>CNRS</cell><cell>VGG deep</cell><cell>* Google</cell><cell>of all the training set</cell><cell></cell></row><row><cell>[18]</cell><cell>network</cell><cell>Image Search</cell><cell>transferring the</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>knowledge about the</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>co-occurrence of some</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>labels.</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.73,658.44,137.01,7.47"><p>http://www.clef-initiative.eu</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,144.73,613.96,255.80,8.12"><p>Challenge website at http://imageclef.org/2016/annotation</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,144.73,624.92,335.87,7.86;4,144.73,635.88,335.86,7.86;4,144.73,646.84,335.86,7.86;4,144.73,657.79,100.55,7.86"><p>A Second teaser task was also introduced, aimed at evaluating systems that identify the GPS coordinates of a text document's topic based on its text and image data. However, we had no participants for this task, and thus will not discuss this second teaser task in this paper.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="6,144.73,657.79,286.29,8.12"><p>Dataset available at http://risenet.prhlt.upv.es/webupv-datasets</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="7,144.73,658.44,216.54,7.47"><p>https://www.crummy.com/software/BeautifulSoup/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="8,144.73,657.79,327.24,8.12"><p>More details can be found at https://github.com/BVLC/caffe/wiki/Model-Zoo</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The <rs type="projectName">Scalable Concept Image Annotation</rs> Task was co-organized by the <rs type="institution">VisualSense (ViSen)</rs> consortium under the <rs type="programName">ERA-NET CHIST-ERA D2K 2011 Programme</rs>, jointly Table 8: Key details of the best system for top performing groups (subtask 2).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_hkJ2Daz">
					<orgName type="project" subtype="full">Scalable Concept Image Annotation</orgName>
					<orgName type="program" subtype="full">ERA-NET CHIST-ERA D2K 2011 Programme</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>Visual Representation   </p><note type="other">Textual</note></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="21,142.96,205.12,337.64,7.86;21,151.52,216.07,329.07,7.86;21,151.52,224.77,257.30,10.13" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="21,311.10,205.12,169.49,7.86;21,151.52,216.07,129.70,7.86">DUTh at the ImageCLEF 2016 Image Annotation Task: Content Selection</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Barlas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ntonti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,301.08,216.07,179.51,7.86;21,151.52,227.03,110.74,7.86">CLEF2016 Working Notes. CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Évora, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-09">September 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,142.96,237.30,337.64,7.86;21,151.52,248.26,329.07,7.86;21,151.52,256.95,329.07,10.13;21,151.52,270.18,71.19,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="21,205.85,248.26,210.23,7.86">Image annotation and two paths to text illustration</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">L</forename><surname>Borgne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Gadeski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">Q N</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tamaazousti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">L</forename><surname>Gînscȃ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,437.46,248.26,43.13,7.86;21,151.52,259.22,254.66,7.86">CLEF2016 Working Notes. CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Évora, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-09">September 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,142.96,280.45,337.64,7.86;21,151.52,291.41,329.07,7.86;21,151.52,300.10,329.07,10.13;21,151.52,313.32,71.19,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="21,252.89,280.45,227.71,7.86;21,151.52,291.41,263.92,7.86">Using Machine Learning Techniques, Textual and Visual Processing in Scalable Concept Image Annotation Challenge</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cristea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Iftene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,437.46,291.41,43.13,7.86;21,151.52,302.36,254.66,7.86">CLEF2016 Working Notes. CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Évora, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-09">September 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,142.96,323.59,337.64,7.86;21,151.52,334.55,329.07,7.86;21,151.52,345.51,154.99,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="21,261.01,323.59,219.59,7.86;21,151.52,334.55,119.67,7.86">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,294.44,334.55,186.15,7.86;21,151.52,345.51,126.33,7.86">Proceedings of the EACL 2014 Workshop on Statistical Machine Translation</title>
		<meeting>the EACL 2014 Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,142.96,355.78,337.63,7.86;21,151.52,366.74,329.07,7.86;21,151.52,377.70,329.07,7.86;21,151.52,388.66,228.61,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="21,238.65,355.78,241.94,7.86;21,151.52,366.74,14.75,7.86">Comparing automatic evaluation measures for image description</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,187.71,366.74,292.88,7.86;21,151.52,377.70,77.60,7.86">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06">June 2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="452" to="457" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct coords="21,142.96,398.92,337.64,7.86;21,151.52,409.88,329.07,7.86;21,151.52,420.84,221.01,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="21,189.45,409.88,232.06,7.86">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="21,428.35,409.88,52.24,7.86;21,151.52,420.84,112.86,7.86">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015-01">Jan 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,142.96,431.11,337.64,7.86;21,151.52,442.07,150.15,7.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="21,234.71,431.11,170.44,7.86">WordNet An Electronic Lexical Database</title>
		<editor>Fellbaum, C.</editor>
		<imprint>
			<date type="published" when="1998-05">May 1998</date>
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge, MA; London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,142.96,452.34,337.63,7.86;21,151.52,463.30,329.07,7.86;21,151.52,474.25,329.07,7.86;21,151.52,485.21,329.07,7.86;21,151.52,496.17,235.64,8.11" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="21,239.87,463.30,240.73,7.86;21,151.52,474.25,162.53,7.86">Overview of the imageclef 2015 scalable image annotation, localization and sentence generation task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dellandréa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-1391/inv-pap6-CR.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="21,334.33,474.25,146.26,7.86;21,151.52,485.21,162.05,7.86">Working Notes of CLEF 2015 -Conference and Labs of the Evaluation forum</title>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">September 8-11, 2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,142.96,506.44,337.64,7.86;21,151.52,517.40,329.07,7.86;21,151.52,528.36,179.13,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="21,325.25,506.44,155.34,7.86;21,151.52,517.40,190.00,7.86">Framing image description as a ranking task: Data, models and evaluation metrics</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="21,349.54,517.40,131.06,7.86;21,151.52,528.36,67.78,7.86">Journal of Artificial Intelligence Research (JAIR)</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="853" to="899" />
			<date type="published" when="2013-05">May 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,142.62,538.63,337.98,7.86;21,151.52,549.59,329.07,7.86;21,151.52,560.55,329.07,7.86;21,151.52,571.50,116.86,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="21,295.97,538.63,184.63,7.86;21,151.52,549.59,181.88,7.86">Combining textual and visual cues for contentbased image retrieval on the World Wide Web</title>
		<author>
			<persName coords=""><forename type="first">La</forename><surname>Cascia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename></persName>
		</author>
		<idno type="DOI">10.1109/IVL.1998.694480</idno>
	</analytic>
	<monogr>
		<title level="m" coord="21,354.94,549.59,125.65,7.86;21,151.52,560.55,80.94,7.86;21,265.77,560.55,121.54,7.86">Content-Based Access of Image and Video Libraries</title>
		<imprint>
			<date type="published" when="1998">1998. 1998</date>
			<biblScope unit="page" from="24" to="28" />
		</imprint>
	</monogr>
	<note>Proceedings. IEEE Workshop</note>
</biblStruct>

<biblStruct coords="21,142.62,581.77,337.97,7.86;21,151.52,592.73,329.07,7.86;21,151.52,603.69,329.07,7.86;21,151.52,614.65,329.07,7.86;21,151.52,625.61,162.29,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="21,304.47,581.77,176.12,7.86;21,151.52,592.73,209.26,7.86">Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2006.68</idno>
	</analytic>
	<monogr>
		<title level="m" coord="21,383.06,592.73,97.53,7.86;21,151.52,603.69,329.07,7.86;21,262.90,614.65,40.20,7.86">Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
	<note>CVPR &apos;06</note>
</biblStruct>

<biblStruct coords="21,142.62,635.88,337.97,7.86;21,151.52,646.84,329.07,7.86;21,151.52,657.79,172.45,8.12" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="21,208.04,646.84,180.59,7.86">Microsoft COCO: common objects in context</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>CoRR abs/1405.0312</idno>
		<ptr target="http://arxiv.org/abs/1405.0312" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,142.62,120.67,337.97,7.86;22,151.52,131.63,329.07,7.86;22,151.52,142.59,119.03,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="22,249.62,120.67,230.97,7.86;22,151.52,131.63,112.89,7.86">Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1011139631724</idno>
	</analytic>
	<monogr>
		<title level="j" coord="22,272.24,131.63,92.79,7.86">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001-05">May 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,142.62,152.71,337.98,7.86;22,151.52,163.67,329.07,7.86;22,151.52,172.37,329.07,10.13;22,151.52,185.59,22.02,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="22,444.06,152.71,36.53,7.86;22,151.52,163.67,227.86,7.86">INAOE&apos;s participation at ImageCLEF 2016: Text Illustration Task</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Pellegrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>López-Monroy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">J</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes-Y-Gómez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,400.50,163.67,80.09,7.86;22,151.52,174.63,209.58,7.86">CLEF2016 Working Notes. CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Évora, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-09">September 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,142.62,195.72,337.98,7.86;22,151.52,206.68,329.07,7.86;22,151.52,215.37,300.63,10.13" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="22,351.91,195.72,128.68,7.86;22,151.52,206.68,167.26,7.86">MRIM-LIG at ImageCLEF 2016 Scalable Concept Image Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Portaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Budnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mulhem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Poignant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,341.01,206.68,139.58,7.86;22,151.52,217.64,154.08,7.86">CLEF2016 Working Notes. CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Évora, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-09">September 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,142.62,227.76,337.98,7.86;22,151.52,238.72,329.07,8.11;22,151.52,250.33,122.39,7.47" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="22,391.80,227.76,88.80,7.86;22,151.52,238.72,169.28,7.86">Breakingnews: Article annotation by image and text processing</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ramisa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<idno>CoRR abs/1603.07141</idno>
		<ptr target="http://arxiv.org/abs/1603.07141" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,142.62,259.81,337.97,7.86;22,151.52,270.77,329.07,7.86;22,151.52,281.72,329.07,7.86;22,151.52,292.68,118.66,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="22,415.77,270.77,64.82,7.86;22,151.52,281.72,145.50,7.86">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="22,305.92,281.72,174.68,7.86;22,151.52,292.68,24.00,7.86">International Journal of Computer Vision (IJCV</title>
		<imprint>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2015-04">April 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,142.62,302.81,337.98,7.86;22,151.52,313.77,329.07,7.86;22,151.52,322.46,329.07,10.13;22,151.52,335.69,71.19,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="22,196.93,302.81,283.66,7.86;22,151.52,313.77,265.23,7.86">CNRS TELECOM ParisTech at ImageCLEF 2016 Scalable Concept Image Annotation Task: Overcoming the Scarcity of Training Data</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sahbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,437.46,313.77,43.13,7.86;22,151.52,324.73,254.66,7.86">CLEF2016 Working Notes. CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Évora, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-09">September 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,142.62,345.81,337.97,7.86;22,151.52,356.77,329.07,7.86;22,151.52,367.73,257.14,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="22,332.92,345.81,147.67,7.86;22,151.52,356.77,107.30,7.86">Evaluating Color Descriptors for Object and Scene Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2009.154</idno>
	</analytic>
	<monogr>
		<title level="j" coord="22,266.09,356.77,214.50,7.86;22,151.52,367.73,45.57,7.86">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1582" to="1596" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,142.62,377.86,337.97,7.86;22,151.52,388.82,245.34,7.86" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="22,278.92,377.86,201.67,7.86;22,151.52,388.82,69.82,7.86">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="22,142.62,398.94,337.98,7.86;22,151.52,409.90,329.07,7.86;22,151.52,420.86,25.60,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="22,417.55,398.94,63.05,7.86;22,151.52,409.90,87.13,7.86">Selective search for object recognition</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="22,246.61,409.90,165.50,7.86">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,142.62,430.99,337.98,7.86;22,151.52,441.95,329.07,7.86;22,151.52,452.90,329.07,7.86;22,151.52,463.86,329.07,7.86;22,151.52,474.82,198.38,7.86" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="22,449.27,452.90,31.32,7.86;22,151.52,463.86,206.00,7.86">General Overview of ImageCLEF at the CLEF 2016 Labs</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schaer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bromuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ramisa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dellandrea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">H</forename><surname>Toselli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="22,366.20,463.86,114.40,7.86;22,151.52,474.82,27.78,7.86">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,142.62,484.95,337.98,7.86;22,151.52,495.91,329.07,7.86;22,151.52,506.87,329.07,7.86;22,151.52,517.83,166.02,7.86" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="22,259.48,484.95,221.11,7.86;22,151.52,495.91,53.91,7.86">Image-Text Dataset Generation for Image Annotation and Retrieval</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,355.91,495.91,124.68,7.86;22,151.52,506.87,145.24,7.86">II Congreso Español de Recuperación de Información, CERI 2012</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Berlanga</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Rosso</surname></persName>
		</editor>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">June 18-19 2012</date>
			<biblScope unit="page" from="115" to="120" />
		</imprint>
		<respStmt>
			<orgName>Universidad Politécnica de Valencia</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="22,142.62,527.95,337.97,7.86;22,151.52,538.91,329.07,7.86;22,151.52,549.87,329.08,7.86;22,151.52,560.83,329.07,8.12;22,151.52,572.43,108.76,7.47" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="22,261.43,527.95,219.16,7.86;22,151.52,538.91,86.92,7.86">Overview of the ImageCLEF 2012 Scalable Web Image Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<ptr target="http://mvillegas.info/pub/Villegas12_CLEF_Annotation-Overview.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="22,151.52,549.87,293.11,7.86">CLEF 2012 Evaluation Labs and Workshop, Online Working Notes</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Karlgren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Womser-Hacker</surname></persName>
		</editor>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">September 17-20 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,142.62,581.91,337.98,7.86;22,151.52,592.87,329.07,7.86;22,151.52,603.83,329.07,7.86;22,151.52,614.79,326.10,8.12" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="22,262.58,581.91,218.02,7.86;22,151.52,592.87,96.12,7.86">Overview of the ImageCLEF 2014 Scalable Concept Image Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-1180/CLEF2014wn-Image-VillegasEt2014.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="22,271.70,592.87,107.11,7.86">CLEF2014 Working Notes</title>
		<title level="s" coord="22,387.41,592.87,93.18,7.86;22,151.52,603.83,31.91,7.86">CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<publisher>CEUR-WS.org</publisher>
			<date type="published" when="2014">September 15-18 2014</date>
			<biblScope unit="volume">1180</biblScope>
			<biblScope unit="page" from="308" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,142.62,624.92,337.97,7.86;22,151.52,635.88,329.07,7.86;22,151.52,646.84,329.07,8.12;22,151.52,658.44,282.93,7.47" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="22,315.57,624.92,165.02,7.86;22,151.52,635.88,167.36,7.86">Overview of the ImageCLEF 2013 Scalable Concept Image Annotation Subtask</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<ptr target="http://mvillegas.info/pub/Villegas13_CLEF_Annotation-Overview.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="22,342.84,635.88,137.75,7.86;22,151.52,646.84,133.20,7.86">CLEF 2013 Evaluation Labs and Workshop, Online Working Notes</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">September 23-26 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,142.62,120.67,337.98,7.86;23,151.52,131.63,329.07,7.86;23,151.52,142.59,329.07,7.86;23,151.52,153.55,329.07,8.12;23,151.52,165.15,104.06,7.47" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="23,255.98,120.67,224.61,7.86;23,151.52,131.63,174.42,7.86">Generating image descriptions with gold standard visual inputs: Motivation, evaluation and baselines</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W15-4722" />
	</analytic>
	<monogr>
		<title level="m" coord="23,345.69,131.63,134.90,7.86;23,151.52,142.59,208.36,7.86">Proceedings of the 15th European Workshop on Natural Language Generation (ENLG)</title>
		<meeting>the 15th European Workshop on Natural Language Generation (ENLG)<address><addrLine>Brighton, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-09">September 2015</date>
			<biblScope unit="page" from="117" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,142.62,175.46,337.97,7.86;23,151.52,186.42,329.07,7.86;23,151.52,197.38,329.07,7.86;23,151.52,208.34,329.07,7.86;23,151.52,219.30,89.16,7.86" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="23,351.55,175.46,129.04,7.86;23,151.52,186.42,329.07,7.86;23,151.52,197.38,19.99,7.86">A poodle or a dog? Evaluating automatic image annotation using human descriptions at different levels of granularity</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,191.46,197.38,240.56,7.86">Proceedings of the Third Workshop on Vision and Language</title>
		<meeting>the Third Workshop on Vision and Language<address><addrLine>Dublin City; Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-08">August 2014</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
		<respStmt>
			<orgName>University and the Association for Computational Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="23,142.62,230.26,337.98,7.86;23,151.52,241.22,329.07,7.86;23,151.52,249.91,257.30,10.13" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="23,367.46,230.26,113.13,7.86;23,151.52,241.22,117.54,7.86">Joint Learning of CNN and LSTM for Image Captioning</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,293.27,241.22,187.32,7.86;23,151.52,252.18,110.74,7.86">CLEF2016 Working Notes. CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Évora, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-09">September 2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
