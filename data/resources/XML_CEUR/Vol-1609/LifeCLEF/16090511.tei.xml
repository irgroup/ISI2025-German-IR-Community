<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,147.47,116.95,320.41,12.62;1,190.11,134.89,235.14,12.62">Feature Learning via Mixtures of DCNNs for Fine-Grained Plant Classification</title>
				<funder ref="#_xuK66qv">
					<orgName type="full">Australian Research Council</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,196.05,172.56,60.41,8.74;1,258.81,170.98,1.83,6.12"><forename type="first">Chris</forename><surname>Mccool</surname></persName>
							<email>c.mccool@qut.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Australian Center for Robotic Vision</orgName>
								<orgName type="institution">Queensland University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,269.05,172.56,59.85,8.74;1,331.25,170.98,1.83,6.12"><forename type="first">Zongyuan</forename><surname>Ge</surname></persName>
							<email>z.ge@qut.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Australian Center for Robotic Vision</orgName>
								<orgName type="institution">Queensland University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,360.86,172.56,51.94,8.74;1,415.16,170.98,1.83,6.12"><forename type="first">Peter</forename><surname>Corke</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Australian Center for Robotic Vision</orgName>
								<orgName type="institution">Queensland University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,147.47,116.95,320.41,12.62;1,190.11,134.89,235.14,12.62">Feature Learning via Mixtures of DCNNs for Fine-Grained Plant Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F94D4E91BCD15C6E6716E905F0BE0D9D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>deep convolutional neural network</term>
					<term>plant classification</term>
					<term>mixture of deep convolutional neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present the plant classification system submitted by the QUT RV team to the LifeCLEF 2016 plant task. Our system learns two deep convolutional neural network models. The first is a domain-specific model and the second is a mixture of content specific models, one for each of the plant organs such as branch, leaf, fruit, flower and stem. We combine these two models and experiments on the PlantCLEF2016 dataset show that this approach provides an improvement over the baseline system with the mean average precision improving from 0.603 to 0.629 on the test set.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Fine-grained image classification has received considerable attention recently with a particular emphasis on classifying various species of birds, dogs and plants <ref type="bibr" coords="1,164.59,441.92,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="1,176.50,441.92,7.75,8.74" target="#b1">2,</ref><ref type="bibr" coords="1,185.63,441.92,7.75,8.74" target="#b3">4,</ref><ref type="bibr" coords="1,194.76,441.92,7.01,8.74" target="#b7">8]</ref>. Fine-grained image classification is a challenging computer vision problem due to the small inter-class variation and large intra-class variation. Plant classification is a particularly important domain because of the implications for automating agriculture as well as enabling robotic agents to detect and measure plant distribution and growth.</p><p>To evaluate the current performance of the state-of-the-art vision technology for plant recognition, the Plant Identification Task of the LifeCLEF challenge <ref type="bibr" coords="1,159.72,525.61,11.47,8.74" target="#b4">[5,</ref><ref type="bibr" coords="1,171.19,525.61,7.64,8.74" target="#b6">7]</ref> focuses on distinguishing 1000 herb, tree and fern species. This is still an observation-centered task where several images from seven organs of a plant are related to one observation. There are seven organs, referred to as content types, and include images of the entire plant, branch, leaf, fruit, flower, stem or a leaf scan. In addition to the 1000 known classes, the 2016 PlantCLEF evaluation includes classes external to this, making this a more open-set recognition problem.</p><p>Inspired by <ref type="bibr" coords="1,200.00,609.29,9.96,8.74" target="#b2">[3]</ref>, we use a deep convolutional neural network (DCNN) approach and learn a separate DCNN for each content type. The DCNN for each content type is combined using a mixture of DCNNs. Combining this approach with a standard fine-tuned DCNN improves the mean average precision (mAP) from 0.601 to 0.629 on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our Approach</head><p>We propose a system that uses content-types during the training phase, but does not use this information at test time. This provides a more practical real-world system that does not require well labelled images from the user. In PlantCLEF 2016 there are 7 organ types ranging from branch through to fruit and stem, example images are given in Figure <ref type="figure" coords="2,292.20,192.27,3.87,8.74" target="#fig_0">1</ref>.</p><p>Our proposed system consists of two key parts. First, we learn a domaingeneric DCNN termed φ GCN N which classifies the plant image regardless of content type. Second, we learn a MixDCNN termed φ M DCN N which first learns a content specific DCNN for each 6 of the organ types<ref type="foot" coords="2,354.99,238.62,3.97,6.12" target="#foot_0">1</ref> . We combined the output of these two systems to form the final classification decision. For all of our systems, the base network that we use is the GoogLeNet model of Szegedy et al. <ref type="bibr" coords="2,449.82,264.10,9.96,8.74" target="#b8">[9]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Domain-Generic DCNN</head><p>We learn a domain-generic DCNN, φ GCN N , that ignores the content type of the plant image. This model uses only the class label information to train a very deep neural network consisting of 22 layers, the GoogLeNet model <ref type="bibr" coords="2,452.36,637.55,9.96,8.74" target="#b8">[9]</ref>. To apply this model to plant data we make use of transfer learning to fine-tune the parameters of this general object classification model to the problem at hand, plant classification.</p><p>Transfer learning has been used for a variety fo tasks with one of its earliest uses for fine-grained classification being to learn a bird classification model <ref type="bibr" coords="3,462.32,167.81,14.61,8.74" target="#b9">[10]</ref>. We use transfer learning to fine-tune the parameters of the GoogLeNet model by training it for approximately 18 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">MixDCNN</head><p>We learn a MixDCNN, φ M DCN N , which consists of K DCNNs. This allows each of the K DCNNs to learn feature appropriate for those samples that have been assigned to it, which in turn allows us to learn more appropriate and discrmininative features. We do this by calculating the probability that the k-th component (DCCN), S k , is responsible for the t-th sample x t . Such an approach also allows us to have a system that does not require the content type of the sample to be labelled at test time.</p><p>For PlantCLEF 2016 there are 7 pre-defined content types consisting of images from the entire plant, branch, leaf, fruit, flower, stem or a leaf scan. For the MixDCNN, we make use of the content type to learn a DCNN that is fine-tuned (specialised) for a subset of the content types. However, because of the similarity between the leaf and leaf scan content types we combine them into one. As such we learn K = 6 content types for the MixDCNN. To train the k-th component (DCNN) we use the N k images assigned to this subset X k = [x 1 , ..., x N k ], with their corresponding class labels. We then fine-tune the GoogLeNet model, similar to Section 2.1, to learn a content-specific model. Once each content-specific DCNN has been trained we then perform joint training using the MixDCNN.</p><p>The K trained content-specific models are then combined in a MixDCNN structure, shown in Figure <ref type="figure" coords="3,251.87,455.62,3.87,8.74" target="#fig_1">2</ref>. An important aspect of the MixDCNN model is to calculate the probability that the k-th component is responsible for the sample. This occupation probability is calculated as,</p><formula xml:id="formula_0" coords="3,262.86,499.21,217.73,26.56">α k = exp{C k } K c=1 exp{C c }<label>(1)</label></formula><p>where C k is the best classification result for S k using the t-th sample:</p><formula xml:id="formula_1" coords="3,266.65,556.04,213.94,14.58">C k,t = max n=1...N z k,n,t<label>(2)</label></formula><p>where there are N = 1000 classes and z k,n,t is classification score from the k-th component for the t-th sample and n-th class. This occupation probability gives higher weight to components that are confident about their prediction. The final classification score is then given by multiplying the output of the final layer from each component by the occupation probability and then summing over the K components:</p><formula xml:id="formula_2" coords="3,266.44,650.16,214.15,20.09">z n = K k=1 z k,n α k (3)</formula><p>This mixes the network outputs together. More details on this method can be found in <ref type="bibr" coords="4,174.33,131.95,9.96,8.74" target="#b2">[3]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section we present a comparative performance evaluation of our four runs.</p><p>We first present the results on the training set and then present the results on the test set followed by a brief discussion. We use Caffe <ref type="bibr" coords="4,386.91,414.73,10.52,8.74" target="#b5">[6]</ref> to learn all of our models, both domain-specific and MixDCNN.</p><p>At test time our model does not use any content information, rather it automatically classifies the image with minimal user information. This means we use all of the 113,205 images of 1,000 classes to train our model. Results on the training set are given in Table <ref type="table" coords="4,272.48,474.78,3.87,8.74" target="#tab_0">1</ref>, this table shows the result of the MixDCNN model after training for 2 epochs and 17 epochs. The system submitted was trained for only 2 epochs 2 due to resource and time constraints. 2 Further fine-tuning was performed after submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Results on Test Set</head><p>In this section, we present our submitted results for the PlantCLEF2016 challenge. We submitted four runs:</p><p>-QUT Run 1 is the Baseline result of using a fine-tuned GoogLeNet using all of the organ types, the rank 1 score submitted for each observation. -QUT Run 2 is the MixDCNN system with the rank 1 score submitted for each observation. -QUT Run 3 is the combination of the Baseline and MixDCNN systems, the rank 1 score was submitted for each observation. -QUT Run 4 is the combiation of the Baseline and MixDCNN system with a threshold to remove potential false positives.</p><p>In Figure <ref type="figure" coords="5,196.00,264.34,4.98,8.74" target="#fig_2">3</ref> we present the overall performance for all of the competitors using the defined score metric. It can be seen that our best performing system is RUN 3 which achieved a score of 0.629. This system, Fusion, consists of the combination of the Domain-Specific model, φ GCN N , with the MixDCNN model, φ M CN N , using equal weight fusion of the classification layers. A summary of these systems is presented in Table <ref type="table" coords="5,290.93,324.11,3.87,8.74" target="#tab_1">2</ref>. RUN4 is the same as RUN3 with a preset threshold τ to remove potential false positives. The precision of this system is considerably lower than any of the other systems and shows that choosing this threshold must be done judiciously. We then jointly optimise these K DCNN models by using the mixture of DCNNs framework. Combining these two approaches yields improved performance and demonstrates the importance of learning complementary models to perform accurate classification with the performance improving from 0.603 to 0.629. We note that the MixDCNN model was only trained for 2 epochs we expect improved performance with a model which has been trained for longer. Finally, this system is fully automatic as it does not require the organ (content) type to be specified at test time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,134.77,530.40,345.82,7.89;2,134.77,541.38,212.11,7.86;2,134.77,296.29,345.83,209.37"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example images of the 7 organs in the PlantCLEF dataset. From (a)-(g), branch, entire, leaf, leaf scan, flower, fruit, and stem.</figDesc><graphic coords="2,134.77,296.29,345.83,209.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,134.77,299.81,345.83,7.89;4,134.77,310.79,268.17,7.86;4,134.77,164.46,345.83,110.61"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. An overview of the structure of MixDCNN network which consists of K subnetworks that have been trained upon the particular content type.</figDesc><graphic coords="4,134.77,164.46,345.83,110.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,134.77,589.84,345.83,7.89;5,134.77,600.83,150.75,7.86;5,134.77,354.15,345.81,210.96"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The results of observation-based for the LifeCLEF Plant Task 2016. Image adapted from the organisers' website.</figDesc><graphic coords="5,134.77,354.15,345.81,210.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,134.77,530.77,345.82,90.53"><head>Table 1 :</head><label>1</label><figDesc>Top-5 accuracy on the training set and the number of epochs used for training the model. The submitted system consisted of the Domain-Specific Model and MixDCNN-v1.</figDesc><table coords="4,191.59,576.27,229.10,45.03"><row><cell>Method</cell><cell cols="2">Accuracy Number of Epochs</cell></row><row><cell cols="2">Domain-Specific Model 80.1%</cell><cell>18</cell></row><row><cell>MixDCNN-v1</cell><cell>81.0%</cell><cell>2</cell></row><row><cell>MixDCNN-v2</cell><cell>86.2%</cell><cell>17</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,134.77,116.95,345.83,175.64"><head>Table 2 :</head><label>2</label><figDesc>Mean average precision on the test set for the submitted models.In this paper we presented a domain-specific and MixDCNN model to perform automatic classification of plant images. The domain-specific model is learnt by fine-tuning a well known model specifically for the plant classification task. The MixDCNN model is learnt by first fine-tuning a model to K subsets of data, in this case by using different organ types.</figDesc><table coords="6,134.77,127.61,303.53,93.10"><row><cell>Method</cell><cell cols="2">Accuracy Number of Epochs</cell></row><row><cell>Domain-Specific Model (RUN1)</cell><cell>0.603</cell><cell>18</cell></row><row><cell>MixDCNN-v1 (RUN2)</cell><cell>0.564</cell><cell>2</cell></row><row><cell>Fusion (RUN3)</cell><cell>0.629</cell><cell>N/A</cell></row><row><cell>Fusion with threshold (RUN4)</cell><cell>0.367</cell><cell>N/A</cell></row><row><cell cols="2">4 Conclusions and Future Work</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.73,657.79,237.54,7.86"><p>The organ type leaf and leaf scan were combined into one.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The <rs type="institution">Australian Centre for Robotic Vision</rs> is supported by the <rs type="funder">Australian Research Council</rs> via the <rs type="programName">Centre of Excellence program</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_xuK66qv">
					<orgName type="program" subtype="full">Centre of Excellence program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,142.96,492.88,337.64,7.86;6,151.52,503.84,219.19,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,323.21,492.88,157.39,7.86;6,151.52,503.84,149.48,7.86">Symbiotic segmentation and part localization for fine-grained categorization</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,320.53,503.84,20.90,7.86">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,514.90,337.63,7.86;6,151.52,525.86,329.07,7.86;6,151.52,536.82,189.59,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,228.09,525.86,191.35,7.86">Local alignments for fine-grained categorization</title>
		<author>
			<persName coords=""><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arnold</forename><forename type="middle">Wm</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tinne</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,428.00,525.86,52.60,7.86;6,151.52,536.82,112.22,7.86">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,547.89,337.64,7.86;6,151.52,558.85,329.07,7.86;6,151.52,569.80,125.63,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="6,230.37,558.85,250.22,7.86;6,151.52,569.80,62.15,7.86">Fine-grained classification via mixture of deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">Zongyuan</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Mccool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Upcroft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Conrad</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Corke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,580.87,337.63,7.86;6,151.52,591.83,329.07,7.86;6,151.52,602.79,20.99,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,453.91,580.87,26.68,7.86;6,151.52,591.83,181.69,7.86">Subset feature learning for fine-grained classification</title>
		<author>
			<persName coords=""><forename type="first">Zongyuan</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Mccool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Conrad</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Corke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,341.78,591.83,134.71,7.86">CVPR Workshop on Deep Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,613.85,337.63,7.86;6,151.52,624.81,204.47,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,334.64,613.85,145.95,7.86;6,151.52,624.81,52.74,7.86">Plant identification in an open-world (lifeclef 2016)</title>
		<author>
			<persName coords=""><forename type="first">Hervé</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexis</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,223.96,624.81,103.70,7.86">CLEF working notes 2016</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,142.96,635.88,337.64,7.86;6,151.52,646.84,329.07,7.86;6,151.52,657.79,248.15,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="6,382.84,646.84,97.75,7.86;6,151.52,657.79,147.06,7.86">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName coords=""><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,120.67,337.64,7.86;7,151.52,131.63,329.07,7.86;7,151.52,142.59,329.07,7.86;7,151.52,153.55,247.31,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,331.78,142.59,148.81,7.86;7,151.52,153.55,94.47,7.86">Lifeclef 2016: multimedia life species identification challenges</title>
		<author>
			<persName coords=""><forename type="first">Alexis</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hervé</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hervé</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Concetto</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Willem-Pier And</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julien</forename><surname>Champ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Simone</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Henning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,265.48,153.55,105.00,7.86">Proceedings of CLEF 2016</title>
		<meeting>CLEF 2016</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,164.51,337.63,7.86;7,151.52,175.46,272.06,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,384.15,164.51,96.44,7.86;7,151.52,175.46,215.87,7.86">Confidence sets for finegrained categorization and plant species identification</title>
		<author>
			<persName coords=""><forename type="first">Asma</forename><surname>Rejeb Sfar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nozha</forename><surname>Boujemaa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,375.15,175.46,19.49,7.86">IJCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.96,186.42,337.64,7.86;7,151.52,197.38,329.07,7.86;7,151.52,208.34,199.46,7.86" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<title level="m" coord="7,456.47,197.38,24.13,7.86;7,151.52,208.34,98.94,7.86">Going deeper with convolutions</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.62,219.30,337.98,7.86;7,151.52,230.26,272.38,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,400.63,219.30,79.97,7.86;7,151.52,230.26,136.92,7.86">Part-based R-CNNs for fine-grained category detection</title>
		<author>
			<persName coords=""><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,307.99,230.26,23.05,7.86">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="834" to="849" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
