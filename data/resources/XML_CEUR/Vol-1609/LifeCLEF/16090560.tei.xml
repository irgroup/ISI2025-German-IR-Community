<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,156.36,143.57,298.80,11.61;1,188.64,160.49,233.95,11.61">Convolutional Neural Networks for Large-Scale Bird Song Classification in Noisy Environment</title>
				<funder>
					<orgName type="full">NVIDIA Corporation</orgName>
				</funder>
				<funder>
					<orgName type="full">NVidia Titan</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,250.20,196.96,55.60,8.48"><forename type="first">Bálint</forename><forename type="middle">Pál</forename><surname>Tóth</surname></persName>
							<email>toth.b@tmit.bme.hu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Telecommunications and Media Informatics</orgName>
								<orgName type="institution">Budapest University of Technology and Economics</orgName>
								<address>
									<addrLine>Magyar Tudósok krt. 2</addrLine>
									<postCode>H-1117</postCode>
									<settlement>Budapest</settlement>
									<country key="HU">Hungary</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,312.36,196.96,48.67,8.48"><forename type="first">Bálint</forename><surname>Czeba</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Telecommunications and Media Informatics</orgName>
								<orgName type="institution">Budapest University of Technology and Economics</orgName>
								<address>
									<addrLine>Magyar Tudósok krt. 2</addrLine>
									<postCode>H-1117</postCode>
									<settlement>Budapest</settlement>
									<country key="HU">Hungary</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,156.36,143.57,298.80,11.61;1,188.64,160.49,233.95,11.61">Convolutional Neural Networks for Large-Scale Bird Song Classification in Noisy Environment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A164C3F673D558B5B076A5A77A81A6E5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Convolutional Neural Network</term>
					<term>Deep Learning</term>
					<term>Classification</term>
					<term>Bird Song</term>
					<term>Audio</term>
					<term>Waveform</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes a convolutional neural network based deep learning approach for bird song classification that was used in an audio record-based bird identification challenge, called BirdCLEF 2016. The training and test set contained about 24k and 8.5k recordings, belonging to 999 bird species. The recorded waveforms were very diverse in terms of length and content. We converted the waveforms into frequency domain and splitted into equal segments. The segments were fed into a convolutional neural network for feature learning, which was followed by fully connected layers for classification. In the official scores our solution reached a MAP score of over 40% for main species, and MAP score of over 33% for main species mixed with background species.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Identification and classification of bird species can greatly help to explore biodiversity and to monitor unique patterns in different soundscapes <ref type="bibr" coords="1,358.20,472.60,10.12,8.48" target="#b0">[1]</ref>. The LifeCLEF 2016 is a competition hosted by CLEF Initiative (Conference and Labs of the Evaluation Forum, formerly known as Cross-Language Evaluation Forum) <ref type="bibr" coords="1,359.04,494.32,10.12,8.48" target="#b1">[2]</ref>. BirdCLEF 2016 <ref type="bibr" coords="1,440.40,494.32,10.91,8.48" target="#b2">[3]</ref> is a part of the LifeCLEF competition and addresses the classification of 999 different bird species based on audio recordings of Xeno-canto collaborative database <ref type="bibr" coords="1,419.28,515.80,10.12,8.48" target="#b3">[4]</ref>. Whereas the original Xeno-canto database includes about 275,000 audio records covering 9450 bird species from all around the world, the BirdCLEF 2016 focuses on South-America (Brazil, Colombia, Venezuela, Guyana, Suriname and French Guiana) and contains 24607 audio recordings belonging to the 999 bird species. The test set included 8596 recordings from the BirdCLEF 2015 challenge extended by soundscape recordings. The latter means that the recordings are not focusing on specific bird species, but contains the environmental sounds with arbitrary number of singing birds. The length of the samples was widely diverse, in the training set the longest recording was ~45 minutes long, and the shortest length of recording was ~260 milliseconds. In the test set the longest was about 2 hours and 18 minutes, while the shortest ~700 milliseconds.</p><p>The LifeCLEF challenge allows manually aided solutions (like crowdsourcing), however we have chosen state-of-the-art deep learning techniques to address the problem. Our solution uses two dimensional convolutional neural networks, that is trained with preprocessed bird songs transformed to the frequency domain.</p><p>The outline of this paper is as follows. Section 2 briefly overviews the application of convolutional neural networks in speech recognition and sound classification, furthermore investigates some solutions for the previous BirdCLEF challenges. Section 3 describes the data preparation method we applied. Section 4 introduces the applied deep learning technique and neural network architectures for bird song classification. Section 5 presents our results and Section 6 draws conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Besides image classification one of the main propelling force of deep learning is speech recognition. In speech recognition different deep learning techniques, like deep belief networks, deep neural networks and convolutional networks, are proven to surpass the accuracy of 'traditional' Gaussian Mixture Models <ref type="bibr" coords="2,339.00,323.32,10.21,8.48" target="#b4">[5]</ref>. Recurrent architectures, especially Long Short-Term Memory (LSTM) networks are successfully applied to speech recognition tasks as well <ref type="bibr" coords="2,236.88,344.92,10.12,8.48" target="#b5">[6]</ref>. Combining convolutional and LSTM-based recurrent networks the accuracy of speech recognition can be further improved <ref type="bibr" coords="2,394.20,355.72,10.12,8.48" target="#b6">[7]</ref>.</p><p>The task of bird song classification with neural networks has been investigated even back in 1997 <ref type="bibr" coords="2,194.28,377.32,10.03,8.48" target="#b7">[8]</ref>. They have applied feedforward neural network with 3-8 hidden neurons to classify 6 bird species from 133 recordings. They have achieved 82% accuracy with neural nets, however Quadratic Discriminant Analysis reached significantly better results, namely 93%. Another approach is presented in <ref type="bibr" coords="2,357.48,409.84,10.12,8.48" target="#b8">[9]</ref>. In their work after noise reduction 13 dimensional Mel-Frequency Cepstral Coefficient (MFCC) features were extracted and their dynamic counterpart were calculated. This 26 dimensional vector of the current, the preceding and the following frames were fed into a feed forward neural network with one hidden layer and 10-160 hidden neurons. They reached 98.7% and 86.8% accuracy on classifying 4 and 14 bird species, respectively. In <ref type="bibr" coords="2,413.40,463.84,15.59,8.48" target="#b9">[10]</ref> a random forest based segmentation method is shown to select bird calls in noisy environments with 93.6% accuracy. The work introduced in <ref type="bibr" coords="2,323.52,485.32,15.59,8.48" target="#b10">[11]</ref> uses binned frequency spectrum, MFCC and Linear Prediction Coefficients (LPC) features, that are classified by an ensemble of logistic regression, random forests and extremely randomized trees. They achieved 4th place on NIPS4B bird classification challenge hosted on Kaggle.</p><p>There have been a number of competitive approaches in the BirdCLEF challenges of previous years, however deep learning was not applied in the BirdCLEF competition before. The winning solution of 2014 used a robust feature extraction (including MFCC, fundamental frequency, zero crossing rate, energy features, etc. -altogether 6669 features per recordings), feature selection (reducing the number of features from 6669 to 1277) and template matching. The last year's challenge was won by the same competitor. His work described in <ref type="bibr" coords="2,270.96,593.56,15.59,8.48" target="#b11">[12]</ref> downsamples the spectrograms for faster feature extraction, applies decision trees for feature ranking and selection and bootstrap aggregating for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data preparation</head><p>As a first step, we downsampled every audio file to 16 kHz frequency in order to reduce the size of the training data. Following the preprocessing steps of <ref type="bibr" coords="3,387.24,176.68,14.45,8.48" target="#b9">[10]</ref>, first a Hamming window and then a short-time FFT were applied with a frame length of 512 samples and 256 samples overlap between subsequent frames. Next we implemented and applied a filtering method to extract the essential parts of the spectrogram, that contains bird calls. Some previous work (e.g. <ref type="bibr" coords="3,281.88,219.88,15.16,8.48" target="#b10">[11]</ref>) filters frequencies below 1 kHz, however in the current dataset we found useful information also in this range (see Figure <ref type="figure" coords="3,434.28,230.80,3.34,8.48" target="#fig_0">1</ref>), so we only applied low-pass filter with cutoff frequency of 6250 Hz. As a result, the vertical dimension (frequency) of the spectogram was 200, and the horizontal dimension (time) depended on the length of the recording. In the time domain (horizontal axis) we split the spectograms into 30 sample long columns (that corresponds ~0.5 seconds) and in the frequency domain (vertical axis) we split the spectrograms into 10 sample high rows. As a result, every spectogram was split into 30✕10 sized cells. We used these cells to remove the irrelevant parts (that is likely not to contain any bird call) of the spectrogram based on the mean and the variance. We calculated the mean and variance of every 10 sample high row (that corresponds a frequency range). If a cell's mean is less than 1.5 times the addition of mean plus variance of the actual row, than we dropped the cell. In case of Run 1, 3 and 4 we also removed those parts of the filtered spectrogram where 95% of the column vectors were zeros (see Figure <ref type="figure" coords="3,157.20,550.24,3.34,8.48" target="#fig_1">2</ref>). This step was skipped at our second submission (referred to as 'BME TMIT Run 2' in the official results; see Table <ref type="table" coords="3,276.84,561.16,3.38,8.48" target="#tab_0">1</ref>). After these preprocessing steps we split the remaining parts of the spectrogram to five seconds long pieces. Thus the dimensions of the resulting arrays were 200✕310 (310 samples corresponds to five seconds). We used this as the input of the convolutional neural network.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Deep learning based classification</head><p>For classifying the bird songs we used convolutional neural networks. The resulting 200✕310 arrays of the spectograms after data preparation were fed into the convolutional neural network and was treated like grayscale images. We used two different CNN architectures: the first one was inspired by the winner architecture of 2012 ImageNet competition <ref type="bibr" coords="5,231.24,211.48,15.59,8.48" target="#b13">[14]</ref> (AlexNet <ref type="bibr" coords="5,288.12,211.48,14.20,8.48" target="#b14">[15]</ref>), the second convolutional neural network was inspired by audio recognition systems.</p><p>In the first type of neural network we modified the shape of the input and the convolutional layers of AlexNet. We also added batch normalization layers before the maxpooling layers. Experiments show that with batch normalization significantly better accuracy can be achieved on MNIST and ImageNet datasets with faster convergence <ref type="bibr" coords="5,450.12,265.48,14.54,8.48" target="#b15">[16]</ref>. This network is referred to as CNN-Bird-1.</p><p>The second type of neural network used a simpler architecture, it consisted four convolutional layers and the fully connected layers had less neurons. We used ReLUs as activation functions <ref type="bibr" coords="5,219.12,308.80,15.71,8.48" target="#b16">[17]</ref> and batch normalization layers were also applied. The number of parameters of the second network was much less, thus the network was learning faster. This network is referred to as CNN-Bird-2. The proposed networks are shown in Figure <ref type="figure" coords="5,179.64,341.20,3.51,8.48" target="#fig_3">4</ref>.</p><p>To train the model we used RMSProp adaptive algorithm as optimizer <ref type="bibr" coords="5,432.48,352.00,15.59,8.48" target="#b17">[18]</ref> with mini-batch learning. Early stopping with a patience of 100 epochs was applied. Due to the fact that we split each audio file to smaller pieces (that were fed to the CNNs) if a recording was longer than five seconds we had to combine the multiple predictions of the neural network. In case of 'BME TMIT Run 1, 2, 3' we simply calculated the mean of the classification results. In case of 'BME TMIT Run 4' we used a custom calculation method for submitting the classification results: if the recording was split into more parts than we calculated the variance of the CNN's outputs of each predicted class throughout the 5 seconds long split parts. Next the six predictions with the highest variance were selected. The predicted bird species came from the mean of these predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>The hardware we used for training were a NVidia GTX 970 (4 GB) and a NVidia Titan X (12 GB) GPU card hosted in two i7 servers with 32 GB RAM. Ubuntu 14.04 with Cuda 7.5 and cuDNN 4.0 was used as general software architecture. For data preparation, training and evaluating deep neural networks the Keras <ref type="bibr" coords="6,389.04,312.52,15.59,8.48" target="#b18">[19]</ref> framework with Theano <ref type="bibr" coords="6,174.36,323.32,15.59,8.48" target="#b19">[20]</ref> backend was used. For calculating area under the precision-recall curve (AUROC) values we used the sklearn Python package. The differences in data preparation (see Section 3), in the architectures, in the combination method of the predictions (see Section 4) and the epochs needed to reached the maximum AUROC measured on validation set are summarized in Table <ref type="table" coords="6,292.56,366.52,3.45,8.48" target="#tab_0">1</ref>. The AUROC values throughout the training of Run 1, 2, 3 and 4 are shown in Figure <ref type="figure" coords="6,295.08,377.32,3.51,8.48" target="#fig_4">5</ref>. The database sizes, the data preparation and CNN training times are shown in Table <ref type="table" coords="6,293.76,388.24,3.51,8.48" target="#tab_1">2</ref>.  We investigated the accuracy of the model on a separated test set. The least average precisions (AP) were achieved by Ochre-rumped Antbird (AP=0.00067), Santa Marta Antpitta (AP=0.00136) and Rufous-breasted Leaftosser (AP=0.0015) bird calls. Yellow-eared Parrot (AP=0.692), Lesser Woodcreeper (AP=0.796) and Spillmann's Tapaculo (AP=0.899) species scored the best in the test. Furthermore, a lot of bird calls were misclassified to Orange-billed Nightingale-Thrush (AP=0.229). Analyzing the waveforms and the spectrograms of these species we couldn't find any particular feature. Hence we suppose the significant difference in AP and the misclassification are generally caused by some shortcomings of the proposed CNN architectures.</p><p>The MAP (Mean Average Precision) values of our submission in the official results are shown in Table <ref type="table" coords="7,215.76,270.16,3.51,8.48" target="#tab_2">3</ref>. The first MAP value corresponds to the recordings in which there was a dominant singing bird in the foreground with some other ones in the background. The second MAP is for recordings with only one singing bird. And the third MAP value is for the soundscape audio, that was not targeting specific species and these recordings might have contained an arbitrary number of singing birds. The results show that the smaller convolutional neural network (CNN-Bird-2; Run 3 and 4), which was faster to train performed similarly as the bigger CNN. However, the gain in AUROC on the validation database is not reflected in the official results (MAP values) in case of Run 3 and 4. Moreover the difference in the combination methods of Run 3 and 4 could be measured on the validation set, but in the official results Run 4 didn't outperform our other approaches. According to the official results we resulted the 4th place out of 6. It should be noted that we joined the competition only on April and we had no previous experience with bird call recognition.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>In this paper a deep learning based approach was presented for large-scale bird species identification based on their songs. In the data preparation process the spectogram for every recording was calculated and the irrelevant parts were removed. The resulting spectogram was sliced into five seconds long segments, these segments were used as input of the CNN. Two different types of CNNs were used that achieved about the same accuracy, while one of them had much less parameters. At the final step the predictions of the slices were combined. The results show that the deep learning based approach is well suitable for the task, however fine-tuning is necessary to reach better accuracy, like separating time and frequency in the CNN feature learning part and applying recurrent architectures, e.g. Long Short-Term Memory (LSTM).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,188.04,400.00,235.38,8.48;3,212.28,261.12,186.84,136.92"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example of useful information (bird call) below 1 kHz.</figDesc><graphic coords="3,212.28,261.12,186.84,136.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,143.16,427.38,325.34,7.66;4,143.16,437.82,95.71,7.66;4,174.48,138.36,262.08,266.04"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Example of a spectrogram before (above) and after (below) preprocessing, when zero elements were kept (Run 2).</figDesc><graphic coords="4,174.48,138.36,262.08,266.04" type="vector_box" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,143.64,617.80,324.16,8.48;4,252.72,628.84,105.90,8.48;4,196.32,467.76,218.64,148.44"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Example for the same spectrogram after preprocessing, when mostly-zero cells were removed (Run 1, 3, 4).</figDesc><graphic coords="4,196.32,467.76,218.64,148.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,143.16,593.94,325.22,7.66;5,143.16,604.26,325.18,7.66;5,143.16,614.70,230.69,7.66"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. CNN-Bird-1 (above) and CNN-Bird-2 (below) convolutional neural networks for bird species identification based on the spectogram of bird song recordings. (A@BxC refers to A number of planes with size BxC. The DxD refers to the kernel size.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,161.64,140.94,288.19,7.66"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The value of AUROC throughout the training for Run 1, Run 2 and Run 3&amp;4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,155.64,410.82,297.05,92.43"><head>Table 1 .</head><label>1</label><figDesc>The experimental setup of the submitted runs.</figDesc><table coords="6,155.64,427.48,297.05,75.77"><row><cell>Run</cell><cell>Data preparation</cell><cell>CNN</cell><cell>Combination of</cell><cell>Epochs</cell></row><row><cell></cell><cell></cell><cell>architecture</cell><cell>the predictions</cell><cell></cell></row><row><cell>BME TMIT Run 1</cell><cell>'Zero' parts removed</cell><cell>CNN-Bird-1</cell><cell>Mean</cell><cell>124</cell></row><row><cell cols="3">BME TMIT Run 2 'Zero' parts not removed CNN-Bird-1</cell><cell>Mean</cell><cell>121</cell></row><row><cell>BME TMIT Run 3</cell><cell>'Zero' parts removed</cell><cell>CNN-Bird-2</cell><cell>Mean</cell><cell>104</cell></row><row><cell>BME TMIT Run 4</cell><cell>'Zero' parts removed</cell><cell cols="2">CNN-Bird-2 Mean of top six pre-</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>dictions with highest</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>variance</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,150.36,422.46,302.39,71.19"><head>Table 2 .</head><label>2</label><figDesc>Database sizes, data preparation (left) and CNN training (right) times.</figDesc><table coords="7,150.36,439.12,302.39,54.53"><row><cell>Method</cell><cell>Preparation time [minutes]</cell><cell>Size [GB]</cell><cell>CNN architecture CNN-Bird-1</cell><cell>Training time [hours] 37.8</cell></row><row><cell>'Zero' parts removed</cell><cell>103</cell><cell>41.8</cell><cell>CNN-Bird-2 CNN-Bird-3 &amp; 4</cell><cell>48.8 27.6</cell></row><row><cell>'Zero' parts not removed</cell><cell>123</cell><cell>48.76</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,154.80,508.02,306.59,75.15"><head>Table 3 .</head><label>3</label><figDesc>Official results: MAP values of our submissions.</figDesc><table coords="7,154.80,524.68,306.59,58.49"><row><cell>Run</cell><cell>MAP</cell><cell>MAP</cell><cell>MAP</cell></row><row><cell></cell><cell>(with background species)</cell><cell>(only main species)</cell><cell>('soundscape' recordings)</cell></row><row><cell>BME TMIT Run 1</cell><cell>0.323</cell><cell>0.407</cell><cell>0.054</cell></row><row><cell>BME TMIT Run 2</cell><cell>0.338</cell><cell>0.426</cell><cell>0.053</cell></row><row><cell>BME TMIT Run 3</cell><cell>0.337</cell><cell>0.426</cell><cell>0.059</cell></row><row><cell>BME TMIT Run 4</cell><cell>0.335</cell><cell>0.424</cell><cell>0.053</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p><rs type="person">Bálint Pál Tóth</rs> gratefully acknowledges the support of <rs type="funder">NVIDIA Corporation</rs> with the donation of an <rs type="funder">NVidia Titan</rs> X GPU used for his research.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,150.57,380.58,317.81,7.66;8,159.12,391.02,309.05,7.66;8,159.12,401.34,182.09,7.66" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,325.92,380.58,142.46,7.66;8,159.12,391.02,39.28,7.66">Computational bioacoustics for assessing biodiversity</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">H</forename><surname>Frommolt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bardeli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Clausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,214.32,391.02,253.85,7.66;8,159.12,401.34,72.57,7.66">Proceedings of the International Expert meeting on IT-based detection of bioacoustical patterns</title>
		<meeting>the International Expert meeting on IT-based detection of bioacoustical patterns</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page">234</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,150.57,411.66,317.86,7.66;8,159.12,421.98,309.26,7.66;8,159.12,432.42,157.01,7.66" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,284.28,421.98,184.10,7.66;8,159.12,432.42,34.43,7.66">LifeCLEF 2016: multimedia life species identification challenges</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Champ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,199.08,432.42,92.42,7.66">Proceedings of CLEF 2016</title>
		<meeting>CLEF 2016</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,150.57,442.62,317.81,7.66;8,159.12,453.06,154.49,7.66" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,368.52,442.62,99.86,7.66;8,159.12,453.06,16.58,7.66">LifeCLEF Bird Identification Task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,198.96,453.06,70.73,7.66">CLEF working notes</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page">2016</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,150.57,463.38,311.86,7.66" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="8,270.24,463.38,188.54,7.66">Xeno-canto: Sharing bird sounds from around the world</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Xeno-canto Foundation</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,150.57,473.70,317.74,7.66;8,159.12,484.02,309.05,7.66;8,159.12,494.46,309.17,7.66;8,159.12,504.66,124.37,7.66" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,326.64,484.02,141.53,7.66;8,159.12,494.46,224.00,7.66">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,388.32,494.46,79.97,7.66;8,159.12,504.66,16.66,7.66">Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012">2012</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,150.57,515.10,317.73,7.66;8,159.12,525.42,309.26,7.66;8,159.12,535.74,133.73,7.66" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,311.64,515.10,156.67,7.66;8,159.12,525.42,29.61,7.66">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,205.32,525.42,263.06,7.66;8,159.12,535.74,51.98,7.66">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2013)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,150.57,546.06,317.74,7.66;8,159.12,556.50,309.19,7.66;8,159.12,566.70,237.17,7.66" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,331.32,546.06,136.99,7.66;8,159.12,556.50,131.76,7.66">Convolutional, long short-term memory, fully connected deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,309.00,556.50,159.31,7.66;8,159.12,566.70,155.42,7.66">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2015)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4580" to="4584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,150.57,577.14,317.81,7.66;8,159.12,587.46,309.07,7.66;8,159.12,597.78,245.81,7.66" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,267.36,577.14,201.02,7.66;8,159.12,587.46,60.28,7.66">Bird song identification using artificial neural networks and statistical analysis</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">L</forename><surname>Mcilraith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">C</forename><surname>Card</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,233.76,587.46,234.43,7.66;8,159.12,597.78,155.28,7.66">IEEE Canadian Conference on Electrical and Computer Engineering, Engineering Innovation: Voyage of Discovery</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="63" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,150.57,608.10,317.72,7.66;8,159.12,618.54,309.05,7.66;8,159.12,628.74,291.53,7.66" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,359.88,608.10,108.41,7.66;8,159.12,618.54,143.70,7.66">Sensor network for the monitoring of ecosystem: Bird species recognition</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Roe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,318.12,618.54,150.05,7.66;8,159.12,628.74,166.94,7.66">IEEE 3rd International Conference on Intelligent Sensors, Sensor Networks and Information</title>
		<meeting><address><addrLine>ISSNIP</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-12">December. 2007. 2007</date>
			<biblScope unit="page" from="293" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,150.37,140.94,317.80,7.66;9,159.12,151.26,309.26,7.66;9,159.12,161.70,89.81,7.66" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,286.80,140.94,181.37,7.66;9,159.12,151.26,91.41,7.66">Multi-label bird classification using an ensemble classifier with simple features</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">R</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">H</forename><surname>Dat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,257.28,151.26,211.10,7.66;9,159.12,161.70,32.92,7.66">Asia Pacific Signal and Information Processing Association (APSIPA)</title>
		<imprint>
			<date type="published" when="2014-12">December. 2014</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,150.37,171.90,317.90,7.66;9,159.12,182.34,309.26,7.66;9,159.12,192.66,196.13,7.66" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,315.36,171.90,152.90,7.66;9,159.12,182.34,95.86,7.66">Time-frequency segmentation of bird song in noisy acoustic environments</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Briggs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Raich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><forename type="middle">Z</forename><surname>Fern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,271.20,182.34,197.18,7.66;9,159.12,192.66,114.40,7.66">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2011)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2012" to="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,150.37,202.98,318.02,7.66;9,159.12,213.30,256.13,7.66" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,215.88,202.98,252.51,7.66;9,159.12,213.30,72.14,7.66">Improved automatic bird identification through decision tree based feature selection and bagging</title>
		<author>
			<persName coords=""><forename type="first">Mario</forename><surname>Lasseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,246.24,213.30,140.58,7.66">Working notes of CLEF 2015 Conference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,150.37,223.74,317.92,7.66;9,159.12,233.94,92.57,7.66" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,215.16,223.74,186.70,7.66">Large-scale Identification of Birds in Audio Recordings</title>
		<author>
			<persName coords=""><forename type="first">Mario</forename><surname>Lasseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,416.40,223.74,51.89,7.66;9,159.12,233.94,18.20,7.66">Working Notes CLEF</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="643" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,150.37,244.38,317.94,7.66;9,159.12,254.70,309.14,7.66;9,159.12,265.02,262.61,7.66" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,336.60,254.70,131.66,7.66;9,159.12,265.02,31.06,7.66">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Hunag</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,195.84,265.02,138.63,7.66">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,150.37,275.34,317.68,7.66;9,159.12,285.78,309.17,7.66;9,159.12,295.98,43.73,7.66" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="9,321.12,275.34,146.93,7.66;9,159.12,285.78,75.82,7.66">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,251.40,285.78,177.06,7.66">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,150.37,306.42,317.90,7.66;9,159.12,316.74,291.29,7.66" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="9,298.74,306.42,169.52,7.66;9,159.12,316.74,146.56,7.66">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName coords=""><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,150.37,327.06,317.90,7.66;9,159.12,337.38,268.01,7.66" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="9,249.24,327.06,206.05,7.66">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,159.12,337.38,194.68,7.66">Proc. 27th International Conference on Machine Learning</title>
		<meeting>27th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,150.37,347.82,318.02,7.66;9,159.12,358.02,309.17,7.66" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="9,254.40,347.82,213.99,7.66;9,159.12,358.02,75.88,7.66">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,240.96,358.02,182.21,7.66">COURSERA: Neural Networks for Machine Learning</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,150.37,368.46,317.94,7.66;9,159.12,378.78,130.25,7.66" xml:id="b18">
	<monogr>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Keras</surname></persName>
		</author>
		<ptr target="http://keras.io." />
		<title level="m" coord="9,224.52,368.46,122.04,7.66">Theano-based deep learning library</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,150.37,389.10,317.92,7.66;9,159.12,399.42,172.61,7.66" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="9,254.40,389.10,213.89,7.66;9,159.12,399.42,30.30,7.66">A Python framework for fast computation of mathematical expressions</title>
		<author>
			<persName coords=""><forename type="first">Theano</forename><surname>The</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Development</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02688</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,150.37,409.86,317.94,7.66;9,159.12,420.06,63.41,7.66" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="9,283.44,409.86,86.12,7.66">Long short-term memory</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,377.52,409.86,67.19,7.66">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
