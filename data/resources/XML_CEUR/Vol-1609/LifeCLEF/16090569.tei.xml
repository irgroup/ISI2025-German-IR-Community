<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,147.84,141.77,315.67,11.61;1,157.20,156.89,296.94,11.61">Deep Learning and SVM Classification for Plant Recognition in Content-Based Large Scale Image Retrieval</title>
				<funder>
					<orgName type="full">NVIDIA Corporation</orgName>
				</funder>
				<funder>
					<orgName type="full">NVidia Titan</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,184.68,193.62,61.40,9.40"><forename type="first">Bálint</forename><forename type="middle">Pál</forename><surname>Tóth</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Telecommunications and Media Informatics</orgName>
								<orgName type="institution">Budapest University of Technology and Economics Magyar Tudósok</orgName>
								<address>
									<addrLine>krt. 2</addrLine>
									<postCode>H-1117</postCode>
									<settlement>Budapest</settlement>
									<country key="HU">Hungary</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,253.08,193.62,60.33,9.40"><forename type="first">Márton</forename><surname>Osváth</surname></persName>
							<email>osvathmarton@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Telecommunications and Media Informatics</orgName>
								<orgName type="institution">Budapest University of Technology and Economics Magyar Tudósok</orgName>
								<address>
									<addrLine>krt. 2</addrLine>
									<postCode>H-1117</postCode>
									<settlement>Budapest</settlement>
									<country key="HU">Hungary</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,320.52,193.62,46.42,9.40"><forename type="first">Dávid</forename><surname>Papp</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Telecommunications and Media Informatics</orgName>
								<orgName type="institution">Budapest University of Technology and Economics Magyar Tudósok</orgName>
								<address>
									<addrLine>krt. 2</addrLine>
									<postCode>H-1117</postCode>
									<settlement>Budapest</settlement>
									<country key="HU">Hungary</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,374.16,193.62,52.51,9.40"><forename type="first">Gábor</forename><surname>Szűcs</surname></persName>
							<email>szucs@tmit.bme.hu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Telecommunications and Media Informatics</orgName>
								<orgName type="institution">Budapest University of Technology and Economics Magyar Tudósok</orgName>
								<address>
									<addrLine>krt. 2</addrLine>
									<postCode>H-1117</postCode>
									<settlement>Budapest</settlement>
									<country key="HU">Hungary</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,147.84,141.77,315.67,11.61;1,157.20,156.89,296.94,11.61">Deep Learning and SVM Classification for Plant Recognition in Content-Based Large Scale Image Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">14C3D6368D6BA57BA95DBEAED341A90B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>deep learning</term>
					<term>convolutional neural networks</term>
					<term>GMM based Fisher vector</term>
					<term>C-support vector classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The PlantCLEF 2016 challenge focused on tree, herb and fern species identification based on different types of images. The aim of the task was to classify the plants in the images to species and to give a confidence score depicting the probability that a prediction is true. We elaborated different classification methods for this challenge. We applied dense SIFT for feature detection and description; and Gaussian Mixture Model based Fisher vector was calculated to represent an image with high-level descriptor. Fisher vectors were classified by a special SVM, the C-support vector classification algorithm with RBF (Radial Basis Function) kernel. Furthermore, we applied deep learning method to train convolutional neural network (CNN) for feature learning and fullyconnected layers with softmax output for classification. We also combined these classifiers using the weighted average of their outputs. The final results show that the CNN achieved better result than the SVM, and the combined method slightly surpasses the CNN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Being able to identify the different species of plants growing in agricultural areas and to automatically detect the presence of invasive species is crucial. Identifying plants is usually a difficult task, sometimes for professionals (such as farmers or wood exploiters) as well. Using content-based image retrieval technologies is a promising possibility in this scenario. In order to solve it a challenge is announced in the LifeCLEF campaign <ref type="bibr" coords="1,355.92,570.66,11.03,9.40" target="#b0">[1]</ref>.</p><p>The image-based plant identification task, briefly PlantCLEF 2016 <ref type="bibr" coords="1,456.12,582.42,12.21,9.40" target="#b1">[2]</ref> was focused on tree, herb and fern species identification based on different types of images. The number of species was 1000, and there were 7 viewpoints at the images: branch, leaf, scan (scan or scan-like pictures of leaf, briefly "LeafScan"), flower, fruit, stem, and entire views. The data were sample of the stream of the raw query images submitted by the users of the popu-lar mobile application called Pl@ntNet (available on iPhone and Android), which accounts for several hundreds of thousands of active users submitting about ten thousands of query images daily.</p><p>The aim of the task was to classify images into the known categories (species), but the classification system had to be robust to unseen categories. It was a more difficult problem, because the test set contained images of species that were not in the training set (these are unseen categories). Besides the images contextual metadata (date, location, author and rating information) were also available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Previous works</head><p>In the last two years there have been a number of successful deep learning, SVM and combined solutions in the LifeCLEF competition. In 2014 a combined system of convolutional neural nets and SVM won the challenge <ref type="bibr" coords="2,453.60,313.86,11.03,9.40" target="#b2">[3]</ref>. The CNNs had five convolutional layers, however their pure deep learning solution was outperformed with the combined systems. A part of our team participated in the same competition <ref type="bibr" coords="2,309.00,349.50,11.03,9.40" target="#b3">[4]</ref>. They used GMM based Fishervector for image representation, and SVM for classification. A different group, in the same year used the BoW model with OpponentColor SIFT descriptors and SVM, but their results were less convincing <ref type="bibr" coords="2,393.72,385.14,11.12,9.40" target="#b4">[5]</ref>. Also in 2014 another group used a pretrained Overfeat <ref type="bibr" coords="2,316.44,397.14,12.09,9.40" target="#b5">[6]</ref> network for feature learning, and the output of the fully-connected layer (before the softmax layer) was fed into a tree-based ensemble classifier <ref type="bibr" coords="2,279.72,420.78,11.03,9.40" target="#b6">[7]</ref>. However, other groups with SVM based solutions resulted better. In 2015 an Inception CNN model based network won the competition <ref type="bibr" coords="2,232.68,444.66,11.03,9.40" target="#b7">[8]</ref>. They have pretrained the model with ImageNet and fine-tuned with the PlantCLEF database. They used the combined output of five CNNs, that were fine-tuned with randomly selected parts of the database. Also in last year's competition a pretrained AlexNet was fine-tuned, which resulted the 4th place <ref type="bibr" coords="2,238.68,492.18,11.03,9.40" target="#b8">[9]</ref>. For fine-tuning they have reset the last (softmax) layer and they trained the last layer with relatively high learning rate <ref type="bibr" coords="2,433.08,504.06,17.37,9.40" target="#b9">(10)</ref> and the rest of the layers with a much lower learning rate (0.1). We elaborated fully automatic methods (one by Fisher vectors and SVM, and another one by deep learning) for the classification of the images, and then they are taken in decreasing order based on reliability of classification decision; the next sections will present the details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Classification by Fisher vector and SVM</head><p>The first part of the classification was the representation of each image based on visual content. Following the general trend, we applied BoW (Bag-of-Words) model [10⎼12] for this purpose. This consists of three steps: (i) fea-ture detection, (ii) feature description, (iii) image description as usual phases in computer vision and we solved these steps similarly to our previous work <ref type="bibr" coords="3,143.16,164.58,11.03,9.40" target="#b2">[3]</ref>.</p><p>For feature detection and description we used the SIFT (Scale Invariant Feature Transform) algorithm <ref type="bibr" coords="3,290.40,188.34,17.25,9.40" target="#b12">[13]</ref> with dense keypoint sampling. After that, we performed PCA (Principal Component Analysis) <ref type="bibr" coords="3,391.32,200.22,16.39,9.40" target="#b13">[14,</ref><ref type="bibr" coords="3,411.48,200.22,13.77,9.40" target="#b14">15]</ref> to reduce the dimensions of the descriptor vectors from 128 to 80. Finally, we encoded the low-level descriptor vectors to get GMM (Gaussian Mixture Model) <ref type="bibr" coords="3,451.80,223.98,16.51,9.40" target="#b15">[16,</ref><ref type="bibr" coords="3,143.16,235.98,13.77,9.40" target="#b16">17]</ref> based Fisher-vectors <ref type="bibr" coords="3,247.68,235.98,16.51,9.40" target="#b15">[16,</ref><ref type="bibr" coords="3,266.88,235.98,12.29,9.40" target="#b17">18]</ref>. These vectors were the final representations (image descriptor) of the images.</p><p>For the classification subtask we used a variation of SVM (Support Vector Machine), the C-SVC (C-support vector classification) <ref type="bibr" coords="3,412.08,271.62,16.39,9.40" target="#b18">[19,</ref><ref type="bibr" coords="3,432.36,271.62,13.65,9.40" target="#b19">20]</ref> with RBF (Radial Basis Function) kernel. We applied the one-against-all technique to extend SVM for multi-class classification. Furthermore, a validation set were used to optimize the two hyperparameters (C from C-SVC and γ from RBF kernel).</p><p>The results of SVM classification was submitted as 'BME TMIT Run2'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Classification by deep learning</head><p>Nowadays state-of-the-art image recognition and classification solutions generally use deep learning methodology. Deep convolutional neural networks are able to learn the descriptive features of the image database in many abstraction levels. Convolutional neural networks raised a lot of interests in 2012, when a team led by Geoffrey Hinton and Alex Krizhevsky won the ImageNet Large Scale Visual Recognition Competition <ref type="bibr" coords="3,375.60,456.54,17.25,9.40" target="#b20">[21]</ref> by a large margin <ref type="bibr" coords="3,143.16,468.42,15.90,9.40" target="#b21">[22]</ref>. This model is often referred to as AlexNet. AlexNet consists of five convolutional layers, from which the first, second and fifth are followed by max-pooling layers. This part is responsible for feature learning. The second part of AlexNet includes three fully-connected layers with an output layer of 1000 softmax neurons for classification.</p><p>In the data preparation phase we applied cropping, scaling and normalization. Hence we only cropped the center of the images along the shorter dimension and scaled it down to the network's input dimension, which is 224x224 pixels. Finally, we normalized the red, green and blue color channels individually to zero mean and unit variance. An example of the resulting image is shown in Figure <ref type="figure" coords="3,239.28,587.34,3.87,9.40" target="#fig_0">1</ref>. For training the PlantCLEF 2015 database we used a modified version of AlexNet <ref type="bibr" coords="4,181.80,256.26,15.99,9.40" target="#b21">[22]</ref>. We changed the ReLU activation functions to parametric Re-LUs (PReLUs) <ref type="bibr" coords="4,208.44,268.26,15.99,9.40" target="#b22">[23]</ref>. Furthermore, we applied batch normalization <ref type="bibr" coords="4,421.92,268.26,17.25,9.40" target="#b23">[24]</ref> before the max-pooling layers of AlexNet. The block diagram of the proposed convolutional network is shown in Figure <ref type="figure" coords="4,303.36,292.02,3.93,9.40" target="#fig_1">2</ref>. For optimizer we chose AdaDelta <ref type="bibr" coords="4,292.08,482.22,15.90,9.40" target="#b24">[25]</ref>, which is a great tool for adaptively adjusting the learning rate. Negative log-likelihood criterion was used for multi-class classification purposes. We performed hyperparameter optimization with manual grid search in terms of batch size. If there wasn't improvement in the global correct rates in 100 epochs the training was stopped. According to the results that are shown in Table <ref type="table" coords="4,337.44,541.62,3.93,9.40" target="#tab_0">1</ref>, 130 was chosen as the batch size. The loss and the average correct classification rates within rows of the confusion matrix are shown in Figure <ref type="figure" coords="4,301.08,565.38,3.87,9.40" target="#fig_2">3</ref>.</p><p>The hardware we used for training were a NVidia GTX 970 (4 GB) and a NVidia Titan X (12 GB) GPU cards hosted in two i7 servers with 32 GB RAM. Ubuntu 14.04 with Cuda 7.5 and cuDNN 4.0 was used as general software architecture. For data preparation, training and evaluating deep neural networks the Torch7 <ref type="bibr" coords="4,245.28,624.90,17.37,9.40" target="#b25">[26]</ref> deep learning framework was used. For calculat-. . .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Combination of the classifiers</head><p>We combined the outputs of the classifiers. Based on our preliminary testing the weighted average of the outputs were used for creating the 'BME TMIT Run4'. Besides two classifiers described above a third one was constructed using metadata. After normalizing and cleaning the data we calculated new metadatas (e.g. the season) in order to get more informative variables. Next we applied Random Forest (RF), and we measured the accuracy. The metadata based classification had the lowest MAP value, SVM and CNN gave much better results. Therefore, we chose the following weight parameters: 0.1 for metadata, 0.3 for SVM and 0.6 for CNN.</p><p>Furthermore, we built an aggregated model to detect unseen categories by attempting to filter out the images with unknown classes. We measured the largest distance among fisher vectors of training images; in the case if a test image's fisher vector is farther from all training fisher vectors then this distance, we reject that particular image (as outlier). As well as, images with very low overall (0.3) decision values were also rejected. As a result, only the remaining test images were included in 'BME TMIT Run3'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation</head><p>We trained 1 CNN and 7 SVM classifiers (one for each viewpoint) and we conducted a preliminary testing on the PlantCLEF 2015 test data, and measured the MAP (Mean Average Precision) values. The results of the preliminary evaluation can be seen in the first row of Table <ref type="table" coords="6,361.08,337.74,3.87,9.40" target="#tab_1">2</ref>. Some of the test images had no viewpoint attribute at all, and some of them were marked as 'Other'. Therefore, an image with known viewpoint was classified with the appropriate SVM classifier (with same viewpoint), and decision values of this classifier were used in the runfile as predictions. For testing an image with unknown viewpoint we constructed a classifier using the weighted average of the decision values coming from all trained classifiers. At the estimation of weight parameters we took the "goodness" of different viewpoint classifiers into consideration: LeafScan: 0.3, Leaf: 0.15, Flower: 0.15, Fruit: 0.15, Stem: 0.15, Branch: 0.05, Entire: 0.05.</p><p>In Table <ref type="table" coords="6,217.20,456.54,3.87,9.40" target="#tab_1">2</ref>, we presented multiple MAP (Mean Average Precision) values based on the size of the sorted lists associated with the classes. The results of the post-testing (i.e. test was after the run submission) of SVM and CNN show that the best MAP value is reached when only the 10 most probable predictions are involved in the calculations. Unfortunately, we analyzed the influence of MAP calculations (second and third rows of this table belong to post-testing) only after the official results were released. For the competition we submitted predictions of all the 1000 classes for every test imagethat caused worse MAP value.</p><p>The evaluation was executed on the PlantCLEF 2015 test data, and we calculated the predictions on 2016 test data. It is important to note that these runfiles contained the total results, which means that we gave all decision values for each ClassId for each MediaId. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Official results</head><p>In the official evaluation MAP was used for measurement of goodness of the image classification, considering each class Ci of the training set as a query.</p><p>In this query evaluation all predictions with ClassId=Ci in the runfile were extracted, and ranked by decreasing probability and the average precision (AP) was computed for that class. The MAP is mean of these AP values. To evaluate more specifically the targeted usage scenario consisting in detecting invasive species, a secondary MAP was computed by considering as queries only a subset of the species that belongs to a blacklist of invasive species.</p><p>Recognition system was expected to be robust to unseen categories by automatically detecting the numerous false positives classification hits. The official results can be seen in Table <ref type="table" coords="7,285.72,367.98,3.87,9.40" target="#tab_2">3</ref>, where the first two columns contain the unseen categories, while the last column ignores them. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We elaborated different classification methods for image-based plant identification task. We applied dense SIFT for feature detection and description; and Gaussian Mixture Model based Fisher vector was calculated to represent an image with high-level descriptor. The chosen classifier was the C-support vector classification algorithm with RBF (Radial Basis Function) kernel, and we optimized two hyperparameters (C from C-SVC and γ from RBF kernel) by a grid search with two-dimensional grid. We also used convolutional neural networks for the task. The images were normalized to zero mean and unit variance, they were also cropped and scaled down. We used a modified version of the AlexNet model, and we performed batch size optimization. With the winning batch size the deep learning method achieved considerably higher MAP score than SVM.</p><p>We constructed a classifier by combining the decisions values of the metadata, SVM and CNN classification methods. The weights of these techniques were determined based on the preliminary tests. Furthermore, a novel approach was used for rejecting the outlier test images (i.e. images with unseen categories), this approach used the information coming from both distance measurement of Fisher vectors and CNN.</p><p>It should be noted that our team was formed at the middle of March and we started working on the project in April. According to the investigation of MAP calculations, we must admit that if we would have optimized the length of the sorted lists for the MAP calculations, we might have achieved better results in the official competition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,154.32,223.02,302.83,7.66"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Example of an image before (left) and after cropping and normalization (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,143.28,443.22,324.65,7.66;4,144.12,453.06,323.18,7.66;4,248.28,462.78,114.65,7.66"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The block diagram of the proposed convolutional network, which is a modification of AlexNet [16]. (A@BxB refers to A number of planes with size BxB. The CxC, s: DxE refers to CxC kernel size with DxE stride.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,148.92,503.22,313.70,7.66;5,159.96,512.94,291.55,7.66;5,146.64,374.52,318.36,117.36"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Loss (left) and average correct rows in the confusion matrix (right) during training (blue: train, green: validation). The horizontal axis corresponds the number of epochs.</figDesc><graphic coords="5,146.64,374.52,318.36,117.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,142.98,321.75,323.82,110.12"><head>Table 1 .</head><label>1</label><figDesc>Batch size optimization. The global correct rates correspond to the ratio of the overall correct classification in the confusion matrix. 'Early stopped #epochs' refers to the number of epoch when early stopping was applied.</figDesc><table coords="4,142.98,321.75,323.82,110.12"><row><cell></cell><cell>Batch norm.</cell><cell>Batch norm.</cell><cell></cell><cell>Batch norm.</cell></row><row><cell></cell><cell>Max-pooling</cell><cell>Max-pooling</cell><cell></cell><cell>Max-pooling</cell></row><row><cell></cell><cell>3x3, s:2x2</cell><cell>3x3, s:2x2</cell><cell></cell><cell>3x3, s:2x2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>. . .</cell></row><row><cell>11x11</cell><cell>2</cell><cell></cell><cell></cell></row><row><cell>s:4x4</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>256@27x27</cell><cell>384@13x13</cell><cell>384@13x13</cell><cell>256@13x13</cell></row><row><cell></cell><cell>96@55x55</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>4096 4096</cell></row><row><cell>3@224x224</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,166.32,140.12,278.73,51.37"><head>Table 2 .</head><label>2</label><figDesc>MAP values of the SVM and CNN network with different length of the sorted lists.</figDesc><table coords="7,223.80,155.92,158.45,35.57"><row><cell>Length of the sorted lists</cell><cell>SVM</cell><cell>CNN</cell></row><row><cell>total</cell><cell>0.1365</cell><cell>0.0592</cell></row><row><cell>top 100</cell><cell>0.1774</cell><cell>0.1852</cell></row><row><cell>top 10</cell><cell>0.2062</cell><cell>0.2671</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,158.88,403.04,300.23,109.69"><head>Table 3 .</head><label>3</label><figDesc>Official results of the 4 submitted runs.</figDesc><table coords="7,158.88,419.80,300.23,92.93"><row><cell>Runs</cell><cell>Official score MAP</cell><cell>MAP restricted to a blacklist of (potentially) invasive species</cell><cell>MAP ignoring unknown classes and queries</cell></row><row><cell>BME TMIT Run1</cell><cell>0.169</cell><cell>0.125</cell><cell>0.196</cell></row><row><cell>BME TMIT Run2</cell><cell>0.066</cell><cell>0.128</cell><cell>0.101</cell></row><row><cell>BME TMIT Run3</cell><cell>0.17</cell><cell>0.125</cell><cell>0.197</cell></row><row><cell>BME TMIT Run4</cell><cell>0.174</cell><cell>0.144</cell><cell>0.213</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p><rs type="person">Bálint Pál Tóth</rs> gratefully acknowledges the support of <rs type="funder">NVIDIA Corporation</rs> with the donation of an <rs type="funder">NVidia Titan</rs> X GPU used for his research.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,163.23,475.26,305.08,7.66;8,177.00,485.70,291.17,7.66;8,177.00,495.90,189.53,7.66" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,311.88,485.70,156.29,7.66;8,177.00,495.90,66.96,7.66">LifeCLEF 2016: multimedia life species identification challenges</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Champ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,249.48,495.90,92.42,7.66">Proceedings of CLEF 2016</title>
		<meeting>CLEF 2016</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,163.24,506.34,305.14,7.66;8,177.00,516.66,138.53,7.66" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,295.08,506.34,173.30,7.66;8,177.00,516.66,18.26,7.66">Plant identification in an open-world (LifeCLEF 2016)</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,201.00,516.66,70.85,7.66">CLEF working notes</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">2016</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,163.24,526.98,305.07,7.66;8,177.00,537.30,291.31,7.66;8,177.00,547.74,22.49,7.66" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,370.80,526.98,97.51,7.66;8,177.00,537.30,141.75,7.66">IBM Research Australia at LifeCLEF2014: Plant Identification Task</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Abedini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Garnavi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,336.00,537.30,81.37,7.66">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="693" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,163.24,557.94,304.94,7.66;8,177.00,568.38,291.31,7.66;8,177.00,578.70,291.17,7.66;8,177.00,589.02,129.77,7.66" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,288.36,557.94,179.81,7.66;8,177.00,568.38,119.90,7.66">Viewpoints Combined Classification Method in Image-based Plant Identification Task</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Szűcs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Papp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lovas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,196.20,578.70,145.85,7.66">Working Notes for CLEF 2014 Conference</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Halvey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</editor>
		<meeting><address><addrLine>Sheffield, Great Britain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">September 15-18. 2014</date>
			<biblScope unit="page">1180</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,163.23,599.34,305.06,7.66;8,177.00,609.78,291.26,7.66;8,177.00,619.98,291.31,7.66;8,177.00,630.42,251.81,7.66" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,325.80,599.34,142.49,7.66;8,177.00,609.78,248.97,7.66">Plant Species Recognition using Bag-Of-Words with SVM classifier in the Context of the LifeCLEF Challenge In</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Issolah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lingrand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Precioso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,318.36,619.98,146.22,7.66">Working Notes for CLEF 2014 Conference</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Halvey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</editor>
		<meeting><address><addrLine>Sheffield, Great Britain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">September 15-18. 2014</date>
			<biblScope unit="page">1180</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,163.23,140.94,305.06,7.66;9,177.00,151.26,291.31,7.66;9,177.00,161.70,132.53,7.66" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="9,448.44,140.94,19.85,7.66;9,177.00,151.26,287.62,7.66">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lecun</forename><forename type="middle">Y</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,163.24,171.90,305.06,7.66;9,177.00,182.34,291.14,7.66;9,177.00,192.66,129.41,7.66" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,371.76,171.90,96.53,7.66;9,177.00,182.34,249.89,7.66">Fine-Grained Plant Classification Using Convolutional Neural Networks for Feature Extraction</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Sünderhauf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Mccool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Upcroft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,447.48,182.34,20.66,7.66;9,177.00,192.66,56.19,7.66">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="756" to="762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,163.24,202.98,305.07,7.66;9,177.00,213.30,291.38,7.66;9,177.00,223.74,79.51,7.66" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,230.64,202.98,237.67,7.66;9,177.00,213.30,192.59,7.66">Plant identification with deep convolutional neural network: SNUMedinfo at LifeCLEF plant identification task 2015</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sungbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,385.32,213.30,83.06,7.66;9,177.00,223.74,54.71,7.66">Working notes of CLEF 2015 conference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,163.24,233.94,304.94,7.66;9,177.00,244.38,272.69,7.66" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,345.72,233.94,122.45,7.66;9,177.00,244.38,91.01,7.66">Fine-tuning deep convolutional networks for plant recognition</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">E</forename><surname>Camargo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,282.60,244.38,138.95,7.66">Working notes of CLEF 2015 conference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,167.17,254.70,301.12,7.66;9,177.00,265.02,291.29,7.66;9,177.00,275.34,69.77,7.66" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,323.88,254.70,144.41,7.66;9,177.00,265.02,11.48,7.66">Recognizing and Learning Object Categories</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,194.28,265.02,274.01,7.66;9,177.00,275.34,40.91,7.66">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,167.17,285.78,301.12,7.66;9,177.00,295.98,291.17,7.66;9,177.00,306.42,105.17,7.66" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,390.60,285.78,77.69,7.66;9,177.00,295.98,185.60,7.66">The devil is in the details: an evaluation of recent feature encoding methods</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,369.00,295.98,99.17,7.66;9,177.00,306.42,23.58,7.66">British Machine Vision Conference</title>
		<imprint>
			<biblScope unit="page" from="1" to="76" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,167.17,316.74,300.97,7.66;9,177.00,327.06,291.05,7.66;9,177.00,337.38,291.29,7.66;9,177.00,347.82,43.73,7.66" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,319.44,316.74,148.70,7.66;9,177.00,327.06,179.58,7.66">Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,362.88,327.06,105.17,7.66;9,177.00,337.38,183.07,7.66">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,167.17,358.02,301.12,7.66;9,177.00,368.46,236.33,7.66" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,228.00,358.02,206.26,7.66">Distinctive Image Features from Scale-Invariant Keypoints</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,441.60,358.02,26.69,7.66;9,177.00,368.46,114.63,7.66">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,167.17,378.78,301.12,7.66;9,177.00,389.10,225.29,7.66" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,264.00,378.78,103.81,7.66">Principal Component Analysis</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Abdi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,374.16,378.78,94.13,7.66;9,177.00,389.10,106.41,7.66">Wiley Interdisciplinary Reviews: Computational Statistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="433" to="459" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,167.17,399.42,301.00,7.66;9,177.00,409.86,291.29,7.66;9,177.00,420.06,280.49,7.66" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="9,268.20,399.42,199.97,7.66;9,177.00,409.86,50.32,7.66">PCA-SIFT: A more distinctive representation for local image descriptors</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,242.76,409.86,225.53,7.66;9,177.00,420.06,188.85,7.66">CVPR 2004. Proceedings of the 2004 IEEE Computer Society Conference on</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">506</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct coords="9,167.17,430.50,301.02,7.66;9,177.00,440.82,137.93,7.66" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="9,235.80,430.50,228.80,7.66">Gaussian Mixture Models, Encyclopedia of Biometric Recognition</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009-02">February. 2009</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="659" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,167.17,451.14,301.26,7.66;9,177.00,461.46,250.49,7.66" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="9,218.76,451.14,203.53,7.66">Estimating gaussian mixture densities with EM: A tutorial</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,240.48,461.46,122.04,7.66">Chinese Journal of Electron Devices</title>
		<imprint>
			<biblScope unit="page" from="15" to="18" />
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>Tech. rep., Duke University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="9,167.17,471.90,301.12,7.66;9,177.00,482.10,291.17,7.66;9,177.00,492.54,69.77,7.66" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="9,266.52,471.90,201.77,7.66;9,177.00,482.10,12.15,7.66">Fisher kernel on visual vocabularies for image categorization</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,195.00,482.10,273.17,7.66;9,177.00,492.54,40.91,7.66">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,167.17,502.86,301.12,7.66;9,177.00,513.18,291.31,7.66;9,177.00,523.50,54.89,7.66" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="9,292.56,502.86,175.73,7.66;9,177.00,513.18,5.81,7.66">A Training Algorithm for Optimal Margin Classifier</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,188.28,513.18,262.93,7.66">Proc. of the 5th Annual ACM Workshop on Computational Learning Theory</title>
		<meeting>of the 5th Annual ACM Workshop on Computational Learning Theory</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="144" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,167.17,533.94,301.14,7.66;9,177.00,544.14,67.61,7.66" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="9,259.44,533.94,82.90,7.66">Support-vector networks</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,348.60,533.94,61.14,7.66">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,167.17,554.58,301.12,7.66;9,177.00,564.90,291.39,7.66;9,177.00,575.22,291.29,7.66;9,177.00,585.54,39.41,7.66" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="9,396.48,564.90,71.91,7.66;9,177.00,575.22,95.03,7.66">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Hunag</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,278.28,575.22,140.91,7.66">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,167.17,595.98,301.12,7.66;9,177.00,606.18,291.19,7.66;9,177.00,616.62,76.13,7.66" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="9,337.08,595.98,131.21,7.66;9,177.00,606.18,91.56,7.66">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,285.72,606.18,178.85,7.66">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,167.17,140.94,301.01,7.66;10,177.00,151.26,291.26,7.66;10,177.00,161.70,22.49,7.66" xml:id="b22">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.01852</idno>
		<title level="m" coord="10,305.52,140.94,162.65,7.66;10,177.00,151.26,163.99,7.66">Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,167.17,171.90,301.09,7.66;10,177.00,182.34,262.73,7.66" xml:id="b23">
	<monogr>
		<title level="m" type="main" coord="10,271.60,171.90,196.66,7.66;10,177.00,182.34,118.16,7.66">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,167.17,192.66,301.14,7.66;10,177.00,202.98,82.13,7.66" xml:id="b24">
	<monogr>
		<title level="m" type="main" coord="10,235.95,192.66,169.56,7.66">ADADELTA: an adaptive learning rate method</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,167.17,213.30,301.00,7.66;10,177.00,223.74,286.13,7.66" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="10,335.76,213.30,132.41,7.66;10,177.00,223.74,57.22,7.66">Torch7: A matlab-like environment for machine learning</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<idno>EPFL-CONF-192376</idno>
	</analytic>
	<monogr>
		<title level="m" coord="10,248.76,223.74,91.94,7.66">BigLearn, NIPS Workshop</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
