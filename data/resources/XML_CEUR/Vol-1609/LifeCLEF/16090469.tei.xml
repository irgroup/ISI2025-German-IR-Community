<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,145.12,115.96,325.11,12.62;1,196.06,133.89,223.25,12.62">Unsupervised individual whales identification: spot the difference in the ocean</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,134.77,171.81,47.81,8.74"><forename type="first">Alexis</forename><surname>Joly</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria ZENITH team</orgName>
								<orgName type="institution" key="instit2">LIRMM</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,192.87,171.81,118.29,8.74"><forename type="first">Jean-Christophe</forename><surname>Lombardo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria ZENITH team</orgName>
								<orgName type="institution" key="instit2">LIRMM</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,321.45,171.81,60.75,8.74"><forename type="first">Julien</forename><surname>Champ</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria ZENITH team</orgName>
								<orgName type="institution" key="instit2">LIRMM</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,411.60,171.81,64.52,8.74"><forename type="first">Anjara</forename><surname>Saloma</surname></persName>
							<email>anjara@cetamada.com</email>
							<affiliation key="aff2">
								<orgName type="department">Cetamada</orgName>
								<orgName type="institution">NGO</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">INA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,145.12,115.96,325.11,12.62;1,196.06,133.89,223.25,12.62">Unsupervised individual whales identification: spot the difference in the ocean</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0E1003E88868887B4851EB83EA0EF9A7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Identifying organisms is a key step in accessing information related to the ecology of species. But unfortunately, this is difficult to achieve due to the level of expertise necessary to correctly identify and record living organisms. To try bridging this gap, enormous work has been done on the development of automated species identification tools such as image-based plant identification or audio recordings-based bird identification. Yet, for some groups, it is preferable to monitor the organisms at the individual level rather than at the species level. The automatizing of this problem has received much less attention than species identification. In this paper, we address the specific scenario of discovering humpack whales individuals in a large collections of pictures collected by nature observers. The process is initiated from scratch, without any knowledge on the number of individuals and without any training samples of these individuals. Thus, the problem is entirely unsupervised. To address it, we set up and experimented a scalable fine-grained matching system allowing to discover small rigid visual patterns in highly clutter background. The evaluation was conducted in blind in the context of the LifeCLEF evaluation campaign. Results show that the proposed system provides very promising results with regard to the difficulty of the task but that there is still room for improvements to reach higher recall and precision in the future.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Identifying organisms is a key step in accessing information related to the ecology of species. This is an essential step in recording any specimen on earth to be used in ecological studies. But unfortunately, this is difficult to achieve due to the level of expertise necessary to correctly identify and record living organisms. Watson et al. <ref type="bibr" coords="1,234.81,584.39,10.52,8.74" target="#b5">[6]</ref> discussed in 2004 the potential of automated species identification approaches typically based on machine learning and multimedia data analysis methods. They suggested that, if the scientific community is able to (i) overcome the production of large training datasets, (ii) more precisely identify and evaluate the error rates, (iii) scale up automated approaches, and (iv) detect novel species, it will then be possible to initiate the development of a generic automated species identification system that could open up vistas of new opportunities for pure and applied work in biological and related fields. Since the question raised by Watson in 2004 ("automated species identification: why not?"), enormous work has been done on the development of effective methods such as image-based plant identification <ref type="bibr" coords="2,307.94,154.86,10.79,8.74" target="#b1">[2,</ref><ref type="bibr" coords="2,318.73,154.86,7.20,8.74" target="#b7">8,</ref><ref type="bibr" coords="2,325.93,154.86,7.20,8.74" target="#b6">7]</ref>, bird songs identification <ref type="bibr" coords="2,444.77,154.86,14.61,8.74" target="#b21">[22]</ref>, fish species identification <ref type="bibr" coords="2,227.86,166.81,14.61,8.74" target="#b19">[20]</ref>, etc. The problem of automatically identifying individual organisms rather than species has received much less attention (except for humans of course). Yet, for some groups, it is preferable to monitor the organisms at the individual level rather than at the species level. This is notably the case of big animals, such as whales and elephants, whose population are scarcer and who are traveling longer distances. Monitoring individual animals allow gathering valuable information about population sizes, migration, health, sexual maturity and behavior patterns. Tracking devices and tagging technologies are only part of the solution because of their invasive character, relatively high cost and limited lifetime. Morphological/biometric approaches are a complementary approach that is less invasive, more durable and cheaper for nature observers mobilized on a given spot. Using natural markings to identify individual animals over time is usually known as photo-identification. This research technique is used on many species of marine mammals. Initially, scientists used artificial tags to identify individual whales, but with limited success (most tagged whales were actually lost or died). In the 1970s, scientists discovered that individuals of many species could be recognized by their natural markings. These scientists began taking photographs of individual animals and comparing these photos against each other to identify individual animal's movements and behavior over time. Since its development, photo-identification has proven to be a useful tool for learning about many marine mammal species including humpbacks, right whales, finbacks, killer whales, sperm whales, bottlenose dolphins and other species to a lesser degree. Nowadays, this process is still mostly done manually making it impossible to get an accurate count of all the individuals in a given large collection of observations. Researchers usually survey a portion of the population, and then use statistical formulae to determine population estimates. To limit the variance and bias of such an estimator, it is however required to use large-enough samples which still makes it a very time-consuming process. Automating the photo-identification process could drastically scale-up such surveys and open brave new research opportunities for the future. In this paper, we address more particularly the problem of discovering all individual humpack whales appearing in a large collection of caudal's images in a fully unsupervised way, i.e. without any knowledge on the number of individuals and without any training samples of these individuals. This is in essence a different and more challenging problem than the supervised recognition of individual whales such as the challenge proposed by NOAA Fisheries through the Kaggle platform <ref type="foot" coords="2,172.15,607.58,3.97,6.12" target="#foot_0">4</ref> . Such supervised scenario is actually only affordable when the individuals are already well known and well illustrated by tens of pictures that were hardly collected along the years. On the other side, the unsupervised identifica-tion scenario targeted in this paper has the great advantage to allow the use of unlabeled or very labeled collections of observations which is the vast majority of available data today. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data and challenge</head><p>The experiment reported in this paper was part of the 2016-th edition of the the LifeCLEF international evaluation campaign <ref type="bibr" coords="3,349.01,493.15,15.50,8.74" target="#b13">[14]</ref> (in particular in the scope of the sea organisms identification task). The data shared through this challenge consisted of 2005 images of humbpack whales caudals collected by the CetaMada 5 NGO between 2009 and 2014 in Madagascar area. Cetamada is a Malagasy Non-Profit Association created in May, 2009 whose goal is to protect marine mammal population and their habitat in Madagascar through sustainable eco-tourism and scientific research. There are presently 4 citizen sciences data collection sites (St. Marys, Majunga, Ifaty and Fort Dauphin) for which hotel-establishements and their customers have become sentinels for data collection. This method helps obtain more than 250 photo IDs each year, which effectively helps produce a photo catalogue of humpback whales reproducing on Malagasy coasts. After acquisition, each photograph was manually cropped so as to focus only on the caudal fin that is the most discriminant pattern for distinguishing an individual whale from another. Figure <ref type="figure" coords="4,289.75,130.95,4.98,8.74" target="#fig_0">1</ref> displays six of such cropped images, each line corresponding to two images of the same individual. As one can see, the individual whales can be distinguished thanks to their natural markings and/or the scars that appear along the years. Automatically finding such matches in the whole dataset and rejecting the false alarms is difficult for three main reasons. The first reason is that the number of individuals in the dataset is high, around 1, 200, so that the proportion of true matches is actually very low (around 0.05% of the total number of potential matches). The second difficulty is that distinct individuals can be very similar at a first glance as illustrated by the false positive examples displayed in Figure <ref type="figure" coords="4,261.65,238.55,3.87,8.74" target="#fig_1">2</ref>. To discriminate the true matches from such false positives, it is required to detect very small and fine-grained visual variations such as in a spot-the-difference game. The third difficulty is that all images have a similar water background of which the texture generates quantities of local mismatches. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method Description</head><p>To differentiate biomarkers from the mass of other visual patterns without any supervision, the research line we investigate in this work is to rely on the spatial filtering of low-level visual correspondences. Our hypothesis is that the biomarkers are sufficiently localized on the fin to be considered as non deformable objects so that two views of the same biomarker in two different images are supposed to be related by epipolar geometry. On the other side, the raw noisy visual correspondences at the origin of false alarms should be filtered by the use of geometric rules. The standard solution to perform such epipolar geometry estimation is to use the RANSAC algorithm <ref type="bibr" coords="5,266.13,190.72,9.96,8.74" target="#b4">[5]</ref>; it consists in generating transformation hypotheses using a minimal number of low-level visual correspondences and then evaluating each hypothesis based on the number of inliers among all features under that hypothesis. The main advantage of the RANSAC algorithm is that it is robust to the presence of a high number of outliers which makes it suitable to deal with the large numbers of false alarms that are generated by the raw visual matching of the local features. As the RANSAC algorithm can be rather slow, an efficient variant, LO-RANSAC, was proposed by Chum et al. <ref type="bibr" coords="5,259.70,286.37,10.52,8.74" target="#b2">[3]</ref> and has been proved to provide consistent speedups in many image retrieval frameworks <ref type="bibr" coords="5,314.07,298.32,15.84,8.74" target="#b18">[19,</ref><ref type="bibr" coords="5,329.91,298.32,11.88,8.74" target="#b17">18,</ref><ref type="bibr" coords="5,341.79,298.32,11.88,8.74" target="#b16">17,</ref><ref type="bibr" coords="5,353.67,298.32,7.92,8.74" target="#b0">1]</ref>. It involves generating hypotheses of an approximate model thanks to the shape information provided with the affine-invariant image regions from which the visual features were extracted. With this method, an hypothesis can be generated with only a single pair of corresponding features whereas two or three are required when using only the feature positions. This greatly reduces the number of possible hypotheses which need to be considered by the RANSAC algorithm and significantly speeds up the spatial verification procedure. An even faster strategy <ref type="bibr" coords="5,440.26,382.01,14.61,8.74" target="#b9">[10]</ref>, consists in considering only the shape information of the image regions, without exploiting the positions of the features at all. A rough approximation of the best transformation can then actually be estimated by a Hough-like voting strategy on the quantized differences of the characteristic orientation and scale of each visual correspondence. Using this so-called weak geometry method allows trading quality for time and is the only acceptable solution when dealing with huge image sets and real-time contexts (e.g. a search engine working on billions of images). The spatial verification we use in our own system is also a variant of the RANSAC algorithm making use of weak geometry rules generated from the region shape characteristics. We however do not use the weak geometry to directly generate an hypothesis from a single visual correspondence. We rather use it to filter the exact hypothesis generated by the classical RANSAC algorithm. Concretely, if we restrict our class of transformations to rotation and scaling, the RANSAC algorithm can generate an hypothesis from any pair of visual correspondences. To quickly decide whether this hypothesis is relevant or not, we check its consistency with regard to the two approximate hypothesis generated from the shape characteristics of each visual correspondence. If any of the two approximate models does not fit the RANSAC hypothesis, we reject that solution without computing the costly consensus phase. In practice, up to 99% of the RANSAC hypothesis can be rejected in that way (leading to a consistent speed-up).</p><p>Another major difference between our method and the ones in <ref type="bibr" coords="6,431.89,118.99,16.24,8.74" target="#b18">[19,</ref><ref type="bibr" coords="6,448.12,118.99,12.18,8.74" target="#b17">18,</ref><ref type="bibr" coords="6,460.30,118.99,12.18,8.74" target="#b16">17,</ref><ref type="bibr" coords="6,472.48,118.99,8.12,8.74" target="#b0">1]</ref> is that we use the ranking of the visual correspondences to further improve the matching. Our retrieval framework does actually not rely on the popular bag-ofwords model to generate the raw visual correspondences but on a more accurate approximate KNN search algorithm (described in section 4). The main benefit is that the precision of our raw visual matches is already much better than the ones produced by the bag-of-words model (based on vector quantization). The RANSAC algorithm therefore works on less correspondences and less false alarms. Another benefit is that each raw visual correspondence {x, y} is associated with a rank r x (y)). This allows two things: (i) to restrict the generation of the hypothesis of the RANSAC algorithm to the best match of each feature x in the transformed image I Y . The number of evaluated hypothesis is consequently reduced, particularly in the presence of numerous repeated visual patterns (the burstiness phenomenon <ref type="bibr" coords="6,240.20,274.41,10.79,8.74" target="#b8">[9]</ref>) (ii) the ranking can be used in the computation of the final score by weighing the contribution of each inlier according to its rank in the whole dataset. Closest points are then favored to the detriment of the farthest ones, independently from the feature space density in the neighborhood of x q . More formally, given a couple of images I X and I Y , represented by sets of local features X and Y , we define the following spatially consistent match kernel:</p><formula xml:id="formula_0" coords="6,231.56,353.56,249.03,22.31">K(I X , I Y ) = 1 2 (S X (I Y ) + S Y (I X ))<label>(1)</label></formula><formula xml:id="formula_1" coords="6,219.63,388.59,260.96,20.09">S X (I Y ) = x∈X max y∈Y [δ X,Y (x, y).ϕ(r x (y))]<label>(2)</label></formula><p>where r x (y) : R d → N + is a ranking function that returns the rank of the local feature y according to its L 2 -distance to the query feature x (within the whole dataset). The function ϕ() is a decreasing function allowing to give more weights to the top ranked features (we used the inverse function in our experiments). Finally, δ X,Y (x, y) is an indicator function equal to one if the correspondence (x, y) is an inlier of the geometric model estimating the transformation between I X and I Y , i.e.:</p><formula xml:id="formula_2" coords="6,220.44,521.08,260.15,12.09">δ X,Y (x, y) = P x -( ÂP y + B) &lt; θ<label>(3)</label></formula><p>where ( Â, B) are the parameters of the best transformation estimated by our accelerated RANSAC algorithm, P x and P y are the spatial positions of x and y, θ is a user defined threshold defining the spatial tolerance of the inliers (we used θ = 16 pixels in our experiments). The number of probes of the RANSAC algorithm was set to 10K in all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Approximate K-NN search scheme</head><p>In practice, to speed up the computation of the matching, the ranking function r x (y) : R d → N + is implemented as an approximate nearest neighbors search algorithm based on hashing and probabilistic accesses in the hash table. It takes as input a query feature x and an inverted index of all local features z ∈ Z extracted from the image collection. It returns a set of m approximated neighbors with an approximated rank r m x (y) (we used m = 500 in all our experiments). The exact ranking function r x (y) is simply replaced by this approximated ranking function in all equations above. Note that the features y that are not returned in the top-m approximated nearest neighbors are simply removed from the match kernel equations conducting to a considerable reduction of the computation time. Consequently, they are implicitly considered as having a rank-based activation function ϕ(r x (y)) equal to zero which is a good approximation as their rank is supposed to be higher than m. The higher the value of m is and the lower the error compared to the exact match kernel. Let us now describe more precisely our approximate nearest neighbors indexing and search method. It first compresses the original feature vectors z ∈ Z into compact binary hash codes h(z) of length b thanks to the use of a data-dependent high-dimensional hash function. In our experiments, we used RMMH hash function <ref type="bibr" coords="7,154.91,310.28,15.50,8.74" target="#b12">[13]</ref> that has the advantage to be easily implemented and to be effective for any kind of visual features or data distribution. The distance between any two features x and z can then be efficiently approximated by the Hamming distance between h(x) and h(y). In all our experiments we used b = 128 bits. To avoid scanning the whole dataset, the hash codes h(z) derived from the local features of the entire set Z are then indexed in a hash table whose keys are the t-length prefix of the hash codes h(z). At search time, the hash code h(x) of a query feature x is computed as well as its t-length prefix. We then use a probabilistic multi-probe search algorithm inspired by the one of <ref type="bibr" coords="7,406.47,405.92,15.50,8.74" target="#b10">[11]</ref> to select the buckets of the hash table that are the most likely to contain exact nearest neighbors. This is done by using a probabilistic search model that is trained offline on the exact m-nearest neighbors of M sampled features z ∈ Z. We however use a simpler search model than the one of <ref type="bibr" coords="7,296.38,453.74,14.61,8.74" target="#b10">[11]</ref>. We actually use a normal distribution with independent components parameterized by a single vector σ that is trained over the exact nearest neighbors of the training samples. At search time, we also use a slightly different probabilistic multi-probe algorithm trading stability for time. Instead of probing the buckets by decreasing probabilities, we rather use a greedy algorithm that computes the probability of neighboring buckets and select only the ones having a probability greater than a threshold ζ that is fixed over all queries. The value of ζ is trained offline on M training samples and their exact nearest neighbors so as to reach on average cumulative probability α over the visited buckets. In our experiments, we always used α = 0.95 meaning that on average we retrieve 95% of the exact nearest neighbors in the original feature space. Once the most probable buckets have been selected, the refinement step computes the Hamming distance between h(x) and the h(z)'s belonging to the selected buckets and keep only the top-m matches thanks to a max heap.</p><p>As mentioned earlier, the system described above was evaluated in the context of the Sea task of the LifeCLEF 2016 evaluation campaign <ref type="bibr" coords="8,388.96,154.41,14.61,8.74" target="#b13">[14]</ref>. This means that the experiment was conducted in blind, i.e. without having access to the ground truth, and thus, without any possibility of learning or tuning the parameters of the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Task Description</head><p>The task was simply to detect as many true matches as possible from the whole dataset, in a fully unsupervised way. Each evaluated system had to return a run file (i.e., a raw text file) containing as much lines as the number of discovered matches, each match being a triplet of the form:</p><formula xml:id="formula_3" coords="8,244.20,384.72,125.75,25.41">AveP = K k=1 P (k) × rel(k) M</formula><p>where M is the total number of true matches in the groundtruth, k is the rank in the sequence of returned matches, K is the number of retrieved matches, P (k) is the precision at cut-off k in the list, and rel(k) is an indicator function equaling 1 if the match at rank k is a relevant match, 0 otherwise. The average is over all true matches and the true matches not retrieved get a precision score of 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Submitted runs</head><p>We submitted a total of 3 run files to the LifeCLEF benchmark corresponding to three configurations of our system. Each run was computed by (i) searching each image of the collection one by one, (ii) computing the score K of the image pairs according to Equation 1 and (iii) rank all pairs by decreasing value of K.</p><p>-Run ZenithINRIA SiftGeo: In this run we used SIFT local features <ref type="bibr" coords="8,448.69,572.54,15.50,8.74" target="#b14">[15]</ref> extracted around Harris Hessian regions <ref type="bibr" coords="8,320.32,584.50,15.50,8.74" target="#b15">[16]</ref> (without threshold). -Run ZenithINRIA GoogleNet 3layers borda: In this run we used off-the-shelf local features extracted at three different layers of GoogLeNet convolutional neural network <ref type="bibr" coords="8,221.99,620.25,15.50,8.74" target="#b20">[21]</ref> (layer conv2-3x3 : 3136 local features per image, layer inception 3b output: 784 local features par image, layer inception 4c output: 196 local features per image). The matches found using the 3 distinct layers were merged through a late-fusion approach based on Borda.</p><p>-Run ZenithINRIA SiftGeo QueryExpansion: This the last run differs from the run ZenithINRIA SiftGeo in that a query expansion strategy was used to re-issue the regions matched with a sufficient degree of confidence as new queries (using the method described in <ref type="bibr" coords="9,324.33,154.86,14.76,8.74" target="#b11">[12]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Official LifeCLEF results</head><p>The table in Figure <ref type="figure" coords="9,225.22,214.85,4.98,8.74" target="#fig_2">3</ref> provides the scores achieved by the three configurations of our system as well as the scores obtained by the system of the other competitor and described in <ref type="bibr" coords="9,228.74,238.76,10.52,8.74" target="#b3">[4]</ref> (using a Fisher Vector image representation based on SIFT features and a GMM visual codebook of 256 visual words). Runs bmetmit whalerun 2 and bmetmit whalerun 3 differ from bmetmit whalerun 1 in that segmentation propagation was used beforehand so as to separate the background (the water) from the whales caudal fin. The main conclusion we can draw from the results of this evaluation is that the spatial arrangement of the local features is a crucial information for rejecting the false positives (as proved by the much higher Average Precision of our system compared to the one of bme mit). As powerful as aggregation-based methods such as Fisher Vectors are for fine-grained classification, they do not capture the spatial arrangement of the local features which is a precious information for rejecting the mismatches without supervision. Another reason explaining the good performance of the best run ZenithINRIA SiftGeo is that it is based on affine invariant local features contrary to ZenithINRIA GoogleNet 3layers borda and bme mit runs that use grid-based local features. Such features are more sensitive to small shifts and local affine deformations even when learned through a powerful CNN such in our run ZenithINRIA GoogleNet 3layers borda. The comparison of our two runs ZenithINRIA SiftGeo and ZenithINRIA SiftGeo QueryExpansion show that query expansion did not succeeded in improving the results. Query expansion is actually a risky solution in that it is highly sensitive to the decision threshold used for selecting the re-issued matched regions. It can be considerably increase recall when the decision threshold is well estimated but at the opposite, it can also boost the false positives when the threshold is too low.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Additional results</head><p>To get a more practical understanding of the performance achieved by our system, Figure <ref type="figure" coords="10,189.00,173.62,4.98,8.74" target="#fig_3">4</ref> plots the recall-precision curve of our best run. It shows that the main strength of our system is that it has a very high precision on the best found matches (thanks to the spatial filtering). Actually, the top-100 matches were found with a perfect precision of 1.00 which makes our system already usable for automatically discovering some matches without any human control. On the other side, our system fails in reaching high recall values automatically. It would require further human validation to reach reasonable recalls in the range of 30 -80%. But still, this would be a much more easier process than discovering the matches from scratch. Table <ref type="table" coords="10,177.04,560.48,4.98,8.74" target="#tab_0">1</ref> provides the search processing time of each run. It shows that using the Affine SIFT features or the off-the-shelf CNN features requires an equivalent amount of time. The total search time for discovering all the matches in the image collection was about 24 hours. This is yet not negligible but definitely acceptable compared to the difficulty of doing that manually. One should also notice that we used a very high quality approximate nearest neighbors search (alpha = 95%) to favor quality over time. Much more faster runs could be obtained using moderate values of alpha (e.g. 80%) without degrading much the results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we addressed the problem of identifying humpack whales individuals in a large collections of aerial pictures of caudal fins in a fully unsupervised way. We therefore designed a scalable fine-grained matching system allowing to discover small rigid visual patterns in highly clutter background. It was experimented in the context of a blind system-oriented evaluation in which it performed the best. The comparison to the other evaluated system show that the spatial arrangement of the local features is a crucial information to discriminate the individual whales as well as to filter the potentially huge number of false positive matches. Overall, the Average Precision of our system is about 49. This is still not satisfactory for a fully automatic detection scenario but, on the other side, this might already drastically simplify the manual work of the biologists through the release of interactive validation tools. In further work, we will attempt to use localized spatially consistent similarities rather than estimating a global affine transformation at the image level. Also, we will explore possible extensions of convolutional auto-encoders as a way to discover the semi-deformable bio-markers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,134.77,415.98,345.82,7.89;3,134.77,426.96,26.63,7.86;3,185.10,184.80,242.07,216.40"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Three good matches (each line corresponds to 2 images of the same individual whale)</figDesc><graphic coords="3,185.10,184.80,242.07,216.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,140.56,592.12,334.23,7.89;4,185.10,315.91,242.08,261.44"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Three false positives (each line corresponds to 2 distinct individual whales)</figDesc><graphic coords="4,185.10,315.91,242.08,261.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,152.09,417.61,311.17,7.89;9,141.88,320.59,328.53,82.26"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Individual whale identification results: AP of the 6 evaluated systems</figDesc><graphic coords="9,141.88,320.59,328.53,82.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="10,153.46,515.16,308.44,7.89;10,159.17,319.79,293.95,180.60"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Recall-Precision curve of our best system (Affine SIFT + Geometry)</figDesc><graphic coords="10,159.17,319.79,293.95,180.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="11,145.89,115.95,317.44,61.13"><head>Table 1 :</head><label>1</label><figDesc>Search time of three configurations of our system</figDesc><table coords="11,145.89,135.11,317.44,41.96"><row><cell>Run name</cell><cell cols="2">Total time Avg time per query image</cell></row><row><cell>ZenithINRIA SiftGeo</cell><cell>86 415s</cell><cell>43.1s</cell></row><row><cell>ZenithINRIA GoogleNet 3layers borda</cell><cell>67 969s</cell><cell>33.9s</cell></row><row><cell cols="2">ZenithINRIA SiftGeo QueryExpansion 31 494s</cell><cell>119.0s</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="2,144.73,657.44,250.98,7.47"><p>https://www.kaggle.com/c/noaa-right-whale-recognition</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="3,144.73,657.44,117.68,7.47"><p>https://www.cetamada.org/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,142.96,460.71,337.64,7.86;11,151.52,471.67,160.44,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,287.18,460.71,193.41,7.86;11,151.52,471.67,59.97,7.86">Three things everyone should know to improve object retrieval</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,232.21,471.67,51.09,7.86">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,482.46,337.63,7.86;11,151.52,493.42,318.02,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,408.54,482.46,72.05,7.86;11,151.52,493.42,118.69,7.86">Fractal dimension applied to plant identification</title>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">M</forename><surname>Bruno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>De Oliveira Plotze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Falvo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Castro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,276.96,493.42,83.53,7.86">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">178</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2722" to="2733" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,504.21,337.63,7.86;11,151.52,515.17,242.31,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,299.98,504.21,180.61,7.86;11,151.52,515.17,33.22,7.86">Enhancing ransac by generalized model optimization</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Obdrzalek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,206.04,515.17,80.28,7.86">Proc. of the ACCV</title>
		<meeting>of the ACCV</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="812" to="817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,525.96,337.64,7.86;11,151.52,536.92,322.95,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,275.47,525.96,205.13,7.86;11,151.52,536.92,176.75,7.86">Object detection, classification, tracking and individual recognition for sea images and videos</title>
		<author>
			<persName coords=""><forename type="first">Dävid</forename><surname>Papp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">L</forename><surname>Szücs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,349.34,536.92,96.45,7.86">Working notes of CLEF</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,547.71,337.64,7.86;11,151.52,558.67,329.07,7.86;11,151.52,569.63,112.37,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,272.60,547.71,208.00,7.86;11,151.52,558.67,282.62,7.86">Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,441.43,558.67,39.16,7.86;11,151.52,569.63,21.75,7.86">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,580.42,337.63,7.86;11,151.52,591.38,329.07,7.86;11,151.52,602.34,60.92,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,265.15,580.42,170.59,7.86">Automated species identification: why not?</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">J</forename><surname>Gaston</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>O'neill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,437.92,580.42,42.67,7.86;11,151.52,591.38,284.26,7.86">Philosophical Transactions of the Royal Society of London B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">359</biblScope>
			<biblScope unit="issue">1444</biblScope>
			<biblScope unit="page" from="655" to="667" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,613.13,337.64,7.86;11,151.52,624.09,271.28,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,173.03,624.09,175.40,7.86">The imageclef 2013 plant identification task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Bakic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Barthélémy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Molino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,369.42,624.09,24.70,7.86">CLEF</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,634.88,337.63,7.86;11,151.52,645.84,329.07,7.86;11,151.52,656.80,119.92,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,251.85,634.88,187.23,7.86">Leaf shape identification based plant biometrics</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Amin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,458.57,634.88,22.02,7.86;11,151.52,645.84,176.37,7.86">Computer and Information Technology (ICCIT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010. 2010</date>
			<biblScope unit="page" from="458" to="463" />
		</imprint>
	</monogr>
	<note>13th International Conference on</note>
</biblStruct>

<biblStruct coords="12,142.96,119.67,337.63,7.86;12,151.52,130.63,329.07,7.86;12,151.52,141.59,113.78,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,292.73,119.67,144.51,7.86">On the burstiness of visual elements</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,458.57,119.67,22.02,7.86;12,151.52,130.63,154.11,7.86;12,338.62,130.63,125.78,7.86">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="1169" to="1176" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference</note>
</biblStruct>

<biblStruct coords="12,142.62,152.55,337.98,7.86;12,151.52,163.51,289.33,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,291.98,152.55,188.61,7.86;12,151.52,163.51,23.54,7.86">Improving bag-of-features for large scale image search</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,182.06,163.51,168.17,7.86">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="316" to="336" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,174.47,337.98,7.86;12,151.52,185.43,329.07,7.86;12,151.52,196.39,50.43,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,240.86,174.47,200.66,7.86">A posteriori multi-probe locality sensitive hashing</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Buisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,463.04,174.47,17.56,7.86;12,151.52,185.43,269.74,7.86">Proceedings of the 16th ACM international conference on Multimedia</title>
		<meeting>the 16th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="209" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,207.34,337.97,7.86;12,151.52,218.30,329.07,7.86;12,151.52,229.26,50.43,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,242.13,207.34,219.73,7.86">Logo retrieval with a contrario visual query expansion</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Buisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,151.52,218.30,272.69,7.86">Proceedings of the 17th ACM international conference on Multimedia</title>
		<meeting>the 17th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="581" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,240.22,337.98,7.86;12,151.52,251.18,329.07,7.86;12,151.52,262.14,25.60,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,244.24,240.22,143.26,7.86">Random maximum margin hashing</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Buisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,410.71,240.22,69.89,7.86;12,151.52,251.18,134.14,7.86">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="873" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,273.10,337.98,7.86;12,151.52,284.06,329.07,7.86;12,151.52,295.02,329.07,7.86;12,151.52,305.98,258.09,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,289.22,295.02,167.04,7.86">Henning booktitle=Proceedings of CLEF</title>
		<author>
			<persName coords=""><forename type="first">Alexis</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hervé</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hervé</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Concetto</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Willem-Pier And</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julien</forename><surname>Champ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Simone</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Müller</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,163.81,305.98,245.80,7.86">Lifeclef 2016: multimedia life species identification challenges</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,316.93,337.98,7.86;12,151.52,327.89,329.07,7.86;12,151.52,338.85,134.91,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,203.60,316.93,214.76,7.86">Object recognition from local scale-invariant features</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,440.12,316.93,40.47,7.86;12,151.52,327.89,22.20,7.86">Computer vision</title>
		<imprint>
			<date type="published" when="1999">1999. 1999</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,349.81,337.98,7.86;12,151.52,360.77,234.86,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,271.69,349.81,190.26,7.86">Scale &amp; affine invariant interest point detectors</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,476.75,349.81,3.84,7.86;12,151.52,360.77,153.45,7.86">ternational journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="86" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,371.73,337.97,7.86;12,151.52,382.69,329.07,7.86;12,151.52,393.65,232.45,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="12,299.31,371.73,181.27,7.86;12,151.52,382.69,106.33,7.86">Efficient representation of local geometry for large scale object retrieval</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Perd'och</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,279.73,382.69,172.03,7.86;12,151.52,393.65,123.28,7.86">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference</note>
</biblStruct>

<biblStruct coords="12,142.62,404.61,337.97,7.86;12,151.52,415.56,329.07,7.86;12,151.52,426.52,311.79,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="12,380.92,404.61,99.67,7.86;12,151.52,415.56,259.61,7.86">Lost in quantization: Improving particular object retrieval in large scale image databases</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,432.52,415.56,48.07,7.86;12,151.52,426.52,283.12,7.86">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,437.48,337.98,7.86;12,151.52,448.44,329.07,7.86;12,151.52,459.40,295.55,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="12,394.44,437.48,86.15,7.86;12,151.52,448.44,181.36,7.86">Object retrieval with large vocabularies and fast spatial matching</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,356.68,448.44,123.91,7.86;12,151.52,459.40,46.11,7.86;12,228.96,459.40,113.56,7.86">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>CVPR&apos;07. IEEE Conference</note>
</biblStruct>

<biblStruct coords="12,142.62,470.36,337.97,7.86;12,151.52,481.32,329.07,7.86;12,151.52,492.28,329.07,7.86;12,151.52,503.24,329.07,7.86;12,151.52,514.19,115.42,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="12,355.50,481.32,125.08,7.86;12,151.52,492.28,324.94,7.86">A review of techniques for the identification and measurement of fish in underwater stereo-video image sequences</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Shortis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ravanbakskh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Shaifat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">S</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Seager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">F</forename><surname>Culverhouse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Cline</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">R</forename><surname>Edgington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,166.16,503.24,98.93,7.86">SPIE Optical Metrology</title>
		<imprint>
			<publisher>International Society for Optics and Photonics</publisher>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="87910G" to="87910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,525.15,337.98,7.86;12,151.52,536.11,329.07,7.86;12,151.52,547.07,329.07,7.86;12,151.52,558.03,25.60,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="12,282.96,536.11,127.48,7.86">Going deeper with convolutions</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,432.52,536.11,48.07,7.86;12,151.52,547.07,290.18,7.86">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,568.99,337.98,7.86;12,151.52,579.95,329.07,7.86;12,151.52,590.91,223.01,7.86" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="12,381.37,568.99,99.23,7.86;12,151.52,579.95,235.00,7.86">Automatic identification of bird calls using spectral ensemble average voice prints</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">A</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Prabhakar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,409.43,579.95,71.16,7.86;12,151.52,590.91,42.62,7.86">Signal Processing Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
	<note>14th European</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
