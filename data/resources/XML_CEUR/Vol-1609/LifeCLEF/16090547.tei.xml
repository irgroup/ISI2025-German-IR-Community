<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,145.44,116.95,324.49,12.62;1,214.47,134.89,186.41,12.62">Audio Based Bird Species Identification using Deep Learning Techniques</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,158.56,172.56,59.99,8.74"><forename type="first">Elias</forename><surname>Sprengel</surname></persName>
							<email>elias.sprengel@alumni.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">Eidgenössische Technische Hochschule (ETH) Zürich</orgName>
								<address>
									<addrLine>Rämistrasse 101</addrLine>
									<postCode>8092</postCode>
									<settlement>Zürich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,226.31,172.56,54.84,8.74"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
							<email>jaggi@inf.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">Eidgenössische Technische Hochschule (ETH) Zürich</orgName>
								<address>
									<addrLine>Rämistrasse 101</addrLine>
									<postCode>8092</postCode>
									<settlement>Zürich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,288.73,172.56,63.02,8.74"><forename type="first">Yannic</forename><surname>Kilcher</surname></persName>
							<email>yannic.kilcher@inf.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">Eidgenössische Technische Hochschule (ETH) Zürich</orgName>
								<address>
									<addrLine>Rämistrasse 101</addrLine>
									<postCode>8092</postCode>
									<settlement>Zürich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,378.71,172.56,78.09,8.74"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
							<email>thomas.hofmann@inf.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">Eidgenössische Technische Hochschule (ETH) Zürich</orgName>
								<address>
									<addrLine>Rämistrasse 101</addrLine>
									<postCode>8092</postCode>
									<settlement>Zürich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,145.44,116.95,324.49,12.62;1,214.47,134.89,186.41,12.62">Audio Based Bird Species Identification using Deep Learning Techniques</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">99B0C60FC2E6EFD610BC672ED2E42E6F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bird Identification</term>
					<term>Deep Learning</term>
					<term>Convolution Neural Network</term>
					<term>Audio Processing</term>
					<term>Data Augmentation</term>
					<term>Bird Species Recognition</term>
					<term>Acoustic classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we present a new audio classification method for bird species identification. Whereas most approaches apply nearest neighbour matching [6] or decision trees [8] using extracted templates for each bird species, ours draws upon techniques from speech recognition and recent advances in the domain of deep learning. With novel preprocessing and data augmentation methods, we train a convolutional neural network on the biggest publicly available dataset <ref type="bibr" coords="1,361.05,343.57,9.22,7.86" target="#b4">[5]</ref>. Our network architecture achieves a mean average precision score of 0.686 when predicting the main species of each sound file and scores 0.555 when background species are used as additional prediction targets. As this performance surpasses current state of the art results, our approach won this years international BirdCLEF 2016 Recognition Challenge <ref type="bibr" coords="1,378.06,398.36,9.98,7.86" target="#b2">[3,</ref><ref type="bibr" coords="1,388.05,398.36,6.66,7.86" target="#b3">4,</ref><ref type="bibr" coords="1,394.70,398.36,6.66,7.86" target="#b0">1]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Motivation</head><p>Large scale, accurate bird recognition is essential for avian biodiversity conservation. It helps us quantify the impact of land use and land management on bird species and is fundamental for bird watchers, conservation organizations, park rangers, ecology consultants, and ornithologists all over the world. Many books have been published <ref type="bibr" coords="1,225.74,565.80,15.99,8.74" target="#b9">[10,</ref><ref type="bibr" coords="1,241.72,565.80,7.99,8.74" target="#b1">2,</ref><ref type="bibr" coords="1,249.72,565.80,11.99,8.74" target="#b10">11]</ref> to help humans determine the correct species and dedicated online forums exist where recordings can be shared and discussed <ref type="bibr" coords="1,462.33,577.76,14.61,8.74" target="#b14">[15]</ref>. Nevertheless, because recordings, spanning hundreds of hours, need to be carefully analysed and categorised, large scale bird identification remains almost an impossible task to be done manually. It, therefore, seems natural to look at ways to automate the process. Unfortunately a number of challenges have made this task extremely difficult to tackle. Most prominent are:</p><p>-Background noise -Multiple birds singing at the same time (multi-label) -Difference between mating calls and songs -Inter-species variance <ref type="bibr" coords="2,248.17,143.90,10.52,8.74" target="#b8">[9]</ref> -Variable length of sound recordings -Large number of different species Because of these, most systems are developed to deal with only a small number of species and require a lot of re-training and fine-tuning for each new species. In this paper, we describe a fully automatic, robust machine learning method that is able to overcome these issues. We evaluated our method on the biggest publicly available dataset which contains over 33'000 recordings of 999 different species. We achieved a mean average precision (MAP) score of 0.69 and an accuracy score of 0.58 which is currently the highest recorded score. Consequently our approach won the international BirdCLEF 2016 Recognition Challenge <ref type="bibr" coords="2,447.98,270.16,10.79,8.74" target="#b2">[3,</ref><ref type="bibr" coords="2,458.77,270.16,7.20,8.74" target="#b3">4,</ref><ref type="bibr" coords="2,465.97,270.16,7.20,8.74" target="#b0">1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Approach</head><p>We use a convolutional neural network with five convolutional and one dense layer. Every convolutional layer uses a rectify activation function and is followed by a max-pooling layer. For preprocessing, we split the sound file into a signal part where bird songs/calls are audible and a noise part where no bird is singing/calling (background noise is still present in these parts). We compute the spectrograms (Short Time Fourier Transform) of both parts and split each spectrogram into equally sized chunks. Each chunk can be seen as the spectrogram of a short time interval (typically around 3 seconds). As such, we can use each chunk from the signal part as a unique training/testing sample for our neural network. A detailed description of every step will be provided in the next chapters.</p><p>Figure <ref type="figure" coords="2,181.14,448.96,4.98,8.74">1</ref> and Figure <ref type="figure" coords="2,240.26,448.96,4.98,8.74" target="#fig_0">2</ref> give an overview of our training / testing pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Feature Generation</head><p>The generation of good input features is vital to the success of the neural network. There are three main stages. First, we decide which parts of the sound file correspond to a bird singing/calling (signal parts) and which parts contain noise or silence (noise parts). Second, we compute the spectrogram for both signal and noise part. Third, we divide the spectrogram of each part into equally sized chunks. We can then use each chunk from the signal spectrogram as a unique sample for training/testing and augment it with a chunk from the noise spectrogram.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Signal/Noise Separation</head><p>To divide the sound file into a signal and a noise part, we first compute the spectrogram of the whole file. Note that all spectrograms in this paper are computed in the same way. First the signal is passed through a short-time Fourier  transform (STFT), this is done using a Hanning window function (size 512, 75% overlap). Then the logarithm of the amplitude of the STFT is taken. However, the signal/noise separation is the exception to this rule because here, we do not take the logarithm of the amplitude but instead divide every element by the maximum value, such that all values end up in the interval [0, 1]. With the spectrogram at hand, we are now able to look for the signal/noise intervals.</p><p>For the signal part we follow <ref type="bibr" coords="4,284.36,191.72,10.52,8.74" target="#b6">[7]</ref> quite closely. We first select all pixels in the spectrogram that are three times bigger than the row median and three times bigger than the column median. Intuitively, this gives us all the important parts of the spectrograms, because a high amplitude usually corresponds to a bird singing/calling. We set these pixels to 1 and everything else to 0. We apply a binary erosion and dilation filter to get rid of the noise and join segments. Experimentally we found that a 4 by 4 filter produced the best results. We create a new indicator vector which has as many elements as there are columns in the spectrogram. The i-th element in this vector is set to 1 if the i-th column contains at least one 1, otherwise it is set to 0. We smooth the indicator vector by applying two more binary dilation filters (filter size 4 by 1). Finally we scale our indicator vector to the length of the original sound file. We can now use it as a mask to extract the signal part. Figure <ref type="figure" coords="4,331.84,335.18,4.98,8.74">3</ref> shows a visual representation of each step.</p><p>For the noise part we follow the same steps but instead of selecting the pixels which are three times bigger than row and column median, we select all pixels which are 2.5 times bigger than the row and column median. We then proceed as described above but invert the result at the very end. Note that, by construction of our algorithm, a single column should never belong to both signal and noise part. On the other hand, it can happen that a column is not part of either noise nor signal part because we use different thresholds (3 versus 2.5). This is intended as it provides a safety margin for our selection process. The reasoning is that everything that was not selected as either signal nor noise, provides almost no information to the neural network. The bird is either barely audible/distorted or the sound does not match our concept of background noise very well.</p><p>The signal and noise masks split the sound file into many short intervals. We simply join these intervals together to form one signal-and one noise-sound-file. Everything that is not selected is disregarded and not used in any future steps. The transition marks, that occur when two segments are joined together, are usually not audible because the cuts happen when no bird is calling/singing. Furthermore, the use of the dilation filters, as described earlier, ensures that we keep the number of generated intervals to a minimum when applying the masks. From the two resulting sound files we can now compute a spectrogram for both signal and noise part. Figure <ref type="figure" coords="4,263.20,586.24,4.98,8.74" target="#fig_1">4</ref> shows an example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dividing the Spectrograms into Chunks</head><p>As described in the last section, we compute a spectrogram for both the signal and noise part of the sound file. Afterwards we split both spectrograms into chunks of equal size (we use a length of 512). The splitting is done for three reasons. For one, we need a fixed sized input for our neural network architecture. We could pad the input but the large variance in the length of the recordings would mean that some samples would contain over 99% padding. We could also try to use varying step sizes of our pooling layers but this would stretch or compress the signal in the time dimension. In comparison, chunks allow us to pad only the last part and keep our step size constant. Second, thanks to our signal/noise separation method we do not have to deal with the issue of empty chunks (without a bird calling/singing) which means we can use each chunk as a unique sample for training/testing. Third, we can let the network make multiple predictions per sound file (one prediction per chunk) and average them to generate a final prediction. This makes our predictions more robust and reliable. As an extension, one could try to merge multiple predictions in a more sophisticated way but, so far, no extensive testing has been done.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data Augmentation</head><p>Because the number of sound files is quite small, compared to the number of classes (the training set (of 24'607 files) contains an average of only 25 sound files per class), we need additional methods to avoid over fitting. Apart from drop-out, data augmentation was one of the most important ingredients to improve the generalization performance of the system. We apply four different data augmentation methods. For an overview of the the impact each data augmentation method has, consult Table <ref type="table" coords="7,273.80,399.30,3.87,8.74" target="#tab_1">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Time Shift</head><p>Every time we present the neural network with a training example, we shift it in time by a random amount. In terms of the spectrogram this means that we cut it into two parts and place the second part in front of the first (wrap around shifts). This creates a sharp corner where the end of the second part meets the beginning of the first part but all the information is preserved. With this augmentation we force the network to deal with irregularities in the spectrogram and also, more importantly, teach the network that bird songs/calls appear at any time, independent of the bird species.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pitch Shift</head><p>In a review of different augmentation methods <ref type="bibr" coords="8,351.35,287.56,15.50,8.74" target="#b11">[12]</ref> showed that pitch shifts (vertical shifts) also helped reducing the classification error. We found that, while a small shift (about 5%) seemed to help, a larger shift was not beneficial. Again we used a wrap-around method to preserve the complete information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Combining Same Class Audio Files</head><p>We follow <ref type="bibr" coords="8,180.55,382.68,15.50,8.74" target="#b13">[14]</ref> and add sound files that correspond to the same class. Adding is a simple process because each sound file can be represented by a single vector. If one of the sound files is shorter than the other we repeat the shorter one as many times as it is necessary. After adding two sound files, we re-normalize the result to preserve the original maximum amplitude of the sound files. The operation describes the effect of multiple birds (of the same species) singing at the same time. Adding files improves convergence because the neural network sees more important patterns at once, we also found a slight increase in the accuracy of the system (see Table <ref type="table" coords="8,232.34,478.32,3.87,8.74" target="#tab_1">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Adding Noise</head><p>One of the most important augmentation steps is to add background noise. In Section 2.1 we described how we split each file into a signal and noise part.</p><p>For every signal sample we can choose an arbitrary noise sample (since the background noise should be independent of the class label) and add it on top of the original training sample at hand. As for combining same class audio files, this operation should be done in the time domain by adding both sound files and repeating the smaller one as often as necessary. We can even add multiple noise samples. In our test we found that three noise samples added on top of the signal, each with a dampening factor of 0.4 produces the best results. This means that, given enough training time, for a single training sample we eventually add every possible background noise which decreases the generalization error.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Network architecture</head><p>Figure <ref type="figure" coords="9,166.47,313.97,4.98,8.74" target="#fig_2">5</ref> shows a visual representation of our neural network architecture. The network contains 5 convolutional layer, each followed by a max-pooling layer. We insert one dense layer before the final soft-max layer. The dense layer contains 1024 and the soft-max layer 1000 units, generating a probability for each class. We use batch normalization before every convolutional and before the dense layer. The convolutional layers use a rectify activation function. Drop-out is used on the input layer (probability 0.2), on the dense layer (probability 0.4) and on the soft-max layer (probability 0.4). As a cost function we use the single label categorical cross entropy function (in the log domain).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Batch Size</head><p>We use batches of 8 or 16 training examples. We found that using 16 training samples per batch produced slightly better results but, due to memory limitations of the GPU, some models were trained with only 8 samples per batch. If many samples, from the same sound file, are present in a single batch, the performance of the batch normalization function drops considerably. We, therefore, select the samples for each batch uniform at random without replacement. Normalizing the sound files beforehand might be an alternative solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Learning method</head><p>We use the Nesterov momentum method to compute the updates for our weights. The momentum is set to 0.9 and the initial learning rate is equal to 0.1. After 4 days of training (around 100 epochs) we reduce the learning rate to 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>We evaluate our results locally by splitting the original training set into a training and validation set. To preserve the original label distribution we group files by their class id (species) and used 10% of each group for validation and the remaining 90% for training. Note that, even for our contest submissions, we never trained on the validation set. Our contest results would probably improve, if training would be performed on both training and validation set.</p><p>Training the neural network takes a lot of time. We, therefore, choose a subset of the training set, containing 50 different species, to fine tune parameters. This (20 times smaller) dataset enabled us to test over 500 different network configurations. Our final configuration was then trained on the complete training set (considering all 999 species) and reached an accuracy score of 0.59 and a mean average precision (MAP) score of 0.67 on the local validation set (999 species). On the remote test set our best run reached a MAP score of 0.69 when considering only the main (foreground) species, 0.55 when considering the background species as well and 0.08 when only background species were considered. This means our approach outperformed the next best contestant by 17% in the category where background species were ignored. Figure <ref type="figure" coords="10,384.82,288.91,4.98,8.74">6</ref> shows a visual comparison of the scores for all participants. As seen in Figure <ref type="figure" coords="10,384.90,300.86,4.98,8.74">6</ref> we submitted a total Fig. <ref type="figure" coords="10,155.98,557.93,4.46,8.77">6</ref>: Official scores from the BirdCLEF 2016 Recognition Challenge. Our team name was "Cube" and we submitted four runs. of four runs. The first run "Cube Run 1" was an early submission where parameters had not yet been tuned and the model was only trained for a single day. The second and third run were almost identical but "Cube Run 2" was trained on spectrograms that were resized by 50% while "Cube Run 3" was trained on the original sized spectrograms. Both times the model was first trained for 4 days, using the Nesterov momentum method (momentum = 0.9, learning rate = 0.1) and then trained for one more day with a decreased learning rate of 0.01. Furthermore, "Cube Run 3" was trained with a batch size of 8 because of the limited GPU memory, while "Cube Run 2" was able to use batches of size 16 (scaled spectrograms). Finally, "Cube Run 4" was created by simply averaging the predictions from "Cube Run 2" and "Cube Run 3". We can see that "Cube Run 4" outperformed all other submission which means that an ensemble of neural networks could increase our score even further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Discussion</head><p>Our approach surpassed state of the art performance when targeting the dominant foreground species. When background species were taken into account, other approaches performed almost as well as ours. When no foreground species was present one other approach was able to outperform us. This should not surprise us, considering our data augmentation and preprocessing method. First of all, we were cutting out the noise part, focusing only on the signal part. In theory this should help our network to focus on the important parts but in practice we might disregard less audible background species. Second, we are augmenting our data by adding background noise from other files on top of the signal part. As shown in Table <ref type="table" coords="11,206.43,371.43,3.87,8.74" target="#tab_1">1</ref>, the score for identifying background species increases if we train without this data augmentation techniques. That means, even though, we do not use any data augmentation method when dealing with the test set, the network is still trained to ignore everything that happens in the background. One possible solution would be to alter the cost function and target background species as well. An other solution could be to employ a preprocessing step that tries to split the original files into differently sized parts, each part containing only one bird call/song. This is similar to <ref type="bibr" coords="11,310.98,455.11,10.51,8.74" target="#b6">[7]</ref> who compares single bird calls/songs instead of complete sound files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Unsuccessful approaches we tested</head><p>We tested a lot of different ideas and not all of them worked, we will briefly list them in this chapter to give a complete picture.</p><p>Bi-directional LSTM Recurrent Neural Networks: We tried different cost functions and parameters but were not able to match the performance of our convolutional neural network.</p><p>Regularization: We tested L1 and L2 regularization of all weights but found that our generalization error did not decrease. Furthermore, adding these extra terms made training considerably slower.</p><p>Non-Square-Filters: For the convolutional layers we tried to use non square filters because we wanted to treat the time dimension differently than the frequency dimension. We found, however, that small variations did not change the performance while an attempt with a 1D (height of filter equals height of spectrogram) convolution produced worse results.</p><p>Deeper-Networks: We tried to add more layers to our neural network but the performance dropped after adding the 5th layer. This seems to be a common problem and many solutions have been proposed, for example, using highway networks <ref type="bibr" coords="12,178.14,231.20,14.61,8.74">[13]</ref>. We have not tested any of these proposed solutions but they might be an important ingredient in an attempt to increase the accuracy of the system even further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Outlook</head><p>We already mentioned a few improvements that could be made. One idea is to use an ensemble of neural networks. An other idea is to modify our cost function to consider the background species or present single bird calls/songs instead of the currently fixed sized samples. One problem with the current approach is that longer files, as they generate more chunks, seem more important to the network. To combat this, we could show the same number of chunks for each class by repeating chunks from classes with a lower number of chunks / shorter files. Finally, the dataset provides us with a lot of meta-data: Date, time and location to name a few. We are currently only relying on the sound files but incorporating these values could greatly increase our score because we could narrow down the total number of species which we need to consider. While testing parameters, we found, for example, that with only 50 different species, we were able to reach a MAP score around 0.84 (compared to 0.67, our best score on the validation dataset). Training models for different regions / species and combining them using the meta-data, therefore, seems like a natural extension to the current approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,134.77,614.67,345.83,8.77;3,134.77,626.66,345.83,8.74;3,134.77,638.62,116.06,8.74"><head>Fig. 2 :</head><label>2</label><figDesc>Fig.2: Overview of the testing pipeline. Note that we get multiple predictions per sound file (one prediction per chunk/sample) which we can average to obtain a single prediction per file.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,134.77,728.93,30.12,8.77;5,186.57,728.96,41.93,8.74;5,247.34,728.96,8.03,8.74;5,274.23,728.96,24.96,8.74;5,318.04,728.96,22.22,8.74;5,359.11,728.96,11.93,8.74;5,389.89,728.96,13.84,8.74;5,422.59,728.96,12.72,8.74;5,454.16,728.96,26.43,8.74;5,134.77,740.92,345.82,8.74;5,134.77,752.87,345.82,8.74;5,134.77,764.83,76.79,8.74"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 3: Detection of signal parts for the file LIFE-CLEF2014 BIRDAMAZON XC WAV RN3508. The two dilation steps at the end are important because they end up improving the smoothness of our mask/signal part.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,134.77,235.07,345.83,8.77;9,134.77,247.05,345.82,8.74;9,134.77,259.01,192.72,8.74"><head>Fig. 5 :</head><label>5</label><figDesc>Fig.5: Architecture used for the run "Cube Run 2" in the BirdCLEF 2016 Recognition Challenge. For "Cube Run 3" the same architecture was used but the input image had dimensions 256 by 512.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="10,134.77,335.37,345.83,211.06"><head></head><label></label><figDesc></figDesc><graphic coords="10,134.77,335.37,345.83,211.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,134.77,433.92,345.83,193.58"><head>Table 1 :</head><label>1</label><figDesc>Mean Average Precision for different runs on a dataset with 50 random bird species. The baseline run uses all data augmentation methods (Background Noise, Same Class Combining, Time Shifts and Pitch Shifts), while all the other runs are missing one or two of the data augmentation methods. We use "w/o" as an abbreviation for "without". The first column corresponds to the mean average precision when only the foreground (FG) species are considered. The second column also considers the species in the background (BG) as prediction targets. Underlined are the best results in each category. We stopped all runs after 12 hours of training time.</figDesc><table coords="7,152.37,551.47,287.54,76.04"><row><cell></cell><cell cols="2">MAP (FG only) MAP (FG &amp; BG)</cell></row><row><cell>Baseline</cell><cell>0.842</cell><cell>0.728</cell></row><row><cell>w/o Noise</cell><cell>0.831</cell><cell>0.731</cell></row><row><cell>w/o Same Class</cell><cell>0.839</cell><cell>0.730</cell></row><row><cell>w/o Time Shift</cell><cell>0.801</cell><cell>0.701</cell></row><row><cell>w/o Pitch Shift</cell><cell>0.828</cell><cell>0.725</cell></row><row><cell>w/o Noise and Same Class</cell><cell>0.768</cell><cell>0.661</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. We would like to thank <rs type="institution">Amazon AWS</rs> for providing us with the computational resources, <rs type="person">Ivan Eggel</rs>, <rs type="person">Hervé Glotin</rs>, <rs type="person">Hervé Goëau</rs>, <rs type="person">Alexis Joly</rs>, <rs type="person">Henning Müller</rs> and <rs type="person">Willem-Pier Vellinga</rs> for organizing this task, the <rs type="institution">Xeno-Canto</rs> foundation for nature sounds as well as the French projects Pl@ntNet (<rs type="institution">INRIA</rs>, <rs type="institution">CIRAD</rs>, <rs type="institution">Tela Botanica)</rs> and <rs type="institution">SABIOD Mastodons</rs> for their support. Last but not least E.S. would like to thank <rs type="person">Samuel Bryner</rs>, <rs type="person">Judith Dittmer</rs>, <rs type="person">Nico Neureiter</rs> and <rs type="person">Boris Schröder-Esselbach</rs> for their helpful remarks and guidance throughout this project.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="12,142.96,646.84,337.64,8.12;12,151.52,657.79,125.31,8.12" xml:id="b0">
	<monogr>
		<ptr target="http://www.imageclef.org/node/199" />
		<title level="m" coord="12,151.53,646.84,203.41,7.86">Imageclef / lifeclef -multimedia retrieval in clef</title>
		<imprint>
			<date type="published" when="2016-05-22">2016-05-22</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,120.67,337.64,7.86;13,151.52,131.63,329.07,7.86;13,151.52,142.59,25.60,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="13,307.50,120.67,173.09,7.86;13,151.52,131.63,67.34,7.86">The species of birds of South America and their distribution</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>De Schauensee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Eisenmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,225.15,131.63,114.38,7.86">Academy of Natural Sciences</title>
		<imprint>
			<date type="published" when="1966">1966</date>
			<pubPlace>Livingston, Narberth, Pa.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,153.55,337.63,7.86;13,151.52,164.51,219.38,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,398.20,153.55,82.39,7.86;13,151.52,164.51,64.20,7.86">Lifeclef bird identification task 2016</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,237.07,164.51,83.66,7.86">CLEF working notes</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">2016</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,175.46,337.63,7.86;13,151.52,186.42,329.07,7.86;13,151.52,197.38,282.60,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,360.12,186.42,120.47,7.86;13,151.52,197.38,125.04,7.86">Lifeclef 2016: multimedia life species identification challenges</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Champ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,297.60,197.38,107.84,7.86">Proceedings of CLEF 2016</title>
		<meeting>CLEF 2016</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,208.34,337.63,7.86;13,151.52,219.30,329.07,7.86;13,151.52,230.26,329.07,7.86;13,151.52,241.22,246.58,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,378.80,219.30,101.79,7.86;13,151.52,230.26,143.37,7.86">Lifeclef 2015: multimedia life species identification challenges</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,318.07,230.26,162.52,7.86;13,151.52,241.22,123.36,7.86">Experimental IR Meets Multilinguality, Multimodality, and Interaction</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="462" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,252.18,337.64,7.86;13,151.52,263.14,329.07,7.86;13,151.52,274.09,25.60,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,347.72,252.18,132.88,7.86;13,151.52,263.14,223.09,7.86">Shared nearest neighbors match kernel for bird songs identification-lifeclef 2015 challenge</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Leveau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Champ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Buisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,394.57,263.14,44.01,7.86">CLEF 2015</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1391</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,285.05,337.64,7.86;13,151.52,296.01,329.07,7.86;13,151.52,306.97,272.42,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,203.00,285.05,277.59,7.86;13,151.52,296.01,67.66,7.86">Bird song classification in field recordings: winning solution for nips4b 2013 competition</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,240.38,296.01,240.21,7.86;13,151.52,306.97,152.38,7.86">Proc. of int. symp. Neural Information Scaled for Bioacoustics, sabiod. org/nips4b, joint to NIPS</title>
		<meeting>of int. symp. Neural Information Scaled for Bioacoustics, sabiod. org/nips4b, joint to NIPS<address><addrLine>, Nevada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="176" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,317.93,337.64,7.86;13,151.52,328.89,328.74,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,205.58,317.93,275.01,7.86;13,151.52,328.89,115.92,7.86">Improved automatic bird identification through decision tree based feature selection and bagging</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,288.82,328.89,162.78,7.86">Working notes of CLEF 2015 conference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,339.85,337.64,7.86;13,151.52,350.81,180.13,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,255.81,339.85,224.78,7.86;13,151.52,350.81,33.57,7.86">Song &quot;dialects&quot; in three populations of white-crowned sparrows</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Marler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,192.37,350.81,48.66,7.86">The Condor</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="368" to="377" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,361.77,337.98,7.86;13,151.52,372.73,100.70,7.86" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L</forename><surname>Restall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Rodner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Lentino</surname></persName>
		</author>
		<title level="m" coord="13,341.83,361.77,134.25,7.86">Birds of northern South America</title>
		<imprint>
			<publisher>Christopher Helm</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,383.68,337.98,7.86;13,151.52,394.64,151.98,7.86" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">S</forename><surname>Ridgely</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Tudor</surname></persName>
		</author>
		<title level="m" coord="13,251.76,383.68,228.83,7.86;13,151.52,394.64,14.38,7.86">Field guide to the songbirds of South America: the passerines</title>
		<imprint>
			<publisher>University of Texas Press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,405.60,337.98,7.86;13,151.52,416.56,124.77,7.86;13,134.77,427.52,11.78,7.86" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="13,246.69,405.60,233.90,7.86;13,151.52,416.56,124.77,7.86;13,134.77,427.52,7.85,7.86">Exploring data augmentation for improved singing voice detection with neural networks 13</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Grill</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="13,151.52,427.52,329.07,7.86;13,151.52,438.48,302.26,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="13,343.63,427.52,117.19,7.86">Training very deep networks</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,151.52,438.48,208.78,7.86">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2368" to="2376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,449.44,337.98,7.86;13,151.52,460.40,329.07,7.86;13,151.52,471.36,97.80,7.86" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="13,379.20,449.44,101.39,7.86;13,151.52,460.40,262.89,7.86">Deep convolutional neural networks and data augmentation for acoustic event detection</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.07160</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,142.62,482.31,314.89,7.86" xml:id="b14">
	<monogr>
		<title level="m" coord="13,151.52,482.31,277.31,7.86">Xeno Canto Foundation: Sharing bird sounds from around the world</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
